<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15506</link><description>&lt;p&gt;
AgentOhana&#65306;&#20026;&#26377;&#25928;&#26234;&#33021;&#20307;&#23398;&#20064;&#35774;&#35745;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15506
&lt;/p&gt;
&lt;p&gt;
AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24341;&#36215;&#20102;&#37325;&#22823;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#36827;&#34892;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20855;&#26377;&#22810;&#36718;&#36712;&#36857;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AgentOhana&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;AgentOhana&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#32858;&#21512;&#26234;&#33021;&#20307;&#36712;&#36857;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#26223;&#12290;&#23427;&#31934;&#24515;&#22320;&#23558;&#36825;&#20123;&#36712;&#36857;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#26684;&#24335;&#20013;&#65292;&#31616;&#21270;&#20102;&#20026;&#26234;&#33021;&#20307;&#35757;&#32451;&#20248;&#21270;&#30340;&#36890;&#29992;&#25968;&#25454;&#21152;&#36733;&#22120;&#30340;&#21019;&#24314;&#12290;&#36890;&#36807;&#25968;&#25454;&#32479;&#19968;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#27700;&#32447;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#35774;&#22791;&#20043;&#38388;&#30340;&#29420;&#31435;&#38543;&#26426;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;xLAM-v0.1&#65292;&#19968;&#20010;&#22823;&#21160;&#20316;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26469;&#25913;&#21892;&#24369;&#21040;&#24378;&#27867;&#21270;&#30340;&#21327;&#21516;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#25945;&#24072;&#20849;&#21516;&#30417;&#30563;&#24378;&#22823;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15505</link><description>&lt;p&gt;
Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26469;&#25913;&#21892;&#24369;&#21040;&#24378;&#27867;&#21270;&#30340;&#21327;&#21516;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#25945;&#24072;&#20849;&#21516;&#30417;&#30563;&#24378;&#22823;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#26377;&#21147;&#30340;&#27169;&#22411;&#32463;&#36807;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#30340;&#39044;&#35757;&#32451;&#21518;&#65292;&#30001;&#20110;&#32570;&#20047;&#32988;&#20219;&#30340;&#30417;&#30563;&#32773;&#65292;&#22312;&#24341;&#23548;&#20854;&#34892;&#20026;&#26102;&#21487;&#33021;&#20250;&#21464;&#24471;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#30417;&#30563;&#22122;&#22768;&#65292;&#19968;&#20010;&#24378;&#22823;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#21518;&#21487;&#33021;&#20250;&#36229;&#36234;&#20854;&#24369;&#25945;&#24072;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#24040;&#22823;&#33021;&#21147;&#24046;&#36317;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#25945;&#24072;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#30340;&#36890;&#25165;&#25945;&#24072;&#65292;&#20849;&#21516;&#30417;&#30563;&#24378;&#22823;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#20998;&#23618;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#38024;&#23545;&#21327;&#21516;&#30417;&#30563;&#30340;&#32452;&#20214;&#65306;(i)&#25105;&#20204;&#36880;&#27493;&#20132;&#26367;&#36827;&#34892;&#23398;&#29983;&#35757;&#32451;&#21644;&#25945;&#24072;&#20998;&#37197;&#65292;&#21033;&#29992;&#24378;&#22823;&#23398;&#29983;&#27169;&#22411;&#30340;&#22686;&#38271;&#26469;&#35782;&#21035;&#21487;&#33021;&#30340;&#30417;&#30563;&#26041;&#24335;&#65307;(ii)&#25105;&#20204;&#35880;&#24910;&#22320;&#24378;&#21270;&#25945;&#24072;-&#23398;&#29983;&#21644;&#23616;&#37096;-&#20840;&#23616;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15505v1 Announce Type: cross  Abstract: Steering the behavior of a strong model pre-trained on internet-scale data can be difficult due to the scarcity of competent supervisors. Recent studies reveal that, despite supervisory noises, a strong student model may surpass its weak teacher when fine-tuned on specific objectives. Yet, the effectiveness of such weak-to-strong generalization remains limited, especially in the presence of large capability gaps. In this paper, we propose to address this challenge by harnessing a diverse set of specialized teachers, instead of a single generalist one, that collectively supervises the strong student. Our approach resembles the classical hierarchical mixture of experts, with two components tailored for co-supervision: (i) we progressively alternate student training and teacher assignment, leveraging the growth of the strong student to identify plausible supervisions; (ii) we conservatively enforce teacher-student and local-global consist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Gen4Gen&#65292;&#20026;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#20010;&#24615;&#21270;&#22810;&#27010;&#24565;&#32452;&#21512;&#30340;&#38382;&#39064;&#32780;&#25552;&#20986;&#30340;&#29983;&#25104;&#25968;&#25454;&#31649;&#36947;&#12290;</title><link>https://arxiv.org/abs/2402.15504</link><description>&lt;p&gt;
Gen4Gen: &#29983;&#25104;&#24335;&#22810;&#27010;&#24565;&#32452;&#21512;&#30340;&#29983;&#25104;&#25968;&#25454;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15504
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Gen4Gen&#65292;&#20026;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#20010;&#24615;&#21270;&#22810;&#27010;&#24565;&#32452;&#21512;&#30340;&#38382;&#39064;&#32780;&#25552;&#20986;&#30340;&#29983;&#25104;&#25968;&#25454;&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21644;&#21512;&#25104;&#21253;&#21547;&#26032;&#39062;&#12289;&#20010;&#24615;&#21270;&#27010;&#24565;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;&#65292;&#20182;&#20204;&#33258;&#24049;&#30340;&#23456;&#29289;&#25110;&#29305;&#23450;&#29289;&#21697;&#65289;&#65292;&#21482;&#38656;&#23569;&#37327;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20010;&#24615;&#21270;&#20013;&#30340;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#24403;&#21069;&#30340;&#20010;&#24615;&#21270;&#25216;&#26415;&#26410;&#33021;&#21487;&#38752;&#22320;&#25193;&#23637;&#21040;&#22810;&#20010;&#27010;&#24565;--&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#65292;LAION&#65289;&#20013;&#22797;&#26434;&#22330;&#26223;&#19982;&#31616;&#21333;&#25991;&#26412;&#25551;&#36848;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#25152;&#23548;&#33268;&#30340;&#12290;&#20854;&#27425;&#65292;&#23545;&#20110;&#21253;&#21547;&#22810;&#20010;&#20010;&#24615;&#21270;&#27010;&#24565;&#30340;&#22270;&#20687;&#65292;&#32570;&#20047;&#19968;&#20010; holistic&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#19981;&#20165;&#20010;&#24615;&#21270;&#27010;&#24565;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#65292;&#36824;&#26377;&#22270;&#20687;&#26159;&#21542;&#21253;&#21547;&#25152;&#26377;&#27010;&#24565;&#20197;&#21450;&#22270;&#20687;&#26159;&#21542;&#20934;&#30830;&#22320;&#21453;&#26144;&#20102;&#25972;&#20307;&#25991;&#26412;&#25551;&#36848;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;Gen4Gen&#65292;&#19968;&#20010;&#21322;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15504v1 Announce Type: cross  Abstract: Recent text-to-image diffusion models are able to learn and synthesize images containing novel, personalized concepts (e.g., their own pets or specific items) with just a few examples for training. This paper tackles two interconnected issues within this realm of personalizing text-to-image diffusion models. First, current personalization techniques fail to reliably extend to multiple concepts -- we hypothesize this to be due to the mismatch between complex scenes and simple text descriptions in the pre-training dataset (e.g., LAION). Second, given an image containing multiple personalized concepts, there lacks a holistic metric that evaluates performance on not just the degree of resemblance of personalized concepts, but also whether all concepts are present in the image and whether the image accurately reflects the overall text description. To address these issues, we introduce Gen4Gen, a semi-automated dataset creation pipeline util
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#30340;API&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.15491</link><description>&lt;p&gt;
API-BLEND&#65306;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;API LLM&#30340;&#32508;&#21512;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#30340;API&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#25928;&#20351;&#29992;&#24037;&#20855;&#21644;&#22806;&#37096;&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;APIs&#65289;&#26469;&#35268;&#21010;&#21644;&#23436;&#25104;&#20219;&#21153;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#65292;&#23545;&#21487;&#20197;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#20851;&#27880;&#35782;&#21035;&#12289;&#25972;&#29702;&#21644;&#36716;&#21270;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#27169;&#25311;&#28041;&#21450;API&#20219;&#21153;&#30340;&#30495;&#23454;&#22330;&#26223;&#65292;&#22914;API/&#24037;&#20855;&#26816;&#27979;&#12289;&#27133;&#22635;&#20805;&#20197;&#21450;&#26816;&#27979;&#21040;&#30340;API&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15491v1 Announce Type: cross  Abstract: There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;</title><link>https://arxiv.org/abs/2402.15487</link><description>&lt;p&gt;
RoboEXP: &#36890;&#36807;&#20132;&#20114;&#24335;&#25506;&#32034;&#23454;&#29616;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#25506;&#32034;&#21608;&#22260;&#29615;&#22659;&#20197;&#36866;&#24212;&#24182;&#24212;&#23545;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#24182;&#29983;&#25104;&#19968;&#20010;&#25429;&#25417;&#22522;&#30784;&#29615;&#22659;&#32467;&#26500;&#30340;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65288;ACSG&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15487v1 Announce Type: cross  Abstract: Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments. Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions. This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments. In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene. To this end, we present the Robotic Explo
&lt;/p&gt;</description></item><item><title>&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#23186;&#20307;&#22320;&#29702;&#23450;&#20301;&#20013;&#23637;&#31034;&#20986;&#21152;&#36895;&#20154;&#21475;&#36137;&#21334;&#35843;&#26597;&#30340;&#37325;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15448</link><description>&lt;p&gt;
&#22810;&#23186;&#20307;&#22320;&#29702;&#23450;&#20301;&#22312;&#20154;&#21475;&#36137;&#21334;&#35843;&#26597;&#20013;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Computer Vision for Multimedia Geolocation in Human Trafficking Investigation: A Systematic Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15448
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#23186;&#20307;&#22320;&#29702;&#23450;&#20301;&#20013;&#23637;&#31034;&#20986;&#21152;&#36895;&#20154;&#21475;&#36137;&#21334;&#35843;&#26597;&#30340;&#37325;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#22320;&#29702;&#23450;&#20301;&#30340;&#20219;&#21153;&#27491;&#22312;&#25104;&#20026;&#25968;&#23383;&#21462;&#35777;&#24037;&#20855;&#21253;&#30340;&#19968;&#20010;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#26377;&#25928;&#25171;&#20987;&#20154;&#21475;&#36137;&#21334;&#12289;&#20799;&#31461;&#24615;&#21093;&#21066;&#21644;&#20854;&#20182;&#38750;&#27861;&#34892;&#20026;&#12290;&#24403;&#22810;&#23186;&#20307;&#20869;&#23481;&#36890;&#36807;&#21363;&#26102;&#36890;&#35759;&#21644;&#31038;&#20132;&#23186;&#20307;&#20998;&#20139;&#26102;&#65292;&#36890;&#24120;&#20250;&#21093;&#31163;&#22522;&#20110;&#20803;&#25968;&#25454;&#30340;&#22320;&#29702;&#23450;&#20301;&#20449;&#24687;&#12290;&#22312;&#36825;&#20123;&#20869;&#23481;&#20013;&#36827;&#34892;&#22320;&#29702;&#23450;&#20301;&#12289;&#22320;&#29702;&#26631;&#35760;&#25110;&#25214;&#21040;&#22320;&#29702;&#32447;&#32034;&#30340;&#22797;&#26434;&#24615;&#24448;&#24448;&#23545;&#35843;&#26597;&#20154;&#21592;&#36896;&#25104;&#19981;&#24517;&#35201;&#30340;&#36127;&#25285;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#24403;&#20195;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#26174;&#31034;&#20986;&#21152;&#36895;&#22810;&#23186;&#20307;&#22320;&#29702;&#23450;&#20301;&#20219;&#21153;&#30340;&#37325;&#22823;&#28508;&#21147;&#12290;&#36825;&#39033;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#24443;&#24213;&#23457;&#26597;&#20102;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#36827;&#34892;&#22810;&#23186;&#20307;&#22320;&#29702;&#23450;&#20301;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#21152;&#36895;&#20154;&#21475;&#36137;&#21334;&#35843;&#26597;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15448v1 Announce Type: cross  Abstract: The task of multimedia geolocation is becoming an increasingly essential component of the digital forensics toolkit to effectively combat human trafficking, child sexual exploitation, and other illegal acts. Typically, metadata-based geolocation information is stripped when multimedia content is shared via instant messaging and social media. The intricacy of geolocating, geotagging, or finding geographical clues in this content is often overly burdensome for investigators. Recent research has shown that contemporary advancements in artificial intelligence, specifically computer vision and deep learning, show significant promise towards expediting the multimedia geolocation task. This systematic literature review thoroughly examines the state-of-the-art leveraging computer vision techniques for multimedia geolocation and assesses their potential to expedite human trafficking investigation. This includes a comprehensive overview of the a
&lt;/p&gt;</description></item><item><title>&#22312;&#36845;&#20195;&#20449;&#24565;&#20462;&#35746;&#20013;&#65292;&#26377;&#26102;&#20505;&#22312;&#20854;&#20182;&#20462;&#35746;&#23384;&#22312;&#26102;&#20250;&#20986;&#29616;&#20449;&#24565;&#20462;&#35746;&#30340;&#22810;&#20313;&#24773;&#20917;&#65292;&#20197;&#21450;&#32473;&#20986;&#20102;&#23548;&#33268;&#24207;&#21015;&#20013;&#31532;&#19968;&#20010;&#20462;&#35746;&#22810;&#20313;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.15445</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#24536;&#35760;&#25105;&#20204;&#26159;&#22914;&#20309;&#23398;&#20064;&#30340;&#65311;&#22312;&#36845;&#20195;&#20449;&#24565;&#20462;&#35746;&#20013;&#30340;&#20449;&#24565;&#22810;&#20313;
&lt;/p&gt;
&lt;p&gt;
Can we forget how we learned? Doxastic redundancy in iterated belief revision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15445
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36845;&#20195;&#20449;&#24565;&#20462;&#35746;&#20013;&#65292;&#26377;&#26102;&#20505;&#22312;&#20854;&#20182;&#20462;&#35746;&#23384;&#22312;&#26102;&#20250;&#20986;&#29616;&#20449;&#24565;&#20462;&#35746;&#30340;&#22810;&#20313;&#24773;&#20917;&#65292;&#20197;&#21450;&#32473;&#20986;&#20102;&#23548;&#33268;&#24207;&#21015;&#20013;&#31532;&#19968;&#20010;&#20462;&#35746;&#22810;&#20313;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#30340;&#33719;&#21462;&#26041;&#24335;&#21487;&#33021;&#21464;&#24471;&#26080;&#20851;&#32039;&#35201;&#12290;&#26126;&#26174;&#30340;&#24773;&#20917;&#26159;&#24403;&#26576;&#20107;&#34987;&#22810;&#27425;&#30830;&#35748;&#26102;&#12290;&#22312;&#36845;&#20195;&#20449;&#24565;&#20462;&#35746;&#26041;&#38754;&#65292;&#29305;&#23450;&#30340;&#20462;&#35746;&#22312;&#20854;&#20182;&#20462;&#35746;&#23384;&#22312;&#26102;&#21487;&#33021;&#21464;&#24471;&#26080;&#20851;&#32039;&#35201;&#12290;&#31616;&#21333;&#30340;&#37325;&#22797;&#26159;&#19968;&#20010;&#20363;&#23376;&#65292;&#20294;&#24182;&#38750;&#21807;&#19968;&#30340;&#24773;&#20917;&#12290;&#26377;&#26102;&#65292;&#21363;&#20351;&#27809;&#26377;&#30456;&#31561;&#30340;&#20462;&#35746;&#23384;&#22312;&#65292;&#29978;&#33267;&#27809;&#26377;&#26263;&#31034;&#23427;&#30340;&#20854;&#20182;&#20462;&#35746;&#65292;&#19968;&#20010;&#20462;&#35746;&#20063;&#20250;&#21464;&#24471;&#22810;&#20313;&#12290;&#32473;&#20986;&#20102;&#35789;&#20856;&#20462;&#35746;&#24207;&#21015;&#20013;&#31532;&#19968;&#20010;&#20462;&#35746;&#22810;&#20313;&#30340;&#19968;&#20010;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#12290;&#21363;&#20351;&#21482;&#26377;&#20004;&#20010;&#21629;&#39064;&#20462;&#35746;&#65292;&#35813;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#20063;&#26159;coNP-&#23436;&#20840;&#30340;&#12290;&#22312;Horn&#24773;&#20917;&#19979;&#22797;&#26434;&#24615;&#30456;&#21516;&#65292;&#20294;&#21482;&#26377;&#19981;&#21463;&#38480;&#21046;&#30340;&#20462;&#35746;&#25968;&#37327;&#65306;&#22312;&#20004;&#20010;&#20462;&#35746;&#24773;&#20917;&#19979;&#23427;&#21464;&#20026;&#22810;&#39033;&#24335;&#12290;&#35789;&#20856;&#20462;&#35746;&#19981;&#20165;&#20165;&#22240;&#20026;&#23427;&#20204;&#26412;&#36523;&#26159;&#30456;&#20851;&#30340;&#65292;&#20063;&#22240;&#20026;&#23427;&#20204;&#30340;&#24207;&#21015;&#26159;&#29992;&#20110;&#34920;&#31034;&#36845;&#20195;&#20462;&#35746;&#36807;&#31243;&#29366;&#24577;&#30340;&#24120;&#35265;&#26426;&#21046;&#20013;&#26368;&#32039;&#20945;&#30340;&#12290;&#32553;&#30701;&#35789;&#20856;&#20462;&#35746;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15445v1 Announce Type: new  Abstract: How information was acquired may become irrelevant. An obvious case is when something is confirmed many times. In terms of iterated belief revision, a specific revision may become irrelevant in presence of others. Simple repetitions are an example, but not the only case when this happens. Sometimes, a revision becomes redundant even in presence of none equal, or even no else implying it. A necessary and sufficient condition for the redundancy of the first of a sequence of lexicographic revisions is given. The problem is coNP-complete even with two propositional revisions only. Complexity is the same in the Horn case but only with an unbounded number of revisions: it becomes polynomial with two revisions. Lexicographic revisions are not only relevant by themselves, but also because sequences of them are the most compact of the common mechanisms used to represent the state of an iterated revision process. Shortening sequences of lexicograp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#65288;AdaMF-MAT&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#23384;&#22312;&#30340;&#27169;&#24577;&#20449;&#24687;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#30340;&#21147;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.15444</link><description>&lt;p&gt;
&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#22312;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#65288;AdaMF-MAT&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#23384;&#22312;&#30340;&#27169;&#24577;&#20449;&#24687;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#30340;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MMKGC&#65289;&#26088;&#22312;&#36890;&#36807;&#23558;&#23454;&#20307;&#30340;&#32467;&#26500;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#32435;&#20837;&#21028;&#21035;&#27169;&#22411;&#26469;&#39044;&#27979;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#20013;&#32570;&#22833;&#30340;&#19977;&#20803;&#32452;&#12290;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#23558;&#20849;&#21516;&#24037;&#20316;&#20197;&#34913;&#37327;&#19977;&#20803;&#32452;&#30340;&#21487;&#33021;&#24615;&#12290;&#29616;&#26377;&#30340;MMKGC&#26041;&#27861;&#24573;&#35270;&#20102;&#23454;&#20307;&#20043;&#38388;&#27169;&#24577;&#20449;&#24687;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#27169;&#24577;&#34701;&#21512;&#19981;&#36275;&#20197;&#21450;&#23545;&#21407;&#22987;&#27169;&#24577;&#20449;&#24687;&#30340;&#20302;&#25928;&#21033;&#29992;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#65288;AdaMF-MAT&#65289;&#65292;&#20197;&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#22312;MMKGC&#20013;&#30340;&#21147;&#37327;&#12290;AdaMF-MAT&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#24577;&#26435;&#37325;&#23454;&#29616;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#22686;&#24378;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;MMKGC&#27169;&#22411;&#21644;&#35757;&#32451;&#30340;&#21327;&#21516;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15444v1 Announce Type: new  Abstract: Multi-modal knowledge graph completion (MMKGC) aims to predict the missing triples in the multi-modal knowledge graphs by incorporating structural, visual, and textual information of entities into the discriminant models. The information from different modalities will work together to measure the triple plausibility. Existing MMKGC methods overlook the imbalance problem of modality information among entities, resulting in inadequate modal fusion and inefficient utilization of the raw modality information. To address the mentioned problems, we propose Adaptive Multi-modal Fusion and Modality Adversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality information for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive modality weights and further generates adversarial samples by modality-adversarial training to enhance the imbalanced modality information. Our approach is a co-design of the MMKGC model and training s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.15441</link><description>&lt;p&gt;
&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Active Few-Shot Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#23569;&#26679;&#26412;&#24494;&#35843;&#26159;&#20256;&#32479;&#20027;&#21160;&#23398;&#20064;&#21644;&#36716;&#23548;&#20027;&#21160;&#23398;&#20064;&#30340;&#27867;&#21270;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#22522;&#20110;&#36716;&#23548;&#23398;&#20064;&#65288;ITL&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#37319;&#26679;&#20197;&#26368;&#22823;&#21270;&#33719;&#24471;&#23545;&#25351;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;ITL&#22343;&#21248;&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#33719;&#21462;&#30340;&#26368;&#23567;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#25209;&#25512;&#23548;&#20986;&#36825;&#31181;&#27867;&#21270;&#30028;&#38480;&#30340;&#20154;&#65292;&#36825;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#23558;ITL&#24212;&#29992;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;ITL&#26126;&#26174;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20854;&#32479;&#35745;&#20445;&#35777;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#21028;&#26029;&#22256;&#38590;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15429</link><description>&lt;p&gt;
ProTIP&#65306;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#25239;&#38543;&#26426;&#25200;&#21160;&#30340;&#27010;&#29575;&#40065;&#26834;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20854;&#32479;&#35745;&#20445;&#35777;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#21028;&#26029;&#22256;&#38590;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#23637;&#29616;&#20102;&#22312;&#31616;&#21333;&#25991;&#26412;&#25551;&#36848;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#19968;&#26679;&#65292;DMs&#23384;&#22312;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;T2I DMs&#30340;&#40065;&#26834;&#24615;&#26102;&#65292;&#23384;&#22312;&#20197;&#20108;&#20803;&#25110;&#26368;&#22351;&#24773;&#20917;&#38382;&#39064;&#35299;&#26041;&#38754;&#30340;&#23581;&#35797;&#65292;&#20294;&#26080;&#27861;&#22238;&#31572;&#27169;&#22411;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AE&#65289;&#26102;&#30340;&#24635;&#20307;&#40065;&#26834;&#24615;&#22914;&#20309;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#24341;&#20837;&#20102;T2I DMs&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65307;&#28982;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#35780;&#20272;&#12290;&#20027;&#35201;&#25361;&#25112;&#28304;&#33258;&#65306;i&#65289;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65307;&#21644;ii&#65289;&#30830;&#23450;&#25200;&#21160;&#36755;&#20837;&#26159;&#21542;&#20026;AE&#28041;&#21450;&#27604;&#36739;&#20004;&#20010;&#36755;&#20986;&#20998;&#24067;&#65292;&#36825;&#19982;&#20854;&#20182;DL&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#65289;&#19981;&#21516;&#65292;&#20854;&#20013;AE&#26159;&#22312;&#26631;&#31614;&#38169;&#35823;&#39044;&#27979;&#26102;&#34987;&#35782;&#21035;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15429v1 Announce Type: cross  Abstract: Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#20102;&#25104;&#21151;&#34900;&#25509;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#25104;&#23545;&#21644;&#22522;&#20110;&#32676;&#20307;&#30340;&#21516;&#27493;&#65292;&#20026;&#20154;&#26426;/&#26426;&#22120;&#20154;&#20132;&#20114;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.15427</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#32676;&#20307;&#20013;&#30340;&#34900;&#25509;&#65306;&#20174;&#20154;&#38469;&#21327;&#20316;&#20013;&#23398;&#21040;&#30340;&#32463;&#39564;&#20248;&#21270;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Understanding Entrainment in Human Groups: Optimising Human-Robot Collaboration from Lessons Learned during Human-Human Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#20102;&#25104;&#21151;&#34900;&#25509;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#25104;&#23545;&#21644;&#22522;&#20110;&#32676;&#20307;&#30340;&#21516;&#27493;&#65292;&#20026;&#20154;&#26426;/&#26426;&#22120;&#20154;&#20132;&#20114;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#34900;&#25509;&#22312;&#21327;&#20316;&#36807;&#31243;&#20013;&#31215;&#26497;&#24433;&#21709;&#20449;&#20219;&#12289;&#24895;&#24847;&#21512;&#20316;&#20197;&#21450;&#23545;&#21512;&#20316;&#32773;&#30340;&#22909;&#24863;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#25506;&#35752;&#20102;&#25104;&#21151;&#34900;&#25509;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#25104;&#23545;&#21644;&#22522;&#20110;&#32676;&#20307;&#30340;&#21516;&#27493;&#12290;&#25105;&#20204;&#20174;&#24037;&#19994;&#22330;&#26223;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#24555;&#33410;&#22863;&#12289;&#30701;&#21608;&#26399;&#37325;&#22797;&#20219;&#21153;&#12290;&#21033;&#29992;&#36816;&#21160;&#36319;&#36394;&#25216;&#26415;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20108;&#20154;&#21644;&#19977;&#20154;&#20219;&#21153;&#23436;&#25104;&#20013;&#30340;&#34900;&#25509;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#38899;&#35270;&#39057;&#24405;&#21046;&#21644;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#26469;&#23545;&#21442;&#19982;&#32773;&#30340;&#32463;&#39564;&#36827;&#34892;&#24773;&#22659;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20026;&#20154;&#26426;/&#26426;&#22120;&#20154;&#20132;&#20114;&#65288;HCI/HRI&#65289;&#25991;&#29486;&#20316;&#20986;&#36129;&#29486;&#65292;&#20197;&#30830;&#23450;&#22312;&#25104;&#23545;&#21644;&#22522;&#20110;&#32676;&#20307;&#30340;&#21327;&#20316;&#20013;&#30340;&#34900;&#25509;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19982;&#25104;&#21151;&#34900;&#25509;&#30456;&#20851;&#30340;&#20116;&#20010;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15427v1 Announce Type: cross  Abstract: Successful entrainment during collaboration positively affects trust, willingness to collaborate, and likeability towards collaborators. In this paper, we present a mixed-method study to investigate characteristics of successful entrainment leading to pair and group-based synchronisation. Drawing inspiration from industrial settings, we designed a fast-paced, short-cycle repetitive task. Using motion tracking, we investigated entrainment in both dyadic and triadic task completion. Furthermore, we utilise audio-video recordings and semi-structured interviews to contextualise participants' experiences. This paper contributes to the Human-Computer/Robot Interaction (HCI/HRI) literature using a human-centred approach to identify characteristics of entrainment during pair- and group-based collaboration. We present five characteristics related to successful entrainment. These are related to the occurrence of entrainment, leader-follower patt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#21644;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#26377;&#25928;&#20943;&#23569;&#24187;&#35273;&#30340;&#29983;&#25104;&#65292;&#24182;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.15422</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24544;&#23454;&#19988;&#39640;&#36136;&#37327;&#30340;&#30149;&#20154;&#24635;&#32467;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#21644;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#26377;&#25928;&#20943;&#23569;&#24187;&#35273;&#30340;&#29983;&#25104;&#65292;&#24182;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#32463;&#24120;&#38754;&#20020;&#38590;&#20197;&#29702;&#35299;&#20854;&#20303;&#38498;&#24773;&#20917;&#30340;&#22256;&#38590;&#65292;&#32780;&#21307;&#25252;&#20154;&#21592;&#36164;&#28304;&#26377;&#38480;&#20197;&#25552;&#20379;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#23545;&#29983;&#25104;&#24635;&#32467;&#30340;&#24544;&#23454;&#24615;&#21644;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#29992;&#20110;&#24187;&#35273;&#65292;&#35753;&#20004;&#20301;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#20102;100&#20010;&#30495;&#23454;&#24635;&#32467;&#21644;100&#20010;&#29983;&#25104;&#30340;&#24635;&#32467;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;Llama 2&#27599;&#20010;&#24635;&#32467;&#30340;&#24187;&#35273;&#20174;2.60&#38477;&#20302;&#21040;1.55&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;&#34429;&#28982;&#25928;&#26524;&#20173;&#28982;&#23384;&#22312;&#65292;&#20294;&#24403;&#20351;&#29992;&#20116;&#20010;&#20363;&#23376;&#25552;&#31034;GPT-4&#26102;&#65292;&#35813;&#25928;&#26524;&#35201;&#23567;&#24471;&#22810;&#65288;0.70&#38477;&#33267;0.40&#65289;&#12290;&#25105;&#20204;&#36824;&#23545;&#26080;&#24187;&#35273;&#21644;&#25913;&#36827;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#12290;&#21363;&#20351;&#22312;&#24187;&#35273;&#33258;&#30001;&#25968;&#25454;&#19979;&#65292;GPT-4&#20063;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15422v1 Announce Type: cross  Abstract: Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we develop a rigorous labeling protocol for hallucinations, and have two medical experts annotate 100 real-world summaries and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. Although the effect is still present, it is much smaller for GPT-4 when prompted with five examples (0.70 to 0.40). We also conduct a qualitative evaluation using hallucination-free and improved training data. GPT-4 shows very good results even in 
&lt;/p&gt;</description></item><item><title>&#36873;&#25321;&#36319;&#38543;&#31639;&#27861;&#26159;&#21542;&#20256;&#36798;&#26377;&#20851;&#20154;&#31867;&#33021;&#21147;&#30340;&#20449;&#24687;&#26159;&#23548;&#33268;&#31639;&#27861;&#21388;&#24694;&#29616;&#35937;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#31639;&#27861;&#21388;&#24694;&#8221;&#12290;</title><link>https://arxiv.org/abs/2402.15418</link><description>&lt;p&gt;
&#22768;&#35465;&#31639;&#27861;&#21388;&#24694;
&lt;/p&gt;
&lt;p&gt;
Reputational Algorithm Aversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15418
&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#36319;&#38543;&#31639;&#27861;&#26159;&#21542;&#20256;&#36798;&#26377;&#20851;&#20154;&#31867;&#33021;&#21147;&#30340;&#20449;&#24687;&#26159;&#23548;&#33268;&#31639;&#27861;&#21388;&#24694;&#29616;&#35937;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#31639;&#27861;&#21388;&#24694;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24120;&#24120;&#19981;&#24895;&#23558;&#31639;&#27861;&#20135;&#29983;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#24049;&#30340;&#20915;&#31574;&#20013;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#31639;&#27861;&#21388;&#24694;&#8221;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#31639;&#27861;&#21388;&#24694;&#26159;&#22914;&#20309;&#20135;&#29983;&#30340;&#65292;&#24403;&#36873;&#25321;&#36319;&#38543;&#31639;&#27861;&#20256;&#36798;&#26377;&#20851;&#20154;&#31867;&#33021;&#21147;&#30340;&#20449;&#24687;&#26102;&#12290;&#25105;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#24037;&#20316;&#32773;&#26681;&#25454;&#33258;&#24049;&#30340;&#31169;&#20154;&#20449;&#24687;&#21644;&#31639;&#27861;&#30340;&#20449;&#21495;&#23545;&#38543;&#26426;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#12290;&#20302;&#25216;&#33021;&#24037;&#20316;&#32773;&#25509;&#25910;&#21040;&#27604;&#31639;&#27861;&#26356;&#24046;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#24212;&#22987;&#32456;&#36981;&#24490;&#31639;&#27861;&#30340;&#20449;&#21495;&#65292;&#32780;&#39640;&#25216;&#33021;&#24037;&#20316;&#32773;&#25509;&#25910;&#21040;&#27604;&#31639;&#27861;&#26356;&#22909;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#26377;&#26102;&#24212;&#35813;&#35206;&#30422;&#31639;&#27861;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22768;&#35465;&#19978;&#30340;&#32771;&#34385;&#65292;&#20302;&#25216;&#33021;&#24037;&#20316;&#32773;&#20250;&#19981;&#21512;&#29702;&#22320;&#35206;&#30422;&#31639;&#27861;&#65292;&#20197;&#22686;&#21152;&#34987;&#35270;&#20026;&#39640;&#25216;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;&#35813;&#27169;&#22411;&#20026;&#19982;AI&#31995;&#32479;&#21487;&#33021;&#20250;&#21462;&#20195;&#35768;&#22810;&#31867;&#22411;&#30340;&#24037;&#20316;&#32773;&#30340;&#24191;&#27867;&#20851;&#27880;&#25552;&#20379;&#20102;&#23436;&#20840;&#29702;&#24615;&#30340;&#24494;&#35266;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15418v1 Announce Type: cross  Abstract: People are often reluctant to incorporate information produced by algorithms into their decisions, a phenomenon called "algorithm aversion". This paper shows how algorithm aversion arises when the choice to follow an algorithm conveys information about a human's ability. I develop a model in which workers make forecasts of a random outcome based on their own private information and an algorithm's signal. Low-skill workers receive worse information than the algorithm and hence should always follow the algorithm's signal, while high-skill workers receive better information than the algorithm and should sometimes override it. However, due to reputational concerns, low-skill workers inefficiently override the algorithm to increase the likelihood they are perceived as high-skill. The model provides a fully rational microfoundation for algorithm aversion that aligns with the broad concern that AI systems will displace many types of workers.
&lt;/p&gt;</description></item><item><title>TransFlower&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#37319;&#29992;Flow-to-Flow&#27880;&#24847;&#21147;&#26469;&#39044;&#27979;&#22478;&#24066;&#36890;&#21220;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15398</link><description>&lt;p&gt;
TransFlower: &#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;Flow-to-Flow&#27880;&#24847;&#21147;&#36827;&#34892;&#36890;&#21220;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow Attention for Commuting Flow Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15398
&lt;/p&gt;
&lt;p&gt;
TransFlower&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#37319;&#29992;Flow-to-Flow&#27880;&#24847;&#21147;&#26469;&#39044;&#27979;&#22478;&#24066;&#36890;&#21220;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22478;&#24066;&#35268;&#21010;&#21644;&#36890;&#21220;&#27969;&#20043;&#38388;&#30340;&#32852;&#31995;&#23545;&#25351;&#23548;&#22478;&#24066;&#21457;&#23637;&#21644;&#25919;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#39033;&#30740;&#31350;&#36328;&#36234;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#22478;&#24066;&#30740;&#31350;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#36825;&#20123;&#39046;&#22495;&#21450;&#20854;&#19981;&#21516;&#20851;&#27880;&#28857;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#22478;&#24066;&#30740;&#31350;&#26041;&#27861;&#65292;&#22914;&#24341;&#21147;&#21644;&#36752;&#23556;&#27169;&#22411;&#65292;&#36890;&#24120;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#22810;&#20010;&#21464;&#37327;&#30340;&#22788;&#29702;&#26377;&#38480;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#36807;&#20110;&#31616;&#21270;&#19988;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#22914;&#31354;&#38388;&#21508;&#21521;&#21516;&#24615;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#20998;&#26512;&#36890;&#21220;&#27969;&#31561;&#22797;&#26434;&#31038;&#20250;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TransFlower&#65292;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#12289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;Flow-to-Flow&#27880;&#24847;&#21147;&#26469;&#39044;&#27979;&#22478;&#24066;&#36890;&#21220;&#27169;&#24335;&#12290;&#23427;&#20855;&#26377;&#19968;&#20010;&#24102;&#26377;&#21508;&#21521;&#24322;&#24615;&#24863;&#30693;&#30340;&#22320;&#29702;&#31354;&#38388;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15398v1 Announce Type: cross  Abstract: Understanding the link between urban planning and commuting flows is crucial for guiding urban development and policymaking. This research, bridging computer science and urban studies, addresses the challenge of integrating these fields with their distinct focuses. Traditional urban studies methods, like the gravity and radiation models, often underperform in complex scenarios due to their limited handling of multiple variables and reliance on overly simplistic and unrealistic assumptions, such as spatial isotropy. While deep learning models offer improved accuracy, their black-box nature poses a trade-off between performance and explainability -- both vital for analyzing complex societal phenomena like commuting flows. To address this, we introduce TransFlower, an explainable, transformer-based model employing flow-to-flow attention to predict urban commuting patterns. It features a geospatial encoder with an anisotropy-aware relative
&lt;/p&gt;</description></item><item><title>NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15393</link><description>&lt;p&gt;
NeuralThink: &#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#36827;&#34892;&#22806;&#25512;&#30340;&#31639;&#27861;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15393
&lt;/p&gt;
&lt;p&gt;
NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25797;&#38271;&#27169;&#24335;&#35782;&#21035;&#65292;&#20294;&#22312;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#26041;&#24335;&#19978;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#24605;&#32500;&#26041;&#27861;&#23637;&#29616;&#20102;&#23398;&#20064;&#21487;&#20197;&#22806;&#25512;&#30340;&#31639;&#27861;&#30340;&#28508;&#21147;&#65306;&#22312;&#36739;&#23567;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#24182;&#22312;&#36739;&#22823;&#30340;&#29615;&#22659;&#20013;&#25191;&#34892;&#23398;&#21040;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23616;&#38480;&#20110;&#23545;&#31216;&#20219;&#21153;&#65292;&#21363;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#30456;&#21516;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; NeuralThink&#65292;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#20854;&#20013;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#23545;&#31216;&#20219;&#21153;&#22806;&#25512;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NeuralThink &#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#19968;&#30452;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
&lt;/p&gt;</description></item><item><title>Genie&#26159;&#31532;&#19968;&#20010;&#32463;&#36807;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#20132;&#20114;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21160;&#20316;&#21487;&#25511;&#30340;&#34394;&#25311;&#19990;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#28508;&#22312;&#34892;&#21160;&#31354;&#38388;&#20197;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#27169;&#20223;&#26410;&#35265;&#35270;&#39057;&#34892;&#20026;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15391</link><description>&lt;p&gt;
Genie&#65306;&#29983;&#25104;&#20132;&#20114;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Genie: Generative Interactive Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15391
&lt;/p&gt;
&lt;p&gt;
Genie&#26159;&#31532;&#19968;&#20010;&#32463;&#36807;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#20132;&#20114;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21160;&#20316;&#21487;&#25511;&#30340;&#34394;&#25311;&#19990;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#28508;&#22312;&#34892;&#21160;&#31354;&#38388;&#20197;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#27169;&#20223;&#26410;&#35265;&#35270;&#39057;&#34892;&#20026;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Genie&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#26080;&#30417;&#30563;&#26041;&#24335;&#20174;&#26410;&#26631;&#35760;&#30340;&#20114;&#32852;&#32593;&#35270;&#39057;&#20013;&#35757;&#32451;&#32780;&#25104;&#30340;&#29983;&#25104;&#24335;&#20132;&#20114;&#29615;&#22659;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#34987;&#25552;&#31034;&#29983;&#25104;&#36890;&#36807;&#25991;&#26412;&#12289;&#21512;&#25104;&#22270;&#20687;&#12289;&#29031;&#29255;&#29978;&#33267;&#32032;&#25551;&#25551;&#36848;&#30340;&#26080;&#38480;&#31181;&#31867;&#30340;&#21160;&#20316;&#21487;&#25511;&#34394;&#25311;&#19990;&#30028;&#12290;&#25317;&#26377;110&#20159;&#20010;&#21442;&#25968;&#30340;Genie&#21487;&#20197;&#34987;&#35270;&#20026;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#12290;&#20854;&#30001;&#26102;&#31354;&#35270;&#39057;&#26631;&#35760;&#22120;&#12289;&#33258;&#22238;&#24402;&#21160;&#21147;&#23398;&#27169;&#22411;&#20197;&#21450;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#28508;&#22312;&#34892;&#21160;&#27169;&#22411;&#32452;&#25104;&#12290;&#23613;&#31649;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#22320;&#38754;&#30495;&#20540;&#34892;&#21160;&#26631;&#31614;&#25110;&#20854;&#20182;&#36890;&#24120;&#22312;&#19990;&#30028;&#27169;&#22411;&#25991;&#29486;&#20013;&#25214;&#21040;&#30340;&#39046;&#22495;&#29305;&#23450;&#35201;&#27714;&#65292;&#20294;Genie&#20351;&#29992;&#25143;&#21487;&#20197;&#22522;&#20110;&#36880;&#24103;&#22522;&#30784;&#22312;&#29983;&#25104;&#30340;&#29615;&#22659;&#20013;&#34892;&#21160;&#12290;&#36827;&#19968;&#27493;&#65292;&#25152;&#24471;&#21040;&#30340;&#23398;&#20064;&#28508;&#22312;&#34892;&#21160;&#31354;&#38388;&#26377;&#21161;&#20110;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#27169;&#20223;&#26469;&#33258;&#26410;&#35265;&#35270;&#39057;&#30340;&#34892;&#20026;&#65292;&#20026;&#26410;&#26469;&#22521;&#35757;&#36890;&#29992;&#20195;&#29702;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15391v1 Announce Type: cross  Abstract: We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.
&lt;/p&gt;</description></item><item><title>&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.15390</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#20462;&#22797;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explorations of Self-Repair in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15390
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#29421;&#31364;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#21457;&#29616;&#20102;&#33258;&#20462;&#22797;&#29616;&#35937;&#65292;&#21363;&#22914;&#26524;&#21093;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#20214;&#65292;&#21518;&#32493;&#32452;&#20214;&#20250;&#25913;&#21464;&#20854;&#34892;&#20026;&#20197;&#36827;&#34892;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#36825;&#20123;&#36807;&#21435;&#30340;&#25991;&#29486;&#65292;&#23637;&#31034;&#20102;&#24403;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#21093;&#31163;&#21333;&#20010;&#27880;&#24847;&#21147;&#22836;&#26102;&#65292;&#33258;&#20462;&#22797;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#65292;&#33258;&#20462;&#22797;&#26159;&#19981;&#23436;&#32654;&#30340;&#65292;&#22240;&#20026;&#22836;&#37096;&#30340;&#21407;&#22987;&#30452;&#25509;&#25928;&#26524;&#24182;&#26410;&#23436;&#20840;&#24674;&#22797;&#65292;&#24182;&#19988;&#26159;&#22024;&#26434;&#30340;&#65292;&#22240;&#20026;&#33258;&#20462;&#22797;&#31243;&#24230;&#22312;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#26174;&#33879;&#21464;&#21270;&#65288;&#26377;&#26102;&#36229;&#36807;&#21407;&#22987;&#25928;&#26524;&#65289;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20419;&#25104;&#33258;&#20462;&#22797;&#30340;&#20004;&#31181;&#19981;&#21516;&#26426;&#21046;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#65288;&#21487;&#20462;&#22797;&#30452;&#25509;&#25928;&#26524;&#30340;30%&#65289;&#20197;&#21450;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15390v1 Announce Type: cross  Abstract: Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;"&#20219;&#21153;"&#30340;&#26041;&#24335;&#21644;&#24341;&#20837;&#20855;&#26377;&#29289;&#29702;&#21644;&#22240;&#26524;&#20851;&#31995;&#29702;&#35299;&#30340;&#30417;&#30563;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#30693;&#35782;&#30340;&#31283;&#24577;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#22797;&#26434;&#35745;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.15384</link><description>&lt;p&gt;
&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#30693;&#35782;&#30340;&#31283;&#24577;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Homeostatic motion planning with innate physics knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15384
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;"&#20219;&#21153;"&#30340;&#26041;&#24335;&#21644;&#24341;&#20837;&#20855;&#26377;&#29289;&#29702;&#21644;&#22240;&#26524;&#20851;&#31995;&#29702;&#35299;&#30340;&#30417;&#30563;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#30693;&#35782;&#30340;&#31283;&#24577;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#22797;&#26434;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20307;&#20197;&#38381;&#29615;&#26041;&#24335;&#19982;&#21608;&#22260;&#29615;&#22659;&#36827;&#34892;&#20114;&#21160;&#65292;&#20854;&#20013;&#24863;&#23448;&#36755;&#20837;&#20915;&#23450;&#34892;&#20026;&#30340;&#21551;&#21160;&#21644;&#32456;&#27490;&#12290;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#21160;&#29289;&#20063;&#33021;&#21046;&#23450;&#24182;&#25191;&#34892;&#22797;&#26434;&#35745;&#21010;&#65292;&#20294;&#32431;&#38381;&#29615;&#36755;&#20837;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#23578;&#26410;&#22797;&#21046;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23450;&#20041;&#19968;&#32452;&#31163;&#25955;&#20020;&#26102;&#38381;&#29615;&#25511;&#21046;&#22120;&#65292;&#31216;&#20026;&#8220;&#20219;&#21153;&#8221;&#65292;&#27599;&#20010;&#20219;&#21153;&#20195;&#34920;&#19968;&#20010;&#38381;&#29615;&#34892;&#20026;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#21644;&#22240;&#26524;&#20851;&#31995;&#29702;&#35299;&#30340;&#30417;&#30563;&#27169;&#22359;&#65292;&#36890;&#36807;&#35813;&#27169;&#22359;&#21487;&#20197;&#27169;&#25311;&#38543;&#26102;&#38388;&#25191;&#34892;&#20219;&#21153;&#24207;&#21015;&#24182;&#23558;&#32467;&#26524;&#23384;&#20648;&#22312;&#29615;&#22659;&#27169;&#22411;&#20013;&#12290;&#22522;&#20110;&#36825;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#38142;&#25509;&#20020;&#26102;&#38381;&#29615;&#25511;&#21046;&#22120;&#36827;&#34892;&#21046;&#23450;&#35745;&#21010;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24050;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#23454;&#26045;&#65292;&#24182;&#22312;&#20004;&#31181;&#22330;&#26223;&#19979;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15384v1 Announce Type: cross  Abstract: Living organisms interact with their surroundings in a closed-loop fashion, where sensory inputs dictate the initiation and termination of behaviours. Even simple animals are able to develop and execute complex plans, which has not yet been replicated in robotics using pure closed-loop input control. We propose a solution to this problem by defining a set of discrete and temporary closed-loop controllers, called "tasks", each representing a closed-loop behaviour. We further introduce a supervisory module which has an innate understanding of physics and causality, through which it can simulate the execution of task sequences over time and store the results in a model of the environment. On the basis of this model, plans can be made by chaining temporary closed-loop controllers. The proposed framework was implemented for a real robot and tested in two scenarios as proof of concept.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#65288;D2E2S&#65289;&#65292;&#32467;&#21512;&#20102;BERT&#36890;&#36947;&#21644;&#22686;&#24378;&#22411;LSTM&#36890;&#36947;&#26469;&#26368;&#22823;&#21270;&#21333;&#35789;&#38388;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#24322;&#26500;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#29992;&#20110;&#25429;&#33719;&#22797;&#26434;&#20114;&#21160;&#21644;&#21160;&#24577;&#36873;&#25321;&#37325;&#35201;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.15370</link><description>&lt;p&gt;
&#21452;&#32534;&#30721;&#22120;&#65306;&#21033;&#29992;&#21477;&#27861;&#21644;&#35821;&#20041;&#28508;&#21147;&#36827;&#34892;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15370
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#65288;D2E2S&#65289;&#65292;&#32467;&#21512;&#20102;BERT&#36890;&#36947;&#21644;&#22686;&#24378;&#22411;LSTM&#36890;&#36947;&#26469;&#26368;&#22823;&#21270;&#21333;&#35789;&#38388;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#24322;&#26500;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#29992;&#20110;&#25429;&#33719;&#22797;&#26434;&#20114;&#21160;&#21644;&#21160;&#24577;&#36873;&#25321;&#37325;&#35201;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25552;&#21462;&#65288;ASTE&#65289;&#26159;&#31934;&#32454;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#26032;&#20852;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#24314;&#27169;&#19977;&#20803;&#32452;&#20803;&#32032;&#22266;&#26377;&#30340;&#21477;&#27861;-&#35821;&#20041;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#23578;&#26410;&#20805;&#20998;&#21457;&#25381;ASTE&#20219;&#21153;&#20013;&#21477;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;\emph{&#21452;&#32534;&#30721;&#22120;&#65306;&#21033;&#29992;&#21477;&#27861;&#21644;&#35821;&#20041;&#28508;&#21147;}&#27169;&#22411;&#65288;D2E2S&#65289;&#65292;&#26368;&#22823;&#21270;&#21333;&#35789;&#38388;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#21452;&#36890;&#36947;&#32534;&#30721;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;BERT&#36890;&#36947;&#26469;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#22686;&#24378;&#22411;LSTM&#36890;&#36947;&#29992;&#20110;&#20840;&#38754;&#25429;&#25417;&#21477;&#27861;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24322;&#26500;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#65292;&#20197;&#25429;&#33719;&#20381;&#36182;&#21477;&#27861;&#19982;&#27880;&#24847;&#21147;&#35821;&#20041;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#24182;&#21160;&#24577;&#36873;&#25321;&#37325;&#35201;&#33410;&#28857;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27169;&#22359;&#30340;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15370v1 Announce Type: cross  Abstract: Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to model the syntax-semantic relationships inherent in triplet elements. However, they have yet to fully tap into the vast potential of syntactic and semantic information within the ASTE task. In this work, we propose a \emph{Dual Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S), which maximizes the syntactic and semantic relationships among words. Specifically, our model utilizes a dual-channel encoder with a BERT channel to capture semantic information, and an enhanced LSTM channel for comprehensive syntactic information capture. Subsequently, we introduce the heterogeneous feature interaction module to capture intricate interactions between dependency syntax and attention semantics, and to dynamically select vital nodes. We leverage the synergy of these m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#24335;LLM&#21644;&#31526;&#21512;&#39044;&#27979;&#25216;&#26415;&#30340;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15368</link><description>&lt;p&gt;
&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#30340;&#25216;&#26415;&#23454;&#29616;&#35821;&#35328;&#25351;&#23548;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#23433;&#20840;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#24335;LLM&#21644;&#31526;&#21512;&#39044;&#27979;&#25216;&#26415;&#30340;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#35821;&#35328;&#25351;&#23548;&#26426;&#22120;&#20154;&#22242;&#38431;&#30340;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#12290;&#20219;&#21153;&#29992;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#34920;&#31034;&#65292;&#35201;&#27714;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#20301;&#32622;&#21644;&#35821;&#20041;&#23545;&#35937;&#19978;&#24212;&#29992;&#23427;&#20204;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#31227;&#21160;&#12289;&#25805;&#20316;&#21644;&#24863;&#30693;&#65289;&#12290;&#26368;&#36817;&#20960;&#31687;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35774;&#35745;&#26377;&#25928;&#30340;&#22810;&#26426;&#22120;&#20154;&#35745;&#21010;&#26469;&#35299;&#20915;&#31867;&#20284;&#30340;&#35268;&#21010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#20219;&#21153;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#24335;LLM&#30340;&#35268;&#21010;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#26469;&#23454;&#29616;&#30340;&#65292;CP&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#23545;&#20854;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25512;&#29702;&#12290;CP&#20801;&#35768;&#25152;&#25552;&#20986;&#30340;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#20197;&#20998;&#24067;&#26041;&#24335;&#25512;&#29702;&#20854;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#22312;&#20805;&#20998;&#20449;&#20219;&#26102;&#33021;&#22815;&#20570;&#20986;&#20010;&#21035;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15368v1 Announce Type: cross  Abstract: This paper addresses task planning problems for language-instructed robot teams. Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities (e.g., mobility, manipulation, and sensing) at various locations and semantic objects. Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans. However, these approaches lack mission performance and safety guarantees. To address this challenge, we introduce a new decentralized LLM-based planner that is capable of achieving high mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models. CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a decentralized fashion, enabling robots to make individual decisions when they are sufficiently ce
&lt;/p&gt;</description></item><item><title>Farsight&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#35774;&#35745;AI&#24212;&#29992;&#21407;&#22411;&#26102;&#35782;&#21035;&#28508;&#22312;&#21361;&#23475;&#65292;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Farsight&#21518;&#65292;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2402.15350</link><description>&lt;p&gt;
Farsight&#65306;&#22312;AI&#24212;&#29992;&#21407;&#22411;&#35774;&#35745;&#36807;&#31243;&#20013;&#22521;&#20859;&#36127;&#36131;&#20219;&#30340;AI&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
Farsight: Fostering Responsible AI Awareness During AI Application Prototyping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15350
&lt;/p&gt;
&lt;p&gt;
Farsight&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#35774;&#35745;AI&#24212;&#29992;&#21407;&#22411;&#26102;&#35782;&#21035;&#28508;&#22312;&#21361;&#23475;&#65292;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Farsight&#21518;&#65292;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#39537;&#21160;&#30028;&#38754;&#20351;&#24471;&#21407;&#22411;&#35774;&#35745;&#21644;&#26500;&#24314;AI&#24212;&#29992;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#21487;&#33021;&#22312;AI&#24212;&#29992;&#20013;&#20986;&#29616;&#30340;&#28508;&#22312;&#21361;&#23475;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#21407;&#22411;&#35774;&#35745;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;Farsight&#65292;&#24110;&#21161;&#20154;&#20204;&#35782;&#21035;&#20182;&#20204;&#27491;&#22312;&#35774;&#35745;&#21407;&#22411;&#30340;AI&#24212;&#29992;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;&#26681;&#25454;&#29992;&#25143;&#30340;&#25552;&#31034;&#65292;Farsight&#31361;&#20986;&#26174;&#31034;&#20102;&#19982;&#30456;&#20851;AI&#20107;&#20214;&#26377;&#20851;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25506;&#32034;&#21644;&#32534;&#36753;LLM&#29983;&#25104;&#30340;&#29992;&#20363;&#12289;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#21361;&#23475;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19982;10&#20301;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#36827;&#34892;&#30340;&#20849;&#21516;&#35774;&#35745;&#30740;&#31350;&#30340;&#35774;&#35745;&#35265;&#35299;&#65292;&#20197;&#21450;&#19982;42&#20301;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#12290;&#22312;&#20351;&#29992;Farsight&#21518;&#65292;&#25105;&#20204;&#29992;&#25143;&#30740;&#31350;&#20013;&#30340;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#24037;&#20855;&#27604;&#29616;&#26377;&#36164;&#28304;&#26356;&#26377;&#29992;&#19988;&#26356;&#26131;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15350v1 Announce Type: cross  Abstract: Prompt-based interfaces for Large Language Models (LLMs) have made prototyping and building AI-powered applications easier than ever before. However, identifying potential harms that may arise from AI applications remains a challenge, particularly during prompt-based prototyping. To address this, we present Farsight, a novel in situ interactive tool that helps people identify potential harms from the AI applications they are prototyping. Based on a user's prompt, Farsight highlights news articles about relevant AI incidents and allows users to explore and edit LLM-generated use cases, stakeholders, and harms. We report design insights from a co-design study with 10 AI prototypers and findings from a user study with 42 AI prototypers. After using Farsight, AI prototypers in our user study are better able to independently identify potential harms associated with a prompt and find our tool more useful and usable than existing resources. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.15347</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Safe Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15347
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#35780;&#20272;&#36829;&#21453;&#20808;&#39564;&#26410;&#30693;&#65288;&#23433;&#20840;&#65289;&#32422;&#26463;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#26410;&#30693;&#20989;&#25968;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#22312;&#26410;&#30693;&#20989;&#25968;&#19978;&#25918;&#32622;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#24182;&#19988;&#20165;&#20801;&#35768;&#22312;&#39640;&#27010;&#29575;&#23433;&#20840;&#21306;&#22495;&#20869;&#36827;&#34892;&#35780;&#20272;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#22495;&#30340;&#31163;&#25955;&#21270;&#65292;&#24182;&#19988;&#19981;&#33021;&#30452;&#25509;&#25193;&#23637;&#21040;&#36830;&#32493;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21033;&#29992;&#32422;&#26463;&#30340;&#35268;&#21017;&#20551;&#35774;&#30340;&#26041;&#24335;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#30452;&#25509;&#21033;&#29992;GP&#21518;&#39564;&#26469;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#30340;&#23433;&#20840;&#21442;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#23558;&#36825;&#19968;&#25506;&#32034;&#20934;&#21017;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#32467;&#21512;&#36215;&#26469;&#65292;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15347v1 Announce Type: cross  Abstract: We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability. Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate. The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion. Our approach 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;LLM&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;NuNER&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#32039;&#20945;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#32988;&#36807;&#30456;&#20284;&#22823;&#23567;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;LLMs&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.15343</link><description>&lt;p&gt;
NuNER: &#21033;&#29992;LLM&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15343
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;LLM&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;NuNER&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#32039;&#20945;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#32988;&#36807;&#30456;&#20284;&#22823;&#23567;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;LLMs&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#25968;&#25454;&#26631;&#27880;&#26041;&#38754;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20026;&#35299;&#20915;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;NuNER&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#30340;&#32039;&#20945;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#12290;NuNER&#21487;&#20197;&#34987;&#24494;&#35843;&#20197;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#19979;&#28216;&#30340;NER&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#32988;&#36807;&#30456;&#20284;&#22823;&#23567;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;LLMs&#31454;&#20105;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#23454;&#20307;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#26159;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#23558;NuNER&#35270;&#20026;&#26368;&#36817;&#34987;LLMs&#35299;&#38145;&#30340;&#26356;&#24191;&#27867;&#30340;&#29305;&#23450;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#30340;&#19968;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15343v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#28145;&#24230;&#23398;&#20064;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#32463;&#20856;-&#37327;&#23376;&#26550;&#26500;co-TenQu&#65292;&#36890;&#36807;&#32463;&#20856;&#32452;&#20214;&#30340;&#21387;&#32553;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#39640;&#32500;&#25968;&#25454;&#32534;&#30721;&#21040;&#36923;&#36753;&#37327;&#23376;&#30005;&#36335;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.15333</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#24577;&#20445;&#30495;&#24230;&#30340;&#37327;&#23376;-&#32463;&#20856;&#21327;&#21516;&#35757;&#32451;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Quantum-Classical Collaborative Training Architecture Based on Quantum State Fidelity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15333
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#28145;&#24230;&#23398;&#20064;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#32463;&#20856;-&#37327;&#23376;&#26550;&#26500;co-TenQu&#65292;&#36890;&#36807;&#32463;&#20856;&#32452;&#20214;&#30340;&#21387;&#32553;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#39640;&#32500;&#25968;&#25454;&#32534;&#30721;&#21040;&#36923;&#36753;&#37327;&#23376;&#30005;&#36335;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#36827;&#23637;&#31361;&#26174;&#20102;&#24403;&#21069;&#37327;&#23376;&#31995;&#32479;&#30340;&#23616;&#38480;&#65292;&#29305;&#21035;&#26159;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#21487;&#29992;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#21463;&#21040;&#38480;&#21046;&#12290;&#36825;&#19968;&#32422;&#26463;&#22823;&#22823;&#38480;&#21046;&#20102;&#33021;&#22815;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#21487;&#29992;&#37327;&#23376;&#27604;&#29305;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24102;&#26469;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#25928;&#21033;&#29992;&#37327;&#23376;&#27604;&#29305;&#65292;&#20197;&#32531;&#35299;&#24403;&#21069;&#30340;&#38480;&#21046;&#21644;&#26410;&#26469;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#37327;&#23376;&#24212;&#29992;&#23581;&#35797;&#22312;&#28151;&#21512;&#26694;&#26550;&#20013;&#38598;&#25104;&#32463;&#20856;&#21644;&#37327;&#23376;&#31995;&#32479;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#37327;&#23376;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;co-TenQu&#30340;&#21327;&#21516;&#32463;&#20856;-&#37327;&#23376;&#26550;&#26500;&#12290;&#32463;&#20856;&#32452;&#20214;&#37319;&#29992;&#24352;&#37327;&#32593;&#32476;&#36827;&#34892;&#21387;&#32553;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#20351;&#39640;&#32500;&#25968;&#25454;&#33021;&#22815;&#34987;&#32534;&#30721;&#21040;&#20855;&#26377;&#38480;&#37327;&#37327;&#23376;&#27604;&#29305;&#30340;&#36923;&#36753;&#37327;&#23376;&#30005;&#36335;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15333v1 Announce Type: cross  Abstract: Recent advancements have highlighted the limitations of current quantum systems, particularly the restricted number of qubits available on near-term quantum devices. This constraint greatly inhibits the range of applications that can leverage quantum computers. Moreover, as the available qubits increase, the computational complexity grows exponentially, posing additional challenges. Consequently, there is an urgent need to use qubits efficiently and mitigate both present limitations and future complexities. To address this, existing quantum applications attempt to integrate classical and quantum systems in a hybrid framework. In this study, we concentrate on quantum deep learning and introduce a collaborative classical-quantum architecture called co-TenQu. The classical component employs a tensor network for compression and feature extraction, enabling higher-dimensional data to be encoded onto logical quantum circuits with limited qub
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;&#65292;&#24212;&#29992;&#33539;&#30068;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#26725;&#26753;&#65292;&#26377;&#25928;&#22320;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#19981;&#21516;&#39118;&#26684;&#65292;&#21516;&#26102;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.15332</link><description>&lt;p&gt;
&#20998;&#31867;&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#31181;&#20851;&#20110;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Categorical Deep Learning: An Algebraic Theory of Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;&#65292;&#24212;&#29992;&#33539;&#30068;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#26725;&#26753;&#65292;&#26377;&#25928;&#22320;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#19981;&#21516;&#39118;&#26684;&#65292;&#21516;&#26102;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#25351;&#23450;&#21644;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#36890;&#29992;&#26694;&#26550;&#30340;&#31435;&#22330;&#12290;&#25105;&#20204;&#35748;&#20026;&#21040;&#30446;&#21069;&#20026;&#27490;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#20851;&#38190;&#23581;&#35797;&#32570;&#20047;&#19968;&#31181;&#19968;&#33268;&#30340;&#26725;&#26753;&#65292;&#33021;&#22815;&#25351;&#23450;&#27169;&#22411;&#24517;&#39035;&#28385;&#36275;&#30340;&#32422;&#26463;&#24182;&#35268;&#23450;&#23427;&#20204;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;&#19987;&#27880;&#20110;&#26500;&#24314;&#36825;&#26679;&#19968;&#20010;&#26725;&#26753;&#65292;&#25105;&#20204;&#24314;&#35758;&#24212;&#29992;&#33539;&#30068;&#35770;&#8212;&#8212;&#20934;&#30830;&#22320;&#35828;&#65292;&#21333;&#23376;&#20540;&#20110;&#21442;&#25968;&#26144;&#23556;&#30340;&#20108;&#33539;&#30068;&#30340;&#36890;&#29992;&#20195;&#25968;&#8212;&#8212;&#20316;&#20026;&#19968;&#31181;&#21333;&#19968;&#29702;&#35770;&#65292;&#20248;&#38597;&#22320;&#21253;&#21547;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#36825;&#20004;&#31181;&#39118;&#26684;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#29702;&#35770;&#22914;&#20309;&#24674;&#22797;&#30001;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23548;&#33268;&#30340;&#32422;&#26463;&#65292;&#20197;&#21450;&#20174;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#31181;&#26550;&#26500;&#65288;&#22914;RNNs&#65289;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#19968;&#29702;&#35770;&#22914;&#20309;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15332v1 Announce Type: cross  Abstract: We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. Focusing on building a such a bridge, we propose to apply category theory -- precisely, the universal algebra of monads valued in a 2-category of parametric maps -- as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs. We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;OpenSUN3D&#30740;&#35752;&#20250;&#19978;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#21253;&#25324;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;</title><link>https://arxiv.org/abs/2402.15321</link><description>&lt;p&gt;
OpenSUN3D: &#24320;&#25918;&#35789;&#27719;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#31532;&#19968;&#27425;&#30740;&#35752;&#20250;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15321
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;OpenSUN3D&#30740;&#35752;&#20250;&#19978;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#21253;&#25324;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#27010;&#36848;&#20102;&#22312;2023&#24180;ICCV&#20250;&#35758;&#19978;&#20030;&#21150;&#30340;OpenSUN3D Workshop&#20851;&#20110;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#35752;&#20250;&#31995;&#21015;&#30340;&#30446;&#26631;&#26159;&#20026;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#25552;&#20379;&#25506;&#32034;&#21644;&#35752;&#35770;&#24179;&#21488;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#20998;&#21106;&#12289;&#26816;&#27979;&#21644;&#26144;&#23556;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30740;&#35752;&#20250;&#19978;&#20030;&#21150;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#23637;&#31034;&#20102;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#20197;&#21450;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;&#12290;&#26356;&#22810;&#35814;&#24773;&#35831;&#21442;&#38405;https://opensun3d.github.io/index_iccv23.html&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15321v1 Announce Type: cross  Abstract: This report provides an overview of the challenge hosted at the OpenSUN3D Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with ICCV 2023. The goal of this workshop series is to provide a platform for exploration and discussion of open-vocabulary 3D scene understanding tasks, including but not limited to segmentation, detection and mapping. We provide an overview of the challenge hosted at the workshop, present the challenge dataset, the evaluation methodology, and brief descriptions of the winning methods. For additional details, please see https://opensun3d.github.io/index_iccv23.html.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.15313</link><description>&lt;p&gt;
ArabianGPT&#65306;&#22522;&#20110;&#21407;&#29983;&#38463;&#25289;&#20271;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ArabianGPT: Native Arabic GPT-based Large Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#21644;&#25289;&#19969;&#35821;&#20026;&#20027;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20027;&#23548;&#22320;&#20301;&#23548;&#33268;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#26174;&#33879;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#32780;&#25104;&#12290;&#36825;&#20123;&#27169;&#22411;&#21253;&#25324;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#65292;&#19982;&#38463;&#25289;&#20271;&#35821;&#30340;&#24494;&#22937;&#35821;&#35328;&#29305;&#24449;&#30456;&#22865;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#22312;&#32447;&#25163;&#20889;&#35782;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23383;&#22696;&#27700;(tokenized representation)&#34920;&#31034;&#26041;&#27861;&#65292;&#23558;&#25163;&#20889;&#21576;&#29616;&#20026;&#25991;&#26412;&#24207;&#21015;&#21644;&#22270;&#20687;&#65292;&#21462;&#24471;&#20102;&#21487;&#19982;&#29616;&#26377;&#26041;&#27861;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15307</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#31034;&#22312;&#32447;&#25163;&#20889;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Representing Online Handwriting for Recognition in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#22312;&#32447;&#25163;&#20889;&#35782;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23383;&#22696;&#27700;(tokenized representation)&#34920;&#31034;&#26041;&#27861;&#65292;&#23558;&#25163;&#20889;&#21576;&#29616;&#20026;&#25991;&#26412;&#24207;&#21015;&#21644;&#22270;&#20687;&#65292;&#21462;&#24471;&#20102;&#21487;&#19982;&#29616;&#26377;&#26041;&#27861;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24102;&#26377;&#35302;&#25720;&#23631;&#21644;&#35302;&#25511;&#31508;&#30340;&#24179;&#26495;&#30005;&#33041;&#30340;&#26222;&#21450;&#65292;&#23558;&#25163;&#20889;&#36716;&#25442;&#20026;&#25991;&#26412;&#30340;&#20851;&#38190;&#21151;&#33021;&#24471;&#20197;&#23454;&#29616;&#65292;&#23454;&#29616;&#20102;&#25628;&#32034;&#12289;&#32034;&#24341;&#21644;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#29616;&#22312;&#24050;&#25104;&#20026;&#22270;&#20687;&#29702;&#35299;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#20197;&#21450;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#32479;&#19968;&#26041;&#27861;&#30340;&#31616;&#27905;&#24615;&#12290;&#34429;&#28982;VLMs&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26420;&#32032;&#24212;&#29992;&#26102;(&#21363;&#23558;&#25163;&#20889;&#21576;&#29616;&#20026;&#22270;&#20687;&#24182;&#25191;&#34892;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;(OCR))&#22312;&#25163;&#20889;&#35782;&#21035;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;VLMs&#20013;&#30340;&#22312;&#32447;&#25163;&#20889;&#35782;&#21035;&#65292;&#36229;&#36234;&#20102;&#26420;&#32032;&#30340;OCR&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25968;&#23383;&#22696;&#27700;(&#22312;&#32447;&#25163;&#20889;)&#30340;&#26032;&#22411;&#26631;&#35760;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#20316;&#20026;&#25991;&#26412;&#30340;&#19968;&#31995;&#21015;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#21015;&#30340;&#31508;&#30011;&#65292;&#20197;&#21450;&#20316;&#20026;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#34920;&#31034;&#24471;&#21040;&#30340;&#32467;&#26524;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15307v1 Announce Type: cross  Abstract: The adoption of tablets with touchscreens and styluses is increasing, and a key feature is converting handwriting to text, enabling search, indexing, and AI assistance. Meanwhile, vision-language models (VLMs) are now the go-to solution for image understanding, thanks to both their state-of-the-art performance across a variety of tasks and the simplicity of a unified approach to training, fine-tuning, and inference. While VLMs obtain high performance on image-based tasks, they perform poorly on handwriting recognition when applied naively, i.e., by rendering handwriting as an image and performing optical character recognition (OCR). In this paper, we study online handwriting recognition with VLMs, going beyond naive OCR. We propose a novel tokenized representation of digital ink (online handwriting) that includes both a time-ordered sequence of strokes as text, and as image. We show that this representation yields results comparable to
&lt;/p&gt;</description></item><item><title>CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.15300</link><description>&lt;p&gt;
&#35265;&#35777;&#20026;&#20449;&#65306;&#36890;&#36807;CLIP&#24341;&#23548;&#35299;&#30721;&#32531;&#35299;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15300
&lt;/p&gt;
&lt;p&gt;
CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#23481;&#26131;&#20986;&#29616;&#23545;&#35937;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#30340;&#25991;&#26412;&#21253;&#21547;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#21477;&#23376;&#32423;LVLM&#24187;&#35273;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#19982;&#22270;&#20687;&#30340;CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#19968;&#20010;&#27604;&#21333;&#35789;&#21487;&#33021;&#24615;&#26356;&#24378;&#22823;&#12289;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#31034;&#22120;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#35299;&#30721;&#26102;&#30340;&#23545;&#35937;&#24187;&#35273;&#12290;CGD&#21033;&#29992;CLIP&#26469;&#24341;&#23548;&#27169;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36890;&#36807;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#19982;&#22270;&#20687;&#30340;&#35270;&#35273;&#32852;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CGD&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15300v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#38899;&#20048;&#29983;&#25104;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;&#38899;&#20048;&#21019;&#20316;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#20027;&#35201;&#28966;&#28857;&#20197;&#21450;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15294</link><description>&lt;p&gt;
&#22312;&#20114;&#21160;&#32972;&#26223;&#19979;&#38899;&#20048;&#29983;&#25104;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Music Generation in the Context of Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15294
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#38899;&#20048;&#29983;&#25104;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;&#38899;&#20048;&#21019;&#20316;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#20027;&#35201;&#28966;&#28857;&#20197;&#21450;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;transformers&#65289;&#65292;&#24050;&#25104;&#21151;&#29992;&#20110;&#21019;&#20316;&#21644;&#29983;&#25104;&#38899;&#20048;&#65292;&#21253;&#25324;&#26059;&#24459;&#21644;&#22797;&#35843;&#20316;&#21697;&#12290;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#39118;&#26684;&#22797;&#21046;&#65288;&#20363;&#22914;&#29983;&#25104;&#24052;&#36203;&#39118;&#26684;&#36171;&#26684;&#26354;&#65289;&#25110;&#39118;&#26684;&#36716;&#31227;&#65288;&#20363;&#22914;&#21476;&#20856;&#21040;&#29237;&#22763;&#65289;&#19978;&#65292;&#22522;&#20110;&#22823;&#37327;&#24405;&#21046;&#25110;&#36716;&#24405;&#30340;&#38899;&#20048;&#65292;&#36825;&#20063;&#20801;&#35768;&#30456;&#24403;&#30452;&#25509;&#30340;&#8220;&#34920;&#29616;&#8221;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#27169;&#22411;&#19981;&#36866;&#21512;&#36890;&#36807;&#23454;&#26102;&#20114;&#21160;&#36827;&#34892;&#20154;&#26426;&#20849;&#21019;&#65292;&#20063;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#21644;&#29983;&#25104;&#30340;&#20316;&#21697;&#23558;&#22914;&#20309;&#35780;&#20272;&#12290;&#26412;&#25991;&#20840;&#38754;&#23457;&#26597;&#20102;&#38899;&#20048;&#34920;&#31034;&#12289;&#29305;&#24449;&#20998;&#26512;&#12289;&#21551;&#21457;&#24335;&#31639;&#27861;&#12289;&#32479;&#35745;&#21644;&#21442;&#25968;&#24314;&#27169;&#65292;&#20197;&#21450;&#20154;&#20026;&#21644;&#33258;&#21160;&#35780;&#20272;&#25514;&#26045;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#21738;&#20123;&#26041;&#27861;&#21644;&#27169;&#22411;&#20284;&#20046;&#26368;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15294v1 Announce Type: cross  Abstract: In recent years, machine learning, and in particular generative adversarial neural networks (GANs) and attention-based neural networks (transformers), have been successfully used to compose and generate music, both melodies and polyphonic pieces. Current research focuses foremost on style replication (eg. generating a Bach-style chorale) or style transfer (eg. classical to jazz) based on large amounts of recorded or transcribed music, which in turn also allows for fairly straight-forward "performance" evaluation. However, most of these models are not suitable for human-machine co-creation through live interaction, neither is clear, how such models and resulting creations would be evaluated. This article presents a thorough review of music representation, feature analysis, heuristic algorithms, statistical and parametric modelling, and human and automatic evaluation measures, along with a discussion of which approaches and models seem m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15290</link><description>&lt;p&gt;
&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Linear Dynamics-embedded Neural Network for Long-Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#25104;&#20026;&#29942;&#39048;&#65292;&#21463;&#21040;&#25511;&#21046;&#29702;&#35770;&#20013;&#20855;&#26377;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290; SSM&#30340;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#21367;&#31215;&#23646;&#24615;&#20351;LDNN&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#30340;&#25512;&#26029;&#21644;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290; &#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26377;&#25928;&#31574;&#30053;&#65292;&#23545;&#35282;&#21270;&#21644;&#8220;&#35299;&#32806;&#28982;&#21518;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#8221;&#65292;&#20197;&#23558;&#21367;&#31215;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$O(LNH\max\{L, N\})$&#38477;&#20302;&#21040;$O(LN\max\{H, \log L\})$&#12290; &#25105;&#20204;&#36890;&#36807;&#21452;&#21521;&#38750;&#22240;&#26524;&#21644;&#22810;&#22836;&#35774;&#32622;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;LDNN&#65292;&#20197;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290; &#23545;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#65288;LRA&#65289;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;LDNN&#30340;&#26377;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#26102;&#31354;&#35266;&#23519;&#32773;&#8221;&#30340;&#35266;&#23519;&#32773;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20026;&#39640;&#32500;&#25968;&#25454;&#30340;&#39044;&#27979;&#23398;&#20064;&#25552;&#20379;&#20102;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#24577;&#27491;&#21017;&#21270;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.15284</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#39044;&#27979;&#23398;&#20064;&#30340;&#26102;&#31354;&#35266;&#23519;&#32773;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal Observer Design for Predictive Learning of High-Dimensional Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#26102;&#31354;&#35266;&#23519;&#32773;&#8221;&#30340;&#35266;&#23519;&#32773;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20026;&#39640;&#32500;&#25968;&#25454;&#30340;&#39044;&#27979;&#23398;&#20064;&#25552;&#20379;&#20102;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#24577;&#27491;&#21017;&#21270;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#26694;&#26550;&#20027;&#35201;&#26159;&#22522;&#20110;&#30452;&#35273;&#35774;&#35745;&#30340;&#12290;&#22914;&#20309;&#36827;&#34892;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#26102;&#31354;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#21160;&#24577;&#31995;&#32479;&#30340;&#39046;&#22495;&#30693;&#35782;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26694;&#26550;&#35774;&#35745;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#26102;&#31354;&#35266;&#23519;&#32773;&#8221;&#30340;&#35266;&#23519;&#32773;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#39044;&#27979;&#23398;&#20064;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#29305;&#28857;&#26159;&#21452;&#37325;&#30340;&#65306;&#39318;&#20808;&#65292;&#23427;&#20026;&#26102;&#31354;&#39044;&#27979;&#25552;&#20379;&#20102;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#21644;&#25910;&#25947;&#20445;&#35777;&#65307;&#20854;&#27425;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21160;&#24577;&#27491;&#21017;&#21270;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#26102;&#31354;&#21160;&#24577;&#24182;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15284v1 Announce Type: cross  Abstract: Although deep learning-based methods have shown great success in spatiotemporal predictive learning, the framework of those models is designed mainly by intuition. How to make spatiotemporal forecasting with theoretical guarantees is still a challenging issue. In this work, we tackle this problem by applying domain knowledge from the dynamical system to the framework design of deep learning models. An observer theory-guided deep learning architecture, called Spatiotemporal Observer, is designed for predictive learning of high dimensional data. The characteristics of the proposed framework are twofold: firstly, it provides the generalization error bound and convergence guarantee for spatiotemporal prediction; secondly, dynamical regularization is introduced to enable the model to learn system dynamics better during training. Further experimental results show that this framework could capture the spatiotemporal dynamics and make accurate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20915;&#31574;&#26102;&#24212;&#29992;&#36845;&#20195;&#25512;&#29702;&#26469;&#24494;&#35843;&#25512;&#26029;&#30340;&#20195;&#29702;&#29366;&#24577;&#65292;&#33021;&#22815;&#22312;&#35270;&#35273;3D&#23548;&#33322;&#20219;&#21153;&#20013;&#21462;&#24471;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#24471;&#21040;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.15283</link><description>&lt;p&gt;
&#29369;&#35947;&#26102;&#65292;&#35201;&#24930;&#24930;&#24605;&#32771;&#65306;&#20855;&#26377;&#28508;&#22312;&#24819;&#35937;&#21147;&#30340;&#36845;&#20195;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15283
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20915;&#31574;&#26102;&#24212;&#29992;&#36845;&#20195;&#25512;&#29702;&#26469;&#24494;&#35843;&#25512;&#26029;&#30340;&#20195;&#29702;&#29366;&#24577;&#65292;&#33021;&#22815;&#22312;&#35270;&#35273;3D&#23548;&#33322;&#20219;&#21153;&#20013;&#21462;&#24471;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#24471;&#21040;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#38476;&#29983;&#30340;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#20250;&#21463;&#21040;&#20854;&#19990;&#30028;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#36825;&#31867;&#20195;&#29702;&#24615;&#33021;&#30340;&#26032;&#39062;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#19982;&#35268;&#21010;&#21644;&#23398;&#20064;&#20998;&#24320;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20915;&#31574;&#26102;&#24212;&#29992;&#36845;&#20195;&#25512;&#29702;&#26469;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22522;&#20110;&#26410;&#26469;&#29366;&#24577;&#34920;&#31034;&#30340;&#36830;&#36143;&#24615;&#26469;&#25913;&#36827;&#25512;&#26029;&#30340;&#20195;&#29702;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#35270;&#35273;3D&#23548;&#33322;&#20219;&#21153;&#26102;&#65292;&#22312;&#37325;&#26500;&#31934;&#24230;&#21644;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#32771;&#34385;&#26356;&#22810;&#30340;&#26410;&#26469;&#29366;&#24577;&#20250;&#36827;&#19968;&#27493;&#25552;&#39640;&#20195;&#29702;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#19981;&#20250;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#35757;&#32451;&#39044;&#35780;&#20272;&#36739;&#23569;&#30340;&#20195;&#29702;&#20174;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#33719;&#30410;&#26368;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15283v1 Announce Type: cross  Abstract: In an unfamiliar setting, a model-based reinforcement learning agent can be limited by the accuracy of its world model. In this work, we present a novel, training-free approach to improving the performance of such agents separately from planning and learning. We do so by applying iterative inference at decision-time, to fine-tune the inferred agent states based on the coherence of future state representations. Our approach achieves a consistent improvement in both reconstruction accuracy and task performance when applied to visual 3D navigation tasks. We go on to show that considering more future states further improves the performance of the agent in partially-observable environments, but not in a fully-observable one. Finally, we demonstrate that agents with less training pre-evaluation benefit most from our approach.
&lt;/p&gt;</description></item><item><title>Text2Pic Swift&#26694;&#26550;&#38024;&#23545;&#22823;&#35268;&#27169;&#24211;&#20013;&#25991;&#26412;&#25551;&#36848;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#31574;&#30053;&#35299;&#20915;&#20102;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15276</link><description>&lt;p&gt;
Text2Pic Swift&#65306;&#22686;&#24378;&#22823;&#35268;&#27169;&#24211;&#20013;&#38271;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale Libraries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15276
&lt;/p&gt;
&lt;p&gt;
Text2Pic Swift&#26694;&#26550;&#38024;&#23545;&#22823;&#35268;&#27169;&#24211;&#20013;&#25991;&#26412;&#25551;&#36848;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#31574;&#30053;&#35299;&#20915;&#20102;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15276v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#26816;&#32034;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#25968;&#23383;&#22270;&#20070;&#39302;&#12289;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#21644;&#22810;&#23186;&#20307;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#26597;&#35810;&#26469;&#25628;&#32034;&#22270;&#20687;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#27169;&#31946;&#30340;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#26174;&#30528;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#29983;&#25104;&#21487;&#27880;&#20837;&#30340;&#23884;&#20837;&#25152;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Text2Pic Swift&#26694;&#26550;&#65292;&#19987;&#20026;&#22312;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#26377;&#25928;&#21644;&#31283;&#20581;&#22320;&#26816;&#32034;&#19982;&#24191;&#27867;&#25991;&#26412;&#25551;&#36848;&#23545;&#24212;&#30340;&#22270;&#20687;&#32780;&#35774;&#35745;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#21021;&#22987;&#22522;&#20110;&#23454;&#20307;&#30340;&#25490;&#24207;&#65288;ER&#65289;&#38454;&#27573;&#36890;&#36807;&#22810;&#26597;&#35810;&#23545;&#22810;&#30446;&#26631;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#22266;&#26377;&#30340;&#27495;&#20041;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#21487;&#33021;&#30340;&#20505;&#36873;&#39033;&#65292;&#20197;&#20415;&#36827;&#34892;&#21518;&#32493;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15276v1 Announce Type: cross  Abstract: Text-to-image retrieval plays a crucial role across various applications, including digital libraries, e-commerce platforms, and multimedia databases, by enabling the search for images using text queries. Despite the advancements in Multimodal Large Language Models (MLLMs), which offer leading-edge performance, their applicability in large-scale, varied, and ambiguous retrieval scenarios is constrained by significant computational demands and the generation of injective embeddings. This paper introduces the Text2Pic Swift framework, tailored for efficient and robust retrieval of images corresponding to extensive textual descriptions in sizable datasets. The framework employs a two-tier approach: the initial Entity-based Ranking (ER) stage addresses the ambiguity inherent in lengthy text queries through a multiple-queries-to-multiple-targets strategy, effectively narrowing down potential candidates for subsequent analysis. Following thi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#26816;&#27979;&#26694;&#26550;EMIFF&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#30456;&#26426;&#24863;&#30693;&#36890;&#36947;&#23631;&#34109;&#27169;&#22359;&#26469;&#22686;&#24378;&#22522;&#30784;&#35774;&#26045;&#21644;&#36710;&#36742;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#36710;&#36335;&#21327;&#21516;&#19977;&#32500;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#23039;&#24577;&#35823;&#24046;&#21644;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15272</link><description>&lt;p&gt;
EMIFF&#65306;&#22686;&#24378;&#22411;&#22810;&#23610;&#24230;&#22270;&#20687;&#29305;&#24449;&#34701;&#21512;&#29992;&#20110;&#36710;&#36335;&#21327;&#21516;&#19977;&#32500;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15272
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#26816;&#27979;&#26694;&#26550;EMIFF&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#30456;&#26426;&#24863;&#30693;&#36890;&#36947;&#23631;&#34109;&#27169;&#22359;&#26469;&#22686;&#24378;&#22522;&#30784;&#35774;&#26045;&#21644;&#36710;&#36742;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#36710;&#36335;&#21327;&#21516;&#19977;&#32500;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#23039;&#24577;&#35823;&#24046;&#21644;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#21512;&#20316;&#24863;&#30693;&#21033;&#29992;&#26469;&#33258;&#36710;&#36742;&#21644;&#22522;&#30784;&#35774;&#26045;&#30340;&#22810;&#35270;&#22270;&#25668;&#20687;&#22836;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#65292;&#36229;&#36234;&#21333;&#20010;&#36710;&#36742;&#35270;&#35282;&#30340;&#36947;&#36335;&#26465;&#20214;&#30340;&#20840;&#23616;&#35270;&#35282;&#12290;&#26412;&#25991;&#38024;&#23545;&#36710;&#36335;&#21327;&#21516;&#19977;&#32500;&#65288;VIC3D&#65289;&#30446;&#26631;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#26816;&#27979;&#26694;&#26550;&#65292;&#22686;&#24378;&#22411;&#22810;&#23610;&#24230;&#22270;&#20687;&#29305;&#24449;&#34701;&#21512;&#65288;EMIFF&#65289;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#26469;&#33258;&#36710;&#36742;&#21644;&#22522;&#30784;&#35774;&#26045;&#30340;&#25972;&#20307;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#21644;&#30456;&#26426;&#24863;&#30693;&#36890;&#36947;&#23631;&#34109;&#65288;CCM&#65289;&#27169;&#22359;&#65292;&#20197;&#22686;&#24378;&#22522;&#30784;&#35774;&#26045;&#21644;&#36710;&#36742;&#29305;&#24449;&#22312;&#23610;&#24230;&#12289;&#31354;&#38388;&#21644;&#36890;&#36947;&#32423;&#21035;&#19978;&#36827;&#34892;&#26657;&#27491;&#65292;&#20197;&#35299;&#20915;&#22810;&#35270;&#22270;&#22270;&#20687;&#34701;&#21512;&#26102;&#22266;&#26377;&#30340;&#23039;&#24577;&#35823;&#24046;&#21644;&#20256;&#36755;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15272v1 Announce Type: cross  Abstract: In autonomous driving, cooperative perception makes use of multi-view cameras from both vehicles and infrastructure, providing a global vantage point with rich semantic context of road conditions beyond a single vehicle viewpoint. Currently, two major challenges persist in vehicle-infrastructure cooperative 3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view images, caused by time asynchrony across cameras; $2)$ information loss in transmission process resulted from limited communication bandwidth. To address these issues, we propose a novel camera-based 3D detection framework for VIC3D task, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit holistic perspectives from both vehicles and infrastructure, we propose Multi-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM) modules to enhance infrastructure and vehicle features at scale, spatial, and channel levels to correct the po
&lt;/p&gt;</description></item><item><title>SGCL&#27169;&#22411;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#20174;&#32780;&#24418;&#25104;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;</title><link>https://arxiv.org/abs/2402.15270</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#32541;&#25509;&#36817;&#24230;&#25972;&#21512;&#30340;&#24179;&#28369;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Smoothed Graph Contrastive Learning via Seamless Proximity Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15270
&lt;/p&gt;
&lt;p&gt;
SGCL&#27169;&#22411;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#20174;&#32780;&#24418;&#25104;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#36890;&#36807;&#23558;&#33410;&#28857;&#23545;&#24402;&#31867;&#20026;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#26469;&#23545;&#40784;&#33410;&#28857;&#34920;&#31034;&#65292;&#20854;&#36873;&#25321;&#36807;&#31243;&#36890;&#24120;&#20381;&#36182;&#20110;&#22312;&#20004;&#20010;&#22686;&#24378;&#22270;&#20013;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#12290;&#20256;&#32479;&#30340;GCL&#26041;&#27861;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#32479;&#19968;&#22320;&#34701;&#20837;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#36127;&#33410;&#28857;&#34987;&#24179;&#31561;&#23545;&#24453;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#19982;&#30495;&#27491;&#27491;&#26679;&#26412;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;SGCL&#65289;&#65292;&#21033;&#29992;&#22686;&#24378;&#22270;&#30340;&#20960;&#20309;&#32467;&#26500;&#26469;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#27880;&#20837;&#19982;&#27491;&#36127;&#26679;&#26412;&#30456;&#20851;&#30340;&#25509;&#36817;&#24230;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#35268;&#33539;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;SGCL&#36890;&#36807;&#25972;&#21512;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#24418;&#25104;&#20102;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15270v1 Announce Type: cross  Abstract: Graph contrastive learning (GCL) aligns node representations by classifying node pairs into positives and negatives using a selection process that typically relies on establishing correspondences within two augmented graphs. The conventional GCL approaches incorporate negative samples uniformly in the contrastive loss, resulting in the equal treatment negative nodes, regardless of their proximity to the true positive. In this paper, we present a Smoothed Graph Contrastive Learning model (SGCL), which leverages the geometric structure of augmented graphs to inject proximity information associated with positive/negative pairs in the contrastive loss, thus significantly regularizing the learning process. The proposed SGCL adjusts the penalties associated with node pairs in the contrastive loss by incorporating three distinct smoothing techniques that result in proximity aware positives and negatives. To enhance scalability for large-scale
&lt;/p&gt;</description></item><item><title>MemoryPrompt&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#24490;&#29615;&#32593;&#32476;&#65292;&#23558;&#20449;&#24687;&#20256;&#36882;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#36319;&#36394;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.15268</link><description>&lt;p&gt;
MemoryPrompt: &#19968;&#31181;&#25913;&#36827;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#36319;&#36394;&#30340;&#36731;&#37327;&#23553;&#35013;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15268
&lt;/p&gt;
&lt;p&gt;
MemoryPrompt&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#24490;&#29615;&#32593;&#32476;&#65292;&#23558;&#20449;&#24687;&#20256;&#36882;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#36319;&#36394;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22823;&#22411;&#30828;&#32534;&#30721;&#36755;&#20837;&#31383;&#21475;&#36319;&#36394;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;MemoryPrompt&#65292;&#19968;&#31181;&#26356;&#31934;&#31616;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#30001;&#19968;&#20010;&#23567;&#30340;&#36741;&#21161;&#24490;&#29615;&#32593;&#32476;&#34917;&#20805;&#65292;&#36890;&#36807;&#22312;&#20854;&#24120;&#35268;&#36755;&#20837;&#20043;&#21069;&#28155;&#21152;&#19968;&#31995;&#21015;&#21521;&#37327;&#65288;&#31867;&#20284;&#20110;&#36719;&#25552;&#31034;&#65289;&#23558;&#20449;&#24687;&#20256;&#36882;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#38656;&#35201;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#23545;&#19968;&#20010;&#26088;&#22312;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#36319;&#36394;&#22810;&#20010;&#20107;&#23454;&#26356;&#26032;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;MemoryPrompt&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#37027;&#20123;&#21487;&#20197;&#35775;&#38382;&#23436;&#25972;&#36755;&#20837;&#21382;&#21490;&#35760;&#24405;&#30340;&#26356;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#38271;&#36317;&#31163;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;MemoryPrompt&#65292;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#65292;&#20854;&#24615;&#33021;&#19982;&#22312;&#25972;&#20010;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#19978;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;&#22312;&#36825;&#20004;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;MemoryPrompt&#22312;&#36866;&#24212;&#26032;&#20219;&#21153;&#26102;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22240;&#27492;&#19981;&#20250;&#30772;&#22351;&#38750;&#19987;&#23478;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15268v1 Announce Type: cross  Abstract: Transformer-based language models (LMs) track contextual information through large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft prompts, without requiring LM finetuning. Tested on a task designed to probe a LM's ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history. We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history. In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the un
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36873;&#25321;&#30456;&#20851;&#30340;&#23383;&#33410;&#23376;&#38598;&#26367;&#20195;&#39640;&#26031;&#22122;&#22768;&#65292;&#22312;&#35757;&#32451;&#20013;&#36827;&#34892;&#22522;&#20110;&#28040;&#34701;&#30340;&#24179;&#28369;&#26041;&#26696;&#65292;&#21152;&#24378;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15267</link><description>&lt;p&gt;
&#36890;&#36807;&#65288;&#21435;&#65289;&#38543;&#26426;&#24179;&#28369;&#25552;&#39640;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness of Deep Learning-based Malware Detectors via (De)Randomized Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15267
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#30456;&#20851;&#30340;&#23383;&#33410;&#23376;&#38598;&#26367;&#20195;&#39640;&#26031;&#22122;&#22768;&#65292;&#22312;&#35757;&#32451;&#20013;&#36827;&#34892;&#22522;&#20110;&#28040;&#34701;&#30340;&#24179;&#28369;&#26041;&#26696;&#65292;&#21152;&#24378;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#30340;&#25915;&#20987;&#65292;&#21363;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#32463;&#36807;&#25925;&#24847;&#25805;&#32437;&#20197;&#36991;&#20813;&#26816;&#27979;&#12290;&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#23545;&#24494;&#22937;&#36755;&#20837;&#25991;&#20214;&#20462;&#25913;&#30340;&#33030;&#24369;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#65288;&#21435;&#65289;&#38543;&#26426;&#24179;&#28369;&#21551;&#21457;&#30340;&#38024;&#23545;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#30340;&#23454;&#29992;&#38450;&#24481;&#26041;&#27861;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36873;&#25321;&#30456;&#20851;&#30340;&#23383;&#33410;&#23376;&#38598;&#32780;&#19981;&#26159;&#20687;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#37027;&#26679;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#26469;&#38543;&#26426;&#21270;&#36755;&#20837;&#65292;&#26469;&#38477;&#20302;&#34987;&#24694;&#24847;&#36719;&#20214;&#20316;&#32773;&#27880;&#20837;&#30340;&#23545;&#25239;&#20869;&#23481;&#34987;&#37319;&#26679;&#30340;&#20960;&#29575;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#30340;&#21435;&#38500;&#22522;&#20110;&#28040;&#34701;&#30340;&#24179;&#28369;&#26041;&#26696;&#35757;&#32451;&#19968;&#20010;&#22522;&#26412;&#20998;&#31867;&#22120;&#23545;&#19968;&#37096;&#20998;&#36830;&#32493;&#23383;&#33410;&#25110;&#23383;&#33410;&#22359;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#22522;&#26412;&#20998;&#31867;&#22120;&#23545;&#22823;&#37327;&#23383;&#33410;&#22359;&#36827;&#34892;&#20998;&#31867;&#65292;&#26368;&#21518;&#39044;&#27979;&#32467;&#26524;&#26159;&#36825;&#20123;&#20998;&#31867;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15267v1 Announce Type: cross  Abstract: Deep learning-based malware detectors have been shown to be susceptible to adversarial malware examples, i.e. malware examples that have been deliberately manipulated in order to avoid detection. In light of the vulnerability of deep learning detectors to subtle input file modifications, we propose a practical defense against adversarial malware examples inspired by (de)randomized smoothing. In this work, we reduce the chances of sampling adversarial content injected by malware authors by selecting correlated subsets of bytes, rather than using Gaussian noise to randomize inputs like in the Computer Vision (CV) domain. During training, our ablation-based smoothing scheme trains a base classifier to make classifications on a subset of contiguous bytes or chunk of bytes. At test time, a large number of chunks are then classified by a base classifier and the consensus among these classifications is then reported as the final prediction. W
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#22238;&#39038;&#24335;&#23398;&#20064;&#27861;&#24459;&#20462;&#27491;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20869;&#23384;&#21333;&#20803;&#30340;&#21160;&#24577;&#21464;&#21270;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#32447;&#24615;&#26356;&#26032;&#35268;&#21017;&#21644;&#23567;&#20869;&#23384;&#30340;&#20248;&#21270;&#22120;&#20013;&#21462;&#24471;&#20248;&#20110;&#32463;&#20856;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15262</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#20869;&#23384;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dynamic Memory Based Adaptive Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#22238;&#39038;&#24335;&#23398;&#20064;&#27861;&#24459;&#20462;&#27491;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20869;&#23384;&#21333;&#20803;&#30340;&#21160;&#24577;&#21464;&#21270;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#32447;&#24615;&#26356;&#26032;&#35268;&#21017;&#21644;&#23567;&#20869;&#23384;&#30340;&#20248;&#21270;&#22120;&#20013;&#21462;&#24471;&#20248;&#20110;&#32463;&#20856;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20248;&#21270;&#22120;&#23450;&#20041;&#20026;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#23384;&#20648;$k$&#20010;&#21160;&#24577;&#21464;&#21270;&#21521;&#37327;&#30340;&#20855;&#26377;&#20869;&#23384;$k$&#30340;&#20248;&#21270;&#22120;&#12290;&#32463;&#20856;&#30340;SGD&#20248;&#21270;&#22120;&#20855;&#26377;&#20869;&#23384;$0$&#65292;&#21160;&#37327;SGD&#20248;&#21270;&#22120;&#20855;&#26377;$1$&#65292;Adam&#20248;&#21270;&#22120;&#20855;&#26377;$2$&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#20248;&#21270;&#22120;&#22914;&#20309;&#21033;&#29992;&#26356;&#22810;&#20869;&#23384;&#21333;&#20803;&#65311;&#24212;&#35813;&#22312;&#20854;&#20013;&#23384;&#20648;&#21738;&#20123;&#20449;&#24687;&#65311;&#22914;&#20309;&#23558;&#23427;&#20204;&#29992;&#20110;&#23398;&#20064;&#27493;&#39588;&#65311;&#20316;&#20026;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#22238;&#39038;&#24335;&#23398;&#20064;&#27861;&#24459;&#20462;&#27491;&#8221;&#25110;&#31616;&#31216;RLLC&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35745;&#31639;&#20869;&#23384;&#21333;&#20803;&#30340;&#21160;&#24577;&#21464;&#21270;&#32447;&#24615;&#32452;&#21512;&#65288;&#31216;&#20026;&#23398;&#20064;&#27861;&#21017;&#65289;&#65292;&#36825;&#20123;&#20869;&#23384;&#21333;&#20803;&#26412;&#36523;&#21487;&#33021;&#20250;&#20219;&#24847;&#28436;&#21464;&#12290;&#25105;&#20204;&#22312;&#20869;&#23384;&#21333;&#20803;&#20855;&#26377;&#32447;&#24615;&#26356;&#26032;&#35268;&#21017;&#21644;&#23567;&#20869;&#23384;&#65288;$\leq 4$&#20869;&#23384;&#21333;&#20803;&#65289;&#30340;&#20248;&#21270;&#22120;&#19978;&#23637;&#31034;&#20102;RLLC&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#26631;&#20934;&#38382;&#39064;&#20013;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#34920;&#29616;&#20248;&#20110;&#19978;&#36848;&#19977;&#31181;&#32463;&#20856;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;RLLC&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15262v1 Announce Type: cross  Abstract: Define an optimizer as having memory $k$ if it stores $k$ dynamically changing vectors in the parameter space. Classical SGD has memory $0$, momentum SGD optimizer has $1$ and Adam optimizer has $2$. We address the following questions: How can optimizers make use of more memory units? What information should be stored in them? How to use them for the learning steps? As an approach to the last question, we introduce a general method called "Retrospective Learning Law Correction" or shortly RLLC. This method is designed to calculate a dynamically varying linear combination (called learning law) of memory units, which themselves may evolve arbitrarily. We demonstrate RLLC on optimizers whose memory units have linear update rules and small memory ($\leq 4$ memory units). Our experiments show that in a variety of standard problems, these optimizers outperform the above mentioned three classical optimizers. We conclude that RLLC is a promisi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#24471;&#20998;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#23398;&#20064;&#35270;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#26469;&#25214;&#21040;&#23548;&#33268;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#22240;&#26524;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.15255</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#32467;&#26500;&#23398;&#20064;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Structure Learning Under Missing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15255
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#24471;&#20998;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#23398;&#20064;&#35270;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#26469;&#25214;&#21040;&#23548;&#33268;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#20250;&#24341;&#20837;&#40481;&#29983;&#34507;&#38382;&#39064;&#12290;&#34429;&#28982;&#30446;&#26631;&#26159;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#20294;&#40065;&#26834;&#30340;&#25554;&#34917;&#38656;&#35201;&#32771;&#34385;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#25110;&#26356;&#22909;&#22320;&#22240;&#26524;&#20851;&#31995;&#12290;&#20165;&#20165;&#29992;&#29616;&#26377;&#30340;&#25554;&#34917;&#26041;&#27861;&#22635;&#20805;&#32570;&#22833;&#20540;&#65292;&#28982;&#21518;&#22312;&#23436;&#25972;&#25968;&#25454;&#19978;&#24212;&#29992;&#32467;&#26500;&#23398;&#20064;&#34987;&#35777;&#26126;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#12290;&#36825;&#31181;&#26368;&#20248;&#36755;&#36816;&#30340;&#35266;&#28857;&#19981;&#21516;&#20110;&#29616;&#26377;&#22522;&#20110;EM&#30340;&#22522;&#20110;&#24471;&#20998;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#32467;&#26500;&#23398;&#20064;&#25237;&#24433;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#24341;&#36215;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#21644;&#35266;&#27979;&#25968;&#25454;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15255v1 Announce Type: cross  Abstract: Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma. While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or preferably causal relations among variables. Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirical shown to be sub-optimal. To this end, we propose in this paper a score-based algorithm, based on optimal transport, for learning causal structure from missing data. This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on EM. We project structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the distribution over the observed data. Through extensive simulations and real-data experiments, our framework 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35848;&#21028;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#29305;&#24449;&#20132;&#26131;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#30340;&#20132;&#26131;</title><link>https://arxiv.org/abs/2402.15247</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#35848;&#21028;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#29305;&#24449;&#20132;&#26131;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Bargaining-based Approach for Feature Trading in Vertical Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35848;&#21028;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#29305;&#24449;&#20132;&#26131;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#30340;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#36328;&#25968;&#25454;&#21644;&#20219;&#21153;&#26041;&#23545;&#30456;&#21516;&#29992;&#25143;&#38598;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#65292;VFL&#36890;&#24120;&#28041;&#21450;&#19968;&#20010;&#20219;&#21153;&#26041;&#21644;&#19968;&#20010;&#25968;&#25454;&#26041;&#12290;&#20844;&#24179;&#21644;&#32463;&#27982;&#39640;&#25928;&#30340;&#29305;&#24449;&#20132;&#26131;&#23545;VFL&#30340;&#21830;&#19994;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#20219;&#21153;&#26041;&#34987;&#35270;&#20026;&#36141;&#20080;&#25968;&#25454;&#26041;&#29305;&#24449;&#30340;&#25968;&#25454;&#28040;&#36153;&#32773;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VFL&#29305;&#24449;&#20132;&#26131;&#20570;&#27861;&#36890;&#24120;&#23558;&#25968;&#25454;&#26041;&#30340;&#25968;&#25454;&#25972;&#20307;&#23450;&#20215;&#65292;&#24182;&#20551;&#23450;&#20132;&#26131;&#21457;&#29983;&#22312;&#25191;&#34892;VFL&#20043;&#21069;&#12290;&#24573;&#30053;&#20132;&#26131;&#29305;&#24449;&#20135;&#29983;&#30340;&#24615;&#33021;&#22686;&#30410;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#25903;&#20184;&#21644;&#36807;&#24230;&#25903;&#20184;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35848;&#21028;&#30340;VFL&#29305;&#24449;&#20132;&#26131;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#30340;&#20132;&#26131;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#22522;&#20110;&#24615;&#33021;&#22686;&#30410;&#30340;&#23450;&#20215;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15247v1 Announce Type: cross  Abstract: Vertical Federated Learning (VFL) has emerged as a popular machine learning paradigm, enabling model training across the data and the task parties with different features about the same user set while preserving data privacy. In production environment, VFL usually involves one task party and one data party. Fair and economically efficient feature trading is crucial to the commercialization of VFL, where the task party is considered as the data consumer who buys the data party's features. However, current VFL feature trading practices often price the data party's data as a whole and assume transactions occur prior to the performing VFL. Neglecting the performance gains resulting from traded features may lead to underpayment and overpayment issues. In this study, we propose a bargaining-based feature trading approach in VFL to encourage economically efficient transactions. Our model incorporates performance gain-based pricing, taking int
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;Chimera&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.15246</link><description>&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;&#20154;&#24037;&#34562;&#32676;&#20248;&#21270;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Artificial Bee Colony optimization of Deep Convolutional Neural Networks in the context of Biomedical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15246
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Chimera&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#33258;&#28982;&#22270;&#20687;&#25110;&#33402;&#26415;&#21697;&#19978;&#65292;&#36825;&#20123;&#22270;&#20687;&#19982;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#25152;&#28041;&#21450;&#30340;&#25968;&#25454;&#22312;&#22823;&#23567;&#21644;&#20869;&#23481;&#19978;&#26377;&#30528;&#26174;&#33879;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#32463;&#36807;&#25163;&#21160;&#24494;&#35843;&#65292;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#36890;&#24120;&#20063;&#34987;&#35777;&#26126;&#26159;&#20122;&#20248;&#21270;&#30340;&#12290;&#30001;&#20110;&#36229;&#21442;&#25968;&#31354;&#38388;&#30340;&#24191;&#38420;&#24615;&#12289;&#26102;&#38388;&#19981;&#36275;&#12289;&#35745;&#31639;&#36164;&#28304;&#19981;&#36275;&#20197;&#21450;&#22823;&#22810;&#25968;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#23454;&#39564;&#23460;&#32570;&#20047;&#28145;&#24230;&#23398;&#20064;&#19987;&#23478;&#65292;&#22240;&#27492;&#20174;&#22836;&#24320;&#22987;&#24320;&#21457;&#26550;&#26500;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25163;&#21160;&#23450;&#20041;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#26159;&#20351;&#29992;&#31070;&#32463;&#36827;&#21270;&#65292;&#23427;&#37319;&#29992;&#20803;&#21551;&#21457;&#25216;&#26415;&#26469;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#36827;&#21270;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#35768;&#22810;&#31639;&#27861;&#35201;&#20040;&#19981;&#22815;&#21487;&#38752;&#65292;&#35201;&#20040;&#21463;&#38480;&#20110;&#36229;&#21442;&#25968;&#31354;&#38388;&#30340;&#23567;&#33539;&#22260;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Chimera&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#28151;&#21512;&#30340;&#31070;&#32463;&#36827;&#21270;&#31639;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15246v1 Announce Type: cross  Abstract: Most efforts in Computer Vision focus on natural images or artwork, which differ significantly both in size and contents from the kind of data biomedical image processing deals with. Thus, Transfer Learning models often prove themselves suboptimal for these tasks, even after manual finetuning. The development of architectures from scratch is oftentimes unfeasible due to the vastness of the hyperparameter space and a shortage of time, computational resources and Deep Learning experts in most biomedical research laboratories. An alternative to manually defining the models is the use of Neuroevolution, which employs metaheuristic techniques to optimize Deep Learning architectures. However, many algorithms proposed in the neuroevolutive literature are either too unreliable or limited to a small, predefined region of the hyperparameter space. To overcome these shortcomings, we propose the Chimera Algorithm, a novel, hybrid neuroevolutive al
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22266;&#23450;&#38543;&#26426;&#20998;&#31867;&#22120;&#37325;&#25490;&#65288;FRCR&#65289;&#30340;&#20004;&#38454;&#27573;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#21487;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#20026;&#22266;&#23450;&#30340;&#38543;&#26426;&#20998;&#31867;&#22120;&#65292;&#22312;&#19981;&#24433;&#21709;&#32593;&#32476;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#32422;&#26463;&#20102;&#31561;&#20215;&#30340;&#21333;&#20998;&#31867;&#22120;&#30340;&#33539;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.15227</link><description>&lt;p&gt;
&#22266;&#23450;&#38543;&#26426;&#20998;&#31867;&#22120;&#37325;&#25490;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fixed Random Classifier Rearrangement for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15227
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22266;&#23450;&#38543;&#26426;&#20998;&#31867;&#22120;&#37325;&#25490;&#65288;FRCR&#65289;&#30340;&#20004;&#38454;&#27573;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#21487;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#20026;&#22266;&#23450;&#30340;&#38543;&#26426;&#20998;&#31867;&#22120;&#65292;&#22312;&#19981;&#24433;&#21709;&#32593;&#32476;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#32422;&#26463;&#20102;&#31561;&#20215;&#30340;&#21333;&#20998;&#31867;&#22120;&#30340;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#21518;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#24536;&#35760;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#22312;&#35270;&#35273;&#20998;&#31867;&#22330;&#26223;&#20013;&#65292;&#32531;&#35299;&#36951;&#24536;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#38480;&#21046;&#20027;&#24178;&#32593;&#32476;&#65292;&#28982;&#32780;&#20998;&#31867;&#22120;&#30340;&#24433;&#21709;&#34987;&#20302;&#20272;&#20102;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;&#39034;&#24207;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#24182;&#21457;&#29616;&#31561;&#20215;&#21333;&#20998;&#31867;&#22120;&#30340;&#33539;&#25968;&#26174;&#33879;&#24433;&#21709;&#36951;&#24536;&#27700;&#24179;&#12290;&#22522;&#20110;&#36825;&#19968;&#32467;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22266;&#23450;&#38543;&#26426;&#20998;&#31867;&#22120;&#37325;&#25490;&#65288;Fixed Random Classifier Rearrangement&#65292;FRCR&#65289;&#30340;&#20004;&#38454;&#27573;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;FRCR&#29992;&#22266;&#23450;&#30340;&#38543;&#26426;&#20998;&#31867;&#22120;&#26367;&#25442;&#21487;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#65292;&#32422;&#26463;&#20102;&#31561;&#20215;&#30340;&#21333;&#20998;&#31867;&#22120;&#30340;&#33539;&#25968;&#65292;&#32780;&#19981;&#24433;&#21709;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15227v1 Announce Type: cross  Abstract: With the explosive growth of data, continual learning capability is increasingly important for neural networks. Due to catastrophic forgetting, neural networks inevitably forget the knowledge of old tasks after learning new ones. In visual classification scenario, a common practice of alleviating the forgetting is to constrain the backbone. However, the impact of classifiers is underestimated. In this paper, we analyze the variation of model predictions in sequential binary classification tasks and find that the norm of the equivalent one-class classifiers significantly affects the forgetting level. Based on this conclusion, we propose a two-stage continual learning algorithm named Fixed Random Classifier Rearrangement (FRCR). In first stage, FRCR replaces the learnable classifiers with fixed random classifiers, constraining the norm of the equivalent one-class classifiers without affecting the performance of the network. In second sta
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36741;&#21161;&#25252;&#22763;&#20070;&#20889;&#26085;&#35760;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;ICU&#24739;&#32773;&#30340;&#38271;&#26399;&#24247;&#22797;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.15205</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#36741;&#21161;&#25252;&#22763;&#20070;&#20889;&#26085;&#35760;&#65292;&#22686;&#24378;ICU&#24739;&#32773;&#24247;&#22797;
&lt;/p&gt;
&lt;p&gt;
Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15205
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36741;&#21161;&#25252;&#22763;&#20070;&#20889;&#26085;&#35760;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;ICU&#24739;&#32773;&#30340;&#38271;&#26399;&#24247;&#22797;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#24739;&#32773;&#22312;&#38271;&#26399;&#24247;&#22797;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#20986;&#29616;&#26032;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#21307;&#25252;&#20154;&#21592;&#35760;&#24405;&#24739;&#32773;&#30041;&#38498;&#24773;&#20917;&#30340;&#26085;&#35760;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#20294;&#38754;&#20020;&#30528;&#35832;&#22810;&#37319;&#32435;&#38556;&#30861;&#65292;&#22914;&#32570;&#20047;&#26102;&#38388;&#21644;&#19981;&#30693;&#36947;&#35813;&#20889;&#20123;&#20160;&#20040;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20973;&#20511;&#20854;&#29983;&#25104;&#20154;&#31867;&#21270;&#25991;&#26412;&#21644;&#21487;&#36866;&#24212;&#24615;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#24895;&#26223;&#38656;&#35201;&#35299;&#20915;&#20960;&#20010;&#31038;&#20250;&#25216;&#26415;&#21644;&#23454;&#38469;&#30740;&#31350;&#25361;&#25112;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#21033;&#29992;LLMs&#22312;ICU&#26085;&#35760;&#25776;&#20889;&#20013;&#30340;&#28508;&#21147;&#65292;&#26368;&#32456;&#25913;&#21892;ICU&#24739;&#32773;&#30340;&#38271;&#26399;&#24247;&#22797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15205v1 Announce Type: cross  Abstract: Intensive care unit (ICU) patients often develop new health-related problems in their long-term recovery. Health care professionals keeping a diary of a patient's stay is a proven strategy to tackle this but faces several adoption barriers, such as lack of time and difficulty in knowing what to write. Large language models (LLMs), with their ability to generate human-like text and adaptability, could solve these challenges. However, realizing this vision involves addressing several socio-technical and practical research challenges. This paper discusses these challenges and proposes future research directions to utilize the potential of LLMs in ICU diary writing, ultimately improving the long-term recovery outcomes for ICU patients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35780;&#35770;&#23478;&#22609;&#36896;&#29615;&#22659;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#24471;&#31574;&#30053;&#21487;&#20197;&#21516;&#26102;&#26397;&#30528;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20248;&#21270;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#26080;&#38656;&#32422;&#26463;&#31574;&#30053;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#21644;&#26368;&#20248;&#24615;&#20043;&#38388;&#30340;&#33258;&#28982;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.15197</link><description>&lt;p&gt;
&#23433;&#20840;&#20248;&#21270;&#30340;&#22810;&#30446;&#26631;&#31574;&#30053;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35780;&#35770;&#23478;&#22609;&#36896;&#29615;&#22659;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#24471;&#31574;&#30053;&#21487;&#20197;&#21516;&#26102;&#26397;&#30528;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20248;&#21270;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#26080;&#38656;&#32422;&#26463;&#31574;&#30053;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#21644;&#26368;&#20248;&#24615;&#20043;&#38388;&#30340;&#33258;&#28982;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;&#23433;&#20840;RL&#65289;&#25351;&#30340;&#26159;&#19968;&#31867;&#25216;&#26415;&#65292;&#26088;&#22312;&#38450;&#27490;RL&#31639;&#27861;&#22312;&#35797;&#38169;&#20915;&#31574;&#21644;&#25506;&#32034;&#36807;&#31243;&#20013;&#36829;&#21453;&#32422;&#26463;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#21046;&#23450;&#30340;&#26032;&#22411;&#26080;&#27169;&#22411;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#20854;&#20013;&#31574;&#30053;&#21516;&#26102;&#26397;&#30528;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#23433;&#20840;&#35780;&#35770;&#23478;&#26469;&#22609;&#36896;&#29615;&#22659;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#20248;&#24615;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#23433;&#20840;&#20248;&#21270;RL&#65288;SORL&#65289;&#31639;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#30465;&#30053;&#20102;&#23545;&#31574;&#30053;&#25628;&#32034;&#31354;&#38388;&#30340;&#32422;&#26463;&#38656;&#35201;&#12290;&#36825;&#20351;&#24471;SORL&#33021;&#22815;&#22312;&#19981;&#21463;&#20005;&#26684;&#25628;&#32034;&#31354;&#38388;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#23433;&#20840;&#24615;&#21644;&#26368;&#20248;&#24615;&#20043;&#38388;&#30340;&#33258;&#28982;&#26435;&#34913;&#65292;&#32780;&#26080;&#38656;&#22240;&#20005;&#26684;&#25628;&#32034;&#31354;&#38388;&#32422;&#26463;&#32780;&#22312;&#23433;&#20840;&#24615;&#25110;&#26368;&#20248;&#24615;&#26041;&#38754;&#24615;&#33021;&#21463;&#25439;&#12290;&#36890;&#36807;&#23545;SORL&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;co
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15197v1 Announce Type: cross  Abstract: Safe reinforcement learning (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error. In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously. The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic. The advantage of the Safety Optimized RL (SORL) algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space. This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints. Through our theoretical analysis of SORL, we propose a co
&lt;/p&gt;</description></item><item><title>AffectToolbox&#26159;&#19968;&#20010;&#26032;&#22411;&#36719;&#20214;&#31995;&#32479;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#24773;&#24863;&#25935;&#24863;&#30740;&#31350;&#21644;&#21407;&#22411;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#25903;&#25345;&#65292;&#26080;&#38656;&#32534;&#31243;&#30693;&#35782;&#21363;&#21487;&#21487;&#38752;&#20998;&#26512;&#29992;&#25143;&#24773;&#24863;&#29366;&#24577;&#65292;&#23454;&#29616;&#22810;&#27169;&#24773;&#24863;&#35782;&#21035;&#21644;&#34701;&#21512;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.15195</link><description>&lt;p&gt;
AffectToolbox&#65306;&#27599;&#20010;&#20154;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The AffectToolbox: Affect Analysis for Everyone
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15195
&lt;/p&gt;
&lt;p&gt;
AffectToolbox&#26159;&#19968;&#20010;&#26032;&#22411;&#36719;&#20214;&#31995;&#32479;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#24773;&#24863;&#25935;&#24863;&#30740;&#31350;&#21644;&#21407;&#22411;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#25903;&#25345;&#65292;&#26080;&#38656;&#32534;&#31243;&#30693;&#35782;&#21363;&#21487;&#21487;&#38752;&#20998;&#26512;&#29992;&#25143;&#24773;&#24863;&#29366;&#24577;&#65292;&#23454;&#29616;&#22810;&#27169;&#24773;&#24863;&#35782;&#21035;&#21644;&#34701;&#21512;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#24863;&#35745;&#31639;&#39046;&#22495;&#65292;&#30740;&#31350;&#19981;&#26029;&#20197;&#24555;&#36895;&#30340;&#36895;&#24230;&#21069;&#36827;&#65292;&#29992;&#25143;&#23545;&#29992;&#25143;&#21451;&#22909;&#24037;&#20855;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#32780;&#26131;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AffectToolbox&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25903;&#25345;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#24773;&#24863;&#25935;&#24863;&#30740;&#31350;&#21644;&#21407;&#22411;&#30340;&#26032;&#22411;&#36719;&#20214;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26694;&#26550;&#25152;&#25552;&#20986;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26694;&#26550;&#36890;&#24120;&#38656;&#35201;&#28145;&#20837;&#30340;&#32534;&#31243;&#30693;&#35782;&#65292;&#24182;&#20027;&#35201;&#38754;&#21521;&#39640;&#32423;&#29992;&#25143;&#25110;&#29087;&#32451;&#24320;&#21457;&#20154;&#21592;&#12290;&#20026;&#20102;&#20419;&#36827;&#26131;&#29992;&#24615;&#65292;AffectToolbox&#19981;&#38656;&#35201;&#32534;&#31243;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26131;&#20110;&#35775;&#38382;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#25552;&#20379;&#20854;&#21151;&#33021;&#65292;&#21487;&#21487;&#38752;&#22320;&#20998;&#26512;&#29992;&#25143;&#30340;&#24773;&#24863;&#29366;&#24577;&#12290;&#20854;&#26550;&#26500;&#28085;&#30422;&#20102;&#22810;&#31181;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22810;&#31181;&#24773;&#24863;&#28192;&#36947;&#21644;&#24773;&#24863;&#26041;&#24335;&#19978;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#65292;&#20197;&#21450;&#19968;&#20010;&#22797;&#26434;&#30340;&#34701;&#21512;&#31995;&#32479;&#65292;&#23558;&#22810;&#27169;&#24577;&#35780;&#20272;&#21512;&#24182;&#20026;&#32479;&#19968;&#32467;&#26524;&#12290;&#25972;&#20010;&#31995;&#32479;&#26159;&#20844;&#24320;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15195v1 Announce Type: cross  Abstract: In the field of affective computing, where research continually advances at a rapid pace, the demand for user-friendly tools has become increasingly apparent. In this paper, we present the AffectToolbox, a novel software system that aims to support researchers in developing affect-sensitive studies and prototypes. The proposed system addresses the challenges posed by existing frameworks, which often require profound programming knowledge and cater primarily to power-users or skilled developers. Aiming to facilitate ease of use, the AffectToolbox requires no programming knowledge and offers its functionality to reliably analyze the affective state of users through an accessible graphical user interface. The architecture encompasses a variety of models for emotion recognition on multiple affective channels and modalities, as well as an elaborate fusion system to merge multi-modal assessments into a unified result. The entire system is op
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#26469;&#20197;&#30446;&#26631;&#23548;&#21521;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#21487;&#33021;&#20250;&#38754;&#20020;&#22870;&#21169;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15194</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#20316;&#20026;&#29109;&#27491;&#21017;&#21270;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15194
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#26469;&#20197;&#30446;&#26631;&#23548;&#21521;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#21487;&#33021;&#20250;&#38754;&#20020;&#22870;&#21169;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#33258;&#28982;&#22270;&#20687;&#21644;&#34507;&#30333;&#36136;&#30340;&#20998;&#24067;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21487;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#65292;&#20294;&#25105;&#20204;&#36890;&#24120;&#26356;&#20851;&#27880;&#20854;&#20182;&#23646;&#24615;&#65292;&#20363;&#22914;&#29983;&#25104;&#22270;&#20687;&#30340;&#32654;&#23398;&#36136;&#37327;&#25110;&#29983;&#25104;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#23646;&#24615;&#12290;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#26576;&#20123;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#65288;&#20363;&#22914;&#22270;&#20687;&#30340;&#32654;&#23398;&#36136;&#37327;&#65289;&#20197;&#30446;&#26631;&#23548;&#21521;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#26679;&#26412;&#22810;&#26679;&#24615;&#20943;&#23569;&#65292;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#20986;&#29616;&#26174;&#33879;&#20559;&#24046;&#65292;&#29978;&#33267;&#30001;&#20110;&#21033;&#29992;&#19981;&#23436;&#32654;&#30340;&#22870;&#21169;&#20989;&#25968;&#32780;&#23548;&#33268;&#26679;&#26412;&#36136;&#37327;&#36739;&#24046;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#22870;&#21169;&#20989;&#25968;&#26159;&#29992;&#20110;&#36817;&#20284;&#30495;&#23454;&#8220;&#30495;&#23454;&#8221;&#22870;&#21169;&#30340;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#32463;&#24120;&#20250;&#20135;&#29983;&#12290;&#36825;&#20123;&#25361;&#25112;&#24635;&#31216;&#20026;&#8220;&#22870;&#21169;&#23849;&#28291;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15194v1 Announce Type: cross  Abstract: Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth "genuine" reward, as is the case in many practical applications. These challenges, collectively termed "reward collapse," pose
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15189</link><description>&lt;p&gt;
&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Biomedical Entity Linking as Multiple Choice Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15189
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65288;BioEL&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#32454;&#31890;&#24230;&#21644;&#38271;&#23614;&#23454;&#20307;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BioELQA&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;BioELQA&#39318;&#20808;&#21033;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23558;&#25552;&#21450;&#21644;&#20505;&#36873;&#23454;&#20307;&#20849;&#21516;&#21576;&#29616;&#32473;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#36755;&#20986;&#19982;&#20854;&#36873;&#23450;&#23454;&#20307;&#30456;&#20851;&#30340;&#39044;&#27979;&#31526;&#21495;&#12290;&#36825;&#31181;&#20844;&#24335;&#20351;&#24471;&#19981;&#21516;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#26126;&#30830;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#25429;&#25417;&#20102;&#25552;&#21450;&#21644;&#23454;&#20307;&#20043;&#38388;&#20197;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#31934;&#32454;&#20132;&#20114;&#12290;&#20026;&#20102;&#25913;&#21892;&#38271;&#23614;&#23454;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#26816;&#32034;&#30456;&#20284;&#30340;&#24050;&#26631;&#35760;&#35757;&#32451;&#23454;&#20363;&#20316;&#20026;&#32447;&#32034;&#65292;&#24182;&#23558;&#36755;&#20837;&#19982;&#26816;&#32034;&#23454;&#20363;&#36830;&#25509;&#21040;&#29983;&#25104;&#22120;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BioELQA&#30340;&#34920;&#29616;&#20248;&#20110;&#32479;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15189v1 Announce Type: cross  Abstract: Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms stat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15183</link><description>&lt;p&gt;
GraphEdit&#65306;&#29992;&#20110;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphEdit: Large Language Models for Graph Structure Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;GSL&#65289;&#33268;&#21147;&#20110;&#36890;&#36807;&#29983;&#25104;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#26469;&#25429;&#25417;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#26174;&#24335;&#22270;&#32467;&#26500;&#20449;&#24687;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15183v1 Announce Type: cross  Abstract: Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy co
&lt;/p&gt;</description></item><item><title>&#36339;&#36291;&#35843;&#35856;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24778;&#20154;&#26377;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25193;&#25955;&#37319;&#26679;&#20013;UNet&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#31361;&#30772;&#20102;ODE&#37319;&#26679;&#22120;&#30340;&#38480;&#21046;</title><link>https://arxiv.org/abs/2402.15170</link><description>&lt;p&gt;
&#36339;&#36291;&#35843;&#35856;&#22312;&#25193;&#25955;&#37319;&#26679;&#20013;&#30340;&#24778;&#20154;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15170
&lt;/p&gt;
&lt;p&gt;
&#36339;&#36291;&#35843;&#35856;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24778;&#20154;&#26377;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25193;&#25955;&#37319;&#26679;&#20013;UNet&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#31361;&#30772;&#20102;ODE&#37319;&#26679;&#22120;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;UNet&#26550;&#26500;&#30340;&#25972;&#21512;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#20027;&#23548;&#21147;&#37327;&#12290;UNet&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#35774;&#35745;&#26159;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#22359;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#36339;&#36291;&#36830;&#25509;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#26679;&#30340;&#25463;&#24452;&#21487;&#33021;&#38480;&#21046;&#20102;&#21464;&#25442;&#30340;&#22797;&#26434;&#24615;&#12290;&#38543;&#30528;&#37319;&#26679;&#27493;&#39588;&#20943;&#23569;&#65292;&#29983;&#25104;&#36807;&#31243;&#21644;UNet&#30340;&#20316;&#29992;&#26356;&#25509;&#36817;&#20110;&#20174;&#39640;&#26031;&#20998;&#24067;&#21521;&#30446;&#26631;&#30340;&#25512;&#36827;&#36716;&#25442;&#65292;&#20026;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Skip-Tuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#20196;&#20154;&#24778;&#35766;&#22320;&#26377;&#25928;&#30340;&#22522;&#20110;&#36339;&#36807;&#36830;&#25509;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#35843;&#25972;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;ImageNet 64&#19978;&#20351;&#29992;19&#20010;NFE&#65288;1.75&#65289;&#20026;&#39044;&#35757;&#32451;&#30340;EDM&#23454;&#29616;100%&#30340;FID&#25913;&#36827;&#65292;&#31361;&#30772;&#20102;ODE&#37319;&#26679;&#22120;&#30340;&#38480;&#21046;&#65292;&#19981;&#35770;&#37319;&#26679;&#27493;&#39588;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15170v1 Announce Type: cross  Abstract: With the incorporation of the UNet architecture, diffusion probabilistic models have become a dominant force in image generation tasks. One key design in UNet is the skip connections between the encoder and decoder blocks. Although skip connections have been shown to improve training stability and model performance, we reveal that such shortcuts can be a limiting factor for the complexity of the transformation. As the sampling steps decrease, the generation process and the role of the UNet get closer to the push-forward transformations from Gaussian distribution to the target, posing a challenge for the network's complexity. To address this challenge, we propose Skip-Tuning, a simple yet surprisingly effective training-free tuning method on the skip connections. Our method can achieve 100% FID improvement for pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of ODE samplers regardless of sampling steps. Surpris
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15163</link><description>&lt;p&gt;
&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#20013;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Studying the Impact of Stochasticity on the Evaluation of Deep Neural Networks for Forest-Fire Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#29992;&#20110;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#37326;&#28779;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#20004;&#31867;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#20998;&#31867;&#30340;&#25351;&#26631;&#65292;&#35780;&#20272;&#23545;&#35266;&#23519;&#22320;&#38754;&#30495;&#30456;&#65288;GT&#65289;&#30340;&#24544;&#23454;&#24230;&#65292;&#20197;&#21450;&#36866;&#24403;&#30340;&#24471;&#20998;&#35268;&#21017;&#65292;&#27979;&#35797;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26862;&#26519;&#28779;&#28798;&#25968;&#25454;&#65292;&#31361;&#26174;&#20102;&#20256;&#32479;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#21487;&#35299;&#37322;&#30340;&#36866;&#29992;&#20110;&#38543;&#26426;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15163v1 Announce Type: cross  Abstract: This paper presents the first systematic study of the evaluation of Deep Neural Networks (DNNs) for discrete dynamical systems under stochastic assumptions, with a focus on wildfire prediction. We develop a framework to study the impact of stochasticity on two classes of evaluation metrics: classification-based metrics, which assess fidelity to observed ground truth (GT), and proper scoring rules, which test fidelity-to-statistic. Our findings reveal that evaluating for fidelity-to-statistic is a reliable alternative in highly stochastic scenarios. We extend our analysis to real-world wildfire data, highlighting limitations in traditional wildfire prediction evaluation methods, and suggest interpretable stochasticity-compatible alternatives.
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#35843;&#30340;&#25688;&#35201;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#23454;&#20307;&#32423;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15162</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#35843;&#30340;&#25277;&#35937;&#24335;&#25688;&#35201;&#27169;&#22411;&#30340;&#23454;&#20307;&#32423;&#20107;&#23454;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15162
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#35843;&#30340;&#25688;&#35201;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#23454;&#20307;&#32423;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#24335;&#25688;&#35201;&#27169;&#22411;&#22312;&#22788;&#29702;&#21442;&#25968;&#21270;&#30693;&#35782;&#19982;&#36755;&#20837;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#26102;&#65292;&#24448;&#24448;&#29983;&#25104;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#35843;&#30340;&#25688;&#35201;&#27169;&#22411;&#23545;&#30693;&#35782;&#20914;&#31361;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#25105;&#20204;&#31216;&#20043;&#20026;&#20107;&#23454;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#35780;&#20272;&#38598;&#65292;&#24182;&#21457;&#29616;&#20107;&#23454;&#36866;&#24212;&#24615;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#24182;&#38750;&#24378;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#25511;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20854;&#20013;&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#31243;&#24230;&#26159;&#21487;&#35843;&#33410;&#30340;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PEGASUS &#21644; BART&#65289;&#21644;&#20004;&#20010;&#24494;&#35843;&#25968;&#25454;&#38598;&#65288;XSum &#21644; CNN/DailyMail&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#23545;&#27604;&#26041;&#27861;&#30456;&#24403;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15162v1 Announce Type: cross  Abstract: Abstractive summarization models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document. In this paper, we analyze the robustness of fine-tuning based summarization models to the knowledge conflict, which we call factual adaptiveness. We utilize pre-trained language models to construct evaluation sets and find that factual adaptiveness is not strongly correlated with factual consistency on original datasets. Furthermore, we introduce a controllable counterfactual data augmentation method where the degree of knowledge conflict within the augmented data can be adjustable. Our experimental results on two pre-trained language models (PEGASUS and BART) and two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method enhances factual adaptiveness while achieving factual consistency on original datasets on par with the contrastiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15160</link><description>&lt;p&gt;
&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#30340;&#21464;&#21387;&#22120;&#35760;&#24518;&#20307;&#29992;&#20110;&#20307;&#39564;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Spatially-Aware Transformer Memory for Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#33410;&#35760;&#24518;&#22312;&#21508;&#31181;&#35748;&#30693;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#27604;&#22914;&#33021;&#22815;&#22312;&#22836;&#33041;&#20013;&#22238;&#24518;&#36807;&#21435;&#20107;&#20214;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#35748;&#30693;&#31185;&#23398;&#24378;&#35843;&#31354;&#38388;&#19978;&#19979;&#25991;&#22312;&#24773;&#33410;&#35760;&#24518;&#30340;&#24418;&#25104;&#21644;&#26816;&#32034;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24403;&#21069;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#24773;&#33410;&#35760;&#24518;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#36890;&#36807;&#23384;&#20648;&#26102;&#38388;&#39034;&#24207;&#20307;&#39564;&#30340;&#21464;&#21387;&#22120;&#65292;&#36825;&#24573;&#30053;&#20102;&#31354;&#38388;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#23558;&#22522;&#30784;&#32467;&#26500;&#25193;&#23637;&#21040;&#38500;&#20102;&#20165;&#26377;&#26102;&#38388;&#39034;&#24207;&#20043;&#22806;&#30340;&#31354;&#38388;&#36724;&#65292;&#24182;&#30001;&#27492;&#33021;&#22815;&#33719;&#24471;&#21738;&#20123;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#32771;&#34385;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#30340;&#22330;&#25152;&#20013;&#24515;&#24773;&#33410;&#35760;&#24518;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#65292;&#23548;&#33268;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15160v1 Announce Type: cross  Abstract: Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.15159</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning of Pre-trained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32972;&#26223;&#19979;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20197;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#35757;&#32451;&#27169;&#22411;&#8212;&#8212;&#19968;&#20010;&#26126;&#26174;&#32570;&#20047;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;LLMs&#20013;&#21246;&#21202;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#19971;&#31181;&#19981;&#21516;&#36951;&#24536;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;arXiv&#12289;&#20070;&#31821;&#21644;GitHub&#30340;&#31574;&#21010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#21147;&#30340;&#26426;&#22120;&#36951;&#24536;&#24615;&#33021;&#22522;&#20934;&#65292;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#27604;&#37325;&#26032;&#35757;&#32451;&#39640;&#20986; $10^5$ &#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#23558;&#26799;&#24230;&#19978;&#21319;&#19982;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#21487;&#20197;&#25913;&#21892;&#36229;&#21442;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36951;&#24536;&#36807;&#31243;&#20013;&#36827;&#34892;&#39640;&#25928;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#35814;&#32454;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#26377;&#20851;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30340;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#21644;&#23545;&#25239;&#35757;&#32451;(AT)&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15152</link><description>&lt;p&gt;
&#20851;&#20110;Sharpness-Aware&#26368;&#23567;&#21270;&#21644;&#23545;&#25239;&#35757;&#32451;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Duality Between Sharpness-Aware Minimization and Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15152
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#21644;&#23545;&#25239;&#35757;&#32451;(AT)&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;(Adversarial Training, AT)&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36755;&#20837;&#26679;&#26412;&#36827;&#34892;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#34987;&#35748;&#20026;&#26159;&#23545;&#25239;&#25915;&#20987;&#20013;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#20043;&#19968;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#19968;&#31181;&#22522;&#26412;&#30340;&#26435;&#34913;&#65292;&#21363;&#24517;&#28982;&#20250;&#38477;&#20302;&#24178;&#20928;&#20934;&#30830;&#24615;&#12290;&#19982;&#23545;&#26679;&#26412;&#36827;&#34892;&#25200;&#21160;&#19981;&#21516;&#65292;Sharpness-Aware&#26368;&#23567;&#21270;(SAM)&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#25200;&#21160;&#65292;&#20197;&#23547;&#25214;&#26356;&#24179;&#22374;&#30340;&#25439;&#22833;&#26354;&#38754;&#24182;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SAM&#26088;&#22312;&#25552;&#39640;&#24178;&#20928;&#20934;&#30830;&#24615;&#65292;&#20854;&#22312;&#22686;&#24378;&#23545;&#25239;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#21040;SAM&#21644;AT&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20174;SAM&#20013;&#27966;&#29983;&#30340;&#23545;&#25239;&#31283;&#20581;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#29420;&#20351;&#29992;SAM&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#29702;&#35299;SAM&#30340;&#36825;&#31181;&#24847;&#22806;&#29305;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;SAM&#22914;&#20309;&#38544;&#24335;&#23398;&#20064;&#26356;&#40065;&#26834;&#29305;&#24449;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#30340;&#35265;&#35299;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15152v1 Announce Type: cross  Abstract: Adversarial Training (AT), which adversarially perturb the input samples during training, has been acknowledged as one of the most effective defenses against adversarial attacks, yet suffers from a fundamental tradeoff that inevitably decreases clean accuracy. Instead of perturbing the samples, Sharpness-Aware Minimization (SAM) perturbs the model weights during training to find a more flat loss landscape and improve generalization. However, as SAM is designed for better clean accuracy, its effectiveness in enhancing adversarial robustness remains unexplored. In this work, considering the duality between SAM and AT, we investigate the adversarial robustness derived from SAM. Intriguingly, we find that using SAM alone can improve adversarial robustness. To understand this unexpected property of SAM, we first provide empirical and theoretical insights into how SAM can implicitly learn more robust features, and conduct comprehensive exper
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#32534;&#30721;&#22120;ReSaE&#65292;&#20855;&#26377;&#20840;&#23616;&#20851;&#31995;&#32467;&#26500;&#24847;&#35782;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#20851;&#31995;&#22312;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#30340;&#20132;&#20114;&#65292;&#24182;&#20248;&#21270;&#20102;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#35835;&#21462;&#32467;&#26500;&#65292;&#22312;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.15140</link><description>&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#20013;&#28040;&#24687;&#20256;&#36882;&#30340;&#20851;&#31995;&#20132;&#20114;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Relation-Interactive Approach for Message Passing in Hyper-relational Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15140
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#32534;&#30721;&#22120;ReSaE&#65292;&#20855;&#26377;&#20840;&#23616;&#20851;&#31995;&#32467;&#26500;&#24847;&#35782;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#20851;&#31995;&#22312;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#30340;&#20132;&#20114;&#65292;&#24182;&#20248;&#21270;&#20102;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#35835;&#21462;&#32467;&#26500;&#65292;&#22312;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#21253;&#21547;&#39069;&#22806;&#30340;&#38190;&#20540;&#23545;&#65292;&#25552;&#20379;&#20851;&#20110;&#20851;&#31995;&#30340;&#26356;&#22810;&#20449;&#24687;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#30456;&#21516;&#30340;&#20851;&#31995;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#30340;&#38190;&#20540;&#23545;&#65292;&#20351;&#21407;&#22987;&#19977;&#20803;&#32452;&#20107;&#23454;&#26356;&#20855;&#35782;&#21035;&#24615;&#21644;&#29305;&#23450;&#24615;&#12290;&#20808;&#21069;&#20851;&#20110;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#30340;&#30740;&#31350;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#31181;&#31283;&#22266;&#30340;&#36229;&#20851;&#31995;&#22270;&#32534;&#30721;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20840;&#23616;&#20851;&#31995;&#32467;&#26500;&#24847;&#35782;&#33021;&#21147;&#30340;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;ReSaE&#12290;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;ReSaE&#24378;&#35843;&#20102;&#22312;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#20851;&#31995;&#30340;&#20132;&#20114;&#65292;&#24182;&#20248;&#21270;&#20102;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#35835;&#21462;&#32467;&#26500;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;ReSaE&#20026;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#32534;&#30721;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#30830;&#20445;&#22312;&#19979;&#28216;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ReSaE&#22312;&#22810;&#20010;&#38142;&#25509;&#39044;&#27979;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15140v1 Announce Type: new  Abstract: Hyper-relational knowledge graphs (KGs) contain additional key-value pairs, providing more information about the relations. In many scenarios, the same relation can have distinct key-value pairs, making the original triple fact more recognizable and specific. Prior studies on hyper-relational KGs have established a solid standard method for hyper-relational graph encoding. In this work, we propose a message-passing-based graph encoder with global relation structure awareness ability, which we call ReSaE. Compared to the prior state-of-the-art approach, ReSaE emphasizes the interaction of relations during message passing process and optimizes the readout structure for link prediction tasks. Overall, ReSaE gives a encoding solution for hyper-relational KGs and ensures stronger performance on downstream link prediction tasks. Our experiments demonstrate that ReSaE achieves state-of-the-art performance on multiple link prediction benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20462;&#25913;&#30340;CycleGAN&#27169;&#22411;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#65292;&#20026;&#23567;&#40614;&#22836;&#37096;&#20998;&#21106;&#26679;&#26412;&#21512;&#25104;&#25552;&#20379;&#20102;&#21487;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.15135</link><description>&lt;p&gt;
&#20462;&#25913;&#30340;CycleGAN&#29992;&#20110;&#23567;&#40614;&#22836;&#37096;&#20998;&#21106;&#26679;&#26412;&#30340;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Modified CycleGAN for the synthesization of samples for wheat head segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20462;&#25913;&#30340;CycleGAN&#27169;&#22411;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#65292;&#20026;&#23567;&#40614;&#22836;&#37096;&#20998;&#21106;&#26679;&#26412;&#21512;&#25104;&#25552;&#20379;&#20102;&#21487;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#21508;&#31181;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#30340;&#65292;&#36825;&#31181;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#22823;&#35268;&#27169;&#24102;&#26377;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#27809;&#26377;&#24102;&#26377;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#36827;&#34892;&#27169;&#22411;&#24320;&#21457;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#65292;&#34987;&#31216;&#20026;&#22495;&#24046;&#24322;&#65292;&#23548;&#33268;&#30340;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#30495;&#23454;&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35745;&#31639;&#27169;&#25311;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#22635;&#34917;&#27169;&#25311;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29616;&#23454;&#27880;&#37322;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15135v1 Announce Type: cross  Abstract: Deep learning models have been used for a variety of image processing tasks. However, most of these models are developed through supervised learning approaches, which rely heavily on the availability of large-scale annotated datasets. Developing such datasets is tedious and expensive. In the absence of an annotated dataset, synthetic data can be used for model development; however, due to the substantial differences between simulated and real data, a phenomenon referred to as domain gap, the resulting models often underperform when applied to real data. In this research, we aim to address this challenge by first computationally simulating a large-scale annotated dataset and then using a generative adversarial network (GAN) to fill the gap between simulated and real images. This approach results in a synthetic dataset that can be effectively utilized to train a deep-learning model. Using this approach, we developed a realistic annotated
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#24207;&#20869;&#21644;&#24207;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#28145;&#24230;&#32806;&#21512;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#22810;&#38454;&#24207;&#20869;&#21644;&#24207;&#38388;&#30340;&#22797;&#26434;&#32806;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.15134</link><description>&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#28145;&#24230;&#32806;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Coupling Network For Multivariate Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15134
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#24207;&#20869;&#21644;&#24207;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#28145;&#24230;&#32806;&#21512;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#22810;&#38454;&#24207;&#20869;&#21644;&#24207;&#38388;&#30340;&#22797;&#26434;&#32806;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#20934;&#30830;&#30340;MTS&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24207;&#20869;&#21644;&#24207;&#38388;&#20851;&#31995;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#20998;&#21035;&#24314;&#27169;&#24207;&#20869;&#21644;&#24207;&#38388;&#20851;&#31995;&#65292;&#24182;&#24573;&#30053;&#20102;&#23384;&#22312;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20869;&#37096;&#21644;&#20043;&#38388;&#30340;&#22810;&#38454;&#20132;&#20114;&#65292;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#24207;&#20869;&#21644;&#24207;&#38388;&#20851;&#31995;&#65292;&#24182;&#30456;&#24212;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#25429;&#25417;&#22797;&#26434;&#22810;&#38454;&#24207;&#20869;&#21644;&#24207;&#38388;&#32806;&#21512;&#30340;&#20840;&#38754;&#20851;&#31995;&#23398;&#20064;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#19968;&#26426;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;MTS&#39044;&#27979;&#30340;&#28145;&#24230;&#32806;&#21512;&#32593;&#32476;&#65292;&#31216;&#20026;DeepCN&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#19987;&#29992;&#20110;&#26126;&#30830;&#25506;&#32034;&#22810;&#38454;&#24207;&#20869;&#21644;&#38388;&#30340;&#32806;&#21512;&#26426;&#21046;&#30340;&#32806;&#21512;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15134v1 Announce Type: cross  Abstract: Multivariate time series (MTS) forecasting is crucial in many real-world applications. To achieve accurate MTS forecasting, it is essential to simultaneously consider both intra- and inter-series relationships among time series data. However, previous work has typically modeled intra- and inter-series relationships separately and has disregarded multi-order interactions present within and between time series data, which can seriously degrade forecasting accuracy. In this paper, we reexamine intra- and inter-series relationships from the perspective of mutual information and accordingly construct a comprehensive relationship learning mechanism tailored to simultaneously capture the intricate multi-order intra- and inter-series couplings. Based on the mechanism, we propose a novel deep coupling network for MTS forecasting, named DeepCN, which consists of a coupling mechanism dedicated to explicitly exploring the multi-order intra- and in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.15131</link><description>&lt;p&gt;
&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36718;&#20132;&#20114;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#30340;&#39046;&#22495;&#12290;KBQA&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#22797;&#26434;&#38382;&#39064;&#35299;&#26512;&#20026;&#21487;&#25191;&#34892;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#65288;SP&#65289;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#21160;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#26223;&#19979;&#20805;&#20998;&#21033;&#29992;LLMs&#23558;&#38382;&#39064;&#35299;&#26512;&#20026;&#36923;&#36753;&#24418;&#24335;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;Interactive-KBQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#19982;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#30452;&#25509;&#20114;&#21160;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#12290;&#23545;&#20110;&#27599;&#31181;&#22797;&#26434;&#38382;&#39064;&#31867;&#21035;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;LLMs&#23436;&#25104;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15131v1 Announce Type: cross  Abstract: This study explores the realm of knowledge-base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. Yet, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20004;&#27493;&#37325;&#36848;&#29983;&#25104;&#36807;&#31243;&#23545;CLIP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#23545;&#37322;&#20041;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15120</link><description>&lt;p&gt;
&#20351;&#29992;&#20004;&#27493;&#37325;&#36848;&#23545;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15120
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20004;&#27493;&#37325;&#36848;&#29983;&#25104;&#36807;&#31243;&#23545;CLIP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#23545;&#37322;&#20041;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Contrastive language-image pre-training (CLIP)&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#26816;&#32034;&#65292;&#20854;&#20013;&#27169;&#22411;&#38656;&#35201;&#26377;&#25928;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20197;&#20135;&#29983;&#20934;&#30830;&#30340;&#35270;&#35273;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#27169;&#22411;&#22312;&#22788;&#29702;&#36755;&#20837;&#26597;&#35810;&#20013;&#30340;&#35821;&#35328;&#21464;&#21270;&#65288;&#22914;&#37322;&#20041;&#65289;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#38480;&#21046;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#22788;&#29702;&#29616;&#23454;&#24212;&#29992;&#20013;&#29992;&#25143;&#26597;&#35810;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;CLIP&#27169;&#22411;&#23545;&#37322;&#20041;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#20010;&#20004;&#27493;&#37322;&#20041;&#29983;&#25104;&#36807;&#31243;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#32593;&#39029;&#35268;&#27169;&#30340;&#22270;&#20687;&#26631;&#39064;&#20013;&#33258;&#21160;&#21019;&#24314;&#20004;&#31867;&#37322;&#20041;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#29983;&#25104;&#30340;&#37322;&#20041;&#26469;&#24494;&#35843;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#20923;&#32467;&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#27169;&#22411;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15120v1 Announce Type: cross  Abstract: Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#20195;&#29702;&#65288;LMAs&#65289;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#28085;&#30422;&#20102;&#24320;&#21457;&#32452;&#20214;&#12289;&#30740;&#31350;&#31867;&#22411;&#20998;&#31867;&#20197;&#21450;&#38598;&#20307;&#25928;&#33021;&#22686;&#24378;&#30340;&#21512;&#20316;&#26694;&#26550;&#31561;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2402.15116</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#20195;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Multimodal Agents: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15116
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#20195;&#29702;&#65288;LMAs&#65289;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#28085;&#30422;&#20102;&#24320;&#21457;&#32452;&#20214;&#12289;&#30740;&#31350;&#31867;&#22411;&#20998;&#31867;&#20197;&#21450;&#38598;&#20307;&#25928;&#33021;&#22686;&#24378;&#30340;&#21512;&#20316;&#26694;&#26550;&#31561;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#21160;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26102;&#21462;&#24471;&#20102;&#21331;&#36234;&#34920;&#29616;&#65292;&#36171;&#20104;&#23427;&#20204;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#20915;&#31574;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#26377;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#36235;&#21183;&#19987;&#27880;&#20110;&#23558;&#36825;&#20123;LLM&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#39046;&#22495;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;LLM&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#20195;&#29702;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22823;&#22411;&#22810;&#27169;&#24577;&#20195;&#29702;&#65288;LMAs&#31616;&#31216;&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21457;&#23637;LMAs&#25152;&#28041;&#21450;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23558;&#24403;&#21069;&#30340;&#30740;&#31350;&#33539;&#30068;&#20998;&#20026;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#38598;&#25104;&#22810;&#20010;LMAs&#20197;&#22686;&#24378;&#38598;&#20307;&#25928;&#33021;&#30340;&#21512;&#20316;&#26694;&#26550;&#12290;&#36825;&#19968;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#29616;&#26377;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#22810;&#26679;&#21270;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15116v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studie
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#24191;&#21578;&#31454;&#26631;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20256;&#32479;RL&#31639;&#27861;&#22312;&#22312;&#32447;&#29615;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15102</link><description>&lt;p&gt;
&#36712;&#36857;&#24335;&#36845;&#20195;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#31454;&#26631;
&lt;/p&gt;
&lt;p&gt;
Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15102
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24191;&#21578;&#31454;&#26631;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20256;&#32479;RL&#31639;&#27861;&#22312;&#22312;&#32447;&#29615;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#24191;&#21578;&#20013;&#65292;&#24191;&#21578;&#20027;&#21442;&#19982;&#24191;&#21578;&#31454;&#25293;&#20197;&#33719;&#21462;&#24191;&#21578;&#26426;&#20250;&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#38656;&#27714;&#26041;&#24179;&#21488;(DSPs)&#25552;&#20379;&#30340;&#33258;&#21160;&#31454;&#26631;&#24037;&#20855;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#31454;&#26631;&#31639;&#27861;&#36890;&#24120;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;RL&#30340;&#33258;&#21160;&#31454;&#26631;&#31574;&#30053;&#26159;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#37096;&#32626;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21487;&#20197;&#24182;&#34892;&#37096;&#32626;&#22810;&#20010;&#33258;&#21160;&#31454;&#26631;&#20195;&#29702;&#20197;&#25910;&#38598;&#22823;&#37327;&#20132;&#20114;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#21033;&#29992;&#31163;&#32447;RL&#31639;&#27861;&#35757;&#32451;&#26032;&#31574;&#30053;&#12290;&#35757;&#32451;&#21518;&#30340;&#31574;&#30053;&#38543;&#21518;&#21487;&#20197;&#37096;&#32626;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#36845;&#20195;&#35757;&#32451;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#36845;&#20195;&#31163;&#32447;RL&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31181;&#36845;&#20195;&#31163;&#32447;RL&#26694;&#26550;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#20854;&#26681;&#28304;&#22312;&#20110;&#30001;&#20110;&#20869;&#22312;&#21407;&#22240;&#32780;&#23548;&#33268;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15102v1 Announce Type: cross  Abstract: In online advertising, advertisers participate in ad auctions to acquire ad opportunities, often by utilizing auto-bidding tools provided by demand-side platforms (DSPs). The current auto-bidding algorithms typically employ reinforcement learning (RL). However, due to safety concerns, most RL-based auto-bidding policies are trained in simulation, leading to a performance degradation when deployed in online environments. To narrow this gap, we can deploy multiple auto-bidding agents in parallel to collect a large interaction dataset. Offline RL algorithms can then be utilized to train a new policy. The trained policy can subsequently be deployed for further data collection, resulting in an iterative training framework, which we refer to as iterative offline RL. In this work, we identify the performance bottleneck of this iterative offline RL framework, which originates from the ineffective exploration and exploitation caused by the inhe
&lt;/p&gt;</description></item><item><title>AttributionBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#21482;&#33021;&#36798;&#21040;80%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15089</link><description>&lt;p&gt;
AttributionBench&#65306;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#26377;&#22810;&#38590;&#65311;
&lt;/p&gt;
&lt;p&gt;
AttributionBench: How Hard is Automatic Attribution Evaluation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15089
&lt;/p&gt;
&lt;p&gt;
AttributionBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#21482;&#33021;&#36798;&#21040;80%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36890;&#36807;&#25552;&#20379;&#24341;&#29992;&#35777;&#25454;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#24402;&#22240;&#65292;&#21363;&#29983;&#25104;&#21709;&#24212;&#20013;&#30340;&#27599;&#20010;&#22768;&#26126;&#26159;&#21542;&#37117;&#24471;&#21040;&#20854;&#24341;&#29992;&#35777;&#25454;&#30340;&#20805;&#20998;&#25903;&#25345;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#20154;&#24037;&#35780;&#20272;&#30340;&#36825;&#31181;&#39564;&#35777;&#24378;&#35843;&#20102;&#23545;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#22522;&#20934;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AttributionBench&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#22522;&#20934;&#65292;&#30001;&#21508;&#31181;&#29616;&#26377;&#30340;&#24402;&#22240;&#25968;&#25454;&#38598;&#32534;&#21046;&#32780;&#25104;&#12290;&#25105;&#20204;&#22312;AttributionBench&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#25581;&#31034;&#20102;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;LLM&#20063;&#26159;&#22914;&#27492;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#32463;&#36807;&#20248;&#21270;&#30340;GPT-3.5&#22312;&#20108;&#20803;&#20998;&#31867;&#20844;&#24335;&#19979;&#20063;&#21482;&#33021;&#36798;&#21040;&#32422;80%&#30340;&#23439;F1&#20998;&#25968;&#12290;&#26356; than 300 error c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15089v1 Announce Type: cross  Abstract: Modern generative search engines enhance the reliability of large language model (LLM) responses by providing cited evidence. However, evaluating the answer's attribution, i.e., whether every claim within the generated responses is fully supported by its cited evidence, remains an open problem. This verification, traditionally dependent on costly human evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standardized benchmarks for these methods, we present AttributionBench, a comprehensive benchmark compiled from various existing attribution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art LLMs. Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error c
&lt;/p&gt;</description></item><item><title>Hands-Free VR &#26159;&#19968;&#31181;&#26080;&#38656;&#25163;&#37096;&#25805;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#38899;&#21629;&#20196;&#23454;&#29616;&#65292;&#20855;&#26377;&#33521;&#35821;&#21475;&#38899;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#23545;&#25991;&#26412;&#30340;&#36716;&#25442;&#21644;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.15083</link><description>&lt;p&gt;
&#26080;&#38656;&#25163;&#37096;&#25805;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hands-Free VR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15083
&lt;/p&gt;
&lt;p&gt;
Hands-Free VR &#26159;&#19968;&#31181;&#26080;&#38656;&#25163;&#37096;&#25805;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#38899;&#21629;&#20196;&#23454;&#29616;&#65292;&#20855;&#26377;&#33521;&#35821;&#21475;&#38899;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#23545;&#25991;&#26412;&#30340;&#36716;&#25442;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Hands-Free VR&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#33258;&#28982;&#35821;&#35328;&#34394;&#25311;&#29616;&#23454;&#30028;&#38754;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35821;&#38899;&#21457;&#20986;&#21629;&#20196;&#65292;&#20854;&#35821;&#38899;&#38899;&#39057;&#25968;&#25454;&#32463;&#36807;&#19968;&#20010;&#38024;&#23545;&#21333;&#35789;&#38899;&#32032;&#30456;&#20284;&#24615;&#21644;&#33521;&#35821;&#21475;&#38899;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#24494;&#35843;&#30340;&#35821;&#38899;&#35782;&#21035;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#28982;&#21518;&#21033;&#29992;&#19968;&#20010;&#23545;&#33258;&#28982;&#35821;&#35328;&#22810;&#26679;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#25991;&#26412;&#26144;&#23556;&#20026;&#21487;&#25191;&#34892;&#30340;&#34394;&#25311;&#29616;&#23454;&#21629;&#20196;&#12290;Hands-Free VR&#22312;&#19968;&#20010;&#21463;&#25511;&#30340;&#34987;&#35797;&#30740;&#31350;&#20013;&#65288;N = 22&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#25214;&#21040;&#29305;&#23450;&#29289;&#20307;&#24182;&#20197;&#21508;&#31181;&#37197;&#32622;&#25918;&#32622;&#23427;&#20204;&#12290;&#22312;&#23545;&#29031;&#26465;&#20214;&#19979;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;&#20256;&#32479;&#30340;&#34394;&#25311;&#29616;&#23454;&#29992;&#25143;&#30028;&#38754;&#36890;&#36807;&#25163;&#25345;&#25511;&#21046;&#22120;&#25235;&#21462;&#12289;&#25644;&#36816;&#21644;&#23450;&#20301;&#29289;&#20307;&#12290;&#22312;&#23454;&#39564;&#26465;&#20214;&#19979;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;Hands-Free VR&#12290;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;Hands-Free VR&#23545;&#33521;&#35821;&#21475;&#38899;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#22312;&#25105;&#20204;&#30340;20&#21517;&#21442;&#19982;&#32773;&#20013;&#65292;&#33521;&#35821;&#19981;&#26159;&#20182;&#20204;&#30340;&#39318;&#36873;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15083v1 Announce Type: cross  Abstract: The paper introduces Hands-Free VR, a voice-based natural-language interface for VR. The user gives a command using their voice, the speech audio data is converted to text using a speech-to-text deep learning model that is fine-tuned for robustness to word phonetic similarity and to spoken English accents, and the text is mapped to an executable VR command using a large language model that is robust to natural language diversity. Hands-Free VR was evaluated in a controlled within-subjects study (N = 22) that asked participants to find specific objects and to place them in various configurations. In the control condition participants used a conventional VR user interface to grab, carry, and position the objects using the handheld controllers. In the experimental condition participants used Hands-Free VR. The results confirm that: (1) Hands-Free VR is robust to spoken English accents, as for 20 of our participants English was not their f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#21483;&#20570;&#22534;&#21472;&#20998;&#35299;&#65288;SF&#65289;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#36125;&#21494;&#26031;&#32593;&#32476;&#27169;&#22411;&#20013;&#22788;&#29702;&#20998;&#21306;&#34920;&#36798;&#24335;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23567;&#22797;&#26434;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.15075</link><description>&lt;p&gt;
&#22312;&#28151;&#21512;&#36125;&#21494;&#26031;&#32593;&#32476;&#27169;&#22411;&#20013;&#22534;&#21472;&#20998;&#35299;&#20998;&#21306;&#34920;&#36798;&#24335;
&lt;/p&gt;
&lt;p&gt;
Stacking Factorizing Partitioned Expressions in Hybrid Bayesian Network Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15075
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#21483;&#20570;&#22534;&#21472;&#20998;&#35299;&#65288;SF&#65289;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#36125;&#21494;&#26031;&#32593;&#32476;&#27169;&#22411;&#20013;&#22788;&#29702;&#20998;&#21306;&#34920;&#36798;&#24335;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23567;&#22797;&#26434;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;HBN&#65289;&#21253;&#21547;&#20197;&#20998;&#21306;&#34920;&#36798;&#24335;&#25351;&#23450;&#30340;&#22797;&#26434;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#65288;CPD&#65289;&#65292;&#28041;&#21450;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#12290;&#22312;&#20351;&#29992;&#31163;&#25955;&#25512;&#26029;&#26102;&#65292;&#38543;&#30528;&#29238;&#33410;&#28857;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#20123;CPD&#30340;&#22823;&#23567;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#23548;&#33268;&#25928;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;&#36890;&#24120;&#65292;&#20943;&#23567;CPD&#22823;&#23567;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#20351;&#29992;&#20108;&#20803;&#20998;&#35299;&#65288;BF&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#36830;&#25509;&#30340;&#29238;&#33410;&#28857;&#25968;&#22240;&#23376;&#21270;&#20026;&#22823;&#23567;&#20026;&#20108;&#30340;&#38598;&#21512;&#26469;&#20998;&#35299;CPD&#20013;&#30340;&#32479;&#35745;&#25110;&#31639;&#26415;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;BF&#31639;&#27861;&#24182;&#38750;&#20026;&#22788;&#29702;&#20998;&#21306;&#34920;&#36798;&#24335;&#32780;&#35774;&#35745;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22534;&#21472;&#20998;&#35299;&#65288;SF&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20998;&#35299;&#20998;&#21306;&#34920;&#36798;&#24335;&#12290;SF&#31639;&#27861;&#21019;&#24314;&#20013;&#38388;&#33410;&#28857;&#65292;&#36880;&#27493;&#37325;&#24314;&#21407;&#22987;&#20998;&#21306;&#34920;&#36798;&#24335;&#20013;&#30340;&#23494;&#24230;&#65292;&#20801;&#35768;&#27599;&#20010;&#23376;&#33410;&#28857;&#36830;&#25509;&#30340;&#19981;&#36229;&#36807;&#20004;&#20010;&#36830;&#32493;&#29238;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15075v1 Announce Type: new  Abstract: Hybrid Bayesian networks (HBN) contain complex conditional probabilistic distributions (CPD) specified as partitioned expressions over discrete and continuous variables. The size of these CPDs grows exponentially with the number of parent nodes when using discrete inference, resulting in significant inefficiency. Normally, an effective way to reduce the CPD size is to use a binary factorization (BF) algorithm to decompose the statistical or arithmetic functions in the CPD by factorizing the number of connected parent nodes to sets of size two. However, the BF algorithm was not designed to handle partitioned expressions. Hence, we propose a new algorithm called stacking factorization (SF) to decompose the partitioned expressions. The SF algorithm creates intermediate nodes to incrementally reconstruct the densities in the original partitioned expression, allowing no more than two continuous parent nodes to be connected to each child node 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23545;&#35805;&#24335;&#32593;&#32476;&#23548;&#33322;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;MT-Mind2Web&#30340;&#29305;&#27530;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Self-MAP&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#25351;&#20196;&#36319;&#36394;&#20013;&#30340;&#38271;&#24230;&#21644;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15057</link><description>&lt;p&gt;
&#20851;&#20110;&#38754;&#21521;&#23545;&#35805;&#24335;&#32593;&#32476;&#20195;&#29702;&#30340;&#22810;&#36718;&#25351;&#20196;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
On the Multi-turn Instruction Following for Conversational Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15057
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23545;&#35805;&#24335;&#32593;&#32476;&#23548;&#33322;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;MT-Mind2Web&#30340;&#29305;&#27530;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Self-MAP&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#25351;&#20196;&#36319;&#36394;&#20013;&#30340;&#38271;&#24230;&#21644;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#32593;&#32476;&#20195;&#29702;&#22312;&#35268;&#21010;&#21644;&#25191;&#34892;&#22797;&#26434;&#22522;&#20110;&#32593;&#32476;&#30340;&#22810;&#27493;&#20132;&#20114;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#23436;&#25104;&#20102;&#21508;&#31181;&#32593;&#32476;&#23548;&#33322;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20197;LLM&#20026;&#21160;&#21147;&#30340;&#20195;&#29702;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#26377;&#25928;&#19982;&#39034;&#24207;&#29992;&#25143;&#25351;&#20196;&#36827;&#34892;&#20132;&#20114;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#23545;&#35805;&#24335;&#32593;&#32476;&#23548;&#33322;&#30340;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#38656;&#35201;&#19982;&#29992;&#25143;&#21644;&#29615;&#22659;&#36827;&#34892;&#36328;&#22810;&#36718;&#30340;&#22797;&#26434;&#20132;&#20114;&#65292;&#25903;&#25345;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;&#22810;&#36718;Mind2Web&#65288;MT-Mind2Web&#65289;&#30340;&#29305;&#21035;&#24320;&#21457;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#30340;&#26377;&#38480;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#23545;&#35805;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#21453;&#26144;&#35760;&#24518;&#22686;&#24378;&#35268;&#21010;&#65288;Self-MAP&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#35760;&#24518;&#21033;&#29992;&#21644;&#33258;&#25105;&#21453;&#24605;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15057v1 Announce Type: cross  Abstract: Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;Transformer&#20013;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#19979;&#28608;&#27963;&#29305;&#23450;token&#39044;&#27979;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#38416;&#26126;&#22312;LLMs&#20013;&#27880;&#24847;&#21147;&#22914;&#20309;&#20419;&#25104;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#21270;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.15055</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#35299;&#37322;&#19978;&#19979;&#25991;&#26597;&#25214;&#65306;&#25506;&#31350;&#27880;&#24847;&#21147;-MLP&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;Transformer&#20013;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#19979;&#28608;&#27963;&#29305;&#23450;token&#39044;&#27979;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#38416;&#26126;&#22312;LLMs&#20013;&#27880;&#24847;&#21147;&#22914;&#20309;&#20419;&#25104;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#21270;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#21644;Multilayer Perceptron&#20013;&#19987;&#38376;&#39044;&#27979;&#29305;&#23450;token&#30340;"next-token"&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#20419;&#20351;&#20687;GPT-4&#36825;&#26679;&#30340;LLM&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#20869;&#37096;&#65292;&#25105;&#20204;&#21487;&#20197;&#38416;&#26126;&#28608;&#27963;&#26576;&#20123;next-token&#31070;&#32463;&#20803;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30830;&#23450;&#20102;&#35782;&#21035;&#19982;&#39044;&#27979;&#29305;&#23450;token&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#30340;attention heads&#65292;&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#28608;&#27963;&#30456;&#20851;&#32852;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#36739;&#26089;&#30340;&#23618;&#20013;&#22987;&#32456;&#28608;&#27963;&#30456;&#21516;next-token&#31070;&#32463;&#20803;&#30340;attention heads&#12290;&#25506;&#32034;&#36825;&#20123;&#19981;&#21516;&#30340;&#28608;&#27963;&#27169;&#24335;&#25581;&#31034;&#20102;&#20026;&#19981;&#21516;&#35821;&#35328;&#19978;&#19979;&#25991;&#19987;&#38376;&#21270;&#30340;&#22836;&#19982;&#29983;&#25104;&#26576;&#20123;tokens&#30456;&#20851;&#32852;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#35299;&#37322;&#21644;&#25506;&#27979;&#23396;&#31435;&#30340;&#32452;&#20214;&#65292;&#20197;&#38416;&#26126;&#27880;&#24847;&#21147;&#22914;&#20309;&#20351;LLMs&#20013;&#30340;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#22788;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15055v1 Announce Type: cross  Abstract: In this paper, we investigate the interplay between attention heads and specialized "next-token" neurons in the Multilayer Perceptron that predict specific tokens. By prompting an LLM like GPT-4 to explain these model internals, we can elucidate attention mechanisms that activate certain next-token neurons. Our analysis identifies attention heads that recognize contexts relevant to predicting a particular token, activating the associated neuron through the residual connection. We focus specifically on heads in earlier layers consistently activating the same next-token neuron across similar prompts. Exploring these differential activation patterns reveals that heads that specialize for distinct linguistic contexts are tied to generating certain tokens. Overall, our method combines neural explanations and probing isolated components to illuminate how attention enables context-dependent, specialized processing in LLMs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ToMBench&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#24515;&#28789;&#29702;&#35770;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#36229;&#36807;10%&#12290;</title><link>https://arxiv.org/abs/2402.15052</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22522;&#20934;&#27979;&#35797;&#24515;&#28789;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
ToMBench: Benchmarking Theory of Mind in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15052
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ToMBench&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#24515;&#28789;&#29702;&#35770;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#36229;&#36807;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#26159;&#25351;&#24863;&#30693;&#21644;&#24402;&#22240;&#33258;&#24049;&#20197;&#21450;&#20182;&#20154;&#30340;&#24515;&#29702;&#29366;&#24577;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#21457;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#34920;&#29616;&#20986;&#19968;&#31181;&#24418;&#24335;&#30340;&#24515;&#28789;&#29702;&#35770;&#30340;&#20105;&#35770;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24515;&#28789;&#29702;&#35770;&#35780;&#20272;&#21463;&#21040;&#35832;&#22914;&#21463;&#38480;&#33539;&#22260;&#12289;&#20027;&#35266;&#21028;&#26029;&#21644;&#24847;&#22806;&#27745;&#26579;&#31561;&#25361;&#25112;&#30340;&#21046;&#32422;&#65292;&#23548;&#33268;&#35780;&#20272;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ToMBench&#65292;&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#65306;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#28085;&#30422;&#31038;&#20250;&#35748;&#30693;&#20013;&#30340;8&#39033;&#20219;&#21153;&#21644;31&#39033;&#33021;&#21147;&#65292;&#22810;&#39033;&#36873;&#25321;&#39064;&#26684;&#24335;&#20197;&#25903;&#25345;&#33258;&#21160;&#21270;&#21644;&#26080;&#20559;&#35265;&#30340;&#35780;&#20272;&#65292;&#20197;&#21450;&#22522;&#20110;&#21452;&#35821;&#28165;&#21333;&#30340;&#20174;&#22836;&#26500;&#24314;&#65292;&#20005;&#26684;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#22522;&#20110;ToMBench&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;10&#20010;&#27969;&#34892;LLMs&#22312;&#20219;&#21153;&#21644;&#33021;&#21147;&#26041;&#38754;&#30340;&#24515;&#28789;&#29702;&#35770;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#20687;GPT-4&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;LLMs&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#33853;&#21518;&#36229;&#36807;10&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15052v1 Announce Type: cross  Abstract: Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicati
&lt;/p&gt;</description></item><item><title>ChatEA&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23454;&#20307;&#23545;&#40784;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;KG-code&#32763;&#35793;&#27169;&#22359;&#21644;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#26469;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15048</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#20307;&#23545;&#40784;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Power of Large Language Models for Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15048
&lt;/p&gt;
&lt;p&gt;
ChatEA&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23454;&#20307;&#23545;&#40784;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;KG-code&#32763;&#35793;&#27169;&#22359;&#21644;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#26469;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#23545;&#20110;&#25972;&#21512;&#19981;&#21516;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;EA&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#27604;&#36739;&#23454;&#20307;&#23884;&#20837;&#65292;&#20294;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#36755;&#20837;KG&#25968;&#25454;&#21644;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;&#33021;&#21147;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#32422;&#26463;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatEA&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#20837;&#20197;&#25913;&#21892;EA&#12290;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#30340;&#36755;&#20837;KG&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;ChatEA&#24341;&#20837;&#20102;&#19968;&#20010;KG-code&#32763;&#35793;&#27169;&#22359;&#65292;&#23558;KG&#32467;&#26500;&#32763;&#35793;&#25104;LLMs&#21487;&#29702;&#35299;&#30340;&#26684;&#24335;&#65292;&#20174;&#32780;&#20351;LLMs&#33021;&#22815;&#21033;&#29992;&#20854;&#24191;&#27867;&#30340;&#32972;&#26223;&#30693;&#35782;&#25552;&#39640;EA&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#23545;&#23454;&#20307;&#23884;&#20837;&#27604;&#36739;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;ChatEA&#23454;&#29616;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#65292;&#21033;&#29992;LLMs&#22312;&#23545;&#35805;&#26684;&#24335;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15048v1 Announce Type: cross  Abstract: Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs' capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy wh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;</title><link>https://arxiv.org/abs/2402.15043</link><description>&lt;p&gt;
KIEval&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#35780;&#20272;&#34987;&#22840;&#22823;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#26088;&#22312;&#26816;&#27979;&#21463;&#27745;&#26579;&#30340;&#25991;&#26412;&#65292;&#20294;&#20391;&#37325;&#20110;&#37327;&#21270;&#27745;&#26579;&#31243;&#24230;&#32780;&#38750;&#20934;&#30830;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;KIEval&#65292;&#36825;&#26159;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;LLM&#39537;&#21160;&#30340;&#8220;&#20132;&#20114;&#32773;&#8221;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#25239;&#27745;&#26579;&#35780;&#20272;&#12290;&#20174;&#28041;&#21450;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#24120;&#35268;LLM&#22522;&#20934;&#38382;&#39064;&#24320;&#22987;&#65292;KIEval&#21033;&#29992;&#21160;&#24577;&#29983;&#25104;&#30340;&#12289;&#22810;&#36718;&#12289;&#20197;&#30693;&#35782;&#20026;&#37325;&#28857;&#30340;&#23545;&#35805;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#30340;&#21709;&#24212;&#26159;&#21542;&#20165;&#26159;&#22522;&#20934;&#31572;&#26696;&#30340;&#22238;&#24518;&#65292;&#36824;&#26159;&#34920;&#26126;&#20102;&#28145;&#20837;&#29702;&#35299;&#24182;&#33021;&#22312;&#26356;&#22797;&#26434;&#30340;&#23545;&#35805;&#20013;&#24212;&#29992;&#30693;&#35782;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19971;&#20010;&#39046;&#20808;&#30340;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;KI
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15043v1 Announce Type: cross  Abstract: Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KI
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20849;&#20139;&#30340;&#21160;&#21147;&#23398;&#32593;&#32476;&#20026;&#19981;&#21516;&#25805;&#20316;&#20219;&#21153;&#29983;&#25104; manipulator &#20960;&#20309;&#35774;&#35745;&#65292;&#36890;&#36807;&#35774;&#35745;&#30446;&#26631;&#26500;&#24314;&#30340;&#26799;&#24230;&#24341;&#23548;&#25163;&#25351;&#20960;&#20309;&#35774;&#35745;&#30340;&#23436;&#21892;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.15038</link><description>&lt;p&gt;
&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154; manipulator &#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Dynamics-Guided Diffusion Model for Robot Manipulator Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20849;&#20139;&#30340;&#21160;&#21147;&#23398;&#32593;&#32476;&#20026;&#19981;&#21516;&#25805;&#20316;&#20219;&#21153;&#29983;&#25104; manipulator &#20960;&#20309;&#35774;&#35745;&#65292;&#36890;&#36807;&#35774;&#35745;&#30446;&#26631;&#26500;&#24314;&#30340;&#26799;&#24230;&#24341;&#23548;&#25163;&#25351;&#20960;&#20309;&#35774;&#35745;&#30340;&#23436;&#21892;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#29983;&#25104; manipulator &#20960;&#20309;&#35774;&#35745;&#12290;&#19982;&#20026;&#27599;&#20010;&#20219;&#21153;&#35757;&#32451;&#19981;&#21516;&#30340;&#35774;&#35745;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#20010;&#36328;&#20219;&#21153;&#20849;&#20139;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#32593;&#32476;&#12290;&#23545;&#20110;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#32452;&#31216;&#20026;&#30446;&#26631;&#30456;&#20114;&#20316;&#29992;&#37197;&#32622;&#25991;&#20214;&#30340;&#20010;&#21035;&#36816;&#21160;&#30446;&#26631;&#65292;&#20854;&#20013;&#27599;&#20010;&#20010;&#21035;&#36816;&#21160;&#21487;&#20197;&#30001;&#20849;&#20139;&#30340;&#21160;&#21147;&#23398;&#32593;&#32476;&#24314;&#27169;&#12290;&#20174;&#30446;&#26631;&#21644;&#39044;&#27979;&#30340;&#30456;&#20114;&#20316;&#29992;&#37197;&#32622;&#25991;&#20214;&#26500;&#24314;&#30340;&#35774;&#35745;&#30446;&#26631;&#20026;&#20219;&#21153;&#30340;&#25163;&#25351;&#20960;&#20309;&#35774;&#35745;&#25552;&#20379;&#20102;&#26799;&#24230;&#24341;&#23548;&#12290;&#36825;&#20010;&#35774;&#35745;&#36807;&#31243;&#34987;&#25191;&#34892;&#20026;&#19968;&#31181;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#35774;&#35745;&#30446;&#26631;&#20316;&#20026;&#20998;&#31867;&#22120;&#24341;&#23548;&#12290;&#25105;&#20204;&#22312;&#21482;&#20351;&#29992;&#24320;&#29615;&#24179;&#34892;&#22841;&#29226;&#36816;&#21160;&#30340;&#26080;&#20256;&#24863;&#22120;&#35774;&#32622;&#19979;&#65292;&#22312;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15038v1 Announce Type: cross  Abstract: We present Dynamics-Guided Diffusion Model, a data-driven framework for generating manipulator geometry designs for a given manipulation task. Instead of training different design models for each task, our approach employs a learned dynamics network shared across tasks. For a new manipulation task, we first decompose it into a collection of individual motion targets which we call target interaction profile, where each individual motion can be modeled by the shared dynamics network. The design objective constructed from the target and predicted interaction profiles provides a gradient to guide the refinement of finger geometry for the task. This refinement process is executed as a classifier-guided diffusion process, where the design objective acts as the classifier guidance. We evaluate our framework on various manipulation tasks, under the sensor-less setting using only an open-loop parallel jaw motion. Our generated designs outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#35270;&#35282;&#25506;&#35752;&#20102;&#25945;&#32946;&#20013;&#19981;&#21516;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#21487;&#25509;&#21463;&#24615;&#65292;&#20851;&#27880;&#25968;&#25454;&#38544;&#31169;&#12289;AI&#20195;&#29702;&#12289;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36947;&#24503;&#37096;&#32626;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15027</link><description>&lt;p&gt;
&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#35270;&#35282;&#19979;&#30340;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#19982;&#25945;&#32946;&#21487;&#25509;&#21463;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#35270;&#35282;&#25506;&#35752;&#20102;&#25945;&#32946;&#20013;&#19981;&#21516;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#21487;&#25509;&#21463;&#24615;&#65292;&#20851;&#27880;&#25968;&#25454;&#38544;&#31169;&#12289;AI&#20195;&#29702;&#12289;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36947;&#24503;&#37096;&#32626;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35270;&#35282;&#65292;&#21253;&#25324;&#23398;&#29983;&#12289;&#25945;&#24072;&#21644;&#23478;&#38271;&#65292;&#35843;&#26597;&#20102;&#25945;&#32946;&#20013;&#19981;&#21516;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#30340;&#21487;&#25509;&#21463;&#24615;&#12290;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#20013;&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#23427;&#20851;&#27880;&#20102;&#19982;&#25968;&#25454;&#38544;&#31169;&#12289;AI&#20195;&#29702;&#12289;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#36947;&#24503;&#37096;&#32626;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21019;&#26223;&#26041;&#27861;&#65292;&#21442;&#19982;&#32773;&#34987;&#21576;&#29616;&#20102;&#22235;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;AI&#30340;&#20195;&#29702;&#12289;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#38544;&#31169;&#36973;&#21040;&#25805;&#32437;&#12290;&#22312;&#27599;&#20010;&#22330;&#26223;&#21518;&#65292;&#21442;&#19982;&#32773;&#23436;&#25104;&#20102;&#19968;&#20010;&#35843;&#26597;&#38382;&#21367;&#65292;&#20854;&#20013;&#25429;&#25417;&#20102;&#20182;&#20204;&#23545;AI&#30340;&#20840;&#29699;&#25928;&#29992;&#12289;&#20010;&#20154;&#23454;&#29992;&#24615;&#12289;&#20844;&#27491;&#24615;&#12289;&#20449;&#24515;&#12289;&#39118;&#38505;&#20197;&#21450;&#33509;&#27599;&#20010;&#22330;&#26223;&#30340;AI&#21487;&#29992;&#35805;&#65292;&#20182;&#20204;&#25171;&#31639;&#20351;&#29992;&#30340;&#24847;&#22270;&#12290;&#25968;&#25454;&#25910;&#38598;&#28085;&#30422;&#20102;&#26368;&#32456;&#26679;&#26412;&#37327;&#20026;1198&#21517;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#21442;&#19982;&#32773;&#65292;&#36890;&#36807;&#19968;&#20010;&#21512;&#20316;&#26426;&#26500;&#21644;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#20998;&#21457;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#23545;&#22235;&#20010;&#22330;&#26223;&#30340;&#20010;&#20307;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15027v1 Announce Type: cross  Abstract: This study investigates the acceptability of different artificial intelligence (AI) applications in education from a multi-stakeholder perspective, including students, teachers, and parents. Acknowledging the transformative potential of AI in education, it addresses concerns related to data privacy, AI agency, transparency, explainability and the ethical deployment of AI. Through a vignette methodology, participants were presented with four scenarios where AI's agency, transparency, explainability, and privacy were manipulated. After each scenario, participants completed a survey that captured their perceptions of AI's global utility, individual usefulness, justice, confidence, risk, and intention to use each scenario's AI if available. The data collection comprising a final sample of 1198 multi-stakeholder participants was distributed through a partner institution and social media campaigns and focused on individual responses to four 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#24341;&#23548;&#28201;&#24230;&#32553;&#25918;&#65288;CTS&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#20379;&#28304;&#22495;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#30417;&#30563;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#22495;&#22806;&#65288;OOD&#65289;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15019</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#24335;&#21644;&#20869;&#23481;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#24341;&#23548;&#28201;&#24230;&#32553;&#25918;&#29992;&#20110;&#22495;&#22806;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15019
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#24341;&#23548;&#28201;&#24230;&#32553;&#25918;&#65288;CTS&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#20379;&#28304;&#22495;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#30417;&#30563;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#22495;&#22806;&#65288;OOD&#65289;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19978;&#65292;&#32780;&#19981;&#26159;&#26657;&#20934;&#24615;&#33021;&#65292;&#32780;&#21518;&#32773;&#26159;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#31995;&#32479;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#35201;&#27714;&#12290;&#28201;&#24230;&#32553;&#25918;&#65288;TS&#65289;&#20316;&#20026;&#19968;&#31181;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#22312;&#39046;&#22495;&#20869;&#29615;&#22659;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#21364;&#19981;&#26159;&#65292;&#22240;&#20026;&#20107;&#20808;&#24456;&#38590;&#33719;&#21462;&#26410;&#35265;&#39046;&#22495;&#30340;&#39564;&#35777;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28201;&#24230;&#32553;&#25918;&#31574;&#30053;&#65292;&#19968;&#33268;&#24615;&#24341;&#23548;&#28201;&#24230;&#32553;&#25918;&#65288;CTS&#65289;&#65292;&#36890;&#36807;&#25552;&#20379;&#28304;&#22495;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#30417;&#30563;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;OOD&#26657;&#20934;&#24615;&#33021;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#35266;&#23519;&#21040;&#30340;&#21457;&#29616;&#65292;&#30001;&#20110;&#19981;&#19968;&#33268;&#30340;&#26679;&#26412;&#39044;&#27979;&#23548;&#33268;&#30340;&#36807;&#24230;&#33258;&#20449;&#26159;OOD&#26657;&#20934;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26657;&#20934;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15019v1 Announce Type: cross  Abstract: Research interests in the robustness of deep neural networks against domain shifts have been rapidly increasing in recent years. Most existing works, however, focus on improving the accuracy of the model, not the calibration performance which is another important requirement for trustworthy AI systems. Temperature scaling (TS), an accuracy-preserving post-hoc calibration method, has been proven to be effective in in-domain settings, but not in out-of-domain (OOD) due to the difficulty in obtaining a validation set for the unseen domain beforehand. In this paper, we propose consistency-guided temperature scaling (CTS), a new temperature scaling strategy that can significantly enhance the OOD calibration performance by providing mutual supervision among data samples in the source domains. Motivated by our observation that over-confidence stemming from inconsistent sample predictions is the main obstacle to OOD calibration, we propose to 
&lt;/p&gt;</description></item><item><title>&#22810;&#20219;&#21153;&#24494;&#35843;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#22522;&#30784;&#27169;&#22411;&#19978;&#23545;&#30456;&#20851;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#36866;&#24212;&#38480;&#21046;&#26631;&#31614;&#25968;&#30340;&#30446;&#26631;&#20219;&#21153;&#65292;&#33021;&#22815;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#20219;&#21153;&#36873;&#25321;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15017</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#22522;&#30784;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15017
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#24494;&#35843;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#22522;&#30784;&#27169;&#22411;&#19978;&#23545;&#30456;&#20851;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#36866;&#24212;&#38480;&#21046;&#26631;&#31614;&#25968;&#30340;&#30446;&#26631;&#20219;&#21153;&#65292;&#33021;&#22815;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#20219;&#21153;&#36873;&#25321;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#23613;&#31649;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26377;&#25928;&#22320;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#25968;&#25454;&#26631;&#31614;&#26377;&#38480;&#30340;&#20219;&#21153;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#19988;&#32570;&#20047;&#29702;&#35770;&#29702;&#35299;&#12290;&#26368;&#36817;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;&#19968;&#31181;&#26032;&#20852;&#35299;&#20915;&#26041;&#26696;&#26159;&#65292;&#22312;&#22522;&#30784;&#27169;&#22411;&#19978;&#23545;&#19968;&#31995;&#21015;&#30456;&#20851;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#20877;&#36866;&#24212;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#26679;&#26412;&#30340;&#30446;&#26631;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#22810;&#20219;&#21153;&#24494;&#35843;&#26041;&#27861;&#30340;&#29702;&#35770;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#30456;&#20851;&#20219;&#21153;&#38598;&#65292;&#36825;&#31181;&#22810;&#20219;&#21153;&#24494;&#35843;&#21487;&#20197;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#35823;&#24046;&#65292;&#19982;&#30452;&#25509;&#36866;&#24212;&#30456;&#21516;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#25351;&#26631;&#37327;&#21270;&#20102;&#24494;&#35843;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#20219;&#21153;&#36873;&#25321;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15017v1 Announce Type: cross  Abstract: Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask finetuning approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Ar-Spider&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#35328;&#30340;&#29420;&#29305;&#24615;&#36136;&#25152;&#24102;&#26469;&#30340;&#27169;&#24335;&#35821;&#35328;&#21644;SQL&#32467;&#26500;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;&#24182;&#27979;&#35797;&#20102;&#20004;&#20010;&#36328;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15012</link><description>&lt;p&gt;
Ar-Spider&#65306;&#38463;&#25289;&#20271;&#35821;&#20013;&#30340;&#25991;&#26412;&#36716;SQL
&lt;/p&gt;
&lt;p&gt;
Ar-Spider: Text-to-SQL in Arabic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Ar-Spider&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#35328;&#30340;&#29420;&#29305;&#24615;&#36136;&#25152;&#24102;&#26469;&#30340;&#27169;&#24335;&#35821;&#35328;&#21644;SQL&#32467;&#26500;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;&#24182;&#27979;&#35797;&#20102;&#20004;&#20010;&#36328;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#26159;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#23427;&#26088;&#22312;&#20351;&#29992;&#25143;&#20197;&#26356;&#33258;&#28982;&#30340;&#26041;&#24335;&#19982;&#25968;&#25454;&#24211;&#36827;&#34892;&#20132;&#20114;&#12290;&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#21040;SQL&#24050;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;Ar-Spider&#12290;&#30001;&#20110;&#35813;&#35821;&#35328;&#30340;&#29420;&#29305;&#24615;&#36136;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#27169;&#24335;&#35821;&#35328;&#21644;SQL &#32467;&#26500;&#25361;&#25112;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#24182;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;LGESQL&#21644;S2SQL&#65292;&#20004;&#32773;&#22343;&#19982;&#20004;&#20010;&#36328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#20943;&#36731;&#27169;&#24335;&#35821;&#35328;&#21644;SQL&#32467;&#26500;&#38142;&#25509;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#22522;&#32447;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;Ar-Spider&#19978;&#34920;&#29616;&#20986;&#19981;&#38169;&#30340;&#21333;&#35821;&#35328;&#24615;&#33021;&#65292;&#20854;&#20013;S2SQL&#23454;&#29616;&#20102;62.48%&#65292;LGESQL&#23454;&#29616;&#20102;65.57%&#65292;&#20165;&#20302;&#20110;8.79%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15012v1 Announce Type: cross  Abstract: In Natural Language Processing (NLP), one of the most important tasks is text-to-SQL semantic parsing, which focuses on enabling users to interact with the database in a more natural manner. In recent years, text-to-SQL has made significant progress, but most were English-centric. In this paper, we introduce Ar-Spider 1, the first Arabic cross-domain text-to-SQL dataset. Due to the unique nature of the language, two major challenges have been encountered, namely schema linguistic and SQL structural challenges. In order to handle these issues and conduct the experiments, we adopt two baseline models LGESQL [4] and S2SQL [12], both of which are tested with two cross-lingual models to alleviate the effects of schema linguistic and SQL structure linking challenges. The baselines demonstrate decent single-language performance on our Arabic text-to-SQL dataset, Ar-Spider, achieving 62.48% for S2SQL and 65.57% for LGESQL, only 8.79% below the
&lt;/p&gt;</description></item><item><title>BAIs&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#26367;&#31070;&#32463;-&#35748;&#30693;&#22788;&#29702;&#31649;&#32447;&#30340;&#37096;&#20998;&#65292;&#35753;&#35748;&#30693;&#21151;&#33021;&#21463;&#25439;&#30340;&#20010;&#20307;&#33021;&#22815;&#36890;&#36807;&#39640;&#23618;&#24847;&#22270;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#20363;&#22914;&#36890;&#36807;&#20027;&#35266;&#25552;&#20379;&#24847;&#22270;&#23436;&#25104;&#27169;&#25311;&#30005;&#35805;&#23545;&#35805;&#12290;</title><link>https://arxiv.org/abs/2402.15011</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#35805;&#24335;&#33041;-&#20154;&#24037;&#26234;&#33021;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
A Conversational Brain-Artificial Intelligence Interface
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15011
&lt;/p&gt;
&lt;p&gt;
BAIs&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#26367;&#31070;&#32463;-&#35748;&#30693;&#22788;&#29702;&#31649;&#32447;&#30340;&#37096;&#20998;&#65292;&#35753;&#35748;&#30693;&#21151;&#33021;&#21463;&#25439;&#30340;&#20010;&#20307;&#33021;&#22815;&#36890;&#36807;&#39640;&#23618;&#24847;&#22270;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#20363;&#22914;&#36890;&#36807;&#20027;&#35266;&#25552;&#20379;&#24847;&#22270;&#23436;&#25104;&#27169;&#25311;&#30005;&#35805;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#33041;-&#20154;&#24037;&#26234;&#33021;&#30028;&#38754;&#65288;BAIs&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#65288;BCIs&#65289;&#31867;&#21035;&#24341;&#20837;&#12290;&#19981;&#21516;&#20110;&#20381;&#36182;&#23436;&#22909;&#35748;&#30693;&#21151;&#33021;&#30340;&#20256;&#32479;BCIs&#65292;BAIs&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#21147;&#37327;&#26469;&#26367;&#20195;&#31070;&#32463;-&#35748;&#30693;&#22788;&#29702;&#31649;&#32447;&#30340;&#37096;&#20998;&#12290;BAIs&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#25552;&#20379;&#39640;&#23618;&#24847;&#22270;&#26469;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#32780;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;AI&#20195;&#29702;&#30830;&#23450;&#20302;&#23618;&#27425;&#32454;&#33410;&#12290;&#35813;&#26041;&#27861;&#23558;BCIs&#30340;&#30446;&#26631;&#21463;&#20247;&#25193;&#22823;&#21040;&#35748;&#30693;&#21151;&#33021;&#21463;&#25439;&#30340;&#20010;&#20307;&#65292;&#36825;&#26159;&#24120;&#24120;&#34987;&#25490;&#38500;&#22312;&#20256;&#32479;BCIs&#22909;&#22788;&#20043;&#22806;&#30340;&#20154;&#32676;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BAIs&#30340;&#19968;&#33324;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#23545;&#35805;&#24335;BAI&#23637;&#31034;&#20102;&#36825;&#31181;&#26032;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#30005;&#35805;&#23545;&#35805;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#23545;&#35805;&#24335;BAI&#33021;&#22815;&#23454;&#29616;&#22797;&#26434;&#36890;&#20449;&#65292;&#32780;&#26080;&#38656;&#29983;&#25104;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#35777;&#26126;&#20102;&#65292;&#23545;&#20110;&#31532;&#19968;&#27425;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15011v1 Announce Type: cross  Abstract: We introduce Brain-Artificial Intelligence Interfaces (BAIs) as a new class of Brain-Computer Interfaces (BCIs). Unlike conventional BCIs, which rely on intact cognitive capabilities, BAIs leverage the power of artificial intelligence to replace parts of the neuro-cognitive processing pipeline. BAIs allow users to accomplish complex tasks by providing high-level intentions, while a pre-trained AI agent determines low-level details. This approach enlarges the target audience of BCIs to individuals with cognitive impairments, a population often excluded from the benefits of conventional BCIs. We present the general concept of BAIs and illustrate the potential of this new approach with a Conversational BAI based on EEG. In particular, we show in an experiment with simulated phone conversations that the Conversational BAI enables complex communication without the need to generate language. Our work thus demonstrates, for the first time, th
&lt;/p&gt;</description></item><item><title>&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;</title><link>https://arxiv.org/abs/2402.15010</link><description>&lt;p&gt;
&#27861;&#35821;&#21307;&#29992;&#21475;&#32617;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26631;&#35760;&#21270;&#26377;&#22810;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Important Is Tokenization in French Medical Masked Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15010
&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23376;&#35789;&#30340;&#26631;&#35760;&#21270;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#20854;&#25104;&#21151;&#30340;&#30830;&#20999;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#65292;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#28982;&#19981;&#22815;&#28165;&#26970;&#12290;&#36825;&#22312;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#29305;&#28857;&#26159;&#20855;&#26377;&#31649;&#29702;&#24418;&#24577;&#32032;&#32452;&#21512;&#30340;&#29305;&#23450;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15010v1 Announce Type: cross  Abstract: Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14992</link><description>&lt;p&gt;
&#23567;&#22411;&#22522;&#20934;&#27979;&#35797;&#65306;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#35780;&#20272;LLM
&lt;/p&gt;
&lt;p&gt;
tinyBenchmarks: evaluating LLMs with fewer examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#21151;&#33021;&#24615;&#23548;&#33268;&#21019;&#24314;&#20102;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#65292;&#24443;&#24213;&#27979;&#35797;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#31034;&#20363;&#65292;&#20351;&#24471;&#35780;&#20272;LLMs&#38750;&#24120;&#26114;&#36149;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35201;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;MMLU&#19978;&#30340;&#24615;&#33021;&#65288;&#19968;&#20010;&#21253;&#21547;14K&#20010;&#31034;&#20363;&#30340;&#27969;&#34892;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#65289;&#65292;&#21482;&#38656;&#35201;&#22312;100&#20010;&#31934;&#24515;&#25361;&#36873;&#30340;&#31034;&#20363;&#19978;&#35780;&#20272;&#36825;&#20010;LLMs&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35780;&#20272;&#24037;&#20855;&#21644;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#24494;&#22411;&#29256;&#26412;&#65306;Open LLM Leaderboard&#12289;MMLU&#12289;HELM&#21644;AlpacaEval 2.0&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#24037;&#20855;&#21644;&#24494;&#22411;&#22522;&#20934;&#27979;&#35797;&#36275;&#20197;&#21487;&#38752;&#19988;&#39640;&#25928;&#22320;&#37325;&#29616;&#21407;&#22987;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14989</link><description>&lt;p&gt;
&#20998;&#26512;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14989
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#23545;&#20110;&#20551;&#35774;&#19968;&#33268;&#38388;&#38548;&#21644;&#23436;&#25972;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#32467;&#21512;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#21521;&#37327;&#22330;&#23398;&#20064;&#36830;&#32493;&#28508;&#22312;&#34920;&#31034;&#12290;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#39033;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#32780;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#26102;&#65292;&#36825;&#31181;&#28155;&#21152;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#20180;&#32454;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#23545;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31895;&#24515;&#30340;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#27809;&#26377;&#24378;&#35299;&#12289;&#38543;&#26426;&#30772;&#22351;&#25110;&#19981;&#31283;&#23450;&#30340;Euler&#31163;&#25955;&#21270;&#31561;&#19981;&#21033;&#30340;&#24615;&#36136;&#65292;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22242;&#20307;&#26500;&#24605;&#20013;&#23558;LLMs&#25972;&#21512;&#21040;&#21019;&#24847;&#36807;&#31243;&#20013;&#30340;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#21457;&#29616;&#23558;LLMs&#25972;&#21512;&#21040;Brainwriting&#20013;&#21487;&#20197;&#22686;&#24378;&#26500;&#24605;&#36807;&#31243;&#21450;&#20854;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#25903;&#25345;&#24819;&#27861;&#35780;&#20272;&#30340;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.14978</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#22836;&#33041;&#20889;&#20316;&#65306;&#25506;&#35752;LLMs&#22312;&#22242;&#20307;&#26500;&#24605;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22242;&#20307;&#26500;&#24605;&#20013;&#23558;LLMs&#25972;&#21512;&#21040;&#21019;&#24847;&#36807;&#31243;&#20013;&#30340;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#21457;&#29616;&#23558;LLMs&#25972;&#21512;&#21040;Brainwriting&#20013;&#21487;&#20197;&#22686;&#24378;&#26500;&#24605;&#36807;&#31243;&#21450;&#20854;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#25903;&#25345;&#24819;&#27861;&#35780;&#20272;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#65289;&#26085;&#30410;&#26222;&#21450;&#65292;&#23545;&#21019;&#24847;&#24037;&#20316;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;LLMs&#25972;&#21512;&#21040;&#21019;&#24847;&#36807;&#31243;&#20013;&#30340;&#20004;&#20010;&#26041;&#38754; - &#21019;&#24847;&#29983;&#25104;&#30340;&#20998;&#27495;&#38454;&#27573;&#20197;&#21450;&#35780;&#20272;&#21644;&#36873;&#25321;&#24819;&#27861;&#30340;&#25910;&#25947;&#38454;&#27573;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21327;&#20316;&#30340;&#22242;&#20307;-AI&#22836;&#33041;&#20889;&#20316;&#26500;&#24605;&#26694;&#26550;&#65292;&#23558;LLM&#20316;&#20026;&#19968;&#20010;&#22686;&#24378;&#22240;&#32032;&#34701;&#20837;&#21040;&#22242;&#20307;&#26500;&#24605;&#36807;&#31243;&#20013;&#65292;&#24182;&#35780;&#20272;&#20102;&#21019;&#24847;&#29983;&#25104;&#36807;&#31243;&#21644;&#32467;&#26524;&#35299;&#31354;&#38388;&#12290;&#20026;&#35780;&#20272;&#22312;&#24819;&#27861;&#35780;&#20272;&#36807;&#31243;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#24341;&#25806;&#65292;&#24182;&#23558;&#20854;&#19982;&#19977;&#21517;&#19987;&#23478;&#21644;&#20845;&#21517;&#26032;&#25163;&#35780;&#20272;&#32773;&#20998;&#37197;&#30340;&#24819;&#27861;&#35780;&#32423;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23558;LLMs&#25972;&#21512;&#21040;&#22836;&#33041;&#20889;&#20316;&#20013;&#21487;&#20197;&#22686;&#24378;&#26500;&#24605;&#36807;&#31243;&#21450;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;LLMs&#21487;&#20197;&#25903;&#25345;&#24819;&#27861;&#35780;&#20272;&#30340;&#35777;&#25454;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14978v1 Announce Type: cross  Abstract: The growing availability of generative AI technologies such as large language models (LLMs) has significant implications for creative work. This paper explores twofold aspects of integrating LLMs into the creative process - the divergence stage of idea generation, and the convergence stage of evaluation and selection of ideas. We devised a collaborative group-AI Brainwriting ideation framework, which incorporated an LLM as an enhancement into the group ideation process, and evaluated the idea generation process and the resulted solution space. To assess the potential of using LLMs in the idea evaluation process, we design an evaluation engine and compared it to idea ratings assigned by three expert and six novice evaluators. Our findings suggest that integrating LLM in Brainwriting could enhance both the ideation process and its outcome. We also provide evidence that LLMs can support idea evaluation. We conclude by discussing implicati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#22522;&#30784;&#28508;&#31354;&#38388;&#20869;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#23450;&#24615;&#35299;&#37322;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#24182;&#26174;&#31034;&#20102;&#23578;&#26410;&#35299;&#20915;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14976</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#22522;&#30784;&#28508;&#31354;&#38388;&#20869;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation within Deep Foundation Latent Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14976
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#22522;&#30784;&#28508;&#31354;&#38388;&#20869;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#23450;&#24615;&#35299;&#37322;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#24182;&#26174;&#31034;&#20102;&#23578;&#26410;&#35299;&#20915;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Vision Transformer&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;ViT&#25110;Dino-V2&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#38656;&#25110;&#24456;&#23569;&#24494;&#35843;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#32452;&#21407;&#22411;&#32593;&#32476;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#22522;&#30784;&#27169;&#22411;&#22312;&#26080;&#38656;&#22312;&#28304;&#22495;&#25110;&#30446;&#26631;&#22495;&#36827;&#34892;&#24494;&#35843;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#20197;&#21450;&#23545;&#20915;&#31574;&#36807;&#31243;&#30340;&#23450;&#24615;&#35299;&#37322;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24314;&#35758;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#29616;&#26377;&#22522;&#32447;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#26377;&#24453;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14976v1 Announce Type: cross  Abstract: The vision transformer-based foundation models, such as ViT or Dino-V2, are aimed at solving problems with little or no finetuning of features. Using a setting of prototypical networks, we analyse to what extent such foundation models can solve unsupervised domain adaptation without finetuning over the source or target domain. Through quantitative analysis, as well as qualitative interpretations of decision making, we demonstrate that the suggested method can improve upon existing baselines, as well as showcase the limitations of such approach yet to be solved.
&lt;/p&gt;</description></item><item><title>&#21457;&#23637;&#20102;&#19968;&#20010;&#20351;&#29992;&#31354;&#38388;&#38598;&#21512;&#26694;&#26550;&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#28857;&#30340;&#25490;&#21015;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#21306;&#20998;&#20004;&#20010;&#31867;&#21035;&#65292;&#23545;&#20110;&#32959;&#30244;&#23398;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>https://arxiv.org/abs/2402.14974</link><description>&lt;p&gt;
&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23454;&#29616;&#31354;&#38388;&#36879;&#26126;&#30340;AI&#20998;&#31867;&#65306;MxIF&#32959;&#30244;&#25968;&#25454;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An Application for MxIF Oncology Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14974
&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#20102;&#19968;&#20010;&#20351;&#29992;&#31354;&#38388;&#38598;&#21512;&#26694;&#26550;&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#28857;&#30340;&#25490;&#21015;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#21306;&#20998;&#20004;&#20010;&#31867;&#21035;&#65292;&#23545;&#20110;&#32959;&#30244;&#23398;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26469;&#33258;&#19981;&#21516;&#22320;&#28857;&#31867;&#22411;&#30340;&#22810;&#31867;&#21035;&#28857;&#38598;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#31354;&#38388;&#36879;&#26126;&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#28857;&#30340;&#25490;&#21015;&#21306;&#20998;&#20004;&#20010;&#31867;&#21035;&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#27604;&#22914;&#32959;&#30244;&#23398;&#65292;&#29992;&#20110;&#20998;&#26512;&#20813;&#30123;-&#32959;&#30244;&#20851;&#31995;&#21644;&#35774;&#35745;&#26032;&#30340;&#20813;&#30123;&#27835;&#30103;&#26041;&#27861;&#12290;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#32771;&#34385;&#31354;&#38388;&#21464;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#38656;&#27714;&#12290;&#20197;&#21069;&#25552;&#20986;&#30340;&#25216;&#26415;&#35201;&#27714;&#23494;&#38598;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25110;&#32773;&#22312;&#22788;&#29702;&#21333;&#20010;&#22320;&#28857;&#31867;&#22411;&#20869;&#30340;&#26174;&#33879;&#31354;&#38388;&#21464;&#24322;&#24615;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26041;&#27861;&#27809;&#26377;&#35774;&#35745;&#29992;&#20110;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#28857;&#38598;&#12290;&#29616;&#26377;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;DNN&#26041;&#27861;&#23616;&#38480;&#20110;&#19968;&#20992;&#20999;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31354;&#38388;&#38598;&#21512;&#26694;&#26550;&#65292;&#26126;&#30830;&#20351;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#21152;&#26435;&#36317;&#31163;&#23398;&#20064;&#29575;&#21644;&#31354;&#38388;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14974v1 Announce Type: cross  Abstract: Given multi-category point sets from different place-types, our goal is to develop a spatially-lucid classifier that can distinguish between two classes based on the arrangements of their points. This problem is important for many applications, such as oncology, for analyzing immune-tumor relationships and designing new immunotherapies. It is challenging due to spatial variability and interpretability needs. Previously proposed techniques require dense training data or have limited ability to handle significant spatial variability within a single place-type. Most importantly, these deep neural network (DNN) approaches are not designed to work in non-Euclidean space, particularly point sets. Existing non-Euclidean DNN methods are limited to one-size-fits-all approaches. We explore a spatial ensemble framework that explicitly uses different training strategies, including weighted-distance learning rate and spatial domain adaptation, on v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14973</link><description>&lt;p&gt;
GenCeption&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;LLM
&lt;/p&gt;
&lt;p&gt;
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#24102;&#26631;&#27880;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#38590;&#20197;&#36319;&#19978;MLLM&#35780;&#20272;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenCeption&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#38656;&#27880;&#37322;&#30340;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#20165;&#38656;&#35201;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#21453;&#26144;&#20986;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;DrawCeption&#28216;&#25103;&#65292;GenCeption&#20174;&#19968;&#20010;&#38750;&#25991;&#26412;&#26679;&#26412;&#24320;&#22987;&#65292;&#24182;&#32463;&#21382;&#19968;&#31995;&#21015;&#36845;&#20195;&#30340;&#25551;&#36848;&#21644;&#29983;&#25104;&#27493;&#39588;&#12290;&#36845;&#20195;&#20043;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#20351;&#29992;GC@T&#25351;&#26631;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;GenCeption&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;GenCeption&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#19988;&#20197;&#21069;&#26410;&#35265;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#25193;&#23637;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#30340;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
&lt;/p&gt;</description></item><item><title>MultiLS&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#21019;&#24314;&#22810;&#20219;&#21153;LS&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;MultiLS-PT&#20316;&#20026;&#31532;&#19968;&#20010;&#20351;&#29992;&#35813;&#26694;&#26550;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35789;&#27719;&#31616;&#21270;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14972</link><description>&lt;p&gt;
MultiLS: &#19968;&#20010;&#22810;&#20219;&#21153;&#35789;&#27719;&#31616;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MultiLS: A Multi-task Lexical Simplification Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14972
&lt;/p&gt;
&lt;p&gt;
MultiLS&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#21019;&#24314;&#22810;&#20219;&#21153;LS&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;MultiLS-PT&#20316;&#20026;&#31532;&#19968;&#20010;&#20351;&#29992;&#35813;&#26694;&#26550;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35789;&#27719;&#31616;&#21270;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#31616;&#21270;&#65288;LS&#65289;&#33258;&#21160;&#26367;&#25442;&#38590;&#20197;&#29702;&#35299;&#30340;&#21333;&#35789;&#20026;&#26356;&#26131;&#35835;&#30340;&#26367;&#20195;&#35789;&#65292;&#21516;&#26102;&#20445;&#30041;&#21477;&#23376;&#30340;&#21407;&#22987;&#21547;&#20041;&#12290;LS&#26159;&#25991;&#26412;&#31616;&#21270;&#30340;&#21069;&#36523;&#65292;&#26088;&#22312;&#25913;&#21892;&#25991;&#26412;&#23545;&#21508;&#31181;&#30446;&#26631;&#20154;&#32676;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#21253;&#25324;&#20799;&#31461;&#12289;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#12289;&#38405;&#35835;&#38556;&#30861;&#25110;&#20302;&#35782;&#23383;&#29575;&#30340;&#20154;&#32676;&#12290;&#23384;&#22312;&#19968;&#20123;&#19987;&#38376;&#29992;&#20110;LS&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;LS&#27969;&#31243;&#20013;&#30340;&#19968;&#20010;&#25110;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#24320;&#21457;&#20986;&#19968;&#20010;&#35206;&#30422;&#25152;&#26377;LS&#23376;&#20219;&#21153;&#30340;&#21333;&#20010;LS&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MultiLS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#21019;&#24314;&#22810;&#20219;&#21153;LS&#25968;&#25454;&#38598;&#30340;LS&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;MultiLS-PT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;MultiLS&#26694;&#26550;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#25191;&#34892;&#25152;&#26377;LS&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#65288;1&#65289;&#35789;&#27719;&#22797;&#26434;&#24615;&#39044;&#27979;&#65288;LCP&#65289;&#12289;&#65288;2&#65289;&#26367;&#20195;&#35789;&#29983;&#25104;&#21644;&#65288;3&#65289;&#26367;&#20195;&#35789;&#25490;&#21517;&#65292;&#23637;&#31034;&#20102;MultiLS-PT&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14972v1 Announce Type: cross  Abstract: Lexical Simplification (LS) automatically replaces difficult to read words for easier alternatives while preserving a sentence's original meaning. LS is a precursor to Text Simplification with the aim of improving text accessibility to various target demographics, including children, second language learners, individuals with reading disabilities or low literacy. Several datasets exist for LS. These LS datasets specialize on one or two sub-tasks within the LS pipeline. However, as of this moment, no single LS dataset has been developed that covers all LS sub-tasks. We present MultiLS, the first LS framework that allows for the creation of a multi-task LS dataset. We also present MultiLS-PT, the first dataset to be created using the MultiLS framework. We demonstrate the potential of MultiLS-PT by carrying out all LS sub-tasks of (1). lexical complexity prediction (LCP), (2). substitute generation, and (3). substitute ranking for Portugu
&lt;/p&gt;</description></item><item><title>Mirror &#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#36890;&#36807;&#23548;&#33322;&#32773;&#21644;&#25512;&#29702;&#32773;&#20043;&#38388;&#30340;&#21551;&#21457;&#24335;&#20132;&#20114;&#65292;&#20419;&#36827;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#21487;&#38752;&#24615;&#30340;&#25512;&#29702;&#36712;&#36857;&#21457;&#23637;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20016;&#23500;&#38382;&#39064;&#19978;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.14963</link><description>&lt;p&gt;
&#38236;&#20687;&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#30693;&#35782;&#20016;&#23500;&#25512;&#29702;&#30340;&#22810;&#35270;&#35282;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14963
&lt;/p&gt;
&lt;p&gt;
Mirror &#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#36890;&#36807;&#23548;&#33322;&#32773;&#21644;&#25512;&#29702;&#32773;&#20043;&#38388;&#30340;&#21551;&#21457;&#24335;&#20132;&#20114;&#65292;&#20419;&#36827;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#21487;&#38752;&#24615;&#30340;&#25512;&#29702;&#36712;&#36857;&#21457;&#23637;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20016;&#23500;&#38382;&#39064;&#19978;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#33021;&#21147;&#21453;&#22797;&#21453;&#24605;&#33258;&#24049;&#30340;&#36755;&#20986;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#27809;&#26377;&#22806;&#37096;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#30693;&#35782;&#20016;&#23500;&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#38500;&#20102;LLMs&#22312;&#33258;&#25105;&#35780;&#20272;&#26041;&#38754;&#30340;&#20302;&#25928;&#29575;&#22806;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#23613;&#31649;&#21463;&#21040;&#26126;&#30830;&#36127;&#38754;&#21453;&#39304;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#37325;&#26032;&#23457;&#35270;&#20854;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mirror&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#30693;&#35782;&#20016;&#23500;&#25512;&#29702;&#30340;&#22810;&#35282;&#24230;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#29305;&#23450;&#21453;&#24605;&#36845;&#20195;&#20013;&#21345;&#20303;&#12290;Mirror&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#23548;&#33322;&#32773;&#21644;&#25512;&#29702;&#32773;&#20043;&#38388;&#30340;&#21551;&#21457;&#24335;&#20132;&#20114;&#33719;&#24471;&#22810;&#35270;&#35282;&#32447;&#32034;&#30340;&#21453;&#24605;&#65292;&#24341;&#23548;&#20195;&#29702;&#21521;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#21487;&#38752;&#24615;&#30340;&#25512;&#29702;&#36712;&#36857;&#21457;&#23637;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22320;&#38754;&#30495;&#30456;&#65292;&#36890;&#36807;&#40723;&#21169;&#65288;1&#65289;&#23548;&#33322;&#32773;&#29983;&#25104;&#30340;&#26041;&#21521;&#30340;&#22810;&#26679;&#24615;&#19982;&#65288;2&#65289;&#31574;&#30053;&#24615;&#24341;&#21457;&#30340;&#25200;&#21160;&#22312;&#20135;&#29983;&#30340;&#22238;&#24212;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14963v1 Announce Type: cross  Abstract: While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;2D&#29289;&#20307;&#36793;&#30028;&#26694;&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#39640;&#28165;&#22320;&#22270;&#25968;&#25454;&#21644;&#21608;&#22260;&#25668;&#20687;&#22836;&#25293;&#25668;&#30340;&#22270;&#20687;&#65292;&#22312;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.14933</link><description>&lt;p&gt;
&#22522;&#20110;2D&#29289;&#20307;&#36793;&#30028;&#26694;&#30340;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Path Planning based on 2D Object Bounding-box
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;2D&#29289;&#20307;&#36793;&#30028;&#26694;&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#39640;&#28165;&#22320;&#22270;&#25968;&#25454;&#21644;&#21608;&#22260;&#25668;&#20687;&#22836;&#25293;&#25668;&#30340;&#22270;&#20687;&#65292;&#22312;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#23454;&#26045;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#21457;&#23637;&#20808;&#36827;&#30340;&#24863;&#30693;&#31995;&#32479;&#21644;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#26469;&#31649;&#29702;&#22797;&#26434;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#20013;&#36890;&#36807;&#27169;&#25311;&#23398;&#20064;&#24320;&#21457;&#30340;2D&#29289;&#20307;&#36793;&#30028;&#26694;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#28165;&#22320;&#22270;&#25968;&#25454;&#19982;&#21608;&#22260;&#25668;&#20687;&#22836;&#25293;&#25668;&#30340;&#22270;&#20687;&#30456;&#32467;&#21512;&#12290;&#25509;&#19979;&#26469;&#30340;&#24863;&#30693;&#20219;&#21153;&#28041;&#21450;&#36793;&#30028;&#26694;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#35268;&#21010;&#37096;&#20998;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14933v1 Announce Type: cross  Abstract: The implementation of Autonomous Driving (AD) technologies within urban environments presents significant challenges. These challenges necessitate the development of advanced perception systems and motion planning algorithms capable of managing situations of considerable complexity. Although the end-to-end AD method utilizing LiDAR sensors has achieved significant success in this scenario, we argue that its drawbacks may hinder its practical application. Instead, we propose the vision-centric AD as a promising alternative offering a streamlined model without compromising performance. In this study, we present a path planning method that utilizes 2D bounding boxes of objects, developed through imitation learning in urban driving scenarios. This is achieved by integrating high-definition (HD) map data with images captured by surrounding cameras. Subsequent perception tasks involve bounding-box detection and tracking, while the planning p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#25935;&#24863;&#32676;&#20307;&#23450;&#20041;&#25110;&#39069;&#22806;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#36229;&#21442;&#25968;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20445;&#35777;&#20219;&#20309;&#36275;&#22815;&#22823;&#30340;&#20154;&#32676;&#23376;&#38598;&#33021;&#33719;&#24471;&#33267;&#23569;&#26368;&#20302;&#25928;&#29992;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14929</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#35775;&#38382;&#25935;&#24863;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32852;&#37030;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Federated Fairness without Access to Sensitive Groups
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14929
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#25935;&#24863;&#32676;&#20307;&#23450;&#20041;&#25110;&#39069;&#22806;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#36229;&#21442;&#25968;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20445;&#35777;&#20219;&#20309;&#36275;&#22815;&#22823;&#30340;&#20154;&#32676;&#23376;&#38598;&#33021;&#33719;&#24471;&#33267;&#23569;&#26368;&#20302;&#25928;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#32852;&#37030;&#23398;&#20064;&#20013;&#20851;&#20110;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#37117;&#20551;&#35774;&#22312;&#35757;&#32451;&#26399;&#38388;&#23384;&#22312;&#39044;&#23450;&#20041;&#21644;&#26631;&#35760;&#30340;&#25935;&#24863;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20174;&#26032;&#20852;&#27861;&#35268;&#21040;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#21160;&#24577;&#21644;&#20301;&#32622;&#20381;&#36182;&#24615;&#31561;&#22810;&#31181;&#22240;&#32032;&#65292;&#36825;&#19968;&#20551;&#35774;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#21512;&#36866;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20445;&#35777;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#39044;&#23450;&#20041;&#30340;&#25935;&#24863;&#32676;&#20307;&#30340;&#23450;&#20041;&#25110;&#39069;&#22806;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#20801;&#35768;&#32852;&#37030;&#23398;&#20064;&#23398;&#20064;&#19968;&#20010;&#24085;&#32047;&#25176;&#26377;&#25928;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#30830;&#20445;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#19968;&#20010;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20165;&#21463;&#21040;&#32676;&#20307;&#22823;&#23567;&#32422;&#26463;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#36275;&#22815;&#22823;&#30340;&#20154;&#32676;&#23376;&#38598;&#37117;&#20445;&#35777;&#33021;&#20174;&#27169;&#22411;&#20013;&#33719;&#24471;&#33267;&#23569;&#30340;&#26368;&#20302;&#25928;&#29992;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#28085;&#30422;&#20102;&#29616;&#26377;&#26041;&#27861;&#20316;&#20026;&#29305;&#27530;&#26696;&#20363;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14929v1 Announce Type: cross  Abstract: Current approaches to group fairness in federated learning assume the existence of predefined and labeled sensitive groups during training. However, due to factors ranging from emerging regulations to dynamics and location-dependency of protected groups, this assumption may be unsuitable in many real-world scenarios. In this work, we propose a new approach to guarantee group fairness that does not rely on any predefined definition of sensitive groups or additional labels. Our objective allows the federation to learn a Pareto efficient global model ensuring worst-case group fairness and it enables, via a single hyper-parameter, trade-offs between fairness and utility, subject only to a group size constraint. This implies that any sufficiently large subset of the population is guaranteed to receive at least a minimum level of utility performance from the model. The proposed objective encompasses existing approaches as special cases, such
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#23398;&#20064;&#23567;&#22411;&#33258;&#21160;&#36710;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#23454;&#29616;&#39640;&#36895;&#22278;&#24418;&#23548;&#33322;&#21644;&#33258;&#20027;&#28418;&#31227;&#65292;&#24110;&#21161;&#36710;&#36742;&#23398;&#20064;&#19990;&#30028;&#29366;&#24577;&#24182;&#36991;&#24320;&#38556;&#30861;&#29289;&#12290;</title><link>https://arxiv.org/abs/2402.14928</link><description>&lt;p&gt;
&#23398;&#20064;&#36870;&#36816;&#21160;&#23398;&#20197;&#23454;&#29616;&#33258;&#21160;&#36710;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Learning Inverse Kinodynamics for Autonomous Vehicle Drifting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14928
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#23398;&#20064;&#23567;&#22411;&#33258;&#21160;&#36710;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#23454;&#29616;&#39640;&#36895;&#22278;&#24418;&#23548;&#33322;&#21644;&#33258;&#20027;&#28418;&#31227;&#65292;&#24110;&#21161;&#36710;&#36742;&#23398;&#20064;&#19990;&#30028;&#29366;&#24577;&#24182;&#36991;&#24320;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#23567;&#22411;&#33258;&#21160;&#36710;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#20854;&#23545;&#36816;&#21160;&#35268;&#21010;&#29305;&#21035;&#26159;&#33258;&#20027;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#36816;&#21160;&#35268;&#21010;&#26102;&#65292;&#23384;&#22312;&#35768;&#22810;&#23548;&#33268;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#35745;&#21010;&#30340;&#20869;&#23481;&#36890;&#24120;&#19982;&#23454;&#38469;&#27773;&#36710;&#19978;&#25191;&#34892;&#30340;&#20869;&#23481;&#19981;&#21516;&#12290;&#22522;&#20110;&#24815;&#24615;&#27979;&#37327;&#21644;&#25191;&#34892;&#21629;&#20196;&#23398;&#20064;&#21160;&#21147;&#23398;&#35268;&#21010;&#22120;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#23398;&#20064;&#19990;&#30028;&#29366;&#24577;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558;&#30446;&#20809;&#36716;&#21521;&#28418;&#31227;&#39046;&#22495;&#65307;&#28418;&#31227;&#26159;&#19968;&#31181;&#22797;&#26434;&#30340;&#28436;&#32451;&#65292;&#38656;&#35201;&#36275;&#22815;&#24179;&#28369;&#30340;&#34920;&#38754;&#12289;&#36275;&#22815;&#39640;&#30340;&#36895;&#24230;&#21644;&#36895;&#24230;&#30340;&#24613;&#21095;&#21464;&#21270;&#12290;&#25105;&#20204;&#23581;&#35797;&#23398;&#20064;&#36825;&#20123;&#28418;&#31227;&#28436;&#32451;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#23581;&#35797;&#20943;&#23567;&#36710;&#36742;&#30340;&#20391;&#28369;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#39640;&#36895;&#22278;&#24418;&#33322;&#34892;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#26657;&#27491;&#33258;&#20027;&#39640;&#36895;&#28418;&#31227;&#19978;&#30340;&#38556;&#30861;&#29289;&#26469;&#36991;&#20813;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14928v1 Announce Type: cross  Abstract: In this work, we explore a data-driven learning-based approach to learning the kinodynamic model of a small autonomous vehicle, and observe the effect it has on motion planning, specifically autonomous drifting. When executing a motion plan in the real world, there are numerous causes for error, and what is planned is often not what is executed on the actual car. Learning a kinodynamic planner based off of inertial measurements and executed commands can help us learn the world state. In our case, we look towards the realm of drifting; it is a complex maneuver that requires a smooth enough surface, high enough speed, and a drastic change in velocity. We attempt to learn the kinodynamic model for these drifting maneuvers, and attempt to tighten the slip of the car. Our approach is able to learn a kinodynamic model for high-speed circular navigation, and is able to avoid obstacles on an autonomous drift at high speed by correcting an exec
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#30693;&#35782;&#33976;&#39311;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#27604;&#36739;&#65292;&#21253;&#25324;&#20248;&#21270;&#30340;&#28201;&#24230;&#21644;&#26435;&#37325;&#21442;&#25968;&#30340;&#35843;&#25972;&#65292;&#20197;&#21450;&#25968;&#25454;&#20998;&#21306;KD&#65292;&#25581;&#31034;&#20102;&#26368;&#26377;&#25928;&#30340;&#30693;&#35782;&#33976;&#39311;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.14922</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#30340;&#23454;&#36341;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Practical Insights into Knowledge Distillation for Pre-Trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14922
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#30693;&#35782;&#33976;&#39311;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#27604;&#36739;&#65292;&#21253;&#25324;&#20248;&#21270;&#30340;&#28201;&#24230;&#21644;&#26435;&#37325;&#21442;&#25968;&#30340;&#35843;&#25972;&#65292;&#20197;&#21450;&#25968;&#25454;&#20998;&#21306;KD&#65292;&#25581;&#31034;&#20102;&#26368;&#26377;&#25928;&#30340;&#30693;&#35782;&#33976;&#39311;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23545;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36807;&#31243;&#30340;&#22686;&#24378;&#65292;&#36825;&#26159;&#30693;&#35782;&#20256;&#36755;&#20013;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#24182;&#23545;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#23613;&#31649;&#37319;&#29992;&#20102;&#35768;&#22810;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#65292;&#20294;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#20102;&#35299;&#30693;&#35782;&#33976;&#39311;&#30340;&#24212;&#29992;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#22810;&#31181;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#65292;&#21253;&#25324;&#26631;&#20934;KD&#12289;&#32463;&#36807;&#20248;&#21270;&#28201;&#24230;&#21644;&#26435;&#37325;&#21442;&#25968;&#35843;&#25972;&#30340;KD&#12289;&#28145;&#24230;&#30456;&#20114;&#23398;&#20064;&#20197;&#21450;&#25968;&#25454;&#20998;&#21306;KD&#12290;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#31574;&#30053;&#19979;&#30340;&#34920;&#29616;&#65292;&#20197;&#30830;&#23450;&#27599;&#31181;&#26041;&#27861;&#26368;&#26377;&#25928;&#30340;&#24773;&#22659;&#12290;&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#32467;&#21512;&#24191;&#27867;&#30340;&#32593;&#26684;&#25628;&#32034;&#35780;&#20272;&#26469;&#33719;&#21462;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14922v1 Announce Type: cross  Abstract: This research investigates the enhancement of knowledge distillation (KD) processes in pre-trained models, an emerging field in knowledge transfer with significant implications for distributed training and federated learning environments. These environments benefit from reduced communication demands and accommodate various model architectures. Despite the adoption of numerous KD approaches for transferring knowledge among pre-trained models, a comprehensive understanding of KD's application in these scenarios is lacking. Our study conducts an extensive comparison of multiple KD techniques, including standard KD, tuned KD (via optimized temperature and weight parameters), deep mutual learning, and data partitioning KD. We assess these methods across various data distribution strategies to identify the most effective contexts for each. Through detailed examination of hyperparameter tuning, informed by extensive grid search evaluations, w
&lt;/p&gt;</description></item><item><title>MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14905</link><description>&lt;p&gt;
MobileLLM&#65306;&#20248;&#21270;&#20122;&#21313;&#20159;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#35774;&#22791;&#31471;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14905
&lt;/p&gt;
&lt;p&gt;
MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36843;&#20999;&#38656;&#27714;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20113;&#25104;&#26412;&#21644;&#24310;&#36831;&#38382;&#39064;&#19981;&#26029;&#22686;&#21152;&#25152;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35774;&#35745;&#20855;&#26377;&#19981;&#21040;&#21313;&#20159;&#21442;&#25968;&#30340;&#39030;&#32423;LLMs&#65292;&#36825;&#26159;&#31227;&#21160;&#37096;&#32626;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#24378;&#35843;&#25968;&#25454;&#21644;&#21442;&#25968;&#25968;&#37327;&#22312;&#30830;&#23450;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20122;&#21313;&#20159;&#35268;&#27169;LLMs&#30340;&#27169;&#22411;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#65292;&#20877;&#21152;&#19978;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#32593;&#32476;&#65292;&#31216;&#20026;MobileLLM&#65292;&#20854;&#22312;&#23558;&#36817;125M/350M&#20808;&#36827;&#27169;&#22411;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;&#24778;&#20154;&#30340;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31435;&#21363;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;&#65292;&#19981;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#65292;&#19988;&#20165;&#20855;&#26377;&#26497;&#23567;&#30340;&#24310;&#36831;&#24320;&#38144;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;MobileLLM-L
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14905v1 Announce Type: cross  Abstract: This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.14904</link><description>&lt;p&gt;
&#25968;&#23383;&#27700;&#21360;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#25918;&#23556;&#24615;
&lt;/p&gt;
&lt;p&gt;
Watermarking Makes Language Models Radioactive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#26816;&#27979;&#21040;&#36825;&#31181;&#36755;&#20837;&#34987;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#25104;&#21592;&#25512;&#26029;&#21487;&#20197;&#20197;&#19968;&#23450;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#36825;&#31181;&#26816;&#27979;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#35757;&#32451;&#25968;&#25454;&#30041;&#19979;&#30340;&#30165;&#36857;&#27604;&#25104;&#21592;&#25512;&#26029;&#26356;&#23481;&#26131;&#26816;&#27979;&#19988;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#23558;&#27745;&#26579;&#27700;&#24179;&#19982;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12289;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#27604;&#20363;&#21644;&#24494;&#35843;&#36807;&#31243;&#32852;&#31995;&#36215;&#26469;&#12290;&#29305;&#21035;&#26159;&#25105;&#20204;&#23637;&#31034;&#65292;&#21363;&#20351;&#21482;&#26377;5&#65285;&#30340;&#35757;&#32451;&#25991;&#26412;&#34987;&#25968;&#23383;&#27700;&#21360;&#26631;&#35760;&#65292;&#35757;&#32451;&#22312;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#21512;&#25104;&#25351;&#20196;&#19978;&#20173;&#28982;&#21487;&#20197;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#65288;p&#20540;&lt;1e-5&#65289;&#34987;&#26816;&#27979;&#21040;&#12290;&#22240;&#27492;&#65292;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;LLM&#27700;&#21360;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#30830;&#23450;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;LLM&#30340;&#36755;&#20986;&#26159;&#21542;&#34987;&#29992;&#26469;&#23545;&#21478;&#19968;&#20010;LLM&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14904v1 Announce Type: cross  Abstract: This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the fine-tuning process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (p-value &lt; 1e-5) even when as little as 5% of training text is watermarked. Thus, LLM watermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked LLM were used to fine-tune another LLM.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#24847;&#22270;&#29702;&#35299;&#30340;&#19968;&#20010;&#26032;&#35270;&#35282;&#65292;&#19981;&#20381;&#36182;&#20110;&#20135;&#21697;&#26412;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#20135;&#21697;&#24674;&#22797;&#22522;&#20934;&#39564;&#35777;&#20102;&#24403;&#21069;&#24847;&#22270;&#30693;&#35782;&#22270;&#30340;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.14901</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20013;&#24847;&#22270;&#29702;&#35299;&#30340;&#20351;&#29992;&#20013;&#24515;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Usage-centric Take on Intent Understanding in E-Commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#24847;&#22270;&#29702;&#35299;&#30340;&#19968;&#20010;&#26032;&#35270;&#35282;&#65292;&#19981;&#20381;&#36182;&#20110;&#20135;&#21697;&#26412;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#20135;&#21697;&#24674;&#22797;&#22522;&#20934;&#39564;&#35777;&#20102;&#24403;&#21069;&#24847;&#22270;&#30693;&#35782;&#22270;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#26159;&#30005;&#23376;&#21830;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#24847;&#22270;&#29702;&#35299;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#23450;&#20041;&#24182;&#19981;&#19968;&#33268;&#65292;&#19988;&#32570;&#20047;&#20934;&#30830;&#30340;&#22522;&#20934;&#12290;&#26412;&#25991;&#20851;&#27880;&#23558;&#29992;&#25143;&#24847;&#22270;&#23450;&#20041;&#20026;"&#39038;&#23458;&#22914;&#20309;&#20351;&#29992;&#20135;&#21697;"&#30340;&#39044;&#27979;&#24615;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#23558;&#24847;&#22270;&#29702;&#35299;&#35270;&#20026;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#29420;&#31435;&#20110;&#20135;&#21697;&#26412;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;FolkScope&#30340;&#20004;&#20010;&#24369;&#28857;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#30005;&#23376;&#21830;&#21153;&#24847;&#22270;&#30693;&#35782;&#22270;&#65292;&#38480;&#21046;&#20102;&#20854;&#25512;&#29702;&#29992;&#25143;&#24847;&#22270;&#21644;&#25512;&#33616;&#22810;&#26679;&#26377;&#29992;&#20135;&#21697;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20135;&#21697;&#24674;&#22797;&#22522;&#20934;&#65292;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#20010;&#31034;&#20363;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#19978;&#36848;FolkScope&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14901v1 Announce Type: cross  Abstract: Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its popularity, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predicative user intents as "how a customer uses a product", and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph, that limit its capacity to reason about user intents and to recommend diverse useful products. Following these observations, we introduce a Product Recovery Benchmark including a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this benchmark.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.14899</link><description>&lt;p&gt;
&#20572;&#27490;&#25512;&#29702;&#65281;&#24403;&#22810;&#27169;&#24577;LLMs&#19982;&#20018;&#32852;&#25512;&#29702;&#36935;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;LLMs&#65288;MLLMs&#65289;&#23637;&#31034;&#20102;&#24456;&#24378;&#30340;&#29702;&#35299;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20687;&#20256;&#32479;&#35270;&#35273;&#27169;&#22411;&#19968;&#26679;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;&#30340;&#25915;&#20987;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20018;&#32852;&#25512;&#29702;&#65288;CoT&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#22312;MLLMs&#19978;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#25552;&#20379;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#20851;&#20110;MLLMs&#22312;CoT&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#22312;MLLMs&#29992;&#23545;&#25239;&#24615;&#22270;&#20687;&#25512;&#26029;&#38169;&#35823;&#31572;&#26696;&#26102;&#25512;&#29702;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#37319;&#29992;CoT&#25512;&#29702;&#26102;MLLMs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;CoT&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25269;&#25239;&#20102;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;CoT&#24341;&#36215;&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CoT&#25512;&#29702;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14899v1 Announce Type: cross  Abstract: Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improves model's performance, but also enhances model's explainability by giving intermediate reasoning steps. Nevertheless, there is still a lack of study regarding MLLMs' adversarial robustness with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with adversarial images. Our research evaluates the adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT marginally improves adversarial robustness against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT reasonin
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14897</link><description>&lt;p&gt;
Chain-of-Thought&#19981;&#24544;&#35802;&#20316;&#20026;&#20266;&#35013;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Unfaithfulness as Disguised Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14897
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought (CoT)&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;LLM&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;CoT&#24544;&#23454;&#24230;&#30340;&#20195;&#29702;&#65292;arXiv:2307.13702&#25552;&#20986;&#20102;&#19968;&#20010;&#24230;&#37327;&#27169;&#22411;&#20381;&#36182;&#20854;CoT&#29983;&#25104;&#31572;&#26696;&#30340;&#25351;&#26631;&#12290;&#22312;&#19968;&#20010;&#19987;&#26377;&#27169;&#22411;&#31995;&#21015;&#20013;&#65292;&#20182;&#20204;&#21457;&#29616;LLM&#34920;&#29616;&#20986;&#27169;&#22411;&#22823;&#23567;&#19982;&#20854;&#24544;&#23454;&#24230;&#27979;&#37327;&#20043;&#38388;&#30340;&#32553;&#25918;-&#21453;&#21521;&#32553;&#25918;&#20851;&#31995;&#65292;&#24182;&#19988;130&#20159;&#21442;&#25968;&#27169;&#22411;&#30456;&#27604;&#20110;&#23610;&#23544;&#20171;&#20110;8.1&#20159;&#21040;1750&#20159;&#21442;&#25968;&#20043;&#38388;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#22686;&#21152;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#32467;&#26524;&#26159;&#21542;&#20316;&#20026;&#25152;&#26377;LLM&#30340;&#29305;&#24615;&#27867;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#31995;&#21015;&#30340;&#27169;&#22411;&#22797;&#21046;&#20182;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20182;&#20204;&#25253;&#21578;&#30340;CoT&#24544;&#23454;&#24230;&#30340;&#32553;&#25918;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#25913;&#21464;&#35774;&#23450;&#20250;&#23548;&#33268;&#36825;&#20123;&#27169;&#24335;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22686;&#24378;&#19981;&#36807;&#26159;&#26356;&#22909;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#25552;&#39640;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14895</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#24050;&#27515;&#65292;&#25968;&#25454;&#22686;&#24378;&#19975;&#23681;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation is Dead, Long Live Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14895
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#19981;&#36807;&#26159;&#26356;&#22909;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#20010;&#32321;&#33635;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#19981;&#26029;&#25552;&#20986;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#21019;&#24314;&#20154;&#24037;&#25968;&#25454;&#65292;&#24050;&#32463;&#22312;&#23567;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#29575;&#65292;&#33267;&#23569;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#20123;&#32467;&#26524;&#65292;&#34920;&#26126;&#32463;&#20856;&#30340;&#25968;&#25454;&#22686;&#24378;&#21482;&#26159;&#19968;&#31181;&#26356;&#22909;&#22320;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#24335;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#20043;&#21069;&#33457;&#26356;&#22810;&#26102;&#38388;&#36827;&#34892;&#24494;&#35843;&#20250;&#25269;&#28040;&#20854;&#25928;&#26524;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#65292;&#22240;&#20026;&#23427;&#22238;&#31572;&#20102;&#26368;&#36817;&#20960;&#24180;&#30041;&#19979;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#21363;&#65306;&#21738;&#31181;DA&#25216;&#26415;&#34920;&#29616;&#26368;&#20339;&#65288;&#21482;&#35201;&#23427;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#35757;&#32451;&#38598;&#36275;&#22815;&#25509;&#36817;&#65292;&#19981;&#20250;&#25439;&#23475;&#35757;&#32451;&#65289;&#65292;&#20026;&#20160;&#20040;DA&#34920;&#29616;&#20986;&#31215;&#26497;&#30340;&#32467;&#26524;&#65288;&#31616;&#21270;&#32593;&#32476;&#35757;&#32451;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#35805;&#20195;&#29702;&#65288;&#22914;ChatGPT&#25110;LLama2&#65289;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20174;&#32780;&#24471;&#20986;&#20102;&#32467;&#35770;&#65292;&#27492;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14895v1 Announce Type: cross  Abstract: Textual data augmentation (DA) is a prolific field of study where novel techniques to create artificial data are regularly proposed, and that has demonstrated great efficiency on small data settings, at least for text classification tasks. In this paper, we challenge those results, showing that classical data augmentation is simply a way of performing better fine-tuning, and that spending more time fine-tuning before applying data augmentation negates its effect. This is a significant contribution as it answers several questions that were left open in recent years, namely~: which DA technique performs best (all of them as long as they generate data close enough to the training set as to not impair training) and why did DA show positive results (facilitates training of network). We furthermore show that zero and few-shot data generation via conversational agents such as ChatGPT or LLama2 can increase performances, concluding that this f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25509;&#22320;&#25925;&#38556;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#22788;&#29702;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#37197;&#30005;&#31995;&#32479;&#20013;&#25925;&#38556;&#30340;&#20934;&#30830;&#39044;&#27979;</title><link>https://arxiv.org/abs/2402.14894</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#37197;&#30005;&#32593;&#20998;&#24067;&#24335;&#21457;&#30005;&#25509;&#22320;&#25925;&#38556;&#23450;&#20301;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Ground-Fault Location Method in Distribution Power System With Distributed Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14894
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25509;&#22320;&#25925;&#38556;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#22788;&#29702;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#37197;&#30005;&#31995;&#32479;&#20013;&#25925;&#38556;&#30340;&#20934;&#30830;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#37197;&#30005;&#32423;&#21035;&#30340;&#22686;&#21152;&#24341;&#20837;&#20102;&#22810;&#26041;&#21521;&#21151;&#29575;&#27969;&#65292;&#20351;&#24471;&#36807;&#26102;&#30340;&#20256;&#32479;&#25925;&#38556;&#23450;&#20301;&#25216;&#26415;&#38590;&#20197;&#36866;&#29992;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#20197;&#30830;&#20445;&#24555;&#36895;&#20934;&#30830;&#30340;&#25925;&#38556;&#23450;&#20301;&#65292;&#20174;&#32780;&#22686;&#24378;&#30005;&#21147;&#31995;&#32479;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37197;&#30005;&#31995;&#32479;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25509;&#22320;&#25925;&#38556;&#23450;&#20301;&#26041;&#27861;&#12290;&#22312;Matlab/Simulink&#20013;&#24314;&#27169;&#20102;&#19968;&#20010;11&#33410;&#28857; 20 kV&#30340;&#30005;&#21147;&#31995;&#32479;&#65292;&#29992;&#20110;&#27169;&#25311;&#25509;&#22320;&#25925;&#38556;&#12290;&#22312;&#19981;&#21516;&#20301;&#32622;&#21644;&#19981;&#21516;&#31995;&#32479;&#36816;&#34892;&#29366;&#24577;&#19979;&#20135;&#29983;&#20102;&#25925;&#38556;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#20998;&#26512;&#31995;&#32479;&#21464;&#30005;&#31449;&#30340;&#26102;&#22495;&#25925;&#38556;&#19977;&#30456;&#30005;&#21387;&#12290;&#26368;&#32456;&#21033;&#29992;&#22788;&#29702;&#21518;&#30340;&#25968;&#25454;&#30340;&#32479;&#35745;&#37327;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANN)&#26469;&#25214;&#21040;&#35745;&#31639;&#30005;&#21387;&#29305;&#24449;&#21644;&#25925;&#38556;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19977;&#20010;ANNs&#21487;&#20197;&#39044;&#27979;&#25925;&#38556;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14894v1 Announce Type: cross  Abstract: The recent increase in renewable energy penetration at the distribution level introduces a multi-directional power flow that outdated traditional fault location techniques. To this extent, the development of new methods is needed to ensure fast and accurate fault localization and, hence, strengthen power system reliability. This paper proposes a data-driven ground fault location method for the power distribution system. An 11-bus 20 kV power system is modeled in Matlab/Simulink to simulate ground faults. The faults are generated at different locations and under various system operational states. Time-domain faulted three-phase voltages at the system substation are then analyzed with discrete wavelet transform. Statistical quantities of the processed data are eventually used to train an Artificial Neural Network (ANN) to find a mapping between computed voltage features and faults. Specifically, three ANNs allow the prediction of faulted
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14891</link><description>&lt;p&gt;
LLMBind: &#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMBind: A Unified Modality-Task Integration Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#38598;&#25104;&#33021;&#21147;&#26377;&#38480;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24102;&#22836;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;LLMBind&#21487;&#20197;&#20197;&#22810;&#31181;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#35299;&#37322;&#36755;&#20837;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#30340;&#21327;&#20316;&#23454;&#29616;&#19981;&#21516;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26377;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;40&#19975;&#26465;&#25351;&#20196;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35299;&#38145;&#20102;&#20132;&#20114;&#24335;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14890</link><description>&lt;p&gt;
Vygotsky Distance: &#29992;&#20110;&#22522;&#20934;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vygotsky Distance: Measure for Benchmark Task Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14890
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#24037;&#20855;&#21644;&#23454;&#36341;&#31639;&#27861;&#26469;&#35745;&#31639;&#22522;&#20934;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#31216;&#20043;&#20026;"&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;"&#12290;&#36825;&#31181;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#8220;&#23398;&#29983;&#8221;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#30340;&#30456;&#23545;&#34920;&#29616;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#20219;&#21153;&#26412;&#36523;&#30340;&#23646;&#24615;&#12290;&#22914;&#26524;&#20004;&#20010;&#20219;&#21153;&#22312;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#19978;&#24444;&#27492;&#25509;&#36817;&#65292;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978; tend to have similar relative performance&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20102;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14890v1 Announce Type: cross  Abstract: Evaluation plays a significant role in modern natural language processing. Most modern NLP benchmarks consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, we call this similarity measure "Vygotsky distance". The core idea of this similarity measure is that it is based on relative performance of the "students" on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on v
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.14889</link><description>&lt;p&gt;
COBIAS&#65306;&#20559;&#35265;&#35780;&#20272;&#20013;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
COBIAS: Contextual Reliability in Bias Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14889
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;&#22266;&#26377;&#20559;&#35265;&#25968;&#25454;&#35757;&#32451;&#30340;&#12290;&#20197;&#24448;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#30740;&#31350;&#20381;&#36182;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#23545;&#20559;&#35265;&#30340;&#26497;&#20854;&#20027;&#35266;&#29702;&#35299;&#32780;&#23384;&#22312;&#22810;&#20010;&#32570;&#38519;&#65292;&#20984;&#26174;&#20986;&#23545;&#24773;&#22659;&#25506;&#32034;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#36755;&#20837;&#29992;&#25143;&#20869;&#23481;&#30340;&#24773;&#22659;&#65292;&#32771;&#34385;&#21040;&#36755;&#20837;&#35821;&#21477;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#20801;&#35768;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#30340;&#26694;&#26550;&#65292;&#32780;&#19981;&#26159;&#20260;&#23475;&#29992;&#25143;&#21442;&#19982;&#30340;&#38450;&#25252;&#35774;&#26045;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;(i) &#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;2287&#20010;&#38472;&#35789;&#28389;&#35843;&#35821;&#21477;&#20197;&#21450;&#28155;&#21152;&#24773;&#22659;&#35201;&#28857;&#30340;&#25968;&#25454;&#38598;&#65307;(ii) &#25105;&#20204;&#24320;&#21457;&#20102;&#38754;&#21521;&#24773;&#22659;&#30340;&#20559;&#35265;&#25351;&#26631;&#21644;&#35780;&#20272;&#20998;&#25968;&#65288;COBIAS&#65289;&#26469;&#35780;&#20272;&#35821;&#21477;&#22312;&#34913;&#37327;&#20559;&#35265;&#26041;&#38754;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26159;&#34913;&#37327;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#24773;&#22659;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#39044;&#27979;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14889v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are trained on inherently biased data. Previous works on debiasing models rely on benchmark datasets to measure model performance. However, these datasets suffer from several pitfalls due to the extremely subjective understanding of bias, highlighting a critical need for contextual exploration. We propose understanding the context of user inputs with consideration of the diverse situations in which input statements are possible. This approach would allow for frameworks that foster bias awareness rather than guardrails that hurt user engagement. Our contribution is twofold: (i) we create a dataset of 2287 stereotyped statements augmented with points for adding context; (ii) we develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to assess statements' contextual reliability in measuring bias. Our metric is a significant predictor of the contextual reliability of bias-benchmark datasets ($
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#22270;&#32467;&#26500;&#30340;&#39640;&#25928;&#25968;&#25454;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#22312;&#19981;&#32463;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#25110;&#20854;&#20182;&#23494;&#38598;&#30340;&#39044;&#22788;&#29702;&#36716;&#25442;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.14888</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#22270;&#32467;&#26500;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#36873;&#25321;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient data selection employing Semantic Similarity-based Graph Structures for model training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14888
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#22270;&#32467;&#26500;&#30340;&#39640;&#25928;&#25968;&#25454;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#22312;&#19981;&#32463;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#25110;&#20854;&#20182;&#23494;&#38598;&#30340;&#39044;&#22788;&#29702;&#36716;&#25442;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#20984;&#26174;&#20102;&#27169;&#22411;&#20934;&#30830;&#25429;&#25417;&#25991;&#26412;&#20449;&#24687;&#25152;&#38656;&#22823;&#37327;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#35757;&#32451;&#27492;&#31867;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;SeSaME&#8221;&#30340;&#25968;&#25454;&#36873;&#25321;&#26426;&#21046;&#65292;&#23427;&#20165;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#39640;&#25928;&#30340;&#25968;&#25454;&#37319;&#26679;&#65292;&#26080;&#38656;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#25110;&#20854;&#20182;&#23494;&#38598;&#30340;&#39044;&#22788;&#29702;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14888v1 Announce Type: cross  Abstract: Recent developments in natural language processing (NLP) have highlighted the need for substantial amounts of data for models to capture textual information accurately. This raises concerns regarding the computational resources and time required for training such models. This paper introduces Semantics for data SAliency in Model performance Estimation (SeSaME). It is an efficient data sampling mechanism solely based on textual information without passing the data through a compute-heavy model or other intensive pre-processing transformations. The application of this approach is demonstrated in the use case of low-resource automated speech recognition (ASR) models, which excessively rely on text-to-speech (TTS) calls when using augmented data. SeSaME learns to categorize new incoming data points into speech recognition difficulty buckets by employing semantic similarity-based graph structures and discrete ASR information from homophilou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#28783;&#24490;&#29615;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#33021;&#26174;&#33879;&#20943;&#23569;&#32039;&#24613;&#20572;&#36710;&#27425;&#25968;&#65292;&#38477;&#20302;&#20132;&#36890;&#25317;&#22581;&#65292;&#25913;&#21892;&#20132;&#36890;&#27969;&#12290;</title><link>https://arxiv.org/abs/2402.14886</link><description>&lt;p&gt;
&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#28783;&#24490;&#29615;
&lt;/p&gt;
&lt;p&gt;
Applying Reinforcement Learning to Optimize Traffic Light Cycles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14886
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#28783;&#24490;&#29615;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#33021;&#26174;&#33879;&#20943;&#23569;&#32039;&#24613;&#20572;&#36710;&#27425;&#25968;&#65292;&#38477;&#20302;&#20132;&#36890;&#25317;&#22581;&#65292;&#25913;&#21892;&#20132;&#36890;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#28783;&#24490;&#29615;&#30340;&#25163;&#21160;&#20248;&#21270;&#26159;&#19968;&#39033;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#26102;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#28783;&#24490;&#29615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#22478;&#24066;&#31227;&#21160;&#27169;&#25311;&#22120;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;Q&#32593;&#32476;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;&#32039;&#24613;&#20572;&#36710;&#27425;&#25968;&#20943;&#23569;&#20102;44.16%&#65292;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12289;&#25913;&#21892;&#20132;&#36890;&#27969;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#36884;&#24452;&#21644;&#23545;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14886v1 Announce Type: cross  Abstract: Manual optimization of traffic light cycles is a complex and time-consuming task, necessitating the development of automated solutions. In this paper, we propose the application of reinforcement learning to optimize traffic light cycles in real-time. We present a case study using the Simulation Urban Mobility simulator to train a Deep Q-Network algorithm. The experimental results showed 44.16% decrease in the average number of Emergency stops, showing the potential of our approach to reduce traffic congestion and improve traffic flow. Furthermore, we discuss avenues for future research and enhancements to the reinforcement learning model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#24182;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;LLM&#24494;&#35843;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#12290;</title><link>https://arxiv.org/abs/2402.14883</link><description>&lt;p&gt;
&#21452;I&#27700;&#21360;&#65306;&#20445;&#25252;LLM&#24494;&#35843;&#27169;&#22411;&#29256;&#26435;
&lt;/p&gt;
&lt;p&gt;
Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14883
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#24182;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;LLM&#24494;&#35843;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#65292;&#19994;&#20027;&#32463;&#24120;&#36890;&#36807;LLM&#25152;&#26377;&#32773;&#25110;&#20113;&#26381;&#21153;&#22120;&#25552;&#20379;&#30340;API&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#21462;&#23450;&#21046;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#23384;&#22312;&#30528;&#27169;&#22411;&#34987;&#28389;&#29992;&#30340;&#39118;&#38505;&#65292;&#21487;&#33021;&#20250;&#32473;&#19994;&#20027;&#24102;&#26469;&#20005;&#37325;&#30340;&#32463;&#27982;&#21518;&#26524;&#12290;&#22240;&#27492;&#65292;&#22312;LLM&#24494;&#35843;&#36807;&#31243;&#20013;&#20445;&#25252;&#36825;&#20123;&#23450;&#21046;&#27169;&#22411;&#30340;&#29256;&#26435;&#24050;&#25104;&#20026;&#32039;&#36843;&#30340;&#23454;&#38469;&#38656;&#27714;&#65292;&#20294;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#32039;&#36843;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21452;I&#27700;&#21360;&#8221;&#30340;&#26032;&#22411;&#27700;&#21360;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#22522;&#20110;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;backdoor&#25968;&#25454;&#33539;&#20363;&#65292;&#20998;&#21035;&#22312;&#25351;&#20196;&#21644;&#36755;&#20837;&#20013;&#35302;&#21457;&#12290;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#23398;&#20064;&#33021;&#21147;&#23558;&#23450;&#21046;&#30340;&#21518;&#38376;&#26679;&#26412;&#32435;&#20837;&#25968;&#25454;&#38598;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#27880;&#20837;&#20102;&#29305;&#23450;&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14883v1 Announce Type: cross  Abstract: To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named "Double-I watermark". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#28385;&#36275;&#36816;&#21160;&#23398;&#21644;&#20934;&#38745;&#24577;&#35201;&#27714;&#30340;&#22810;&#36830;&#26438;&#22235;&#36830;&#26438;&#26426;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.14882</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28385;&#36275;&#30446;&#26631;&#26465;&#20214;&#30340;&#22235;&#36830;&#26438;&#26426;&#26500;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms with Target Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#28385;&#36275;&#36816;&#21160;&#23398;&#21644;&#20934;&#38745;&#24577;&#35201;&#27714;&#30340;&#22810;&#36830;&#26438;&#22235;&#36830;&#26438;&#26426;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26500;&#26159;&#21508;&#31181;&#26426;&#26800;&#31995;&#32479;&#20013;&#35774;&#35745;&#29992;&#20110;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#28385;&#36275;&#29305;&#23450;&#36816;&#21160;&#23398;&#25110;&#20934;&#38745;&#24577;&#35201;&#27714;&#30340;&#26426;&#26500;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#28385;&#36275;&#36816;&#21160;&#23398;&#21644;&#20934;&#38745;&#24577;&#35201;&#27714;&#30340;&#22810;&#36830;&#26438;&#22235;&#36830;&#26438;&#26426;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22522;&#20110;&#26377;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(cGAN)&#65292;&#24182;&#32463;&#36807;&#38024;&#23545;&#26426;&#26500;&#21512;&#25104;&#30340;&#20462;&#25913;&#65292;&#20854;&#35757;&#32451;&#30446;&#30340;&#26159;&#23398;&#20064;&#26426;&#26500;&#30340;&#35201;&#27714;&#19982;&#36830;&#26438;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#21512;&#25104;&#28385;&#36275;&#35201;&#27714;&#30340;&#22235;&#36830;&#26438;&#26426;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14882v1 Announce Type: cross  Abstract: Mechanisms are essential components designed to perform specific tasks in various mechanical systems. However, designing a mechanism that satisfies certain kinematic or quasi-static requirements is a challenging task. The kinematic requirements may include the workspace of a mechanism, while the quasi-static requirements of a mechanism may include its torque transmission, which refers to the ability of the mechanism to transfer power and torque effectively. In this paper, we propose a deep learning-based generative model for generating multiple crank-rocker four-bar linkage mechanisms that satisfy both the kinematic and quasi-static requirements aforementioned. The proposed model is based on a conditional generative adversarial network (cGAN) with modifications for mechanism synthesis, which is trained to learn the relationship between the requirements of a mechanism with respect to linkage lengths. The results demonstrate that the pro
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#20316;&#24330;&#23545;&#27979;&#35797;&#39064;&#30340;&#28431;&#27934;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#36776;&#21035;&#27979;&#35797;&#39064;&#20013;&#23545;ChatGPT&#26368;&#23481;&#26131;&#22238;&#31572;&#38169;&#35823;&#30340;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14881</link><description>&lt;p&gt;
&#23545;&#25239;&#22522;&#20110;ChatGPT&#20316;&#24330;&#30340;&#27979;&#35797;&#39064;&#28431;&#27934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Vulnerability of Test Questions against ChatGPT-based Cheating
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14881
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#20316;&#24330;&#23545;&#27979;&#35797;&#39064;&#30340;&#28431;&#27934;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#36776;&#21035;&#27979;&#35797;&#39064;&#20013;&#23545;ChatGPT&#26368;&#23481;&#26131;&#22238;&#31572;&#38169;&#35823;&#30340;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21487;&#20197;&#30456;&#24403;&#20934;&#30830;&#22320;&#22238;&#31572;&#25991;&#26412;&#25552;&#31034;&#65292;&#29978;&#33267;&#22312;&#30740;&#31350;&#29983;&#32423;&#21035;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#35768;&#22810;&#25945;&#32946;&#24037;&#20316;&#32773;&#21457;&#29616;&#20182;&#20204;&#30340;&#35838;&#19994;&#25110;&#36828;&#31243;&#27979;&#35797;&#21644;&#32771;&#35797;&#23481;&#26131;&#21463;&#21040;&#22522;&#20110;ChatGPT&#30340;&#20316;&#24330;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#23398;&#29983;&#21487;&#33021;&#30452;&#25509;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#25552;&#20379;&#30340;&#31572;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#22238;&#31572;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;ChatGPT&#33021;&#22810;&#22909;&#22238;&#31572;&#27979;&#35797;&#39064;&#65292;&#20197;&#21450;&#25105;&#20204;&#22914;&#20309;&#26816;&#27979;&#27979;&#35797;&#39064;&#26159;&#21542;&#33021;&#34987;ChatGPT&#27491;&#30830;&#22238;&#31572;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;ChatGPT&#23545;MedMCQA&#25968;&#25454;&#38598;&#30340;&#21709;&#24212;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;10,000&#20010;&#21307;&#23398;&#38498;&#20837;&#23398;&#32771;&#35797;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#22238;&#31572;&#65292;&#24182;&#25581;&#31034;&#20102;ChatGPT&#22312;&#26576;&#20123;&#38382;&#39064;&#19978;&#30340;&#22238;&#31572;&#27604;&#20854;&#20182;&#38382;&#39064;&#26356;&#19981;&#20934;&#30830;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19968;&#32452;&#38382;&#39064;&#25110;&#26679;&#26412;&#32771;&#35797;&#20013;&#31579;&#36873;&#20986;&#23545;ChatGPT&#26368;&#26131;&#21463;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#21487;&#20197;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14881v1 Announce Type: cross  Abstract: ChatGPT is a chatbot that can answer text prompts fairly accurately, even performing very well on postgraduate-level questions. Many educators have found that their take-home or remote tests and exams are vulnerable to ChatGPT-based cheating because students may directly use answers provided by tools like ChatGPT. In this paper, we try to provide an answer to an important question: how well ChatGPT can answer test questions and how we can detect whether the questions of a test can be answered correctly by ChatGPT. We generated ChatGPT's responses to the MedMCQA dataset, which contains over 10,000 medical school entrance exam questions. We analyzed the responses and uncovered certain types of questions ChatGPT answers more inaccurately than others. In addition, we have created a basic natural language processing model to single out the most vulnerable questions to ChatGPT in a collection of questions or a sample exam. Our tool can be us
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#30452;&#26041;&#22270;&#21487;&#35270;&#21270;&#24037;&#20855;AutoHistograms&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#12289;&#20197;&#30452;&#26041;&#22270;&#24418;&#24335;&#23637;&#31034;&#24182;&#20801;&#35768;&#29992;&#25143;&#20132;&#20114;&#24335;&#22320;&#26597;&#35810;&#25968;&#25454;&#38598;&#65292;&#24110;&#21161;&#25968;&#25454;&#24037;&#20316;&#32773;&#24555;&#36895;&#25506;&#32034;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.14880</link><description>&lt;p&gt;
&#33258;&#21160;&#30452;&#26041;&#22270;&#65306;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#25968;&#25454;&#38598;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Automatic Histograms: Leveraging Language Models for Text Dataset Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#30452;&#26041;&#22270;&#21487;&#35270;&#21270;&#24037;&#20855;AutoHistograms&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#12289;&#20197;&#30452;&#26041;&#22270;&#24418;&#24335;&#23637;&#31034;&#24182;&#20801;&#35768;&#29992;&#25143;&#20132;&#20114;&#24335;&#22320;&#26597;&#35810;&#25968;&#25454;&#38598;&#65292;&#24110;&#21161;&#25968;&#25454;&#24037;&#20316;&#32773;&#24555;&#36895;&#25506;&#32034;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#38598;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25968;&#25454;&#24037;&#20316;&#20154;&#21592;&#24120;&#24120;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#25688;&#35201;&#65292;&#29305;&#21035;&#26159;&#21508;&#31181;&#27966;&#29983;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;&#19968;&#20123;&#29305;&#24449;&#65292;&#22914;&#27602;&#24615;&#25110;&#20027;&#39064;&#65292;&#23545;&#35768;&#22810;&#25968;&#25454;&#38598;&#37117;&#26377;&#24433;&#21709;&#65292;&#20294;&#35768;&#22810;&#26377;&#36259;&#30340;&#29305;&#24449;&#26159;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#65306;&#38899;&#20048;&#25968;&#25454;&#38598;&#30340;&#20048;&#22120;&#21644;&#27969;&#27966;&#65292;&#25110;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#30142;&#30149;&#21644;&#30151;&#29366;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#24037;&#20316;&#32773;&#32463;&#24120;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#36816;&#34892;&#33258;&#23450;&#20041;&#20998;&#26512;&#65292;&#36825;&#26082;&#32321;&#29712;&#21448;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AutoHistograms&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;AutoHistograms&#33258;&#21160;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#65292;&#29992;&#30452;&#26041;&#22270;&#24418;&#24335;&#23637;&#31034;&#23427;&#20204;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#20132;&#20114;&#24335;&#22320;&#26597;&#35810;&#25968;&#25454;&#38598;&#30340;&#23454;&#20307;&#31867;&#21035;&#24182;&#21019;&#24314;&#26032;&#30340;&#30452;&#26041;&#22270;&#12290;&#22312;&#19982;10&#21517;&#25968;&#25454;&#24037;&#20316;&#32773;&#65288;n=10&#65289;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21442;&#19982;&#32773;&#21487;&#20197;&#24555;&#36895;&#21033;&#29992;AutoHistograms&#35782;&#21035;&#35265;&#35299;&#24182;&#25506;&#32034;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14880v1 Announce Type: cross  Abstract: Making sense of unstructured text datasets is perennially difficult, yet increasingly relevant with Large Language Models. Data workers often rely on dataset summaries, especially distributions of various derived features. Some features, like toxicity or topics, are relevant to many datasets, but many interesting features are domain specific: instruments and genres for a music dataset, or diseases and symptoms for a medical dataset. Accordingly, data workers often run custom analyses for each dataset, which is cumbersome and difficult. We present AutoHistograms, a visualization tool leveragingLLMs. AutoHistograms automatically identifies relevant features, visualizes them with histograms, and allows the user to interactively query the dataset for categories of entities and create new histograms. In a user study with 10 data workers (n=10), we observe that participants can quickly identify insights and explore the data using AutoHistogr
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#20540;&#65292;&#22312;&#35270;&#39057;&#28216;&#25103;&#35282;&#33394;&#24320;&#21457;&#20013;&#20195;&#34920;&#32473;&#23450;&#30340;&#20154;&#26684;&#29305;&#24449;&#65292;&#22686;&#24378;&#28216;&#25103;&#35282;&#33394;&#30340;&#31867;&#20154;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14879</link><description>&lt;p&gt;
&#20197;&#20154;&#26684;&#39537;&#21160;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Driving Generative Agents With Their Personality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14879
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#20540;&#65292;&#22312;&#35270;&#39057;&#28216;&#25103;&#35282;&#33394;&#24320;&#21457;&#20013;&#20195;&#34920;&#32473;&#23450;&#30340;&#20154;&#26684;&#29305;&#24449;&#65292;&#22686;&#24378;&#28216;&#25103;&#35282;&#33394;&#30340;&#31867;&#20154;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#20540;&#65292;&#29305;&#21035;&#26159;&#20154;&#26684;&#20449;&#24687;&#65292;&#22312;&#35270;&#39057;&#28216;&#25103;&#35282;&#33394;&#24320;&#21457;&#32972;&#26223;&#19979;&#30340;&#28508;&#21147;&#12290;&#24773;&#24863;&#35745;&#31639;&#65288;AC&#65289;&#31995;&#32479;&#37327;&#21270;&#20102;&#38750;&#29609;&#23478;&#35282;&#33394;&#65288;NPC&#65289;&#30340;&#24515;&#29702;&#65292;LLM&#21487;&#20197;&#21033;&#29992;&#35813;&#31995;&#32479;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#20540;&#36827;&#34892;&#25552;&#31034;&#29983;&#25104;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#21487;&#20197;&#22987;&#32456;&#20195;&#34920;&#32473;&#23450;&#30340;&#20154;&#26684;&#29305;&#24449;&#65292;&#20174;&#32780;&#22686;&#24378;&#28216;&#25103;&#35282;&#33394;&#30340;&#31867;&#20154;&#29305;&#24615;&#12290;&#23558;&#20154;&#31867;&#26816;&#26597;&#37325;&#26032;&#29992;&#20110;&#35780;&#20272;LLM&#30340;&#22269;&#38469;&#20154;&#26684;&#39033;&#30446;&#27744;&#65288;IPIP&#65289;&#38382;&#21367;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#19982;&#25152;&#25552;&#20379;&#20154;&#26684;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#30340;&#25913;&#36827;&#65292;&#22914;&#26368;&#26032;&#30340;GPT-4&#27169;&#22411;&#65292;&#21487;&#20197;&#22987;&#32456;&#21033;&#29992;&#21644;&#35299;&#37322;&#20154;&#26684;&#20197;&#20195;&#34920;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14879v1 Announce Type: cross  Abstract: This research explores the potential of Large Language Models (LLMs) to utilize psychometric values, specifically personality information, within the context of video game character development. Affective Computing (AC) systems quantify a Non-Player character's (NPC) psyche, and an LLM can take advantage of the system's information by using the values for prompt generation. The research shows an LLM can consistently represent a given personality profile, thereby enhancing the human-like characteristics of game characters. Repurposing a human examination, the International Personality Item Pool (IPIP) questionnaire, to evaluate an LLM shows that the model can accurately generate content concerning the personality provided. Results show that the improvement of LLM, such as the latest GPT-4 model, can consistently utilize and interpret a personality to represent behavior.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.14878</link><description>&lt;p&gt;
&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#30340;&#33021;&#25928;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Energy-efficiency Limits on Training AI Systems using Learning-in-Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 &#20844;&#21578;&#31867;&#22411;: cross &#25688;&#35201;: &#20869;&#23384;&#20013;&#23398;&#20064;&#65288;LIM&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#33539;Paradigm&#65292;&#26088;&#22312;&#20811;&#26381;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#20869;&#23384;&#29942;&#39048;&#12290;&#34429;&#28982;&#35745;&#31639;&#20110;&#20869;&#23384;&#65288;CIM&#65289;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25152;&#35859;&#30340;&#20869;&#23384;&#22681;&#38382;&#39064;&#65288;&#21363;&#30001;&#20110;&#37325;&#22797;&#20869;&#23384;&#35835;&#21462;&#35775;&#38382;&#32780;&#28040;&#32791;&#30340;&#33021;&#37327;&#65289;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#20197;&#35757;&#32451;&#25152;&#38656;&#30340;&#31934;&#24230;&#37325;&#22797;&#20869;&#23384;&#20889;&#20837;&#26102;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#26356;&#26032;&#22681;&#65289;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#32771;&#34385;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#20043;&#38388;&#20256;&#36755;&#20449;&#24687;&#26102;&#25152;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#25972;&#21512;&#22681;&#65289;&#12290;LIM&#33539;&#24335;&#25552;&#20986;&#65292;&#22914;&#26524;&#29289;&#29702;&#20869;&#23384;&#30340;&#33021;&#37327;&#23631;&#38556;&#34987;&#33258;&#36866;&#24212;&#35843;&#21046;&#65292;&#20351;&#24471;&#23384;&#20648;&#22120;&#26356;&#26032;&#21644;&#25972;&#21512;&#30340;&#21160;&#24577;&#19982;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;AI&#27169;&#22411;&#30340;Lyapunov&#21160;&#24577;&#30456;&#21305;&#37197;&#65292;&#37027;&#20040;&#36825;&#20123;&#29942;&#39048;&#20063;&#21487;&#20197;&#34987;&#20811;&#26381;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;&#29992;&#19981;&#21516;LIM&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#32791;&#30340;&#26032;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 Announce Type: cross  Abstract: Learning-in-memory (LIM) is a recently proposed paradigm to overcome fundamental memory bottlenecks in training machine learning systems. While compute-in-memory (CIM) approaches can address the so-called memory-wall (i.e. energy dissipated due to repeated memory read access) they are agnostic to the energy dissipated due to repeated memory writes at the precision required for training (the update-wall), and they don't account for the energy dissipated when transferring information between short-term and long-term memories (the consolidation-wall). The LIM paradigm proposes that these bottlenecks, too, can be overcome if the energy barrier of physical memories is adaptively modulated such that the dynamics of memory updates and consolidation match the Lyapunov dynamics of gradient-descent training of an AI model. In this paper, we derive new theoretical lower bounds on energy dissipation when training AI systems using different LIM app
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.14875</link><description>&lt;p&gt;
&#21517;&#23383;&#30340;&#21547;&#20041;&#26159;&#20160;&#20040;&#65311;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
What's in a Name? Auditing Large Language Models for Race and Gender Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14875
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#23457;&#35745;&#35774;&#35745;&#26469;&#35843;&#26597;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#21253;&#25324;GPT-4&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#21457;&#27169;&#22411;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#20026;&#20010;&#20154;&#25552;&#20379;&#24314;&#35758;&#65292;&#27604;&#22914;&#22312;&#36141;&#36710;&#35848;&#21028;&#25110;&#36873;&#20030;&#32467;&#26524;&#39044;&#27979;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#24314;&#35758;&#31995;&#32479;&#24615;&#22320;&#23545;&#19982;&#31181;&#26063;&#23569;&#25968;&#32676;&#20307;&#21644;&#22899;&#24615;&#24120;&#35265;&#30456;&#20851;&#30340;&#21517;&#23383;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#24471;&#21040;&#30340;&#32467;&#26524;&#26368;&#19981;&#21033;&#12290;&#36825;&#20123;&#20559;&#35265;&#22312;42&#20010;&#25552;&#31034;&#27169;&#26495;&#21644;&#22810;&#20010;&#27169;&#22411;&#20013;&#37117;&#26159;&#19968;&#33268;&#30340;&#65292;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#24615;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#20107;&#20214;&#12290;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#25968;&#20540;&#12289;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#38170;&#28857;&#21487;&#20197;&#25104;&#21151;&#25269;&#28040;&#20559;&#35265;&#65292;&#32780;&#23450;&#24615;&#32454;&#33410;&#30340;&#24433;&#21709;&#24182;&#19981;&#19968;&#33268;&#65292;&#29978;&#33267;&#21487;&#33021;&#20250;&#21152;&#21095;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#36827;&#34892;&#23457;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20943;&#36731;&#20854;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14875v1 Announce Type: cross  Abstract: We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4. In our study, we elicit prompt the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 prompt templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the prompt can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14874</link><description>&lt;p&gt;
&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65306;&#21033;&#29992;&#23545;&#27604;&#35299;&#30721;&#21644;&#33976;&#39311;&#25552;&#21319;LLM&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#36739;&#23567;&#30340;&#19994;&#20313;&#27169;&#22411;&#25110;&#38544;&#34255;&#29366;&#24577;&#24046;&#24322;&#20998;&#26512;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;DCD&#37319;&#29992;&#20102;&#23545;&#27604;&#24335;&#24605;&#32500;&#24341;&#23548;&#21644;&#20808;&#36827;&#30340;&#33976;&#39311;&#25216;&#26415;&#65292;&#21253;&#25324;Dropout&#21644;&#37327;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23545;&#27604;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#21518;&#32773;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#21644;&#19994;&#20313;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;&#36890;&#36807;&#23558;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;DCD&#28040;&#38500;&#20102;&#23545;&#19994;&#20313;&#27169;&#22411;&#30340;&#38656;&#27714;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DCD&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#21508;&#31181;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;GSM8K&#21644;StrategyQA&#25968;&#25454;&#38598;&#20013;&#22343;&#36229;&#36807;&#20102;CD&#21644;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14874v1 Announce Type: cross  Abstract: We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization. This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.
&lt;/p&gt;</description></item><item><title>Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#20986;&#20102;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#35757;&#32451;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14873</link><description>&lt;p&gt;
Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Technical Report on the Checkfor.ai AI-Generated Text Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14873
&lt;/p&gt;
&lt;p&gt;
Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#20986;&#20102;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#35757;&#32451;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Checkfor.ai&#25991;&#26412;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#21306;&#20998;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#30340;&#25991;&#26412;&#21644;&#30001;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#12290;Checkfor.ai&#22312;&#30001;&#21313;&#31181;&#25991;&#26412;&#39046;&#22495;&#65288;&#23398;&#29983;&#20889;&#20316;&#12289;&#21019;&#24847;&#20889;&#20316;&#12289;&#31185;&#23398;&#20889;&#20316;&#12289;&#20070;&#31821;&#12289;&#30334;&#31185;&#20840;&#20070;&#12289;&#26032;&#38395;&#12289;&#30005;&#23376;&#37038;&#20214;&#12289;&#31185;&#23398;&#35770;&#25991;&#12289;&#31616;&#31572;&#38382;&#31572;&#65289;&#21644;8&#20010;&#24320;&#28304;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#34920;&#29616;&#20248;&#20110;&#38646;&#20914;&#20987;&#26041;&#27861;&#22914;DetectGPT&#20197;&#21450;&#20027;&#27969;&#21830;&#19994;AI&#26816;&#27979;&#24037;&#20855;&#65292;&#35823;&#24046;&#29575;&#38477;&#20302;&#20102;9&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31639;&#27861;&#65292;&#21363;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#65292;&#20351;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#35780;&#35770;&#31561;&#39640;&#25968;&#25454;&#39046;&#22495;&#23454;&#29616;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26356;&#20302;&#35823;&#25253;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Checkfor.ai&#19981;&#23545;&#38750;&#27597;&#35821;&#33521;&#35821;&#20154;&#22763;&#20135;&#29983;&#20559;&#35265;&#65292;&#24182;&#25512;&#24191;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#39046;&#22495;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14873v1 Announce Type: cross  Abstract: We present the Checkfor.ai text classifier, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Checkfor.ai outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 9 times lower error rates on a comprehensive benchmark comprised of ten text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q\&amp;A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Checkfor.ai is not biased against nonnative English speakers and generalizes to domains and models unseen during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.14872</link><description>&lt;p&gt;
&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;:&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#38024;&#23545;&#24320;&#28304;LLM&#30340;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#29992;&#20110;&#21019;&#24847;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#65292;&#26681;&#25454;&#36755;&#20837;&#24207;&#21015;&#29983;&#25104;&#25991;&#26412;&#65292;&#20294;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#26377;&#23475;&#36755;&#20986;&#12290;&#22823;&#22810;&#25968;&#36234;&#29425;&#25552;&#31034;&#26041;&#27861;&#20351;&#29992;&#19968;&#32452;&#36234;&#29425;&#27169;&#26495;&#65292;&#28982;&#21518;&#36319;&#38543;&#25552;&#20986;&#38382;&#39064;&#65292;&#21019;&#24314;&#36234;&#29425;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36234;&#29425;&#25552;&#31034;&#35774;&#35745;&#36890;&#24120;&#23384;&#22312;&#36807;&#22810;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#23548;&#33268;&#26080;&#27861;&#25269;&#24481;&#20351;&#29992;&#31616;&#21333;&#35821;&#20041;&#24230;&#37327;&#20316;&#20026;&#38408;&#20540;&#30340;&#38450;&#24481;&#12290;&#36234;&#29425;&#25552;&#31034;&#22312;&#35821;&#20041;&#19978;&#27604;&#29992;&#20110;&#26597;&#35810;&#30340;&#21407;&#22987;&#38382;&#39064;&#26356;&#21152;&#22810;&#26679;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;&#25105;&#20204;&#23558;&#23547;&#25214;&#26082;&#28385;&#36275;&#35821;&#20041;&#30456;&#20284;&#24615;&#21448;&#20855;&#26377;&#36234;&#29425;&#26377;&#25928;&#24615;&#30340;&#36234;&#29425;&#25552;&#31034;&#24314;&#27169;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14872v1 Announce Type: cross  Abstract: Large Language Models (LLMs), used in creative writing, code generation, and translation, generate text based on input sequences but are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak prompt methods use a combination of jailbreak templates followed by questions to ask to create jailbreak prompts. However, existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds. Jailbreak prompts are semantically more varied than the original questions used for queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach that bypasses LLMs by generating jailbreak prompts that are semantically similar to the original question. We model the search for jailbreak prompts that satisfy both semantic similarity and jailbreak validity as a multi-objective optimization proble
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;LLMs&#19982;&#25552;&#31034;&#24037;&#31243;&#21644;&#22810;Agent&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#32467;&#26500;&#30340;&#26032;&#25991;&#26723;&#12290;</title><link>https://arxiv.org/abs/2402.14871</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#22810;Agent&#29983;&#25104;&#20844;&#20849;&#34892;&#25919;&#39046;&#22495;&#35821;&#20041;&#27169;&#26495;&#20013;&#30340;&#21322;&#32467;&#26500;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;LLMs&#19982;&#25552;&#31034;&#24037;&#31243;&#21644;&#22810;Agent&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#32467;&#26500;&#30340;&#26032;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#30340;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#65292;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#20844;&#20849;&#34892;&#25919;&#39046;&#22495;&#20013;&#25991;&#26723;&#30340;&#21019;&#24314;&#21644;&#31649;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#12290;&#21322;&#32467;&#26500;&#25991;&#26723;&#38656;&#35201;&#22788;&#29702;&#19968;&#31995;&#21015;&#29305;&#23450;&#25968;&#25454;&#20294;&#27809;&#26377;&#22266;&#23450;&#26684;&#24335;&#65292;&#22240;&#27492;&#19981;&#33021;&#20351;&#29992;&#22522;&#20110;&#27169;&#26495;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;LLMs&#19982;&#25552;&#31034;&#24037;&#31243;&#21644;&#22810;Agent&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#31526;&#21512;&#26399;&#26395;&#32467;&#26500;&#30340;&#26032;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14871v1 Announce Type: cross  Abstract: In the last years' digitalization process, the creation and management of documents in various domains, particularly in Public Administration (PA), have become increasingly complex and diverse. This complexity arises from the need to handle a wide range of document types, often characterized by semi-structured forms. Semi-structured documents present a fixed set of data without a fixed format. As a consequence, a template-based solution cannot be used, as understanding a document requires the extraction of the data structure. The recent introduction of Large Language Models (LLMs) has enabled the creation of customized text output satisfying user requests. In this work, we propose a novel approach that combines the LLMs with prompt engineering and multi-agent systems for generating new documents compliant with a desired structure. The main contribution of this work concerns replacing the commonly used manual prompting with a task descr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#19981;&#21516;&#30340;&#21152;&#26435;&#29305;&#24449;&#26041;&#27861;&#65288;&#20108;&#20803;&#21644;&#35789;&#39057;&#21152;&#26435;&#65289;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#20572;&#29992;&#35789;&#26102;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#21484;&#22238;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;F-&#24230;&#37327;&#20540;&#65292;&#32467;&#26524;&#34920;&#26126;&#20572;&#29992;&#35789;&#30340;&#22788;&#29702;&#26041;&#24335;&#23545;&#25991;&#26412;&#20998;&#31867;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.14867</link><description>&lt;p&gt;
&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#20572;&#29992;&#35789;&#23545;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#30340;&#21152;&#26435;&#26041;&#27861;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of term weighting approach with and without stop words removing on Arabic text classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#19981;&#21516;&#30340;&#21152;&#26435;&#29305;&#24449;&#26041;&#27861;&#65288;&#20108;&#20803;&#21644;&#35789;&#39057;&#21152;&#26435;&#65289;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#20572;&#29992;&#35789;&#26102;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#21484;&#22238;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;F-&#24230;&#37327;&#20540;&#65292;&#32467;&#26524;&#34920;&#26126;&#20572;&#29992;&#35789;&#30340;&#22788;&#29702;&#26041;&#24335;&#23545;&#25991;&#26412;&#20998;&#31867;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#25991;&#26412;&#26159;&#19968;&#31181;&#23558;&#25991;&#26723;&#20998;&#31867;&#20026;&#39044;&#20808;&#24314;&#31435;&#30340;&#32676;&#32452;&#30340;&#26041;&#27861;&#12290;&#22312;&#20998;&#31867;&#20043;&#21069;&#65292;&#25991;&#26412;&#25991;&#26723;&#24517;&#39035;&#20197;&#36866;&#21512;&#25968;&#25454;&#25366;&#25496;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#30340;&#26041;&#24335;&#36827;&#34892;&#20934;&#22791;&#21644;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#21019;&#24314;&#20102;&#35768;&#22810;&#26415;&#35821;&#21152;&#26435;&#31574;&#30053;&#26469;&#22686;&#24378;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#30340;&#21151;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20108;&#20803;&#21644;&#35789;&#39057;&#21152;&#26435;&#29305;&#24449;&#26041;&#27861;&#23545;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#19968;&#27425;&#21024;&#38500;&#20572;&#29992;&#35789;&#21644;&#19981;&#21024;&#38500;&#20572;&#29992;&#35789;&#12290;&#20026;&#20102;&#35780;&#20272;&#20808;&#21069;&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#23545;&#20998;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;322&#20221;&#25991;&#26723;&#30340;&#38463;&#25289;&#20271;&#25968;&#25454;&#38598;&#65292;&#20998;&#20026;&#20845;&#20010;&#20027;&#39064;&#65288;&#20892;&#19994;&#12289;&#32463;&#27982;&#12289;&#20581;&#24247;&#12289;&#25919;&#27835;&#12289;&#31185;&#23398;&#21644;&#20307;&#32946;&#65289;&#65292;&#27599;&#20010;&#20027;&#39064;&#21253;&#21547;50&#20221;&#25991;&#26723;&#65292;&#21807;&#29420;&#20581;&#24247;&#31867;&#21035;&#38500;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14867v1 Announce Type: cross  Abstract: Classifying text is a method for categorizing documents into pre-established groups. Text documents must be prepared and represented in a way that is appropriate for the algorithms used for data mining prior to classification. As a result, a number of term weighting strategies have been created in the literature to enhance text categorization algorithms' functionality. This study compares the effects of Binary and Term frequency weighting feature methodologies on the text's classification method when stop words are eliminated once and when they are not. In recognition of assessing the effects of prior weighting of features approaches on classification results in terms of accuracy, recall, precision, and F-measure values, we used an Arabic data set made up of 322 documents divided into six main topics (agriculture, economy, health, politics, science, and sport), each of which contains 50 documents, with the exception of the health categ
&lt;/p&gt;</description></item><item><title>APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;</title><link>https://arxiv.org/abs/2402.14866</link><description>&lt;p&gt;
APTQ: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14866
&lt;/p&gt;
&lt;p&gt;
APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#39640;&#35745;&#31639;&#36127;&#36733;&#21644;&#24040;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#23545;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;APTQ&#65288;Attention-aware Post-Training Mixed-Precision Quantization&#65289;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#20102;&#27599;&#23618;&#26435;&#37325;&#30340;&#20108;&#38454;&#20449;&#24687;&#65292;&#32780;&#19988;&#39318;&#27425;&#32771;&#34385;&#20102;&#27880;&#24847;&#21147;&#36755;&#20986;&#23545;&#25972;&#20010;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#21033;&#29992;Hessian&#36857;&#20316;&#20026;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25935;&#24863;&#24230;&#24230;&#37327;&#65292;&#30830;&#20445;&#32463;&#36807;&#29702;&#24615;&#30340;&#31934;&#24230;&#38477;&#20302;&#33021;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;APTQ&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;C4&#25968;&#25454;&#38598;&#20013;&#20197;&#24179;&#22343;4&#20301;&#23485;&#24230;&#33719;&#24471;5.22&#22256;&#24785;&#24230;&#65292;&#20960;&#20046;&#31561;&#25928;&#20110;&#20840;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;APTQ&#22312;LLaMa-7B&#21644;LLaMa-1&#20013;&#20197;&#24179;&#22343;3.8&#20301;&#23485;&#24230;&#36798;&#21040;&#20102;68.24&#65285;&#21644;70.48&#65285;&#30340;&#26368;&#20808;&#36827;&#38646;-shot&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#24605;&#24819;&#30340;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14865</link><description>&lt;p&gt;
DyVal 2: &#20803;&#25506;&#27979;&#20195;&#29702;&#21160;&#24577;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#24605;&#24819;&#30340;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#35774;&#35745;&#20102;&#20351;&#29992;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#26126;&#30830;&#23450;&#20041;&#31639;&#27861;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36825;&#20123;&#21327;&#35758;&#26080;&#27861;&#36731;&#26494;&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#21482;&#33021;&#25552;&#20379;&#25972;&#20307;&#22522;&#20934;&#32467;&#26524;&#65292;&#19981;&#33021;&#25903;&#25345;&#23545;LLMs&#33021;&#21147;&#36827;&#34892;&#32454;&#31890;&#24230;&#21644;&#22810;&#26041;&#38754;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#24515;&#29702;&#27979;&#37327;&#23398;&#21551;&#21457;&#30340;&#36890;&#29992;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#12290; MPA &#26159; DyVal 2 &#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#33258;&#28982;&#22320;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340; DyVal&#12290; MPA &#35774;&#35745;&#20102;&#25506;&#27979;&#21644;&#35780;&#21028;&#20195;&#29702;&#65292;&#20197;&#33258;&#21160;&#23558;&#21407;&#22987;&#35780;&#20272;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#36981;&#24490;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#22312;&#19977;&#20010;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#19978;&#30340;&#24212;&#29992;: &#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#39064;&#35299;&#20915;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14865v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;CloudNine&#8221;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#27668;&#35937;&#35266;&#27979;&#23545;&#29305;&#23450;&#22825;&#27668;&#39044;&#27979;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2402.14861</link><description>&lt;p&gt;
CloudNine&#65306;&#20351;&#29992;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#27668;&#35937;&#35266;&#27979;&#23545;&#22825;&#27668;&#39044;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
CloudNine: Analyzing Meteorological Observation Impact on Weather Prediction Using Explainable Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14861
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;CloudNine&#8221;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#27668;&#35937;&#35266;&#27979;&#23545;&#29305;&#23450;&#22825;&#27668;&#39044;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#35937;&#35266;&#27979;&#23545;&#22825;&#27668;&#39044;&#25253;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#20256;&#24863;&#22120;&#31867;&#22411;&#12289;&#20301;&#32622;&#12289;&#26102;&#38388;&#21644;&#20854;&#20182;&#29615;&#22659;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#23450;&#37327;&#20998;&#26512;&#35266;&#27979;&#24433;&#21709;&#23545;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;CloudNine&#8221;&#30340;&#26032;&#31995;&#32479;&#65292;&#22522;&#20110;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;XGNNs&#65289;&#20998;&#26512;&#21333;&#20010;&#35266;&#27979;&#23545;&#29305;&#23450;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#23558;&#22522;&#20110;XGNN&#30340;&#22823;&#27668;&#29366;&#24577;&#20272;&#35745;&#27169;&#22411;&#19982;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#19968;&#20010;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#22312;&#22320;&#29699;&#31995;&#32479;&#30340;&#19977;&#32500;&#31354;&#38388;&#20013;&#25628;&#32034;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14861v1 Announce Type: cross  Abstract: The impact of meteorological observations on weather forecasting varies with sensor type, location, time, and other environmental factors. Thus, quantitative analysis of observation impacts is crucial for effective and efficient development of weather forecasting systems. However, the existing impact analysis methods are difficult to be widely applied due to their high dependencies on specific forecasting systems. Also, they cannot provide observation impacts at multiple spatio-temporal scales, only global impacts of observation types. To address these issues, we present a novel system called ``CloudNine,'' which allows analysis of individual observations' impacts on specific predictions based on explainable graph neural networks (XGNNs). Combining an XGNN-based atmospheric state estimation model with a numerical weather prediction model, we provide a web application to search for observations in the 3D space of the Earth system and to
&lt;/p&gt;</description></item><item><title>&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14860</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#23454;&#20917;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking Large Language Models without Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14860
&lt;/p&gt;
&lt;p&gt;
&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#24433;&#21709;&#21147;&#30340;&#22686;&#24378;&#65292;&#35780;&#20272;&#21644;&#25490;&#21517;LLMs&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#31867;&#21709;&#24212;&#65292;&#35201;&#20040;&#20351;&#29992;LLMs&#25104;&#23545;&#22320;&#20114;&#30456;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#25552;&#31034;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;&#38382;&#39064;&#12289;&#35828;&#26126;&#31561;&#65289;&#21644;&#19968;&#32452;LLMs&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#24773;&#20917;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#12290;&#21463;&#21040;&#29616;&#23454;&#29983;&#27963;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#19987;&#23478;&#21644;&#26377;&#30693;&#35782;&#30340;&#20154;&#37117;&#33021;&#35782;&#21035;&#19968;&#20010;&#26032;&#25163;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35780;&#20272;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#24182;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#21453;&#22797;&#24212;&#29992;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;LLMs&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35770;&#25991;&#25506;&#35752;&#20102;&#22312;MLLM&#31038;&#20250;&#20013;&#36890;&#36807;&#21333;&#20010;&#25805;&#20316;&#21592;&#38388;&#25509;&#24433;&#21709;&#20854;&#20182;&#20195;&#29702;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#26032;&#22411;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.14859</link><description>&lt;p&gt;
&#20869;&#22312;&#30340;&#29436;&#65306;&#36890;&#36807;MLLM&#25805;&#20316;&#21592;&#21521;MLLM&#31038;&#20250;&#20013;&#28183;&#20837;&#24694;&#24847;
&lt;/p&gt;
&lt;p&gt;
The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14859
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35770;&#25991;&#25506;&#35752;&#20102;&#22312;MLLM&#31038;&#20250;&#20013;&#36890;&#36807;&#21333;&#20010;&#25805;&#20316;&#21592;&#38388;&#25509;&#24433;&#21709;&#20854;&#20182;&#20195;&#29702;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#26032;&#22411;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#22788;&#29702;&#21644;&#21709;&#24212;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#33021;&#21147;&#65292;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#19981;&#26029;&#23450;&#20041;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#26032;&#36793;&#30028;&#12290;&#38543;&#30528;&#36825;&#20123;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24418;&#25104;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#30340;&#21327;&#20316;&#32593;&#32476;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#12298;&#20869;&#22312;&#30340;&#29436;&#12299;&#25506;&#35752;&#20102;MLLM&#31038;&#20250;&#20013;&#30340;&#19968;&#31181;&#26032;&#22411;&#28431;&#27934; - &#24694;&#24847;&#20869;&#23481;&#30340;&#38388;&#25509;&#20256;&#25773;&#12290;&#19982;&#30452;&#25509;&#20026;MLLM&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#20010;MLLM&#20195;&#29702;&#22914;&#20309;&#34987;&#24494;&#22937;&#22320;&#24433;&#21709;&#65292;&#20197;&#29983;&#25104;&#20877;&#27425;&#35825;&#20351;&#31038;&#20250;&#20013;&#20854;&#20182;MLLM&#20195;&#29702;&#36755;&#20986;&#24694;&#24847;&#20869;&#23481;&#30340;&#25552;&#31034;&#12290;&#36825;&#31181;&#24494;&#22937;&#32780;&#24378;&#26377;&#21147;&#30340;&#38388;&#25509;&#24433;&#21709;&#26041;&#27861;&#26631;&#24535;&#30528;&#19982;MLLM&#30456;&#20851;&#30340;&#23433;&#20840;&#39118;&#38505;&#30340;&#26174;&#33879;&#21319;&#32423;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#20960;&#20046;&#27809;&#26377;&#25110;&#26159;&#26681;&#26412;&#27809;&#26377;&#35775;&#38382;MLLM&#21442;&#25968;&#65292;&#19968;&#20010;MLLM&#20195;&#29702;&#65292;&#24403;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14859v1 Announce Type: cross  Abstract: Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when 
&lt;/p&gt;</description></item><item><title>ChatEL&#26694;&#26550;&#36890;&#36807;&#19977;&#27493;&#26694;&#26550;&#25913;&#36827;&#20102;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#24179;&#22343;F1&#24615;&#33021;&#25552;&#39640;&#36229;&#36807;2&#65285;</title><link>https://arxiv.org/abs/2402.14858</link><description>&lt;p&gt;
ChatEL: &#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#19968;&#36215;&#36827;&#34892;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
ChatEL: Entity Linking with Chatbots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14858
&lt;/p&gt;
&lt;p&gt;
ChatEL&#26694;&#26550;&#36890;&#36807;&#19977;&#27493;&#26694;&#26550;&#25913;&#36827;&#20102;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#24179;&#22343;F1&#24615;&#33021;&#25552;&#39640;&#36229;&#36807;2&#65285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#25991;&#26723;&#25110;&#21477;&#23376;&#20013;&#34920;&#31034;&#23454;&#20307;&#30340;&#19968;&#20123;&#25991;&#26412;&#19982;&#23383;&#20856;&#25110;&#30693;&#35782;&#24211;&#20013;&#30456;&#24212;&#30340;&#26465;&#30446;&#36827;&#34892;&#38142;&#25509;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#21019;&#24314;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#65292;&#20197;&#23547;&#25214;&#21608;&#22260;&#21333;&#35789;&#30340;&#32447;&#32034;&#26469;&#24110;&#21161;&#35299;&#20915;&#38142;&#25509;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#32463;&#36807;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#38590;&#20197;&#22788;&#29702;&#65292;&#38590;&#20197;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#20854;&#20182;&#39046;&#22495;&#36716;&#31227;&#25928;&#26524;&#19981;&#20339;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20687;GPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;EL&#27169;&#22411;&#20013;&#22266;&#26377;&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#24230;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;LLM&#36827;&#34892;&#31616;&#21333;&#30340;&#25552;&#31034;&#24182;&#19981;&#22863;&#25928;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;ChatEL&#65292;&#36825;&#26159;&#19968;&#20010;&#19977;&#27493;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#31034;LLM&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;ChatEL&#26694;&#26550;&#23558;10&#20010;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;F1&#24615;&#33021;&#25552;&#39640;&#20102;&#36229;&#36807;2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14858v1 Announce Type: cross  Abstract: Entity Linking (EL) is an essential and challenging task in natural language processing that seeks to link some text representing an entity within a document or sentence with its corresponding entry in a dictionary or knowledge base. Most existing approaches focus on creating elaborate contextual models that look for clues the words surrounding the entity-text to help solve the linking problem. Although these fine-tuned language models tend to work, they can be unwieldy, difficult to train, and do not transfer well to other domains. Fortunately, Large Language Models (LLMs) like GPT provide a highly-advanced solution to the problems inherent in EL models, but simply naive prompts to LLMs do not work well. In the present work, we define ChatEL, which is a three-step framework to prompt LLMs to return accurate results. Overall the ChatEL framework improves the average F1 performance across 10 datasets by more than 2%. Finally, a thorough
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#28040;&#24687;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#23545;&#25269;&#25239;&#36234;&#29425;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#65292;&#19988;&#36234;&#29425;&#21487;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14857</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#28040;&#24687;&#23545;&#36234;&#29425;&#26159;&#21542;&#30495;&#30340;&#24456;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the System Message Really Important to Jailbreaks in Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14857
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#28040;&#24687;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#23545;&#25269;&#25239;&#36234;&#29425;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#65292;&#19988;&#36234;&#29425;&#21487;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#19981;&#21487;&#25110;&#32570;&#12290;&#23613;&#31649;&#36890;&#24120;&#20250;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#22312;&#21457;&#24067;&#21069;&#23558;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#29616;&#35937;&#65292;&#34987;&#31216;&#20026;"&#36234;&#29425;"&#12290;&#36825;&#20010;&#26415;&#35821;&#25351;&#30340;&#26159;&#24403;LLMs&#21463;&#21040;&#24694;&#24847;&#38382;&#39064;&#25552;&#31034;&#26102;&#20135;&#29983;&#24847;&#22806;&#19988;&#21487;&#33021;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#29983;&#25104;&#36234;&#29425;&#25552;&#31034;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#19981;&#21516;&#30340;&#38382;&#39064;&#65306;&#31995;&#32479;&#28040;&#24687;&#23545;LLMs&#20013;&#30340;&#36234;&#29425;&#26159;&#21542;&#30495;&#30340;&#24456;&#37325;&#35201;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#31283;&#23450;&#30340;GPT&#29256;&#26412;gpt-3.5-turbo-0613&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#29983;&#25104;&#20102;&#20855;&#26377;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#30340;&#36234;&#29425;&#25552;&#31034;&#65306;&#30701;&#65292;&#38271;&#21644;&#26080;&#28040;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#31995;&#32479;&#28040;&#24687;&#36890;&#36807;&#23454;&#39564;&#20855;&#26377;&#19981;&#21516;&#30340;&#25269;&#25239;&#36234;&#29425;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36234;&#29425;&#22312;LLMs&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#31995;&#32479;&#28040;&#24687;&#22312;&#38450;&#27490;LLMs&#36234;&#29425;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14857v1 Announce Type: cross  Abstract: The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named "jailbreak." This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the signi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20986;&#23427;&#20204;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#27169;&#24335;&#21644;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.14856</link><description>&lt;p&gt;
&#22312;&#25512;&#29702;&#24605;&#32500;&#20013;&#27604;&#36739;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14856
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20986;&#23427;&#20204;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#27169;&#24335;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#24605;&#32500;&#22312;&#21046;&#23450;&#20581;&#20840;&#21644;&#36830;&#36143;&#35770;&#28857;&#26041;&#38754;&#25198;&#28436;&#20102;&#20851;&#38190;&#35282;&#33394;&#12290;&#23427;&#20801;&#35768;&#20010;&#20307;&#26681;&#25454;&#25152;&#25552;&#20379;&#20449;&#24687;&#30340;&#30495;&#20540;&#24471;&#20986;&#36923;&#36753;&#19978;&#30340;&#32467;&#35770;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25191;&#34892;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#20027;&#35201;&#35780;&#20272;LLMs&#22312;&#35299;&#20915;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#20854;&#25512;&#29702;&#34892;&#20026;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#35748;&#30693;&#24515;&#29702;&#23398;&#21407;&#29702;&#65292;&#36890;&#36807;&#23545;&#23427;&#20204;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#65292;&#26469;&#30740;&#31350;LLMs&#37319;&#29992;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#35266;&#23519;&#21040;&#30340;&#25512;&#29702;&#27169;&#24335;&#65292;&#21253;&#25324;&#35832;&#22914;&#8220;&#20551;&#23450;&#36319;&#38543;&#8221;&#25110;&#8220;&#38142;&#26500;&#24314;&#8221;&#31561;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;arXiv:2402.14856v1 Announce Type: cross
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14856v1 Announce Type: cross  Abstract: Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the ar
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#21040;&#26597;&#35810;&#24212;&#29992;&#30340;LLM&#25104;&#29087;&#24230;&#27169;&#22411;&#65292;&#19981;&#20165;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#36824;&#25193;&#23637;&#21040;&#26356;&#22810;&#32500;&#24230;&#12290;&#21516;&#26102;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#29992;&#20110;&#25191;&#27861;&#39046;&#22495;&#30340;&#23454;&#38469;&#26696;&#20363;&#65292;&#20171;&#32461;&#20102;&#22495;&#29305;&#23450;&#25991;&#26412;&#21040;&#26597;&#35810;&#21161;&#25163;QueryIQ&#12290;</title><link>https://arxiv.org/abs/2402.14855</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#38752;&#36879;&#26126;&#25991;&#26412;&#21040;&#26597;&#35810;&#30340;LLM&#25104;&#29087;&#24230;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An LLM Maturity Model for Reliable and Transparent Text-to-Query
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14855
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#21040;&#26597;&#35810;&#24212;&#29992;&#30340;LLM&#25104;&#29087;&#24230;&#27169;&#22411;&#65292;&#19981;&#20165;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#36824;&#25193;&#23637;&#21040;&#26356;&#22810;&#32500;&#24230;&#12290;&#21516;&#26102;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#29992;&#20110;&#25191;&#27861;&#39046;&#22495;&#30340;&#23454;&#38469;&#26696;&#20363;&#65292;&#20171;&#32461;&#20102;&#22495;&#29305;&#23450;&#25991;&#26412;&#21040;&#26597;&#35810;&#21161;&#25163;QueryIQ&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#35782;&#21040;&#35299;&#20915;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#30340;&#24517;&#35201;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;&#26597;&#35810;&#24212;&#29992;&#30340;LLM&#25104;&#29087;&#24230;&#27169;&#22411;&#12290;&#35813;&#25104;&#29087;&#24230;&#27169;&#22411;&#26088;&#22312;&#22635;&#34917;&#22312;&#35780;&#20272;LLM&#22312;&#27492;&#31867;&#24212;&#29992;&#20013;&#30340;&#19981;&#36275;&#65292;&#21516;&#26102;&#32435;&#20837;&#20102;&#36229;&#36234;&#32431;&#31929;&#27491;&#30830;&#24615;&#25110;&#20934;&#30830;&#24615;&#30340;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#24037;&#20316;&#24341;&#20837;&#20102;&#25191;&#27861;&#39046;&#22495;&#30340;&#19968;&#20010;&#30495;&#23454;&#29992;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;QueryIQ&#65292;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#21040;&#26597;&#35810;&#21161;&#25163;&#65292;&#20197;&#21152;&#36895;&#29992;&#25143;&#24037;&#20316;&#27969;&#31243;&#24182;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14855v1 Announce Type: cross  Abstract: Recognizing the imperative to address the reliability and transparency issues of Large Language Models (LLM), this work proposes an LLM maturity model tailored for text-to-query applications. This maturity model seeks to fill the existing void in evaluating LLMs in such applications by incorporating dimensions beyond mere correctness or accuracy. Moreover, this work introduces a real-world use case from the law enforcement domain and showcases QueryIQ, an LLM-powered, domain-specific text-to-query assistant to expedite user workflows and reveal hidden relationship in data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19987;&#23478;&#36523;&#20221;&#21644;&#33258;&#26432;&#35789;&#20856;&#19982;&#24515;&#29702;&#20581;&#24247;&#29305;&#23450;LLM&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.14854</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Dual-Prompting for Interpretable Mental Health Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19987;&#23478;&#36523;&#20221;&#21644;&#33258;&#26432;&#35789;&#20856;&#19982;&#24515;&#29702;&#20581;&#24247;&#29305;&#23450;LLM&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24037;&#26234;&#33021;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#24037;&#20855;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#23427;&#20204;&#23545;&#20020;&#24202;&#21307;&#29983;&#30340;&#23454;&#38469;&#25928;&#29992;&#26377;&#38480;&#12290;CLPsych 2024&#20849;&#20139;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#33258;&#26432;&#24847;&#35782;&#30340;&#35777;&#25454;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#25552;&#31034;&#26041;&#27861;&#65306;&#65288;i&#65289;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#36523;&#20221;&#21644;&#33258;&#26432;&#35789;&#20856;&#19982;&#24515;&#29702;&#20581;&#24247;&#29305;&#23450;LLM&#30456;&#32467;&#21512;&#65292;&#36827;&#34892;&#30693;&#35782;&#24863;&#30693;&#35777;&#25454;&#25552;&#21462;&#65307;&#20197;&#21450;&#65288;ii&#65289;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#19968;&#33268;&#24615;&#35780;&#20272;&#22120;&#26469;&#36827;&#34892;&#35777;&#25454;&#24635;&#32467;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#32467;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#25581;&#31034;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#21644;&#35813;&#26041;&#27861;&#22312;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#36827;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14854v1 Announce Type: cross  Abstract: Despite the increasing demand for AI-based mental health monitoring tools, their practical utility for clinicians is limited by the lack of interpretability.The CLPsych 2024 Shared Task (Chim et al., 2024) aims to enhance the interpretability of Large Language Models (LLMs), particularly in mental health analysis, by providing evidence of suicidality through linguistic content. We propose a dual-prompting approach: (i) Knowledge-aware evidence extraction by leveraging the expert identity and a suicide dictionary with a mental health-specific LLM; and (ii) Evidence summarization by employing an LLM-based consistency evaluator. Comprehensive experiments demonstrate the effectiveness of combining domain-specific information, revealing performance improvements and the approach's potential to aid clinicians in assessing mental state progression.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;NL2Formula&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#22522;&#20110;&#30005;&#23376;&#34920;&#26684;&#34920;&#26684;&#30340;&#21487;&#25191;&#34892;&#20844;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;fCoder&#30340;&#22522;&#20934;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14853</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#30005;&#23376;&#34920;&#26684;&#20844;&#24335;&#30340;NL2Formula
&lt;/p&gt;
&lt;p&gt;
NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;NL2Formula&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#22522;&#20110;&#30005;&#23376;&#34920;&#26684;&#34920;&#26684;&#30340;&#21487;&#25191;&#34892;&#20844;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;fCoder&#30340;&#22522;&#20934;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#34920;&#26684;&#19978;&#32534;&#20889;&#20844;&#24335;&#65292;&#22914;Microsoft Excel&#21644;Google Sheets&#65292;&#26159;&#35768;&#22810;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#30340;&#29992;&#25143;&#24191;&#27867;&#20351;&#29992;&#30340;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26368;&#32456;&#29992;&#25143;&#26469;&#35828;&#65292;&#21046;&#20316;&#30005;&#23376;&#34920;&#26684;&#20844;&#24335;&#20173;&#28982;&#26159;&#19968;&#39033;&#32321;&#29712;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22797;&#26434;&#25805;&#20316;&#26102;&#12290;&#20026;&#20102;&#20943;&#36731;&#32534;&#20889;&#30005;&#23376;&#34920;&#26684;&#20844;&#24335;&#25152;&#24102;&#26469;&#30340;&#36127;&#25285;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;NL2Formula&#30340;&#26032;&#22411;&#22522;&#20934;&#20219;&#21153;&#65292;&#26088;&#22312;&#26681;&#25454;&#36755;&#20837;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#26597;&#35810;&#29983;&#25104;&#22522;&#20110;&#30005;&#23376;&#34920;&#26684;&#30340;&#21487;&#25191;&#34892;&#20844;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;70,799&#20010;&#37197;&#23545;NL&#26597;&#35810;&#21644;&#30456;&#24212;&#30005;&#23376;&#34920;&#26684;&#20844;&#24335;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;21,670&#20010;&#34920;&#26684;&#21644;37&#31181;&#20844;&#24335;&#20989;&#25968;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#31216;&#20026;fCoder&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#22522;&#20934;&#23454;&#29616;&#26469;&#23454;&#29616;NL2Formula&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;fCoder&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14853v1 Announce Type: cross  Abstract: Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior per
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#26032;&#30340;GPT-4&#27169;&#22411;&#22312;&#31243;&#24207;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#22312;HumanEval&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#22312;&#38646;&#26679;&#26412;Python&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#21644;&#26356;&#22810;&#22810;&#27493;&#39588;&#33539;&#24335;&#32508;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14852</link><description>&lt;p&gt;
&#26368;&#26032;GPT&#27169;&#22411;&#19978;&#30340;HumanEval -- 2024
&lt;/p&gt;
&lt;p&gt;
HumanEval on Latest GPT Models -- 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14852
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#26032;&#30340;GPT-4&#27169;&#22411;&#22312;&#31243;&#24207;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#22312;HumanEval&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#22312;&#38646;&#26679;&#26412;Python&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#21644;&#26356;&#22810;&#22810;&#27493;&#39588;&#33539;&#24335;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2023&#24180;&#65292;&#25105;&#20204;&#27491;&#22312;&#20351;&#29992;&#26368;&#26032;&#30340;GPT-4&#27169;&#22411;&#26469;&#25512;&#36827;&#31243;&#24207;&#21512;&#25104;&#12290;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26174;&#33879;&#25913;&#36827;&#20102;&#36825;&#19968;&#30446;&#30340;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#36827;&#23637;&#26356;&#26131;&#20110;&#35775;&#38382;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#23558;&#36825;&#20123;&#27169;&#22411;&#36830;&#25509;&#21040;Human Eval&#30340;&#23384;&#20648;&#24211;&#12290;&#35813;&#25968;&#25454;&#38598;&#26368;&#21021;&#26159;&#20026;&#19982;&#21517;&#20026;CODEGEN&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#25968;&#25454;&#19978;&#20351;&#29992;&#32780;&#24320;&#21457;&#30340;&#12290;&#36890;&#36807;&#23637;&#31034;&#36825;&#20123;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#19982;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#22312;HumanEval&#20219;&#21153;&#19978;&#38646;&#26679;&#26412;Python&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#35757;&#32451;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20026;&#24320;&#21457;&#26356;&#22810;&#30340;&#22810;&#27493;&#39588;&#33539;&#24335;&#32508;&#21512;&#21019;&#36896;&#20102;&#21487;&#33021;&#12290;&#36825;&#19968;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;160&#20010;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#38598;&#65292;&#36825;&#20123;&#38382;&#39064;&#38598;&#34987;&#20998;&#35299;&#25104;&#22810;&#27493;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#36825;&#26174;&#33879;&#25913;&#36827;&#20102;&#21333;&#36718;&#36755;&#20837;&#19978;&#30340;&#31243;&#24207;&#32508;&#21512;&#12290;&#25152;&#26377;&#20195;&#30721;&#22343;&#20197;&#24320;&#28304;&#26041;&#24335;&#21457;&#24067;&#22312;https://github.com/daniel442li/gpt-human-eval&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14852v1 Announce Type: cross  Abstract: In 2023, we are using the latest models of GPT-4 to advance program synthesis. The large language models have significantly improved the state-of-the-art for this purpose. To make these advancements more accessible, we have created a repository that connects these models to Huamn Eval. This dataset was initally developed to be used with a language model called CODEGEN on natural and programming language data. The utility of these trained models is showcased by demonstrating their competitive performance in zero-shot Python code generation on HumanEval tasks compared to previous state-of-the-art solutions. Additionally, this gives way to developing more multi-step paradigm synthesis. This benchmark features 160 diverse problem sets factorized into multistep prompts that our analysis shows significantly improves program synthesis over single-turn inputs. All code is open source at https://github.com/daniel442li/gpt-human-eval .
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SQL-CRAFT&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#65292;&#25552;&#21319;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;5.7%&#65292;&#24182;&#22312;Spider&#27036;&#21333;&#19978;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.14851</link><description>&lt;p&gt;
SQL-CRAFT: &#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14851
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SQL-CRAFT&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#65292;&#25552;&#21319;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;5.7%&#65292;&#24182;&#22312;Spider&#27036;&#21333;&#19978;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20294;&#22312;&#19987;&#38376;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#21040;SQL&#65289;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SQL-CRAFT&#65292;&#19968;&#20010;&#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#26469;&#25552;&#21319;&#22823;&#35821;&#35328;&#27169;&#22411;SQL&#29983;&#25104;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#20132;&#20114;&#24335;&#32416;&#38169;&#24490;&#29615;&#65288;IC-Loop&#65289;&#20351;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#25968;&#25454;&#24211;&#33258;&#21160;&#20132;&#20114;&#65292;&#21516;&#26102;&#37319;&#29992;&#22686;&#24378;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;Spider&#21644;Bird&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24615;&#33021;&#27604;&#26420;&#32032;&#25552;&#31034;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;5.7%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Spider&#27036;&#21333;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14851v1 Announce Type: cross  Abstract: Modern LLMs have become increasingly powerful, but they are still facing challenges in specialized tasks such as Text-to-SQL. We propose SQL-CRAFT, a framework to advance LLMs' SQL generation Capabilities through inteRActive reFinemenT and enhanced reasoning. We leverage an Interactive Correction Loop (IC-Loop) for LLMs to interact with databases automatically, as well as Python-enhanced reasoning. We conduct experiments on two Text-to-SQL datasets, Spider and Bird, with performance improvements of up to 5.7% compared to the naive prompting method. Moreover, our method surpasses the current state-of-the-art on the Spider Leaderboard, demonstrating the effectiveness of our framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHATATC&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#23454;&#29616;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14850</link><description>&lt;p&gt;
CHATATC&#65306;&#29992;&#20110;&#25903;&#25345;&#25112;&#30053;&#31354;&#20013;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHATATC&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#23454;&#29616;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#36890;&#36807;&#35832;&#22914;ChatGPT&#31561;&#20844;&#24320;&#21487;&#29992;&#24037;&#20855;&#24555;&#36895;&#36208;&#32418;&#12290;LLMs&#22312;&#20010;&#20154;&#21644;&#19987;&#19994;&#39046;&#22495;&#30340;&#24212;&#29992;&#24471;&#21040;&#25512;&#21160;&#65292;&#26159;&#30001;&#20110;&#20154;&#31867;&#29992;&#25143;&#19982;ChatGPT&#31561;&#35745;&#31639;&#26426;&#24212;&#29992;&#20043;&#38388;&#33258;&#28982;&#30340;&#20114;&#21160;&#65292;&#20197;&#21450;&#24378;&#22823;&#30340;&#25688;&#35201;&#21644;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#29983;&#25104;AI&#24037;&#20855;&#22914;&#20309;&#22312;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#20013;&#37096;&#32626;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;&#21253;&#21547;&#36229;&#36807;80,000&#20010;GDP&#23454;&#26045;&#12289;&#20462;&#35746;&#21644;&#21462;&#28040;&#30340;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#65292;&#23545;CHATATC&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;CHATATC&#30340;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#65292;&#35760;&#24405;&#20102;&#25104;&#21151;&#20043;&#22788;&#65288;&#20363;&#22914;&#65292;&#25552;&#20379;&#27491;&#30830;&#30340;GDP&#29575;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#21407;&#22240;&#65289;&#20197;&#21450;&#19981;&#36275;&#20043;&#22788;&#65288;&#20363;&#22914;&#65292;&#26368;&#39640;&#27700;&#24179;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14850v1 Announce Type: cross  Abstract: Generative artificial intelligence (AI) and large language models (LLMs) have gained rapid popularity through publicly available tools such as ChatGPT. The adoption of LLMs for personal and professional use is fueled by the natural interactions between human users and computer applications such as ChatGPT, along with powerful summarization and text generation capabilities. Given the widespread use of such generative AI tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#32534;&#30721;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14849</link><description>&lt;p&gt;
&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#32534;&#30721;&#23545;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Asynchronous and Segmented Bidirectional Encoding for NMT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#32534;&#30721;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#39640;&#32763;&#35793;&#25928;&#29575;&#21644;&#36136;&#37327;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#23454;&#26045;&#20102;&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#35299;&#30721;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#32763;&#35793;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#20174;&#24038;&#21040;&#21491;&#25110;&#20174;&#21491;&#21040;&#24038;&#30340;&#21333;&#21521;&#32763;&#35793;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#21477;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#22909;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;IWSLT2017&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21152;&#36895;&#32763;&#35793;&#21644;&#25552;&#39640;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#21333;&#21521;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14849v1 Announce Type: cross  Abstract: With the rapid advancement of Neural Machine Translation (NMT), enhancing translation efficiency and quality has become a focal point of research. Despite the commendable performance of general models such as the Transformer in various aspects, they still fall short in processing long sentences and fully leveraging bidirectional contextual information. This paper introduces an improved model based on the Transformer, implementing an asynchronous and segmented bidirectional decoding strategy aimed at elevating translation efficiency and accuracy. Compared to traditional unidirectional translations from left-to-right or right-to-left, our method demonstrates heightened efficiency and improved translation quality, particularly in handling long sentences. Experimental results on the IWSLT2017 dataset confirm the effectiveness of our approach in accelerating translation and increasing accuracy, especially surpassing traditional unidirection
&lt;/p&gt;</description></item><item><title>&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#38477;&#32423;&#36235;&#21183;&#20986;&#29616;&#22312;&#27604;&#25216;&#26415;&#26368;&#22823;&#20540;&#30701;&#24471;&#22810;&#30340;&#36755;&#20837;&#38271;&#24230;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.14848</link><description>&lt;p&gt;
&#20219;&#21153;&#30456;&#21516;&#65292;&#20196;&#29260;&#26356;&#22810;&#65306;&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14848
&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#38477;&#32423;&#36235;&#21183;&#20986;&#29616;&#22312;&#27604;&#25216;&#26415;&#26368;&#22823;&#20540;&#30701;&#24471;&#22810;&#30340;&#36755;&#20837;&#38271;&#24230;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25193;&#23637;&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;LLMs&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#19981;&#21516;&#36755;&#20837;&#38271;&#24230;&#19979;&#30340;&#24615;&#33021;&#19968;&#33268;&#24615;&#23578;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#31572;&#25512;&#29702;&#26694;&#26550;&#26469;&#30740;&#31350;&#27492;&#26041;&#38754;&#65292;&#35813;&#26694;&#26550;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#36755;&#20837;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21516;&#19968;&#26679;&#26412;&#30340;&#22810;&#20010;&#29256;&#26412;&#65292;&#27599;&#20010;&#29256;&#26412;&#37117;&#36890;&#36807;&#19981;&#21516;&#38271;&#24230;&#12289;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#22635;&#20805;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20174;&#32780;&#20998;&#31163;&#20102;&#36755;&#20837;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27604;&#23427;&#20204;&#30340;&#25216;&#26415;&#26368;&#22823;&#20540;&#30701;&#24471;&#22810;&#30340;&#36755;&#20837;&#38271;&#24230;&#19979;&#65292;LLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#26126;&#26174;&#38477;&#20302;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38477;&#32423;&#36235;&#21183;&#20986;&#29616;&#22312;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#27599;&#20010;&#29256;&#26412;&#20013;&#65292;&#23613;&#31649;&#24378;&#24230;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20256;&#32479;&#30340;&#22256;&#24785;&#24230;&#24230;&#37327;&#19982;LLMs&#22312;&#38271;&#36755;&#20837;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#27809;&#26377;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24182;&#35782;&#21035;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14848v1 Announce Type: cross  Abstract: This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that traditional perplexity metrics do not correlate with performance of LLMs' in long input reasoning tasks. We analyse our results and identif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#26426;&#38382;&#39064;&#35843;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#26368;&#20339;&#38382;&#39064;&#21010;&#20998;&#26041;&#24335;&#20197;&#26368;&#23567;&#21270;&#24635;&#28382;&#21518;&#12290;</title><link>https://arxiv.org/abs/2402.14847</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#26426;&#38382;&#39064;&#26368;&#23567;&#21270;&#24635;&#28382;&#21518;&#30340;&#35843;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep learning-driven scheduling algorithm for a single machine problem minimizing the total tardiness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#26426;&#38382;&#39064;&#35843;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#26368;&#20339;&#38382;&#39064;&#21010;&#20998;&#26041;&#24335;&#20197;&#26368;&#23567;&#21270;&#24635;&#28382;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#19968;&#20010;&#33879;&#21517;&#30340;NP&#38590;&#39064;&#21333;&#26426;&#35843;&#24230;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#24635;&#28382;&#21518;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20316;&#20026;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#20272;&#35745;&#37327;&#26469;&#29992;&#20110;&#22522;&#20110;Lawler&#20998;&#35299;&#21644;Della Croce&#31561;&#20154;&#25552;&#20986;&#30340;&#23545;&#31216;&#20998;&#35299;&#30340;&#21333;&#36941;&#35843;&#24230;&#31639;&#27861;&#12290;&#23454;&#36136;&#19978;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20272;&#35745;&#38382;&#39064;&#21010;&#20998;&#20026;&#23376;&#38382;&#39064;&#30340;&#26368;&#20339;&#26041;&#24335;&#26469;&#24341;&#23548;&#31639;&#27861;&#12290;&#26412;&#25991;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26032;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#36895;&#24230;&#65292;&#20943;&#23569;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#24179;&#22343;&#26368;&#20248;&#24615;&#24046;&#36317;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#35757;&#32451;&#38454;&#27573;&#27010;&#25324;&#20449;&#24687;&#21040;&#26356;&#22823;&#35268;&#27169;&#30340;&#23454;&#20363;&#12290;&#21363;&#20351;&#22312;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#23454;&#20363;&#20174;75
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14847v1 Announce Type: cross  Abstract: In this paper, we investigate the use of the deep learning method for solving a well-known NP-hard single machine scheduling problem with the objective of minimizing the total tardiness. We propose a deep neural network that acts as a polynomial-time estimator of the criterion value used in a single-pass scheduling algorithm based on Lawler's decomposition and symmetric decomposition proposed by Della Croce et al. Essentially, the neural network guides the algorithm by estimating the best splitting of the problem into subproblems. The paper also describes a new method for generating the training data set, which speeds up the training dataset generation and reduces the average optimality gap of solutions. The experimental results show that our machine learning-driven approach can efficiently generalize information from the training phase to significantly larger instances. Even though the instances used in the training phase have from 75
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.14846</link><description>&lt;p&gt;
&#22362;&#25345;&#20320;&#30340;&#35282;&#33394;&#65281;&#20010;&#20154;&#20215;&#20540;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stick to your Role! Stability of Personal Values Expressed in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#25110;&#24515;&#29702;&#38382;&#21367;&#30340;&#26631;&#20934;&#26041;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#25552;&#20379;&#35768;&#22810;&#26469;&#28304;&#20110;&#31867;&#20284;&#26368;&#23567;&#32972;&#26223;&#30340;&#19981;&#21516;&#26597;&#35810;&#65288;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#39640;&#24230;&#20381;&#36182;&#20110;&#32972;&#26223;&#65292;&#22240;&#27492;&#20174;&#36825;&#31181;&#26368;&#23567;&#32972;&#26223;&#35780;&#20272;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#21487;&#33021;&#23545;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#34892;&#20026;&#65288;&#22312;&#37027;&#37324;&#23427;&#23558;&#26292;&#38706;&#20110;&#35768;&#22810;&#26032;&#32972;&#26223;&#65289;&#30340;&#35828;&#26126;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20381;&#36182;&#20110;&#32972;&#26223;&#30340;&#29305;&#24615;&#24212;&#35813;&#20316;&#20026;LLM&#27604;&#36739;&#30340;&#21478;&#19968;&#20010;&#32500;&#24230;&#26469;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#32500;&#24230;&#65292;&#22914;&#35748;&#30693;&#33021;&#21147;&#12289;&#30693;&#35782;&#25110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#65288;&#27169;&#25311;&#23545;&#19981;&#21516;&#35805;&#39064;&#30340;&#23545;&#35805;&#65289;&#20215;&#20540;&#34920;&#36798;&#31283;&#23450;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#65289;&#21644;&#34892;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#27979;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#20116;&#20010;&#23478;&#26063;&#30340;19&#20010;&#24320;&#28304;LLM&#12290;&#20511;&#37492;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31561;&#32423;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#20928;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20445;&#25345;&#20854;&#24615;&#33021;&#24182;&#20943;&#36731;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;&#38544;&#31169;&#20405;&#29359;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.14845</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20928;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Purifying Large Language Models by Ensembling a Small Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14845
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#20928;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20445;&#25345;&#20854;&#24615;&#33021;&#24182;&#20943;&#36731;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;&#38544;&#31169;&#20405;&#29359;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20174;&#22806;&#37096;&#65288;&#19981;&#21463;&#20449;&#20219;&#65289;&#26469;&#28304;&#25910;&#38598;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#36827;&#34892;&#25968;&#25454;&#28165;&#27927;&#21644;&#31934;&#24515;&#31574;&#21010;&#65292;&#20294;&#24050;&#26377;&#25253;&#36947;&#26174;&#31034;&#26500;&#24314;&#33391;&#22909;&#30340;LLMs&#23384;&#22312;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;/&#25110;&#38544;&#31169;&#20405;&#29359;&#38382;&#39064;&#65292;&#36825;&#23558;&#38459;&#30861;LLMs&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;LLMs&#19982;&#33391;&#24615;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#38598;&#25104;&#26469;&#20928;&#21270;LLMs&#20813;&#21463;&#26410;&#32463;&#31579;&#36873;&#25968;&#25454;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#38500;&#20102;&#29702;&#35770;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#20174;&#32463;&#39564;&#35777;&#23454;&#65292;LLMs&#19982;SLMs&#38598;&#25104;&#21487;&#20197;&#26377;&#25928;&#20445;&#25345;LLMs&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#36731;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;&#38544;&#31169;&#20405;&#29359;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14845v1 Announce Type: cross  Abstract: The emerging success of large language models (LLMs) heavily relies on collecting abundant training data from external (untrusted) sources. Despite substantial efforts devoted to data cleaning and curation, well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs. In this study, we propose a simple and easily implementable method for purifying LLMs from the negative effects caused by uncurated data, namely, through ensembling LLMs with benign and small language models (SLMs). Aside from theoretical guarantees, we perform comprehensive experiments to empirically confirm the efficacy of ensembling LLMs with SLMs, which can effectively preserve the performance of LLMs while mitigating issues such as copyright infringement, data poisoning, and privacy violations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREC&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#35843;&#33410;&#21644;&#26102;&#38388;&#24863;&#30693;&#26041;&#24046;&#32553;&#25918;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#25105;&#35843;&#33410;&#30340;&#36864;&#21270;&#21644;&#35757;&#32451;&#19982;&#37319;&#26679;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14843</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#35843;&#33410;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Text Diffusion with Reinforced Conditioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14843
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREC&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#35843;&#33410;&#21644;&#26102;&#38388;&#24863;&#30693;&#26041;&#24046;&#32553;&#25918;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#25105;&#35843;&#33410;&#30340;&#36864;&#21270;&#21644;&#35757;&#32451;&#19982;&#37319;&#26679;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#30001;&#20110;&#22312;&#36845;&#20195;&#25913;&#36827;&#20013;&#30340;&#36866;&#24212;&#24615;&#65292;&#23427;&#20204;&#23545;&#23454;&#29616;&#26356;&#22909;&#30340;&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#29983;&#25104;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22788;&#29702;&#35821;&#35328;&#30340;&#31163;&#25955;&#24615;&#30340;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#25991;&#23545;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#25105;&#35843;&#33410;&#30340;&#36864;&#21270;&#21644;&#35757;&#32451;&#19982;&#37319;&#26679;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TREC&#30340;&#26032;&#22411;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#35843;&#33410;&#32531;&#35299;&#20102;&#36864;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#26102;&#38388;&#24863;&#30693;&#26041;&#24046;&#32553;&#25918;&#35299;&#20915;&#20102;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;TREC&#22312;&#33258;&#22238;&#24402;&#12289;&#38750;&#33258;&#22238;&#24402;&#21644;&#25193;&#25955;&#22522;&#32447;&#20013;&#30340;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#23450;&#24615;&#20998;&#26512;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14843v1 Announce Type: cross  Abstract: Diffusion models have demonstrated exceptional capability in generating high-quality images, videos, and audio. Due to their adaptiveness in iterative refinement, they provide a strong potential for achieving better non-autoregressive sequence generation. However, existing text diffusion models still fall short in their performance due to a challenge in handling the discreteness of language. This paper thoroughly analyzes text diffusion models and uncovers two significant limitations: degradation of self-conditioning during training and misalignment between training and sampling. Motivated by our findings, we propose a novel Text Diffusion model called TREC, which mitigates the degradation with Reinforced Conditioning and the misalignment by Time-Aware Variance Scaling. Our extensive experiments demonstrate the competitiveness of TREC against autoregressive, non-autoregressive, and diffusion baselines. Moreover, qualitative analysis sh
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;RJUA-MedDQA&#65292;&#19968;&#20010;&#21307;&#23398;&#19987;&#19994;&#39046;&#22495;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35201;&#27714;&#65292;&#28041;&#21450;&#35299;&#37322;&#22270;&#20687;&#20869;&#23481;&#12289;&#25968;&#20540;&#25512;&#29702;&#21644;&#20020;&#24202;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14840</link><description>&lt;p&gt;
RJUA-MedDQA&#65306;&#21307;&#30103;&#25991;&#26723;&#38382;&#31572;&#21644;&#20020;&#24202;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14840
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;RJUA-MedDQA&#65292;&#19968;&#20010;&#21307;&#23398;&#19987;&#19994;&#39046;&#22495;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35201;&#27714;&#65292;&#28041;&#21450;&#35299;&#37322;&#22270;&#20687;&#20869;&#23481;&#12289;&#25968;&#20540;&#25512;&#29702;&#21644;&#20020;&#24202;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#26174;&#31034;&#20986;&#22312;&#21508;&#31181;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#65292;&#27604;&#22914;&#26234;&#33021;&#21307;&#23398;&#35786;&#26029;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22522;&#20934;&#24182;&#26410;&#21453;&#26144;&#20986;&#30495;&#23454;&#21307;&#30103;&#25253;&#21578;&#30340;&#22797;&#26434;&#24615;&#21644;&#19987;&#19994;&#30340;&#28145;&#20837;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RJUA-MedDQA&#65292;&#36825;&#26159;&#21307;&#23398;&#19987;&#19994;&#39046;&#22495;&#30340;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#25361;&#25112;&#65306;&#20840;&#38754;&#35299;&#37322;&#19981;&#21516;&#25361;&#25112;&#24615;&#24067;&#23616;&#20013;&#30340;&#22270;&#20687;&#20869;&#23481;&#65292;&#20855;&#22791;&#35782;&#21035;&#24322;&#24120;&#25351;&#26631;&#30340;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20020;&#24202;&#25512;&#29702;&#33021;&#21147;&#65292;&#26681;&#25454;&#21307;&#23398;&#32972;&#26223;&#25552;&#20379;&#30142;&#30149;&#35786;&#26029;&#12289;&#29366;&#24577;&#21644;&#24314;&#35758;&#30340;&#38472;&#36848;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;&#26088;&#22312;&#24674;&#22797;&#25991;&#26412;&#21644;&#34920;&#26684;&#30340;&#39640;&#25928;&#32467;&#26500;&#24674;&#22797;&#27880;&#37322;&#65288;ESRA&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14840v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) and Large Multi-modal Models (LMMs) have shown potential in various medical applications, such as Intelligent Medical Diagnosis. Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities. In this work, we introduced RJUA-MedDQA, a comprehensive benchmark in the field of medical specialization, which poses several challenges: comprehensively interpreting imgage content across diverse challenging layouts, possessing numerical reasoning ability to identify abnormal indicators and demonstrating clinical reasoning ability to provide statements of disease diagnosis, status and advice based on medical contexts. We carefully design the data generation pipeline and proposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed at restoring textual and tabu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#35821;&#20041;&#21644;&#21477;&#27861;&#20004;&#20010;&#26041;&#38754;&#29992;&#20110;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#20934;&#30830;&#24230;&#30340;AI&#27169;&#22411;&#65292;&#22312;M4&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14838</link><description>&lt;p&gt;
RFBES&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#24212;&#29992;&#65306;&#25506;&#31350;&#29992;&#20110;&#21306;&#20998;AI&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#35821;&#20041;&#21644;&#21477;&#27861;&#20004;&#20010;&#26041;&#38754;&#29992;&#20110;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#20934;&#30830;&#24230;&#30340;AI&#27169;&#22411;&#65292;&#22312;M4&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#24182;&#19988;LLMs&#24050;&#34987;&#29992;&#20110;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#19981;&#21516;&#20219;&#21153;&#20013;&#29983;&#25104;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35895;&#27468;&#21644;OpenAI&#31561;&#30693;&#21517;&#20844;&#21496;&#30340;&#21442;&#19982;&#65292;LLMs&#29616;&#22312;&#26356;&#26131;&#33719;&#24471;&#65292;&#20154;&#20204;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#21306;&#21035;&#12290;&#26412;&#25991;&#20174;&#35821;&#20041;&#21644;&#21477;&#27861;&#20004;&#20010;&#26041;&#38754;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#38382;&#39064;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;M4&#25968;&#25454;&#38598;&#19978;&#39640;&#20934;&#30830;&#24230;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#65292;&#26080;&#35770;&#26159;&#22810;&#35821;&#35328;&#36824;&#26159;&#21333;&#35821;&#20219;&#21153;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#20351;&#29992;&#35821;&#20041;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#26356;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#22312;&#21477;&#27861;&#26041;&#27861;&#19978;&#36824;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#65292;&#36825;&#23558;&#26159;&#26410;&#26469;&#24037;&#20316;&#30340;&#19968;&#20010;&#33391;&#22909;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14838v1 Announce Type: cross  Abstract: Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs have been used to generate texts in different languages and for different tasks. Additionally, due to the participation of remarkable companies such as Google and OpenAI, LLMs are now more accessible, and people can easily use them. However, an important issue is how we can detect AI-generated texts from human-written ones. In this article, we have investigated the problem of AI-generated text detection from two different aspects: semantics and syntax. Finally, we presented an AI model that can distinguish AI-generated texts from human-written ones with high accuracy on both multilingual and monolingual tasks using the M4 dataset. According to our results, using a semantic approach would be more helpful for detection. However, there is a lot of room for improvement in the syntactic approach, and it would be a good approach for future work.
&lt;/p&gt;</description></item><item><title>&#32534;&#21046;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25216;&#26415;&#28165;&#21333;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20174;&#19994;&#32773;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.14837</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25216;&#26415;&#30340;&#23454;&#35777;&#20998;&#31867;&#65306;&#20174;&#19994;&#32773;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
An Empirical Categorization of Prompting Techniques for Large Language Models: A Practitioner's Guide
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14837
&lt;/p&gt;
&lt;p&gt;
&#32534;&#21046;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25216;&#26415;&#28165;&#21333;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20174;&#19994;&#32773;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26368;&#36817;&#29992;&#25552;&#31034;&#35821;&#26469;&#32534;&#31243;&#36825;&#20123;&#27169;&#22411;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;&#25968;&#37327;&#24222;&#22823;&#65292;&#23545;&#20110;&#24076;&#26395;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#30340;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#36825;&#26500;&#25104;&#20102;&#19968;&#20010;&#20196;&#20154;&#38590;&#20197;&#24212;&#23545;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;LLMs&#65292;&#32534;&#21046;&#19968;&#20010;&#20840;&#38754;&#30340;&#25552;&#31034;&#25216;&#26415;&#28165;&#21333;&#24182;&#24314;&#31435;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#36328;&#23398;&#31185;&#20998;&#31867;&#26694;&#26550;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#19968;&#20123;&#26368;&#30693;&#21517;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20174;&#23398;&#26415;&#21644;&#23454;&#36341;&#35282;&#24230;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20998;&#20026;&#19971;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#27599;&#20010;&#31867;&#21035;&#65292;&#26088;&#22312;&#28548;&#28165;&#23427;&#20204;&#30340;&#29420;&#29305;&#36129;&#29486;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#31034;&#20363;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20197;&#20026;&#21516;&#34892;&#20174;&#19994;&#32773;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#26694;&#26550;&#65292;&#24110;&#21161;&#20182;&#20204;&#29702;&#35299;&#21644;&#24402;&#31867;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14837v1 Announce Type: cross  Abstract: Due to rapid advancements in the development of Large Language Models (LLMs), programming these models with prompts has recently gained significant attention. However, the sheer number of available prompt engineering techniques creates an overwhelming landscape for practitioners looking to utilize these tools. For the most efficient and effective use of LLMs, it is important to compile a comprehensive list of prompting techniques and establish a standardized, interdisciplinary categorization framework. In this survey, we examine some of the most well-known prompting techniques from both academic and practical viewpoints and classify them into seven distinct categories. We present an overview of each category, aiming to clarify their unique contributions and showcase their practical applications in real-world examples in order to equip fellow practitioners with a structured framework for understanding and categorizing prompting techniqu
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#38544;&#31192;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#25991;&#26412;&#20869;&#23481;&#22312;&#19981;&#24178;&#39044;&#27169;&#22411;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#29289;&#21697;&#30340;&#26333;&#20809;&#24230;&#65292;&#32780;&#36825;&#31181;&#25915;&#20987;&#23545;&#25972;&#20307;&#25512;&#33616;&#24615;&#33021;&#26080;&#24433;&#21709;&#19988;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#12290;</title><link>https://arxiv.org/abs/2402.14836</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#20013;&#30340;&#38544;&#31192;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Stealthy Attack on Large Language Model based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14836
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#38544;&#31192;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#25991;&#26412;&#20869;&#23481;&#22312;&#19981;&#24178;&#39044;&#27169;&#22411;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#29289;&#21697;&#30340;&#26333;&#20809;&#24230;&#65292;&#32780;&#36825;&#31181;&#25915;&#20987;&#23545;&#25972;&#20307;&#25512;&#33616;&#24615;&#33021;&#26080;&#24433;&#21709;&#19988;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25512;&#21160;&#25512;&#33616;&#31995;&#32479;(RS)&#30340;&#36827;&#23637;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#31995;&#32479;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#23433;&#20840;&#23041;&#32961;&#30340;&#25935;&#24863;&#24615;&#21364;&#34987;&#22823;&#22810;&#24573;&#35270;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLMs&#24341;&#20837;&#25512;&#33616;&#27169;&#22411;&#20013;&#20135;&#29983;&#26032;&#23433;&#20840;&#28431;&#27934;&#30340;&#24773;&#20917;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#27880;&#37325;&#29289;&#21697;&#30340;&#25991;&#26412;&#20869;&#23481;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#27979;&#35797;&#38454;&#27573;&#20165;&#36890;&#36807;&#25913;&#21464;&#29289;&#21697;&#30340;&#25991;&#26412;&#20869;&#23481;&#26174;&#33879;&#22686;&#21152;&#20854;&#26333;&#20809;&#24230;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#24178;&#39044;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#26174;&#33879;&#30340;&#38544;&#31192;&#24615;&#65292;&#22240;&#20026;&#23427;&#19981;&#20250;&#24433;&#21709;&#25972;&#20307;&#25512;&#33616;&#24615;&#33021;&#65292;&#23545;&#25991;&#26412;&#30340;&#20462;&#25913;&#24494;&#22937;&#65292;&#20351;&#29992;&#25143;&#21644;&#24179;&#21488;&#38590;&#20197;&#26816;&#27979;&#21040;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20027;&#27969;&#30340;LLM-based&#25512;&#33616;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14836v1 Announce Type: cross  Abstract: Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item's exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model's training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior
&lt;/p&gt;</description></item><item><title>MIKE&#26159;&#19968;&#20010;&#38024;&#23545;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#30340;&#20840;&#38754;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#31361;&#30772;&#20102;&#29616;&#26377;&#22522;&#20934;&#20027;&#35201;&#20391;&#37325;&#20110;&#31895;&#31890;&#24230;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#24418;&#24335;&#20197;&#35780;&#20272;&#32534;&#36753;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14835</link><description>&lt;p&gt;
MIKE&#65306;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14835
&lt;/p&gt;
&lt;p&gt;
MIKE&#26159;&#19968;&#20010;&#38024;&#23545;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#30340;&#20840;&#38754;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#31361;&#30772;&#20102;&#29616;&#26377;&#22522;&#20934;&#20027;&#35201;&#20391;&#37325;&#20110;&#31895;&#31890;&#24230;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#24418;&#24335;&#20197;&#35780;&#20272;&#32534;&#36753;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#32534;&#36753;&#26159;&#22686;&#24378;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21151;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#23613;&#31649;&#20854;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#24403;&#21069;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#30693;&#35782;&#19978;&#65292;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#22823;&#22810;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MIKE&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14835v1 Announce Type: cross  Abstract: Multimodal knowledge editing represents a critical advancement in enhancing the capabilities of Multimodal Large Language Models (MLLMs). Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored. This gap presents a notable challenge, as FG entity recognition is pivotal for the practical deployment and effectiveness of MLLMs in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a comprehensive benchmark and dataset specifically designed for the FG multimodal entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess different perspectives, including Vanilla Name Answering, Entity-Level Caption, and Complex-Scenario Recognition. In addition, a new form of knowledge editing, Multi-step Editing, is introduced to evaluate the editing efficiency. Through our extensive evaluations
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#26469;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;</title><link>https://arxiv.org/abs/2402.14834</link><description>&lt;p&gt;
MSynFD: &#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MSynFD: Multi-hop Syntax aware Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14834
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#26469;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#24191;&#27867;&#20256;&#25773;&#21161;&#38271;&#20102;&#20551;&#26032;&#38395;&#30340;&#24555;&#36895;&#20256;&#25773;&#65292;&#23545;&#25105;&#20204;&#30340;&#29616;&#23454;&#31038;&#20250;&#26500;&#25104;&#23041;&#32961;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#65292;&#36890;&#36807;&#20998;&#26512;&#26032;&#38395;&#20869;&#23481;&#21644;/&#25110;&#20854;&#31038;&#20250;&#32972;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#20102;&#22522;&#26412;&#30340;&#25991;&#26412;&#26032;&#38395;&#20869;&#23481;&#65288;&#25991;&#31456;&#65289;&#65292;&#24182;&#19988;&#36807;&#20998;&#20381;&#36182;&#24207;&#21015;&#24314;&#27169;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#26469;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#22797;&#26434;&#12289;&#24494;&#22937;&#30340;&#36716;&#25240;&#65292;&#27604;&#22914;&#21477;&#27861;-&#35821;&#20041;&#19981;&#21305;&#37197;&#21644;&#20808;&#39564;&#20559;&#24046;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#20302;&#65292;&#24182;&#22312;&#32570;&#22833;&#27169;&#24577;&#25110;&#31038;&#20250;&#32972;&#26223;&#26102;&#21487;&#33021;&#22833;&#36133;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#37325;&#35201;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65288;MSynFD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#65292;&#20197;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14834v1 Announce Type: cross  Abstract: The proliferation of social media platforms has fueled the rapid dissemination of fake news, posing threats to our real-life society. Existing methods use multimodal data or contextual information to enhance the detection of fake news by analyzing news content and/or its social context. However, these methods often overlook essential textual news content (articles) and heavily rely on sequential modeling and global attention to extract semantic information. These existing methods fail to handle the complex, subtle twists in news articles, such as syntax-semantics mismatches and prior biases, leading to lower performance and potential failure when modalities or social context are missing. To bridge these significant gaps, we propose a novel multi-hop syntax aware fake news detection (MSynFD) method, which incorporates complementary syntax information to deal with subtle twists in fake news. Specifically, we introduce a syntactical depen
&lt;/p&gt;</description></item><item><title>CliqueParcel&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#25209;&#22788;&#29702;&#26469;&#25552;&#39640;LLM&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#36755;&#20986;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#25240;&#20215;&#36755;&#20986;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14833</link><description>&lt;p&gt;
CliqueParcel&#65306;&#19968;&#31181;&#21516;&#26102;&#20248;&#21270;&#25928;&#29575;&#21644;&#24544;&#23454;&#24230;&#30340;&#25209;&#22788;&#29702;LLM&#25552;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14833
&lt;/p&gt;
&lt;p&gt;
CliqueParcel&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#25209;&#22788;&#29702;&#26469;&#25552;&#39640;LLM&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#36755;&#20986;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#25240;&#20215;&#36755;&#20986;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;LLM&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CliqueParcel&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#25552;&#31034;&#25209;&#22788;&#29702;&#26469;&#25552;&#39640;LLM&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#20248;&#21270;&#25512;&#29702;&#25928;&#29575;&#30340;&#31574;&#30053;&#36890;&#24120;&#20250;&#23545;&#36755;&#20986;&#36136;&#37327;&#36827;&#34892;&#22949;&#21327;&#65292;&#23548;&#33268;&#25240;&#20215;&#36755;&#20986;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#25110;&#36755;&#20986;&#32570;&#20047;&#32454;&#33410;&#12290;CliqueParcel&#26159;&#25105;&#20204;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#22238;&#24212;&#12290;&#22312;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#36755;&#20986;&#30340;&#20559;&#24046;&#65288;&#21363;&#24544;&#23454;&#24230;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#20026;&#20102;&#22880;&#23450;&#22522;&#30784;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25490;&#38500;&#30001;&#20110;&#38271;&#24230;&#32553;&#30701;&#32780;&#23548;&#33268;&#30340;&#36816;&#34892;&#26102;&#38388;&#20943;&#23569;&#26469;&#37325;&#26032;&#23450;&#20041;&#25928;&#29575;&#27979;&#37327;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25928;&#29575;&#21644;&#24544;&#23454;&#24230;&#20043;&#38388;&#30340;&#20840;&#38754;&#26435;&#34913;&#65292;&#20197;&#38416;&#26126;&#8220;&#25240;&#20215;&#36755;&#20986;&#8221;&#38382;&#39064;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14833v1 Announce Type: cross  Abstract: Large language models (LLMs) have become pivotal in recent research. However, during the inference process, LLMs still require substantial resources. In this paper, we propose CliqueParcel, a method designed to improve the efficiency of LLMs via prompt batching. Existing strategies to optimize inference efficiency often compromise on output quality, leading to a discounted output problem. This issue might result in reduced accuracy or outputs that are less detailed. CliqueParcel is our answer to this challenge. While ensuring accuracy and minimizing deviations from the original outputs (i.e., faithfulness), our method significantly improves efficiency during inference.   To lay the groundwork, we first redefine efficiency measurements by excluding the reduction in running time due to shorter lengths. Then, we provide a comprehensive trade-off between efficiency and faithfulness to clarify the nature of the 'discounted output' problem. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Mistral-7B&#30340;70&#20159;&#21442;&#25968;&#30340;Orca-Math&#23567;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#22312;&#23567;&#23398;&#25968;&#23398;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14830</link><description>&lt;p&gt;
Orca-Math&#65306;&#37322;&#25918;&#23567;&#35821;&#35328;&#27169;&#22411;&#22312;&#23567;&#23398;&#25968;&#23398;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Orca-Math: Unlocking the potential of SLMs in Grade School Math
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14830
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Mistral-7B&#30340;70&#20159;&#21442;&#25968;&#30340;Orca-Math&#23567;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#22312;&#23567;&#23398;&#25968;&#23398;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#35299;&#20915;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#38754;&#20020;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#20551;&#35774;&#65292;&#20026;&#20102;&#22312;GSM8K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#36807;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#26368;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#38656;&#35201;&#20026;340&#20159;&#20010;&#21442;&#25968;&#12290;&#20026;&#20102;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#36798;&#21040;&#36825;&#19968;&#24615;&#33021;&#27700;&#24179;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#35757;&#32451;SLMs&#29983;&#25104;Python&#20195;&#30721;&#25110;&#20351;&#29992;&#24037;&#20855;&#26469;&#24110;&#21161;&#36991;&#20813;&#35745;&#31639;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#20351;&#29992;&#38598;&#25104;&#65292;&#23558;&#22810;&#36798;100&#27425;&#27169;&#22411;&#36816;&#34892;&#30340;&#36755;&#20986;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#36873;&#25321;&#26159;&#36890;&#36807;&#20849;&#35782;&#12289;&#22810;&#25968;&#25237;&#31080;&#25110;&#19982;SLM&#19968;&#36215;&#20351;&#29992;&#30340;&#21333;&#29420;&#30340;&#39564;&#35777;&#22120;&#27169;&#22411;&#26469;&#36827;&#34892;&#30340;&#12290;&#38598;&#25104;&#22823;&#22823;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20294;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#23545;&#27169;&#22411;&#30340;&#22810;&#27425;&#35843;&#29992;&#36896;&#25104;&#30340;&#26174;&#33879;&#25104;&#26412;&#22686;&#21152;&#65288;&#20363;&#22914;&#65292;Phi-GSM&#20351;&#29992;&#21069;48&#20010;&#26469;&#23558;&#24615;&#33021;&#20174;68.2&#25552;&#21319;&#21040;81.5&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Orca-Math&#65292;&#19968;&#20010;&#22522;&#20110;Mistral-7B&#30340;70&#20159;&#21442;&#25968;SLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14830v1 Announce Type: cross  Abstract: Mathematical word problem-solving has long been recognized as a complex task for small language models (SLMs). A recent study hypothesized that the smallest model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34 billion parameters. To reach this level of performance with smaller models, researcher often train SLMs to generate Python code or use tools to help avoid calculation errors. Additionally, they employ ensembling, where outputs of up to 100 model runs are combined to arrive at a more accurate result. Result selection is done using consensus, majority vote or a separate a verifier model used in conjunction with the SLM. Ensembling provides a substantial boost in accuracy but at a significant cost increase with multiple calls to the model (e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).   In this work, we present Orca-Math, a 7-billion-parameter SLM based on the Mistral-7B, which achie
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31383;&#21475;&#26041;&#27861;&#21644;&#33410;&#28857;&#20248;&#21270;&#20998;&#26512;EHG&#20449;&#21495;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20248;&#21270;&#22922;&#23072;&#21644;&#20998;&#23081;&#20013;&#30340;&#23376;&#23467;&#21516;&#27493;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.14827</link><description>&lt;p&gt;
&#36890;&#36807;&#31383;&#21475;&#36873;&#25321;&#21644;&#33410;&#28857;&#20248;&#21270;&#20248;&#21270;&#22922;&#23072;&#21644;&#20998;&#23081;&#20013;&#30340;&#23376;&#23467;&#21516;&#27493;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Optimizing Uterine Synchronization Analysis in Pregnancy and Labor through Window Selection and Node Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14827
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31383;&#21475;&#26041;&#27861;&#21644;&#33410;&#28857;&#20248;&#21270;&#20998;&#26512;EHG&#20449;&#21495;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20248;&#21270;&#22922;&#23072;&#21644;&#20998;&#23081;&#20013;&#30340;&#23376;&#23467;&#21516;&#27493;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#20135;&#26159;&#20840;&#29699;5&#23681;&#20197;&#19979;&#20799;&#31461;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35760;&#24405;&#22312;&#21171;&#21160;&#21644;&#22922;&#23072;&#26399;&#38388;&#27597;&#20146;&#33145;&#37096;&#30340;EHG&#20449;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;EHG&#20449;&#21495;&#21453;&#26144;&#20102;&#35825;&#23548;&#23376;&#23467;&#32908;&#30005;&#26426;&#25910;&#32553;&#30340;&#30005;&#27963;&#21160;&#12290;&#30001;&#20110;EHG&#20449;&#21495;&#34987;&#35748;&#20026;&#26159;&#38750;&#24179;&#31283;&#20449;&#21495;&#65292;&#24182;&#19988;&#25105;&#20204;&#39044;&#26399;&#22312;&#25910;&#32553;&#36807;&#31243;&#20013;&#36830;&#25509;&#24615;&#20250;&#21457;&#29983;&#25913;&#21464;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#31383;&#21475;&#26041;&#27861;&#22312;&#23454;&#38469;&#20449;&#21495;&#19978;&#65292;&#20197;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#29992;&#20110;&#20998;&#31867;&#30340;&#26368;&#20339;&#31383;&#21475;&#21644;&#26368;&#20339;&#33410;&#28857;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14827v1 Announce Type: cross  Abstract: Preterm labor (PL) has globally become the leading cause of death in children under the age of 5 years. To address this problem, this paper will provide a new approach by analyzing the EHG signals, which are recorded on the abdomen of the mother during labor and pregnancy. The EHG signal reflects the electrical activity that induces the mechanical contraction of the myometrium. Because EHGs are known to be non-stationary signals, and because we anticipate connectivity to alter during contraction, we applied the windowing approach on real signals to help us identify the best windows and the best nodes with the most significant data to be used for classification. The suggested pipeline includes i) divide the 16 EHG signals that are recorded from the abdomen of pregnant women in N windows; ii) apply the connectivity matrices on each window; iii) apply the Graph theory-based measures on the connectivity matrices on each window; iv) apply t
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;ConceptMath&#65292;&#19968;&#31181;&#21452;&#35821;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#22833;&#36133;&#12290;</title><link>https://arxiv.org/abs/2402.14660</link><description>&lt;p&gt;
ConceptMath&#65306;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#21452;&#35821;&#27010;&#24565;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14660
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;ConceptMath&#65292;&#19968;&#31181;&#21452;&#35821;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ConceptMath&#65292;&#36825;&#26159;&#19968;&#20010;&#21452;&#35821;&#65288;&#33521;&#35821;&#21644;&#20013;&#25991;&#65289;&#65292;&#32454;&#31890;&#24230;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#35780;&#20272;&#19968;&#33324;&#25968;&#23398;&#25512;&#29702;&#30340;&#20256;&#32479;&#22522;&#20934;&#19981;&#21516;&#65292;ConceptMath&#23558;&#25968;&#23398;&#38382;&#39064;&#31995;&#32479;&#22320;&#32452;&#32455;&#22312;&#25968;&#23398;&#27010;&#24565;&#30340;&#23618;&#27425;&#32467;&#26500;&#19979;&#65292;&#20174;&#32780;&#21487;&#20197;&#20197;&#27010;&#24565;&#20026;&#21333;&#20301;&#20934;&#30830;&#24615;&#35780;&#20272;&#25968;&#23398;&#25512;&#29702;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;ConceptMath&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24191;&#27867;&#33539;&#22260;&#30340;LLMs&#65292;&#24182;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;LLMs&#23613;&#31649;&#22312;&#20256;&#32479;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#39640;&#24179;&#22343;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#20005;&#37325;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24494;&#35843;&#31574;&#30053;&#26469;&#22686;&#24378;&#29616;&#26377;LLMs&#30340;&#24369;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24076;&#26395;ConceptMath&#33021;&#22815;&#25351;&#23548;&#24320;&#21457;&#32773;&#29702;&#35299;&#32454;&#33268;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14660v1 Announce Type: cross  Abstract: This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grai
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#30340;&#20132;&#21449;&#30740;&#31350;&#23558;&#23545;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#26684;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.14601</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#25945;&#32946;&#20013;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bringing Generative AI to Adaptive Learning in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14601
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#30340;&#20132;&#21449;&#30740;&#31350;&#23558;&#23545;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#26684;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#28608;&#22686;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#21457;&#23637;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#36825;&#19968;&#27010;&#24565;&#22312;&#25945;&#32946;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#25552;&#39640;&#23398;&#29983;&#23398;&#20064;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#36890;&#36807;&#35752;&#35770;&#36825;&#19968;&#39046;&#22495;&#30340;&#22909;&#22788;&#12289;&#25361;&#25112;&#21644;&#28508;&#21147;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#32467;&#21512;&#23558;&#20026;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#24418;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14601v1 Announce Type: cross  Abstract: The recent surge in generative AI technologies, such as large language models and diffusion models, have boosted the development of AI applications in various domains, including science, finance, and education. Concurrently, adaptive learning, a concept that has gained substantial interest in the educational sphere, has proven its efficacy in enhancing students' learning efficiency. In this position paper, we aim to shed light on the intersectional studies of these two methods, which combine generative AI with adaptive learning concepts. By presenting discussions about the benefits, challenges, and potentials in this field, we argue that this union will contribute significantly to the development of the next stage learning format in education.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>RepoGenix&#29420;&#29305;&#34701;&#21512;&#31867;&#27604;&#19978;&#19979;&#25991;&#21644;&#29702;&#24615;&#19978;&#19979;&#25991;&#65292;&#24182;&#25552;&#20986;&#20102;&#25130;&#26029;&#25490;&#21517;&#29983;&#25104;&#65288;RTG&#65289;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#20179;&#24211;&#32423;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#30340;&#20934;&#30830;&#24615;&#32780;&#19981;&#29306;&#29298;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14323</link><description>&lt;p&gt;
REPOFUSE&#65306;&#20855;&#26377;&#34701;&#21512;&#21452;&#37325;&#19978;&#19979;&#25991;&#30340;&#20179;&#24211;&#32423;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
REPOFUSE: Repository-Level Code Completion with Fused Dual Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14323
&lt;/p&gt;
&lt;p&gt;
RepoGenix&#29420;&#29305;&#34701;&#21512;&#31867;&#27604;&#19978;&#19979;&#25991;&#21644;&#29702;&#24615;&#19978;&#19979;&#25991;&#65292;&#24182;&#25552;&#20986;&#20102;&#25130;&#26029;&#25490;&#21517;&#29983;&#25104;&#65288;RTG&#65289;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#20179;&#24211;&#32423;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#30340;&#20934;&#30830;&#24615;&#32780;&#19981;&#29306;&#29298;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#36741;&#21161;&#26041;&#38754;&#21462;&#24471;&#30340;&#25104;&#21151;&#25512;&#21160;&#20102;&#25552;&#20986;&#20179;&#24211;&#32423;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#20316;&#20026;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25163;&#27573;&#65292;&#21033;&#29992;&#25972;&#20010;&#20195;&#30721;&#24211;&#30340;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#22686;&#21152;&#25512;&#29702;&#24310;&#36831;&#65292;&#28508;&#22312;&#22320;&#25439;&#23475;&#24320;&#21457;&#32773;&#20307;&#39564;&#24182;&#22952;&#30861;&#24037;&#20855;&#30340;&#37319;&#29992;-&#36825;&#26159;&#25105;&#20204;&#31216;&#20043;&#20026;&#19978;&#19979;&#25991;-&#24310;&#36831;&#22256;&#22659;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; RepoGenix&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#20179;&#24211;&#32423;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#32780;&#26080;&#38656;&#24310;&#36831;&#25240;&#34935;&#30340;&#24320;&#21019;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;RepoGenix &#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#19978;&#19979;&#25991;&#65306;&#26681;&#26893;&#20110;&#20195;&#30721;&#31867;&#27604;&#30340;&#31867;&#27604;&#19978;&#19979;&#25991;&#21644;&#21253;&#21547;&#28145;&#24230;&#35821;&#20041;&#20851;&#31995;&#30340;&#29702;&#24615;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25130;&#26029;&#25490;&#21517;&#29983;&#25104;&#65288;RTG&#65289;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#38480;&#21046;&#22823;&#23567;&#30340;&#25552;&#31034;&#12290;&#36825;&#20351;&#24471; RepoGenix &#33021;&#22815;&#22312;&#20445;&#25345;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#25552;&#20379;&#31934;&#30830;&#30340;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14323v1 Announce Type: cross  Abstract: The success of language models in code assistance has spurred the proposal of repository-level code completion as a means to enhance prediction accuracy, utilizing the context from the entire codebase. However, this amplified context can inadvertently increase inference latency, potentially undermining the developer experience and deterring tool adoption-a challenge we termed the Context-Latency Conundrum. This paper introduces RepoGenix, a pioneering solution designed to enhance repository-level code completion without the latency trade-off. RepoGenix uniquely fuses two types of contexts: the analogy context, rooted in code analogies, and the rationale context, which encompasses in-depth semantic relationships. We propose a novel rank truncated generation (RTG) technique that efficiently condenses these contexts into prompts with restricted size. This enables RepoGenix to deliver precise code completions while maintaining inference ef
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14208</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#20869;&#23481;&#26465;&#20214;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Conditional Debiasing for Fair Text Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14208
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20844;&#24179;&#30340;&#25991;&#26412;&#23884;&#20837;&#19978;&#65292;&#36825;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25928;&#29992;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#20855;&#26377;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#20294;&#30456;&#21516;&#20869;&#23481;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#19982;&#20854;&#23545;&#24212;&#20013;&#31435;&#25991;&#26412;&#30340;&#23884;&#20837;&#20445;&#25345;&#30456;&#21516;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#25991;&#26412;&#22686;&#24378;&#20026;&#19981;&#21516;&#30340;&#25935;&#24863;&#32452;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20844;&#24179;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#23884;&#20837;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;EyeTrans&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#34701;&#20837;&#26426;&#22120;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14096</link><description>&lt;p&gt;
EyeTrans: &#21512;&#24182;&#20154;&#31867;&#21644;&#26426;&#22120;&#27880;&#24847;&#21147;&#20197;&#23454;&#29616;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
EyeTrans: Merging Human and Machine Attention for Neural Code Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14096
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;EyeTrans&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#34701;&#20837;&#26426;&#22120;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural code summarization &#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#30340;&#31616;&#35201;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#12290;Transformer&#27169;&#22411;&#30340;&#21457;&#23637;&#23548;&#33268;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24191;&#27867;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#34701;&#20837;&#26426;&#22120;&#27880;&#24847;&#21147;&#20197;&#22686;&#24378;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#34701;&#21512;&#24182;&#39564;&#35777;&#36825;&#19968;&#20551;&#35774;&#65292;&#24341;&#20837;&#20102;EyeTrans&#65292;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;(1) &#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30524;&#21160;&#20154;&#31867;&#30740;&#31350;&#65292;&#25910;&#38598;&#21644;&#39044;&#20998;&#26512;&#25968;&#25454;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#65292;(2) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#25972;&#21512;&#20154;&#31867;&#27880;&#24847;&#21147;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14096v1 Announce Type: cross  Abstract: Neural code summarization leverages deep learning models to automatically generate brief natural language summaries of code snippets. The development of Transformer models has led to extensive use of attention during model design. While existing work has primarily and almost exclusively focused on static properties of source code and related structural representations like the Abstract Syntax Tree (AST), few studies have considered human attention, that is, where programmers focus while examining and comprehending code. In this paper, we develop a method for incorporating human attention into machine attention to enhance neural code summarization. To facilitate this incorporation and vindicate this hypothesis, we introduce EyeTrans, which consists of three steps: (1) we conduct an extensive eye-tracking human study to collect and pre-analyze data for model training, (2) we devise a data-centric approach to integrate human attention wit
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13777</link><description>&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65306;&#25945;&#31243;&#12289;&#35843;&#26597;&#21644;&#26410;&#26469;&#26041;&#21521;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13777
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20174;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#26041;&#38754;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20063;&#38656;&#35201;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#29983;&#25104;&#20989;&#25968;&#20316;&#20026;&#31574;&#30053;&#25110;&#25919;&#31574;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#22240;&#27492;&#19981;&#21516;&#20998;&#25903;&#30340;&#21457;&#23637;&#30456;&#23545;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#24212;&#29992;&#26041;&#38754;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#24402;&#19968;&#21270;&#27969;&#12289;&#21464;&#21387;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.13764</link><description>&lt;p&gt;
CriticBench: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#35770;&#23478;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Evaluating Large Language Models as Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13764
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102; CriticBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22235;&#20010;&#20851;&#38190;&#35780;&#35770;&#33021;&#21147;&#32500;&#24230;&#65288;&#21453;&#39304;&#12289;&#27604;&#36739;&#12289;&#25913;&#36827;&#21644;&#20803;&#21453;&#39304;&#65289;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;CriticBench&#21253;&#21547;&#20061;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#36136;&#37327;&#32454;&#31890;&#24230;&#27700;&#24179;&#19978;&#35780;&#35770;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#25581;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#26377;&#36259;&#30340;&#20851;&#31995;&#12290;CriticBench&#30340;&#25968;&#25454;&#38598;&#12289;&#36164;&#28304;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#23558;&#22312;https://github.com/gmftbyGMFTBY/Cri&#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
&lt;/p&gt;</description></item><item><title>DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13711</link><description>&lt;p&gt;
DSLR&#65306;&#22810;&#26679;&#24615;&#22686;&#24378;&#21644;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13711
&lt;/p&gt;
&lt;p&gt;
DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20013;&#22238;&#25918;&#32531;&#20914;&#21306;&#23545;&#22270;&#25345;&#32493;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#25773;&#30340;GCL&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#33410;&#28857;&#24182;&#23558;&#23427;&#20204;&#23384;&#20648;&#22312;&#37325;&#25773;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#20379;&#22312;&#35757;&#32451;&#21518;&#32493;&#20219;&#21153;&#26102;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#32771;&#34385;&#27599;&#20010;&#22238;&#25918;&#33410;&#28857;&#30340;&#31867;&#21035;&#20195;&#34920;&#24615;&#20250;&#20351;&#22238;&#25918;&#33410;&#28857;&#38598;&#20013;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#20013;&#24515;&#21608;&#22260;&#65292;&#21487;&#33021;&#23384;&#22312;&#36807;&#25311;&#21512;&#20110;&#20301;&#20110;&#37027;&#20123;&#21306;&#22495;&#30340;&#33410;&#28857;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#21152;&#21095;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#23569;&#25968;&#22238;&#25918;&#33410;&#28857;&#26469;&#20445;&#30041;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20855;&#26377;&#19981;&#30456;&#20851;&#37051;&#23621;&#30340;&#22238;&#25918;&#33410;&#28857;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#26174;&#30528;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSLR&#30340;GCL&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#65288;CD&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.13352</link><description>&lt;p&gt;
KetGPT -- &#20351;&#29992;Transformer&#23545;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13352
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31639;&#27861;&#65292;&#34920;&#31034;&#20026;&#37327;&#23376;&#30005;&#36335;&#65292;&#21487;&#29992;&#20316;&#35780;&#20272;&#37327;&#23376;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22312;&#35813;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#65292;&#23548;&#33268;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#30005;&#36335;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#30005;&#36335;&#24182;&#19981;&#26159;&#20195;&#34920;&#24615;&#22522;&#20934;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#37327;&#23376;&#31995;&#32479;&#21046;&#36896;&#30340;&#30495;&#23454;&#37327;&#23376;&#31639;&#27861;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;&#36825;&#31181;&#32570;&#20047;&#8220;&#26377;&#29992;&#8221;&#30340;&#37327;&#23376;&#22522;&#20934;&#26500;&#25104;&#20102;&#25512;&#21160;&#37327;&#23376;&#32534;&#35793;&#22120;&#21644;&#30828;&#20214;&#24320;&#21457;&#19982;&#27604;&#36739;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KetGPT&#65292;&#19968;&#31181;&#20197;OpenQASM&#35821;&#35328;&#29983;&#25104;&#21512;&#25104;&#30005;&#36335;&#30340;&#24037;&#20855;&#65292;&#20854;&#32467;&#26500;&#26159;&#22522;&#20110;&#25512;&#23548;&#33258;&#37327;&#23376;&#30005;&#36335;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13352v1 Announce Type: cross  Abstract: Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware.   This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24066;&#22330;&#24433;&#21709;&#21160;&#24577;&#22871;&#26399;&#20445;&#20540;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20984;&#24066;&#22330;&#24433;&#21709;&#21644;&#38543;&#26102;&#38388;&#25345;&#32493;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#19982;&#24120;&#29992;&#31243;&#24207;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#26399;&#26435;&#22871;&#26399;&#20445;&#20540;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13326</link><description>&lt;p&gt;
&#20855;&#26377;&#24066;&#22330;&#24433;&#21709;&#21147;&#30340;&#28145;&#24230;&#22871;&#26399;&#20445;&#20540;
&lt;/p&gt;
&lt;p&gt;
Deep Hedging with Market Impact
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24066;&#22330;&#24433;&#21709;&#21160;&#24577;&#22871;&#26399;&#20445;&#20540;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20984;&#24066;&#22330;&#24433;&#21709;&#21644;&#38543;&#26102;&#38388;&#25345;&#32493;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#19982;&#24120;&#29992;&#31243;&#24207;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#26399;&#26435;&#22871;&#26399;&#20445;&#20540;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22871;&#26399;&#20445;&#20540;&#26159;&#23450;&#26399;&#36827;&#34892;&#37329;&#34701;&#24037;&#20855;&#20132;&#26131;&#65292;&#20197;&#25269;&#28040;&#25237;&#36164;&#25110;&#36127;&#20538;&#25152;&#24102;&#26469;&#39118;&#38505;&#30340;&#23454;&#36341;&#12290;&#21160;&#24577;&#22871;&#26399;&#20445;&#20540;&#20248;&#21270;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65307;&#22240;&#27492;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22871;&#26399;&#20445;&#20540;RL&#24037;&#20316;&#24182;&#26410;&#32771;&#34385;&#30001;&#20132;&#26131;&#24037;&#20855;&#30340;&#26377;&#38480;&#27969;&#21160;&#24615;&#24341;&#36215;&#30340;&#24066;&#22330;&#24433;&#21709;&#12290;&#25972;&#21512;&#36825;&#26679;&#30340;&#29305;&#24449;&#23545;&#20110;&#22312;&#20855;&#26377;&#26377;&#38480;&#27969;&#21160;&#24615;&#30340;&#32929;&#31080;&#26399;&#26435;&#22871;&#26399;&#20445;&#20540;&#26102;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#21487;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#36890;&#29992;&#24066;&#22330;&#24433;&#21709;&#21160;&#24577;&#22871;&#26399;&#20445;&#20540;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20960;&#20010;&#36924;&#30495;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#20984;&#24066;&#22330;&#24433;&#21709;&#21644;&#38543;&#26102;&#38388;&#25345;&#32493;&#30340;&#24433;&#21709;&#12290;&#20174;DRL&#27169;&#22411;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#36890;&#36807;&#20960;&#20010;&#26399;&#26435;&#22871;&#26399;&#20445;&#20540;&#27169;&#25311;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#19982;&#24120;&#29992;&#31243;&#24207;&#65288;&#22914;&#24503;&#23572;&#22612;&#22871;&#26399;&#20445;&#20540;&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13326v1 Announce Type: cross  Abstract: Dynamic hedging is the practice of periodically transacting financial instruments to offset the risk caused by an investment or a liability. Dynamic hedging optimization can be framed as a sequential decision problem; thus, Reinforcement Learning (RL) models were recently proposed to tackle this task. However, existing RL works for hedging do not consider market impact caused by the finite liquidity of traded instruments. Integrating such feature can be crucial to achieve optimal performance when hedging options on stocks with limited liquidity. In this paper, we propose a novel general market impact dynamic hedging model based on Deep Reinforcement Learning (DRL) that considers several realistic features such as convex market impacts, and impact persistence through time. The optimal policy obtained from the DRL model is analysed using several option hedging simulations and compared to commonly used procedures such as delta hedging. Re
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#39318;&#20010;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;MedRAG&#24037;&#20855;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13178</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Retrieval-Augmented Generation for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13178
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#39318;&#20010;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;MedRAG&#24037;&#20855;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#24191;&#27867;&#30340;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#24187;&#35273;&#21644;&#36807;&#26102;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24471;&#21040;&#20102;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;RAG&#31995;&#32479;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#28789;&#27963;&#30340;&#32452;&#20214;&#65292;&#24182;&#19988;&#32570;&#20047;&#20851;&#20110;&#21508;&#31181;&#21307;&#23398;&#30446;&#30340;&#30340;&#26368;&#20339;RAG&#35774;&#32622;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#21019;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;7,663&#20010;&#38382;&#39064;&#12290;&#21033;&#29992;MIRAGE&#65292;&#25105;&#20204;&#36890;&#36807;&#26412;&#25991;&#20171;&#32461;&#30340;MedRAG&#24037;&#20855;&#21253;&#65292;&#22312;41&#31181;&#19981;&#21516;&#35821;&#26009;&#24211;&#12289;&#26816;&#32034;&#22120;&#21644;&#39592;&#24178;LLMs&#30340;&#32452;&#21512;&#19978;&#36827;&#34892;&#20102;&#36229;&#36807;1.8&#19975;&#20159;&#30340;&#25552;&#31034;&#26631;&#35760;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;MedRAG&#25552;&#39640;&#20102;&#20845;&#31181;&#19981;&#21516;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13178v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13035</link><description>&lt;p&gt;
&#23398;&#20064;&#26816;&#26597;&#65306;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#26657;&#27491;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13035
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#19981;&#26029;&#21162;&#21147;&#36890;&#36807;&#33258;&#25105;&#26657;&#27491;&#26469;&#23436;&#21892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27809;&#26377;&#22806;&#37096;&#20934;&#30830;&#30693;&#35782;&#30340;&#33258;&#25105;&#26657;&#27491;&#21487;&#33021;&#23384;&#22312;&#23616;&#38480;&#24615;&#29978;&#33267;&#21487;&#33021;&#36866;&#24471;&#20854;&#21453;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#20851;&#20110;&#33258;&#25105;&#26657;&#27491;&#30340;&#38480;&#21046;&#21644;&#26377;&#25928;&#24615;&#30340;&#30097;&#38382;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLM&#30340;&#33258;&#26816;&#21151;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#38169;&#35823;&#31867;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#65292;&#31216;&#20026;&#8220;Step CoT Check&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#23558;&#21407;&#22987;CoT&#25968;&#25454;&#21644;&#26816;&#26597;&#26657;&#27491;&#25968;&#25454;&#25972;&#21512;&#21518;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#20854;&#33258;&#26816;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#24182;&#28040;&#38500;&#20102;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13035v1 Announce Type: cross  Abstract: Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#24341;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;RiPAMI&#20803;&#25968;&#25454;&#25968;&#25454;&#24211;&#21644;&#20027;&#39064;&#25968;&#25454;&#38598;&#20197;&#33719;&#21462;PAMI&#32508;&#36848;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.12928</link><description>&lt;p&gt;
&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#24341;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;RiPAMI&#20803;&#25968;&#25454;&#25968;&#25454;&#24211;&#21644;&#20027;&#39064;&#25968;&#25454;&#38598;&#20197;&#33719;&#21462;PAMI&#32508;&#36848;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#20998;&#25955;&#30340;&#30693;&#35782;&#65292;&#25991;&#29486;&#32508;&#36848;&#25552;&#20379;&#20102;&#23545;&#25152;&#30740;&#31350;&#20027;&#39064;&#30340;&#20840;&#38754;&#20102;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#65288;PAMI&#65289;&#36825;&#19968;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#36807;&#22810;&#30340;&#32508;&#36848;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#35780;&#35770;&#32773;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#23545;&#36825;&#20123;&#20851;&#27880;&#30340;&#22238;&#24212;&#65292;&#26412;&#25991;&#26088;&#22312;&#20174;&#22810;&#20010;&#35282;&#24230;&#20840;&#38754;&#23457;&#35270;PAMI&#39046;&#22495;&#30340;&#32508;&#36848;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12928v1 Announce Type: cross  Abstract: By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers. In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews. Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords. Second, based on these indicators, th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10877</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#26234;&#33021;&#20307;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust agents learn causal world models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10877
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#30452;&#26377;&#20154;&#20551;&#35774;&#22240;&#26524;&#25512;&#29702;&#22312;&#24378;&#20581;&#19988;&#20855;&#26377;&#36890;&#29992;&#26234;&#33021;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#65292;&#28982;&#32780;&#19981;&#28165;&#26970;&#26234;&#33021;&#20307;&#26159;&#21542;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#25110;&#32773;&#20854;&#20182;&#24402;&#32435;&#20559;&#24046;&#26159;&#21542;&#36275;&#22815;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#34920;&#26126;&#20219;&#20309;&#33021;&#22815;&#22312;&#22823;&#37327;&#20998;&#24067;&#36716;&#21464;&#19979;&#28385;&#36275;&#21518;&#24724;&#30028;&#38480;&#30340;&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#36817;&#20284;&#22240;&#26524;&#27169;&#22411;&#65292;&#23545;&#20110;&#20248;&#21270;&#26234;&#33021;&#20307;&#26469;&#35828;&#65292;&#35813;&#36817;&#20284;&#27169;&#22411;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
&lt;/p&gt;</description></item><item><title>ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.09727</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#20154;&#24037;&#26234;&#33021;&#38405;&#35835;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09727
&lt;/p&gt;
&lt;p&gt;
ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#38480;&#21046;&#22312;&#26576;&#20010;&#26368;&#22823;&#19978;&#19979;&#25991;&#38271;&#24230;&#20869;&#65292;&#32780;&#19988;&#26080;&#27861;&#31283;&#23450;&#22320;&#22788;&#29702;&#38271;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReadAgent&#65292;&#19968;&#20010;&#22686;&#21152;&#20102;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#31995;&#32479;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21487;&#20197;&#36798;&#21040;20&#20493;&#12290;&#21463;&#21040;&#20154;&#31867;&#20132;&#20114;&#24335;&#38405;&#35835;&#38271;&#25991;&#26723;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;ReadAgent&#23454;&#29616;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#21033;&#29992;LLM&#30340;&#39640;&#32423;&#35821;&#35328;&#33021;&#21147;&#26469;&#65306;&#65288;1&#65289;&#20915;&#23450;&#23558;&#21738;&#20123;&#20869;&#23481;&#23384;&#20648;&#22312;&#19968;&#20010;&#35760;&#24518;&#29255;&#27573;&#20013;&#65292;&#65288;2&#65289;&#23558;&#36825;&#20123;&#35760;&#24518;&#29255;&#27573;&#21387;&#32553;&#25104;&#20026;&#31216;&#20026;&#27010;&#35201;&#35760;&#24518;&#30340;&#30701;&#26102;&#35760;&#24518;&#65292;&#65288;3&#65289;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#21407;&#22987;&#25991;&#26412;&#26597;&#25214;&#27573;&#33853;&#26469;&#25552;&#37266;&#33258;&#24049;&#30456;&#20851;&#32454;&#33410;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#12289;&#20351;&#29992;&#21407;&#22987;&#38271;&#19978;&#19979;&#25991;&#20197;&#21450;&#20351;&#29992;&#27010;&#35201;&#35760;&#24518;&#26469;&#35780;&#20272;ReadAgent&#19982;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#35780;&#20272;&#26159;&#22312;&#19977;&#20010;&#38271;&#25991;&#26723;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09727v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension task
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09015</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#22909;&#30340;&#20154;&#26426;&#23545;&#40784;&#26041;&#21521;&#65306;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#20013;&#30340;&#20219;&#21153;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#24212;&#29992;&#36890;&#36807;&#21327;&#21161;&#22810;&#20010;&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#21512;&#20316;&#65292;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#23384;&#22312;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#26159;&#21542;&#30495;&#27491;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#12290;&#36825;&#20984;&#26174;&#20102;&#39564;&#35777;LLM&#39537;&#21160;&#24212;&#29992;&#25928;&#29992;&#30340;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#35201;&#30830;&#20445;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#19982;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AgentEval&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#26045;&#25968;&#23398;&#38382;&#39064;&#30340;&#20272;&#27979;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#20219;&#20309;&#32473;&#23450;&#24212;&#29992;&#31243;&#24207;&#29420;&#29305;&#30446;&#26631;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#12290;&#36825;&#26679;&#21487;&#20197;&#23545;&#24212;&#29992;&#31243;&#24207;&#30340;&#25928;&#29992;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#37327;&#21270;&#20854;&#19982;&#24314;&#35758;&#26631;&#20934;&#30456;&#27604;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09015v1 Announce Type: cross Abstract: The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of 
&lt;/p&gt;</description></item><item><title>UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07939</link><description>&lt;p&gt;
UFO: &#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#20132;&#20114;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
UFO: A UI-Focused Agent for Windows OS Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07939
&lt;/p&gt;
&lt;p&gt;
UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UFO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20102;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;UFO&#37319;&#29992;&#21452;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#31934;&#30830;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#25511;&#21046;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#21333;&#20010;&#24212;&#29992;&#31243;&#24207;&#20869;&#20197;&#21450;&#36328;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23548;&#33322;&#21644;&#25805;&#20316;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#28041;&#21450;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#21160;&#20316;&#36830;&#25509;&#65292;&#24182;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;UFO&#23558;&#33392;&#24040;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#36716;&#21464;&#20026;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#23601;&#21487;&#20197;&#23436;&#25104;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;9&#20010;&#27969;&#34892;&#30340;Windows&#24212;&#29992;&#31243;&#24207;&#19978;&#23545;UFO&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#21453;&#26144;&#29992;&#25143;&#26085;&#24120;&#20351;&#29992;&#24773;&#26223;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#24471;&#20986;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;UFO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
&lt;/p&gt;</description></item><item><title>&#20026;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#19982;&#20998;&#31867;&#30495;&#23454;&#23398;&#29983;&#38382;&#39064;&#65292;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07913</link><description>&lt;p&gt;
&#20026;&#24110;&#21161;&#20013;&#22269;Python&#32534;&#31243;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07913
&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#19982;&#20998;&#31867;&#30495;&#23454;&#23398;&#29983;&#38382;&#39064;&#65292;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#24179;&#21488;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#24555;&#36895;&#22686;&#38271;&#30340;&#35745;&#31639;&#26426;&#32534;&#31243;&#35838;&#31243;&#20013;&#65292;&#35299;&#31572;&#25104;&#21315;&#19978;&#19975;&#23398;&#29983;&#30340;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#25104;&#26412;&#12290;&#20026;&#32534;&#31243;&#25945;&#32946;&#23450;&#21046;&#26234;&#33021;&#21161;&#25163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21019;&#24314;&#38656;&#35201;&#29420;&#29305;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;&#27492;&#31867;LLMs&#30340;&#25968;&#25454;&#36164;&#28304;&#30456;&#23545;&#31232;&#32570;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20026;&#30830;&#20445;&#38382;&#39064;&#30340;&#26469;&#28304;&#30340;&#30495;&#23454;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#23454;&#38469;&#23398;&#29983;&#25552;&#20986;&#30340;&#38382;&#39064;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#31867;&#22411;&#21644;&#23398;&#20064;&#32773;&#30340;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#31181;&#27880;&#37322;&#21407;&#21017;&#26088;&#22312;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#65292;&#20026;&#24320;&#21457;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#25552;&#20379;&#22362;&#23454;&#30340;&#25968;&#25454;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online learning platforms, particularly in rapidly growing computer programming courses, addressing the thousands of students' learning queries requires considerable human cost. The creation of intelligent assistant large language models (LLMs) tailored for programming education necessitates distinct data support. However, in real application scenarios, the data resources for training such LLMs are relatively scarce. Therefore, to address the data scarcity in intelligent educational systems for programming, this paper proposes a new Chinese question-and-answer dataset for Python learners. To ensure the authenticity and reliability of the sources of the questions, we collected questions from actual student questions and categorized them according to various dimensions such as the type of questions and the type of learners. This annotation principle is designed to enhance the effectiveness and quality of online programming education, providing a solid data foundation for developing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07703</link><description>&lt;p&gt;
&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Sequential Decision-Making with Unknown Delays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#39046;&#22495;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#26694;&#26550;&#35299;&#20915;&#20102;&#20855;&#26377;&#24310;&#36831;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#30340;&#21453;&#39304;&#21487;&#33021;&#20197;&#26410;&#30693;&#24310;&#36831;&#21040;&#36798;&#12290;&#19982;&#20043;&#21069;&#20165;&#38480;&#20110;&#27431;&#20960;&#37324;&#24471;&#33539;&#25968;&#21644;&#26799;&#24230;&#20449;&#24687;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#22522;&#20110;&#36817;&#20284;&#35299;&#30340;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25509;&#25910;&#21453;&#39304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22810;&#21151;&#33021;&#19988;&#36866;&#29992;&#20110;&#36890;&#29992;&#33539;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#23436;&#25972;&#25439;&#22833;&#20989;&#25968;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#35268;&#33539;&#21270;&#39046;&#23548;&#31639;&#27861;&#26063;&#65292;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#26799;&#24230;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#38024;&#23545;&#30456;&#24212;&#20915;&#31574;&#28857;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#20540;&#20449;&#24687;&#21453;&#39304;&#30340;&#31616;&#21270;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#12290;&#23545;&#20110;&#27599;&#31181;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21453;&#20107;&#23454;&#20998;&#26512;&#26694;&#26550;&#35780;&#20272;&#20102;&#21313;&#20010;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#31181;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#65292;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.05980</link><description>&lt;p&gt;
&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#32534;&#31243;&#27010;&#24565;&#65311;&#19968;&#31181;&#40657;&#30418;&#26041;&#27861;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do Large Code Models Understand Programming Concepts? A Black-box Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21453;&#20107;&#23454;&#20998;&#26512;&#26694;&#26550;&#35780;&#20272;&#20102;&#21313;&#20010;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#31181;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#65292;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#25104;&#21151;&#20063;&#20351;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#32534;&#30721;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#24037;&#20316;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#34917;&#20840;&#21644;&#32534;&#36753;&#31561;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20026;&#20160;&#20040;&#23427;&#20204;&#33021;&#22815;&#25104;&#21151;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#33258;&#22238;&#24402;&#27169;&#22411;&#23545;&#24213;&#23618;&#31243;&#24207;&#30340;&#36923;&#36753;&#32467;&#26500;&#29702;&#35299;&#31243;&#24230;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#32534;&#31243;&#27010;&#24565;&#35859;&#35789;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;&#65288;CACP&#65289;&#20316;&#20026;&#19968;&#31181;&#21453;&#20107;&#23454;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#32534;&#31243;&#27010;&#24565;&#12290;&#21482;&#36890;&#36807;&#40657;&#30418;&#35775;&#38382;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;CACP&#35780;&#20272;&#20102;&#21313;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#20010;&#19981;&#21516;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models' success on text generation has also made them better at code generation and coding tasks. While a lot of work has demonstrated their remarkable performance on tasks such as code completion and editing, it is still unclear as to why. We help bridge this gap by exploring to what degree auto-regressive models understand the logical constructs of the underlying programs. We propose Counterfactual Analysis for Programming Concept Predicates (CACP) as a counterfactual testing framework to evaluate whether Large Code Models understand programming concepts. With only black-box access to the model, we use CACP to evaluate ten popular Large Code Models for four different programming concepts. Our findings suggest that current models lack understanding of concepts such as data flow and control flow.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KICGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26816;&#32034;&#22120;&#30340;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#30693;&#35782;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#38271;&#23614;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02389</link><description>&lt;p&gt;
KICGPT: &#20855;&#22791;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KICGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26816;&#32034;&#22120;&#30340;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#30693;&#35782;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#38271;&#23614;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#23545;&#20110;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#19981;&#23436;&#25972;&#24615;&#21644;&#25903;&#25345;&#19979;&#28216;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#19977;&#20803;&#32452;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#20004;&#31867;&#12290;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#26041;&#27861;&#30001;&#20110;&#32467;&#26500;&#20449;&#24687;&#26377;&#38480;&#21644;&#23454;&#20307;&#20998;&#24067;&#19981;&#22343;&#34913;&#32780;&#22256;&#38590;&#37325;&#37325;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#29305;&#23450;&#30340;&#30693;&#35782;&#22270;&#35889;&#24494;&#35843;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;KICGPT&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26816;&#32034;&#22120;&#30340;&#26694;&#26550;&#12290;&#23427;&#21487;&#20197;&#32531;&#35299;&#38271;&#23614;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;KICGPT&#20351;&#29992;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#31216;&#20026;&#30693;&#35782;&#25552;&#31034;&#65292;&#23427;&#23558;&#32467;&#26500;&#30693;&#35782;&#32534;&#30721;&#20026;&#28436;&#31034;&#65292;&#20197;&#24341;&#23548;LLM&#30340;&#23398;&#20064;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC. They can be categorized into two main classes: triple-based and text-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced entity distributions. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate these limitations, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. It alleviates the long-tail problem without incurring additional training overhead. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. Empirical results on benchmark datasets demonstrate the effectiven
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02380</link><description>&lt;p&gt;
&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#26102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models in Analysing Classroom Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#25945;&#23398;&#35786;&#26029;&#21644;&#36136;&#37327;&#25913;&#36827;&#30340;&#37325;&#35201;&#30740;&#31350;&#20219;&#21153;&#12290;&#37492;&#20110;&#20256;&#32479;&#25945;&#32946;&#30740;&#31350;&#20013;&#30693;&#35782;&#23494;&#38598;&#21644;&#21171;&#21160;&#23494;&#38598;&#30340;&#23450;&#24615;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLM&#22312;&#20248;&#21270;&#21644;&#22686;&#24378;&#20998;&#26512;&#36807;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#35813;&#30740;&#31350;&#28041;&#21450;&#20013;&#23398;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#23398;&#21644;&#35821;&#25991;&#35838;&#22530;&#19978;&#30340;&#23545;&#35805;&#12290;&#36825;&#20123;&#23545;&#35805;&#30001;&#25945;&#32946;&#19987;&#23478;&#25163;&#21160;&#32534;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#23450;&#21046;&#30340;GPT-4&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#27604;&#36739;&#25163;&#21160;&#27880;&#37322;&#19982;GPT-4&#30340;&#36755;&#20986;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20998;&#26512;&#25945;&#32946;&#23545;&#35805;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#35780;&#20272;&#26102;&#38388;&#25928;&#29575;&#12289;&#32534;&#30721;&#32773;&#38388;&#19968;&#33268;&#24615;&#21644;&#32534;&#30721;&#32773;&#38388;&#21487;&#38752;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GPT-4&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#24182;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in codin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;W4A16&#37327;&#21270;&#25512;&#26029;&#30340;Triton&#34701;&#21512;&#20869;&#26680;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;SplitK&#24037;&#20316;&#20998;&#35299;&#23454;&#29616;&#35299;&#37327;&#21270;&#21644;GEMM&#25805;&#20316;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#30246;&#30697;&#38453;&#20056;&#27861;&#30340;&#25191;&#34892;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.00025</link><description>&lt;p&gt;
&#20351;&#29992;SplitK&#24037;&#20316;&#20998;&#35299;&#21152;&#36895;W4A16&#37327;&#21270;&#25512;&#26029;&#30340;Triton&#34701;&#21512;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with SplitK work decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;W4A16&#37327;&#21270;&#25512;&#26029;&#30340;Triton&#34701;&#21512;&#20869;&#26680;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;SplitK&#24037;&#20316;&#20998;&#35299;&#23454;&#29616;&#35299;&#37327;&#21270;&#21644;GEMM&#25805;&#20316;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#30246;&#30697;&#38453;&#20056;&#27861;&#30340;&#25191;&#34892;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#34701;&#21512;&#30697;&#38453;&#20056;&#27861;&#20869;&#26680;&#30340;&#23454;&#29616;&#65292;&#29992;&#20110;W4A16&#37327;&#21270;&#25512;&#26029;&#65292;&#22312;&#34701;&#21512;&#20869;&#26680;&#20013;&#25191;&#34892;&#35299;&#37327;&#21270;&#21644;GEMM&#25805;&#20316;&#65292;&#24182;&#20351;&#29992;SplitK&#24037;&#20316;&#20998;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#23545;&#20110;&#22522;&#20110;foundation&#27169;&#22411;&#25512;&#26029;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#30246;&#30697;&#38453;&#20056;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#30246;&#28608;&#27963;&#30697;&#38453;&#21644;&#26041;&#24418;&#26435;&#37325;&#30697;&#38453;&#20043;&#38388;&#30340;&#30697;&#38453;&#20056;&#27861;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#31995;&#21015;&#30697;&#38453;&#32500;&#24230;&#65288;&#21253;&#25324;llama-style&#27169;&#22411;&#20013;&#30340;&#32500;&#24230;&#65292;&#20854;&#20013;m &lt; n = k&#65289;&#19978;&#65292;A100&#30340;&#24179;&#22343;&#36895;&#24230;&#25552;&#21319;&#20102;65&#65285;&#65292;H100&#30340;&#24179;&#22343;&#36895;&#24230;&#25552;&#21319;&#20102;124&#65285;&#65288;&#23792;&#20540;&#20026;295&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an implementation of an efficient fused matrix multiplication kernel for W4A16 quantized inference, where we perform dequantization and GEMM in a fused kernel using a SplitK work decomposition. Our implementation shows improvement for the type of skinny matrix-matrix multiplications found in foundation model inference workloads. In particular, this paper surveys the type of matrix multiplication between a skinny activation matrix and a square weight matrix. Our results show an average of 65% speed improvement on A100, and an average of 124% speed improvement on H100 (with a peak of 295%) for a range of matrix dimensions including those found in a llama-style model, where m &lt; n = k.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#35299;&#37322;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;IOH-Xplainer&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#29702;&#35299;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#24433;&#21709;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35780;&#20272;&#21644;&#35299;&#37322;&#36845;&#20195;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17842</link><description>&lt;p&gt;
&#36845;&#20195;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#21487;&#35299;&#37322;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Explainable Benchmarking for Iterative Optimization Heuristics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#35299;&#37322;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;IOH-Xplainer&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#29702;&#35299;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#24433;&#21709;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35780;&#20272;&#21644;&#35299;&#37322;&#36845;&#20195;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#23545;&#20110;&#29702;&#35299;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#20197;&#21450;&#22312;&#20309;&#31181;&#38382;&#39064;&#19978;&#26576;&#20123;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#23545;&#21551;&#21457;&#24335;&#20248;&#21270;&#31639;&#27861;&#30340;&#30740;&#31350;&#21482;&#25506;&#32034;&#20102;&#38750;&#24120;&#26377;&#38480;&#30340;&#22330;&#26223;&#12289;&#31639;&#27861;&#37197;&#32622;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#65292;&#23548;&#33268;&#20102;&#19981;&#23436;&#25972;&#19988;&#24120;&#24120;&#26377;&#20559;&#35265;&#30340;&#35265;&#35299;&#21644;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#21487;&#35299;&#37322;&#22522;&#20934;&#27979;&#35797;&#12290;&#20171;&#32461;&#20102;IOH-Xplainer&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#29702;&#35299;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#20197;&#21450;&#23427;&#20204;&#19981;&#21516;&#32452;&#20214;&#21644;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22359;&#21270;&#20248;&#21270;&#26694;&#26550;&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#31639;&#27861;&#32452;&#20214;&#21644;&#37197;&#32622;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#35299;&#37322;&#36845;&#20195;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#34892;&#20026;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This paper presents a novel approach we call explainable benchmarking. Introducing the IOH-Xplainer software framework, for analyzing and understanding the performance of various optimization algorithms and the impact of their different components and hyper-parameters. We showcase the framework in the context of two modular optimization frameworks. Through this framework, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterativ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17791</link><description>&lt;p&gt;
&#19981;&#24102;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Transformers without Positional Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#21387;&#22120;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#21333;&#29420;&#20351;&#29992;&#36824;&#26159;&#19982;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MP-GNN&#65289;&#32467;&#21512;&#12290;&#23558;&#22270;&#24402;&#32435;&#20559;&#35265;&#34701;&#20837;&#22825;&#28982;&#19982;&#32467;&#26500;&#26080;&#20851;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20197;&#32467;&#26500;&#25110;&#20301;&#32622;&#32534;&#30721;&#65288;PEs&#65289;&#30340;&#24418;&#24335;&#65292;&#26159;&#23454;&#29616;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#26159;&#26840;&#25163;&#30340;&#65292;&#20154;&#20204;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#23581;&#35797;&#26469;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#65292;&#21253;&#25324;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#12289;&#30456;&#23545;&#38543;&#26426;&#34892;&#36208;&#27010;&#29575;&#65288;RRWP&#65289;&#12289;&#31354;&#38388;&#32534;&#30721;&#12289;&#20013;&#24515;&#24230;&#32534;&#30721;&#12289;&#36793;&#32536;&#32534;&#30721;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32534;&#30721;&#21487;&#33021;&#26681;&#26412;&#19981;&#38656;&#35201;&#65292;&#21482;&#35201;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#26377;&#20851;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Eigenformer&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#35889;&#24863;&#30693;&#27880;&#24847;&#26426;&#21046;&#65292;&#20102;&#35299;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17548</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#65306;&#20174;&#39046;&#20808;&#25351;&#26631;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29420;&#31435;&#20110;&#36890;&#36947;&#30340;&#26041;&#27861;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#20294;&#23427;&#20204;&#38169;&#36807;&#20102;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#23616;&#37096;&#24179;&#31283;&#30340;&#39046;&#20808;-&#28382;&#21518;&#20851;&#31995;&#65292;&#21363;&#19968;&#20123;&#28382;&#21518;&#21464;&#37327;&#22312;&#30701;&#26102;&#38388;&#20869;&#21487;&#33021;&#36981;&#24490;&#39046;&#20808;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#31181;&#36890;&#36947;&#30456;&#20851;&#24615;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#39046;&#20808;&#25351;&#26631;&#25552;&#20379;&#20102;&#20808;&#36827;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#28382;&#21518;&#21464;&#37327;&#30340;&#39044;&#27979;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFT&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#39640;&#25928;&#22320;&#20272;&#35745;&#39046;&#20808;&#25351;&#26631;&#21450;&#20854;&#39046;&#20808;&#27493;&#39588;&#65292;&#28982;&#21518;&#24039;&#22937;&#22320;&#20801;&#35768;&#28382;&#21518;&#21464;&#37327;&#21033;&#29992;&#26469;&#33258;&#39046;&#20808;&#25351;&#26631;&#30340;&#20808;&#36827;&#20449;&#24687;&#12290;LIFT&#20316;&#20026;&#19968;&#20010;&#25554;&#20214;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#21327;&#20316;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LIFT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
&lt;/p&gt;</description></item><item><title>PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2401.03855</link><description>&lt;p&gt;
PythonSaga&#65306;&#37325;&#26032;&#23450;&#20041;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;LLM&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03855
&lt;/p&gt;
&lt;p&gt;
PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#20195;&#30721;&#28608;&#22686;&#30340;&#25512;&#21160;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;LLMs&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#23545;HumanEval&#21644;MBPP&#20004;&#20010;&#27969;&#34892;&#30340;Python&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#21644;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#32534;&#31243;&#27010;&#24565;&#23384;&#22312;&#20005;&#37325;&#20559;&#35265;&#65292;&#23436;&#20840;&#24573;&#35270;&#20102;&#22823;&#22810;&#25968;&#20854;&#20182;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22823;&#37327;&#31616;&#21333;&#20219;&#21153;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#21487;&#33021;&#22840;&#22823;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;PythonSaga&#65292;&#21253;&#21547;&#20102;185&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#65292;&#28085;&#30422;&#20102;38&#20010;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#32534;&#31243;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03855v2 Announce Type: replace-cross  Abstract: Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of HumanEval and MBPP, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks, potentially inflating model performance estimations. To address these limitations, we propose a novel benchmark, PythonSaga, featuring 185 hand-crafted prompts on a balanced representation of 38 programming concepts across diverse difficulty levels.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;ChatGPT&#22312;&#21476;&#20195;&#27721;&#35821;&#32763;&#35793;&#21644;&#20154;&#21517;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#32763;&#35793;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#26377;&#24453;&#25552;&#39640;&#65292;&#26368;&#20339;&#34920;&#29616;&#26159;&#22312;&#36755;&#20837;&#19977;&#20010;&#19978;&#19979;&#25991;&#21477;&#23376;&#26102;&#23454;&#29616;&#30340;&#12290;</title><link>https://arxiv.org/abs/2312.15304</link><description>&lt;p&gt;
&#25506;&#32034;ChatGPT&#22312;&#21476;&#20195;&#27721;&#35821;&#32763;&#35793;&#21644;&#20154;&#21517;&#35782;&#21035;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Capabilities of ChatGPT in Ancient Chinese Translation and Person Name Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;ChatGPT&#22312;&#21476;&#20195;&#27721;&#35821;&#32763;&#35793;&#21644;&#20154;&#21517;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#32763;&#35793;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#26377;&#24453;&#25552;&#39640;&#65292;&#26368;&#20339;&#34920;&#29616;&#26159;&#22312;&#36755;&#20837;&#19977;&#20010;&#19978;&#19979;&#25991;&#21477;&#23376;&#26102;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#22788;&#29702;&#29616;&#20195;&#26631;&#20934;&#35821;&#35328;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#26126;&#20854;&#20855;&#26377;&#28508;&#21147;&#29992;&#20110;&#29702;&#35299;&#21476;&#20195;&#27721;&#35821;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#39033;&#20219;&#21153;&#25506;&#35752;&#20102;ChatGPT&#22312;&#21476;&#20195;&#27721;&#35821;&#26041;&#38754;&#30340;&#33021;&#21147;&#65306;&#23558;&#21476;&#20195;&#27721;&#35821;&#32763;&#35793;&#20026;&#29616;&#20195;&#27721;&#35821;&#21644;&#35782;&#21035;&#21476;&#20195;&#27721;&#35821;&#20154;&#21517;&#12290;&#36890;&#36807;&#23558;ChatGPT&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#32763;&#35793;&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#20854;&#23545;&#21476;&#20195;&#27721;&#35821;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65306;&#65288;1.&#65289;ChatGPT&#23545;&#21476;&#20195;&#27721;&#35821;&#30340;&#29087;&#32451;&#31243;&#24230;&#23578;&#26410;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#27700;&#24179;&#65307;&#65288;2.&#65289;&#22312;&#36755;&#20837;&#19977;&#20010;&#19978;&#19979;&#25991;&#21477;&#23376;&#26102;&#65292;ChatGPT&#22312;&#21476;&#20195;&#27721;&#35821;&#21040;&#29616;&#20195;&#27721;&#35821;&#30340;&#32763;&#35793;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#20026;&#20102;&#24110;&#21161;&#37325;&#29616;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;Python&#20195;&#30721;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15304v2 Announce Type: replace-cross  Abstract: ChatGPT's proficiency in handling modern standard languages suggests potential for its use in understanding ancient Chinese. This paper explores ChatGPT's capabilities on ancient Chinese via two tasks: translating ancient Chinese to modern Chinese and recognizing ancient Chinese names. A comparison of ChatGPT's output with human translations serves to evaluate its comprehension of ancient Chinese. The findings indicate that: (1.)the proficiency of ancient Chinese by ChatGPT is yet to reach a satisfactory level; (2.) ChatGPT performs the best on ancient-to-modern translation when feeding with three context sentences. To help reproduce our work, we display the python code snippets used in this study.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;POMDP&#30340;&#20998;&#35299;&#22312;&#32447;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21152;&#26435;&#31890;&#23376;&#28388;&#27874;&#21644;&#21487;&#25193;&#23637;&#30340;&#20449;&#24565;&#36924;&#36817;&#26041;&#27861;&#35299;&#20915;&#20102;&#20540;&#20272;&#35745;&#21644;&#20449;&#24565;&#20272;&#35745;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.11434</link><description>&lt;p&gt;
&#35768;&#22810;&#26234;&#33021;&#20307;POMDP&#20013;&#30340;&#20998;&#35299;&#22312;&#32447;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Factored Online Planning in Many-Agent POMDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;POMDP&#30340;&#20998;&#35299;&#22312;&#32447;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21152;&#26435;&#31890;&#23376;&#28388;&#27874;&#21644;&#21487;&#25193;&#23637;&#30340;&#20449;&#24565;&#36924;&#36817;&#26041;&#27861;&#35299;&#20915;&#20102;&#20540;&#20272;&#35745;&#21644;&#20449;&#24565;&#20272;&#35745;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38598;&#20013;&#24335;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#22810;&#26234;&#33021;&#20307;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MPOMDPs&#65289;&#65292;&#21160;&#20316;&#21644;&#35266;&#27979;&#31354;&#38388;&#38543;&#26234;&#33021;&#20307;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23548;&#33268;&#21333;&#26234;&#33021;&#20307;&#22312;&#32447;&#35268;&#21010;&#30340;&#20215;&#20540;&#21644;&#20449;&#24565;&#20272;&#35745;&#21464;&#24471;&#26080;&#25928;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#21033;&#29992;&#25152;&#35859;&#30340;&#21327;&#35843;&#22270;&#26469;&#37096;&#20998;&#35299;&#20915;&#20215;&#20540;&#20272;&#35745;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#23558;&#35266;&#27979;&#27010;&#29575;&#32435;&#20837;&#36924;&#36817;&#20013;&#25913;&#36827;&#20102;&#20449;&#24565;&#20272;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20215;&#20540;&#20272;&#35745;&#21644;&#20449;&#24565;&#20272;&#35745;&#30340;&#25361;&#25112;&#20165;&#34987;&#21333;&#29420;&#22788;&#29702;&#65292;&#36825;&#38459;&#27490;&#20102;&#29616;&#26377;&#26041;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#35768;&#22810;&#26234;&#33021;&#20307;&#30340;&#24773;&#22659;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21516;&#26102;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#21152;&#26435;&#31890;&#23376;&#28388;&#27874;&#24341;&#20837;&#20102;&#29992;&#20110;MPOMDP&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#22312;&#32447;&#35268;&#21010;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20449;&#24565;&#36924;&#36817;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11434v3 Announce Type: replace  Abstract: In centralized multi-agent systems, often modeled as multi-agent partially observable Markov decision processes (MPOMDPs), the action and observation spaces grow exponentially with the number of agents, making the value and belief estimation of single-agent online planning ineffective. Prior work partially tackles value estimation by exploiting the inherent structure of multi-agent settings via so-called coordination graphs. Additionally, belief estimation methods have been improved by incorporating the likelihood of observations into the approximation. However, the challenges of value estimation and belief estimation have only been tackled individually, which prevents existing methods from scaling to settings with many agents. Therefore, we address these challenges simultaneously. First, we introduce weighted particle filtering to a sample-based online planner for MPOMDPs. Second, we present a scalable approximation of the belief. T
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#20013;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#21457;&#29616;&#26631;&#31614;&#24179;&#28369;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#27169;&#22411;&#30340;&#25910;&#25947;&#65292;&#24182;&#20351;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#26356;&#23481;&#26131;&#21306;&#20998;</title><link>https://arxiv.org/abs/2312.06522</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26631;&#31614;&#24179;&#28369;&#22312;&#22686;&#24378;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Role of Label Smoothing in Enhanced Text Sentiment Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06522
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#20013;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#21457;&#29616;&#26631;&#31614;&#24179;&#28369;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#27169;&#22411;&#30340;&#25910;&#25947;&#65292;&#24182;&#20351;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#26356;&#23481;&#26131;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#25216;&#26415;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#65292;&#20197;&#26377;&#25928;&#23545;&#25239;&#27169;&#22411;&#36807;&#25311;&#21512;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26631;&#31614;&#24179;&#28369;&#22914;&#20309;&#22686;&#24378;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#32454;&#33268;&#20998;&#26512;&#21364;&#24456;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#22312;&#20843;&#20010;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;TextCNN&#12289;BERT&#21644;RoBERTa&#65289;&#20197;&#21450;&#20004;&#31181;&#23398;&#20064;&#26041;&#26696;&#19979;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#28145;&#20837;&#20998;&#26512;&#12290;&#36890;&#36807;&#35843;&#25972;&#24179;&#28369;&#21442;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#27599;&#20010;&#27169;&#22411;&#26550;&#26500;&#30340;&#20960;&#20046;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#30340;&#22909;&#22788;&#65292;&#21457;&#29616;&#26631;&#31614;&#24179;&#28369;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#27169;&#22411;&#30340;&#25910;&#25947;&#65292;&#24182;&#20351;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#26356;&#23481;&#26131;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06522v2 Announce Type: replace-cross  Abstract: Label smoothing is a widely used technique in various domains, such as text classification, image classification and speech recognition, known for effectively combating model overfitting. However, there is little fine-grained analysis on how label smoothing enhances text sentiment classification. To fill in the gap, this article performs a set of in-depth analyses on eight datasets for text sentiment classification and three deep learning architectures: TextCNN, BERT, and RoBERTa, under two learning schemes: training from scratch and fine-tuning. By tuning the smoothing parameters, we can achieve improved performance on almost all datasets for each model architecture. We further investigate the benefits of label smoothing, finding that label smoothing can accelerate the convergence of deep models and make samples of different labels easily distinguishable.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24207;&#36143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#21382;&#21490;&#65292;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2312.04772</link><description>&lt;p&gt;
&#22312;&#24207;&#36143;&#20915;&#31574;&#20013;&#35760;&#20303;&#20844;&#24179;&#65306;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24207;&#36143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#21382;&#21490;&#65292;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;&#20915;&#31574;&#21046;&#23450;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38024;&#23545;&#21333;&#19968;&#20915;&#31574;&#36827;&#34892;&#30740;&#31350;&#30340;&#12290;&#26412;&#25991;&#22312;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#21487;&#33021;&#21463;&#21040;&#20915;&#31574;&#32467;&#26524;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#20844;&#24179;&#27010;&#24565;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#30340;&#21382;&#21490;&#65292;&#20174;&#36825;&#20010;&#24847;&#20041;&#19978;&#35762;&#65292;&#23427;&#26159;&#22266;&#26377;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#65292;&#20844;&#24179;&#36890;&#24120;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#26576;&#20010;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#36807;&#31243;&#32467;&#26463;&#26102;&#12290;&#20026;&#20102;&#25512;&#36827;&#25105;&#20204;&#23545;&#36825;&#31867;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#39034;&#24207;&#20915;&#31574;&#32972;&#26223;&#19979;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#30340;&#23646;&#24615;&#65292;&#21253;&#25324;&#38271;&#26399;&#20844;&#24179;&#24615;&#12289;&#20219;&#24847;&#26102;&#21051;&#20844;&#24179;&#24615;&#12289;&#21608;&#26399;&#24615;&#20844;&#24179;&#24615;&#21644;&#26377;&#30028;&#20844;&#24179;&#24615;&#31561;&#27010;&#24565;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#21644;&#35760;&#24518;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#36825;&#22914;&#20309;&#25903;&#25345;&#21046;&#23450;&#20844;&#24179;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04772v3 Announce Type: replace  Abstract: Fair decision making has largely been studied with respect to a single decision. In this paper we investigate the notion of fairness in the context of sequential decision making where multiple stakeholders can be affected by the outcomes of decisions. We observe that fairness often depends on the history of the sequential decision-making process, and in this sense that it is inherently non-Markovian. We further observe that fairness often needs to be assessed at time points within the process, not just at the end of the process. To advance our understanding of this class of fairness problems, we explore the notion of non-Markovian fairness in the context of sequential decision making. We identify properties of non-Markovian fairness, including notions of long-term, anytime, periodic, and bounded fairness. We further explore the interplay between non-Markovian fairness and memory, and how this can support construction of fair policies
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25509;&#35302;&#33021;&#37327;&#30340;&#20248;&#20808;&#32423;&#25490;&#24207;&#26041;&#27861;&#65288;CEBP&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#36873;&#25321;&#20855;&#26377;&#20016;&#23500;&#25509;&#35302;&#20449;&#24687;&#30340;&#26679;&#26412;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.02677</link><description>&lt;p&gt;
&#22522;&#20110;&#25509;&#35302;&#33021;&#37327;&#30340;&#20107;&#21518;&#32463;&#39564;&#20248;&#20808;&#32423;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Contact Energy Based Hindsight Experience Prioritization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25509;&#35302;&#33021;&#37327;&#30340;&#20248;&#20808;&#32423;&#25490;&#24207;&#26041;&#27861;&#65288;CEBP&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#36873;&#25321;&#20855;&#26377;&#20016;&#23500;&#25509;&#35302;&#20449;&#24687;&#30340;&#26679;&#26412;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#30446;&#26631;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#32780;&#35328;&#26159;&#22256;&#38590;&#30340;&#65292;&#21407;&#22240;&#22312;&#20110;&#25910;&#38598;&#25104;&#21151;&#32463;&#39564;&#30340;&#20302;&#25928;&#24615;&#12290;&#26368;&#36817;&#30340;&#31639;&#27861;&#65292;&#22914;&#20107;&#21518;&#32463;&#39564;&#37325;&#25918;&#65288;HER&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#22833;&#36133;&#30340;&#36712;&#36857;&#24182;&#23558;&#26399;&#26395;&#30446;&#26631;&#26367;&#25442;&#20026;&#24050;&#23454;&#29616;&#29366;&#24577;&#20043;&#19968;&#26469;&#21152;&#24555;&#23398;&#20064;&#65292;&#20351;&#24471;&#20219;&#20309;&#22833;&#36133;&#30340;&#36712;&#36857;&#37117;&#21487;&#20197;&#34987;&#21033;&#29992;&#20316;&#20026;&#23398;&#20064;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;HER&#20250;&#22343;&#21248;&#36873;&#25321;&#22833;&#36133;&#30340;&#36712;&#36857;&#65292;&#32780;&#19981;&#32771;&#34385;&#21738;&#20123;&#21487;&#33021;&#23545;&#23398;&#20064;&#26368;&#26377;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;Contact Energy Based Prioritization~(CEBP)&#65292;&#26681;&#25454;&#25509;&#35302;&#30340;&#20016;&#23500;&#20449;&#24687;&#20174;&#37325;&#25918;&#32531;&#20914;&#21306;&#20013;&#36873;&#25321;&#26679;&#26412;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#22841;&#29226;&#21644;&#29289;&#20307;&#20301;&#31227;&#20013;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#12290;&#25105;&#20204;&#30340;&#20248;&#20808;&#32423;&#26041;&#26696;&#20559;&#21521;&#20110;&#37319;&#26679;&#25509;&#35302;&#20016;&#23500;&#30340;&#32463;&#39564;&#65292;&#36825;&#20123;&#32463;&#39564;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#26368;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02677v2 Announce Type: replace-cross  Abstract: Multi-goal robot manipulation tasks with sparse rewards are difficult for reinforcement learning (RL) algorithms due to the inefficiency in collecting successful experiences. Recent algorithms such as Hindsight Experience Replay (HER) expedite learning by taking advantage of failed trajectories and replacing the desired goal with one of the achieved states so that any failed trajectory can be utilized as a contribution to learning. However, HER uniformly chooses failed trajectories, without taking into account which ones might be the most valuable for learning. In this paper, we address this problem and propose a novel approach Contact Energy Based Prioritization~(CEBP) to select the samples from the replay buffer based on rich information due to contact, leveraging the touch sensors in the gripper of the robot and object displacement. Our prioritization scheme favors sampling of contact-rich experiences, which are arguably the
&lt;/p&gt;</description></item><item><title>InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.12943</link><description>&lt;p&gt;
InteRACT&#65306;&#22522;&#20110;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#20154;&#31867;&#24847;&#22270;&#39044;&#27979;&#30340;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12943
&lt;/p&gt;
&lt;p&gt;
InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#30340;&#20154;&#26426;&#25805;&#32437;&#20013;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#39044;&#27979;&#20154;&#31867;&#24847;&#22270;&#24182;&#30456;&#24212;&#35843;&#25972;&#20854;&#34892;&#21160;&#65292;&#20197;&#24179;&#31283;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24847;&#22270;&#21453;&#36807;&#26469;&#21448;&#21462;&#20915;&#20110;&#26426;&#22120;&#20154;&#37319;&#21462;&#30340;&#21160;&#20316;&#65292;&#36896;&#25104;&#20102;&#19968;&#20010;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26159;&#35757;&#32451;&#29420;&#31435;&#20110;&#26426;&#22120;&#20154;&#34892;&#21160;&#30340;&#36793;&#38469;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#32570;&#20047;&#37197;&#23545;&#30340;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#26465;&#20214;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#33021;&#21542;&#36716;&#32780;&#21033;&#29992;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#65311;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#21033;&#29992;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#34892;&#21160;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;-&#20154;&#31867;&#21040;&#20154;&#31867;-&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;InteRACT&#65292;&#35813;&#26550;&#26500;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#30495;&#23454;&#19990;&#30028;&#30340;&#21327;&#20316;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12943v2 Announce Type: replace-cross  Abstract: In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collabo
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;H.264&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#20013;&#30340;&#36816;&#21160;&#30690;&#37327;&#21644;&#20449;&#24687;&#25513;&#27169;&#26469;&#26816;&#27979;DeepFake&#30340;&#26102;&#22495;&#19981;&#19968;&#33268;&#65292;&#26377;&#25928;&#19988;&#35745;&#31639;&#25104;&#26412;&#20302;&#65292;&#21487;&#33021;&#20026;&#35270;&#39057;&#36890;&#35805;&#21644;&#27969;&#23186;&#20307;&#25552;&#20379;&#26032;&#30340;&#23454;&#26102;DeepFake&#26816;&#27979;&#26041;&#27861;</title><link>https://arxiv.org/abs/2311.10788</link><description>&lt;p&gt;
&#20351;&#29992;H.264&#36816;&#21160;&#30690;&#37327;&#23454;&#29616;&#39640;&#25928;&#30340;&#26102;&#22495;&#24863;&#30693;DeepFake&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Temporally-Aware DeepFake Detection using H.264 Motion Vectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10788
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;H.264&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#20013;&#30340;&#36816;&#21160;&#30690;&#37327;&#21644;&#20449;&#24687;&#25513;&#27169;&#26469;&#26816;&#27979;DeepFake&#30340;&#26102;&#22495;&#19981;&#19968;&#33268;&#65292;&#26377;&#25928;&#19988;&#35745;&#31639;&#25104;&#26412;&#20302;&#65292;&#21487;&#33021;&#20026;&#35270;&#39057;&#36890;&#35805;&#21644;&#27969;&#23186;&#20307;&#25552;&#20379;&#26032;&#30340;&#23454;&#26102;DeepFake&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;DeepFake&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21019;&#24314;&#30340;&#34394;&#20551;&#23186;&#20307;&#65292;&#21487;&#25805;&#32437;&#20154;&#30340;&#34920;&#24773;&#25110;&#36523;&#20221;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;DeepFake&#26816;&#27979;&#26041;&#27861;&#29420;&#31435;&#20998;&#26512;&#27599;&#19968;&#24103;&#65292;&#24573;&#30053;&#24103;&#38388;&#30340;&#19981;&#19968;&#33268;&#21644;&#19981;&#33258;&#28982;&#36816;&#21160;&#12290;&#19968;&#20123;&#36739;&#26032;&#30340;&#26041;&#27861;&#21033;&#29992;&#20809;&#27969;&#27169;&#22411;&#25429;&#25417;&#36825;&#31181;&#26102;&#24207;&#26041;&#38754;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26469;&#33258;H.264&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#30340;&#30456;&#20851;&#20294;&#24120;&#34987;&#24573;&#35270;&#30340;&#36816;&#21160;&#30690;&#37327;&#65288;MVs&#65289;&#21644;&#20449;&#24687;&#25513;&#27169;&#65288;IMs&#65289;&#26469;&#26816;&#27979;DeepFake&#20013;&#30340;&#26102;&#22495;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#65292;&#24182;&#19988;&#19982;&#20165;&#22522;&#20110;&#27599;&#24103;RGB&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26032;&#30340;&#12289;&#23454;&#26102;&#30340;&#26102;&#22495;&#24863;&#30693;DeepFake&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#39057;&#36890;&#35805;&#21644;&#27969;&#23186;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10788v2 Announce Type: replace-cross  Abstract: Video DeepFakes are fake media created with Deep Learning (DL) that manipulate a person's expression or identity. Most current DeepFake detection methods analyze each frame independently, ignoring inconsistencies and unnatural movements between frames. Some newer methods employ optical flow models to capture this temporal aspect, but they are computationally expensive. In contrast, we propose using the related but often ignored Motion Vectors (MVs) and Information Masks (IMs) from the H.264 video codec, to detect temporal inconsistencies in DeepFakes. Our experiments show that this approach is effective and has minimal computational costs, compared with per-frame RGB-only methods. This could lead to new, real-time temporally-aware DeepFake detection methods for video calls and streaming.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2311.08045</link><description>&lt;p&gt;
&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08045
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#35843;&#25972;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25351;&#23548;LLM&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#26356;&#26032;LLMs&#20250;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#19982;&#20154;&#31867;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#39069;&#22806;&#36827;&#34892;&#20559;&#22909;&#27880;&#37322;&#65292;&#20197;&#36866;&#24212;&#36716;&#31227;&#20998;&#24067;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#38024;&#23545;&#26356;&#39640;&#25928;&#30340;&#20154;&#31867;&#20559;&#22909;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#20195;&#29702;&#21644;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20132;&#26367;&#26356;&#26032;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;APO&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20999;&#30340;&#24605;&#32771;&#8221;&#65288;XoT&#65289;&#30340;&#26032;&#22411;&#24605;&#32771;&#20419;&#36827;&#26041;&#27861;&#65292;&#20511;&#21161;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#23558;&#22806;&#37096;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;&#24605;&#24819;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25512;&#24191;&#21040;&#26410;&#30693;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.04254</link><description>&lt;p&gt;
&#19968;&#20999;&#30340;&#24605;&#32771;&#65306;&#25171;&#30772;&#24429;&#32599;&#26031;&#19977;&#35282;&#23450;&#24459;&#20197;&#29983;&#25104;&#24605;&#24819;
&lt;/p&gt;
&lt;p&gt;
Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04254
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20999;&#30340;&#24605;&#32771;&#8221;&#65288;XoT&#65289;&#30340;&#26032;&#22411;&#24605;&#32771;&#20419;&#36827;&#26041;&#27861;&#65292;&#20511;&#21161;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#23558;&#22806;&#37096;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;&#24605;&#24819;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25512;&#24191;&#21040;&#26410;&#30693;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#26131;&#22788;&#29702;&#30340;&#35821;&#35328;&#24207;&#21015;&#65288;&#21363;&#8220;&#24605;&#24819;&#8221;&#65289;&#24443;&#24213;&#25913;&#21464;&#20102;&#20915;&#31574;&#12290;&#19968;&#20010;&#26377;&#25928;&#30340;&#24605;&#24819;&#35774;&#35745;&#24212;&#35813;&#32771;&#34385;&#19977;&#20010;&#20851;&#38190;&#35270;&#35282;&#65306;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24605;&#24819;&#26368;&#22810;&#21482;&#33021;&#20307;&#29616;&#36825;&#20123;&#23646;&#24615;&#20013;&#30340;&#20004;&#20010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20999;&#30340;&#24605;&#32771;&#8221;&#65288;XoT&#65289;&#30340;&#26032;&#22411;&#24605;&#32771;&#20419;&#36827;&#26041;&#27861;&#65292;&#20197;&#25171;&#30772;&#29616;&#26377;&#24605;&#32771;&#33539;&#24335;&#30340;&#8220;&#24429;&#32599;&#26031;&#19977;&#35282;&#23450;&#24459;&#8221;&#12290;XoT&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#23558;&#22806;&#37096;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;&#24605;&#24819;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#39640;&#25928;&#22320;&#25512;&#24191;&#21040;&#26410;&#35265;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;MCTS-LLM&#21327;&#20316;&#24605;&#32771;&#20462;&#35746;&#26694;&#26550;&#65292;&#36825;&#31181;&#26041;&#27861;&#33258;&#20027;&#29983;&#20135;&#39640;&#36136;&#37327;&#30340;&#32508;&#21512;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04254v3 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as "thoughts". An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called "Everything of Thoughts" (XoT) to defy the law of "Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitiv
&lt;/p&gt;</description></item><item><title>&#32858;&#31867;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#38382;&#39064;&#65292;&#25552;&#20986;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;</title><link>https://arxiv.org/abs/2311.02181</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#20010;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#32852;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Joint Problems in Learning Multiple Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02181
&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#38382;&#39064;&#65292;&#25552;&#20986;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#32858;&#31867;&#26159;&#19968;&#20010;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#20854;&#24212;&#29992;&#33539;&#22260;&#20174;&#36890;&#36807;&#20195;&#35874;&#20135;&#29289;&#27987;&#24230;&#33719;&#24471;&#30340;&#23450;&#37327;&#20010;&#24615;&#21270;&#20195;&#35874;&#27169;&#22411;&#21040;&#37327;&#23376;&#20449;&#24687;&#29702;&#35770;&#20013;&#30340;&#29366;&#24577;&#21028;&#21035;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21464;&#31181;&#65292;&#21363;&#32473;&#23450;&#19968;&#32452;&#36712;&#36857;&#21644;&#19968;&#20123;&#37096;&#20998;&#65292;&#25105;&#20204;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#27169;&#22411;&#65292;&#20197;&#20351;&#24471;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23616;&#25910;&#25947;&#30340;&#26041;&#27861;&#21644;EM&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#38468;&#19978;&#20102;&#26377;&#21069;&#26223;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02181v2 Announce Type: replace-cross  Abstract: Clustering of time series is a well-studied problem, with applications ranging from quantitative, personalized models of metabolism obtained from metabolite concentrations to state discrimination in quantum information theory. We consider a variant, where given a set of trajectories and a number of parts, we jointly partition the set of trajectories and learn linear dynamical system (LDS) models for each part, so as to minimize the maximum error across all the models. We present globally convergent methods and EM heuristics, accompanied by promising computational results.
&lt;/p&gt;</description></item><item><title>&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#36827;&#34892;&#20102;&#20262;&#29702;&#35752;&#35770;&#30340;&#33539;&#22260;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#26816;&#26597;&#34920;&#65292;&#20197;&#25512;&#21160;&#20262;&#29702;&#35752;&#35770;&#30340;&#20840;&#38754;&#35780;&#20272;&#21644;&#36879;&#26126;&#35760;&#24405;&#12290;</title><link>https://arxiv.org/abs/2311.02107</link><description>&lt;p&gt;
&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65306;&#20262;&#29702;&#32771;&#34385;&#19982;&#35780;&#20272;&#26816;&#26597;&#34920;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02107
&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#36827;&#34892;&#20102;&#20262;&#29702;&#35752;&#35770;&#30340;&#33539;&#22260;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#26816;&#26597;&#34920;&#65292;&#20197;&#25512;&#21160;&#20262;&#29702;&#35752;&#35770;&#30340;&#20840;&#38754;&#35780;&#20272;&#21644;&#36879;&#26126;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#26032;&#20852;&#25216;&#26415;&#22522;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#36215;&#20102;&#23545;&#28508;&#22312;&#20262;&#29702;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#20294;&#20262;&#29702;&#35752;&#35770;&#23578;&#26410;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#27491;&#22312;&#36827;&#34892;&#30340;&#20262;&#29702;&#35752;&#35770;&#24120;&#24120;&#24573;&#35270;&#20854;&#20182;&#31867;&#22411;&#30340;GenAI&#65292;&#36825;&#20123;GenAI&#24050;&#34987;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#36827;&#34892;&#30740;&#31350;&#21644;&#23454;&#38469;&#30446;&#30340;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#19968;&#20123;&#20262;&#29702;&#38382;&#39064;&#24182;&#26292;&#38706;&#20102;&#20854;&#20182;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#21307;&#30103;&#20445;&#20581;&#20013;GenAI&#20262;&#29702;&#35752;&#35770;&#30340;&#33539;&#22260;&#23457;&#26597;&#65292;&#20197;&#20840;&#38754;&#20998;&#26512;&#24403;&#21069;&#30740;&#31350;&#20013;&#30340;&#24046;&#36317;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#35758;&#36890;&#36807;&#21046;&#23450;&#19968;&#20221;&#26816;&#26597;&#34920;&#26469;&#20943;&#23569;&#36825;&#20123;&#24046;&#36317;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#21644;&#36879;&#26126;&#35760;&#24405;GenAI&#30740;&#31350;&#20013;&#30340;&#20262;&#29702;&#35752;&#35770;&#12290;&#36825;&#20221;&#26816;&#26597;&#34920;&#21487;&#20197;&#36731;&#26494;&#25972;&#21512;&#21040;&#24403;&#21069;&#30340;&#21516;&#34892;&#35780;&#23457;&#21644;&#21457;&#24067;&#31995;&#32479;&#20013;&#65292;&#20197;&#22686;&#24378;GenAI&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02107v2 Announce Type: replace-cross  Abstract: The widespread use of ChatGPT and other emerging technology powered by generative artificial intelligence (GenAI) has drawn much attention to potential ethical issues, especially in high-stakes applications such as healthcare, but ethical discussions are yet to translate into operationalisable solutions. Furthermore, ongoing ethical discussions often neglect other types of GenAI that have been used to synthesise data (e.g., images) for research and practical purposes, which resolved some ethical issues and exposed others. We conduct a scoping review of ethical discussions on GenAI in healthcare to comprehensively analyse gaps in the current research, and further propose to reduce the gaps by developing a checklist for comprehensive assessment and transparent documentation of ethical discussions in GenAI research. The checklist can be readily integrated into the current peer review and publication system to enhance GenAI researc
&lt;/p&gt;</description></item><item><title>&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36807;&#31243;&#20013;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#20102;&#19981;&#21516;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#21457;&#29616;&#20102;&#35760;&#24518;&#22312;&#21508;&#31181;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#29702;&#35770;&#35299;&#37322;&#20102;&#36825;&#31181;&#20219;&#21153;&#24046;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.06714</link><description>&lt;p&gt;
&#25506;&#32034;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring Memorization in Fine-tuned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06714
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36807;&#31243;&#20013;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#20102;&#19981;&#21516;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#21457;&#29616;&#20102;&#35760;&#24518;&#22312;&#21508;&#31181;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#29702;&#35770;&#35299;&#37322;&#20102;&#36825;&#31181;&#20219;&#21153;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#33021;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#34920;&#29616;&#20986;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#38544;&#31169;&#21644;&#29256;&#26435;&#25285;&#24551;&#12290; &#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26102;&#30340;&#35760;&#24518;&#29616;&#35937;&#12290; &#25105;&#20204;&#20351;&#29992;&#24320;&#28304;&#21644;&#25105;&#20204;&#33258;&#24049;&#30340;&#24494;&#35843;LMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19981;&#21516;&#24494;&#35843;&#20219;&#21153;&#20013;&#65292;&#35760;&#24518;&#21576;&#29616;&#20986;&#36739;&#24378;&#30340;&#24046;&#24322;&#24615;&#12290; &#25105;&#20204;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#29702;&#35770;&#25552;&#20379;&#20102;&#23545;&#36825;&#31181;&#20219;&#21153;&#24046;&#24322;&#24615;&#30340;&#30452;&#35266;&#35299;&#37322;&#65292;&#24182;&#25581;&#31034;&#20102;&#35760;&#24518;&#21644;&#27880;&#24847;&#21147;&#20998;&#25968;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06714v2 Announce Type: replace  Abstract: Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns. While prior works have studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared to pre-training, fine-tuning typically involves more sensitive data and diverse objectives, thus may bring distinct privacy risks and unique memorization behaviors. In this work, we conduct the first comprehensive analysis to explore language models' (LMs) memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that memorization presents a strong disparity among different fine-tuning tasks. We provide an intuitive explanation of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention scor
&lt;/p&gt;</description></item><item><title>FedDefender&#26159;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#38024;&#23545;&#24615;&#30340;&#20013;&#27602;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#24046;&#20998;&#27979;&#35797;&#26469;&#35782;&#21035;&#28508;&#22312;&#21253;&#21547;&#21518;&#38376;&#30340;&#24694;&#24847;&#23458;&#25143;&#65292;&#26377;&#25928;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#21040;10%&#12290;</title><link>https://arxiv.org/abs/2307.08672</link><description>&lt;p&gt;
FedDefender&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
FedDefender: Backdoor Attack Defense in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.08672
&lt;/p&gt;
&lt;p&gt;
FedDefender&#26159;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#38024;&#23545;&#24615;&#30340;&#20013;&#27602;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#24046;&#20998;&#27979;&#35797;&#26469;&#35782;&#21035;&#28508;&#22312;&#21253;&#21547;&#21518;&#38376;&#30340;&#24694;&#24847;&#23458;&#25143;&#65292;&#26377;&#25928;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#21040;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL)&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23427;&#20351;&#24471;&#20010;&#20307;&#23458;&#25143;&#65288;&#20363;&#22914;&#29992;&#25143;&#21442;&#19982;&#32773;&#12289;&#36793;&#32536;&#35774;&#22791;&#25110;&#32452;&#32455;&#65289;&#33021;&#22815;&#22312;&#23433;&#20840;&#29615;&#22659;&#20013;&#22522;&#20110;&#26412;&#22320;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#19982;&#32858;&#21512;&#22120;&#20849;&#20139;&#35757;&#32451;&#27169;&#22411;&#20197;&#21327;&#20316;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDefender&#65292;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#38024;&#23545;&#24615;&#30340;&#20013;&#27602;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#23427;&#21033;&#29992;&#24046;&#20998;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#30456;&#21516;&#36755;&#20837;&#30340;&#23458;&#25143;&#27169;&#22411;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#36827;&#34892;&#25351;&#32441;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#24046;&#20998;&#27979;&#35797;&#26469;&#35782;&#21035;&#28508;&#22312;&#21253;&#21547;&#21518;&#38376;&#30340;&#24694;&#24847;&#23458;&#25143;&#12290;&#25105;&#20204;&#20351;&#29992;MNIST&#21644;FashionMNIST&#25968;&#25454;&#38598;&#20197;&#21450;20&#20010;&#21644;30&#20010;&#23458;&#25143;&#23545;FedDefender&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;FedDefender&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#27492;&#31867;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#38477;&#20302;&#21040;10%&#65292;&#32780;&#19981;&#20250;&#24694;&#21270;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.08672v2 Announce Type: replace-cross  Abstract: Federated Learning (FL) is a privacy-preserving distributed machine learning technique that enables individual clients (e.g., user participants, edge devices, or organizations) to train a model on their local data in a secure environment and then share the trained model with an aggregator to build a global model collaboratively. In this work, we propose FedDefender, a defense mechanism against targeted poisoning attacks in FL by leveraging differential testing. Our proposed method fingerprints the neuron activations of clients' models on the same input and uses differential testing to identify a potentially malicious client containing a backdoor. We evaluate FedDefender using MNIST and FashionMNIST datasets with 20 and 30 clients, and our results demonstrate that FedDefender effectively mitigates such attacks, reducing the attack success rate (ASR) to 10\% without deteriorating the global model performance.
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#65292;&#30740;&#31350;&#34920;&#26126;&#22686;&#21152;&#32593;&#32476;&#28145;&#24230;&#20250;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21253;&#25324;&#28145;&#24230;&#34920;&#31034;&#21644;&#27973;&#23618;&#65292;&#25552;&#20986;&#20102;BlockGCL&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;</title><link>https://arxiv.org/abs/2306.02117</link><description>&lt;p&gt;
&#36807;&#24230;&#24179;&#28369;&#65306;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#22121;&#26790;&#65311;
&lt;/p&gt;
&lt;p&gt;
Oversmoothing: A Nightmare for Graph Contrastive Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02117
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#65292;&#30740;&#31350;&#34920;&#26126;&#22686;&#21152;&#32593;&#32476;&#28145;&#24230;&#20250;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21253;&#25324;&#28145;&#24230;&#34920;&#31034;&#21644;&#27973;&#23618;&#65292;&#25552;&#20986;&#20102;BlockGCL&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#24179;&#28369;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#24120;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#32593;&#32476;&#28145;&#24230;&#30340;&#22686;&#21152;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#27491;&#26085;&#30410;&#25104;&#20026;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#22270;&#25968;&#25454;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#24335;&#12290;&#20316;&#20026;GNNs&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#23578;&#19981;&#28165;&#26970;GCL&#26159;&#21542;&#20250;&#32487;&#25215;GNNs&#30340;&#36807;&#24230;&#24179;&#28369;&#32570;&#38519;&#12290;&#26412;&#25991;&#20174;&#36807;&#24230;&#24179;&#28369;&#30340;&#35282;&#24230;&#23545;GCL&#36827;&#34892;&#20102;&#22522;&#30784;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;GCL&#20013;&#22686;&#21152;&#32593;&#32476;&#28145;&#24230;&#20063;&#20250;&#23548;&#33268;&#23427;&#20204;&#30340;&#28145;&#24230;&#34920;&#31034;&#36807;&#24230;&#24179;&#28369;&#65292;&#32780;&#19988;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#27973;&#23618;&#20063;&#20250;&#20986;&#29616;&#36825;&#31181;&#29616;&#35937;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#22312;GCL&#20013;&#31216;&#20026;&#8220;&#38271;&#36317;&#31163;&#39269;&#39295;&#8221;&#65292;&#21363;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#36739;&#20302;&#23618;&#30001;&#20110;&#32570;&#20047;&#26469;&#33258;&#30417;&#30563;&#30340;&#20805;&#20998;&#25351;&#23548;&#32780;&#36973;&#21463;&#36864;&#21270;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BlockGCL&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22359;wi
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02117v2 Announce Type: replace-cross  Abstract: Oversmoothing is a common phenomenon observed in graph neural networks (GNNs), in which an increase in the network depth leads to a deterioration in their performance. Graph contrastive learning (GCL) is emerging as a promising way of leveraging vast unlabeled graph data. As a marriage between GNNs and contrastive learning, it remains unclear whether GCL inherits the same oversmoothing defect from GNNs. This work undertakes a fundamental analysis of GCL from the perspective of oversmoothing on the first hand. We demonstrate empirically that increasing network depth in GCL also leads to oversmoothing in their deep representations, and surprisingly, the shallow ones. We refer to this phenomenon in GCL as `long-range starvation', wherein lower layers in deep networks suffer from degradation due to the lack of sufficient guidance from supervision. Based on our findings, we present BlockGCL, a remarkably simple yet effective blockwi
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#23545;&#20110;&#25506;&#27979;&#35789;&#12289;&#19978;&#19979;&#25991;&#21644;&#25552;&#31034;&#25935;&#24863;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#36741;&#21161;&#22240;&#26524;&#22270;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2303.05279</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26500;&#24314;&#22240;&#26524;&#22270;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models build causal graphs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.05279
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#23545;&#20110;&#25506;&#27979;&#35789;&#12289;&#19978;&#19979;&#25991;&#21644;&#25552;&#31034;&#25935;&#24863;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#36741;&#21161;&#22240;&#26524;&#22270;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#22240;&#26524;&#22270;&#21487;&#33021;&#26159;&#19968;&#20010;&#36153;&#26102;&#36153;&#21147;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#30830;&#20445;&#25429;&#25417;&#21040;&#25152;&#26377;&#30456;&#20851;&#30340;&#22240;&#26524;&#36335;&#24452;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#19981;&#20165;&#35201;&#19982;&#20020;&#24202;&#21307;&#29983;&#21644;&#19987;&#23478;&#35752;&#35770;&#65292;&#36824;&#35201;&#23457;&#38405;&#22823;&#37327;&#30456;&#20851;&#30340;&#21307;&#23398;&#25991;&#29486;&#12290;&#36890;&#36807;&#32534;&#30721;&#24120;&#35265;&#21644;&#21307;&#23398;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20195;&#34920;&#20102;&#19968;&#31181;&#26426;&#20250;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#35780;&#20998;&#28508;&#22312;&#22270;&#20013;&#30340;&#36793;&#32536;&#65288;&#21363;&#20004;&#20010;&#21464;&#37327;&#20043;&#38388;&#30340;&#32852;&#31995;&#65289;&#26469;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;LLMs&#23545;&#29992;&#25143;&#36873;&#25321;&#30340;&#25506;&#27979;&#35789;&#12289;&#19978;&#19979;&#25991;&#21644;&#25552;&#31034;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#26159;&#21542;&#33021;&#22815;&#25104;&#20026;&#34917;&#20805;&#22240;&#26524;&#22270;&#21457;&#23637;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.05279v2 Announce Type: replace-cross  Abstract: Building causal graphs can be a laborious process. To ensure all relevant causal pathways have been captured, researchers often have to discuss with clinicians and experts while also reviewing extensive relevant medical literature. By encoding common and medical knowledge, large language models (LLMs) represent an opportunity to ease this process by automatically scoring edges (i.e., connections between two variables) in potential graphs. LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs. In this work, we evaluate if LLMs can be a useful tool in complementing causal graph development.
&lt;/p&gt;</description></item><item><title>DMODE&#26159;&#19968;&#31181;&#26080;&#38656;&#29289;&#20307;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#22823;&#23567;&#21464;&#21270;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#21644;&#26410;&#30693;&#29289;&#20307;&#30340;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#20013;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#23545;&#35937;&#29305;&#23450;&#32447;&#32034;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2210.12596</link><description>&lt;p&gt;
DMODE: &#26080;&#38656;&#29305;&#23450;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
DMODE: Differential Monocular Object Distance Estimation Module without Class Specific Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.12596
&lt;/p&gt;
&lt;p&gt;
DMODE&#26159;&#19968;&#31181;&#26080;&#38656;&#29289;&#20307;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#22823;&#23567;&#21464;&#21270;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#21644;&#26410;&#30693;&#29289;&#20307;&#30340;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#20013;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#23545;&#35937;&#29305;&#23450;&#32447;&#32034;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21333;&#20010;&#25668;&#20687;&#22836;&#27979;&#37327;&#29289;&#20307;&#36317;&#31163;&#26159;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#32780;&#19981;&#38656;&#35201;&#31435;&#20307;&#35270;&#35273;&#21644;&#28608;&#20809;&#38647;&#36798;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25506;&#35752;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#29289;&#20307;&#31867;&#21035;&#30693;&#35782;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#22312;&#32570;&#20047;&#36825;&#20123;&#24773;&#22659;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#29289;&#20307;&#29305;&#23450;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#32034;&#21487;&#33021;&#20250;&#35823;&#23548;&#19982;&#33539;&#22260;&#24191;&#27867;&#21464;&#21270;&#25110;&#23545;&#25239;&#24773;&#20917;&#19979;&#30340;&#23545;&#35937;&#65292;&#36825;&#26159;&#38754;&#21521;&#23545;&#35937;&#19981;&#21487;&#30693;&#36317;&#31163;&#20272;&#35745;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DMODE&#65292;&#19968;&#31181;&#19981;&#38656;&#35201;&#29289;&#20307;&#31867;&#21035;&#30693;&#35782;&#30340;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#31867;&#21035;&#19981;&#21487;&#30693;&#26041;&#27861;&#12290;DMODE&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22823;&#23567;&#27874;&#21160;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#20272;&#35745;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#26410;&#30693;&#29289;&#20307;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.12596v2 Announce Type: replace-cross  Abstract: Utilizing a single camera for measuring object distances is a cost-effective alternative to stereo-vision and LiDAR. Although monocular distance estimation has been explored in the literature, most existing techniques rely on object class knowledge to achieve high performance. Without this contextual data, monocular distance estimation becomes more challenging, lacking reference points and object-specific cues. However, these cues can be misleading for objects with wide-range variation or adversarial situations, which is a challenging aspect of object-agnostic distance estimation. In this paper, we propose DMODE, a class-agnostic method for monocular distance estimation that does not require object class knowledge. DMODE estimates an object's distance by fusing its fluctuation in size over time with the camera's motion, making it adaptable to various object detectors and unknown objects, thus addressing these challenges. We eva
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#35745;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2210.06891</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#35745;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#23454;&#39564;&#35774;&#35745;&#33539;&#24335;&#65292;&#26088;&#22312;&#32553;&#30701;&#37319;&#38598;&#26102;&#38388;&#12289;&#38477;&#20302;&#25104;&#26412;&#12289;&#21152;&#36895;&#25104;&#20687;&#35774;&#22791;&#30340;&#37096;&#32626;&#12290;&#24403;&#21069;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#19978;&#65292;&#24182;&#35201;&#27714;&#23545;&#29305;&#23450;&#27169;&#22411;&#36827;&#34892;&#35268;&#33539;&#65292;&#32780;&#22312;&#25104;&#20687;&#39046;&#22495;&#65292;&#20854;&#20182;&#20219;&#21153;&#21487;&#33021;&#39537;&#21160;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25104;&#20687;&#24212;&#29992;&#20013;&#23548;&#33268;&#38590;&#20197;&#27714;&#35299;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#35774;&#35745;&#33539;&#24335;&#65292;&#21516;&#26102;&#20248;&#21270;&#35774;&#35745;&#65288;&#22270;&#20687;&#36890;&#36947;&#38598;&#65289;&#24182;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#31354;&#38388;&#19978;&#23494;&#38598;&#37319;&#26679;&#25968;&#25454;&#65288;&#35768;&#22810;&#22270;&#20687;&#36890;&#36947;&#65289;&#36827;&#34892;&#20102;&#23569;&#37327;&#37319;&#38598;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#39044;&#20808;&#25351;&#23450;&#23610;&#23544;&#30340;&#26368;&#20339;&#25903;&#25345;&#20219;&#21153;&#30340;&#36890;&#36947;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.06891v3 Announce Type: replace-cross  Abstract: This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a user-specified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experime
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;GNNInterpreter&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#39640;&#32423;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#32423;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27010;&#29575;&#29983;&#25104;&#22270;&#20998;&#24067;&#26469;&#25581;&#31034;GNN&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2209.07924</link><description>&lt;p&gt;
GNNInterpreter&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#27169;&#22411;&#32423;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.07924
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;GNNInterpreter&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#39640;&#32423;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#32423;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27010;&#29575;&#29983;&#25104;&#22270;&#20998;&#24067;&#26469;&#25581;&#31034;GNN&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#22270;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#25216;&#26415;&#31361;&#30772;&#20351;&#20154;&#20204;&#20135;&#29983;&#20102;&#30097;&#38382;&#65306;GNN&#26159;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#30340;&#65292;&#25105;&#20204;&#33021;&#21542;&#39640;&#24230;&#20449;&#20219;&#20854;&#39044;&#27979;&#65311;&#22312;&#19968;&#20123;&#20851;&#38190;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;&#65292;&#20570;&#20986;&#38169;&#35823;&#20915;&#31574;&#21487;&#33021;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#65292;&#22240;&#27492;&#22312;&#24212;&#29992;&#20043;&#21069;&#35299;&#37322;GNN&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36981;&#24490;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#19981;&#21516;GNN&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#27169;&#22411;&#32423;&#35299;&#37322;&#26041;&#27861;GNNInterpreter&#65292;&#26469;&#35299;&#37322;GNN&#27169;&#22411;&#30340;&#39640;&#32423;&#20915;&#31574;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GNNInterpreter&#36890;&#36807;&#20248;&#21270;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#20135;&#29983;GNN&#22312;&#20570;&#20986;&#26576;&#20010;&#39044;&#27979;&#26102;&#35797;&#22270;&#26816;&#27979;&#21040;&#30340;&#26368;&#20855;&#36776;&#35782;&#24615;&#22270;&#27169;&#24335;&#30340;&#27010;&#29575;&#29983;&#25104;&#22270;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.07924v4 Announce Type: replace-cross  Abstract: Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifical
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;KGE&#27169;&#22411;&#65292;&#21517;&#20026;HypH&#65292;&#21033;&#29992;&#36229;&#20960;&#20309;&#31354;&#38388;&#23884;&#20837;&#20998;&#23618;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#22270;&#20013;&#38142;&#25509;&#39044;&#27979;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2204.13704</link><description>&lt;p&gt;
&#36229;&#20960;&#20309;&#20998;&#23618;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#20302;&#32500;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Hierarchical Knowledge Graph Embeddings for Link Prediction in Low Dimensions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.13704
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;KGE&#27169;&#22411;&#65292;&#21517;&#20026;HypH&#65292;&#21033;&#29992;&#36229;&#20960;&#20309;&#31354;&#38388;&#23884;&#20837;&#20998;&#23618;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#22270;&#20013;&#38142;&#25509;&#39044;&#27979;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#24050;&#34987;&#35777;&#23454;&#26159;&#25512;&#26029;&#30693;&#35782;&#22270;&#65288;KGs&#65289;&#20013;&#32570;&#22833;&#38142;&#25509;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#23427;&#20204;&#36890;&#24120;&#23558;&#23454;&#20307;&#26144;&#23556;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#24182;&#23558;&#20851;&#31995;&#35270;&#20026;&#23454;&#20307;&#30340;&#36716;&#25442;&#12290; &#26368;&#36817;&#65292;&#19968;&#20123;&#27431;&#20960;&#37324;&#24471;KGE&#26041;&#27861;&#24050;&#32463;&#22686;&#24378;&#65292;&#20197;&#24314;&#27169;KGs&#20013;&#24120;&#35265;&#30340;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#65292;&#25552;&#39640;&#38142;&#25509;&#39044;&#27979;&#24615;&#33021;&#12290; &#20026;&#20102;&#23884;&#20837;&#20998;&#23618;&#25968;&#25454;&#65292;&#36229;&#20960;&#20309;&#31354;&#38388;&#24050;&#32463;&#25104;&#20026;&#20256;&#32479;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#24230;&#20445;&#30495;&#24230;&#21644;&#36739;&#20302;&#30340;&#20869;&#23384;&#28040;&#32791;&#12290; &#19982;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19981;&#21516;&#65292;&#36229;&#20960;&#20309;&#31354;&#38388;&#25552;&#20379;&#20102;&#26080;&#25968;&#21487;&#20379;&#36873;&#25321;&#30340;&#26354;&#29575;&#12290; &#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#36229;&#20960;&#20309;KGE&#26041;&#27861;&#38590;&#20197;&#25163;&#21160;&#33719;&#21462;&#26368;&#20339;&#26354;&#29575;&#35774;&#32622;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#26377;&#25928;&#22320;&#23545;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#12290; &#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textbf{Hyp}$erbolic $\textbf{H}$ierarchical $\textb
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.13704v2 Announce Type: replace-cross  Abstract: Knowledge graph embeddings (KGE) have been validated as powerful methods for inferring missing links in knowledge graphs (KGs) that they typically map entities into Euclidean space and treat relations as transformations of entities. Recently, some Euclidean KGE methods have been enhanced to model semantic hierarchies commonly found in KGs, improving the performance of link prediction. To embed hierarchical data, hyperbolic space has emerged as a promising alternative to traditional Euclidean space, offering high fidelity and lower memory consumption. Unlike Euclidean, hyperbolic space provides countless curvatures to choose from. However, it is difficult for existing hyperbolic KGE methods to obtain the optimal curvature settings manually, thereby limiting their ability to effectively model semantic hierarchies. To address this limitation, we propose a novel KGE model called $\textbf{Hyp}$erbolic $\textbf{H}$ierarchical $\textb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#33258;&#21160;&#26500;&#24314;&#33021;&#26368;&#22823;&#31243;&#24230;&#28789;&#27963;&#19988;&#36275;&#22815;&#35299;&#37322;&#24615;&#30340;&#35821;&#35328;&#65292;&#29992;&#20110;&#21327;&#35843;&#20219;&#21153;&#35268;&#21010;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#23558;&#35745;&#21010;&#34920;&#36798;&#20026;&#8220;&#35745;&#21010;&#33609;&#22270;&#8221;&#65292;&#23454;&#29616;&#31283;&#20581;&#21327;&#35843;&#12290;</title><link>https://arxiv.org/abs/1905.00517</link><description>&lt;p&gt;
&#20174;&#25277;&#35937;&#21040;&#22522;&#20110;&#23454;&#36341;&#30340;&#35821;&#35328;&#65306;&#23454;&#29616;&#20219;&#21153;&#35268;&#21010;&#26426;&#22120;&#20154;&#30340;&#31283;&#20581;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
From Abstractions to Grounded Languages for Robust Coordination of Task Planning Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1905.00517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#33258;&#21160;&#26500;&#24314;&#33021;&#26368;&#22823;&#31243;&#24230;&#28789;&#27963;&#19988;&#36275;&#22815;&#35299;&#37322;&#24615;&#30340;&#35821;&#35328;&#65292;&#29992;&#20110;&#21327;&#35843;&#20219;&#21153;&#35268;&#21010;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#23558;&#35745;&#21010;&#34920;&#36798;&#20026;&#8220;&#35745;&#21010;&#33609;&#22270;&#8221;&#65292;&#23454;&#29616;&#31283;&#20581;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21327;&#35843;&#20219;&#21153;&#35268;&#21010;&#26426;&#22120;&#20154;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29615;&#33410;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#26500;&#24314;&#26368;&#22823;&#28789;&#27963;&#24615;&#19988;&#36275;&#22815;&#35299;&#37322;&#24615;&#30340;&#35821;&#35328;&#65292;&#29992;&#20110;&#21327;&#35843;&#20219;&#21153;&#35268;&#21010;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#35270;&#20026;&#25351;&#23450;&#35745;&#21010;&#26102;&#38388;-&#29366;&#24577;&#32422;&#26463;&#30340;&#26426;&#21046;&#12290;&#36825;&#31181;&#30475;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#36870;&#21521;&#24037;&#31243;&#22320;&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;&#35821;&#35328;&#65292;&#23558;&#36825;&#20123;&#21487;&#32452;&#21512;&#32422;&#26463;&#26144;&#23556;&#21040;&#35789;&#35821;&#19978;&#12290;&#25105;&#20204;&#30340;&#35821;&#35328;&#23558;&#19968;&#20010;&#32473;&#23450;&#20219;&#21153;&#30340;&#35745;&#21010;&#34920;&#36798;&#20026;&#19968;&#20010;&#8220;&#35745;&#21010;&#33609;&#22270;&#8221;&#65292;&#20256;&#36798;&#36275;&#22815;&#30340;&#32454;&#33410;&#20197;&#26368;&#22823;&#31243;&#24230;&#23454;&#29616;&#28789;&#27963;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#31283;&#20581;&#21327;&#35843;&#24182;&#20855;&#26377;&#26368;&#20248;&#24615;&#20445;&#35777;&#31561;&#20854;&#20182;&#20248;&#28857;&#12290;&#25105;&#20204;&#21046;&#23450;&#24182;&#20998;&#26512;&#38382;&#39064;&#65292;&#25552;&#20379;&#36817;&#20284;&#35299;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#65292;&#24182;&#20026;&#20854;&#24212;&#29992;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1905.00517v3 Announce Type: replace  Abstract: In this paper, we consider a first step to bridge a gap in coordinating task planning robots. Specifically, we study the automatic construction of languages that are maximally flexible while being sufficiently explicative for coordination. To this end, we view language as a machinery for specifying temporal-state constraints of plans. Such a view enables us to reverse-engineer a language from the ground up by mapping these composable constraints to words. Our language expresses a plan for any given task as a "plan sketch" to convey just-enough details while maximizing the flexibility to realize it, leading to robust coordination with optimality guarantees among other benefits. We formulate and analyze the problem, provide an approximate solution, and validate the advantages of our approach under various scenarios to shed light on its applications.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#25193;&#23637;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#31232;&#30095;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.12686</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65306;&#19968;&#31181;&#28151;&#21512;&#22270;&#24418;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach. (arXiv:2401.12686v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12686
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#25193;&#23637;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#31232;&#30095;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22823;&#35268;&#27169;&#20195;&#29702;&#32676;&#20307;&#30340;&#34892;&#20026;&#26159;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#34429;&#28982;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#22312;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#20195;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#19988;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65288;MFGs&#65289;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#20195;&#29702;&#20043;&#38388;&#30340;&#32593;&#32476;&#32467;&#26500;&#30340;&#22270;&#24418;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65288;GMFGs&#65289;&#12290;&#23613;&#31649;&#20855;&#26377;&#35832;&#22810;&#20248;&#28857;&#65292;&#20294;GMFGs&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#21463;&#21040;&#22270;&#24418;&#21482;&#33021;&#25429;&#25417;&#23494;&#38598;&#22270;&#30340;&#38480;&#21046;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#23454;&#39564;&#35777;&#26126;&#30340;&#32593;&#32476;&#26174;&#31034;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#31232;&#30095;&#24615;&#65292;&#20363;&#22914;&#24130;&#24459;&#22270;&#65292;&#22240;&#27492;GMFG&#26694;&#26550;&#26080;&#27861;&#25429;&#25417;&#36825;&#20123;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#23545;&#23616;&#21338;&#24328;&#65288;GXMFGs&#65289;&#30340;&#27010;&#24565;&#65292;&#23427;&#24314;&#31435;&#22312;&#22270;&#35770;&#27010;&#24565;&#22270;&#24418;&#25193;&#23637;&#65288;graphexes&#65289;&#22522;&#30784;&#19978;&#12290;&#22270;&#24418;&#25193;&#23637;&#26159;&#31232;&#30095;&#22270;&#24207;&#21015;&#30340;&#26497;&#38480;&#23545;&#35937;&#65292;&#36824;&#20855;&#26377;&#20854;&#20182;&#19968;&#20123;&#29702;&#24819;&#29305;&#24615;&#65292;&#22914;sma
&lt;/p&gt;
&lt;p&gt;
Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the sma
&lt;/p&gt;</description></item><item><title>SeeClick&#26159;&#19968;&#31181;&#22522;&#20110;&#23631;&#24149;&#25130;&#22270;&#30340;&#35270;&#35273;GUI&#20195;&#29702;&#65292;&#36890;&#36807;GUI grounding&#39044;&#35757;&#32451;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#33258;&#21160;&#21270;&#20013;&#20934;&#30830;&#23450;&#20301;&#23631;&#24149;&#20803;&#32032;&#65292;&#24182;&#21019;&#24314;&#20102;&#20840;&#38754;&#35206;&#30422;&#31227;&#21160;&#12289;&#26700;&#38754;&#21644;Web&#29615;&#22659;&#30340;GUI grounding&#25968;&#25454;&#38598;ScreenSpot&#12290;</title><link>http://arxiv.org/abs/2401.10935</link><description>&lt;p&gt;
SeeClick&#65306;&#21033;&#29992;GUI Grounding&#23454;&#29616;&#39640;&#32423;&#21487;&#35270;&#21270;GUI&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents. (arXiv:2401.10935v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10935
&lt;/p&gt;
&lt;p&gt;
SeeClick&#26159;&#19968;&#31181;&#22522;&#20110;&#23631;&#24149;&#25130;&#22270;&#30340;&#35270;&#35273;GUI&#20195;&#29702;&#65292;&#36890;&#36807;GUI grounding&#39044;&#35757;&#32451;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#33258;&#21160;&#21270;&#20013;&#20934;&#30830;&#23450;&#20301;&#23631;&#24149;&#20803;&#32032;&#65292;&#24182;&#21019;&#24314;&#20102;&#20840;&#38754;&#35206;&#30422;&#31227;&#21160;&#12289;&#26700;&#38754;&#21644;Web&#29615;&#22659;&#30340;GUI grounding&#25968;&#25454;&#38598;ScreenSpot&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;(GUI)&#20195;&#29702;&#34987;&#35774;&#35745;&#29992;&#20110;&#33258;&#21160;&#21270;&#25968;&#23383;&#35774;&#22791;&#19978;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#22914;&#26234;&#33021;&#25163;&#26426;&#21644;&#26700;&#38754;&#30005;&#33041;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GUI&#20195;&#29702;&#36890;&#36807;&#25552;&#21462;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#29305;&#21035;&#20887;&#38271;&#65288;&#20363;&#22914;HTML&#65289;&#19988;&#26377;&#26102;&#26080;&#27861;&#35775;&#38382;&#65288;&#20363;&#22914;&#22312;&#26700;&#38754;&#19978;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#24149;&#25130;&#22270;&#36827;&#34892;&#20219;&#21153;&#33258;&#21160;&#21270;&#30340;&#35270;&#35273;GUI&#20195;&#29702;--SeeClick&#12290;&#22312;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#24320;&#21457;&#35270;&#35273;GUI&#20195;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;GUI grounding - &#26681;&#25454;&#25351;&#20196;&#20934;&#30830;&#23450;&#20301;&#23631;&#24149;&#20803;&#32032;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;GUI grounding&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;SeeClick&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;GUI grounding&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;&#20197;&#19978;&#24037;&#20316;&#65292;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;ScreenSpot&#65292;&#31532;&#19968;&#20010;&#28085;&#30422;&#31227;&#21160;&#12289;&#26700;&#38754;&#21644;Web&#29615;&#22659;&#30340;&#30495;&#23454;GUI grounding&#25968;&#25454;&#38598;&#12290;&#32463;&#36807;&#39044;&#35757;&#32451;&#65292;SeeClick&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a visual GUI agent -- SeeClick, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding -the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance SeeClick with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created ScreenSpot, the first realistic GUI grounding dataset that encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#21644;&#32467;&#21512;FixMatch&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#35774;&#35745;&#30340;&#33258;&#35757;&#32451;&#27969;&#31243;&#65292;&#29992;&#20110;&#35299;&#20915;&#30333;&#32454;&#32990;&#20998;&#21106;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#36807;&#26102;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21644;&#33258;&#35757;&#32451;&#26041;&#26696;&#30340;&#25903;&#25345;&#19979;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.07278</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26032;&#35774;&#35745;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#22312;&#30333;&#32454;&#32990;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Semantic Segmentation using Redesigned Self-Training for White Blood Cell. (arXiv:2401.07278v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#21644;&#32467;&#21512;FixMatch&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#35774;&#35745;&#30340;&#33258;&#35757;&#32451;&#27969;&#31243;&#65292;&#29992;&#20110;&#35299;&#20915;&#30333;&#32454;&#32990;&#20998;&#21106;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#36807;&#26102;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21644;&#33258;&#35757;&#32451;&#26041;&#26696;&#30340;&#25903;&#25345;&#19979;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#30333;&#32454;&#32990;&#30284;&#30151;&#35786;&#26029;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21463;&#21040;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#38459;&#30861;&#65306;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#30333;&#32454;&#32990;&#65288;WBC&#65289;&#20998;&#21106;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#36807;&#26102;&#30340;&#20998;&#21106;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#24212;&#35813;&#24341;&#20837;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#26469;&#39640;&#25928;&#22320;&#27880;&#37322;&#22823;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35757;&#32451;&#27969;&#31243;&#24182;&#32467;&#21512;FixMatch&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#22312;&#33258;&#35757;&#32451;&#27969;&#31243;&#20013;&#32467;&#21512;FixMatch&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#24615;&#33021;&#22312;DeepLab-V3&#26550;&#26500;&#21644;ResNet-50&#19978;&#37319;&#29992;&#20102;&#33258;&#35757;&#32451;&#26041;&#26696;&#21644;&#19968;&#33268;&#24615;&#65292;&#20998;&#21035;&#22312;Zheng 1&#12289;Zheng 2&#21644;LISC&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;90.69%&#12289;87.37%&#21644;76.49%&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) in healthcare, especially in white blood cell cancer diagnosis, is hindered by two primary challenges: the lack of large-scale labeled datasets for white blood cell (WBC) segmentation and outdated segmentation methods. To address the first challenge, a semi-supervised learning framework should be brought to efficiently annotate the large dataset. In this work, we address this issue by proposing a novel self-training pipeline with the incorporation of FixMatch. We discover that by incorporating FixMatch in the self-training pipeline, the performance improves in the majority of cases. Our performance achieved the best performance with the self-training scheme with consistency on DeepLab-V3 architecture and ResNet-50, reaching 90.69%, 87.37%, and 76.49% on Zheng 1, Zheng 2, and LISC datasets, respectively.
&lt;/p&gt;</description></item><item><title>Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2401.06477</link><description>&lt;p&gt;
Kun: &#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#30340;&#20013;&#22269;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#31572;&#26696;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. (arXiv:2401.06477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06477
&lt;/p&gt;
&lt;p&gt;
Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Kun&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;Kun&#21033;&#29992;&#26469;&#33258;&#21566;&#36947;&#12289;&#23436;&#21367;&#21644;SkyPile&#31561;&#22810;&#20010;&#26469;&#28304;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#37319;&#29992;&#22522;&#20110;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#20013;&#25991;&#25351;&#23548;&#25968;&#25454;&#28857;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#23436;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#65292;&#26174;&#33879;&#20559;&#31163;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;6B&#21442;&#25968;&#30340;Yi&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;Kun&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#19988;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#23545;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#36825;&#31181;&#26041;&#27861;ological&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20013;&#25991;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Kun, a novel approach for creating high-quality instruction-tuning datasets for large language models (LLMs) without relying on manual annotations. Adapting a self-training algorithm based on instruction back-translation and answer polishment, Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points. This approach significantly deviates from traditional methods by using a self-curation process to refine and select the most effective instruction-output pairs. Our experiments with the 6B-parameter Yi model across various benchmarks demonstrate Kun's robustness and scalability. Our method's core contributions lie in its algorithmic advancement, which enhances data retention and clarity, and its innovative data generation approach that substantially reduces the reliance on costly and time-consuming manual annotations. This methodology presents a sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04385</link><description>&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#25200;&#21160;&#23454;&#29616;&#26426;&#22120;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning through fine-grained model parameters perturbation. (arXiv:2401.04385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#28041;&#21450;&#21040;&#25764;&#38144;&#25968;&#25454;&#35760;&#24405;&#21644;&#20943;&#23567;&#35813;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30446;&#26631;&#65292;&#20294;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;&#21442;&#25968;&#25200;&#21160;&#30340;&#26435;&#37325;&#21435;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#28041;&#21450;&#21040;&#20840;&#23616;&#20462;&#25913;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;Top-K&#21644;Random-k&#21442;&#25968;&#25200;&#21160;&#19981;&#31934;&#30830;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#38544;&#31169;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#21487;&#25511;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#35780;&#20272;&#26426;&#22120;&#21435;&#23398;&#20064;&#25928;&#26524;&#30340;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#22312;&#21435;&#23398;&#20064;&#21644;&#21097;&#20313;&#25968;&#25454;&#19978;&#30340;&#24191;&#20041;&#24615;&#33021;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25351;&#26631;&#65292;&#21363;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#31934;&#30830;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#65292;&#29616;&#26377;&#30340;&#25351;&#26631;&#26080;&#27861;&#23545;&#21435;&#23398;&#20064;&#31243;&#24230;&#36827;&#34892;&#20934;&#30830;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.  In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21464;&#37327;&#21644;&#20854;&#22495;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#23454;&#29616;&#20102;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02602</link><description>&lt;p&gt;
&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Neural Causal Abstractions. (arXiv:2401.02602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21464;&#37327;&#21644;&#20854;&#22495;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#23454;&#29616;&#20102;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29702;&#35299;&#19990;&#30028;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#23558;&#20449;&#24687;&#21387;&#32553;&#25104;&#25277;&#35937;&#27010;&#24565;&#30340;&#33021;&#21147;&#26159;&#20154;&#31867;&#26234;&#24935;&#30340;&#20004;&#20010;&#26631;&#24535;&#24615;&#29305;&#24449;&#12290;&#36825;&#20004;&#20010;&#20027;&#39064;&#22312;&#25991;&#29486;&#20013;&#34987;&#32479;&#31216;&#20026;&#22240;&#26524;&#25277;&#35937;&#29702;&#35770;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22914;&#20309;&#22312;&#30495;&#23454;&#30340;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#20805;&#20998;&#21033;&#29992;&#25277;&#35937;&#29702;&#35770;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#30495;&#23454;&#26426;&#21046;&#26159;&#26410;&#30693;&#30340;&#65292;&#21482;&#26377;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21464;&#37327;&#21450;&#20854;&#22495;&#36827;&#34892;&#32858;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25277;&#35937;&#23478;&#26063;&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#36827;&#21644;&#27010;&#25324;&#20102;&#20043;&#21069;&#30340;&#25277;&#35937;&#27010;&#24565;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;Pearl&#30340;&#22240;&#26524;&#23618;&#27425;&#32467;&#26500;&#24341;&#21457;&#30340;&#20010;&#20307;&#22240;&#26524;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#65288;Xia&#31561;&#65292;2021&#65289;&#21487;&#20197;&#23398;&#24471;&#36825;&#26679;&#30340;&#25277;&#35937;&#27010;&#24565;&#65292;&#20174;&#32780;&#33021;&#22815;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abilities of humans to understand the world in terms of cause and effect relationships, as well as to compress information into abstract concepts, are two hallmark features of human intelligence. These two topics have been studied in tandem in the literature under the rubric of causal abstractions theory. In practice, it remains an open problem how to best leverage abstraction theory in real-world causal inference tasks, where the true mechanisms are unknown and only limited data is available. In this paper, we develop a new family of causal abstractions by clustering variables and their domains. This approach refines and generalizes previous notions of abstractions to better accommodate individual causal distributions that are spawned by Pearl's causal hierarchy. We show that such abstractions are learnable in practical settings through Neural Causal Models (Xia et al., 2021), enabling the use of the deep learning toolkit to solve various challenging causal inference tasks -- iden
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33258;&#21160;&#38899;&#20048;&#26631;&#35760;&#20013;&#25506;&#32034;&#20102;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#24037;&#20316;&#27969;&#26469;&#25552;&#21462;&#38899;&#39057;&#25991;&#20214;&#20013;&#30340;&#24863;&#30693;&#29305;&#24449;&#65292;&#20174;&#32780;&#35757;&#32451;&#20986;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2312.11234</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#38899;&#39057;&#26631;&#35760;&#30340;&#24863;&#30693;&#38899;&#20048;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Perceptual Musical Features for Interpretable Audio Tagging. (arXiv:2312.11234v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33258;&#21160;&#38899;&#20048;&#26631;&#35760;&#20013;&#25506;&#32034;&#20102;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#24037;&#20316;&#27969;&#26469;&#25552;&#21462;&#38899;&#39057;&#25991;&#20214;&#20013;&#30340;&#24863;&#30693;&#29305;&#24449;&#65292;&#20174;&#32780;&#35757;&#32451;&#20986;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#20048;&#27969;&#23186;&#20307;&#24179;&#21488;&#30340;&#26102;&#20195;&#65292;&#33258;&#21160;&#26631;&#35760;&#38899;&#20048;&#38899;&#39057;&#30340;&#20219;&#21153;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#26088;&#22312;&#25552;&#39640;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#25351;&#26631;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23613;&#31649;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20063;&#20855;&#26377;&#19981;&#36879;&#26126;&#24615;&#65292;&#20351;&#24471;&#38590;&#20197;&#35299;&#37322;&#20854;&#23545;&#32473;&#23450;&#36755;&#20837;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#38382;&#39064;&#22312;&#20854;&#20182;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#22791;&#21463;&#24378;&#35843;&#65292;&#20294;&#22312;&#38899;&#20048;&#30456;&#20851;&#20219;&#21153;&#20013;&#24182;&#26410;&#24471;&#21040;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#33258;&#21160;&#38899;&#20048;&#26631;&#35760;&#30340;&#32972;&#26223;&#19979;&#35299;&#37322;&#24615;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24037;&#20316;&#27969;&#65292;&#32467;&#21512;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#20449;&#24687;&#25552;&#21462;&#25216;&#26415;&#65306;a&#65289;&#21033;&#29992;&#31526;&#21495;&#30693;&#35782;&#65292;b&#65289;&#21033;&#29992;&#36741;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;c&#65289;&#21033;&#29992;&#20449;&#21495;&#22788;&#29702;&#20174;&#38899;&#39057;&#25991;&#20214;&#20013;&#25552;&#21462;&#24863;&#30693;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the age of music streaming platforms, the task of automatically tagging music audio has garnered significant attention, driving researchers to devise methods aimed at enhancing performance metrics on standard datasets. Most recent approaches rely on deep neural networks, which, despite their impressive performance, possess opacity, making it challenging to elucidate their output for a given input. While the issue of interpretability has been emphasized in other fields like medicine, it has not received attention in music-related tasks. In this study, we explored the relevance of interpretability in the context of automatic music tagging. We constructed a workflow that incorporates three different information extraction techniques: a) leveraging symbolic knowledge, b) utilizing auxiliary deep neural networks, and c) employing signal processing to extract perceptual features from audio files. These features were subsequently used to train an interpretable machine-learning model for ta
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01223</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;: &#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01223
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#24050;&#32463;&#20986;&#29616;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#21253;&#25324;&#20316;&#20026;&#36712;&#36857;&#35268;&#21010;&#22120;&#12289;&#34920;&#36798;&#33021;&#21147;&#20016;&#23500;&#30340;&#31574;&#30053;&#31867;&#21035;&#12289;&#25968;&#25454;&#21512;&#25104;&#22120;&#31561;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#21551;&#21457;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#24403;&#21069;RL&#31639;&#27861;&#36935;&#21040;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25193;&#25955;&#27169;&#22411;&#22312;RL&#20013;&#25152;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#25552;&#20986;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;RL&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#39033;&#32508;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#37325;&#28857;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
&lt;/p&gt;</description></item><item><title>Alquist 5.0&#26159;&#19968;&#31181;&#26032;&#30340;SocialBot&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#26641;&#21644;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#21450;&#24341;&#20837;NRG Barista&#21644;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#20445;&#25345;&#20102;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16119</link><description>&lt;p&gt;
Alquist 5.0&#65306;&#23545;&#35805;&#26641;&#19982;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#22686;&#24378;SocialBot&#23545;&#35805;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations. (arXiv:2310.16119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16119
&lt;/p&gt;
&lt;p&gt;
Alquist 5.0&#26159;&#19968;&#31181;&#26032;&#30340;SocialBot&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#26641;&#21644;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#21450;&#24341;&#20837;NRG Barista&#21644;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#20445;&#25345;&#20102;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;SocialBot- Alquist 5.0-&#65292;&#35813;&#31995;&#32479;&#26159;&#20026;Alexa Prize SocialBot&#22823;&#25361;&#25112;5&#24320;&#21457;&#30340;&#12290;&#22312;&#25105;&#20204;&#31995;&#32479;&#30340;&#21069;&#20960;&#20010;&#29256;&#26412;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NRG Barista&#65292;&#24182;&#27010;&#36848;&#20102;&#23558;Barista&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;SocialBot&#20013;&#30340;&#20960;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25972;&#20307;&#30340;&#23545;&#35805;&#20307;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;SocialBot&#20197;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;Alquist 5.0&#24320;&#21457;&#30340;&#35265;&#35299;&#65292;&#35813;&#31995;&#32479;&#22312;&#28385;&#36275;&#29992;&#25143;&#19981;&#26029;&#21464;&#21270;&#30340;&#26399;&#26395;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#23545;&#21508;&#31181;&#20027;&#39064;&#30340;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize SocialBot Grand Challenge~5. Building upon previous versions of our system, we introduce the NRG Barista and outline several innovative approaches for integrating Barista into our SocialBot, improving the overall conversational experience. Additionally, we extend our SocialBot to support multimodal devices. This paper offers insights into the development of Alquist~5.0, which meets evolving user expectations while maintaining empathetic and knowledgeable conversational abilities across diverse topics.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;EpiK-Eval&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#23384;&#22312;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#20197;&#22823;&#24133;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15372</link><description>&lt;p&gt;
EpiK-Eval&#65306;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#35782;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
EpiK-Eval: Evaluation for Language Models as Epistemic Models. (arXiv:2310.15372v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15372
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;EpiK-Eval&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#23384;&#22312;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#20197;&#22823;&#24133;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20316;&#29992;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#26085;&#30410;&#26222;&#21450;&#65292;&#20294;&#23427;&#20204;&#22312;&#20174;&#19981;&#21516;&#35757;&#32451;&#25991;&#26723;&#20013;&#25972;&#21512;&#30693;&#35782;&#30340;&#33021;&#21147;&#8212;&#8212;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#26159;&#20851;&#38190;&#33021;&#21147;&#8212;&#8212;&#20173;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;LLMs&#22312;&#20854;&#21442;&#25968;&#31354;&#38388;&#20869;&#26377;&#25928;&#22320;&#32467;&#21512;&#36825;&#31181;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;EpiK-Eval&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#19968;&#31181;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;LLMs&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#30340;&#26174;&#33879;&#24369;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32570;&#28857;&#28304;&#20110;&#29616;&#26377;&#35757;&#32451;&#30446;&#26631;&#30340;&#22266;&#26377;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#36825;&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents - a crucial ability in numerous applications - remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25513;&#30721;&#39044;&#35757;&#32451;&#26694;&#26550;(MaskMA)&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#20010;&#26694;&#26550;&#37319;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#25513;&#30721;&#30340;&#21327;&#20316;&#23398;&#20064;&#31574;&#30053;&#65292;&#21516;&#26102;&#25972;&#21512;&#20102;&#21487;&#27867;&#21270;&#30340;&#21160;&#20316;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.11846</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#25513;&#30721;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Masked Pretraining for Multi-Agent Decision Making. (arXiv:2310.11846v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11846
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25513;&#30721;&#39044;&#35757;&#32451;&#26694;&#26550;(MaskMA)&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#20010;&#26694;&#26550;&#37319;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#25513;&#30721;&#30340;&#21327;&#20316;&#23398;&#20064;&#31574;&#30053;&#65292;&#21516;&#26102;&#25972;&#21512;&#20102;&#21487;&#27867;&#21270;&#30340;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#20915;&#31574;&#39046;&#22495;&#65292;&#26500;&#24314;&#20855;&#26377;&#38646;&#26679;&#26412;&#33021;&#21147;&#30340;&#36890;&#29992;&#21333;&#26234;&#33021;&#20307;&#26085;&#30410;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#33021;&#21147;&#25193;&#23637;&#21040;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#23384;&#22312;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#22312;&#38646;&#26679;&#26412;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21407;&#22240;&#26159;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#23384;&#22312;&#20004;&#20010;&#29305;&#23450;&#25361;&#25112;&#65306;&#38598;&#20013;&#24335;&#39044;&#35757;&#32451;&#21644;&#20998;&#25955;&#24335;&#25191;&#34892;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#25968;&#37327;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#20351;&#24471;&#38590;&#20197;&#21019;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#34920;&#31034;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#25513;&#30721;&#39044;&#35757;&#32451;&#26694;&#26550;(MaskMA)&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#37319;&#29992;&#36866;&#21512;&#20110;&#24102;&#26377;&#37096;&#20998;&#35266;&#27979;&#30340;&#20998;&#25955;&#24335;&#25191;&#34892;&#30340;&#22522;&#20110;&#25513;&#30721;&#30340;&#21327;&#20316;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;MaskMA&#36890;&#36807;&#23558;&#21160;&#20316;&#31354;&#38388;&#21010;&#20998;&#20026;&#38754;&#21521;&#33258;&#36523;&#20449;&#24687;&#30340;&#21160;&#20316;&#21644;&#19982;&#20182;&#20154;&#30456;&#20851;&#30340;&#21160;&#20316;&#65292;&#34701;&#20837;&#20102;&#21487;&#27867;&#21270;&#30340;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a single generalist agent with zero-shot capability has recently sparked significant advancements in decision-making. However, extending this capability to multi-agent scenarios presents challenges. Most current works struggle with zero-shot capabilities, due to two challenges particular to the multi-agent settings: a mismatch between centralized pretraining and decentralized execution, and varying agent numbers and action spaces, making it difficult to create generalizable representations across diverse downstream tasks. To overcome these challenges, we propose a \textbf{Mask}ed pretraining framework for \textbf{M}ulti-\textbf{a}gent decision making (MaskMA). This model, based on transformer architecture, employs a mask-based collaborative learning strategy suited for decentralized execution with partial observation. Moreover, MaskMA integrates a generalizable action representation by dividing the action space into actions toward self-information and actions related to other 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.04218</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;&#20063;&#31216;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#65289;&#26159;&#32534;&#30721;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#22312;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#65292;&#38543;&#26426;&#21464;&#37327;&#34987;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#19988;&#35268;&#23450;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#22312;&#32473;&#23450;&#20854;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#19982;&#20854;&#31062;&#20808;&#33410;&#28857;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21516;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#21487;&#20197;&#20934;&#30830;&#32534;&#30721;&#30456;&#21516;&#30340;&#19968;&#32452;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#65292;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31561;&#20215;&#31867;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65288;MEC&#65289;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#23545;&#20110;MEC&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20123;&#32654;&#20029;&#30340;&#32452;&#21512;&#29305;&#24449;&#65292;&#24182;&#19988;&#24050;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#21516;&#19968;MEC&#20013;&#30340;&#25152;&#26377;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#24517;&#39035;&#20855;&#26377;&#30456;&#21516;&#30340;&#8220;&#39592;&#26550;&#8221;&#65288;&#24213;&#23618;&#26080;&#21521;&#22270;&#65289;&#21644;v-&#32467;&#26500;&#65288;&#24418;&#24335;&#20026;$a\rightarrow b \leftarrow c$&#30340;&#35825;&#23548;&#23376;&#22270;&#65289;&#12290;&#36825;&#20123;&#32452;&#21512;&#29305;&#24449;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#33258;&#28982;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
&lt;/p&gt;</description></item><item><title>&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.02861</link><description>&lt;p&gt;
Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02861
&lt;/p&gt;
&lt;p&gt;
&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#37238;&#39044;&#27979;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#22270;&#24322;&#24120;&#30340;&#28508;&#22312;&#23646;&#24615;&#65292;&#23548;&#33268;&#26694;&#26550;&#35774;&#35745;&#19981;&#21487;&#35299;&#37322;&#21644;&#24615;&#33021;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#37325;&#26032;&#30740;&#31350;&#20102;&#24322;&#24120;&#21644;&#27491;&#24120;&#22270;&#20043;&#38388;&#30340;&#20809;&#35889;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31867;&#20043;&#38388;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#20449;&#21495;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#21487;&#20197;&#29992;&#20854;&#29790;&#21033;&#21830;&#34920;&#31034;&#65292;&#36825;&#34920;&#26126;&#29790;&#21033;&#21830;&#26159;&#22270;&#24322;&#24120;&#23646;&#24615;&#30340;&#19968;&#20010;&#39537;&#21160;&#22240;&#32032;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rayleigh Quotient Graph Neural Network&#65288;RQGNN&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#20809;&#35889;GNN&#65292;&#20026;&#25506;&#32034;&#24322;&#24120;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#20197;&#27491;&#30830;&#24615;&#30340;&#35282;&#24230;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#19982;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#36923;&#36753;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.10092</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#65306;&#30693;&#36947;&#20309;&#26102;&#20570;&#20160;&#20040;&#21644;&#20309;&#26102;&#23547;&#27714;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help. (arXiv:2309.10092v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#20197;&#27491;&#30830;&#24615;&#30340;&#35282;&#24230;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#19982;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#36923;&#36753;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#20219;&#21153;&#26159;&#20197;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#34920;&#36798;&#24182;&#20197;&#26102;&#38388;&#21644;&#36923;&#36753;&#39034;&#24207;&#23436;&#25104;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#12290;&#20026;&#20102;&#27491;&#24335;&#23450;&#20041;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;NL&#30340;&#21407;&#23376;&#35859;&#35789;&#22312;LTL&#19978;&#23450;&#20041;&#20102;&#27169;&#22411;&#12290;&#36825;&#19982;&#30456;&#20851;&#30340;&#35268;&#21010;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21407;&#23376;&#35859;&#35789;&#19978;&#23450;&#20041;&#20102;&#25429;&#25417;&#25152;&#38656;&#20302;&#32423;&#31995;&#32479;&#37197;&#32622;&#30340;LTL&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#26426;&#22120;&#20154;&#35745;&#21010;&#65292;&#28385;&#36275;&#22522;&#20110;NL&#30340;&#21407;&#23376;&#21629;&#39064;&#23450;&#20041;&#30340;LTL&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#19968;&#20010;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#22312;&#20110;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#19982;&#36825;&#20123;LTL&#32534;&#30721;&#30340;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HERACLEs&#65292;&#19968;&#20010;&#20998;&#23618;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#29616;&#26377;&#24037;&#20855;&#30340;&#26032;&#22411;&#25972;&#21512;&#65292;&#21253;&#25324;&#65288;i&#65289;&#33258;&#21160;&#26426;&#29702;&#35770;&#65292;&#20197;&#30830;&#23450;&#26426;&#22120;&#20154;&#24212;&#35813;&#23436;&#25104;&#30340;NL&#25351;&#23450;&#30340;&#23376;&#20219;&#21153;&#20197;&#25512;&#36827;&#20219;&#21153;&#36827;&#23637;&#65307;
&lt;/p&gt;
&lt;p&gt;
This paper addresses a new motion planning problem for mobile robots tasked with accomplishing multiple high-level sub-tasks, expressed using natural language (NL), in a temporal and logical order. To formally define such missions, we leverage LTL defined over NL-based atomic predicates modeling the considered NL-based sub-tasks. This is contrast to related planning approaches that define LTL tasks over atomic predicates capturing desired low-level system configurations. Our goal is to design robot plans that satisfy LTL tasks defined over NL-based atomic propositions. A novel technical challenge arising in this setup lies in reasoning about correctness of a robot plan with respect to such LTL-encoded tasks. To address this problem, we propose HERACLEs, a hierarchical conformal natural language planner, that relies on a novel integration of existing tools that include (i) automata theory to determine the NL-specified sub-task the robot should accomplish next to make mission progress; (
&lt;/p&gt;</description></item><item><title>GenDOM&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#25805;&#20316;&#31574;&#30053;&#21644;&#22810;&#26679;&#21270;&#27169;&#25311;&#35757;&#32451;&#65292;&#20351;&#25805;&#20316;&#31574;&#30053;&#33021;&#22815;&#20165;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#21644;&#27169;&#25311;&#20043;&#38388;&#30340;&#28857;&#20113;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#26032;&#29289;&#20307;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.09051</link><description>&lt;p&gt;
GenDOM&#65306;&#20855;&#26377;&#21442;&#25968;&#24863;&#30693;&#30340;&#27867;&#21270;&#24615;&#19968;&#27425;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
GenDOM: Generalizable One-shot Deformable Object Manipulation with Parameter-Aware Policy. (arXiv:2309.09051v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09051
&lt;/p&gt;
&lt;p&gt;
GenDOM&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#25805;&#20316;&#31574;&#30053;&#21644;&#22810;&#26679;&#21270;&#27169;&#25311;&#35757;&#32451;&#65292;&#20351;&#25805;&#20316;&#31574;&#30053;&#33021;&#22815;&#20165;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#21644;&#27169;&#25311;&#20043;&#38388;&#30340;&#28857;&#20113;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#26032;&#29289;&#20307;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#36816;&#21160;&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#21464;&#24418;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#24448;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#26041;&#27861;&#65288;&#22914;&#32499;&#23376;&#21644;&#24067;&#26009;&#65289;&#36890;&#24120;&#38656;&#35201;&#25968;&#30334;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#35757;&#32451;&#27599;&#20010;&#29289;&#20307;&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GenDOM&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#20801;&#35768;&#25805;&#20316;&#31574;&#30053;&#21482;&#38656;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#25805;&#20316;&#31574;&#30053;&#19982;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#21487;&#21464;&#24418;&#29289;&#20307;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#31574;&#30053;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#29289;&#20307;&#21442;&#25968;&#35843;&#25972;&#21160;&#20316;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#32473;&#23450;&#19968;&#20010;&#26032;&#30340;&#29289;&#20307;&#65292;GenDOM&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#21644;&#27169;&#25311;&#20043;&#38388;&#28857;&#20113;&#30340;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#65292;&#32780;&#21482;&#38656;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the inherent uncertainty in their deformability during motion, previous methods in deformable object manipulation, such as rope and cloth, often required hundreds of real-world demonstrations to train a manipulation policy for each object, which hinders their applications in our ever-changing world. To address this issue, we introduce GenDOM, a framework that allows the manipulation policy to handle different deformable objects with only a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable object parameters and training it with a diverse range of simulated deformable objects so that the policy can adjust actions based on different object parameters. At the time of inference, given a new object, GenDOM can estimate the deformable object parameters with only a single real-world demonstration by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations in a differentiable phys
&lt;/p&gt;</description></item><item><title>ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.15459</link><description>&lt;p&gt;
ParaGuide: &#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;
&lt;/p&gt;
&lt;p&gt;
ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer. (arXiv:2308.15459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15459
&lt;/p&gt;
&lt;p&gt;
ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#26159;&#22312;&#20445;&#30041;&#24847;&#20041;&#30340;&#21516;&#26102;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#23646;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#26631;&#39118;&#26684;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#23450;&#20041;&#65292;&#20174;&#21333;&#19968;&#23646;&#24615;&#65288;&#20363;&#22914;&#27491;&#24335;&#24615;&#65289;&#21040;&#20316;&#32773;&#65288;&#20363;&#22914;&#33678;&#22763;&#27604;&#20122;&#65289;&#12290;&#20808;&#21069;&#30340;&#26080;&#30417;&#30563;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20165;&#36866;&#29992;&#20110;&#22266;&#23450;&#30340;&#39118;&#26684;&#38598;&#65292;&#25110;&#38656;&#35201;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;ParaGuide&#21033;&#29992;&#20102;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#26469;&#33258;&#29616;&#25104;&#30340;&#20998;&#31867;&#22120;&#21644;&#24378;&#22823;&#30340;&#39118;&#26684;&#23884;&#20837;&#22120;&#30340;&#26799;&#24230;&#24341;&#23548;&#65292;&#20197;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;Enron&#37038;&#20214;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27491;&#24335;&#24615;&#21644;... (&#20869;&#23481;&#22826;&#22810;&#65292;&#35831;&#21442;&#32771;&#33521;&#25991;&#25688;&#35201;)
&lt;/p&gt;
&lt;p&gt;
Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CodeCoT&#21644;Beyond&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#20174;&#23569;&#37327;&#29305;&#23450;&#25968;&#25454;&#20013;&#36827;&#34892;&#36866;&#24212;&#12290;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#24341;&#23548;&#65292;&#27169;&#22411;&#21487;&#20197;&#25581;&#31034;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#26816;&#26597;&#19981;&#26029;&#20248;&#21270;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2308.08784</link><description>&lt;p&gt;
CodeCoT&#21450;&#20854;&#36827;&#23637;&#65306;&#23398;&#20064;&#20687;&#24320;&#21457;&#32773;&#19968;&#26679;&#32534;&#31243;&#21644;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CodeCoT and Beyond: Learning to Program and Test like a Developer. (arXiv:2308.08784v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CodeCoT&#21644;Beyond&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#20174;&#23569;&#37327;&#29305;&#23450;&#25968;&#25454;&#20013;&#36827;&#34892;&#36866;&#24212;&#12290;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#24341;&#23548;&#65292;&#27169;&#22411;&#21487;&#20197;&#25581;&#31034;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#26816;&#26597;&#19981;&#26029;&#20248;&#21270;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;OpenAI&#24320;&#21457;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-x&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#29616;&#29366;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#20219;&#21153;&#26102;&#24120;&#24120;&#36935;&#21040;&#25361;&#25112;&#65292;&#36896;&#25104;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26377;&#20215;&#20540;&#25216;&#26415;&#65292;&#20801;&#35768;LLM&#22312;&#26368;&#23569;&#30340;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#19978;&#36827;&#34892;&#36866;&#24212;&#12290;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#65292;&#24050;&#34987;&#24341;&#20837;&#20197;&#25351;&#23548;LLM&#22312;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#20013;&#25581;&#31034;&#35748;&#30693;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Code Chain-of-Thought&#65288;CodeCoT&#65289;&#65292;&#23427;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#32463;&#20856;CodeCoT&#21644;&#33258;&#25105;&#26816;&#26597;CodeCoT&#12290;&#21518;&#32773;&#21152;&#20837;&#20102;&#33258;&#25105;&#26816;&#26597;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36845;&#20195;&#29983;&#25104;&#20195;&#30721;&#65292;&#21046;&#23450;&#27979;&#35797;&#29992;&#20363;&#24182;&#25913;&#21892;&#20854;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#27169;&#22411;&#29983;&#25104;&#19982;&#20998;&#31867;&#21035;&#29305;&#24449;&#23545;&#24212;&#30340;&#27979;&#35797;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In natural language processing, transformer-based large language models (LLMs) like GPT-x models developed by OpenAI have revolutionized the landscape. Despite their impressive capabilities, these models often encounter challenges when handling tasks that differ from their training data, resulting in compromised performance. To address this, few-shot learning has emerged as a valuable technique, allowing LLMs to adapt with minimal task-specific data. One innovative strategy, known as Chain-of-Thought Prompting (CoT), has been introduced to guide LLMs in revealing cognitive processes during multi-step reasoning. In this paper, we propose Code Chain-of-Thought~(CodeCoT), which consists of two components: the Vanilla CodeCoT and the Self-exam CodeCoT. The latter incorporates self-examination, empowering the model to iteratively generate code, formulate test cases, and refine its outputs. Specifically, the process entails the generation of test examples by the model corresponding to the co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#25913;&#21892;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13292</link><description>&lt;p&gt;
&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#25913;&#36827;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Variance-Covariance Regularization Improves Representation Learning. (arXiv:2306.13292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13292
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#25913;&#21892;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#20174;&#19968;&#20010;&#39046;&#22495;&#33719;&#24471;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20851;&#20110;&#36825;&#20123;&#21518;&#32493;&#20219;&#21153;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24378;&#26377;&#21147;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#35201;&#27714;&#22312;&#21021;&#22987;&#39044;&#35757;&#32451;&#38454;&#27573;&#25429;&#33719;&#21508;&#31181;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#36275;&#22815;&#30340;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#38598;&#20013;&#20110;&#20027;&#35201;&#20943;&#23569;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#36235;&#21183;&#21487;&#33021;&#23548;&#33268;&#19981;&#20805;&#20998;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#21463;&#25439;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26041;&#24046;-&#21327;&#26041;&#24046;&#27491;&#21017;&#21270;&#65288;VCR&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#20064;&#32593;&#32476;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#12290;&#20511;&#37492;&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#34920;&#29616;&#20986;&#39640;&#26041;&#24046;&#21644;&#39640;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has emerged as a key approach in the machine learning domain, enabling the application of knowledge derived from one domain to improve performance on subsequent tasks. Given the often limited information about these subsequent tasks, a strong transfer learning approach calls for the model to capture a diverse range of features during the initial pretraining stage. However, recent research suggests that, without sufficient regularization, the network tends to concentrate on features that primarily reduce the pretraining loss function. This tendency can result in inadequate feature learning and impaired generalization capability for target tasks. To address this issue, we propose Variance-Covariance Regularization (VCR), a regularization technique aimed at fostering diversity in the learned network features. Drawing inspiration from recent advancements in the self-supervised learning approach, our approach promotes learned representations that exhibit high variance and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04366</link><description>&lt;p&gt;
&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#21487;&#20197;&#36890;&#36807;&#20419;&#36827;&#20219;&#21153;&#24863;&#30693;&#30340;&#22242;&#38431;&#21512;&#20316;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#32780;&#24037;&#20154;&#25307;&#21215;&#21017;&#20195;&#34920;&#30528;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#31574;&#30053;&#20027;&#35201;&#20851;&#27880;&#24037;&#20154;&#26412;&#36523;&#30340;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20219;&#21153;&#25928;&#29992;&#35780;&#20272;&#30340;&#21512;&#29702;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;Mini-Batch K-Means&#32858;&#31867;&#31639;&#27861;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#24037;&#20154;&#25307;&#21215;&#12290;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#21644;&#20219;&#21153;&#35201;&#27714;&#33719;&#24471;&#24037;&#20154;&#30340;&#33021;&#21147;&#31867;&#22411;&#21644;&#36317;&#31163;&#12290;&#20351;&#29992;&#24037;&#20154;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20449;&#20219;&#23548;&#21521;&#22270;&#36755;&#20837;&#33267;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#12290;&#36890;&#36807;&#24037;&#20154;&#20043;&#38388;&#30340;&#39640;&#20449;&#20219;&#20540;&#65292;&#38450;&#27490;CMCS&#22330;&#26223;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26368;&#32456;&#65292;&#21033;&#29992;&#39044;&#27979;&#30340;&#20449;&#20219;&#21644;&#24037;&#20154;&#33021;&#21147;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#21521;&#25307;&#21215;&#22270;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#25307;&#21215;&#26041;&#27861;&#22312;&#25307;&#21215;&#20934;&#30830;&#24230;&#12289;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#30340;&#19977;&#31181;&#29366;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29992;&#23383;&#20856;&#24207;&#20462;&#35746;&#30340;&#37325;&#20889;&#21382;&#21490;&#26159;&#26368;&#26377;&#25928;&#29575;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;Horn&#20844;&#24335;&#26159;&#21542;&#31561;&#20215;&#20110;neg&#12290;</title><link>http://arxiv.org/abs/2305.09200</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#24536;&#35760;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#24335;&#65311;&#27604;&#36739;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#30340;&#29366;&#24577;&#34920;&#31034;(arXiv:2305.09200v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Can we forget how we learned? Representing states in iterated belief revision}. (arXiv:2305.09200v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#30340;&#19977;&#31181;&#29366;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29992;&#23383;&#20856;&#24207;&#20462;&#35746;&#30340;&#37325;&#20889;&#21382;&#21490;&#26159;&#26368;&#26377;&#25928;&#29575;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;Horn&#20844;&#24335;&#26159;&#21542;&#31561;&#20215;&#20110;neg&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#19977;&#31181;&#26368;&#24120;&#35265;&#30340;&#29366;&#24577;&#34920;&#31034;&#26041;&#27861;&#65306;&#26174;&#24335;&#34920;&#31034;&#65292;&#25353;&#23618;&#27425;&#34920;&#31034;&#21644;&#25353;&#21382;&#21490;&#34920;&#31034;&#12290;&#21069;&#32773;&#26159;&#27169;&#22411;&#20043;&#38388;&#30340;&#36830;&#36890;&#20559;&#24207;&#20851;&#31995;&#65292;&#31532;&#20108;&#31181;&#26159;&#34920;&#31034;&#31561;&#20215;&#31867;&#30340;&#20844;&#24335;&#21015;&#34920;&#65292;&#31532;&#19977;&#31181;&#26159;&#20808;&#21069;&#20462;&#35746;&#30340;&#24207;&#21015;&#12290;&#21518;&#32773;&#21462;&#20915;&#20110;&#20462;&#35746;&#35821;&#20041;&#21644;&#21382;&#21490;&#37325;&#20889;&#65292;&#32780;&#21069;&#32773;&#21017;&#21462;&#20915;&#20110;&#20801;&#35768;&#30340;&#37325;&#20889;&#12290;&#25152;&#26377;&#26426;&#21046;&#37117;&#34920;&#31034;&#25152;&#26377;&#21487;&#33021;&#30340;&#29366;&#24577;&#12290;&#29992;&#23383;&#20856;&#24207;&#20462;&#35746;&#30340;&#37325;&#20889;&#21382;&#21490;&#22312;&#22823;&#23567;&#26041;&#38754;&#27604;&#20854;&#20182;&#32771;&#34385;&#30340;&#34920;&#31034;&#26041;&#27861;&#26356;&#26377;&#25928;&#29575;&#12290;&#35777;&#26126;&#20102;&#36825;&#26679;&#19968;&#20010;&#21382;&#21490;&#30340;&#20887;&#20313;&#26159;&#19968;&#31181;&#36731;&#24494;&#30340;&#37325;&#20889;&#12290;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;coNP&#23436;&#20840;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;Horn&#20844;&#24335;&#30340;&#20004;&#27425;&#20462;&#35746;&#21382;&#21490;&#25110;&#20219;&#24847;&#38271;&#24230;&#30340;&#20462;&#35746;&#21382;&#21490;&#19978;&#65292;&#36825;&#20063;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#22312;&#20004;&#20010;Horn&#20844;&#24335;&#30340;&#21382;&#21490;&#19978;&#65292;&#23427;&#26159;&#22810;&#39033;&#24335;&#30340;&#12290;&#19968;&#20010;&#27425;&#35201;&#30340;&#25216;&#26415;&#32467;&#26524;&#26159;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#19968;&#20010;Horn&#20844;&#24335;&#26159;&#21542;&#31561;&#20215;&#20110;neg&#12290;
&lt;/p&gt;
&lt;p&gt;
The three most common representations of states in iterated belief revision are compared: explicit, by levels and by history. The first is a connected preorder between models, the second is a list of formulae representing equivalence classes, the third is the sequence of the previous revisions. The latter depends on the revision semantics and on history rewriting, and the latter depends on the allowed rewritings. All mechanisms represent all possible states. A rewritten history of lexicographic revision is more efficient than the other considered representations in terms of size with arbitrary history rewritings. Establishing the redundancy of such a history is a mild rewriting. It is coNP-complete in the general case, and is hard even on histories of two revisions or revisions of arbitrary length of Horn formulae, and is polynomial on histories of two Horn formulae. A minor technical result is a polynomial-time algorithm for establishing whether a Horn formula is equivalent to the neg
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#65292;&#20197;&#20943;&#23569;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#25552;&#39640;&#31163;&#32447;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03954</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#20197;&#36827;&#34892;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Learning Action Embeddings for Off-Policy Evaluation. (arXiv:2305.03954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#65292;&#20197;&#20943;&#23569;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#25552;&#39640;&#31163;&#32447;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#35780;&#20272;&#65288;OPE&#65289;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#30001;&#19981;&#21516;&#31574;&#30053;&#25910;&#38598;&#30340;&#35760;&#24405;&#25968;&#25454;&#26469;&#35745;&#31639;&#31574;&#30053;&#30340;&#39044;&#26399;&#22870;&#21169;&#12290; OPE&#26159;&#36816;&#34892;&#26114;&#36149;&#30340;&#22312;&#32447;A / B&#27979;&#35797;&#30340;&#21487;&#34892;&#36873;&#25321;&#65306;&#23427;&#21487;&#20197;&#21152;&#24555;&#26032;&#31574;&#30053;&#30340;&#24320;&#21457;&#65292;&#24182;&#38477;&#20302;&#21521;&#23458;&#25143;&#26292;&#38706;&#27425;&#20248;&#27835;&#30103;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#24403;&#21160;&#20316;&#25968;&#37327;&#24456;&#22823;&#25110;&#35760;&#24405;&#31574;&#30053;&#26410;&#20805;&#20998;&#25506;&#32034;&#26576;&#20123;&#25805;&#20316;&#26102;&#65292;&#22522;&#20110;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#30340;&#29616;&#26377;&#20272;&#35745;&#22120;&#21487;&#33021;&#20855;&#26377;&#39640;&#29978;&#33267;&#26080;&#38480;&#26041;&#24046;&#12290;Saito&#21644;Joachims&#25552;&#20986;&#20351;&#29992;&#21160;&#20316;&#23884;&#20837;&#30340;&#36793;&#38469;IPS&#65288;MIPS&#65289;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#38477;&#20302;IPS&#30340;&#26041;&#24046;&#12290; MIPS&#20551;&#35774;&#20174;&#19994;&#32773;&#21487;&#20197;&#23450;&#20041;&#33391;&#22909;&#30340;&#21160;&#20316;&#23884;&#20837;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#38590;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#20013;&#38388;&#36755;&#20986;&#26469;&#23450;&#20041;&#21160;&#20316;&#23884;&#20837;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;MIPS&#20272;&#35745;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy evaluation (OPE) methods allow us to compute the expected reward of a policy by using the logged data collected by a different policy. OPE is a viable alternative to running expensive online A/B tests: it can speed up the development of new policies, and reduces the risk of exposing customers to suboptimal treatments. However, when the number of actions is large, or certain actions are under-explored by the logging policy, existing estimators based on inverse-propensity scoring (IPS) can have a high or even infinite variance. Saito and Joachims (arXiv:2202.06317v2 [cs.LG]) propose marginalized IPS (MIPS) that uses action embeddings instead, which reduces the variance of IPS in large action spaces. MIPS assumes that good action embeddings can be defined by the practitioner, which is difficult to do in many real-world applications. In this work, we explore learning action embeddings from logged data. In particular, we use intermediate outputs of a trained reward model to defin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03237</link><description>&lt;p&gt;
&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24847;&#22270;&#26816;&#27979;&#23545;&#20110;&#23454;&#29992;&#30340;&#23545;&#35805;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#36890;&#24120;&#38656;&#35201;&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#20165;&#38480;&#20110;&#21333;&#36718;&#23545;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#65288;Caro&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36981;&#24490;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#20174;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#26500;&#24314;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20351;&#29992;&#22810;&#35270;&#22270;&#20449;&#24687;&#29942;&#39048;&#25439;&#22833;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;Caro&#20013;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#26469;&#20174;&#36825;&#20123;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25366;&#25496;OOD&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#33258;&#20030;&#26041;&#27861;&#29992;&#36825;&#20123;OOD&#26679;&#26412;&#26469;&#35757;&#32451;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Caro&#22312;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20165;&#32771;&#34385;&#21333;&#36718;&#19978;&#19979;&#25991;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.05949</link><description>&lt;p&gt;
CMOS + &#38543;&#26426;&#32435;&#31859;&#30913;&#20307;&#65306;&#27010;&#29575;&#25512;&#29702;&#19982;&#23398;&#20064;&#24322;&#26500;&#35745;&#31639;&#26426;
&lt;/p&gt;
&lt;p&gt;
CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning. (arXiv:2304.05949v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#30340;&#25918;&#32531;&#65292;&#21033;&#29992;&#26032;&#20852;&#30340;&#32435;&#31859;&#25216;&#26415;&#65288;X&#65289;&#22686;&#24378;&#20114;&#34917;&#37329;&#23646;&#27687;&#21270;&#29289;&#21322;&#23548;&#20307;&#65288;CMOS&#65289;&#26230;&#20307;&#31649;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#12290;&#23613;&#31649;sMTJs&#35774;&#22791;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#24322;&#26500;&#35745;&#31639;&#26426;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;&#20351;&#29992;CMOS&#39044;&#27979;&#27969;&#31243;&#35774;&#35745;&#22871;&#20214;&#65288;PDK&#65289;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25968;&#23383;CMOS-based p-bits&#27169;&#25311;&#39640;&#36136;&#37327;&#38543;&#26426;&#24615;&#38656;&#35201;&#36229;&#36807;10,000&#20010;&#26230;&#20307;&#31649;&#65292;&#27599;&#29983;&#25104;&#19968;&#20010;&#38543;&#26426;&#25968;&#30340;&#33021;&#37327;&#27604;&#20351;&#29992;&#21482;&#28040;&#32791;2fJ&#30340;sMTJ-based p-bits&#39640;&#32422;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#32553;&#25918;&#21644;&#38598;&#25104;&#29256;&#26412;&#21487;&#20197;&#26174;&#30528;&#25512;&#36827;&#27010;&#29575;&#24615;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the slowing down of Moore's law, augmenting complementary-metal-oxide semiconductor (CMOS) transistors with emerging nanotechnologies (X) is becoming increasingly important. In this paper, we demonstrate how stochastic magnetic tunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combined with versatile Field Programmable Gate Arrays (FPGA) to design an energy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Our heterogeneous computer successfully performs probabilistic inference and asynchronous Boltzmann learning despite device-to-device variations in sMTJs. A comprehensive comparison using a CMOS predictive process design kit (PDK) reveals that digital CMOS-based p-bits emulating high-quality randomness use over 10,000 transistors with the energy per generated random number being roughly two orders of magnitude greater than the sMTJ-based p-bits that dissipate only 2 fJ. Scaled and integrated versions of our approach can significantly advance probabilistic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.16755</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#35268;&#27169;&#21270;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#29983;&#25104;&#19981;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#23475;&#30340;&#25991;&#26412;&#25110;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31616;&#21333;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#65288;&#21363;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#20043;&#38388;&#30340;&#27604;&#36739;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#27604;&#36739;&#21453;&#39304;&#21482;&#33021;&#20256;&#36798;&#26377;&#38480;&#30340;&#20851;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;ILF&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#12290;ILF&#30001;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#32452;&#25104;&#65306;&#31532;&#19968;&#27493;&#65292;&#26681;&#25454;&#36755;&#20837;&#65292;&#21021;&#22987;LM&#36755;&#20986;&#21644;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#33410;&#20197;&#29983;&#25104;&#25913;&#36827;&#12290;&#31532;&#20108;&#27493;&#65292;&#36873;&#25321;&#26368;&#22810;&#21453;&#39304;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#27493;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#30340;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ILF&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#31867;&#20284;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;ILF&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16749</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#28508;&#21147;&#26159;&#26368;&#36817;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Language Feedback&#65288;ILF&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;ILF&#22312;&#35757;&#32451;&#26399;&#38388;&#20165;&#38656;&#35201;&#23569;&#37327;&#30340;&#20154;&#24037;&#32534;&#20889;&#21453;&#39304;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#26102;&#19981;&#38656;&#35201;&#30456;&#21516;&#30340;&#21453;&#39304;&#65292;&#22240;&#27492;&#20351;&#29992;&#36215;&#26469;&#26082;&#26041;&#20415;&#21448;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;ILF&#21487;&#20197;&#34987;&#35270;&#20026;&#26368;&#23567;&#21270;&#19982;&#22522;&#20934;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#24182;&#22312;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;ILF&#22312;Mostly Basic Python Problems(MBPP)&#22522;&#20934;&#27979;&#35797;&#19978;&#23558;Codegen-Mono 6.1B&#27169;&#22411;&#30340;pass @ 1&#35206;&#30422;&#29575;&#30456;&#23545;&#25552;&#39640;&#20102;38%&#65288;&#32477;&#23545;&#25552;&#39640;&#20102;10%&#65289;&#65292;&#32988;&#36807;&#20102;&#22312;MBPP&#19978;&#24494;&#35843;&#21644;&#22312;&#20154;&#31867;&#20462;&#22797;&#30340;&#31243;&#24207;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.15103</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#30456;&#20284;&#24615;&#22270;&#35889;&#19978;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning Is Spectral Clustering On Similarity Graph. (arXiv:2303.15103v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#36816;&#20316;&#21407;&#29702;&#21644;&#21407;&#22240;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#12290;&#21033;&#29992;&#36825;&#31181;&#31561;&#20215;&#24615;&#20316;&#20026;&#22522;&#30707;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#24182;&#20005;&#26684;&#25551;&#36848;&#22810;&#27169;&#24577;&#23545;&#35937;&#22914;&#20309;&#34987;&#23884;&#20837;&#21040;&#19968;&#36215;&#12290;&#22312;&#29702;&#35770;&#27934;&#35265;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#28151;&#21512;&#25439;&#22833;&#65292;&#32467;&#21512;&#26032;&#39062;&#30340;&#26680;&#20989;&#25968;&#65292;&#22312;&#22810;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26631;&#20934;&#39640;&#26031;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the kernel mixture loss, incorporating novel kernel functions that outperform the standard Gaussian kernel on several vision datasets.
&lt;/p&gt;</description></item><item><title>&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12292</link><description>&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot causal learning. (arXiv:2301.12292v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12292
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#20844;&#20849;&#25919;&#31574;&#21644;&#22312;&#32447;&#33829;&#38144;&#31561;&#39046;&#22495;&#65292;&#39044;&#27979;&#19981;&#21516;&#24178;&#39044;&#25514;&#26045;&#23545;&#29305;&#23450;&#20010;&#20307;&#30340;&#22240;&#26524;&#24433;&#21709;&#38750;&#24120;&#37325;&#35201;&#12290;&#39044;&#27979;&#29616;&#26377;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#26377;&#35768;&#22810;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#25509;&#21463;&#36807;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#20307;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#20063;&#24456;&#37325;&#35201;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#65306;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CaML&#65292;&#36825;&#26159;&#19968;&#20010;&#22240;&#26524;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#23558;&#27599;&#20010;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#39044;&#27979;&#25928;&#26524;&#20316;&#20026;&#19968;&#20010;&#20219;&#21153;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;CaML&#22312;&#25968;&#21315;&#20010;&#20219;&#21153;&#20013;&#35757;&#32451;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#36890;&#36807;&#25277;&#26679;&#29983;&#25104;&#19968;&#20010;&#24178;&#39044;&#25514;&#26045;&#21450;&#20854;&#25509;&#25910;&#32773;&#21644;&#38750;&#25509;&#25910;&#32773;&#26469;&#26500;&#24314;&#30340;&#12290;&#36890;&#36807;&#21033;&#29992;&#24178;&#39044;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#30340;&#23646;&#24615;&#65289;&#21644;&#20010;&#20307;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#29305;&#23450;&#20010;&#20307;&#30340;&#21307;&#30103;&#35760;&#24405;&#65289;&#65292;CaML&#23398;&#20064;&#22914;&#20309;&#23558;&#24050;&#35266;&#23519;&#21040;&#30340;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#20256;&#36755;&#32473;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#24178;&#39044;&#25514;&#26045;&#24182;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (\emph{e.g.}, a newly invented drug), which these methods do not address. Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, along with its recipients and nonrecipients. By leveraging both intervention information (\emph{e.g.}, a drug's attributes) and individual features~(\emph{e.g.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#26465;&#20214;&#21477;&#24448;&#24448;&#23384;&#22312;&#30528;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#27861;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21457;&#29616;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#26041;&#27861;&#26469;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00068</link><description>&lt;p&gt;
&#20851;&#20110;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#26465;&#20214;&#21477;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Inconsistencies of Conditionals Learned by Masked Language Models. (arXiv:2301.00068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#26465;&#20214;&#21477;&#24448;&#24448;&#23384;&#22312;&#30528;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#27861;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21457;&#29616;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#26041;&#27861;&#26469;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#24207;&#21015;&#20013;&#23398;&#20064;&#39044;&#27979;&#36974;&#34109;&#26631;&#35760;&#26159;&#19968;&#20010;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#24456;&#26377;&#21147;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#35757;&#32451;&#21518;&#65292;&#36825;&#20123;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#26631;&#35760;&#20998;&#24067;&#12290;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19982;&#24120;&#35265;&#20551;&#35774;&#30456;&#21453;&#65292;&#36825;&#31181;&#21452;&#21521;&#26465;&#20214;&#21477;&#32463;&#24120;&#34920;&#29616;&#20986;&#30456;&#24403;&#22823;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#22312;&#32771;&#34385;&#22312;&#19968;&#36215;&#26102;&#19981;&#33021;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#23548;&#20986;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#31181;&#24120;&#35265;&#39118;&#26684;&#65288;T5&#39118;&#26684;&#21644;BERT&#39118;&#26684;&#65289;&#30340;&#31616;&#21333;&#21452;&#23383;&#27597;&#35789;&#27604;&#36739;&#22330;&#26223;&#20013;&#36890;&#36807;&#23454;&#35777;&#37327;&#21270;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;T5&#27169;&#22411;&#32463;&#24120;&#28151;&#28102;&#33258;&#24049;&#23545;&#20004;&#20010;&#30456;&#20284;&#21452;&#23383;&#27597;&#35789;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19981;&#19968;&#33268;&#24615;&#22312;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;RoBERTa-base&#21040;GLM-130B&#12290;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21021;&#22987;&#23581;&#35797;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#65292;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to predict masked tokens in a sequence has been shown to be a powerful pretraining objective for large language models. After training, such masked language models can provide distributions of tokens conditioned on bidirectional context.  In this paper, we show that contrary to popular assumptions, such bidirectional conditionals often demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together. We empirically quantify such inconsistencies in the simple scenario of bigram comparison for two common styles of masked language models: T5-style and BERT-style. For example, we show that T5 models often confuse their own preference regarding two similar bigrams. We show that inconsistencies exist ubiquitously in masked language models of diverse sizes and configurations, from RoBERTa-base to GLM-130B.  As an initial attempt to address this issue during the inference phase, we propose Ensemble of Conditionals, a se
&lt;/p&gt;</description></item></channel></rss>