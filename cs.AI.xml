<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#27010;&#29575;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#22810;&#27169;&#24577;&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27010;&#29575;&#30340;&#20860;&#23481;&#24615;&#20551;&#35774;&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03650</link><description>&lt;p&gt;
&#37327;&#23376;&#27010;&#29575;&#39537;&#21160;&#19979;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Quantum Probability Driven Framework for Joint Multi-Modal Sarcasm, Sentiment and Emotion Analysis. (arXiv:2306.03650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#27010;&#29575;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#22810;&#27169;&#24577;&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27010;&#29575;&#30340;&#20860;&#23481;&#24615;&#20551;&#35774;&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#26159;&#20154;&#31867;&#23545;&#22806;&#37096;&#20107;&#20214;&#20135;&#29983;&#30340;&#19977;&#31181;&#20856;&#22411;&#33258;&#21457;&#24773;&#24863;&#21453;&#24212;&#65292;&#23427;&#20204;&#19982;&#24444;&#27492;&#32806;&#21512;&#12290;&#36825;&#20123;&#20107;&#20214;&#21487;&#20197;&#29992;&#22810;&#31181;&#34920;&#36798;&#26041;&#24335;&#34920;&#36798;&#65292;&#20363;&#22914;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#32852;&#21512;&#20998;&#26512;&#20154;&#31867;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#20010;&#28041;&#21450;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#36328;&#24773;&#24863;&#30456;&#20851;&#24615;&#30340;&#22797;&#26434;&#35748;&#30693;&#36807;&#31243;&#12290;&#20174;&#27010;&#29575;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36328;&#24773;&#24863;&#30456;&#20851;&#24615;&#24847;&#21619;&#30528;&#23545;&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#30340;&#21028;&#26029;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29616;&#35937;&#30001;&#20110;&#32463;&#20856;&#27010;&#29575;&#29702;&#35770;&#30340;&#20860;&#23481;&#24615;&#20551;&#35774;&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#32780;&#26080;&#27861;&#34987;&#20805;&#20998;&#24314;&#27169;&#12290;&#32771;&#34385;&#21040;&#37327;&#23376;&#27010;&#29575;&#22312;&#24314;&#27169;&#20154;&#31867;&#35748;&#30693;&#26041;&#38754;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#19978;&#19979;&#25991;&#19981;&#30456;&#23481;&#20915;&#31574;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#37327;&#23376;&#27010;&#29575;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sarcasm, sentiment, and emotion are three typical kinds of spontaneous affective responses of humans to external events and they are tightly intertwined with each other. Such events may be expressed in multiple modalities (e.g., linguistic, visual and acoustic), e.g., multi-modal conversations. Joint analysis of humans' multi-modal sarcasm, sentiment, and emotion is an important yet challenging topic, as it is a complex cognitive process involving both cross-modality interaction and cross-affection correlation. From the probability theory perspective, cross-affection correlation also means that the judgments on sarcasm, sentiment, and emotion are incompatible. However, this exposed phenomenon cannot be sufficiently modelled by classical probability theory due to its assumption of compatibility. Neither do the existing approaches take it into consideration. In view of the recent success of quantum probability (QP) in modeling human cognition, particularly contextual incompatible decisio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Jacobi&#22810;&#39033;&#24335;&#21644;&#39057;&#29575;&#20998;&#35299;&#31574;&#30053;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;JGCF&#65292;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03624</link><description>&lt;p&gt;
&#20851;&#20110;&#29992;&#25143;-&#29289;&#21697;&#22270;&#20449;&#21495;&#22788;&#29702;&#30340;&#30740;&#31350;&#65306;&#22522;&#20110;Jacobi&#22810;&#39033;&#24335;&#30340;&#22270;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
On Manipulating Signals of User-Item Graph: A Jacobi Polynomial-based Graph Collaborative Filtering. (arXiv:2306.03624v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Jacobi&#22810;&#39033;&#24335;&#21644;&#39057;&#29575;&#20998;&#35299;&#31574;&#30053;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;JGCF&#65292;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#21033;&#29992;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#20449;&#24687;&#36827;&#34892;&#25512;&#33616;&#12290;&#22270;&#21327;&#21516;&#36807;&#28388;&#30001;&#20110;&#20854;&#22312;&#21033;&#29992;&#29992;&#25143;-&#29289;&#21697;&#20108;&#20998;&#22270;&#20013;&#30340;&#39640;&#38454;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#25512;&#33616;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#32780;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25104;&#21151;&#24402;&#22240;&#20110;&#20854;&#20302;&#36890;&#28388;&#27874;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#32570;&#20047;&#23545;&#19981;&#21516;&#20449;&#21495;&#20998;&#37327;&#22914;&#20309;&#24433;&#21709;&#25512;&#33616;&#20197;&#21450;&#22914;&#20309;&#35774;&#35745;&#31574;&#30053;&#20197;&#27491;&#30830;&#20351;&#29992;&#23427;&#20204;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#20174;&#35889;&#21464;&#25442;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#22270;&#28388;&#27874;&#22120;&#24212;&#32771;&#34385;&#30340;&#37325;&#35201;&#22240;&#32032;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;JGCF&#65292;&#19968;&#31181;&#22522;&#20110;Jacobi&#22810;&#39033;&#24335;&#21644;&#39057;&#29575;&#20998;&#35299;&#31574;&#30053;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39640;&#25928;&#19988;&#26377;&#25928;&#12290;&#22312;&#22235;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22270;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) is an important research direction in recommender systems that aims to make recommendations given the information on user-item interactions. Graph CF has attracted more and more attention in recent years due to its effectiveness in leveraging high-order information in the user-item bipartite graph for better recommendations. Specifically, recent studies show the success of graph neural networks (GNN) for CF is attributed to its low-pass filtering effects. However, current researches lack a study of how different signal components contributes to recommendations, and how to design strategies to properly use them well. To this end, from the view of spectral transformation, we analyze the important factors that a graph filter should consider to achieve better performance. Based on the discoveries, we design JGCF, an efficient and effective method for CF based on Jacobi polynomial bases and frequency decomposition strategies. Extensive experiments on four widely
&lt;/p&gt;</description></item><item><title>BioBLP&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#32534;&#30721;&#19981;&#21516;&#27169;&#24577;&#30340;&#23646;&#24615;&#25968;&#25454;&#24182;&#25903;&#25345;&#23454;&#20307;&#30340;&#32570;&#23569;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.03606</link><description>&lt;p&gt;
BioBLP&#65306;&#29992;&#20110;&#23398;&#20064;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BioBLP: A Modular Framework for Learning on Multimodal Biomedical Knowledge Graphs. (arXiv:2306.03606v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03606
&lt;/p&gt;
&lt;p&gt;
BioBLP&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#32534;&#30721;&#19981;&#21516;&#27169;&#24577;&#30340;&#23646;&#24615;&#25968;&#25454;&#24182;&#25903;&#25345;&#23454;&#20307;&#30340;&#32570;&#23569;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#34920;&#31034;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#36825;&#20123;&#22270;&#35889;&#20013;&#30340;&#26032;&#38142;&#36335;&#12290;&#19968;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#23646;&#24615;&#25968;&#25454;&#65292;&#22914;&#34507;&#30333;&#36136;&#24207;&#21015;&#25110;&#20998;&#23376;&#22270;&#12290;&#20854;&#20182;&#26041;&#27861;&#21017;&#21253;&#21547;&#27492;&#31867;&#25968;&#25454;&#65292;&#20294;&#20551;&#35774;&#23454;&#20307;&#21487;&#20197;&#29992;&#30456;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#34920;&#31034;&#12290;&#28982;&#32780;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#24120;&#24120;&#34920;&#29616;&#20986;&#24322;&#36136;&#27169;&#24577;&#65292;&#36825;&#26159;&#20854;&#22312;&#35813;&#39046;&#22495;&#20013;&#34920;&#31034;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#23454;&#20307;&#23646;&#24615;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#20854;&#20801;&#35768;&#23545;&#19981;&#21516;&#27169;&#24577;&#30340;&#23646;&#24615;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#65292;&#21516;&#26102;&#25903;&#25345;&#32570;&#23569;&#23646;&#24615;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#22810;&#27169;&#24577;&#23454;&#20307;&#25968;&#25454;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are an important tool for representing complex relationships between entities in the biomedical domain. Several methods have been proposed for learning embeddings that can be used to predict new links in such graphs. Some methods ignore valuable attribute data associated with entities in biomedical KGs, such as protein sequences, or molecular graphs. Other works incorporate such data, but assume that entities can be represented with the same data modality. This is not always the case for biomedical KGs, where entities exhibit heterogeneous modalities that are central to their representation in the subject domain.  We propose a modular framework for learning embeddings in KGs with entity attributes, that allows encoding attribute data of different modalities while also supporting entities with missing attributes. We additionally propose an efficient pretraining strategy for reducing the required training runtime. We train models using a biomedical KG containing ap
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#39640;&#25928;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2306.03604</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20419;&#36827;&#31639;&#27861;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#30340;&#39640;&#25928;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach. (arXiv:2306.03604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#39640;&#25928;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21253;&#21547;&#20174;&#28023;&#37327;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30340;&#22823;&#37327;&#19990;&#30028;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#39640;&#23618;&#25351;&#20196;&#26469;&#21327;&#21161;&#31639;&#27861;&#20195;&#29702;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#19982;LLMs&#36827;&#34892;&#20132;&#20114;&#21487;&#33021;&#32791;&#26102;&#36739;&#38271;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#65292;&#21482;&#33021;&#37096;&#32626;&#22312;&#36828;&#31243;&#20113;&#26381;&#21153;&#22120;&#33410;&#28857;&#19978;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21830;&#19994;LLMs&#21487;&#33021;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#26681;&#25454;&#20351;&#29992;&#39057;&#29575;&#25910;&#36153;&#12290;&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#30340;&#39640;&#25928;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#20309;&#26102;&#38656;&#35201;&#26597;&#35810;LLMs&#20197;&#23436;&#25104;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#32423;&#25351;&#20196;&#12290;&#22312;&#28041;&#21450;&#35268;&#21010;&#23376;&#30446;&#26631;&#30340;4&#20010;MiniGrid&#29615;&#22659;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#25552;&#21319;&#20102;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encode a vast amount of world knowledge acquired from massive text datasets. Recent studies have demonstrated that LLMs can assist an algorithm agent in solving complex sequential decision making tasks in embodied environments by providing high-level instructions. However, interacting with LLMs can be time-consuming, as in many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency. In this paper, we explore how to enable efficient and cost-effective interactions between the agent and an LLM. We propose a reinforcement learning based mediator model that determines when it is necessary to consult LLMs for high-level instructions to accomplish a target task. Experiments on 4 MiniGrid environments that entail planning sub-goals demonstrate that our method can learn to solve target tasks with o
&lt;/p&gt;</description></item><item><title>TestLab&#26159;&#19968;&#20010;&#26234;&#33021;&#21270;&#30340;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#27979;&#35797;&#26041;&#27861;&#24182;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#21270;&#36825;&#20123;&#26041;&#27861;&#65292;&#20174;&#24320;&#21457;&#20154;&#21592;&#21040;&#26368;&#32456;&#29992;&#25143;&#30340;&#19981;&#21516;&#33539;&#22260;&#23545;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#36830;&#32493;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#27169;&#22359;&#26469;&#35782;&#21035;&#28431;&#27934;&#21644;&#22686;&#24378;&#20256;&#32479;&#30340;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.03602</link><description>&lt;p&gt;
TestLab&#65306;&#19968;&#31181;&#26234;&#33021;&#21270;&#30340;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TestLab: An Intelligent Automated Software Testing Framework. (arXiv:2306.03602v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03602
&lt;/p&gt;
&lt;p&gt;
TestLab&#26159;&#19968;&#20010;&#26234;&#33021;&#21270;&#30340;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#27979;&#35797;&#26041;&#27861;&#24182;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#21270;&#36825;&#20123;&#26041;&#27861;&#65292;&#20174;&#24320;&#21457;&#20154;&#21592;&#21040;&#26368;&#32456;&#29992;&#25143;&#30340;&#19981;&#21516;&#33539;&#22260;&#23545;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#36830;&#32493;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#27169;&#22359;&#26469;&#35782;&#21035;&#28431;&#27934;&#21644;&#22686;&#24378;&#20256;&#32479;&#30340;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#31995;&#32479;&#30340;&#26222;&#21450;&#24050;&#25104;&#20026;&#29616;&#20195;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#12290;&#36719;&#20214;&#20351;&#29992;&#37327;&#26174;&#33879;&#22686;&#21152;&#65292;&#23548;&#33268;&#36719;&#20214;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#36719;&#20214;&#24320;&#21457;&#21464;&#24471;&#26356;&#21152;&#32791;&#26102;&#12290;&#20026;&#20102;&#21152;&#36895;&#24320;&#21457;&#21608;&#26399;&#65292;&#27979;&#35797;&#38454;&#27573;&#32463;&#24120;&#34987;&#24573;&#30053;&#65292;&#23548;&#33268;&#37096;&#32626;&#26377;&#32570;&#38519;&#30340;&#31995;&#32479;&#65292;&#21487;&#33021;&#23545;&#29992;&#25143;&#30340;&#26085;&#24120;&#27963;&#21160;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TestLab&#65292;&#19968;&#31181;&#26234;&#33021;&#21270;&#30340;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#26694;&#26550;&#65292;&#35797;&#22270;&#25910;&#38598;&#19968;&#32452;&#27979;&#35797;&#26041;&#27861;&#24182;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#21270;&#36825;&#20123;&#26041;&#27861;&#65292;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#21644;&#19981;&#21516;&#33539;&#22260;&#65288;&#20174;&#24320;&#21457;&#20154;&#21592;&#21040;&#26368;&#32456;&#29992;&#25143;&#65289;&#23545;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#36830;&#32493;&#27979;&#35797;&#12290;&#35813;&#24037;&#20855;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26377;&#19981;&#21516;&#30340;&#29992;&#36884;&#12290;&#21069;&#20004;&#20010;&#27169;&#22359;&#26088;&#22312;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#35782;&#21035;&#28431;&#27934;&#65292;&#32780;&#31532;&#19977;&#20010;&#27169;&#22359;&#36890;&#36807;&#33258;&#21160;&#21270;&#22686;&#24378;&#20256;&#32479;&#30340;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of software systems has become an integral part of modern-day living. Software usage has increased significantly, leading to its growth in both size and complexity. Consequently, software development is becoming a more time-consuming process. In an attempt to accelerate the development cycle, the testing phase is often neglected, leading to the deployment of flawed systems that can have significant implications on the users daily activities. This work presents TestLab, an intelligent automated software testing framework that attempts to gather a set of testing methods and automate them using Artificial Intelligence to allow continuous testing of software systems at multiple levels from different scopes, ranging from developers to end-users. The tool consists of three modules, each serving a distinct purpose. The first two modules aim to identify vulnerabilities from different perspectives, while the third module enhances traditional automated software testing by automati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#31995;&#32479;&#20013;&#21019;&#26032;&#21644;&#23454;&#29992;&#24615;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#36807;&#20998;&#27880;&#37325;&#21019;&#26032;&#24615;&#21487;&#33021;&#23548;&#33268;&#24187;&#35273;&#65292;&#32780;&#36807;&#24230;&#20851;&#27880;&#23454;&#29992;&#24615;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#21270;&#12290;&#20316;&#32773;&#25552;&#20986;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#35201;&#32032;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#26088;&#22312;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#29983;&#25104;&#21019;&#26032;&#19988;&#23454;&#29992;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2306.03601</link><description>&lt;p&gt;
&#29983;&#25104;AI&#30340;&#21019;&#36896;&#24615;&#26032;&#39046;&#22495;&#65306;&#24179;&#34913;&#21019;&#26032;&#21644;&#23454;&#29992;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
The Creative Frontier of Generative AI: Managing the Novelty-Usefulness Tradeoff. (arXiv:2306.03601v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#31995;&#32479;&#20013;&#21019;&#26032;&#21644;&#23454;&#29992;&#24615;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#36807;&#20998;&#27880;&#37325;&#21019;&#26032;&#24615;&#21487;&#33021;&#23548;&#33268;&#24187;&#35273;&#65292;&#32780;&#36807;&#24230;&#20851;&#27880;&#23454;&#29992;&#24615;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#21270;&#12290;&#20316;&#32773;&#25552;&#20986;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#35201;&#32032;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#26088;&#22312;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#29983;&#25104;&#21019;&#26032;&#19988;&#23454;&#29992;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20154;&#31867;&#21019;&#36896;&#21147;&#25991;&#29486;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#25506;&#35752;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#20013;&#21019;&#26032;&#21644;&#23454;&#29992;&#24615;&#30340;&#26368;&#20339;&#24179;&#34913;&#12290;&#25105;&#20204;&#35748;&#20026;&#36807;&#20998;&#24378;&#35843;&#20219;&#19968;&#26041;&#38754;&#37117;&#20250;&#23548;&#33268;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#24187;&#35273;&#21644;&#35760;&#24518;&#21270;&#12290;&#24403;&#27169;&#22411;&#23558;&#21019;&#26032;&#24615;&#32622;&#20110;&#23454;&#29992;&#24615;&#20043;&#19978;&#26102;&#65292;&#20250;&#20986;&#29616;&#21253;&#21547;&#38543;&#26426;&#19981;&#20934;&#30830;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;AI&#21709;&#24212;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#36807;&#24230;&#20851;&#27880;&#23454;&#29992;&#24615;&#21017;&#21487;&#33021;&#23548;&#33268;AI&#27169;&#22411;&#22797;&#21046;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#20869;&#23481;&#65292;&#21363;&#35760;&#24518;&#21270;&#65292;&#21487;&#33021;&#20250;&#38480;&#21046;&#21019;&#36896;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#29305;&#23450;&#39046;&#22495;&#30340;&#20998;&#26512;&#12289;&#25968;&#25454;&#21644;&#36716;&#31227;&#23398;&#20064;&#12289;&#29992;&#25143;&#20559;&#22909;&#21644;&#23450;&#21046;&#12289;&#33258;&#23450;&#20041;&#35780;&#20215;&#25351;&#26631;&#20197;&#21450;&#21327;&#20316;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#29983;&#25104;&#26082;&#26032;&#39062;&#21448;&#23454;&#29992;&#30340;&#20869;&#23481;&#65292;&#21516;&#26102;&#32771;&#34385;&#21508;&#31181;&#19978;&#19979;&#25991;&#30340;&#29420;&#29305;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, drawing inspiration from the human creativity literature, we explore the optimal balance between novelty and usefulness in generative Artificial Intelligence (AI) systems. We posit that overemphasizing either aspect can lead to limitations such as hallucinations and memorization. Hallucinations, characterized by AI responses containing random inaccuracies or falsehoods, emerge when models prioritize novelty over usefulness. Memorization, where AI models reproduce content from their training data, results from an excessive focus on usefulness, potentially limiting creativity. To address these challenges, we propose a framework that includes domain-specific analysis, data and transfer learning, user preferences and customization, custom evaluation metrics, and collaboration mechanisms. Our approach aims to generate content that is both novel and useful within specific domains, while considering the unique requirements of various contexts.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27604;&#36739;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#21644;&#20799;&#31461;&#30340;&#23398;&#20064;&#36712;&#36857;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#36981;&#24490;&#23558;&#38899;&#38901;&#20316;&#20026;&#36215;&#28857;&#36880;&#27493;&#20064;&#24471;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#27169;&#24335;&#65292;&#32780;&#19988;&#37117;&#34920;&#29616;&#20986;&#23545;&#20110;&#26576;&#20123;&#35821;&#35328;&#32467;&#26500;&#26377;&#20020;&#30028;&#26399;&#30340;&#23398;&#20064;&#24773;&#20917;&#65292;&#20294;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#36824;&#26159;&#23384;&#22312;&#37325;&#35201;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03586</link><description>&lt;p&gt;
&#35821;&#35328;&#20064;&#24471;&#65306;&#20799;&#31461;&#21644;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#36981;&#24490;&#30456;&#20284;&#30340;&#23398;&#20064;&#38454;&#27573;&#65311;
&lt;/p&gt;
&lt;p&gt;
Language acquisition: do children and language models follow similar learning stages?. (arXiv:2306.03586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03586
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27604;&#36739;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#21644;&#20799;&#31461;&#30340;&#23398;&#20064;&#36712;&#36857;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#36981;&#24490;&#23558;&#38899;&#38901;&#20316;&#20026;&#36215;&#28857;&#36880;&#27493;&#20064;&#24471;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#27169;&#24335;&#65292;&#32780;&#19988;&#37117;&#34920;&#29616;&#20986;&#23545;&#20110;&#26576;&#20123;&#35821;&#35328;&#32467;&#26500;&#26377;&#20020;&#30028;&#26399;&#30340;&#23398;&#20064;&#24773;&#20917;&#65292;&#20294;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#36824;&#26159;&#23384;&#22312;&#37325;&#35201;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#20064;&#24471;&#36807;&#31243;&#20013;&#65292;&#20799;&#31461;&#20250;&#25353;&#29031;&#20856;&#22411;&#30340;&#23398;&#20064;&#38454;&#27573;&#39034;&#24207;&#23398;&#20064;&#35821;&#35328;&#65292;&#39318;&#20808;&#23398;&#20064;&#21457;&#38899;&#20998;&#31867;&#65292;&#28982;&#21518;&#21457;&#23637;&#35789;&#27719;&#65292;&#26368;&#32456;&#25484;&#25569;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#21477;&#27861;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#36825;&#31181;&#23398;&#20064;&#36712;&#36857;&#30340;&#35745;&#31639;&#21407;&#21017;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#30693;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#36712;&#36857;&#21644;&#20799;&#31461;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;GPT-2&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;18&#20010;&#26376;&#33267;6&#23681;&#20799;&#31461;&#30456;&#20284;&#30340;&#35821;&#35328;&#20064;&#24471;&#38454;&#27573;&#12290;&#36890;&#36807;&#20174;BLiMP&#12289;Zorro&#21644;BIG-Bench&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;96&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;48&#20010;GPT-2&#27169;&#22411;&#65292;&#24182;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#33021;&#21147;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#35780;&#20272;&#19982;54&#20010;&#20799;&#31461;&#30340;&#35821;&#35328;&#20135;&#29983;&#36807;&#31243;&#34892;&#20026;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#19982;&#20799;&#31461;&#19968;&#26679;&#65292;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#39318;&#20808;&#20064;&#24471;&#38899;&#38901;&#20449;&#24687;&#65292;&#28982;&#21518;&#36880;&#28176;&#23398;&#20064;&#20351;&#29992;&#27491;&#30830;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29983;&#25104;&#21333;&#35789;&#21644;&#21477;&#23376;&#12290;&#20854;&#27425;&#65292;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#23545;&#26576;&#20123;&#35821;&#35328;&#32467;&#26500;&#30340;&#23398;&#20064;&#26377;&#20020;&#30028;&#26399;&#65292;&#31867;&#20284;&#20110;&#20799;&#31461;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#24635;&#20307;&#19978;&#30340;&#23398;&#20064;&#36712;&#36857;&#30456;&#20284;&#65292;&#20294;&#20063;&#23384;&#22312;&#20799;&#31461;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#25351;&#21521;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend
&lt;/p&gt;</description></item><item><title>RDFC-GAN&#20351;&#29992;&#20004;&#20010;&#20998;&#25903;&#32467;&#26500;&#29983;&#25104;&#31934;&#30830;&#30340;&#28145;&#24230;&#22270;&#20687;&#65292;&#23427;&#36890;&#36807;&#35299;&#20915;&#23460;&#20869;&#29615;&#22659;&#20013;&#26222;&#36941;&#32570;&#22833;&#30340;&#22823;&#38754;&#31215;&#28145;&#24230;&#20540;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#23460;&#20869;&#28145;&#24230;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03584</link><description>&lt;p&gt;
RDFC-GAN:&#23460;&#20869;&#28145;&#24230;&#23436;&#24418;&#34917;&#20840;&#30340;RGB-Depth&#34701;&#21512;CycleGAN(arXiv:2306.03584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
RDFC-GAN: RGB-Depth Fusion CycleGAN for Indoor Depth Completion. (arXiv:2306.03584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03584
&lt;/p&gt;
&lt;p&gt;
RDFC-GAN&#20351;&#29992;&#20004;&#20010;&#20998;&#25903;&#32467;&#26500;&#29983;&#25104;&#31934;&#30830;&#30340;&#28145;&#24230;&#22270;&#20687;&#65292;&#23427;&#36890;&#36807;&#35299;&#20915;&#23460;&#20869;&#29615;&#22659;&#20013;&#26222;&#36941;&#32570;&#22833;&#30340;&#22823;&#38754;&#31215;&#28145;&#24230;&#20540;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#23460;&#20869;&#28145;&#24230;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#28145;&#24230;&#20256;&#24863;&#22120;&#25429;&#25417;&#30340;&#21407;&#22987;&#28145;&#24230;&#22270;&#20687;&#36890;&#24120;&#20855;&#26377;&#22823;&#37327;&#32570;&#22833;&#28145;&#24230;&#20540;&#30340;&#33539;&#22260;&#65292;&#23548;&#33268;&#20102;&#24456;&#22810;&#24102;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#30340;&#19981;&#23436;&#25972;&#30340;&#28145;&#24230;&#22270;&#65292;&#22240;&#27492;&#24050;&#32463;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#23436;&#25104;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;RDFC-GAN&#30340;&#26032;&#39062;&#30340;&#21452;&#25903;&#31471;&#21040;&#31471;&#34701;&#21512;&#32593;&#32476;&#65292;&#23427;&#38656;&#35201;&#19968;&#23545;RGB&#21644;&#19981;&#23436;&#25972;&#28145;&#24230;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#19968;&#20010;&#23494;&#38598;&#30340;&#21644;&#23436;&#25104;&#30340;&#28145;&#24230;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The raw depth image captured by indoor depth sensors usually has an extensive range of missing depth values due to inherent limitations such as the inability to perceive transparent objects and the limited distance range. The incomplete depth map with missing values burdens many downstream vision tasks, and a rising number of depth completion methods have been proposed to alleviate this issue. While most existing methods can generate accurate dense depth maps from sparse and uniformly sampled depth maps, they are not suitable for complementing large contiguous regions of missing depth values, which is common and critical in images captured in indoor environments. To overcome these challenges, we design a novel two-branch end-to-end fusion network named RDFC-GAN, which takes a pair of RGB and incomplete depth images as input to predict a dense and completed depth map. The first branch employs an encoder-decoder structure, by adhering to the Manhattan world assumption and utilizing norma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; L-C2ST &#30340;&#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30446;&#21069;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#38480;&#21046;&#35299;&#20915;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03580</link><description>&lt;p&gt;
L-C2ST: &#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference. (arXiv:2306.03580v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; L-C2ST &#30340;&#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30446;&#21069;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#38480;&#21046;&#35299;&#20915;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#27169;&#25311;&#25512;&#26029;&#65288;SBI&#65289;&#30340;&#24037;&#20316;&#37117;&#20381;&#36182;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#36817;&#20284;&#22797;&#26434;&#12289;&#39640;&#32500;&#24230;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#20123;&#36817;&#20284;&#26159;&#21542;&#21487;&#20449;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#22312;&#35266;&#27979;&#31354;&#38388;&#26399;&#26395;&#19979;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19981;&#33021;&#36275;&#22815;&#22320;&#30830;&#23450;&#21738;&#20123;&#35266;&#27979;&#32467;&#26524;&#21487;&#20197;&#20449;&#20219;&#36825;&#20123;&#36817;&#20284;&#25110;&#24212;&#35813;&#25913;&#36827;&#12290;&#25105;&#20204;&#22522;&#20110;&#33879;&#21517;&#30340;&#20998;&#31867;&#22120;&#20004;&#26679;&#26412;&#26816;&#39564; (C2ST)&#65292;&#24341;&#20837; L-C2ST&#65292;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#12290;&#23427;&#25552;&#20379;&#26377;&#29702;&#35770;&#22522;&#30784;&#21644;&#26131;&#20110;&#35299;&#37322;&#30340;&#65292;&#22914;&#22270;&#31034;&#35786;&#26029;&#12290;&#19982; C2ST &#19981;&#21516;&#30340;&#26159;&#65292;L-C2ST &#19981;&#38656;&#35201;&#35775;&#38382;&#30495;&#23454;&#21518;&#39564;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;L-C2ST &#21487;&#20197;&#19987;&#38376;&#25552;&#20379;&#26356;&#22909;&#30340;&#32479;&#35745;&#21151;&#29575;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret - e.g. graphical - diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, L-C2ST can be specialized to offer better statistical power, while being computationally more efficien
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Clausal Tableaux&#35777;&#26126;&#31995;&#32479;&#23454;&#29616;&#21487;&#34892;&#30340;&#33539;&#22260;&#38480;&#21046;&#25554;&#20540;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03572</link><description>&lt;p&gt;
&#36890;&#36807;Clausal Tableaux&#23454;&#29616;&#33539;&#22260;&#38480;&#21046;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Range-Restricted Interpolation through Clausal Tableaux. (arXiv:2306.03572v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03572
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Clausal Tableaux&#35777;&#26126;&#31995;&#32479;&#23454;&#29616;&#21487;&#34892;&#30340;&#33539;&#22260;&#38480;&#21046;&#25554;&#20540;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#38454;&#36923;&#36753;&#30340;Clausal Tableaux&#35777;&#26126;&#31995;&#32479;&#65292;&#20174;&#36755;&#20837;&#21040;&#36755;&#20986;&#20256;&#36882;&#21464;&#21270;&#30340;&#33539;&#22260;&#38480;&#21046;&#21644;Horn&#24615;&#36136;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#23558;&#35777;&#26126;&#32467;&#26500;&#30340;&#25805;&#20316;&#19982;&#39640;&#24230;&#20248;&#21270;&#30340;&#19968;&#38454;&#35777;&#26126;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#21487;&#34892;&#30340;&#23454;&#29616;&#26041;&#27861;&#12290;&#20027;&#35201;&#24212;&#29992;&#20110;&#26597;&#35810;&#21512;&#25104;&#21644;&#25554;&#20540;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how variations of range-restriction and also the Horn property can be passed from inputs to outputs of Craig interpolation in first-order logic. The proof system is clausal tableaux, which stems from first-order ATP. Our results are induced by a restriction of the clausal tableau structure, which can be achieved in general by a proof transformation, also if the source proof is by resolution/paramodulation. Primarily addressed applications are query synthesis and reformulation with interpolation. Our methodical approach combines operations on proof structures with the immediate perspective of feasible implementation through incorporating highly optimized first-order provers.
&lt;/p&gt;</description></item><item><title>CIN++&#26159;&#19968;&#31181;&#25299;&#25169;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#24213;&#23618;&#22797;&#21512;&#20307;&#20013;&#29615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#34920;&#36798;&#33021;&#21147;&#12289;&#22788;&#29702;&#38271;&#31243;&#20132;&#20114;&#21644;&#24314;&#27169;&#39640;&#38454;&#32467;&#26500;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03561</link><description>&lt;p&gt;
CIN++&#65306;&#22686;&#24378;&#25299;&#25169;&#20449;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
CIN++: Enhancing Topological Message Passing. (arXiv:2306.03561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03561
&lt;/p&gt;
&lt;p&gt;
CIN++&#26159;&#19968;&#31181;&#25299;&#25169;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#24213;&#23618;&#22797;&#21512;&#20307;&#20013;&#29615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#34920;&#36798;&#33021;&#21147;&#12289;&#22788;&#29702;&#38271;&#31243;&#20132;&#20114;&#21644;&#24314;&#27169;&#39640;&#38454;&#32467;&#26500;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#23384;&#22312;&#26174;&#33879;&#30340;&#23616;&#38480;&#24615;&#65292;&#38590;&#20197;&#22788;&#29702;&#38271;&#31243;&#20132;&#20114;&#65292;&#24182;&#32570;&#20047;&#23545;&#24314;&#27169;&#39640;&#38454;&#32467;&#26500;&#21644;&#32676;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#22522;&#26412;&#26041;&#27861;&#12290;&#32454;&#32990;&#21516;&#26500;&#32593;&#32476;&#26368;&#36817;&#36890;&#36807;&#22522;&#20110;&#32454;&#32990;&#22797;&#21512;&#20307;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#35299;&#20915;&#20102;&#22823;&#37096;&#20998;&#36825;&#20123;&#25361;&#25112;&#12290;&#23613;&#31649;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;CIN&#20165;&#20351;&#29992;&#36793;&#30028;&#21644;&#19978;&#37096;&#20449;&#24687;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#24213;&#23618;&#22797;&#21512;&#20307;&#20013;&#23384;&#22312;&#30340;&#29615;&#20043;&#38388;&#30340;&#30452;&#25509;&#30456;&#20114;&#20316;&#29992;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23545;&#23398;&#20064;&#35768;&#22810;&#30495;&#23454;&#22797;&#26434;&#29616;&#35937;&#30340;&#34920;&#31034;&#38750;&#24120;&#37325;&#35201;&#65292;&#22914;&#36229;&#20998;&#23376;&#32452;&#35013;&#30340;&#21160;&#21147;&#23398;&#12289;&#33041;&#20869;&#31070;&#32463;&#27963;&#21160;&#21644;&#22522;&#22240;&#35843;&#25511;&#36807;&#31243;&#12290;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;CIN++&#65292;&#36825;&#26159;CIN&#24341;&#20837;&#30340;&#25299;&#25169;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#19968;&#31181;&#22686;&#24378;&#29256;&#12290;&#25105;&#20204;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#32771;&#34385;&#20102;&#29615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have demonstrated remarkable success in learning from graph-structured data. However, they face significant limitations in expressive power, struggling with long-range interactions and lacking a principled approach to modeling higher-order structures and group interactions. Cellular Isomorphism Networks (CINs) recently addressed most of these challenges with a message passing scheme based on cell complexes. Despite their advantages, CINs make use only of boundary and upper messages which do not consider a direct interaction between the rings present in the underlying complex. Accounting for these interactions might be crucial for learning representations of many real-world complex phenomena such as the dynamics of supramolecular assemblies, neural activity within the brain, and gene regulation processes. In this work, we propose CIN++, an enhancement of the topological message passing scheme introduced in CINs. Our message passing scheme accounts for the af
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#20351;&#29992;&#21487;&#36873;&#38899;&#26631;&#26469;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#36890;&#36807;&#24341;&#20837;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#38543;&#26426;&#25513;&#34109;&#21644;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#25552;&#21319;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03557</link><description>&lt;p&gt;
&#21033;&#29992;&#37096;&#20998;&#26631;&#27880;&#30340;&#25991;&#26412;&#25552;&#21319;&#38463;&#25289;&#20271;&#35821;&#38899;&#26631;&#27880;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text. (arXiv:2306.03557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#20351;&#29992;&#21487;&#36873;&#38899;&#26631;&#26469;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#36890;&#36807;&#24341;&#20837;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#38543;&#26426;&#25513;&#34109;&#21644;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#25552;&#21319;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#38463;&#25289;&#20271;&#35821;&#38899;&#26631;&#27880;&#22312;&#24456;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#37117;&#38750;&#24120;&#26377;&#29992;&#65292;&#27604;&#22914;&#23545;&#20110;&#35821;&#35328;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#26631;&#27880;&#21487;&#20197;&#25552;&#20379;&#38405;&#35835;&#25903;&#25345;&#65292;&#32780;&#23545;&#20110;&#35821;&#38899;&#21512;&#25104;&#36825;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#26631;&#27880;&#20934;&#30830;&#24615;&#23545;&#20110;&#21457;&#38899;&#39044;&#27979;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#25968;&#19987;&#27880;&#20110;&#22788;&#29702;&#27809;&#26377;&#38899;&#26631;&#30340;&#21407;&#22987;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#36890;&#36807;&#32473;&#20154;&#31867;&#25552;&#20379;&#36873;&#23450;&#30340;&#25110;&#37096;&#20998;&#26631;&#27880;&#30340;&#25935;&#24863;&#35789;&#27719;&#65292;&#21487;&#20197;&#20351;&#24471;&#29983;&#20135;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25903;&#25345;&#36755;&#20837;&#20013;&#30340;&#21487;&#36873;&#38899;&#26631;&#20197;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#21033;&#29992;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#21644;&#19981;&#21516;&#31561;&#32423;&#30340;&#38543;&#26426;&#25513;&#34109;&#26469;&#25552;&#21319;&#26631;&#27880;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27979;&#35797;&#26399;&#38388;&#25552;&#20379;&#30340;&#26631;&#27880;&#33021;&#22815;&#24433;&#21709;&#26356;&#22810;&#30340;&#36755;&#20986;&#20301;&#32622;&#65292;&#23454;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20943;&#23569;60%&#30340;&#21442;&#25968;&#25968;&#30446;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Arabic diacritization is useful in many applications, ranging from reading support for language learners to accurate pronunciation predictor for downstream tasks like speech synthesis. While most of the previous works focused on models that operate on raw non-diacritized text, production systems can gain accuracy by first letting humans partly annotate ambiguous words. In this paper, we propose 2SDiac, a multi-source model that can effectively support optional diacritics in input to inform all predictions. We also introduce Guided Learning, a training scheme to leverage given diacritics in input with different levels of random masking. We show that the provided hints during test affect more output positions than those annotated. Moreover, experiments on two common benchmarks show that our approach i) greatly outperforms the baseline also when evaluated on non-diacritized text; and ii) achieves state-of-the-art results while reducing the parameter count by over 60%.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;GPT4&#35757;&#32451;&#19968;&#20010;&#22810;&#20195;&#29702;&#31995;&#32479;&#65292;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#20915;&#22823;&#37096;&#20998;ARC&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.03553</link><description>&lt;p&gt;
&#35299;&#20915;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25361;&#25112;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Approach to Solving the Abstraction and Reasoning Corpus (ARC) Challenge. (arXiv:2306.03553v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03553
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;GPT4&#35757;&#32451;&#19968;&#20010;&#22810;&#20195;&#29702;&#31995;&#32479;&#65292;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#24037;&#20855;&#65292;&#21487;&#20197;&#35299;&#20915;&#22823;&#37096;&#20998;ARC&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;GPT4&#65292;&#23558;&#20854;&#32534;&#31243;&#29992;&#20110;&#25191;&#34892;&#20219;&#24847;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#25991;&#26412;&#32473;&#20104;&#35813;&#27169;&#22411;&#19968;&#20123;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#21450;&#19968;&#20123;&#35299;&#20915;ARC&#20219;&#21153;&#30340;&#20856;&#22411;&#36807;&#31243;&#65292;&#24182;&#35201;&#27714;&#20854;&#29983;&#25104;i&#65289;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#30340;&#24191;&#27867;&#25551;&#36848;&#65292;ii&#65289;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#30340;&#35814;&#32454;&#27493;&#39588;&#65292;iii&#65289;&#20351;&#29992;&#35814;&#32454;&#27493;&#39588;&#23545;&#27979;&#35797;&#36755;&#20837;&#36827;&#34892;&#25805;&#20316;&#24182;&#24471;&#20986;&#27979;&#35797;&#36755;&#20986;&#12290;&#30446;&#21069;&#30340;GPT3.5 / GPT4&#25552;&#31034;&#35299;&#20915;&#20102;4&#20010;&#27979;&#35797;&#30340;&#23567;ARC&#25361;&#25112;&#20013;&#30340;2&#20010;&#65288;&#37027;&#20123;&#25317;&#26377;8x8&#21450;&#20197;&#19979;&#23567;&#32593;&#26684;&#30340;&#25361;&#25112;&#65289;&#12290;&#36890;&#36807;&#23545;&#25552;&#31034;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#26356;&#20855;&#20307;&#22320;&#20026;&#29992;&#20363;&#26381;&#21153;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#26356;&#22810;&#38382;&#39064;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#24403;&#34987;&#25193;&#23637;&#25104;&#20026;&#19968;&#20010;&#22810;&#20195;&#29702;&#31995;&#32479;&#65292;&#24182;&#20351;&#29992;&#36807;&#21435;&#30340;&#35760;&#24518;&#20197;&#21450;&#36890;&#36807;&#35270;&#35273;&#38382;&#31572;&#24037;&#20855;&#26469;&#35299;&#37322;&#22270;&#29255;&#26102;&#65292;&#25105;&#20204;&#23454;&#38469;&#19978;&#21487;&#33021;&#33021;&#22815;&#35299;&#20915;&#22823;&#37096;&#20998;ARC&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We utilise the power of Large Language Models (LLMs), in particular GPT4, to be prompt engineered into performing an arbitrary task. Here, we give the model some human priors via text, along with some typical procedures for solving the ARC tasks, and ask it to generate the i) broad description of the input-output relation, ii) detailed steps of the input-output mapping, iii) use the detailed steps to perform manipulation on the test input and derive the test output. The current GPT3.5/GPT4 prompt solves 2 out of 4 tested small ARC challenges (those with small grids of 8x8 and below). With tweaks to the prompt to make it more specific for the use case, it can solve more. We posit that when scaled to a multi-agent system with usage of past memory and equipped with an image interpretation tool via Visual Question Answering, we may actually be able to solve the majority of the ARC challenge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03552</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#21160;&#24577;&#20559;&#31227;&#30340;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#21463;&#21040;&#21160;&#24577;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#30340;&#29615;&#22659;&#21160;&#24577;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#29615;&#22659;&#21442;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26681;&#25454;&#20854;&#29615;&#22659;&#21442;&#25968;&#23558;&#24102;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20998;&#24320;&#20197;&#35757;&#32451;&#30456;&#24212;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20986;&#29616;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#26159;&#8220;&#29305;&#23450;&#22330;&#26223;&#8221;&#20351;&#29992;&#30340;&#65292;&#38024;&#23545;&#26576;&#20010;&#29615;&#22659;&#35757;&#32451;&#30340;&#31574;&#30053;&#19981;&#33021;&#20174;&#25910;&#38598;&#22312;&#20854;&#20182;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#25152;&#26377;&#20854;&#20182;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#21644;&#19981;&#21516;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#31574;&#30053;&#20855;&#26377;&#31867;&#20284;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#65292;&#24182;&#20174;&#20855;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31283;&#24577;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#37325;&#29992;&#12290;&#36825;&#31181;&#20998;&#24067;&#29992;&#20110;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23548;&#33268;&#20102; SRPO&#65288;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;&#65289;&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SRPO &#22312;&#20855;&#26377;&#21160;&#24577;&#20559;&#31227;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#24212;&#29992;&#20110;&#24037;&#19994;4.0&#22330;&#26223;&#65292;&#24182;&#25913;&#36827;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#27010;&#24565;&#37325;&#35201;&#24615;&#35745;&#31639;&#31243;&#24207;&#65292;&#23558;&#23616;&#37096;&#29305;&#24449;&#19982;&#25972;&#20307;&#22270;&#20687;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2306.03551</link><description>&lt;p&gt;
&#24037;&#19994;4.0&#20013;&#21487;&#25193;&#23637;&#30340;&#27010;&#24565;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Scalable Concept Extraction in Industry 4.0. (arXiv:2306.03551v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#24212;&#29992;&#20110;&#24037;&#19994;4.0&#22330;&#26223;&#65292;&#24182;&#25913;&#36827;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#27010;&#24565;&#37325;&#35201;&#24615;&#35745;&#31639;&#31243;&#24207;&#65292;&#23558;&#23616;&#37096;&#29305;&#24449;&#19982;&#25972;&#20307;&#22270;&#20687;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;4.0&#27491;&#22312;&#21033;&#29992;&#25968;&#23383;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#36830;&#25509;&#21644;&#20248;&#21270;&#21046;&#36896;&#36807;&#31243;&#12290;&#36825;&#19968;&#27010;&#24565;&#30340;&#26680;&#24515;&#22312;&#20110;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#21270;&#20026;&#21487;&#38752;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#30693;&#35782;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#22788;&#29702;&#22270;&#20687;&#25968;&#25454;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#8220;&#40657;&#30418;&#23376;&#8221;&#26412;&#36136;&#20351;&#24471;&#23427;&#20204;&#30340;&#39044;&#27979;&#36807;&#31243;&#38590;&#20197;&#29702;&#35299;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20986;&#20102;&#27010;&#24565;&#30340;&#25552;&#21462;&#21644;&#23450;&#20301;&#65292;&#21363;&#35270;&#35273;&#32447;&#32034;&#22914;&#20309;&#20171;&#20837;CNN&#30340;&#39044;&#27979;&#36807;&#31243;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#24037;&#19994;4.0&#22330;&#26223;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;&#25216;&#26415;&#8220;&#20351;&#29992;&#26412;&#22320;&#32858;&#21512;&#25551;&#36848;&#31526;&#25552;&#21462;&#27010;&#24565;&#8221;&#65288;ECLAD&#65289;&#65292;&#24182;&#25913;&#36827;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#37325;&#35201;&#24615;&#35745;&#31639;&#31243;&#24207;&#65292;&#21033;&#29992;&#21152;&#26435;&#24179;&#22343;&#26469;&#23558;&#23616;&#37096;&#29305;&#24449;&#19982;&#25972;&#20307;&#22270;&#20687;&#32852;&#31995;&#36215;&#26469;&#65292;&#25913;&#21892;ECLAD&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The industry 4.0 is leveraging digital technologies and machine learning techniques to connect and optimize manufacturing processes. Central to this idea is the ability to transform raw data into human understandable knowledge for reliable data-driven decision-making. Convolutional Neural Networks (CNNs) have been instrumental in processing image data, yet, their ``black box'' nature complicates the understanding of their prediction process. In this context, recent advances in the field of eXplainable Artificial Intelligence (XAI) have proposed the extraction and localization of concepts, or which visual cues intervene on the prediction process of CNNs. This paper tackles the application of concept extraction (CE) methods to industry 4.0 scenarios. To this end, we modify a recently developed technique, ``Extracting Concepts with Local Aggregated Descriptors'' (ECLAD), improving its scalability. Specifically, we propose a novel procedure for calculating concept importance, utilizing a w
&lt;/p&gt;</description></item><item><title>SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.03538</link><description>&lt;p&gt;
SDR-GAIN&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39640;&#23454;&#26102;&#36974;&#25377;&#34892;&#20154;&#23039;&#24577;&#23436;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving. (arXiv:2306.03538v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03538
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32531;&#35299;&#22522;&#20110;&#20154;&#20307;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#34892;&#20154;&#26816;&#27979;&#31639;&#27861;&#20013;&#37096;&#20998;&#36974;&#25377;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#31163;&#21644;&#38477;&#32500;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#34917;&#20840;&#32593;&#32476;(SDR-GAIN)&#30340;&#26032;&#22411;&#34892;&#20154;&#23039;&#21183;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;OpenPose&#22312;&#22270;&#20687;&#20013;&#20272;&#35745;&#34892;&#20154;&#30340;&#23039;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#30001;&#20110;&#36974;&#25377;&#25110;&#20854;&#20182;&#22240;&#32032;&#32780;&#19981;&#23436;&#25972;&#30340;&#34892;&#20154;&#22836;&#37096;&#21644;&#36527;&#24178;&#20851;&#38190;&#28857;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#24182;&#36827;&#19968;&#27493;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;Huber&#25439;&#22833;&#12289;&#27531;&#24046;&#32467;&#26500;&#21644;L1&#27491;&#21017;&#21270;&#26469;&#29983;&#25104;&#37096;&#20998;&#36974;&#25377;&#34892;&#20154;&#19981;&#23436;&#25972;&#22836;&#37096;&#21644;&#36527;&#24178;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#32570;&#22833;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23039;&#24577;&#34917;&#20840;&#12290;&#25105;&#20204;&#22312;MS COCO&#21644;JAAD&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SDR-GAIN&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate the challenges arising from partial occlusion in human pose keypoint based pedestrian detection methods , we present a novel pedestrian pose keypoint completion method called the separation and dimensionality reduction-based generative adversarial imputation networks (SDR-GAIN) . Firstly, we utilize OpenPose to estimate pedestrian poses in images. Then, we isolate the head and torso keypoints of pedestrians with incomplete keypoints due to occlusion or other factors and perform dimensionality reduction to enhance features and further unify feature distribution. Finally, we introduce two generative models based on the generative adversarial networks (GAN) framework, which incorporate Huber loss, residual structure, and L1 regularization to generate missing parts of the incomplete head and torso pose keypoints of partially occluded pedestrians, resulting in pose completion. Our experiments on MS COCO and JAAD datasets demonstrate that SDR-GAIN outperforms basic GAIN framework
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#22522;&#20934;TTAB&#65292;&#24182;&#21457;&#29616;&#20102;&#20043;&#21069;&#26041;&#27861;&#20013;&#30340;&#19977;&#20010;&#24120;&#35265;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2306.03536</link><description>&lt;p&gt;
&#27973;&#35848;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
On Pitfalls of Test-Time Adaptation. (arXiv:2306.03536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03536
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#22522;&#20934;TTAB&#65292;&#24182;&#21457;&#29616;&#20102;&#20043;&#21069;&#26041;&#27861;&#20013;&#30340;&#19977;&#20010;&#24120;&#35265;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#19968;&#33268;&#30340;&#35774;&#32622;&#21644;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#36825;&#22952;&#30861;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TTAB&#65292;&#19968;&#20010;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#22522;&#20934;&#65292;&#21253;&#25324;&#21313;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#22810;&#31181;&#19981;&#21516;&#30340;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#21644;&#20004;&#31181;&#35780;&#20272;&#21327;&#35758;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#25581;&#31034;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#30340;&#19977;&#20010;&#24120;&#35265;&#32570;&#38519;&#12290;&#39318;&#20808;&#65292;&#30001;&#20110;&#22312;&#32447;&#25209;&#27425;&#20381;&#36182;&#24615;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#27169;&#22411;&#36873;&#25321;&#65292;&#38750;&#24120;&#22256;&#38590;&#12290;&#20854;&#27425;&#65292;TTA&#30340;&#26377;&#25928;&#24615;&#22240;&#34987;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#23646;&#24615;&#32780;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#31532;&#19977;&#65292;&#21363;&#20351;&#22312;&#26368;&#20339;&#31639;&#27861;&#26465;&#20214;&#19979;&#65292;&#29616;&#26377;&#26041;&#27861;&#20063;&#19981;&#33021;&#35299;&#20915;&#25152;&#26377;&#24120;&#35265;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#25216;&#26415;&#21450;&#20854;&#19968;&#33268;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#21327;&#35758;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation (TTA) has recently emerged as a promising approach for tackling the robustness challenge under distribution shifts. However, the lack of consistent settings and systematic studies in prior literature hinders thorough assessments of existing methods. To address this issue, we present TTAB, a test-time adaptation benchmark that encompasses ten state-of-the-art algorithms, a diverse array of distribution shifts, and two evaluation protocols. Through extensive experiments, our benchmark reveals three common pitfalls in prior efforts. First, selecting appropriate hyper-parameters, especially for model selection, is exceedingly difficult due to online batch dependency. Second, the effectiveness of TTA varies greatly depending on the quality and properties of the model being adapted. Third, even under optimal algorithmic conditions, none of the existing methods are capable of addressing all common types of distribution shifts. Our findings underscore the need for future r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#29702;&#35770;&#21644;&#35777;&#25454;&#25299;&#25169;&#27169;&#22411;&#30340;&#20449;&#24565;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20026;&#26222;&#36866;&#22320;&#35745;&#31639;&#20195;&#29702;&#20154;&#30340;&#19981;&#21516;&#26631;&#20934;&#19979;&#30340;&#20449;&#24565;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03532</link><description>&lt;p&gt;
&#20914;&#31361;&#21644;&#19981;&#30830;&#23450;&#35777;&#25454;&#30340;&#20449;&#24565;&#27169;&#22411;&#8212;&#8212;&#36830;&#25509;Dempster-Shafer&#29702;&#35770;&#21644;&#35777;&#25454;&#25299;&#25169;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Belief Model for Conflicting and Uncertain Evidence -- Connecting Dempster-Shafer Theory and the Topology of Evidence. (arXiv:2306.03532v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#29702;&#35770;&#21644;&#35777;&#25454;&#25299;&#25169;&#27169;&#22411;&#30340;&#20449;&#24565;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20026;&#26222;&#36866;&#22320;&#35745;&#31639;&#20195;&#29702;&#20154;&#30340;&#19981;&#21516;&#26631;&#20934;&#19979;&#30340;&#20449;&#24565;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#34701;&#21512;&#12289;&#20915;&#31574;&#21644;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#30340;&#32972;&#26223;&#19979;&#65292;&#38656;&#35201;&#26681;&#25454;&#35777;&#25454;&#35745;&#31639;&#21512;&#29702;&#30340;&#20449;&#24565;&#12290;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#36825;&#20123;&#35777;&#25454;&#21487;&#33021;&#19981;&#19968;&#33268;&#12289;&#19981;&#23436;&#25972;&#25110;&#19981;&#30830;&#23450;&#65292;&#20351;&#35777;&#25454;&#34701;&#21512;&#30340;&#38382;&#39064;&#38750;&#24120;&#22797;&#26434;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#33021;&#19981;&#19968;&#33268;&#12289;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#35777;&#25454;&#30340;&#27979;&#37327;&#20449;&#24565;&#31243;&#24230;&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;Dempster-Shafer&#29702;&#35770;&#21644;&#35777;&#25454;&#25299;&#25169;&#27169;&#22411;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#20449;&#24565;&#27169;&#22411;&#27604;&#21069;&#36848;&#26041;&#27861;&#26356;&#20026;&#26222;&#36866;&#65292;&#23427;&#22312;&#20004;&#20010;&#37325;&#35201;&#26041;&#38754;&#27604;&#21069;&#36848;&#26041;&#27861;&#26356;&#20248;&#65306;&#65288;1&#65289;&#24403;&#36866;&#24403;&#30340;&#32422;&#26463;&#34987;&#26045;&#21152;&#26102;&#65292;&#23427;&#21487;&#20197;&#37325;&#29616;&#23427;&#20204;&#65307;&#65288;2&#65289;&#23427;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#26681;&#25454;&#20195;&#29702;&#20154;&#30340;&#35777;&#25454;&#35201;&#27714;&#35745;&#31639;&#20449;&#24565;&#30340;&#19981;&#21516;&#26631;&#20934;&#12290;&#21518;&#32773;&#30340;&#21019;&#26032;&#20351;&#24471;&#20351;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#29992;&#25143;&#33021;&#22815;&#22312;&#20195;&#29702;&#20154;&#20855;&#26377;&#19981;&#21516;&#30340;&#27491;&#24403;&#21270;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#21516;&#26679;&#30340;&#35777;&#25454;&#35745;&#31639;&#20195;&#29702;&#20154;&#30340;&#65288;&#21487;&#33021;&#65289;&#19981;&#21516;&#30340;&#20449;&#24565;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
One problem to solve in the context of information fusion, decision-making, and other artificial intelligence challenges is to compute justified beliefs based on evidence. In real-life examples, this evidence may be inconsistent, incomplete, or uncertain, making the problem of evidence fusion highly non-trivial. In this paper, we propose a new model for measuring degrees of beliefs based on possibly inconsistent, incomplete, and uncertain evidence, by combining tools from Dempster-Shafer Theory and Topological Models of Evidence. Our belief model is more general than the aforementioned approaches in two important ways: (1) it can reproduce them when appropriate constraints are imposed, and, more notably, (2) it is flexible enough to compute beliefs according to various standards that represent agents' evidential demands. The latter novelty allows the users of our model to employ it to compute an agent's (possibly) distinct degrees of belief, based on the same evidence, in situations wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#19981;&#36275;&#30340;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38169;&#35823;&#20998;&#31867;&#35299;&#37322;&#38382;&#39064;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#12289;&#35780;&#20998;&#21644;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2306.03531</link><description>&lt;p&gt;
&#25299;&#23637;&#21487;&#35299;&#37322;&#24615;&#35270;&#37326;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#31995;&#32479;&#29992;&#20110;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38169;&#35823;&#20998;&#31867;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expanding Explainability Horizons: A Unified Concept-Based System for Local, Global, and Misclassification Explanations. (arXiv:2306.03531v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#19981;&#36275;&#30340;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38169;&#35823;&#20998;&#31867;&#35299;&#37322;&#38382;&#39064;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#12289;&#35780;&#20998;&#21644;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26234;&#33021;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#21508;&#31181;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#27010;&#24565;&#30340;&#25216;&#26415;&#20197;&#21033;&#29992;&#19968;&#32452;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#20026;&#29305;&#28857;&#65292;&#32780;&#19981;&#26159;&#20851;&#27880;&#20110;&#21333;&#20010;&#20687;&#32032;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#25552;&#20379;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#65292;&#24182;&#33021;&#35299;&#37322;&#38169;&#35823;&#20998;&#31867;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#31995;&#32479;&#65292;&#23558;&#22810;&#20010;&#36229;&#20687;&#32032;&#22270;&#20687;&#36755;&#20837;&#32593;&#32476;&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#30446;&#26631;&#23545;&#35937;&#20197;&#21450;&#30446;&#26631;&#27010;&#24565;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#12289;&#35780;&#20998;&#21644;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#38500;&#20102;&#25552;&#39640;&#24615;&#33021;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#39044;&#27979;&#65292;&#24182;&#38416;&#26126;&#38169;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability of intelligent models has been garnering increasing attention in recent years. Of the various explainability approaches, concept-based techniques are notable for utilizing a set of human-meaningful concepts instead of focusing on individual pixels. However, there is a scarcity of methods that consistently provide both local and global explanations. Moreover, most of the methods have no offer to explain misclassification cases. To address these challenges, our study follows a straightforward yet effective approach. We propose a unified concept-based system, which inputs a number of super-pixelated images into the networks, allowing them to learn better representations of the target's objects as well as the target's concepts. This method automatically learns, scores, and extracts local and global concepts. Our experiments revealed that, in addition to enhancing performance, the models could provide deeper insights into predictions and elucidate false classifications.
&lt;/p&gt;</description></item><item><title>BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.03530</link><description>&lt;p&gt;
BackpropTools: &#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03530
&lt;/p&gt;
&lt;p&gt;
BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20135;&#29983;&#20986;&#20855;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#35757;&#32451;&#26102;&#38388;&#36807;&#38271;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#23454;&#26102;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#30340;&#32570;&#20047;&#38480;&#21046;&#20102;&#23398;&#20064;&#31574;&#30053;&#22312;&#23454;&#38469;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BackpropTools&#65292;&#19968;&#31181;&#20381;&#36182;&#24615;-free&#12289;header-only&#12289;pure C++&#30340;&#28145;&#24230;&#30417;&#30563;&#21644;&#24378;&#21270;&#23398;&#20064;&#24211;&#12290;&#21033;&#29992;&#26368;&#36817;C++&#26631;&#20934;&#30340;&#27169;&#26495;&#20803;&#32534;&#31243;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20197;&#30001;&#32534;&#35793;&#22120;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#12290;&#20854;&#26032;&#39062;&#30340;&#26550;&#26500;&#20801;&#35768;BackpropTools&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#20174;HPC&#38598;&#32676;&#12289;&#24037;&#20316;&#31449;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#21040;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#25163;&#34920;&#21644;&#24494;&#25511;&#21046;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30001;&#20110;RL&#31639;&#27861;&#19982;&#27169;&#25311;&#29615;&#22659;&#30340;&#32039;&#23494;&#38598;&#25104;&#65292;BackpropTools&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#20351;&#20854;&#25104;&#20026;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#24335;SemCom&#23454;&#29616;&#30340;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#30340;&#36890;&#20449;&#36164;&#28304;&#21387;&#21147;&#65292;&#24182;&#38416;&#36848;&#20102;&#35813;&#26694;&#26550;&#20013;SemCom&#27169;&#22359;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#21487;&#34892;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03528</link><description>&lt;p&gt;
&#22312;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#20013;&#36827;&#34892;&#35821;&#20041;&#36890;&#20449;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses for Semantic Communication in Vehicular Metaverses. (arXiv:2306.03528v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#24335;SemCom&#23454;&#29616;&#30340;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#30340;&#36890;&#20449;&#36164;&#28304;&#21387;&#21147;&#65292;&#24182;&#38416;&#36848;&#20102;&#35813;&#26694;&#26550;&#20013;SemCom&#27169;&#22359;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#21487;&#34892;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#65292;&#20248;&#21270;&#20174;&#36710;&#19978;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#21644;&#26381;&#21153;&#36136;&#37327;&#26159;&#26368;&#32456;&#30446;&#26631;&#20043;&#19968;&#12290;&#35821;&#20041;&#36890;&#20449;&#65288;SemCom&#65289;&#34987;&#24341;&#20837;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#33539;&#20363;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#30340;&#36890;&#20449;&#36164;&#28304;&#21387;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#35813;&#30446;&#26631;&#12290;SemCom&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#36229;&#39640;&#25928;&#30340;&#36710;&#36733;&#36890;&#20449;&#65292;&#29978;&#33267;&#22312;&#36710;&#36742;&#20043;&#38388;&#30340;&#25968;&#25454;&#27969;&#37327;&#39134;&#36895;&#22686;&#38271;&#26102;&#20173;&#33021;&#22914;&#27492;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#24335;SemCom&#23454;&#29616;&#30340;&#36710;&#36733;&#34394;&#25311;&#29616;&#23454;&#26694;&#26550;&#65292;&#21253;&#25324;&#20840;&#23616;&#34394;&#25311;&#29616;&#23454;&#12289;&#26412;&#22320;&#34394;&#25311;&#29616;&#23454;&#12289;SemCom&#27169;&#22359;&#21644;&#36164;&#28304;&#27744;&#12290;&#20174;&#20998;&#24067;&#35282;&#24230;&#32771;&#34385;&#29992;&#25143;&#30340;&#26381;&#21153;&#36136;&#37327;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#30340;&#28508;&#22312;&#23433;&#20840;&#28431;&#27934;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#35813;&#26694;&#26550;&#30340;SemCom&#27169;&#22359;&#30340;&#29305;&#23450;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For vehicular metaverses, one of the ultimate user-centric goals is to optimize the immersive experience and Quality of Service (QoS) for users on board. Semantic Communication (SemCom) has been introduced as a revolutionary paradigm that significantly eases communication resource pressure for vehicular metaverse applications to achieve this goal. SemCom enables high-quality and ultra-efficient vehicular communication, even with explosively increasing data traffic among vehicles. In this article, we propose a hierarchical SemCom-enabled vehicular metaverses framework consisting of the global metaverse, local metaverses, SemCom module, and resource pool. The global and local metaverses are brand-new concepts from the metaverse's distribution standpoint. Considering the QoS of users, this article explores the potential security vulnerabilities of the proposed framework. To that purpose, this study highlights a specific security risk to the framework's SemCom module and offers a viable de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35299;&#20915;&#20102;&#20855;&#26377;&#20840;&#23616;&#32422;&#26463;&#30340;&#19981;&#19968;&#33268;&#25968;&#25454;&#24211;&#30340;&#20462;&#22797;&#21644;&#26597;&#35810;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#31216;&#24046;&#20998;&#20462;&#22797;&#24182;&#25351;&#23450;&#39318;&#36873;&#20462;&#22797;&#34892;&#21160;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#26368;&#20248;&#20462;&#22797;&#27010;&#24565;&#65292;&#24182;&#19988;&#30740;&#31350;&#20102;&#20462;&#22797;&#27010;&#24565;&#30340;&#35745;&#31639;&#23646;&#24615;&#65292;&#21516;&#26102;&#28548;&#28165;&#20102;&#19982;&#20027;&#21160;&#23436;&#25972;&#24615;&#32422;&#26463;&#26694;&#26550;&#20013;&#24341;&#20837;&#30340;&#20462;&#22797;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.03523</link><description>&lt;p&gt;
&#20855;&#26377;&#20840;&#23616;&#32422;&#26463;&#30340;&#20248;&#20808;&#25968;&#25454;&#24211;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#22788;&#29702;&#65306;&#22797;&#26434;&#24230;&#20998;&#26512;&#21644;&#19982;&#20027;&#21160;&#23436;&#25972;&#24615;&#32422;&#26463;&#30340;&#32852;&#31995;(arXiv:2306.03523v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
Inconsistency Handling in Prioritized Databases with Universal Constraints: Complexity Analysis and Links with Active Integrity Constraints. (arXiv:2306.03523v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35299;&#20915;&#20102;&#20855;&#26377;&#20840;&#23616;&#32422;&#26463;&#30340;&#19981;&#19968;&#33268;&#25968;&#25454;&#24211;&#30340;&#20462;&#22797;&#21644;&#26597;&#35810;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#31216;&#24046;&#20998;&#20462;&#22797;&#24182;&#25351;&#23450;&#39318;&#36873;&#20462;&#22797;&#34892;&#21160;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#26368;&#20248;&#20462;&#22797;&#27010;&#24565;&#65292;&#24182;&#19988;&#30740;&#31350;&#20102;&#20462;&#22797;&#27010;&#24565;&#30340;&#35745;&#31639;&#23646;&#24615;&#65292;&#21516;&#26102;&#28548;&#28165;&#20102;&#19982;&#20027;&#21160;&#23436;&#25972;&#24615;&#32422;&#26463;&#26694;&#26550;&#20013;&#24341;&#20837;&#30340;&#20462;&#22797;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#24102;&#26377;&#20840;&#23616;&#32422;&#26463;&#30340;&#19981;&#19968;&#33268;&#25968;&#25454;&#24211;&#30340;&#20462;&#22797;&#21644;&#26597;&#35810;&#38382;&#39064;&#12290;&#37319;&#29992;&#23545;&#31216;&#24046;&#20998;&#20462;&#22797;&#65292;&#21363;&#36890;&#36807;&#21024;&#38500;&#21644;&#28155;&#21152;&#20107;&#23454;&#26469;&#24674;&#22797;&#19968;&#33268;&#24615;&#65292;&#24182;&#20551;&#35774;&#36890;&#36807;&#23545;&#65288;&#21542;&#23450;&#65289;&#20107;&#23454;&#30340;&#20108;&#20803;&#20248;&#20808;&#20851;&#31995;&#26469;&#25351;&#23450;&#39318;&#36873;&#20462;&#22797;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23637;&#31034;&#22914;&#20309;&#36866;&#24403;&#22320;&#23558;&#29616;&#26377;&#30340;&#26368;&#20248;&#20462;&#22797;&#27010;&#24565;&#65288;&#20165;&#23545;&#22522;&#20110;&#20107;&#23454;&#21024;&#38500;&#30340;&#31616;&#21333;&#25298;&#32477;&#32422;&#26463;&#21644;&#20462;&#22797;&#23450;&#20041;&#65289;&#25193;&#23637;&#21040;&#25105;&#20204;&#26356;&#20016;&#23500;&#30340;&#35774;&#32622;&#20013;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#24471;&#21040;&#30340;&#20462;&#22797;&#27010;&#24565;&#30340;&#35745;&#31639;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20462;&#22797;&#26816;&#26597;&#21644;&#23481;&#24525;&#19981;&#19968;&#33268;&#26597;&#35810;&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28548;&#28165;&#20102;&#20248;&#20808;&#25968;&#25454;&#24211;&#30340;&#26368;&#20248;&#20462;&#22797;&#19982;&#22312;&#20027;&#21160;&#23436;&#25972;&#24615;&#32422;&#26463;&#26694;&#26550;&#20013;&#24341;&#20837;&#30340;&#20462;&#22797;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#20462;&#22797;&#23545;&#24212;&#20110; founded&#12289;grounded &#21644; just&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the problem of repairing and querying inconsistent databases equipped with universal constraints. We adopt symmetric difference repairs, in which both deletions and additions of facts can be used to restore consistency, and suppose that preferred repair actions are specified via a binary priority relation over (negated) facts. Our first contribution is to show how existing notions of optimal repairs, defined for simpler denial constraints and repairs solely based on fact deletion, can be suitably extended to our richer setting. We next study the computational properties of the resulting repair notions, in particular, the data complexity of repair checking and inconsistency-tolerant query answering. Finally, we clarify the relationship between optimal repairs of prioritized databases and repair notions introduced in the framework of active integrity constraints. In particular, we show that Pareto-optimal repairs in our setting correspond to founded, grounded and just
&lt;/p&gt;</description></item><item><title>&#35813;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36923;&#36753;&#25193;&#25955;&#65288;LoD&#65289;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25512;&#29702;&#27169;&#22411;&#21463;&#35757;&#32451;&#26679;&#26412;&#38480;&#21046;&#12289;&#34920;&#29616;&#19981;&#22815;&#24378;&#30340;&#38382;&#39064;&#12290;LoD&#36890;&#36807;&#20851;&#31995;&#25193;&#25955;&#12289;&#38543;&#26426;&#28216;&#36208;&#23376;&#36923;&#36753;&#37319;&#26679;&#21644;&#26799;&#24230;&#33258;&#36866;&#24212;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#26597;&#35810;&#30340;&#21457;&#29616;&#21644;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#30340;&#21160;&#24577;&#24179;&#34913;&#65292;&#24182;&#37197;&#22791;&#20102;&#29305;&#27530;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#36923;&#36753;&#25193;&#25955;&#12290;</title><link>http://arxiv.org/abs/2306.03515</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#36923;&#36753;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Logic Diffusion for Knowledge Graph Reasoning. (arXiv:2306.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36923;&#36753;&#25193;&#25955;&#65288;LoD&#65289;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25512;&#29702;&#27169;&#22411;&#21463;&#35757;&#32451;&#26679;&#26412;&#38480;&#21046;&#12289;&#34920;&#29616;&#19981;&#22815;&#24378;&#30340;&#38382;&#39064;&#12290;LoD&#36890;&#36807;&#20851;&#31995;&#25193;&#25955;&#12289;&#38543;&#26426;&#28216;&#36208;&#23376;&#36923;&#36753;&#37319;&#26679;&#21644;&#26799;&#24230;&#33258;&#36866;&#24212;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#26597;&#35810;&#30340;&#21457;&#29616;&#21644;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#30340;&#21160;&#24577;&#24179;&#34913;&#65292;&#24182;&#37197;&#22791;&#20102;&#29305;&#27530;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#36923;&#36753;&#25193;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22238;&#31572;&#19968;&#38454;&#36923;&#36753;&#26597;&#35810;&#65292;&#36890;&#36807;&#22810;&#36339;&#36923;&#36753;&#39044;&#27979;&#26469;&#25506;&#32034;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25512;&#29702;&#27169;&#22411;&#21463;&#21040;&#35757;&#32451;&#26679;&#26412;&#25152;&#22260;&#32469;&#30340;&#36923;&#36753;&#33539;&#24335;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#36923;&#36753;&#25512;&#29702;&#19978;&#34920;&#29616;&#36824;&#19981;&#22815;&#24378;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36923;&#36753;&#25193;&#25955;&#65288;LoD&#65289;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#21608;&#22260;&#29615;&#22659;&#20013;&#21457;&#29616;&#26410;&#35265;&#26597;&#35810;&#65292;&#24182;&#23454;&#29616;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#30340;&#21160;&#24577;&#24179;&#34913;&#12290;LoD&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20851;&#31995;&#25193;&#25955;&#21644;&#38543;&#26426;&#28216;&#36208;&#23376;&#36923;&#36753;&#37319;&#26679;&#20197;&#21450;&#19968;&#31181;&#29305;&#27530;&#30340;&#35757;&#32451;&#26426;&#21046;&#8212;&#8212;&#26799;&#24230;&#33258;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;LoD&#36824;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#36827;&#19968;&#27493;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#38598;&#20013;&#24212;&#23545;&#22024;&#26434;&#25968;&#25454;&#26102;&#23454;&#29616;&#31283;&#20581;&#30340;&#36923;&#36753;&#25193;&#25955;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#24102;&#26377;LoD&#30340;&#20027;&#27969;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#27169;&#22411;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#20102;&#36923;&#36753;&#25193;&#25955;&#22312;&#20811;&#26381;&#29616;&#26377;&#25512;&#29702;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#23454;&#29616;&#26356;&#22909;&#30340;&#26410;&#35265;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most recent works focus on answering first order logical queries to explore the knowledge graph reasoning via multi-hop logic predictions. However, existing reasoning models are limited by the circumscribed logical paradigms of training samples, which leads to a weak generalization of unseen logic. To address these issues, we propose a plug-in module called Logic Diffusion (LoD) to discover unseen queries from surroundings and achieves dynamical equilibrium between different kinds of patterns. The basic idea of LoD is relation diffusion and sampling sub-logic by random walking as well as a special training mechanism called gradient adaption. Besides, LoD is accompanied by a novel loss function to further achieve the robust logical diffusion when facing noisy data in training or testing sets. Extensive experiments on four public datasets demonstrate the superiority of mainstream knowledge graph reasoning models with LoD over state-of-the-art. Moreover, our ablation study proves the gene
&lt;/p&gt;</description></item><item><title>Mega-TTS&#26159;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#37326;&#22806;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;TTS&#31995;&#32479;&#12290;&#23427;&#23558;&#35821;&#38899;&#20998;&#35299;&#25104;&#20960;&#20010;&#23646;&#24615;&#24182;&#20998;&#21035;&#20351;&#29992;&#20855;&#26377;&#36866;&#24403;&#24402;&#32435;&#20559;&#24046;&#30340;&#27169;&#22359;&#36827;&#34892;&#24314;&#27169;&#65292;&#21253;&#25324;&#20351;&#29992;&#39057;&#35889;&#22270;&#20316;&#20026;&#20013;&#38388;&#29305;&#24449;&#12289;&#22810;&#35828;&#35805;&#20154;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#38899;&#33394;&#21644;&#38901;&#24459;&#20197;&#21450;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#25311;&#20869;&#23481;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#35821;&#38899;&#36136;&#37327;&#21644;&#35828;&#35805;&#20154;&#30456;&#20284;&#24230;&#25351;&#26631;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.03509</link><description>&lt;p&gt;
Mega-TTS&#65306;&#20855;&#26377;&#20869;&#22312;&#24402;&#32435;&#20559;&#24046;&#30340;&#35268;&#27169;&#38646;&#26679;&#26412;TTS
&lt;/p&gt;
&lt;p&gt;
Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias. (arXiv:2306.03509v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03509
&lt;/p&gt;
&lt;p&gt;
Mega-TTS&#26159;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#37326;&#22806;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;TTS&#31995;&#32479;&#12290;&#23427;&#23558;&#35821;&#38899;&#20998;&#35299;&#25104;&#20960;&#20010;&#23646;&#24615;&#24182;&#20998;&#21035;&#20351;&#29992;&#20855;&#26377;&#36866;&#24403;&#24402;&#32435;&#20559;&#24046;&#30340;&#27169;&#22359;&#36827;&#34892;&#24314;&#27169;&#65292;&#21253;&#25324;&#20351;&#29992;&#39057;&#35889;&#22270;&#20316;&#20026;&#20013;&#38388;&#29305;&#24449;&#12289;&#22810;&#35828;&#35805;&#20154;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#38899;&#33394;&#21644;&#38901;&#24459;&#20197;&#21450;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#25311;&#20869;&#23481;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#35821;&#38899;&#36136;&#37327;&#21644;&#35828;&#35805;&#20154;&#30456;&#20284;&#24230;&#25351;&#26631;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25991;&#26412;&#36716;&#35821;&#38899;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#21644;&#28151;&#26434;&#30340;&#25968;&#25454;&#38598;&#20013;&#24050;&#34987;&#35777;&#26126;&#22312;&#23454;&#29616;&#38899;&#33394;&#21644;&#35821;&#38899;&#39118;&#26684;&#36890;&#29992;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;TTS&#20013;&#26356;&#26159;&#22914;&#27492;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#38899;&#39057;&#32534;&#35299;&#30721;&#23558;&#35821;&#38899;&#32534;&#30721;&#25104;&#28508;&#21464;&#37327;&#65292;&#24182;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#25110;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#23427;&#65292;&#36825;&#24573;&#30053;&#20102;&#35821;&#38899;&#30340;&#20869;&#22312;&#24615;&#36136;&#65292;&#21487;&#33021;&#23548;&#33268;&#32467;&#26524;&#21155;&#36136;&#25110;&#19981;&#21487;&#25511;&#21046;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#35748;&#20026;&#35821;&#38899;&#21487;&#20197;&#20998;&#35299;&#25104;&#20960;&#20010;&#23646;&#24615;&#65288;&#20363;&#22914;&#20869;&#23481;&#65292;&#38899;&#33394;&#65292;&#38901;&#24459;&#21644;&#30456;&#20301;&#65289;&#65292;&#27599;&#20010;&#23646;&#24615;&#37117;&#24212;&#35813;&#20351;&#29992;&#20855;&#26377;&#36866;&#24403;&#24402;&#32435;&#20559;&#24046;&#30340;&#27169;&#22359;&#36827;&#34892;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#35268;&#27169;&#38646;&#26679;&#26412;TTS&#31995;&#32479;Mega-TTS&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#37326;&#22806;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#23545;&#19981;&#21516;&#30340;&#23646;&#24615;&#36827;&#34892;&#24314;&#27169;&#65306;1&#65289;&#25105;&#20204;&#20173;&#28982;&#36873;&#25321;&#20351;&#29992;&#39057;&#35889;&#22270;&#20316;&#20026;&#20013;&#38388;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#38899;&#39057;&#32534;&#35299;&#30721;&#32534;&#30721;&#30340;&#28508;&#21464;&#37327;&#12290;&#39057;&#35889;&#22270;&#38750;&#24120;&#22909;&#22320;&#20998;&#31163;&#20102;&#30456;&#20301;&#21644;&#20854;&#20182;&#23646;&#24615;&#12290;&#30456;&#20301;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#34987;&#36866;&#24403;&#22320;&#24573;&#30053;&#65292;&#24182;&#22312;&#25512;&#29702;&#20013;&#28155;&#21152;&#22238;&#26469;&#20197;&#33719;&#24471;&#33258;&#28982;&#30340;&#35821;&#38899;&#12290;2&#65289;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#22810;&#35828;&#35805;&#20154;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#38899;&#33394;&#21644;&#38901;&#24459;&#65292;&#24182;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#20110;&#29305;&#23450;&#35828;&#35805;&#20154;&#12290;3&#65289;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#25311;&#20869;&#23481;&#65292;&#23427;&#21487;&#20197;&#21463;&#30410;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#22788;&#29702;&#25991;&#26412;&#30340;&#38271;&#31243;&#20381;&#36182;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;Mega-TTS&#22312;&#35821;&#38899;&#36136;&#37327;&#21644;&#35828;&#35805;&#20154;&#30456;&#20284;&#24230;&#25351;&#26631;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling text-to-speech to a large and wild dataset has been proven to be highly effective in achieving timbre and speech style generalization, particularly in zero-shot TTS. However, previous works usually encode speech into latent using audio codec and use autoregressive language models or diffusion models to generate it, which ignores the intrinsic nature of speech and may lead to inferior or uncontrollable results. We argue that speech can be decomposed into several attributes (e.g., content, timbre, prosody, and phase) and each of them should be modeled using a module with appropriate inductive biases. From this perspective, we carefully design a novel and large zero-shot TTS system called Mega-TTS, which is trained with large-scale wild data and models different attributes in different ways: 1) Instead of using latent encoded by audio codec as the intermediate feature, we still choose spectrogram as it separates the phase and other attributes very well. Phase can be appropriately 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;(SGNCL)&#65292;&#36890;&#36807;&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#65292;&#24182;&#25506;&#31350;&#20102;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.03506</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Subgraph Networks Based Contrastive Learning. (arXiv:2306.03506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;(SGNCL)&#65292;&#36890;&#36807;&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#65292;&#24182;&#25506;&#31350;&#20102;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;(GCL)&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#35299;&#20915;&#27880;&#37322;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290; &#23427;&#22312;&#26410;&#27880;&#37322;&#30340;&#22270;&#24418;&#20013;&#25366;&#25496;&#26174;&#24335;&#29305;&#24449;&#20197;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#21033;&#22270;&#24418;&#34920;&#31034;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#20391;&#37325;&#20110;&#22270;&#24418;&#22686;&#24378;&#31574;&#30053;&#21644;&#30456;&#20114;&#20449;&#24687;&#20272;&#35745;&#25805;&#20316;&#30340;&#35774;&#35745;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#23376;&#22270;&#20013;&#23384;&#22312;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#25506;&#32034;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;subgraph network-based contrastive learning (SGNCL)&#30340;&#26032;&#26694;&#26550;&#12290;SGNCL&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#12290;&#35813;&#31574;&#30053;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#20026;&#20855;&#26377;&#25299;&#25169;&#21644;&#23646;&#24615;&#29305;&#24449;&#30340;&#36793;&#21040;&#33410;&#28857;&#26144;&#23556;&#32593;&#32476;&#12290;&#21333;&#27425;&#22686;&#24378;&#35270;&#22270;&#26159;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL), as a self-supervised learning method, can solve the problem of annotated data scarcity. It mines explicit features in unannotated graphs to generate favorable graph representations for downstream tasks. Most existing GCL methods focus on the design of graph augmentation strategies and mutual information estimation operations. Graph augmentation produces augmented views by graph perturbations. These views preserve a locally similar structure and exploit explicit features. However, these methods have not considered the interaction existing in subgraphs. To explore the impact of substructure interactions on graph representations, we propose a novel framework called subgraph network-based contrastive learning (SGNCL). SGNCL applies a subgraph network generation strategy to produce augmented views. This strategy converts the original graph into an Edge-to-Node mapping network with both topological and attribute features. The single-shot augmented view is a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#65292;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.03503</link><description>&lt;p&gt;
&#24212;&#29992;&#26631;&#20934;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#28216;&#20262;&#29702;
&lt;/p&gt;
&lt;p&gt;
Applying Standards to Advance Upstream &amp; Downstream Ethics in Large Language Models. (arXiv:2306.03503v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#65292;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;AI&#25152;&#26377;&#32773;&#22914;&#20309;&#20511;&#37492;&#20854;&#20182;&#20869;&#23481;&#21019;&#20316;&#34892;&#19994;&#30340;&#34892;&#20026;&#20934;&#21017;&#21644;&#20262;&#29702;&#26631;&#20934;&#65292;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#12290;&#23427;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20262;&#29702;&#24847;&#35782;&#29616;&#29366;&#12290;&#36890;&#36807;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65288;&#19978;&#19979;&#28216;&#21644;&#29992;&#25143;&#25552;&#31034;/&#22238;&#31572;&#65289;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20445;&#38556;&#25514;&#26045;&#12290;&#38543;&#21518;&#65292;&#23545;&#36825;&#22235;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#21253;&#25324;&#22312;&#25104;&#26412;&#12289;&#26377;&#25928;&#24615;&#21644;&#19982;&#34892;&#19994;&#24815;&#20363;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#35780;&#20272;&#29616;&#26377;&#30340;&#20262;&#29702;&#20445;&#38556;&#25514;&#26045;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#35266;&#28857;&#26159;&#65292;&#29616;&#26377;&#30340;&#19982;IT&#30456;&#20851;&#30340;&#20262;&#29702;&#20934;&#21017;&#34429;&#28982;&#36866;&#29992;&#20110;&#20256;&#32479;&#30340;IT&#24037;&#31243;&#39046;&#22495;&#65292;&#20294;&#19981;&#36275;&#20197;&#24212;&#23545;&#22522;&#20110;LLMs&#20869;&#23481;&#29983;&#25104;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20511;&#37492;&#26032;&#38395;&#19994;&#20869;&#24050;&#26377;&#30340;&#23454;&#36341;&#65292;&#20026;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores how AI-owners can develop safeguards for AI-generated content by drawing from established codes of conduct and ethical standards in other content-creation industries. It delves into the current state of ethical awareness on Large Language Models (LLMs). By dissecting the mechanism of content generation by LLMs, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified. A comparative analysis of these four areas follows and includes an evaluation of the existing ethical safeguards in terms of cost, effectiveness, and alignment with established industry practices. The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation. Drawing from established practices within journalism, we propose potential standards for businesses involved in distributing and selling LLM-generated cont
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#23553;&#31105;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#22403;&#22334;&#37038;&#20214;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#35753;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.03502</link><description>&lt;p&gt;
&#20420;&#20044;&#25112;&#20105;&#65306;&#39044;&#27979;&#21644;&#35299;&#37322;Twitter&#30340;&#23553;&#31105;
&lt;/p&gt;
&lt;p&gt;
Russo-Ukrainian War: Prediction and explanation of Twitter suspension. (arXiv:2306.03502v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#23553;&#31105;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#22403;&#22334;&#37038;&#20214;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#35753;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2022&#24180;2&#26376;24&#26085;&#65292;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#65292;&#24320;&#22987;&#20102;&#29616;&#22312;&#24050;&#30693;&#30340;&#20420;&#20044;&#25112;&#20105;&#65292;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24341;&#21457;&#20102;&#22312;&#32447;&#35805;&#35821;&#12290;Twitter&#20316;&#20026;&#26368;&#21463;&#27426;&#36814;&#30340;&#31038;&#20132;&#32593;&#32476;&#20043;&#19968;&#65292;&#20197;&#20854;&#24320;&#25918;&#21644;&#27665;&#20027;&#30340;&#29305;&#28857;&#65292;&#22312;&#20854;&#24222;&#22823;&#30340;&#29992;&#25143;&#32676;&#20013;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#35752;&#35770;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;Twitter&#30340;&#25919;&#31574;&#36829;&#35268;&#12289;&#23459;&#20256;&#12289;&#28389;&#29992;&#34892;&#20026;&#12289;&#20405;&#29359;&#20844;&#27665;&#26435;&#21033;&#65292;&#22240;&#27492;&#23548;&#33268;&#29992;&#25143;&#36134;&#25143;&#34987;&#23553;&#31105;&#21644;&#21024;&#38500;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#25506;&#35752;&#20102;Twitter&#30340;&#23553;&#31105;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#21487;&#33021;&#23548;&#33268;&#36134;&#25143;&#34987;&#23553;&#31105;&#30340;&#20849;&#20139;&#20869;&#23481;&#21644;&#29992;&#25143;&#36134;&#25143;&#30340;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;Twitter API&#33719;&#24471;&#20102;&#21253;&#21547;107.7M&#26465;&#25512;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;980&#19975;&#29992;&#25143;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#34987;&#23553;&#31105;&#36134;&#25143;&#30340;&#20849;&#20139;&#20869;&#23481;&#31867;&#21035;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#21644;&#20313;&#24358;&#30456;&#20284;&#24615;&#32858;&#31867;&#26469;&#35299;&#37322;&#20854;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20123;&#28389;&#29992;Twitter&#25919;&#31574;&#26631;&#20934;&#30340;&#39575;&#23376;&#27963;&#21160;&#12289;&#22403;&#22334;&#37038;&#20214;&#21644;&#23459;&#20256;&#27963;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#31881;&#19997;&#25968;&#36739;&#23569;&#30340;&#36134;&#25143;&#65292;&#25317;&#26377;&#26356;&#22810;&#31881;&#19997;&#30340;&#36134;&#25143;&#26356;&#26377;&#21487;&#33021;&#34987;&#23553;&#31105;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#20026;Twitter&#21644;&#20854;&#20182;&#31038;&#20132;&#32593;&#32476;&#25913;&#36827;&#20854;&#20869;&#23481;&#36807;&#28388;&#26426;&#21046;&#65292;&#26368;&#23567;&#21270;&#26377;&#23475;&#20869;&#23481;&#30340;&#20256;&#25773;&#25552;&#20379;&#26377;&#29992;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
On 24 February 2022, Russia invaded Ukraine, starting what is now known as the Russo-Ukrainian War, initiating an online discourse on social media. Twitter as one of the most popular SNs, with an open and democratic character, enables a transparent discussion among its large user base. Unfortunately, this often leads to Twitter's policy violations, propaganda, abusive actions, civil integrity violation, and consequently to user accounts' suspension and deletion. This study focuses on the Twitter suspension mechanism and the analysis of shared content and features of the user accounts that may lead to this. Toward this goal, we have obtained a dataset containing 107.7M tweets, originating from 9.8 million users, using Twitter API. We extract the categories of shared content of the suspended accounts and explain their characteristics, through the extraction of text embeddings in junction with cosine similarity clustering. Our results reveal scam campaigns taking advantage of trending top
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#20855;&#26377;&#21452;&#37325;&#25928;&#24212;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#21644;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2306.03481</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#32416;&#32544;&#25968;&#25454;&#30340;&#36716;&#25442;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transition role of entangled data in quantum machine learning. (arXiv:2306.03481v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#20855;&#26377;&#21452;&#37325;&#25928;&#24212;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#21644;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32416;&#32544;&#20316;&#20026;&#22686;&#24378;&#37327;&#23376;&#35745;&#31639;&#30340;&#36164;&#28304;&#65292;&#24050;&#32463;&#22312;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23558;&#32416;&#32544;&#34701;&#20837;&#21040;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25805;&#20316;&#25110;&#27979;&#37327;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#21516;&#26102;&#22312;&#36798;&#21040;&#25351;&#23450;&#39044;&#27979;&#35823;&#24046;&#38408;&#20540;&#26102;&#21462;&#24471;&#20102;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#32416;&#32544;&#31243;&#24230;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#20998;&#26512;&#24615;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#20351;&#29992;&#32416;&#32544;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#37327;&#23376;&#19981;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#12290;&#19982;&#20197;&#24448;&#21457;&#29616;&#30340;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#39044;&#27979;&#35823;&#24046;&#30340;&#24433;&#21709;&#21576;&#29616;&#20986;&#21452;&#37325;&#25928;&#24212;&#65292;&#21462;&#20915;&#20110;&#20801;&#35768;&#30340;&#27979;&#37327;&#27425;&#25968;&#12290;&#22312;&#26377;&#20805;&#20998;&#30340;&#27979;&#37327;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#32416;&#32544;&#24230;&#21487;&#20197;&#25345;&#32493;&#38477;&#20302;&#39044;&#27979;&#35823;&#24046;&#65292;&#25110;&#20943;&#23569;&#36798;&#21040;&#32473;&#23450;&#35823;&#24046;&#38408;&#20540;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#12290;&#26412;&#30740;&#31350;&#38416;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#36716;&#25442;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entanglement serves as the resource to empower quantum computing. Recent progress has highlighted its positive impact on learning quantum dynamics, wherein the integration of entanglement into quantum operations or measurements of quantum machine learning (QML) models leads to substantial reductions in training data size, surpassing a specified prediction error threshold. However, an analytical understanding of how the entanglement degree in data affects model performance remains elusive. In this study, we address this knowledge gap by establishing a quantum no-free-lunch (NFL) theorem for learning quantum dynamics using entangled data. Contrary to previous findings, we prove that the impact of entangled data on prediction error exhibits a dual effect, depending on the number of permitted measurements. With a sufficient number of measurements, increasing the entanglement of training data consistently reduces the prediction error or decreases the required size of the training data to ac
&lt;/p&gt;</description></item><item><title>GSHOT&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.03480</link><description>&lt;p&gt;
GSHOT: &#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GSHOT: Few-shot Generative Modeling of Labeled Graphs. (arXiv:2306.03480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03480
&lt;/p&gt;
&lt;p&gt;
GSHOT&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#22270;&#29983;&#25104;&#24314;&#27169;&#22240;&#20854;&#30452;&#25509;&#23398;&#20064;&#28508;&#22312;&#38544;&#34255;&#22270;&#20998;&#24067;&#30340;&#24778;&#20154;&#33021;&#21147;&#32780;&#21463;&#21040;&#26497;&#22823;&#20851;&#27880;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#26368;&#21021;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20687;&#35768;&#22810;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#19968;&#26679;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#23398;&#20064;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#32597;&#35265;&#30142;&#30149;&#30340;&#33647;&#29289;&#21457;&#29616;&#31561;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#19981;&#24635;&#26159;&#26377;&#36275;&#22815;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#29992;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#36827;&#23637;&#20026;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24212;&#29992;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23569;&#26679;&#26412;&#22270;&#29983;&#25104;&#24314;&#27169;&#36825;&#19968;&#36804;&#20170;&#26410;&#26366;&#25506;&#32034;&#30340;&#33539;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GSHOT&#65292;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#12290;GSHOT&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#12290;&#21033;&#29992;&#36825;&#20123;&#20808;&#21069;&#30340;&#32463;&#39564;&#65292;GSHOT&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#33258;&#25105;&#35843;&#25972;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph generative modeling has gained enormous attraction in recent years due to its impressive ability to directly learn the underlying hidden graph distribution. Despite their initial success, these techniques, like much of the existing deep generative methods, require a large number of training samples to learn a good model. Unfortunately, large number of training samples may not always be available in scenarios such as drug discovery for rare diseases. At the same time, recent advances in few-shot learning have opened door to applications where available training data is limited. In this work, we introduce the hitherto unexplored paradigm of few-shot graph generative modeling. Towards this, we develop GSHOT, a meta-learning based framework for few-shot labeled graph generative modeling. GSHOT learns to transfer meta-knowledge from similar auxiliary graph datasets. Utilizing these prior experiences, GSHOT quickly adapts to an unseen graph dataset through self-paced fine-tuning. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;TwistList&#25968;&#25454;&#38598;&#21644;TwisterMisters&#22522;&#20934;&#31995;&#32479;&#65292;&#24182;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03457</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#65306;&#20197;&#32469;&#21475;&#20196;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Phonetically-Grounded Language Generation: The Case of Tongue Twisters. (arXiv:2306.03457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;TwistList&#25968;&#25454;&#38598;&#21644;TwisterMisters&#22522;&#20934;&#31995;&#32479;&#65292;&#24182;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#38899;&#38901;&#23398;&#35821;&#35328;&#29983;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#35789;&#27468;&#21644;&#35799;&#27468;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22260;&#32469;&#32469;&#21475;&#20196;&#29983;&#25104;&#23637;&#24320;&#30340;&#24037;&#20316;&#65292;&#32469;&#21475;&#20196;&#38656;&#35201;&#22312;&#20445;&#25345;&#35821;&#20041;&#27491;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#21270;&#38899;&#39057;&#37325;&#21472;&#24182;&#20445;&#25345;&#35821;&#27861;&#27491;&#30830;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;TwistList&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;2.1K&#20154;&#24037;&#32534;&#20889;&#30340;&#32469;&#21475;&#20196;&#30340;&#22823;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20934;&#31995;&#32479;(TwisterMisters)&#65292;&#21253;&#25324;&#38656;&#35201;&#21644;&#19981;&#38656;&#35201;&#22312;&#22495;&#20869;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#30340;&#32467;&#26524;&#26469;&#35777;&#26126;&#29616;&#26377;&#20027;&#27969;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#24615;&#33021;&#20248;&#33391;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#20219;&#21153;&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work in phonetically-grounded language generation has mainly focused on domains such as lyrics and poetry. In this paper, we present work on the generation of tongue twisters - a form of language that is required to be phonetically conditioned to maximise sound overlap, whilst maintaining semantic consistency with an input topic, and still being grammatically correct. We present \textbf{TwistList}, a large annotated dataset of tongue twisters, consisting of 2.1K+ human-authored examples. We additionally present several benchmark systems (referred to as TwisterMisters) for the proposed task of tongue twister generation, including models that both do and do not require training on in-domain data. We present the results of automatic and human evaluation to demonstrate the performance of existing mainstream pre-trained models in this task with limited (or no) task specific training and data, and no explicit phonetic knowledge. We find that the task of tongue twister generation is 
&lt;/p&gt;</description></item><item><title>GRAFENNE&#26159;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#21407;&#22270;&#19978;&#36827;&#34892;&#24322;&#26500;&#36716;&#21270;&#65292;&#23558;&#33410;&#28857;&#21644;&#29305;&#24449;&#35299;&#32806;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26222;&#36941;&#23384;&#22312;&#30340;&#29305;&#24449;&#38745;&#24577;&#12289;&#36716;&#31227;&#35823;&#24046;&#31561;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#36866;&#29992;&#20110;&#26410;&#30693;&#33410;&#28857;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.03447</link><description>&lt;p&gt;
GRAFENNE&#65306;&#22312;&#20855;&#26377;&#24322;&#36136;&#21644;&#21160;&#24577;&#29305;&#24449;&#38598;&#30340;&#22270;&#19978;&#36827;&#34892;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GRAFENNE: Learning on Graphs with Heterogeneous and Dynamic Feature Sets. (arXiv:2306.03447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03447
&lt;/p&gt;
&lt;p&gt;
GRAFENNE&#26159;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#21407;&#22270;&#19978;&#36827;&#34892;&#24322;&#26500;&#36716;&#21270;&#65292;&#23558;&#33410;&#28857;&#21644;&#29305;&#24449;&#35299;&#32806;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26222;&#36941;&#23384;&#22312;&#30340;&#29305;&#24449;&#38745;&#24577;&#12289;&#36716;&#31227;&#35823;&#24046;&#31561;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#36866;&#29992;&#20110;&#26410;&#30693;&#33410;&#28857;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36890;&#24120;&#22522;&#20110;&#23545;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#38745;&#24577;&#29305;&#24449;&#38598;&#30340;&#20551;&#35774;&#26469;&#26500;&#24314;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#36825;&#19968;&#20551;&#35774;&#32463;&#24120;&#34987;&#36829;&#21453;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#25554;&#34917;&#37096;&#20998;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#29305;&#24449;&#38598;&#22343;&#19968;&#12289;&#36716;&#31227;&#35823;&#24046;&#12289;&#26080;&#27861;&#36866;&#24212;&#21160;&#24577;&#29305;&#24449;&#31561;&#23616;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#26694;&#26550;GRAFENNE&#26469;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#21407;&#22270;&#19978;&#36827;&#34892;&#24322;&#26500;&#36716;&#21270;&#65292;&#23558;&#33410;&#28857;&#21644;&#29305;&#24449;&#35299;&#32806;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#27861;&#20351;&#24471;&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#19982;&#29305;&#24449;&#25968;&#37327;&#26080;&#20851;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#33410;&#28857;&#21644;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GRAFENNE&#22312;Weisfeil&#26041;&#31243;&#24615;&#33021;&#19978;&#33267;&#23569;&#19982;&#29616;&#26377;&#30340;&#20449;&#24687;&#20256;&#36882;GNN&#19968;&#26679;&#20855;&#26377;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), in general, are built on the assumption of a static set of features characterizing each node in a graph. This assumption is often violated in practice. Existing methods partly address this issue through feature imputation. However, these techniques (i) assume uniformity of feature set across nodes, (ii) are transductive by nature, and (iii) fail to work when features are added or removed over time. In this work, we address these limitations through a novel GNN framework called GRAFENNE. GRAFENNE performs a novel allotropic transformation on the original graph, wherein the nodes and features are decoupled through a bipartite encoding. Through a carefully chosen message passing framework on the allotropic transformation, we make the model parameter size independent of the number of features and thereby inductive to both unseen nodes and features. We prove that GRAFENNE is at least as expressive as any of the existing message-passing GNNs in terms of Weisfeil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;MetaGait&#25216;&#26415;&#26469;&#23398;&#20064;&#20840;&#26679;&#26412;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#36890;&#36807;&#27880;&#20837;&#20803;&#30693;&#35782;&#21040;&#26657;&#20934;&#32593;&#32476;&#20013;&#65292;&#20174;&#20840;&#23610;&#24230;&#12289;&#20840;&#32500;&#24230;&#21644;&#20840;&#36807;&#31243;&#30340;&#35282;&#24230;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03445</link><description>&lt;p&gt;
MetaGait&#65306;&#23398;&#20064;&#23398;&#20064;&#27493;&#24577;&#35782;&#21035;&#30340;&#20840;&#26679;&#26412;&#33258;&#36866;&#24212;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MetaGait: Learning to Learn an Omni Sample Adaptive Representation for Gait Recognition. (arXiv:2306.03445v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;MetaGait&#25216;&#26415;&#26469;&#23398;&#20064;&#20840;&#26679;&#26412;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#36890;&#36807;&#27880;&#20837;&#20803;&#30693;&#35782;&#21040;&#26657;&#20934;&#32593;&#32476;&#20013;&#65292;&#20174;&#20840;&#23610;&#24230;&#12289;&#20840;&#32500;&#24230;&#21644;&#20840;&#36807;&#31243;&#30340;&#35282;&#24230;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27493;&#24577;&#35782;&#21035;&#26088;&#22312;&#36890;&#36807;&#20010;&#20307;&#30340;&#34892;&#36208;&#27169;&#24335;&#35782;&#21035;&#20854;&#36523;&#20221;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#27493;&#24577;&#35782;&#21035;&#20173;&#38754;&#20020;&#30528;&#36718;&#24275;&#30340;&#26377;&#38480;&#20108;&#36827;&#21046;&#35270;&#35273;&#32447;&#32034;&#21644;&#20247;&#22810;&#23610;&#24230;&#19981;&#21516;&#30340;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#36825;&#32473;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;MetaGait&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#20914;&#31361;&#65292;&#35813;&#25216;&#26415;&#23398;&#20064;&#23398;&#20064;&#20840;&#26679;&#26412;&#33258;&#36866;&#24212;&#34920;&#31034;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;MetaGait&#23558;&#20803;&#30693;&#35782;&#27880;&#20837;&#21040;&#27880;&#24847;&#26426;&#21046;&#30340;&#26657;&#20934;&#32593;&#32476;&#20013;&#65292;&#20174;&#20840;&#26679;&#26412;&#33258;&#36866;&#24212;&#24615;&#30340;&#20840;&#23610;&#24230;&#12289;&#20840;&#32500;&#24230;&#21644;&#20840;&#36807;&#31243;&#30340;&#35282;&#24230;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36328;&#36234;&#25972;&#20010;&#36807;&#31243;&#21033;&#29992;&#20102;&#20803;&#30693;&#35782;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;Meta Triple Attention&#21644;Meta Temporal Pooling&#26469;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#31354;&#38388;/&#36890;&#36947;/&#26102;&#38388;&#32500;&#24230;&#30340;&#20840;&#23610;&#24230;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gait recognition, which aims at identifying individuals by their walking patterns, has recently drawn increasing research attention. However, gait recognition still suffers from the conflicts between the limited binary visual clues of the silhouette and numerous covariates with diverse scales, which brings challenges to the model's adaptiveness. In this paper, we address this conflict by developing a novel MetaGait that learns to learn an omni sample adaptive representation. Towards this goal, MetaGait injects meta-knowledge, which could guide the model to perceive sample-specific properties, into the calibration network of the attention mechanism to improve the adaptiveness from the omni-scale, omni-dimension, and omni-process perspectives. Specifically, we leverage the meta-knowledge across the entire process, where Meta Triple Attention and Meta Temporal Pooling are presented respectively to adaptively capture omni-scale dependency from spatial/channel/temporal dimensions simultaneo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03438</link><description>&lt;p&gt;
&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22635;&#20889;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#26102;&#23384;&#22312;&#22833;&#36133;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models of Code Fail at Completing Code with Potential Bugs. (arXiv:2306.03438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#22312;&#20195;&#30721;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#36825;&#26159;&#32534;&#31243;&#36741;&#21161;&#21644;&#20195;&#30721;&#26234;&#33021;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#38382;&#39064;&#65292;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#36825;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#21463;&#23454;&#26102;&#20195;&#30721;&#24314;&#35758;&#30340;&#29616;&#23454;&#22330;&#26223;&#21551;&#21457;&#65292;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#21487;&#33021;&#30340;&#28431;&#27934;-&#21453;&#27169;&#24335;&#65292;&#36825;&#20123;&#21453;&#27169;&#24335;&#21487;&#20197;&#25104;&#20026;&#23436;&#25104;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#30740;&#31350;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20174;&#35821;&#20041;&#25913;&#21464;&#25805;&#20316;&#20013;&#27966;&#29983;&#30340;&#21512;&#25104;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-HumanEval&#65289;&#65292;&#21478;&#19968;&#20010;&#26159;&#20174;&#29992;&#25143;&#25552;&#20132;&#30340;&#32534;&#31243;&#38382;&#39064;&#20013;&#27966;&#29983;&#30340;&#29616;&#23454;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-FixEval&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#24773;&#20917;&#26174;&#33879;&#38477;&#20302;&#20102;&#39640;&#24615;&#33021;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;CodeGen-2B-mono&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#36890;&#36807;&#29575;
&lt;/p&gt;
&lt;p&gt;
Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CodeGen-2B-mono on test 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#25298;&#32477;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#34892;&#20026;&#19981;&#26159;&#23436;&#20840;&#20108;&#20803;&#30340;&#12290;&#20316;&#32773;&#23545;ChatGPT&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#26469;&#33258;&#20110;&#20010;&#21035;&#24037;&#31243;&#24072;&#21644;&#20844;&#21496;&#25919;&#31574;&#65292;&#24182;&#24433;&#21709;&#27169;&#22411;&#36873;&#25321;&#25298;&#32477;&#21738;&#20123;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.03423</link><description>&lt;p&gt;
&#25105;&#23475;&#24597;&#25105;&#20570;&#19981;&#21040;&#65306;&#39044;&#27979;&#40657;&#21283;&#23376;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#25298;&#32477;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models. (arXiv:2306.03423v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#25298;&#32477;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#34892;&#20026;&#19981;&#26159;&#23436;&#20840;&#20108;&#20803;&#30340;&#12290;&#20316;&#32773;&#23545;ChatGPT&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#26469;&#33258;&#20110;&#20010;&#21035;&#24037;&#31243;&#24072;&#21644;&#20844;&#21496;&#25919;&#31574;&#65292;&#24182;&#24433;&#21709;&#27169;&#22411;&#36873;&#25321;&#25298;&#32477;&#21738;&#20123;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;OpenAI&#30340;ChatGPT&#21457;&#24067;&#20197;&#26469;&#65292;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22686;&#21152;&#30340;&#20351;&#29992;&#37327;&#20984;&#26174;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#23454;&#29992;&#24615;&#65292;&#20294;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#19968;&#20123;&#23884;&#20837;&#24335;&#20559;&#35265;&#12290;&#20854;&#20013;&#19968;&#20123;&#26159;&#30001;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#24341;&#36215;&#30340;&#65307;&#20294;&#26159;&#65292;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#39069;&#22806;&#20559;&#35265;&#26469;&#33258;&#20110;&#20027;&#35266;&#24494;&#35843;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#24494;&#35843;&#20559;&#35265;&#21487;&#33021;&#26469;&#33258;&#20010;&#21035;&#24037;&#31243;&#24072;&#21644;&#20844;&#21496;&#25919;&#31574;&#65292;&#24182;&#24433;&#21709;&#27169;&#22411;&#36873;&#25321;&#25298;&#32477;&#21738;&#20123;&#25552;&#31034;&#12290;&#26412;&#23454;&#39564;&#20351;&#29992;&#40657;&#30418;&#25915;&#20987;&#65292;&#23545;ChatGPT&#30340;&#25298;&#32477;&#34892;&#20026;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21508;&#31181;&#25915;&#20987;&#24615;&#21644;&#33391;&#24615;&#25552;&#31034;&#65288;n = 1,730&#65289;&#26597;&#35810;ChatGPT&#65292;&#28982;&#21518;&#25163;&#21160;&#26631;&#35760;&#27599;&#20010;&#21709;&#24212;&#26159;&#21542;&#23653;&#34892;&#25110;&#25298;&#32477;&#12290;&#21709;&#24212;&#30340;&#25163;&#21160;&#26816;&#26597;&#34920;&#26126;&#65292;&#25298;&#32477;&#19981;&#26159;&#23436;&#20840;&#20108;&#20803;&#30340;&#65292;&#24182;&#19988;&#22312;&#36830;&#32493;&#30340;&#33539;&#22260;&#20869;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#20960;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21709;&#24212;&#26144;&#23556;&#21040;&#23653;&#34892;&#25110;&#25298;&#32477;&#30340;&#20108;&#20803;&#20540;&#20013;&#12290;&#20351;&#29992;&#20102;&#23567;&#22411;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the release of OpenAI's ChatGPT, generative language models have attracted extensive public attention. The increased usage has highlighted generative models' broad utility, but also revealed several forms of embedded bias. Some is induced by the pre-training corpus; but additional bias specific to generative models arises from the use of subjective fine-tuning to avoid generating harmful content. Fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse. In this experiment, we characterize ChatGPT's refusal behavior using a black-box attack. We first query ChatGPT with a variety of offensive and benign prompts (n=1,730), then manually label each response as compliance or refusal. Manual examination of responses reveals that refusal is not cleanly binary, and lies on a continuum; as such, we map several different kinds of responses to a binary of compliance or refusal. The small manually-labeled dataset is used 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DreamSparse &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#20960;&#20309;&#27169;&#22359;&#21644;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#26469;&#35299;&#20915; 2D &#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#20174;&#23569;&#35270;&#35282;&#24773;&#20917;&#19979;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.03414</link><description>&lt;p&gt;
DreamSparse: &#21033;&#29992; 2D &#25193;&#25955;&#27169;&#22411;&#20174;&#31232;&#30095;&#35270;&#35282;&#20013;&#21512;&#25104;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
DreamSparse: Escaping from Plato's Cave with 2D Diffusion Model Given Sparse Views. (arXiv:2306.03414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DreamSparse &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#20960;&#20309;&#27169;&#22359;&#21644;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#26469;&#35299;&#20915; 2D &#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#20174;&#23569;&#35270;&#35282;&#24773;&#20917;&#19979;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23569;&#37327;&#35270;&#35282;&#20013;&#21512;&#25104;&#26032;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23454;&#38469;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#25110;&#22312;&#27492;&#31867;&#23569;&#35270;&#35282;&#35774;&#32622;&#20013;&#38656;&#35201;&#36880;&#20010;&#23545;&#35937;&#20248;&#21270;&#65292;&#22240;&#20026;&#25552;&#20379;&#30340;&#20449;&#24687;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#26469;&#21512;&#25104;&#26032;&#39062;&#30340;&#35270;&#35282;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;2D &#25193;&#25955;&#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#65292;&#23548;&#33268;&#22270;&#20687;&#21512;&#25104;&#22833;&#30495;&#65292;&#24433;&#21709;&#20102;&#22270;&#20687;&#30340;&#35782;&#21035;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DreamSparse&#65292;&#19968;&#20010;&#21487;&#20197;&#29983;&#25104;&#20960;&#20309;&#21644;&#35782;&#21035;&#32852;&#21512;&#19968;&#33268;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DreamSparse &#21253;&#25324;&#19968;&#20010;&#20960;&#20309;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#31232;&#30095;&#35270;&#35282;&#33719;&#21462; 3D &#29305;&#24449;&#20316;&#20026; 3D &#20808;&#39564;&#65292;&#38543;&#21518;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#23558;&#36825;&#20123; 3D &#29305;&#24449;&#22270;&#36716;&#25442;&#20026;&#29983;&#25104;&#36807;&#31243;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#28982;&#21518;&#29992;&#20110;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#25351;&#23548;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DreamSparse &#22312;&#23569;&#35270;&#35282;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#29289;&#20307;&#20960;&#20309;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing novel view images from a few views is a challenging but practical problem. Existing methods often struggle with producing high-quality results or necessitate per-object optimization in such few-view settings due to the insufficient information provided. In this work, we explore leveraging the strong 2D priors in pre-trained diffusion models for synthesizing novel view images. 2D diffusion models, nevertheless, lack 3D awareness, leading to distorted image synthesis and compromising the identity. To address these problems, we propose DreamSparse, a framework that enables the frozen pre-trained diffusion model to generate geometry and identity-consistent novel view image. Specifically, DreamSparse incorporates a geometry module designed to capture 3D features from sparse views as a 3D prior. Subsequently, a spatial guidance model is introduced to convert these 3D feature maps into spatial information for the generative process. This information is then used to guide the pre-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24847;&#22270;&#24863;&#30693;FAQ&#26816;&#32034;&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#22312;&#21830;&#21697;&#25628;&#32034;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#24847;&#22270;&#20998;&#31867;&#22120;&#21644;&#37325;&#26500;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.03411</link><description>&lt;p&gt;
&#21830;&#21697;&#25628;&#32034;&#20013;&#30340;&#24847;&#22270;&#24863;&#30693;FAQ&#26816;&#32034;&#65306;&#29983;&#25104;-&#26816;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search. (arXiv:2306.03411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24847;&#22270;&#24863;&#30693;FAQ&#26816;&#32034;&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#22312;&#21830;&#21697;&#25628;&#32034;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#24847;&#22270;&#20998;&#31867;&#22120;&#21644;&#37325;&#26500;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21830;&#21697;&#25628;&#32034;&#24341;&#25806;&#20132;&#20114;&#30340;&#23458;&#25143;&#36234;&#26469;&#36234;&#22810;&#22320;&#21046;&#23450;&#20449;&#24687;&#26597;&#35810;&#35831;&#27714;&#12290;&#24120;&#38382;&#38382;&#39064;&#65288;FAQ&#65289;&#26816;&#32034;&#26088;&#22312;&#36890;&#36807;&#38382;&#39064;&#24847;&#22270;&#26469;&#26816;&#32034;&#29992;&#25143;&#26597;&#35810;&#30340;&#24120;&#35265;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#23558;FAQ&#26816;&#32034;&#19982;&#21830;&#21697;&#25628;&#32034;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#36141;&#20080;&#20915;&#31574;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#39640;&#25928;&#30340;&#21806;&#21518;&#25903;&#25345;&#22686;&#24378;&#29992;&#25143;&#20445;&#30041;&#29575;&#12290;&#22312;&#21830;&#21697;&#25628;&#32034;&#20013;&#30830;&#23450;&#20309;&#26102;&#21487;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#30340;FAQ&#26465;&#30446;&#65292;&#32780;&#19981;&#20250;&#25171;&#25200;&#20854;&#36141;&#29289;&#20307;&#39564;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24847;&#22270;&#24863;&#30693;FAQ&#26816;&#32034;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#65288;1&#65289;&#19968;&#20010;&#24847;&#22270;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#39044;&#27979;FAQ&#26159;&#21542;&#33021;&#22815;&#22238;&#31572;&#29992;&#25143;&#30340;&#38382;&#39064;&#65307;&#65288;2&#65289;&#19968;&#20010;&#37325;&#26500;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#26597;&#35810;&#37325;&#20889;&#20026;&#33258;&#28982;&#38382;&#39064;&#12290;&#31163;&#32447;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#22522;&#20934;FAQ&#26102;&#23558;Hit @ 1&#25552;&#39640;&#20102;13&#65285;&#65292;&#21516;&#26102;&#23558;&#24310;&#36831;&#38477;&#20302;&#20102;95&#65285;&#12290;&#36825;&#20123;&#25913;&#36827;&#32467;&#26524;&#35828;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#24847;&#22270;&#24863;&#30693;FAQ&#26816;&#32034;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customers interacting with product search engines are increasingly formulating information-seeking queries. Frequently Asked Question (FAQ) retrieval aims to retrieve common question-answer pairs for a user query with question intent. Integrating FAQ retrieval in product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Determining when an FAQ entry can satisfy a user's information need within product search, without disrupting their shopping experience, represents an important challenge. We propose an intent-aware FAQ retrieval system consisting of (1) an intent classifier that predicts when a user's information need can be answered by an FAQ; (2) a reformulation model that rewrites a query into a natural question. Offline evaluation demonstrates that our approach improves Hit@1 by 13% on retrieving ground-truth FAQs, while reducing latency by 95% compared to baseline systems. These impr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#30446;&#26631;&#38382;&#39064;&#30340;&#36827;&#21270;&#31639;&#27861;MOEA/D&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#38024;&#23545;&#35813;&#31639;&#27861;&#30340;&#20005;&#26684;&#36816;&#34892;&#26102;&#20998;&#26512;&#65292;&#35777;&#26126;&#22312;&#30456;&#24212;&#26465;&#20214;&#19979;MOEA/D&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#22312;oracle&#27169;&#22411;&#20013;&#21487;&#20197;&#20197;&#26399;&#26395;&#30340;&#22266;&#23450;&#22810;&#39033;&#24335;&#26102;&#38388;&#23436;&#25104;&#12290;&#22312;&#38543;&#26426;&#30340;&#23454;&#20363;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#29702;&#35770;&#20998;&#26512;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03409</link><description>&lt;p&gt;
&#23545;&#20110;&#27714;&#35299;&#22810;&#30446;&#26631;&#26368;&#23567;&#26435;&#37325;&#22522;&#38382;&#39064;&#30340;MOEA/D&#30340;&#20005;&#26684;&#36816;&#34892;&#26102;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Rigorous Runtime Analysis of MOEA/D for Solving Multi-Objective Minimum Weight Base Problems. (arXiv:2306.03409v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#30446;&#26631;&#38382;&#39064;&#30340;&#36827;&#21270;&#31639;&#27861;MOEA/D&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#38024;&#23545;&#35813;&#31639;&#27861;&#30340;&#20005;&#26684;&#36816;&#34892;&#26102;&#20998;&#26512;&#65292;&#35777;&#26126;&#22312;&#30456;&#24212;&#26465;&#20214;&#19979;MOEA/D&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#22312;oracle&#27169;&#22411;&#20013;&#21487;&#20197;&#20197;&#26399;&#26395;&#30340;&#22266;&#23450;&#22810;&#39033;&#24335;&#26102;&#38388;&#23436;&#25104;&#12290;&#22312;&#38543;&#26426;&#30340;&#23454;&#20363;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#29702;&#35770;&#20998;&#26512;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#26368;&#23567;&#26435;&#37325;&#22522;&#38382;&#39064;&#65292;&#36825;&#26159;&#32463;&#20856;&#30340;NP&#38590;&#39064;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65288;&#22914;&#22810;&#30446;&#26631;&#26368;&#23567;&#29983;&#25104;&#26641;&#38382;&#39064;&#65289;&#30340;&#19968;&#20010;&#25277;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20123;&#38750;&#25903;&#37197;&#21069;&#27839;&#30340;&#20984;&#21253;&#37325;&#35201;&#24615;&#36136;&#65292;&#20363;&#22914;&#20854;&#36924;&#36817;&#36136;&#37327;&#21644;&#26497;&#28857;&#25968;&#37327;&#30340;&#19978;&#30028;&#12290;&#21033;&#29992;&#36825;&#20123;&#24615;&#36136;&#65292;&#25105;&#20204;&#38024;&#23545;&#35813;&#38382;&#39064;&#32473;&#20986;&#20102;MOEA/D&#31639;&#27861;&#30340;&#39318;&#20010;&#36816;&#34892;&#26102;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#22810;&#30446;&#26631;&#20998;&#35299;&#20026;&#21333;&#30446;&#26631;&#32452;&#25104;&#37096;&#20998;&#24182;&#26377;&#25928;&#20248;&#21270;&#30340;&#36827;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#32473;&#23450;&#36866;&#24403;&#20998;&#35299;&#35774;&#32622;&#30340;&#24773;&#20917;&#19979;&#65292;MOEA/D&#33021;&#20197;&#26399;&#26395;&#30340;&#22266;&#23450;&#22810;&#39033;&#24335;&#26102;&#38388;&#22312;oracle&#27169;&#22411;&#20013;&#25214;&#21040;&#25152;&#26377;&#26497;&#28857;&#65292;&#21442;&#25968;&#26159;&#30446;&#26631;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#30340;&#21452;&#30446;&#26631;&#26368;&#23567;&#29983;&#25104;&#26641;&#23454;&#20363;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#19982;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#19982;&#20808;&#21069;&#38024;&#23545;GSEMO&#38382;&#39064;&#30740;&#31350;&#30340;&#36827;&#21270;&#31639;&#27861;&#30456;&#27604;&#36739;&#65292;...
&lt;/p&gt;
&lt;p&gt;
We study the multi-objective minimum weight base problem, an abstraction of classical NP-hard combinatorial problems such as the multi-objective minimum spanning tree problem. We prove some important properties of the convex hull of the non-dominated front, such as its approximation quality and an upper bound on the number of extreme points. Using these properties, we give the first run-time analysis of the MOEA/D algorithm for this problem, an evolutionary algorithm that effectively optimizes by decomposing the objectives into single-objective components. We show that the MOEA/D, given an appropriate decomposition setting, finds all extreme points within expected fixed-parameter polynomial time in the oracle model, the parameter being the number of objectives. Experiments are conducted on random bi-objective minimum spanning tree instances, and the results agree with our theoretical findings. Furthermore, compared with a previously studied evolutionary algorithm for the problem GSEMO,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#26410;&#34987;&#35775;&#38382;&#30340;&#20915;&#31574;&#26641;&#29366;&#24577;&#21644;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;MuZero&#26234;&#33021;&#20307;&#25913;&#36827;&#20102;&#26641;&#25628;&#32034;&#35268;&#21010;&#21644;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03408</link><description>&lt;p&gt;
&#26234;&#33021;&#20307;&#36890;&#36807;&#25506;&#32034;&#20915;&#31574;&#26641;&#20013;&#30340;&#29366;&#24577;&#26469;&#25552;&#39640;&#20854;&#20915;&#31574;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Agents Explore the Environment Beyond Good Actions to Improve Their Model for Better Decisions. (arXiv:2306.03408v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03408
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#26410;&#34987;&#35775;&#38382;&#30340;&#20915;&#31574;&#26641;&#29366;&#24577;&#21644;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;MuZero&#26234;&#33021;&#20307;&#25913;&#36827;&#20102;&#26641;&#25628;&#32034;&#35268;&#21010;&#21644;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#26234;&#33021;&#20307;&#20915;&#31574;&#33021;&#21147;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#36947;&#36335;&#19978;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;MuZero&#26234;&#33021;&#20307;&#36890;&#36807;&#32593;&#32476;&#27169;&#22411;&#30340;&#39044;&#27979;&#21644;&#22522;&#20110;&#39044;&#27979;&#32467;&#26524;&#30340;&#26641;&#25628;&#32034;&#35268;&#21010;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#35268;&#21010;&#25216;&#33021;&#65292;&#20294;&#24403;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#19981;&#20934;&#30830;&#26102;&#65292;&#23398;&#20064;&#36827;&#31243;&#21487;&#33021;&#20250;&#36935;&#21040;&#29942;&#39048;&#12290;&#25105;&#20204;&#36890;&#36807;&#35753;&#26234;&#33021;&#20307;&#25506;&#32034;&#29615;&#22659;&#20013;&#20915;&#31574;&#26641;&#20013;&#19968;&#20123;&#19981;&#20250;&#34987;&#35775;&#38382;&#21040;&#30340;&#29366;&#24577;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26234;&#33021;&#20307;&#39318;&#20808;&#36890;&#36807;&#35268;&#21010;&#24471;&#21040;&#25913;&#36827;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#35757;&#32451;&#38454;&#27573;&#30340;&#24320;&#22987;&#26102;&#38543;&#26426;&#20559;&#31163;&#36825;&#20010;&#31574;&#30053;&#12290;&#22312;&#19968;&#20010;&#38543;&#26426;&#30340;&#26102;&#38388;&#38454;&#27573;&#65292;&#26234;&#33021;&#20307;&#23558;&#21448;&#24674;&#22797;&#21040;&#25913;&#36827;&#31574;&#30053;&#20197;&#24471;&#21040;&#29615;&#22659;&#22870;&#21169;&#24182;&#23545;&#26399;&#26395;&#20215;&#20540;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20117;&#23383;&#26827;&#28216;&#25103;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the decision-making capabilities of agents is a key challenge on the road to artificial intelligence. To improve the planning skills needed to make good decisions, MuZero's agent combines prediction by a network model and planning by a tree search using the predictions. MuZero's learning process can fail when predictions are poor but planning requires them. We use this as an impetus to get the agent to explore parts of the decision tree in the environment that it otherwise would not explore. The agent achieves this, first by normal planning to come up with an improved policy. Second, it randomly deviates from this policy at the beginning of each training episode. And third, it switches back to the improved policy at a random time step to experience the rewards from the environment associated with the improved policy, which is the basis for learning the correct value expectation. The simple board game Tic-Tac-Toe is used to illustrate how this approach can improve the agent's 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20960;&#20309;&#23398;&#21644;&#25299;&#25169;&#23398;&#30340;&#35282;&#24230;&#65292;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#21644;&#25345;&#20037;&#21516;&#35843;&#20998;&#24418;&#32500;&#24230;&#23545;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#21644;&#25551;&#36848;&#65292;&#26088;&#22312;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.03406</link><description>&lt;p&gt;
&#20174;&#27969;&#24418;&#23398;&#20064;&#30340;&#35282;&#24230;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks architectures from the perspective of manifold learning. (arXiv:2306.03406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20960;&#20309;&#23398;&#21644;&#25299;&#25169;&#23398;&#30340;&#35282;&#24230;&#65292;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#21644;&#25345;&#20037;&#21516;&#35843;&#20998;&#24418;&#32500;&#24230;&#23545;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#21644;&#25551;&#36848;&#65292;&#26088;&#22312;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;&#20960;&#20309;&#23398;&#21644;&#25299;&#25169;&#23398;&#30340;&#35282;&#24230;&#20840;&#38754;&#27604;&#36739;&#21644;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#34920;&#31034;&#20197;&#21450;&#22312;&#19981;&#21516;&#23618;&#19978;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#32467;&#26500;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#21644;&#25345;&#20037;&#21516;&#35843;&#20998;&#24418;&#32500;&#24230;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#26500;&#20197;&#21450;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#22312;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#20869;&#20026;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advances in the field of deep learning in ap-plications to various areas, an explanation of the learning pro-cess of neural network models remains an important open ques-tion. The purpose of this paper is a comprehensive comparison and description of neural network architectures in terms of ge-ometry and topology. We focus on the internal representation of neural networks and on the dynamics of changes in the topology and geometry of a data manifold on different layers. In this paper, we use the concepts of topological data analysis (TDA) and persistent homological fractal dimension. We present a wide range of experiments with various datasets and configurations of convolutional neural network (CNNs) architectures and Transformers in CV and NLP tasks. Our work is a contribution to the development of the important field of explainable and interpretable AI within the framework of geometrical deep learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SGAT4PASS&#65292;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#36890;&#36807;&#21152;&#20837;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#26223;&#22270;&#20687;&#30340;3D&#23646;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03403</link><description>&lt;p&gt;
SGAT4PASS&#65306;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer
&lt;/p&gt;
&lt;p&gt;
SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation. (arXiv:2306.03403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SGAT4PASS&#65292;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#36890;&#36807;&#21152;&#20837;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#26223;&#22270;&#20687;&#30340;3D&#23646;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;&#21487;&#20197;&#26681;&#25454;&#36229;&#24191;&#35282;&#35266;&#23519;&#21040;&#30340;&#23436;&#25972;&#22330;&#26223;&#26469;&#36827;&#34892;&#24863;&#30693;&#12290;&#20256;&#32479;&#30340;&#38024;&#23545;2D&#20840;&#26223;&#22270;&#20687;&#30340;PASS&#26041;&#27861;&#20391;&#37325;&#20110;&#35299;&#20915;&#22270;&#20687;&#30072;&#21464;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047;&#23545;&#21407;&#22987;360&#176;&#25968;&#25454;&#30340;3D&#23646;&#24615;&#30340;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#24403;&#36755;&#20837;&#20855;&#26377;3D&#25200;&#21160;&#30340;&#20840;&#26223;&#22270;&#20687;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#22823;&#24133;&#19979;&#38477;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#24212;&#23545;3D&#25200;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#21363;SGAT4PASS&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20998;&#21106;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65292;&#21363;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#22270;&#20687;&#25237;&#24433;&#65292;&#29699;&#38754;&#21487;&#24418;&#21464;&#34917;&#19969;&#23884;&#20837;&#21644;&#20840;&#26223;&#24863;&#30693;&#25439;&#22833;&#65292;&#23427;&#23545;&#20855;&#26377;3D&#25200;&#21160;&#30340;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#23545;&#24050;&#26377;&#30340;&#21487;&#24418;&#21464;&#34917;&#19969;&#23884;&#20837;&#21152;&#20837;&#20102;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an important and challenging problem in computer vision, PAnoramic Semantic Segmentation (PASS) gives complete scene perception based on an ultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic image input focus on solving image distortions but lack consideration of the 3D properties of original $360^{\circ}$ data. Therefore, their performance will drop a lot when inputting panoramic images with the 3D disturbance. To be more robust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical geometry knowledge. Specifically, a spherical geometry-aware framework is proposed for PASS. It includes three modules, i.e., spherical geometry-aware image projection, spherical deformable patch embedding, and a panorama-aware loss, which takes input images with 3D disturbance into account, adds a spherical geometry-aware constraint on the existing deformable patch embedding, and indicates
&lt;/p&gt;</description></item><item><title>G-CAME &#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;&#39640;&#26031;&#31867;&#28608;&#27963;&#26144;&#23556;&#35299;&#37322;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#28608;&#27963;&#26144;&#23556;&#19982;&#39640;&#26031;&#26680;&#29983;&#25104;&#26174;&#33879;&#24615;&#22270;&#26469;&#31361;&#20986;&#26174;&#31034;&#22270;&#20687;&#20013;&#19982;&#39044;&#27979;&#26694;&#30456;&#20851;&#30340;&#37325;&#35201;&#21306;&#22495;&#65292;&#20855;&#26377;&#24456;&#30701;&#26102;&#38388;&#35299;&#37322;&#23545;&#35937;&#31561;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.03400</link><description>&lt;p&gt;
G-CAME: &#38754;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;&#39640;&#26031;&#31867;&#28608;&#27963;&#26144;&#23556;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
G-CAME: Gaussian-Class Activation Mapping Explainer for Object Detectors. (arXiv:2306.03400v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03400
&lt;/p&gt;
&lt;p&gt;
G-CAME &#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;&#39640;&#26031;&#31867;&#28608;&#27963;&#26144;&#23556;&#35299;&#37322;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#28608;&#27963;&#26144;&#23556;&#19982;&#39640;&#26031;&#26680;&#29983;&#25104;&#26174;&#33879;&#24615;&#22270;&#26469;&#31361;&#20986;&#26174;&#31034;&#22270;&#20687;&#20013;&#19982;&#39044;&#27979;&#26694;&#30456;&#20851;&#30340;&#37325;&#35201;&#21306;&#22495;&#65292;&#20855;&#26377;&#24456;&#30701;&#26102;&#38388;&#35299;&#37322;&#23545;&#35937;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#65292;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#26222;&#21450;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#29992;&#25143;&#24456;&#38590;&#29702;&#35299;&#27169;&#22411;&#20026;&#20160;&#20040;&#20250;&#26816;&#27979;&#20986;&#36825;&#20123;&#23545;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#26031;&#31867;&#28608;&#27963;&#26144;&#23556;&#35299;&#37322;&#22120;&#65288;G-CAME&#65289;&#65292;&#23427;&#29983;&#25104;&#26174;&#33879;&#24615;&#22270;&#20316;&#20026;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#35828;&#26126;&#12290; G-CAME &#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#22522;&#20110; CAM &#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#23618;&#30340;&#28608;&#27963;&#26144;&#23556;&#19982;&#39640;&#26031;&#26680;&#26469;&#31361;&#20986;&#26174;&#31034;&#22270;&#20687;&#20013;&#19982;&#39044;&#27979;&#26694;&#30456;&#20851;&#30340;&#37325;&#35201;&#21306;&#22495;&#12290;&#19982;&#20854;&#20182;&#22522;&#20110;&#21306;&#22495;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;G-CAME &#21487;&#20197;&#36229;&#36234;&#26102;&#38388;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#24456;&#30701;&#26102;&#38388;&#23601;&#33021;&#35299;&#37322;&#19968;&#20010;&#23545;&#35937;&#12290;&#25105;&#20204;&#36824;&#22312; MS-COCO 2017 &#25968;&#25454;&#38598;&#19978;&#20351;&#29992; YOLOX &#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#23548;&#23558; G-CAME &#24212;&#29992;&#20110;&#20004;&#38454;&#27573; Faster-RCNN &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, deep neural networks for object detection in images are very prevalent. However, due to the complexity of these networks, users find it hard to understand why these objects are detected by models. We proposed Gaussian Class Activation Mapping Explainer (G-CAME), which generates a saliency map as the explanation for object detection models. G-CAME can be considered a CAM-based method that uses the activation maps of selected layers combined with the Gaussian kernel to highlight the important regions in the image for the predicted box. Compared with other Region-based methods, G-CAME can transcend time constraints as it takes a very short time to explain an object. We also evaluated our method qualitatively and quantitatively with YOLOX on the MS-COCO 2017 dataset and guided to apply G-CAME into the two-stage Faster-RCNN model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35843;&#33410;&#26694;&#26550;ColdNAS&#26469;&#35299;&#20915;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23547;&#25214;&#36866;&#24403;&#30340;&#35843;&#33410;&#32467;&#26500;&#65292;&#21253;&#25324;&#20989;&#25968;&#21644;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.03387</link><description>&lt;p&gt;
ColdNAS: &#25628;&#32034;&#20197;&#35843;&#33410;&#20026;&#29992;&#25143;&#20919;&#21551;&#21160;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
ColdNAS: Search to Modulate for User Cold-Start Recommendation. (arXiv:2306.03387v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35843;&#33410;&#26694;&#26550;ColdNAS&#26469;&#35299;&#20915;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23547;&#25214;&#36866;&#24403;&#30340;&#35843;&#33410;&#32467;&#26500;&#65292;&#21253;&#25324;&#20989;&#25968;&#21644;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20026;&#20919;&#21551;&#21160;&#29992;&#25143;(&#21482;&#26377;&#19968;&#20123;&#20132;&#20114;&#21382;&#21490;)&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#36229;&#32593;&#32476;&#23558;&#29992;&#25143;&#20132;&#20114;&#21382;&#21490;&#30452;&#25509;&#26144;&#23556;&#21040;&#29992;&#25143;&#29305;&#23450;&#21442;&#25968;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#24615;&#32447;&#24615;&#35843;&#21046;&#20989;&#25968;&#26469;&#35843;&#21046;&#39044;&#27979;&#22120;&#12290; &#36825;&#20123;&#30740;&#31350;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290; &#28982;&#32780;&#65292;&#32553;&#25918;&#21644;&#31227;&#20301;&#22312;&#25512;&#33616;&#25968;&#25454;&#20013;&#30340;&#29289;&#29702;&#21547;&#20041;&#26159;&#19981;&#28165;&#26970;&#30340;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35843;&#33410;&#26694;&#26550;ColdNAS&#26469;&#35299;&#20915;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23547;&#25214;&#36866;&#24403;&#30340;&#35843;&#33410;&#32467;&#26500;&#65292;&#21253;&#25324;&#20989;&#25968;&#21644;&#20301;&#32622;&#12290; &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25628;&#32034;&#31354;&#38388;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#27169;&#22411;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#35813;&#25628;&#32034;&#31354;&#38388;&#21487;&#20197;&#36716;&#25442;&#20026;&#19968;&#20010;&#26356;&#23567;&#30340;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#19988;&#40065;&#26834;&#30340;&#19968;&#27425;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making personalized recommendation for cold-start users, who only have a few interaction histories, is a challenging problem in recommendation systems. Recent works leverage hypernetworks to directly map user interaction histories to user-specific parameters, which are then used to modulate predictor by feature-wise linear modulation function. These works obtain the state-of-the-art performance. However, the physical meaning of scaling and shifting in recommendation data is unclear. Instead of using a fixed modulation function and deciding modulation position by expertise, we propose a modulation framework called ColdNAS for user cold-start problem, where we look for proper modulation structure, including function and position, via neural architecture search. We design a search space which covers broad models and theoretically prove that this search space can be transformed to a much smaller space, enabling an efficient and robust one-shot search algorithm. Extensive experimental resul
&lt;/p&gt;</description></item><item><title>VR.net&#26159;&#19968;&#20010;&#29992;&#20110;&#34394;&#25311;&#29616;&#23454;&#26197;&#21160;&#30740;&#31350;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#22312;10&#20010;&#19981;&#21516;&#39118;&#26684;&#30340;&#28216;&#25103;&#20013;&#25552;&#20379;&#20102;&#22823;&#32422;12&#23567;&#26102;&#30340;&#28216;&#25103;&#29609;&#27861;&#35270;&#39057;&#65292;&#20197;&#21450;&#19982;&#26197;&#21160;&#30456;&#20851;&#30340;&#26631;&#31614;&#12290;&#21487;&#20197;&#29992;&#20110;&#39118;&#38505;&#22240;&#32032;&#26816;&#27979;&#21644;&#26197;&#21160;&#27700;&#24179;&#39044;&#27979;&#31561;&#24212;&#29992;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2306.03381</link><description>&lt;p&gt;
VR.net&#65306;&#29992;&#20110;&#34394;&#25311;&#29616;&#23454;&#26197;&#21160;&#30740;&#31350;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VR.net: A Real-world Dataset for Virtual Reality Motion Sickness Research. (arXiv:2306.03381v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03381
&lt;/p&gt;
&lt;p&gt;
VR.net&#26159;&#19968;&#20010;&#29992;&#20110;&#34394;&#25311;&#29616;&#23454;&#26197;&#21160;&#30740;&#31350;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#22312;10&#20010;&#19981;&#21516;&#39118;&#26684;&#30340;&#28216;&#25103;&#20013;&#25552;&#20379;&#20102;&#22823;&#32422;12&#23567;&#26102;&#30340;&#28216;&#25103;&#29609;&#27861;&#35270;&#39057;&#65292;&#20197;&#21450;&#19982;&#26197;&#21160;&#30456;&#20851;&#30340;&#26631;&#31614;&#12290;&#21487;&#20197;&#29992;&#20110;&#39118;&#38505;&#22240;&#32032;&#26816;&#27979;&#21644;&#26197;&#21160;&#27700;&#24179;&#39044;&#27979;&#31561;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;VR&#20307;&#39564;&#20013;&#35782;&#21035;&#26197;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#31934;&#30830;&#26631;&#35760;&#12289;&#29616;&#23454;&#19990;&#30028;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#26222;&#36866;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;VR.net&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#26469;&#33258;10&#31181;&#19981;&#21516;&#39118;&#26684;&#30340;10&#31181;&#29616;&#23454;&#19990;&#30028;&#28216;&#25103;&#20013;&#32422;12&#20010;&#23567;&#26102;&#30340;&#28216;&#25103;&#29609;&#27861;&#35270;&#39057;&#12290;&#23545;&#20110;&#27599;&#20010;&#35270;&#39057;&#24103;&#65292;&#31934;&#20934;&#22320;&#20998;&#37197;&#20102;&#19968;&#32452;&#19982;&#26197;&#21160;&#30456;&#20851;&#30340;&#26631;&#31614;&#65292;&#22914;&#30456;&#26426;/&#29289;&#20307;&#36816;&#21160;&#12289;&#28145;&#24230;&#22330;&#21644;&#36816;&#21160;&#27969;&#12290;&#26500;&#24314;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#25163;&#21160;&#26631;&#35760;&#38656;&#35201;&#19981;&#21487;&#34892;&#30340;&#26102;&#38388;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#24037;&#20855;&#20174;3D&#24341;&#25806;&#30340;&#28210;&#26579;&#31649;&#36947;&#20013;&#33258;&#21160;&#31934;&#30830;&#22320;&#25552;&#21462;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;VR&#28216;&#25103;&#30340;&#28304;&#20195;&#30721;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#24212;&#29992;&#31243;&#24207;&#23637;&#31034;&#20102;VR.net&#30340;&#23454;&#29992;&#24615;&#65292;&#22914;&#39118;&#38505;&#22240;&#32032;&#26816;&#27979;&#21644;&#26197;&#21160;&#27700;&#24179;&#39044;&#27979;&#12290;&#25105;&#20204;&#19981;&#26029;&#25193;&#23637;VR.net&#65292;&#24182;&#24076;&#26395;&#20854;&#19979;&#19968;&#20010;&#29256;&#26412;&#25552;&#20379;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have used machine learning approaches to identify motion sickness in VR experience. These approaches demand an accurately-labeled, real-world, and diverse dataset for high accuracy and generalizability. As a starting point to address this need, we introduce `VR.net', a dataset offering approximately 12-hour gameplay videos from ten real-world games in 10 diverse genres. For each video frame, a rich set of motion sickness-related labels, such as camera/object movement, depth field, and motion flow, are accurately assigned. Building such a dataset is challenging since manual labeling would require an infeasible amount of time. Instead, we utilize a tool to automatically and precisely extract ground truth data from 3D engines' rendering pipelines without accessing VR games' source code. We illustrate the utility of VR.net through several applications, such as risk factor detection and sickness level prediction. We continuously expand VR.net and envision its next version offeri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#27861;&#25506;&#32034;&#20154;&#33041;&#20013;&#20851;&#20110;&#35270;&#35273;&#27010;&#24565;&#30340;&#39640;&#24230;&#32454;&#20998;&#30340;&#35821;&#20041;&#32593;&#32476;&#65292;&#20174;&#32780;&#35782;&#21035;&#20849;&#20139;&#30340;&#21487;&#35299;&#30721;&#27010;&#24565;&#65292;&#20197;&#25512;&#26029;&#26159;&#21542;&#23384;&#22312;&#19987;&#38376;&#29992;&#20110;&#37325;&#35201;&#35821;&#20041;&#27010;&#24565;&#30340;&#33041;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.03375</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#20687;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#22823;&#33041;&#20013;&#35782;&#21035;&#20849;&#20139;&#30340;&#21487;&#35299;&#30721;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Identifying Shared Decodable Concepts in the Human Brain Using Image-Language Foundation Models. (arXiv:2306.03375v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#27861;&#25506;&#32034;&#20154;&#33041;&#20013;&#20851;&#20110;&#35270;&#35273;&#27010;&#24565;&#30340;&#39640;&#24230;&#32454;&#20998;&#30340;&#35821;&#20041;&#32593;&#32476;&#65292;&#20174;&#32780;&#35782;&#21035;&#20849;&#20139;&#30340;&#21487;&#35299;&#30721;&#27010;&#24565;&#65292;&#20197;&#25512;&#26029;&#26159;&#21542;&#23384;&#22312;&#19987;&#38376;&#29992;&#20110;&#37325;&#35201;&#35821;&#20041;&#27010;&#24565;&#30340;&#33041;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#34920;&#31034;&#25506;&#32034;&#20154;&#31867;&#22823;&#33041;&#20013;&#32454;&#31890;&#24230;&#35821;&#20041;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#21487;&#35299;&#30721;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#20174;&#32780;&#35782;&#21035;&#19987;&#38376;&#29992;&#20110;&#20854;&#20182;&#37325;&#35201;&#35821;&#20041;&#27010;&#24565;&#30340;&#22823;&#33041;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method that takes advantage of high-quality pretrained multimodal representations to explore fine-grained semantic networks in the human brain. Previous studies have documented evidence of functional localization in the brain, with different anatomical regions preferentially activating for different types of sensory input. Many such localized structures are known, including the fusiform face area and parahippocampal place area. This raises the question of whether additional brain regions (or conjunctions of brain regions) are also specialized for other important semantic concepts. To identify such brain regions, we developed a data-driven approach to uncover visual concepts that are decodable from a massive functional magnetic resonance imaging (fMRI) dataset. Our analysis is broadly split into three sections. First, a fully connected neural network is trained to map brain responses to the outputs of an image-language foundation model, CLIP (Radford et al., 2021). Subseq
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20013;&#38388;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#27169;&#24577;&#36712;&#36857;&#20197;&#21450;&#33258;&#30417;&#30563;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22810;&#27493;&#21644;&#21333;&#27493;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;INTERACTION&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03367</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#26041;&#27861;&#24357;&#21512;&#22810;&#27493;&#21644;&#21333;&#27493;&#36712;&#36857;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap Between Multi-Step and One-Shot Trajectory Prediction via Self-Supervision. (arXiv:2306.03367v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20013;&#38388;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#27169;&#24577;&#36712;&#36857;&#20197;&#21450;&#33258;&#30417;&#30563;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22810;&#27493;&#21644;&#21333;&#27493;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;INTERACTION&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#23384;&#22312;&#35768;&#22810;&#24320;&#25918;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#37319;&#29992;&#19968;&#27425;&#24615;&#39044;&#27979;&#65292;&#35201;&#20040;&#37319;&#29992;&#36880;&#27493;&#39044;&#27979;&#30340;&#26041;&#24335;&#36827;&#34892;&#36712;&#36857;&#22238;&#24402;&#12290;&#34429;&#28982;&#19968;&#27425;&#24615;&#26041;&#27861;&#36890;&#24120;&#22240;&#20854;&#31616;&#21333;&#24615;&#32780;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#25918;&#24323;&#20102;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#26426;&#21046;&#65292;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#36890;&#36807;&#38142;&#25509;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#26469;&#26500;&#24314;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20013;&#38388;&#38454;&#27573;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#36712;&#36857;&#29255;&#27573;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#20998;&#25903;&#33258;&#30417;&#30563;&#39044;&#27979;&#22120;&#22312;&#24320;&#22987;&#26032;&#39044;&#27979;&#30340;&#20013;&#38388;&#26410;&#26469;&#26102;&#38388;&#27573;&#25509;&#25910;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#36890;&#36807;&#26641;&#29366;&#26041;&#24335;&#32452;&#21512;&#22810;&#27169;&#24577;&#36712;&#36857;&#65292;&#21516;&#26102;&#8220;&#24819;&#35937;&#8221;&#28508;&#22312;&#29615;&#22659;&#65292;&#36890;&#36807;&#8220;&#39044;&#27979;&#36807;&#21435;&#8221;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#25925;&#24847;&#20445;&#25345;&#20132;&#20114;&#21644;&#29615;&#22659;&#24314;&#27169;&#31561;&#26041;&#38754;&#30340;&#31616;&#21333;&#24615;&#65292;&#20173;&#28982;&#22312;INTERACTION&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26410;&#32463;&#20805;&#20998;&#25506;&#32034;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate vehicle trajectory prediction is an unsolved problem in autonomous driving with various open research questions. State-of-the-art approaches regress trajectories either in a one-shot or step-wise manner. Although one-shot approaches are usually preferred for their simplicity, they relinquish powerful self-supervision schemes that can be constructed by chaining multiple time-steps. We address this issue by proposing a middle-ground where multiple trajectory segments are chained together. Our proposed Multi-Branch Self-Supervised Predictor receives additional training on new predictions starting at intermediate future segments. In addition, the model 'imagines' the latent context and 'predicts the past' while combining multi-modal trajectories in a tree-like manner. We deliberately keep aspects such as interaction and environment modeling simplistic and nevertheless achieve competitive results on the INTERACTION dataset. Furthermore, we investigate the sparsely explored uncertai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Offline-with-Action-Preferences&#65288;OAP&#65289;&#30340;&#26080;&#20132;&#20114;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#26597;&#35810;&#20808;&#21069;&#25910;&#38598;&#30340;&#21644;&#23398;&#20064;&#21040;&#30340;&#34892;&#21160;&#20043;&#38388;&#30340;&#20559;&#22909;&#65292;&#26469;&#24110;&#21161;&#35299;&#20915;&#38169;&#35823;&#20272;&#35745;&#38382;&#39064;&#65292;&#20174;&#32780;&#33719;&#24471;&#23545;&#26410;&#35265;&#25968;&#25454;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.03362</link><description>&lt;p&gt;
&#20351;&#29992;&#34892;&#20026;&#20559;&#22909;&#26597;&#35810;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Offline Reinforcement Learning with Action Preference Query. (arXiv:2306.03362v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Offline-with-Action-Preferences&#65288;OAP&#65289;&#30340;&#26080;&#20132;&#20114;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#26597;&#35810;&#20808;&#21069;&#25910;&#38598;&#30340;&#21644;&#23398;&#20064;&#21040;&#30340;&#34892;&#21160;&#20043;&#38388;&#30340;&#20559;&#22909;&#65292;&#26469;&#24110;&#21161;&#35299;&#20915;&#38169;&#35823;&#20272;&#35745;&#38382;&#39064;&#65292;&#20174;&#32780;&#33719;&#24471;&#23545;&#26410;&#35265;&#25968;&#25454;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#23454;&#29992;&#20195;&#29702;&#36890;&#24120;&#28041;&#21450;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20197;&#24179;&#34913;&#25919;&#31574;&#30340;&#24615;&#33021;&#21644;&#20132;&#20114;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#20132;&#20114;&#30340;&#35757;&#32451;&#26041;&#26696; Offline-with-Action-Preferences&#65288;OAP&#65289;&#12290; OAP&#30340;&#20027;&#35201;&#35265;&#35299;&#26159;&#65292;&#19982;&#22312;&#32447;&#24494;&#35843;&#30456;&#27604;&#65292;&#26597;&#35810;&#20107;&#20808;&#25910;&#38598;&#30340;&#21644;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#20559;&#22909;&#21487;&#20197;&#21516;&#26679;&#25110;&#29978;&#33267;&#26356;&#26377;&#21161;&#20110;&#35299;&#20915;&#38169;&#35823;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#26681;&#25454;&#34892;&#20026;&#20559;&#22909;&#33258;&#36866;&#24212;&#22320;&#40723;&#21169;&#25110;&#25233;&#21046;&#31574;&#30053;&#32422;&#26463;&#65292;OAP&#21487;&#20197;&#21306;&#20998;&#36807;&#24230;&#20272;&#35745;&#21644;&#26377;&#30410;&#30340;&#31574;&#30053;&#25913;&#36827;&#65292;&#20174;&#32780;&#33719;&#24471;&#23545;&#26410;&#35265;&#25968;&#25454;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training practical agents usually involve offline and online reinforcement learning (RL) to balance the policy's performance and interaction costs. In particular, online fine-tuning has become a commonly used method to correct the erroneous estimates of out-of-distribution data learned in the offline training phase. However, even limited online interactions can be inaccessible or catastrophic for high-stake scenarios like healthcare and autonomous driving. In this work, we introduce an interaction-free training scheme dubbed Offline-with-Action-Preferences (OAP). The main insight is that, compared to online fine-tuning, querying the preferences between pre-collected and learned actions can be equally or even more helpful to the erroneous estimate problem. By adaptively encouraging or suppressing policy constraint according to action preferences, OAP could distinguish overestimation from beneficial policy improvement and thus attains a more accurate evaluation of unseen data. Theoretica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03361</link><description>&lt;p&gt;
&#35774;&#35745;&#29992;&#25143;&#35282;&#33394;&#24863;&#30693;&#30340;&#23545;&#35805;&#20195;&#29702;&#36827;&#34892;&#26377;&#36259;&#30340;&#23545;&#35805;&#65306;$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground
&lt;/p&gt;
&lt;p&gt;
$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20197;&#35299;&#20915;&#21830;&#19994;&#35774;&#32622;&#20013;&#28041;&#21450;&#20010;&#24615;&#21270;&#23545;&#35805;&#21709;&#24212;&#19982;&#38750;&#27491;&#24335;&#21709;&#24212;&#20132;&#26367;&#30340;$\textit{WWH}$&#65288;$\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20197;&#24212;&#23545;&#20010;&#24615;&#21270;&#12289;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;$\textit{WWH}$&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#21709;&#24212;&#31867;&#22411;&#26631;&#31614;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#32452;&#21512;&#23548;&#33268;&#20102;&#26356;&#21152;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#20027;&#35266;&#20154;&#31867;&#35780;&#20272;&#21644;&#23458;&#35266;&#35780;&#20272;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for building a personalized open-domain dialogue system to address the $\textit{WWH}$ ($\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of $\textit{WWH}$ in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.
&lt;/p&gt;</description></item><item><title>Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.03360</link><description>&lt;p&gt;
Vid2Act&#65306;&#20026;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#28608;&#27963;&#31163;&#32447;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Vid2Act: Activate Offline Videos for Visual RL. (arXiv:2306.03360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03360
&lt;/p&gt;
&lt;p&gt;
Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26159;&#25552;&#39640;&#20854;&#22312;&#32447;&#20219;&#21153;&#25928;&#29575;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#36328;&#22495;&#20013;&#20219;&#21153;&#12289;&#21160;&#24577;&#21644;&#34892;&#20026;&#30340;&#22266;&#26377;&#19981;&#21305;&#37197;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;APV&#30340;&#27169;&#22411;&#36991;&#20813;&#20102;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#20276;&#38543;&#21160;&#20316;&#35760;&#24405;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#22312;&#28304;&#22495;&#20869;&#39044;&#35757;&#32451;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#12289;&#19981;&#28041;&#21450;&#25805;&#20316;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Vid2Act&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20174;&#31163;&#32447;&#21040;&#22312;&#32447;&#29615;&#22659;&#20013;&#20256;&#36755;&#26377;&#20215;&#20540;&#30340;&#21160;&#20316;&#26465;&#20214;&#21160;&#24577;&#21644;&#28508;&#22312;&#26377;&#29992;&#30340;&#21160;&#20316;&#28436;&#31034;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#19981;&#20165;&#23558;&#19990;&#30028;&#27169;&#22411;&#29992;&#20316;&#34892;&#20026;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#65292;&#36824;&#23558;&#20854;&#29992;&#20316;&#27979;&#37327;&#39046;&#22495;&#30456;&#20851;&#24615;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#36827;&#34892;&#21160;&#24577;&#34920;&#31034;&#20256;&#36755;&#21644;&#31574;&#30053;&#20256;&#36755;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22495;&#36873;&#25321;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#35757;&#32451;&#19990;&#30028;&#27169;&#22411;&#29983;&#25104;&#19968;&#32452;&#26102;&#38388;&#21464;&#21270;&#30340;&#20219;&#21153;&#30456;&#20284;&#24230;&#12290;&#36825;&#20123;&#30456;&#20284;&#24230;&#26377;&#20004;&#20010;&#30446;&#30340;&#65306;&#65288;i&#65289;&#33258;&#36866;&#24212;&#22320;&#23558;&#26368;&#30456;&#20851;&#30340;&#39046;&#22495;&#30340;&#21160;&#24577;&#20256;&#36755;&#21040;&#22312;&#32447;&#29615;&#22659;&#65292;&#21644;&#65288;ii&#65289;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25351;&#23548;&#20195;&#29702;&#38598;&#20013;&#25191;&#34892;&#20219;&#21153;&#30456;&#20851;&#30340;&#21160;&#20316;&#12290;&#22312;Atari&#21644;DMControl&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining RL models on offline video datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV, sidesteps the accompanied action records in offline datasets and instead focuses on pretraining a task-irrelevant, action-free world model within the source domains. We present Vid2Act, a model-based RL method that learns to transfer valuable action-conditioned dynamics and potentially useful action demonstrations from offline to online settings. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the domain relevance for both dynamics representation transfer and policy transfer. Specifically, we train the world models to generate a set of time-varying task similarities using a domain-selective knowledge distillation loss. These similarities serve two purposes: (i) adaptively transferring th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#29983;&#23545;&#8220;AI-giarism&#8221;&#30340;&#35748;&#30693;&#65292;&#25552;&#20986;&#20102;&#21021;&#22987;&#27010;&#24565;&#21270;&#30340;AI-giarism&#24037;&#20855;&#65292;&#26377;&#21161;&#20110;&#24212;&#23545;&#19981;&#26029;&#21457;&#23637;&#30340;AI&#25216;&#26415;&#24102;&#26469;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#65292;&#21516;&#26102;&#36824;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.03358</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25913;&#21464;&#20102;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#30340;&#35268;&#21017;&#21527;&#65311;&#28145;&#20837;&#25506;&#31350;&#23398;&#29983;&#23545;&#8220;AI-giarism&#8221;&#30340;&#30475;&#27861;
&lt;/p&gt;
&lt;p&gt;
Is AI Changing the Rules of Academic Misconduct? An In-depth Look at Students' Perceptions of 'AI-giarism'. (arXiv:2306.03358v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#29983;&#23545;&#8220;AI-giarism&#8221;&#30340;&#35748;&#30693;&#65292;&#25552;&#20986;&#20102;&#21021;&#22987;&#27010;&#24565;&#21270;&#30340;AI-giarism&#24037;&#20855;&#65292;&#26377;&#21161;&#20110;&#24212;&#23545;&#19981;&#26029;&#21457;&#23637;&#30340;AI&#25216;&#26415;&#24102;&#26469;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#65292;&#21516;&#26102;&#36824;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#25506;&#35752;&#20102;&#39640;&#31561;&#25945;&#32946;&#32972;&#26223;&#19979;&#65292;AI&#21644;&#25220;&#34989;&#21512;&#20307;&#25152;&#28041;&#21450;&#30340;&#26032;&#20852;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#8220;AI-giarism&#8221;&#30340;&#23398;&#29983;&#35748;&#30693;&#12290;&#20849;&#26377;&#26469;&#33258;&#19981;&#21516;&#23398;&#31185;&#30340;393&#21517;&#26412;&#31185;&#21644;&#30740;&#31350;&#29983;&#21442;&#19982;&#20102;&#35843;&#26597;&#65292;&#21463;&#35775;&#32773;&#38024;&#23545;&#22810;&#31181;AI-giarism&#24773;&#22659;&#34920;&#36798;&#20102;&#19981;&#21516;&#30340;&#30475;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#35748;&#30693;&#26684;&#23616;&#65292;&#26126;&#30830;&#21453;&#23545;&#30452;&#25509;&#29983;&#25104;AI&#20869;&#23481;&#65292;&#20294;&#23545;&#20110;&#26356;&#24494;&#22937;&#30340;AI&#20351;&#29992;&#26041;&#24335;&#21017;&#25345;&#26356;&#20026;&#26279;&#26151;&#24577;&#24230;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#24037;&#20855;&#8212;&#8212;&#20316;&#20026;AI-giarism&#30340;&#21021;&#22987;&#27010;&#24565;&#21270;&#8212;&#8212;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#12290;&#35813;&#37327;&#20855;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#25506;&#35752;&#19982;AI&#26377;&#20851;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#65292;&#26377;&#21161;&#20110;&#22312;AI&#38598;&#25104;&#26102;&#36827;&#34892;&#25945;&#23398;&#35774;&#35745;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#23450;&#20041;&#65292;&#24378;&#35843;&#20102;&#26681;&#25454;&#19981;&#26029;&#21457;&#23637;&#30340;AI&#25216;&#26415;&#36827;&#34892;&#36866;&#24212;&#30340;&#24517;&#35201;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#38480;&#21046;&#65292;&#20363;&#22914;&#20855;&#26377;&#26679;&#26412;&#20559;&#24046;&#31561;&#38382;&#39064;&#65292;&#20294;&#26159;&#35813;&#30740;&#31350;&#20173;&#28982;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This pioneering study explores students' perceptions of AI-giarism, an emergent form of academic dishonesty involving AI and plagiarism, within the higher education context. A survey, undertaken by 393 undergraduate and postgraduate students from a variety of disciplines, investigated their perceptions of diverse AI-giarism scenarios. The findings portray a complex landscape of understanding, with clear disapproval for direct AI content generation, yet more ambivalent attitudes towards subtler uses of AI. The study introduces a novel instrument, as an initial conceptualization of AI-giarism, offering a significant tool for educators and policy-makers. This scale facilitates understanding and discussions around AI-related academic misconduct, aiding in pedagogical design and assessment in an era of AI integration. Moreover, it challenges traditional definitions of academic misconduct, emphasizing the need to adapt in response to evolving AI technology. Despite limitations, such as the r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#21453;&#20107;&#23454;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#38382;&#39064;&#21644;&#20351;&#29992;&#21453;&#20107;&#23454;&#20223;&#30495;&#26469;&#35299;&#20915;&#22240;&#26524;&#20851;&#31995;&#38750;&#31283;&#24577;&#38382;&#39064;&#21644;&#24178;&#39044;&#38480;&#21046;&#65292;&#22312;&#30495;&#23454;&#39550;&#39542;&#34892;&#20026;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.03354</link><description>&lt;p&gt;
&#22522;&#20110;&#20223;&#30495;&#30340;&#21453;&#20107;&#23454;&#22240;&#26524;&#21457;&#29616;&#19982;&#30495;&#23454;&#39550;&#39542;&#34892;&#20026;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Simulation-Based Counterfactual Causal Discovery on Real World Driver Behaviour. (arXiv:2306.03354v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#21453;&#20107;&#23454;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#38382;&#39064;&#21644;&#20351;&#29992;&#21453;&#20107;&#23454;&#20223;&#30495;&#26469;&#35299;&#20915;&#22240;&#26524;&#20851;&#31995;&#38750;&#31283;&#24577;&#38382;&#39064;&#21644;&#24178;&#39044;&#38480;&#21046;&#65292;&#22312;&#30495;&#23454;&#39550;&#39542;&#34892;&#20026;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#33258;&#24049;&#34892;&#20026;&#22914;&#20309;&#24433;&#21709;&#20182;&#20154;&#34892;&#20026;&#26159;&#39550;&#39542;&#26234;&#33021;&#20307;&#25152;&#38656;&#30340;&#26680;&#24515;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#26234;&#33021;&#20307;&#21457;&#29616;&#33258;&#24049;&#21644;&#20182;&#20154;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#38656;&#27714;&#12290;&#35266;&#23519;&#24615;&#26041;&#27861;&#38754;&#20020;&#30528;&#21160;&#24577;&#29615;&#22659;&#23548;&#33268;&#22240;&#26524;&#20851;&#31995;&#38750;&#31283;&#24577;&#21270;&#65292;&#20197;&#21450;&#22240;&#26524;&#20132;&#20114;&#31232;&#30095;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#38656;&#35201;&#22312;&#32447;&#24037;&#20316;&#12290;&#32780;&#24178;&#39044;&#24615;&#26041;&#27861;&#21017;&#22240;&#20026;&#36710;&#36742;&#26080;&#27861;&#22312;&#20844;&#20849;&#36947;&#36335;&#19978;&#36827;&#34892;&#23454;&#39564;&#32780;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#22240;&#26524;&#20851;&#31995;&#30340;&#38750;&#31283;&#24577;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20107;&#20214;&#25552;&#21462;&#26041;&#38754;&#37325;&#26032;&#23450;&#20041;&#20102;&#38382;&#39064;&#65292;&#32780;&#20043;&#21069;&#25552;&#21040;&#30340;&#24178;&#39044;&#38480;&#21046;&#21487;&#20197;&#36890;&#36807;&#21453;&#20107;&#23454;&#20223;&#30495;&#26469;&#20811;&#26381;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21464;&#20307;&#30340;&#21453;&#20107;&#23454;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#35266;&#23519;&#24615;&#26102;&#38388;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;3396&#20010;&#22240;&#26524;&#26679;&#26412;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to reason about how one's behaviour can affect the behaviour of others is a core skill required of intelligent driving agents. Despite this, the state of the art struggles to meet the need of agents to discover causal links between themselves and others. Observational approaches struggle because of the non-stationarity of causal links in dynamic environments, and the sparsity of causal interactions while requiring the approaches to work in an online fashion. Meanwhile interventional approaches are impractical as a vehicle cannot experiment with its actions on a public road. To counter the issue of non-stationarity we reformulate the problem in terms of extracted events, while the previously mentioned restriction upon interventions can be overcome with the use of counterfactual simulation. We present three variants of the proposed counterfactual causal discovery method and evaluate these against state of the art observational temporal causal discovery methods across 3396 caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#23454;&#39564;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03346</link><description>&lt;p&gt;
&#31283;&#23450;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;: &#31163;&#32447;&#30446;&#26631;&#36798;&#25104;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Contrastive RL: Techniques for Offline Goal Reaching. (arXiv:2306.03346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#23454;&#39564;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24050;&#32463;&#24320;&#21457;&#20102;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24378;&#21270;&#23398;&#20064;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#33258;&#30417;&#30563;&#38382;&#39064;&#65306;&#23398;&#20064;&#36798;&#21040;&#20219;&#20309;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#25351;&#23450;&#30340;&#22870;&#21169;&#25110;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#24314;&#31435;&#33258;&#30417;&#30563;&#22522;&#30784;&#23454;&#38469;&#19978;&#38754;&#20020;&#30528;&#19968;&#20123;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#27492;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#21078;&#26512;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19982;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36890;&#36807;&#36825;&#20123;&#35774;&#35745;&#20915;&#31574;&#65292;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#20854;&#20013;&#20219;&#21153;&#30001;&#35757;&#32451;&#21518;&#25552;&#20379;&#30340;&#21333;&#20010;&#30446;&#26631;&#22270;&#20687;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the same way that the computer vision (CV) and natural language processing (NLP) communities have developed self-supervised methods, reinforcement learning (RL) can be cast as a self-supervised problem: learning to reach any goal, without requiring human-specified rewards or labels. However, actually building a self-supervised foundation for RL faces some important challenges. Building on prior contrastive approaches to this RL problem, we conduct careful ablation experiments and discover that a shallow and wide architecture, combined with careful weight initialization and data augmentation, can significantly boost the performance of these contrastive RL approaches on challenging simulated benchmarks. Additionally, we demonstrate that, with these design decisions, contrastive approaches can solve real-world robotic manipulation tasks, with tasks being specified by a single goal image provided after training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#25945;&#24072;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#26469;&#25913;&#36827;&#23569;&#26679;&#26412;&#27169;&#22411;&#65292;&#23454;&#29616;&#21516;&#26102;&#29983;&#25104;&#20219;&#21153;&#26631;&#31614;&#21644;&#21407;&#29702;&#30340;&#25928;&#26524;&#65307;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Masked Label Regularization&#65292;&#21487;&#20197;&#26126;&#30830;&#22320;&#24378;&#21046;&#35299;&#37322;&#26126;&#30830;&#22320;&#26465;&#20214;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.03315</link><description>&lt;p&gt;
&#21452;&#25945;&#24072;&#33258;&#25105;&#35757;&#32451;&#30340;&#23569;&#26679;&#26412;&#21407;&#29702;&#29983;&#25104;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Few Shot Rationale Generation using Self-Training with Dual Teachers. (arXiv:2306.03315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#25945;&#24072;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#26469;&#25913;&#36827;&#23569;&#26679;&#26412;&#27169;&#22411;&#65292;&#23454;&#29616;&#21516;&#26102;&#29983;&#25104;&#20219;&#21153;&#26631;&#31614;&#21644;&#21407;&#29702;&#30340;&#25928;&#26524;&#65307;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Masked Label Regularization&#65292;&#21487;&#20197;&#26126;&#30830;&#22320;&#24378;&#21046;&#35299;&#37322;&#26126;&#30830;&#22320;&#26465;&#20214;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35299;&#37322;&#27169;&#22411;&#21516;&#26102;&#20026;&#20854;&#39044;&#27979;&#30340;&#26631;&#31614;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#26159;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#30001;&#20110;&#20026;&#27880;&#37322;&#26631;&#31614;&#29983;&#25104;&#35299;&#37322;&#26159;&#19968;&#20010;&#36153;&#21147;&#19988;&#25104;&#26412;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#22240;&#27492;&#36817;&#26399;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20316;&#20026;&#20854;&#39592;&#24178;&#65292;&#24182;&#19988;&#37319;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#23569;&#26679;&#26412;&#27169;&#22411;&#65292;&#20551;&#35774;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#37117;&#27809;&#26377;&#20154;&#24037;&#32534;&#20889;&#30340;&#21407;&#29702;&#25110;&#26631;&#27880;&#20219;&#21153;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#25945;&#24072;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#25105;&#35757;&#32451;&#21644;&#31934;&#28860;&#20102;&#20004;&#20010;&#19987;&#19994;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#29992;&#20110;&#20219;&#21153;&#39044;&#27979;&#21644;&#29702;&#24615;&#21270;&#65292;&#23558;&#23427;&#20204;&#30340;&#30693;&#35782;&#36716;&#21270;&#20026;&#33021;&#22815;&#20849;&#21516;&#29983;&#25104;&#20219;&#21153;&#26631;&#31614;&#21644;&#21407;&#29702;&#30340;&#22810;&#20219;&#21153;&#23398;&#29983;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25513;&#30721;&#26631;&#31614;&#27491;&#21017;&#21270;&#65288;MLR&#65289;&#65292;&#23558;&#35299;&#37322;&#26126;&#30830;&#22320;&#24378;&#21046;&#26465;&#20214;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#24490;&#29615;&#38382;&#39064;&#12289;&#23433;&#20840;&#39118;&#38505;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#31995;&#32479;&#35780;&#20272;&#20197;&#21450;&#36947;&#24503;&#32771;&#34385;&#31561;&#25361;&#25112;&#65292;&#25552;&#20379;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20248;&#21183;&#30340;&#26041;&#24335;&#65292;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03314</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#65306;&#21457;&#25381;&#26234;&#33021; LLM &#26234;&#33021;&#20307;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents. (arXiv:2306.03314v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#24490;&#29615;&#38382;&#39064;&#12289;&#23433;&#20840;&#39118;&#38505;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#31995;&#32479;&#35780;&#20272;&#20197;&#21450;&#36947;&#24503;&#32771;&#34385;&#31561;&#25361;&#25112;&#65292;&#25552;&#20379;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20248;&#21183;&#30340;&#26041;&#24335;&#65292;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#21327;&#20316;&#29615;&#22659;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#32452;&#20214;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#23646;&#24615;&#21644;&#35282;&#33394;&#65292;&#20849;&#21516;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#26356;&#21152;&#39640;&#25928;&#26377;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;Auto-GPT &#21644;BabyAGI &#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#8220;Gorilla&#8221;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#22806;&#37096; API &#38598;&#25104;&#21040; LLM&#20013;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35299;&#20915;&#20102;&#24490;&#29615;&#38382;&#39064;&#12289;&#23433;&#20840;&#39118;&#38505;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#31995;&#32479;&#35780;&#20272;&#20197;&#21450;&#36947;&#24503;&#32771;&#34385;&#31561;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#27861;&#24237;&#27169;&#25311;&#21644;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#31561;&#19981;&#21516;&#39046;&#22495;&#30340;&#24314;&#27169;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#35758;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#30410;&#22788;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20248;&#21183;&#30340;&#36884;&#24452;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the "Gorilla" model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an aven
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26495;&#22359;&#25512;&#26029;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#20027;&#39064;&#22411;&#31169;&#21215;&#32929;&#26435;&#22522;&#37329;&#30340;&#25237;&#36164;&#19987;&#19994;&#20154;&#22763;&#25512;&#26029;&#20844;&#21496;&#25152;&#22312;&#30340;&#34892;&#19994;&#26495;&#22359;&#12290;&#35813;&#31995;&#32479;&#24314;&#31435;&#22312;&#20013;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;Prompt+&#27169;&#22411;&#24494;&#35843;&#31243;&#24207;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03313</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#31995;&#32479;&#29992;&#20110;&#25512;&#26029;&#20844;&#21496;&#30340;&#34892;&#19994;&#26495;&#22359;&#65306;&#22522;&#20110;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;Prompt+&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
A Scalable and Adaptive System to Infer the Industry Sectors of Companies: Prompt + Model Tuning of Generative Language Models. (arXiv:2306.03313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26495;&#22359;&#25512;&#26029;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#20027;&#39064;&#22411;&#31169;&#21215;&#32929;&#26435;&#22522;&#37329;&#30340;&#25237;&#36164;&#19987;&#19994;&#20154;&#22763;&#25512;&#26029;&#20844;&#21496;&#25152;&#22312;&#30340;&#34892;&#19994;&#26495;&#22359;&#12290;&#35813;&#31995;&#32479;&#24314;&#31435;&#22312;&#20013;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;Prompt+&#27169;&#22411;&#24494;&#35843;&#31243;&#24207;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#21215;&#32929;&#26435;&#20844;&#21496;&#36890;&#36807;&#25910;&#36141;&#21644;&#31649;&#29702;&#20844;&#21496;&#26469;&#23454;&#29616;&#39640;&#25910;&#30410;&#65292;&#35768;&#22810;&#31169;&#21215;&#32929;&#26435;&#22522;&#37329;&#26159;&#20027;&#39064;&#22411;&#30340;&#65292;&#24847;&#21619;&#30528;&#25237;&#36164;&#19987;&#19994;&#20154;&#22763;&#35201;&#35206;&#30422;&#23613;&#21487;&#33021;&#22810;&#30340;&#34892;&#19994;&#26495;&#22359;&#65292;&#24182;&#22312;&#36825;&#20123;&#26495;&#22359;&#20013;&#36873;&#25321;&#26377;&#21069;&#36884;&#30340;&#20844;&#21496;&#65292;&#22240;&#27492;&#25512;&#26029;&#20844;&#21496;&#30340;&#26495;&#22359;&#23545;&#20027;&#39064;&#22411;&#31169;&#21215;&#32929;&#26435;&#22522;&#37329;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26631;&#20934;&#21270;&#34892;&#19994;&#26495;&#22359;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#20856;&#22411;&#30340;&#25361;&#25112;&#65307;&#28982;&#21518;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26495;&#22359;&#25512;&#26029;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#24314;&#31435;&#22312;&#20013;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#65292;&#36890;&#36807;Prompt+&#27169;&#22411;&#24494;&#35843;&#31243;&#24207;&#36827;&#34892;&#24494;&#35843;&#12290;&#37096;&#32626;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#27604;&#24120;&#35265;&#22522;&#32447;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#35813;&#31995;&#32479;&#24050;&#32463;&#20026;&#35768;&#22810;&#31169;&#21215;&#32929;&#26435;&#19987;&#19994;&#20154;&#21592;&#26381;&#21153;&#36229;&#36807;&#19968;&#24180;&#65292;&#24182;&#26174;&#31034;&#20986;&#23545;&#25968;&#25454;&#37327;&#30340;&#33391;&#22909;&#21487;&#25193;&#23637;&#24615;&#21644;&#23545;&#34892;&#19994;&#26495;&#22359;&#21644;/&#25110;&#27880;&#37322;&#30340;&#20219;&#20309;&#21464;&#21270;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Private Equity (PE) firms operate investment funds by acquiring and managing companies to achieve a high return upon selling. Many PE funds are thematic, meaning investment professionals aim to identify trends by covering as many industry sectors as possible, and picking promising companies within these sectors. So, inferring sectors for companies is critical to the success of thematic PE funds. In this work, we standardize the sector framework and discuss the typical challenges; we then introduce our sector inference system addressing these challenges. Specifically, our system is built on a medium-sized generative language model, finetuned with a prompt + model tuning procedure. The deployed model demonstrates a superior performance than the common baselines. The system has been serving many PE professionals for over a year, showing great scalability to data volume and adaptability to any change in sector framework and/or annotation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#20195;&#29702;&#20154;&#32676;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03311</link><description>&lt;p&gt;
&#20351;&#29992;&#20195;&#29702;&#20154;&#32676;&#20307;&#23398;&#20064;&#24207;&#21015;&#20219;&#21153;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning Embeddings for Sequential Tasks Using Population of Agents. (arXiv:2306.03311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#20195;&#29702;&#20154;&#32676;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#26679;&#30340;&#24819;&#27861;&#65306;&#22914;&#26524;&#35266;&#23519;&#19968;&#20010;&#20195;&#29702;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20943;&#23569;&#20102;&#25105;&#20204;&#20851;&#20110;&#20182;&#22312;&#21478;&#19968;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#37027;&#20040;&#20004;&#20010;&#20219;&#21153;&#23601;&#30456;&#20284;&#12290;&#25105;&#20204;&#30340;&#20449;&#24687;&#29702;&#35770;&#20934;&#21017;&#25429;&#25417;&#20102;&#36825;&#31181;&#30452;&#35273;&#65292;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#20195;&#29702;&#20154;&#32676;&#20307;&#26469;&#27979;&#37327;&#24207;&#21015;&#20915;&#31574;&#29615;&#22659;&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#38500;&#20102;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#20004;&#20010;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#37327;&#21270;&#27604;&#36739;&#65292;&#22522;&#20110;&#20219;&#21153;&#23884;&#20837;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65306;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an information-theoretic framework to learn fixed-dimensional embeddings for tasks in reinforcement learning. We leverage the idea that two tasks are similar to each other if observing an agent's performance on one task reduces our uncertainty about its performance on the other. This intuition is captured by our information-theoretic criterion which uses a diverse population of agents to measure similarity between tasks in sequential decision-making settings. In addition to qualitative assessment, we empirically demonstrate the effectiveness of our techniques based on task embeddings by quantitative comparisons against strong baselines on two application scenarios: predicting an agent's performance on a test task by observing its performance on a small quiz of tasks, and selecting tasks with desired characteristics from a given set of options.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29983;&#21629;&#21608;&#26399;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;LIBERO&#12290;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#32456;&#36523;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;LLDM&#20013;&#30340;&#20116;&#20010;&#20851;&#38190;&#30740;&#31350;&#20027;&#39064;&#65292;&#24076;&#26395;&#23427;&#33021;&#22815;&#21152;&#36895;&#26500;&#24314;&#21487;&#20197;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#23398;&#20064;&#21644;&#36866;&#24212;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.03310</link><description>&lt;p&gt;
LIBERO: &#29983;&#21629;&#21608;&#26399;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#30693;&#35782;&#36716;&#31227;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning. (arXiv:2306.03310v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29983;&#21629;&#21608;&#26399;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;LIBERO&#12290;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#32456;&#36523;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;LLDM&#20013;&#30340;&#20116;&#20010;&#20851;&#38190;&#30740;&#31350;&#20027;&#39064;&#65292;&#24076;&#26395;&#23427;&#33021;&#22815;&#21152;&#36895;&#26500;&#24314;&#21487;&#20197;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#23398;&#20064;&#21644;&#36866;&#24212;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#24314;&#31435;&#36890;&#29992;&#20195;&#29702;&#30340;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#35813;&#20195;&#29702;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#23398;&#20064;&#21644;&#36866;&#24212;&#12290;&#19982;&#20256;&#32479;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#39046;&#22495;&#30340;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#19981;&#21516;&#65292;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#32456;&#36523;&#23398;&#20064;&#65288;LLDM&#65289;&#36824;&#38656;&#35201;&#20256;&#36882;&#31243;&#24207;&#21270;&#30693;&#35782;&#65292;&#20363;&#22914;&#25805;&#20316;&#21644;&#34892;&#20026;&#12290;&#20026;&#20102;&#25512;&#36827;LLDM&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LIBERO&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#32456;&#36523;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LIBERO&#24378;&#35843;&#20102;LLDM&#20013;&#30340;&#20116;&#20010;&#20851;&#38190;&#30740;&#31350;&#20027;&#39064;&#65306;1&#65289;&#22914;&#20309;&#26377;&#25928;&#22320;&#20256;&#36882;&#22768;&#26126;&#24615;&#30693;&#35782;&#12289;&#31243;&#24207;&#24615;&#30693;&#35782;&#25110;&#20108;&#32773;&#28151;&#21512;&#20307;&#65307;2&#65289;&#22914;&#20309;&#35774;&#35745;&#26377;&#25928;&#30340;&#31574;&#30053;&#26550;&#26500;&#21644;3&#65289;LLDM&#30340;&#26377;&#25928;&#31639;&#27861;&#65307;4&#65289;&#32456;&#36523;&#23398;&#20064;&#32773;&#22312;&#20219;&#21153;&#25490;&#24207;&#26041;&#38754;&#30340;&#31283;&#20581;&#24615;&#65307;5&#65289;&#27169;&#22411;&#39044;&#35757;&#32451;&#23545;LLDM&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#31243;&#24207;&#29983;&#25104;&#31649;&#36947;&#65292;&#21487;&#20197;&#21407;&#21017;&#19978;&#29983;&#25104;&#26080;&#38480;&#22810;&#30340;&#36716;&#31227;&#22330;&#26223;&#12290;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#21253;&#25324;5&#20010;&#20219;&#21153;&#26063;&#20013;&#30340;50&#20010;&#19981;&#21516;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#26063;&#37117;&#35774;&#35745;&#20197;&#27979;&#35797;&#20854;&#20013;&#19968;&#20010;&#20116;&#20010;&#30740;&#31350;&#20027;&#39064;&#65292;&#32780;&#22312;&#27599;&#20010;&#20219;&#21153;&#26063;&#20869;&#65292;&#20219;&#21153;&#30340;&#38590;&#24230;&#21644;&#26032;&#39062;&#31243;&#24230;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20219;&#21153;&#25490;&#24207;&#23545;&#32456;&#36523;&#23398;&#20064;&#32773;&#30340;&#34920;&#29616;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#27169;&#22411;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#20219;&#21153;&#25490;&#24207;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;LIBERO&#33021;&#22815;&#20316;&#20026;&#19968;&#20010;&#31038;&#21306;&#33539;&#22260;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;LLDM&#24182;&#21152;&#36895;&#26500;&#24314;&#21487;&#20197;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#23398;&#20064;&#21644;&#36866;&#24212;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowledge of entities and concepts, lifelong learning in decision-making (LLDM) also necessitates the transfer of procedural knowledge, such as actions and behaviors. To advance research in LLDM, we introduce LIBERO, a novel benchmark of lifelong learning for robot manipulation. Specifically, LIBERO highlights five key research topics in LLDM: 1) how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both; 2) how to design effective policy architectures and 3) effective algorithms for LLDM; 4) the robustness of a lifelong learner with respect to task ordering; and 5) the effect of model pretraining for LLDM. We develop an extendible procedural generation pipeline that can in principle generate infinitely many t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRS&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21360;&#35937;&#26041;&#24046;&#24863;&#30693;&#30340;&#26041;&#24335;&#23545;&#24191;&#21578;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20010;&#24615;&#21270;&#24191;&#21578;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03293</link><description>&lt;p&gt;
&#37319;&#29992;&#21360;&#35937;&#26041;&#24046;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20010;&#24615;&#21270;&#24191;&#21578;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Fairness in Personalized Ads Using Impression Variance Aware Reinforcement Learning. (arXiv:2306.03293v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRS&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21360;&#35937;&#26041;&#24046;&#24863;&#30693;&#30340;&#26041;&#24335;&#23545;&#24191;&#21578;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20010;&#24615;&#21270;&#24191;&#21578;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#24191;&#21578;&#31995;&#32479;&#20013;&#65292;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;&#24191;&#21578;&#21360;&#35937;&#32467;&#26524;&#24046;&#24322;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#35270;&#20026;&#31639;&#27861;&#20559;&#35265;&#30340;&#21487;&#33021;&#26631;&#24535;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31216;&#20026;VRS&#65288;Variance Reduction System&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;Meta&#24191;&#21578;&#31995;&#32479;&#32467;&#26524;&#65292;&#21516;&#26102;&#36890;&#36807;&#21360;&#35937;&#26041;&#24046;&#24863;&#30693;&#30340;&#26041;&#24335;&#23545;&#24191;&#21578;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variances in ad impression outcomes across demographic groups are increasingly considered to be potentially indicative of algorithmic bias in personalized ads systems. While there are many definitions of fairness that could be applicable in the context of personalized systems, we present a framework which we call the Variance Reduction System (VRS) for achieving more equitable outcomes in Meta's ads systems. VRS seeks to achieve a distribution of impressions with respect to selected protected class (PC) attributes that more closely aligns the demographics of an ad's eligible audience (a function of advertiser targeting criteria) with the audience who sees that ad, in a privacy-preserving manner. We first define metrics to quantify fairness gaps in terms of ad impression variances with respect to PC attributes including gender and estimated race. We then present the VRS for re-ranking ads in an impression variance-aware manner. We evaluate VRS via extensive simulations over different pa
&lt;/p&gt;</description></item><item><title>&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21363;&#20351;&#20351;&#29992;&#38169;&#35823;&#30340;&#22870;&#21169;&#26631;&#31614;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#65292;&#36825;&#31181;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#30001;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24754;&#35266;&#20027;&#20041;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#30340;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#65292;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03286</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29983;&#23384;&#26412;&#33021;
&lt;/p&gt;
&lt;p&gt;
Survival Instinct in Offline Reinforcement Learning. (arXiv:2306.03286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03286
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21363;&#20351;&#20351;&#29992;&#38169;&#35823;&#30340;&#22870;&#21169;&#26631;&#31614;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#65292;&#36825;&#31181;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#30001;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24754;&#35266;&#20027;&#20041;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#30340;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#65292;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26032;&#35266;&#23519;&#65306;&#22312;&#35768;&#22810;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21363;&#20351;&#20351;&#29992;&#8220;&#38169;&#35823;&#8221;&#30340;&#22870;&#21169;&#26631;&#31614;&#65288;&#20363;&#22914;&#22312;&#25152;&#26377;&#22320;&#26041;&#37117;&#20026;&#38646;&#25110;&#26159;&#30495;&#23454;&#22870;&#21169;&#30340;&#36127;&#25968;&#65289;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#12290;&#36825;&#31181;&#29616;&#35937;&#19981;&#33021;&#20165;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#26469;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#23427;&#36171;&#20104;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#22312;&#20854;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#23545;&#24212;&#29289;&#20013;&#26159;&#19981;&#20856;&#22411;&#30340;&#65292;&#22240;&#20026;&#21518;&#32773;&#23545;&#22870;&#21169;&#35774;&#35745;&#25935;&#24863;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#24778;&#20154;&#30340;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#24754;&#35266;&#20027;&#20041;&#27010;&#24565;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#26576;&#31181;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#12290;&#24754;&#35266;&#20027;&#20041;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#65292;&#21363;&#38271;&#26399;&#20869;&#30041;&#22312;&#25968;&#25454;&#25903;&#25345;&#20013;&#30340;&#28608;&#21169;&#65292;&#32780;&#26377;&#38480;&#19988;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#35206;&#30422;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#29983;&#23384;&#34892;&#20026;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with "wrong" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of pessimism in offline RL algorithms and a certain bias implicit in common data collection practices. As we prove in this work, pessimism endows the agent with a "survival instinct", i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#28040;&#36153;&#32773;&#32423;&#35745;&#31639;&#26426;&#19978;&#25968;&#31186;&#20869;&#20248;&#21270;&#26426;&#22120;&#20154;&#32467;&#26500;&#20197;&#36798;&#21040;&#25152;&#38656;&#34892;&#20026;&#65292;&#20026;&#33258;&#21160;&#21270;&#35774;&#35745;&#22797;&#26434;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03263</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#39640;&#25928;&#33258;&#21160;&#21270;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient automatic design of robots. (arXiv:2306.03263v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#39318;&#27425;&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#28040;&#36153;&#32773;&#32423;&#35745;&#31639;&#26426;&#19978;&#25968;&#31186;&#20869;&#20248;&#21270;&#26426;&#22120;&#20154;&#32467;&#26500;&#20197;&#36798;&#21040;&#25152;&#38656;&#34892;&#20026;&#65292;&#20026;&#33258;&#21160;&#21270;&#35774;&#35745;&#22797;&#26434;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#20154;&#30340;&#29289;&#29702;&#32467;&#26500;&#12289;&#24863;&#23448;&#12289;&#39532;&#36798;&#24067;&#23616;&#21644;&#34892;&#20026;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#23427;&#20204;&#30340;&#35774;&#35745;&#36890;&#24120;&#38750;&#24120;&#22256;&#38590;&#12290;20&#24180;&#26469;&#65292;&#21551;&#21457;&#20110;&#33258;&#28982;&#30028;&#30340;&#36827;&#21270;&#35774;&#35745;&#65292;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#33258;&#21160;&#35774;&#35745;&#26426;&#22120;&#20154;&#24050;&#32463;&#23581;&#35797;&#36807;&#26080;&#25968;&#27425;&#65292;&#20294;&#36825;&#20063;&#38750;&#24120;&#20302;&#25928;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#39318;&#27425;&#22312;&#21333;&#20010;&#28040;&#36153;&#32773;&#32423;&#35745;&#31639;&#26426;&#19978;&#30340;&#25968;&#31186;&#20869;&#20248;&#21270;&#26426;&#22120;&#20154;&#32467;&#26500;&#20197;&#23637;&#29616;&#25152;&#38656;&#34892;&#20026;&#65292;&#24182;&#21046;&#36896;&#20986;&#20855;&#26377;&#35813;&#34892;&#20026;&#30340;&#26426;&#22120;&#20154;&#12290;&#19982;&#20854;&#20182;&#22522;&#20110;&#26799;&#24230;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#26412;&#31639;&#27861;&#19981;&#39044;&#35774;&#20219;&#20309;&#29305;&#23450;&#30340;&#35299;&#21078;&#24418;&#24335;&#65292;&#32780;&#26159;&#20174;&#38543;&#26426;&#29983;&#25104;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#31181;&#32676;&#24320;&#22987;&#65292;&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#26469;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#33258;&#21160;&#21270;&#35774;&#35745;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#38590;&#20197;&#25163;&#21160;&#35774;&#35745;&#30340;&#22797;&#26434;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots are notoriously difficult to design because of complex interdependencies between their physical structure, sensory and motor layouts, and behavior. Despite this, almost every detail of every robot built to date has been manually determined by a human designer after several months or years of iterative ideation, prototyping, and testing. Inspired by evolutionary design in nature, the automated design of robots using evolutionary algorithms has been attempted for two decades, but it too remains inefficient: days of supercomputing are required to design robots in simulation that, when manufactured, exhibit desired behavior. Here we show for the first time de-novo optimization of a robot's structure to exhibit a desired behavior, within seconds on a single consumer-grade computer, and the manufactured robot's retention of that behavior. Unlike other gradient-based robot design methods, this algorithm does not presuppose any particular anatomical form; starting instead from a randoml
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03241</link><description>&lt;p&gt;
&#29702;&#35299;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#23545;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models. (arXiv:2306.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#20215;&#39640;&#26114;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33267;&#25910;&#25947;&#24182;&#19981;&#39640;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24819;&#27861;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27839;&#30528;&#36712;&#36857;&#36827;&#34892;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#65292;&#20197;&#22312;&#27169;&#22411;&#25910;&#25947;&#20043;&#21069;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35757;&#32451;&#25110;&#25512;&#29702;&#26399;&#38388;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;10&#20159;&#21040;120&#20159;&#21442;&#25968;&#30340;Pythia LLM&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#24182;&#35777;&#26126;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#21644;&#20013;&#26399;&#38454;&#27573;&#65292;&#36825;&#31181;&#24819;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#24182;&#25552;&#39640;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#12290;&#25439;&#22833;&#27874;&#21160;&#26159;LLM&#35757;&#32451;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65307;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#20004;&#31181;&#22522;&#30784;&#36712;&#36857;&#30340;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#24179;&#22343;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#20004;&#31181;&#24773;&#20917;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#19968;&#20010;&#25317;&#26377;69&#20159;&#21442;&#25968;&#30340;LLM&#65292;&#25105;&#20204;&#30340;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#37197;&#26041;&#21487;&#20197;&#33410;&#30465;&#39640;&#36798;4200&#23567;&#26102;&#30340;GPU&#26102;&#38388;&#65292;&#36825;&#23545;&#20113;&#35745;&#31639;&#25104;&#26412;&#26469;&#35828;&#26159;&#26174;&#33879;&#30340;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training LLMs is expensive, and recent evidence indicates training all the way to convergence is inefficient. In this paper, we investigate the ability of a simple idea, checkpoint averaging along the trajectory of a training run to improve the quality of models before they have converged. This approach incurs no extra cost during training or inference. Specifically, we analyze the training trajectories of Pythia LLMs with 1 to 12 billion parameters and demonstrate that, particularly during the early to mid stages of training, this idea accelerates convergence and improves both test and zero-shot generalization. Loss spikes are a well recognized problem in LLM training; in our analysis we encountered two instances of this in the underlying trajectories, and both instances were mitigated by our averaging.  For a 6.9B parameter LLM, for example, our early weight averaging recipe can save upto 4200 hours of GPU time, which corresponds to significant savings in cloud compute costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#20840;&#23616;&#21644;&#24773;&#22659;&#22870;&#21169;&#36827;&#34892;&#25506;&#32034;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#24773;&#22659;&#22870;&#21169;&#22312;&#20849;&#20139;&#32467;&#26500;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#26356;&#26377;&#25928;&#65292;&#32780;&#20840;&#23616;&#22870;&#21169;&#21017;&#22312;&#20849;&#20139;&#32467;&#26500;&#26356;&#22810;&#30340;&#24773;&#20917;&#19979;&#26356;&#26377;&#25928;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20849;&#20139;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.03236</link><description>&lt;p&gt;
&#20840;&#23616;&#21450;&#24773;&#22659;&#22870;&#21169;&#23545;&#20110;&#19978;&#19979;&#25991;MDPs&#20013;&#30340;&#25506;&#32034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Global and Episodic Bonuses for Exploration in Contextual MDPs. (arXiv:2306.03236v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#20840;&#23616;&#21644;&#24773;&#22659;&#22870;&#21169;&#36827;&#34892;&#25506;&#32034;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#24773;&#22659;&#22870;&#21169;&#22312;&#20849;&#20139;&#32467;&#26500;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#26356;&#26377;&#25928;&#65292;&#32780;&#20840;&#23616;&#22870;&#21169;&#21017;&#22312;&#20849;&#20139;&#32467;&#26500;&#26356;&#22810;&#30340;&#24773;&#20917;&#19979;&#26356;&#26377;&#25928;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20849;&#20139;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25506;&#32034;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#29615;&#22659;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#20840;&#23616;&#26032;&#22855;&#22870;&#21169;&#21644;&#24773;&#22659;&#26032;&#22855;&#22870;&#21169;&#30340;&#26576;&#31181;&#32452;&#21512;&#65292;&#20351;&#29992;&#20195;&#29702;&#30340;&#25972;&#20010;&#35757;&#32451;&#32463;&#39564;&#36827;&#34892;&#35745;&#31639;&#65292;&#20197;&#21450;&#20165;&#20351;&#29992;&#24403;&#21069;&#24773;&#33410;&#30340;&#32463;&#39564;&#36827;&#34892;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#22870;&#21169;&#30340;&#20351;&#29992;&#26159;&#19981;&#25104;&#20307;&#31995;&#21644;&#19981;&#29702;&#35299;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#26131;&#20110;&#35299;&#37322;&#30340;&#20219;&#21153;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20687;&#32032;&#35774;&#32622;&#36827;&#34892;&#25511;&#21046;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#36825;&#20004;&#31181;&#22870;&#21169;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20004;&#31181;&#22870;&#21169;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#25104;&#21151;&#65292;&#24773;&#22659;&#22870;&#21169;&#22312;&#24773;&#33410;&#20043;&#38388;&#20849;&#20139;&#32467;&#26500;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#26368;&#20026;&#26377;&#25928;&#65292;&#32780;&#20840;&#23616;&#22870;&#21169;&#22312;&#20849;&#20139;&#32467;&#26500;&#26356;&#22810;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#19978;&#19979;&#25991;&#20013;&#30340;&#20540;&#20989;&#25968;&#26041;&#24046;&#65292;&#20351;&#36825;&#31181;&#20849;&#20139;&#32467;&#26500;&#30340;&#27010;&#24565;&#26126;&#30830;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in environments which differ across episodes has received increasing attention in recent years. Current methods use some combination of global novelty bonuses, computed using the agent's entire training experience, and \textit{episodic novelty bonuses}, computed using only experience from the current episode. However, the use of these two types of bonuses has been ad-hoc and poorly understood. In this work, we shed light on the behavior of these two types of bonuses through controlled experiments on easily interpretable tasks as well as challenging pixel-based settings. We find that the two types of bonuses succeed in different settings, with episodic bonuses being most effective when there is little shared structure across episodes and global bonuses being effective when more structure is shared. We develop a conceptual framework which makes this notion of shared structure precise by considering the variance of the value function across contexts, and which provides a unify
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#32771;&#34385;&#20102;&#37327;&#23376;&#31639;&#27861;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#22522;&#20110;&#29366;&#24577;&#21472;&#21152;&#12289;&#37327;&#23376;&#32416;&#32544;&#21644;&#24178;&#28041;&#30340;QAG&#23384;&#20648;&#20449;&#24687;&#65292;&#26368;&#23567;&#21270;&#32463;&#20856;&#21644;&#37327;&#23376;&#20449;&#24687;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#23558;&#27492;&#20316;&#20026;QA&#35745;&#31639;&#26234;&#33021;&#27979;&#37327;&#30340;&#32456;&#27490;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/2306.03233</link><description>&lt;p&gt;
&#37327;&#23376;&#20915;&#31574;&#21644;&#25628;&#32034;&#31639;&#27861;&#30340;&#32479;&#19968;&#20449;&#24687;&#21160;&#24577;&#20998;&#26512;&#65306;&#35745;&#31639;&#26234;&#33021;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unified Information Dynamic Analysis of Quantum Decision-Making and Search Algorithms: Computational Intelligence Measure. (arXiv:2306.03233v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#32771;&#34385;&#20102;&#37327;&#23376;&#31639;&#27861;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#22522;&#20110;&#29366;&#24577;&#21472;&#21152;&#12289;&#37327;&#23376;&#32416;&#32544;&#21644;&#24178;&#28041;&#30340;QAG&#23384;&#20648;&#20449;&#24687;&#65292;&#26368;&#23567;&#21270;&#32463;&#20856;&#21644;&#37327;&#23376;&#20449;&#24687;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#23558;&#27492;&#20316;&#20026;QA&#35745;&#31639;&#26234;&#33021;&#27979;&#37327;&#30340;&#32456;&#27490;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#37325;&#35201;&#30340;&#31639;&#27861;&#26159;&#22522;&#20110;&#28151;&#21512;&#20351;&#29992;&#30340;&#22522;&#26412;&#25216;&#24039;&#26500;&#24314;&#30340;&#65307;&#20363;&#22914;&#65292;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#20351;&#29992;&#20998;&#27835;&#21644;&#21464;&#25442;&#25216;&#24039;&#12290;&#26412;&#25991;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#32771;&#34385;&#20102;&#37327;&#23376;&#31639;&#27861;&#65288;QA&#65289;&#30340;&#28436;&#21464;&#12290;&#23558;&#36827;&#20837;&#37327;&#23376;&#31639;&#27861;&#38376;(QAG)&#30340;&#22797;&#21521;&#37327;&#20316;&#20026;&#19968;&#20010;&#20449;&#24687;&#28304;&#65292;&#21516;&#26102;&#20174;&#32463;&#20856;&#21644;&#37327;&#23376;&#23618;&#38754;&#36827;&#34892;&#32771;&#34385;&#12290;&#20351;&#29992;&#24503;&#27779;&#26031;-&#20052;&#25166;&#65292;&#32918;&#23572;&#21644;&#26684;&#32599;&#24343;&#31639;&#27861;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#20449;&#24687;&#27969;&#20998;&#26512;&#12290;&#36890;&#36807;&#22522;&#20110;&#29366;&#24577;&#21472;&#21152;&#12289;&#37327;&#23376;&#32416;&#32544;&#21644;&#24178;&#28041;&#30340;QAG&#22312;&#20316;&#29992;&#20110;&#36755;&#20837;&#21521;&#37327;&#26102;&#65292;&#23558;&#20449;&#24687;&#23384;&#20648;&#21040;&#31995;&#32479;&#29366;&#24577;&#20013;&#65292;&#24182;&#26368;&#23567;&#21270;&#32463;&#20856;&#39321;&#20892;&#29109;&#21644;&#37327;&#23376;&#20911;&#183;&#35834;&#20234;&#26364;&#29109;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23558;&#26368;&#23567;&#21270;&#39321;&#20892;&#21644;&#20911;&#183;&#35834;&#20234;&#26364;&#29109;&#20043;&#38388;&#30340;&#24046;&#36317;&#35270;&#20026;QA&#35745;&#31639;&#26234;&#33021;&#27979;&#37327;&#30340;&#32456;&#27490;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are important algorithms built upon a mixture of basic techniques described; for example, the Fast Fourier Transform (FFT) employs both Divide-and-Conquer and Transform-and-Conquer techniques. In this article, the evolution of a quantum algorithm (QA) is examined from an information theory viewpoint. The complex vector entering the quantum algorithmic gate - QAG is considered as an information source both from the classical and the quantum level. The analysis of the classical and quantum information flow in Deutsch-Jozsa, Shor and Grover algorithms is used. It is shown that QAG, based on superposition of states, quantum entanglement and interference, when acting on the input vector, stores information into the system state, minimizing the gap between classical Shannon entropy and quantum von Neumann entropy. Minimizing of the gap between Shannon and von Neumann entropies is considered as a termination criterion of QA computational intelligence measure.
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#25915;&#20987;&#24050;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24369;&#28857;, &#32780;&#23545;&#25239;&#23545;&#40784;&#26159;&#19968;&#31181;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#32771;&#34385;&#26356;&#22810;&#12290;</title><link>http://arxiv.org/abs/2306.03229</link><description>&lt;p&gt;
&#23545;&#25239;&#23545;&#40784;: &#25171;&#30772;&#25915;&#20987;&#24378;&#24230;&#19982;&#20854;&#23545;&#20154;&#31867;&#24863;&#30693;&#24433;&#21709;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Adversarial alignment: Breaking the trade-off between the strength of an attack and its relevance to human perception. (arXiv:2306.03229v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03229
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#24050;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24369;&#28857;, &#32780;&#23545;&#25239;&#23545;&#40784;&#26159;&#19968;&#31181;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#32771;&#34385;&#26356;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs), &#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#25935;&#24863;&#12290;&#36825;&#20123;&#25915;&#20987;&#20250;&#23545;&#36755;&#20837;&#36827;&#34892;&#24494;&#23567;&#30340;&#25200;&#21160;&#65292;&#20294;&#36275;&#20197;&#25913;&#21464;&#27169;&#22411;&#30340;&#35270;&#35273;&#20915;&#31574;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;DNNs&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#38543;&#30528;&#20854;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#24615;&#25913;&#21892;&#32780;&#21457;&#23637;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#25239;&#23545;&#40784;&#26159;DNNs&#30340;&#26032;&#30340;&#22522;&#26412;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#20854;&#40065;&#26834;&#24615;&#26102;&#24212;&#21152;&#20197;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are known to have a fundamental sensitivity to adversarial attacks, perturbations of the input that are imperceptible to humans yet powerful enough to change the visual decision of a model. Adversarial attacks have long been considered the "Achilles' heel" of deep learning, which may eventually force a shift in modeling paradigms. Nevertheless, the formidable capabilities of modern large-scale DNNs have somewhat eclipsed these early concerns. Do adversarial attacks continue to pose a threat to DNNs?  Here, we investigate how the robustness of DNNs to adversarial attacks has evolved as their accuracy on ImageNet has continued to improve. We measure adversarial robustness in two different ways: First, we measure the smallest adversarial attack needed to cause a model to change its object categorization decision. Second, we measure how aligned successful attacks are with the features that humans find diagnostic for object recognition. We find that adversarial a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#37325;&#21152;&#26435;&#65288;StruRW&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;GDA&#65289;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#26465;&#20214;&#32467;&#26500;&#20559;&#31227;&#65288;CSS&#65289;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.03221</link><description>&lt;p&gt;
&#32467;&#26500;&#37325;&#21152;&#26435;&#25913;&#21892;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Structural Re-weighting Improves Graph Domain Adaptation. (arXiv:2306.03221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#37325;&#21152;&#26435;&#65288;StruRW&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;GDA&#65289;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#26465;&#20214;&#32467;&#26500;&#20559;&#31227;&#65288;CSS&#65289;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#65292;&#20363;&#22914;&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;&#30340;&#27169;&#25311;&#25968;&#25454;&#21487;&#33021;&#19982;&#23454;&#39564;&#19981;&#21305;&#37197;&#12290;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;GDA&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#24046;&#24322;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;GDA&#20027;&#35201;&#36890;&#36807;&#23545;&#21333;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#36755;&#20986;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#26469;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#20250;&#20135;&#29983;&#27425;&#20248;&#35299;&#12290;&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#30001;&#22270;&#32467;&#26500;&#25110;&#33410;&#28857;&#23646;&#24615;&#24341;&#36215;&#30340;&#19981;&#21516;&#20998;&#24067;&#20559;&#31227;&#30340;&#19981;&#21516;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#32467;&#26500;&#20559;&#31227;&#65288;CSS&#65289;&#30340;&#26032;&#31867;&#22411;&#20559;&#31227;&#65292;&#35777;&#26126;&#20102;&#24403;&#21069;&#30340;GDA&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32467;&#26500;&#37325;&#21152;&#26435;&#65288;StruRW&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#21512;&#25104;&#22270;&#12289;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26032;&#30340;&#39640;&#33021;&#29289;&#29702;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;StruRW&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world applications, graph-structured data used for training and testing have differences in distribution, such as in high energy physics (HEP) where simulation data used for training may not match real experiments. Graph domain adaptation (GDA) is a method used to address these differences. However, current GDA primarily works by aligning the distributions of node representations output by a single graph neural network encoder shared across the training and testing domains, which may often yield sub-optimal solutions. This work examines different impacts of distribution shifts caused by either graph structure or node attributes and identifies a new type of shift, named conditional structure shift (CSS), which current GDA approaches are provably sub-optimal to deal with. A novel approach, called structural reweighting (StruRW), is proposed to address this issue and is tested on synthetic graphs, four benchmark datasets, and a new application in HEP. StruRW has shown signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#20013;RL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#21152;&#39118;&#38505;&#24863;&#30693;&#30340;&#22870;&#21169;&#24418;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#35757;&#32451;&#21644;&#27979;&#35797;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#37325;&#22609;&#22870;&#21169;&#39033;&#26469;&#40723;&#21169;&#25506;&#32034;&#24182;&#24809;&#32602;&#39118;&#38505;&#39550;&#39542;&#34892;&#20026;&#65292;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;RL&#20195;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.03220</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#21160;&#39550;&#39542;&#30340;&#39118;&#38505;&#24863;&#30693;&#22870;&#21169;&#24418;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Risk-Aware Reward Shaping of Reinforcement Learning Agents for Autonomous Driving. (arXiv:2306.03220v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#20013;RL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#21152;&#39118;&#38505;&#24863;&#30693;&#30340;&#22870;&#21169;&#24418;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#35757;&#32451;&#21644;&#27979;&#35797;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#37325;&#22609;&#22870;&#21169;&#39033;&#26469;&#40723;&#21169;&#25506;&#32034;&#24182;&#24809;&#32602;&#39118;&#38505;&#39550;&#39542;&#34892;&#20026;&#65292;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;RL&#20195;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#26377;&#25928;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#25968;&#25454;&#33258;&#21160;&#23398;&#20064;&#26368;&#20248;&#39550;&#39542;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;RL&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20854;&#23545;&#20110;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#22823;&#65292;&#20294;&#26159;&#24456;&#38590;&#30830;&#23450;&#12290;&#20256;&#32479;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23433;&#20840;&#39550;&#39542;&#29366;&#24577;&#30340;&#22870;&#21169;&#65292;&#20294;&#24182;&#26410;&#32435;&#20837;&#36710;&#36742;&#39118;&#38505;&#39550;&#39542;&#34892;&#20026;&#30340;&#24863;&#30693;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#39118;&#38505;&#24863;&#30693;&#30340;&#22870;&#21169;&#24418;&#25104;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#20013;RL&#20195;&#29702;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#24615;&#33021;&#12290;&#26681;&#25454;&#23454;&#36341;&#20013;&#35268;&#23450;&#30340;&#19968;&#33324;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#37325;&#22609;&#22870;&#21169;&#39033;&#65292;&#20197;&#40723;&#21169;&#25506;&#32034;&#24182;&#24809;&#32602;&#39118;&#38505;&#39550;&#39542;&#34892;&#20026;&#12290; OpenAI Gym&#20013;&#30340;&#27169;&#25311;&#30740;&#31350;&#34920;&#26126;&#20102;&#39118;&#38505;&#24863;&#30693;&#22870;&#21169;&#24418;&#25104;&#22312;&#21508;&#31181;RL&#20195;&#29702;&#20013;&#30340;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25351;&#20986;&#20195;&#29702;&#36716;&#31227;&#30340;&#26041;&#24335;&#23545;&#39118;&#38505;&#24863;&#30693;&#22870;&#21169;&#24418;&#25104;&#24433;&#21709;&#30340;&#29616;&#23454;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is an effective approach to motion planning in autonomous driving, where an optimal driving policy can be automatically learned using the interaction data with the environment. Nevertheless, the reward function for an RL agent, which is significant to its performance, is challenging to be determined. The conventional work mainly focuses on rewarding safe driving states but does not incorporate the awareness of risky driving behaviors of the vehicles. In this paper, we investigate how to use risk-aware reward shaping to leverage the training and test performance of RL agents in autonomous driving. Based on the essential requirements that prescribe the safety specifications for general autonomous driving in practice, we propose additional reshaped reward terms that encourage exploration and penalize risky driving behaviors. A simulation study in OpenAI Gym indicates the advantage of risk-aware reward shaping for various RL agents. Also, we point out that proxi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#25968;&#25454;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#26399;&#23545;&#19981;&#37325;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#25171;&#20998;&#21644;&#25243;&#24323;&#65292;&#20943;&#23569;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#65292;&#24182;&#19988;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#21644;&#22235;&#20010;&#32852;&#21512;NLU&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.03208</link><description>&lt;p&gt;
&#25968;&#25454;&#39278;&#39135;&#19979;&#30340;NLU&#65306;NLP&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks. (arXiv:2306.03208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#25968;&#25454;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#26399;&#23545;&#19981;&#37325;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#25171;&#20998;&#21644;&#25243;&#24323;&#65292;&#20943;&#23569;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#65292;&#24182;&#19988;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#21644;&#22235;&#20010;&#32852;&#21512;NLU&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20250;&#22686;&#21152;NLU&#24212;&#29992;&#30340;&#25104;&#26412;&#65292;&#24182;&#20173;&#26159;&#24320;&#21457;&#21608;&#26399;&#30340;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#20462;&#21098;&#26469;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#37319;&#29992;&#38745;&#24577;&#26041;&#27861;&#36827;&#34892;&#20462;&#21098;&#25968;&#25454;&#36873;&#25321;&#26159;&#22522;&#20110;&#24494;&#35843;&#20043;&#21069;&#20026;&#27599;&#20010;&#35757;&#32451;&#26679;&#20363;&#35745;&#31639;&#30340;&#24471;&#20998;&#65292;&#36825;&#28041;&#21450;&#37325;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#35813;&#24471;&#20998;&#21487;&#33021;&#24182;&#19981;&#20195;&#34920;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#26679;&#20363;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31934;&#32454;&#29256;&#26412;&#30340;&#21160;&#24577;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23450;&#26399;&#23545;&#19981;&#37325;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#25171;&#20998;&#21644;&#25243;&#24323;&#30340;&#35838;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#21040;&#32852;&#21512;&#24847;&#22270;&#21644;&#27133;&#20998;&#31867;&#20219;&#21153;&#30340;EL2N&#24230;&#37327;&#21644;&#23545;&#23436;&#25972;&#35757;&#32451;&#38598;&#36827;&#34892;&#30340;&#21021;&#22987;&#24494;&#35843;&#38454;&#27573;&#12290;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#21644;&#22235;&#20010;&#32852;&#21512;NLU&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#38745;&#24577;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;50&#65285;&#26102;&#20445;&#25345;&#23436;&#20840;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning large language models inflates the costs of NLU applications and remains the bottleneck of development cycles. Recent works in computer vision use data pruning to reduce training time. Pruned data selection with static methods is based on a score calculated for each training example prior to finetuning, which involves important computational overhead. Moreover, the score may not necessarily be representative of sample importance throughout the entire training duration. We propose to address these issues with a refined version of dynamic data pruning, a curriculum which periodically scores and discards unimportant examples during finetuning. Our method leverages an EL2N metric that we extend to the joint intent and slot classification task, and an initial finetuning phase on the full train set. Our results on the GLUE benchmark and four joint NLU datasets show a better time-accuracy trade-off compared to static methods. Our method preserves full accuracy while training on 50%
&lt;/p&gt;</description></item><item><title>&#22812;Pulse&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#22812;&#38388;&#20809;&#65288;NTL&#65289;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#20998;&#21106;&#12289;&#32858;&#31867;&#21644;&#21464;&#21270;&#27169;&#24335;&#26816;&#27979;&#26469;&#35782;&#21035;&#22478;&#24066;&#21457;&#23637;&#21644;&#25193;&#23637;&#27169;&#24335;&#65292;&#24182;&#22238;&#31572;&#26377;&#20851;&#20154;&#21475;&#22240;&#32032;&#12289;&#22478;&#24066;&#36793;&#30028;&#21644;&#24322;&#24120;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03195</link><description>&lt;p&gt;
&#22812;&#31354;&#20013;&#30340;Lumos&#65306;&#29992;&#20110;&#25506;&#32034;&#22812;&#38388;&#20809;&#27169;&#24335;&#30340;AI&#21487;&#35270;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Lumos in the Night Sky: AI-enabled Visual Tool for Exploring Night-Time Light Patterns. (arXiv:2306.03195v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03195
&lt;/p&gt;
&lt;p&gt;
&#22812;Pulse&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#22812;&#38388;&#20809;&#65288;NTL&#65289;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#20998;&#21106;&#12289;&#32858;&#31867;&#21644;&#21464;&#21270;&#27169;&#24335;&#26816;&#27979;&#26469;&#35782;&#21035;&#22478;&#24066;&#21457;&#23637;&#21644;&#25193;&#23637;&#27169;&#24335;&#65292;&#24182;&#22238;&#31572;&#26377;&#20851;&#20154;&#21475;&#22240;&#32032;&#12289;&#22478;&#24066;&#36793;&#30028;&#21644;&#24322;&#24120;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;NightPulse&#65292;&#36825;&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#29992;&#20110;&#22812;&#38388;&#20809;&#65288;NTL&#65289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#20351;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;&#24179;&#21488;&#25506;&#32034;&#21644;&#20998;&#26512;NTL&#25968;&#25454;&#12290;&#30001;&#39640;&#25928;&#30340;&#31995;&#32479;&#26550;&#26500;&#39537;&#21160;&#65292;NightPulse&#25903;&#25345;&#22270;&#20687;&#20998;&#21106;&#12289;&#32858;&#31867;&#21644;&#21464;&#21270;&#27169;&#24335;&#26816;&#27979;&#65292;&#20197;&#35782;&#21035;&#22478;&#24066;&#21457;&#23637;&#21644;&#25193;&#23637;&#27169;&#24335;&#12290;&#23427;&#25429;&#25417;NTL&#30340;&#26102;&#38388;&#36235;&#21183;&#21644;&#22478;&#24066;&#30340;&#35821;&#20041;&#65292;&#22238;&#31572;&#20851;&#20110;&#20154;&#21475;&#22240;&#32032;&#12289;&#22478;&#24066;&#36793;&#30028;&#21644;&#24322;&#24120;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce NightPulse, an interactive tool for Night-time light (NTL) data visualization and analytics, which enables researchers and stakeholders to explore and analyze NTL data with a user-friendly platform. Powered by efficient system architecture, NightPulse supports image segmentation, clustering, and change pattern detection to identify urban development and sprawl patterns. It captures temporal trends of NTL and semantics of cities, answering questions about demographic factors, city boundaries, and unusual differences.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#30828;&#24065;&#32763;&#36716;&#26469;&#25512;&#23548;&#29366;&#24577;&#30340;&#35775;&#38382;&#35745;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22870;&#21169;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.03186</link><description>&lt;p&gt;
&#32763;&#36716;&#30828;&#24065;&#26469;&#20272;&#35745;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#30340;&#20266;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement Learning. (arXiv:2306.03186v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03186
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#30828;&#24065;&#32763;&#36716;&#26469;&#25512;&#23548;&#29366;&#24577;&#30340;&#35775;&#38382;&#35745;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22870;&#21169;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#22522;&#20110;&#35745;&#25968;&#30340;&#25506;&#32034;&#26032;&#26041;&#27861;&#12290;&#19982;&#20381;&#36182;&#23494;&#24230;&#27169;&#22411;&#30340;&#20197;&#24448;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35745;&#25968;&#21487;&#20197;&#36890;&#36807;&#20174;Rademacher&#20998;&#24067;&#65288;&#25110;&#30828;&#24065;&#32763;&#36716;&#65289;&#20013;&#24179;&#22343;&#26679;&#26412;&#24471;&#21040;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#32622;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#24403;&#20248;&#21270;&#26102;&#65292;&#20250;&#20135;&#29983;&#19968;&#20010;&#29366;&#24577;&#30340;&#35775;&#38382;&#35745;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#33021;&#26377;&#25928;&#22320;&#25512;&#23548;&#20986;&#30495;&#27491;&#30340;&#35775;&#38382;&#35745;&#25968;&#12290;&#24403;&#20316;&#20026;&#27169;&#22411;&#26080;&#20851;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25506;&#32034;&#22870;&#21169;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;Atari&#28216;&#25103;Montezuma's Revenge&#22312;&#20869;&#30340;9&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25506;&#32034;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method for count-based exploration in high-dimensional state spaces. Unlike previous work which relies on density models, we show that counts can be derived by averaging samples from the Rademacher distribution (or coin flips). This insight is used to set up a simple supervised learning objective which, when optimized, yields a state's visitation count. We show that our method is significantly more effective at deducing ground-truth visitation counts than previous work; when used as an exploration bonus for a model-free reinforcement learning algorithm, it outperforms existing approaches on most of 9 challenging exploration tasks, including the Atari game Montezuma's Revenge.
&lt;/p&gt;</description></item><item><title>LatFormer&#26159;&#19968;&#31181;&#23558;&#26684;&#28857;&#23545;&#31216;&#20808;&#39564;&#34701;&#20837;&#21040;&#27880;&#24847;&#21147;&#25513;&#30721;&#20013;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#29992;&#21367;&#31215;&#32593;&#32476;&#29983;&#25104;&#36719;&#25513;&#30721;&#26469;&#35843;&#25972;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#20960;&#20309;&#25512;&#29702;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03175</link><description>&lt;p&gt;
&#22522;&#20110;&#26684;&#28857;&#23545;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#20808;&#39564;&#21152;&#20837;&#65292;&#20197;&#25552;&#39640;&#25277;&#35937;&#20960;&#20309;&#25512;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning. (arXiv:2306.03175v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03175
&lt;/p&gt;
&lt;p&gt;
LatFormer&#26159;&#19968;&#31181;&#23558;&#26684;&#28857;&#23545;&#31216;&#20808;&#39564;&#34701;&#20837;&#21040;&#27880;&#24847;&#21147;&#25513;&#30721;&#20013;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#29992;&#21367;&#31215;&#32593;&#32476;&#29983;&#25104;&#36719;&#25513;&#30721;&#26469;&#35843;&#25972;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#20960;&#20309;&#25512;&#29702;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#21450;&#20854;&#26368;&#36817;&#30340;&#35821;&#35328;&#23436;&#25972;&#23454;&#20363;&#65288;LARC&#65289;&#34987;&#35748;&#20026;&#26159;&#36890;&#24448;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#20063;&#38590;&#20197;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#24615;&#33021;&#65292;&#33853;&#21518;&#20110;&#38750;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26497;&#31471;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21482;&#26377;&#36890;&#36807;&#36866;&#24403;&#32771;&#34385;&#26680;&#24515;&#30693;&#35782;&#20808;&#39564;&#25165;&#33021;&#23454;&#29616;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#20960;&#20309;&#20808;&#39564;&#65292;&#24182;&#24341;&#20837;LatFormer&#27169;&#22411;&#65292;&#23558;&#26684;&#28857;&#23545;&#31216;&#20808;&#39564;&#34701;&#20837;&#21040;&#27880;&#24847;&#21147;&#25513;&#30721;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#36229;&#31435;&#26041;&#26684;&#30340;&#20219;&#20309;&#21464;&#25442;&#65292;&#37117;&#23384;&#22312;&#19968;&#20010;&#20108;&#20540;&#27880;&#24847;&#21147;&#25513;&#30721;&#26469;&#23454;&#29616;&#35813;&#32676;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#28608;&#21457;&#20102;&#23545;&#26631;&#20934;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20462;&#25913;&#65292;&#20854;&#20013;&#20351;&#29992;&#21367;&#31215;&#32593;&#32476;&#29983;&#25104;&#30340;&#36719;&#25513;&#30721;&#26469;&#35843;&#25972;&#20851;&#27880;&#26435;&#37325;&#12290;&#22312;&#21512;&#25104;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LatFormer
&lt;/p&gt;
&lt;p&gt;
The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks. We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action. Hence, our study motivates a modification to the standard attention mechanism, where attention weights are scaled using soft masks generated by a convolutional network. Experiments on synthetic geometric reasoning show that LatFormer 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#37327;&#23376;&#36864;&#28779;&#25216;&#26415;&#26469;&#35299;&#20915;&#24358;&#29702;&#35770;&#20013;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#20197;&#24448;&#26410;&#33021;&#21457;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#24358;&#22330;&#35770;&#30340;&#28508;&#22312;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03147</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#30340;&#24037;&#20855;&#35299;&#30721;&#33258;&#28982;&#65306;&#36951;&#20256;&#31639;&#27861;&#21644;&#37327;&#23376;&#36864;&#28779;&#32467;&#21512;&#30340;&#24322;&#26500;&#32447;&#19995;&#27169;&#22411;&#22312;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Decoding Nature with Nature's Tools: Heterotic Line Bundle Models of Particle Physics with Genetic Algorithms and Quantum Annealing. (arXiv:2306.03147v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#37327;&#23376;&#36864;&#28779;&#25216;&#26415;&#26469;&#35299;&#20915;&#24358;&#29702;&#35770;&#20013;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#20197;&#24448;&#26410;&#33021;&#21457;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#24358;&#22330;&#35770;&#30340;&#28508;&#22312;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24358;&#29702;&#35770;&#26223;&#35937;&#21253;&#25324;&#22810;&#20010;&#26631;&#20934;&#27169;&#22411;&#30340;&#32043;&#22806;&#23884;&#20837;&#65292;&#20294;&#30001;&#20110;&#21487;&#29992;&#30340;&#24358;&#32039;&#33268;&#21270;&#25968;&#37327;&#24040;&#22823;&#65292;&#22240;&#27492;&#35782;&#21035;&#36825;&#20123;&#23884;&#20837;&#21464;&#24471;&#22256;&#38590;&#12290;&#36951;&#20256;&#31639;&#27861;&#65288;GAs&#65289;&#20195;&#34920;&#20102;&#19968;&#31867;&#24378;&#22823;&#30340;&#31163;&#25955;&#20248;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#24358;&#26223;&#35937;&#30340;&#24191;&#34980;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#19982;&#37327;&#23376;&#36864;&#28779;&#30340;&#36755;&#20837;&#32467;&#21512;&#26102;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#24322;&#26500;$E_8\times E_8$&#24358;&#35770;&#22312;&#20809;&#28369;Calabi-Yau&#19977;&#21472;&#19978;&#30340;&#20960;&#20309;&#32039;&#33268;&#21270;&#19982;&#38463;&#36125;&#23572;&#19995;&#12290;&#25105;&#20204;&#21033;&#29992;&#19995;&#20540;&#19978;&#21516;&#35843;&#30340;&#35299;&#26512;&#20844;&#24335;&#65292;&#20197;&#28385;&#36275;&#25972;&#20010;&#35889;&#35201;&#27714;&#30340;&#25152;&#26377;&#33539;&#22260;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#19981;&#21487;&#33021;&#30340;&#12290;&#23545;&#20110;&#20855;&#26377;&#30456;&#23545;&#36739;&#23569;Kahler&#21442;&#25968;&#30340;&#27969;&#24418;&#65292;&#25105;&#20204;&#23558;GA&#25628;&#32034;&#32467;&#26524;&#19982;&#20197;&#21069;&#31995;&#32479;&#24615;&#25195;&#25551;&#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;&#26174;&#31034;&#20986;GA&#21487;&#20197;&#25214;&#21040;&#20960;&#20046;&#25152;&#26377;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#21482;&#35775;&#38382;&#20102;&#24494;&#23567;&#30340;&#19968;&#37096;&#20998;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GAs&#22914;&#20309;&#33021;&#22815;&#24456;&#22909;&#22320;&#19982;&#37327;&#23376;&#36864;&#28779;&#32467;&#21512;&#65292;&#20197;&#22788;&#29702;&#39640;&#32500;&#31354;&#38388;&#21644;&#39640;&#24230;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#20248;&#21270;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#25214;&#21040;&#23578;&#26410;&#20174;&#31995;&#32479;&#25195;&#25551;&#20013;&#21457;&#29616;&#30340;&#26032;&#35299;&#20013;&#30340;&#22256;&#38590;&#23454;&#20363;&#65292;&#36825;&#20351;&#24471;GAs&#25104;&#20026;&#22823;&#35268;&#27169;&#22825;&#28982;&#25195;&#25551;&#20013;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The string theory landscape may include a multitude of ultraviolet embeddings of the Standard Model, but identifying these has proven difficult due to the enormous number of available string compactifications. Genetic Algorithms (GAs) represent a powerful class of discrete optimisation techniques that can efficiently deal with the immensity of the string landscape, especially when enhanced with input from quantum annealers. In this letter we focus on geometric compactifications of the $E_8\times E_8$ heterotic string theory compactified on smooth Calabi-Yau threefolds with Abelian bundles. We make use of analytic formulae for bundle-valued cohomology to impose the entire range of spectrum requirements, something that has not been possible so far. For manifolds with a relatively low number of Kahler parameters we compare the GA search results with results from previous systematic scans, showing that GAs can find nearly all the viable solutions while visiting only a tiny fraction of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03116</link><description>&lt;p&gt;
&#36890;&#36807;&#36716;&#21270;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds. (arXiv:2306.03116v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20174;&#20247;&#21253;&#26381;&#21153;&#20013;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;&#27599;&#20010;&#27880;&#37322;&#32773;&#37117;&#23436;&#25104;&#33258;&#24049;&#30340;&#23567;&#37096;&#20998;&#27880;&#37322;&#65292;&#19981;&#21516;&#27880;&#37322;&#32773;&#30340;&#26631;&#27880;&#38169;&#35823;&#24448;&#24448;&#19981;&#21516;&#12290;&#36890;&#36807;&#26631;&#31614;&#22122;&#22768;&#30340;&#36716;&#31227;&#30697;&#38453;&#26469;&#24314;&#27169;&#22122;&#22768;&#20135;&#29983;&#36807;&#31243;&#26159;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#30340;&#19968;&#31181;&#26377;&#25928;&#24037;&#20855;&#12290;&#22312;&#23454;&#38469;&#20247;&#21253;&#27169;&#22411;&#20013;&#65292;&#36716;&#31227;&#30697;&#38453;&#26082;&#30001;&#27880;&#37322;&#32773;&#20381;&#36182;&#65292;&#20063;&#30001;&#23454;&#20363;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#37322;&#32773;&#21644;&#23454;&#20363;&#20381;&#36182;&#30340;&#36716;&#31227;&#30697;&#38453;(AIDTM)&#20855;&#26377;&#39640;&#22797;&#26434;&#24230;&#65292;&#32780;&#23454;&#38469;&#27880;&#37322;&#24448;&#24448;&#28041;&#21450;&#27880;&#37322;&#31232;&#30095;&#24615;&#65292;&#36825;&#20351;&#24471;&#24314;&#31435;AIDTM&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26082;&#35201;&#20445;&#25345;&#24314;&#27169;&#30340;&#24191;&#27867;&#24615;&#65292;&#21448;&#33021;&#26356;&#30495;&#23454;&#22320;&#35299;&#20915;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#31639;AIDTM&#21644;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from crowds describes that the annotations of training data are obtained with crowd-sourcing services. Multiple annotators each complete their own small part of the annotations, where labeling mistakes that depend on annotators occur frequently. Modeling the label-noise generation process by the noise transition matrix is a power tool to tackle the label noise. In real-world crowd-sourcing scenarios, noise transition matrices are both annotator- and instance-dependent. However, due to the high complexity of annotator- and instance-dependent transition matrices (AIDTM), \textit{annotation sparsity}, which means each annotator only labels a little part of instances, makes modeling AIDTM very challenging. Prior works simplify the problem by assuming the transition matrix is instance-independent or using simple parametric way, while lose modeling generality. Motivated by this, we target a more realistic problem, estimating general AIDTM in practice. Without losing modeling general
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#22810;&#23398;&#31185;&#12289;&#22810;&#20256;&#24863;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#20056;&#23458;&#30340;&#27963;&#21160;&#65292;&#24182;&#22312;&#26368;&#36817;&#23454;&#38469;&#26465;&#20214;&#19979;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#26469;&#21019;&#24314;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.03115</link><description>&lt;p&gt;
AutoExp: &#19968;&#31181;&#22810;&#23398;&#31185;&#12289;&#22810;&#20256;&#24863;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#30340;&#20154;&#31867;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
AutoExp: A multidisciplinary, multi-sensor framework to evaluate human activities in self-driving cars. (arXiv:2306.03115v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#22810;&#23398;&#31185;&#12289;&#22810;&#20256;&#24863;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#20056;&#23458;&#30340;&#27963;&#21160;&#65292;&#24182;&#22312;&#26368;&#36817;&#23454;&#38469;&#26465;&#20214;&#19979;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#26469;&#21019;&#24314;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#26222;&#21450;&#23558;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#20840;&#33258;&#20027;&#21270;&#21487;&#33021;&#27604;&#26368;&#21021;&#39044;&#35745;&#30340;&#38656;&#35201;&#26356;&#38271;&#26102;&#38388;&#12290;&#30446;&#21069;&#65292;&#35813;&#25216;&#26415;&#30340;&#31532;&#19968;&#25209;&#36710;&#36742;&#24050;&#32463;&#20986;&#29616;&#22312;&#19990;&#30028;&#26576;&#20123;&#22478;&#24066;&#20013;&#65292;&#20316;&#20026;&#23454;&#39564;&#24615;&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#26381;&#21153;&#30340;&#19968;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#24615;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#23398;&#31185;&#26041;&#27861;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#20154;&#25991;&#31038;&#20250;&#31185;&#23398;&#30456;&#32467;&#21512;&#65289;&#65292;&#29305;&#21035;&#26159;&#38750;&#39550;&#39542;&#30456;&#20851;&#27963;&#21160;&#65292;&#26469;&#30740;&#31350;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20056;&#23458;&#30340;&#27963;&#21160;&#12290;&#35813;&#26694;&#26550;&#30001;&#23454;&#39564;&#22330;&#26223;&#21644;&#25968;&#25454;&#37319;&#38598;&#27169;&#22359;&#32452;&#25104;&#65292;&#26088;&#22312;&#39318;&#20808;&#22312;&#26368;&#36817;&#21487;&#33021;&#30340;&#23454;&#38469;&#26465;&#20214;&#19979;&#25429;&#33719;&#26377;&#20851;&#36710;&#36742;&#20351;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#26085;&#24535;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of self-driving cars will certainly revolutionize our lives, even though they may take more time to become fully autonomous than initially predicted. The first vehicles are already present in certain cities of the world, as part of experimental robot-taxi services. However, most existing studies focus on the navigation part of such vehicles. We currently miss methods, datasets, and studies to assess the in-cabin human component of the adoption of such technology in real-world conditions. This paper proposes an experimental framework to study the activities of occupants of self-driving cars using a multidisciplinary approach (computer vision associated with human and social sciences), particularly non-driving related activities. The framework is composed of an experimentation scenario, and a data acquisition module. We seek firstly to capture real-world data about the usage of the vehicle in the nearest possible, real-world conditions, and secondly to create a dataset conta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#31070;&#32463;&#29983;&#29702;&#20449;&#21495;&#26469;&#35299;&#20915;&#20844;&#20849;&#24773;&#24863;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#29983;&#25104;&#27169;&#22411;&#22312;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#24212;&#29992;&#30340;&#20248;&#21183;&#12289;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#28145;&#20837;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.03112</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#24773;&#24863;&#31070;&#32463;&#29983;&#29702;&#20449;&#21495;&#30340;&#32508;&#36848;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Affective Neurophysiological Signals Using Generative Models: A Review Paper. (arXiv:2306.03112v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#31070;&#32463;&#29983;&#29702;&#20449;&#21495;&#26469;&#35299;&#20915;&#20844;&#20849;&#24773;&#24863;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#29983;&#25104;&#27169;&#22411;&#22312;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#24212;&#29992;&#30340;&#20248;&#21183;&#12289;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#28145;&#20837;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#20013;&#65292;&#23558;&#24773;&#24863;&#26234;&#33021;&#24341;&#20837;&#26426;&#22120;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#36825;&#38656;&#35201;&#24320;&#21457;&#21487;&#38752;&#30340;&#31471;&#21040;&#31471;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20844;&#20849;&#24773;&#24863;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#29486;&#35780;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#35299;&#20915;&#31070;&#32463;&#29983;&#29702;&#23398;&#20449;&#21495;&#20013;&#65292;&#23588;&#20854;&#26159;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#21644;&#21151;&#33021;&#24615;&#36817;&#32418;&#22806;&#20809;&#35889;&#65288;fNIRS&#65289;&#20013;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#26816;&#39564;&#20102;&#23427;&#20204;&#30340;&#36755;&#20837;&#26684;&#24335;&#12289;&#37096;&#32626;&#31574;&#30053;&#20197;&#21450;&#29992;&#20110;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#32508;&#36848;&#26159;&#19968;&#31687;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#29983;&#25104;&#27169;&#22411;&#22312;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12289;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#28145;&#20837;&#35265;&#35299;&#12290;&#36890;&#36807;&#36825;&#31687;&#32508;&#36848;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#31070;&#32463;&#29983;&#29702;&#25968;&#25454;&#22686;&#24378;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of emotional intelligence in machines is an important step in advancing human-computer interaction. This demands the development of reliable end-to-end emotion recognition systems. However, the scarcity of public affective datasets presents a challenge. In this literature review, we emphasize the use of generative models to address this issue in neurophysiological signals, particularly Electroencephalogram (EEG) and Functional Near-Infrared Spectroscopy (fNIRS). We provide a comprehensive analysis of different generative models used in the field, examining their input formulation, deployment strategies, and methodologies for evaluating the quality of synthesized data. This review serves as a comprehensive overview, offering insights into the advantages, challenges, and promising future directions in the application of generative models in emotion recognition systems. Through this review, we aim to facilitate the progression of neurophysiological data augmentation, there
&lt;/p&gt;</description></item><item><title>SwinRDM&#26159;&#19968;&#31181;&#23558;SwinRNN&#19982;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#39044;&#25253;&#20934;&#30830;&#24615;&#20197;&#21450;&#22312;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03110</link><description>&lt;p&gt;
SwinRDM: &#23558;SwinRNN&#19982;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#19988;&#39640;&#36136;&#37327;&#30340;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
SwinRDM: Integrate SwinRNN with Diffusion Model towards High-Resolution and High-Quality Weather Forecasting. (arXiv:2306.03110v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03110
&lt;/p&gt;
&lt;p&gt;
SwinRDM&#26159;&#19968;&#31181;&#23558;SwinRNN&#19982;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#39044;&#25253;&#20934;&#30830;&#24615;&#20197;&#21450;&#22312;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#39640;&#20998;&#36776;&#29575;&#19979;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20173;&#28982;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#20026;&#20102;&#36861;&#27714;&#39640;&#20998;&#36776;&#29575;&#21644;&#39640;&#36136;&#37327;&#30340;&#22825;&#27668;&#39044;&#25253;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;SwinRDM&#65292;&#23427;&#23558;&#25913;&#36827;&#21518;&#30340;SwinRNN&#19982;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;SwinRDM&#22312;0.25&#24230;&#20998;&#36776;&#29575;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#22312;500 hPa&#20301;&#21183;&#39640;&#24230;&#65288;Z500&#65289;&#65292;850 hPa&#28201;&#24230;&#65288;T850&#65289;&#65292;2&#31859;&#28201;&#24230;&#65288;T2M&#65289;&#21644;&#24635;&#38477;&#27700;&#65288;TP&#65289;&#31561;&#20195;&#34920;&#24615;&#22823;&#27668;&#21464;&#37327;&#19978;&#65292;&#20197;&#26368;&#22810;5&#22825;&#30340;&#20808;&#23548;&#26102;&#38388;&#65292;&#27604;IFS&#65288;&#38598;&#25104;&#39044;&#25253;&#31995;&#32479;&#65289;&#36825;&#31181;&#26368;&#20808;&#36827;&#30340;&#36816;&#33829;&#25968;&#20540;&#39044;&#25253;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#39044;&#25253;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#31574;&#30053;&#65292;&#36890;&#36807;&#26435;&#34913;&#35745;&#31639;&#20869;&#23384;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#26469;&#23454;&#29616;0.25&#24230;&#20998;&#36776;&#29575;&#30340;&#39640;&#20998;&#36776;&#29575;&#39044;&#27979;&#12290;&#39318;&#20808;&#22312;1.40625&#24230;&#20998;&#36776;&#29575;&#19979;&#23436;&#25104;&#23545;&#26410;&#26469;&#22823;&#27668;&#22330;&#30340;&#24490;&#29615;&#39044;&#27979;&#65292;&#28982;&#21518;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#23558;&#36755;&#20986;&#31934;&#32454;&#21270;&#21040;0.25&#24230;&#20998;&#36776;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SwinRDM&#19981;&#20165;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#39640;&#20998;&#36776;&#29575;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven medium-range weather forecasting has attracted much attention in recent years. However, the forecasting accuracy at high resolution is unsatisfactory currently. Pursuing high-resolution and high-quality weather forecasting, we develop a data-driven model SwinRDM which integrates an improved version of SwinRNN with a diffusion model. SwinRDM performs predictions at 0.25-degree resolution and achieves superior forecasting accuracy to IFS (Integrated Forecast System), the state-of-the-art operational NWP model, on representative atmospheric variables including 500 hPa geopotential (Z500), 850 hPa temperature (T850), 2-m temperature (T2M), and total precipitation (TP), at lead times of up to 5 days. We propose to leverage a two-step strategy to achieve high-resolution predictions at 0.25-degree considering the trade-off between computation memory and forecasting accuracy. Recurrent predictions for future atmospheric fields are firstly performed at 1.40625-degree resolution, and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#25311;&#20154;&#26684;&#30340;&#22242;&#38431;&#12289;&#25552;&#20379;&#35821;&#22659;&#12289;&#25552;&#31034;&#19982;&#24341;&#23548;&#65292;&#33021;&#22815;&#20197;&#27169;&#20223;&#19987;&#23478;&#20154;&#26684;&#26469;&#36827;&#34892;&#35748;&#30693;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.03104</link><description>&lt;p&gt;
&#20197;&#27169;&#25311;&#19987;&#23478;&#20154;&#26684;&#20026;&#21521;&#23548;&#30340;&#22330;&#26223;&#25351;&#23548;&#65306;&#25191;&#34892;&#35748;&#30693;&#24037;&#20316;&#30340;&#26174;&#33879;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Guided scenarios with simulated expert personae: a remarkable strategy to perform cognitive work. (arXiv:2306.03104v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03104
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#20154;&#26684;&#30340;&#22242;&#38431;&#12289;&#25552;&#20379;&#35821;&#22659;&#12289;&#25552;&#31034;&#19982;&#24341;&#23548;&#65292;&#33021;&#22815;&#20197;&#27169;&#20223;&#19987;&#23478;&#20154;&#26684;&#26469;&#36827;&#34892;&#35748;&#30693;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#24040;&#22823;&#30340;&#20154;&#31867;&#30693;&#35782;&#21644;&#25991;&#23398;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;&#35813;&#35821;&#26009;&#24211;&#30340;&#22823;&#37327;&#20107;&#23454;,&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#36824;&#33021;&#22815;&#37325;&#26032;&#21019;&#36896;&#20986;&#34987;&#25429;&#25417;&#22312;&#35821;&#26009;&#24211;&#20013;&#30340;&#20154;&#26684;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#24418;&#25104;&#27169;&#25311;&#20154;&#26684;&#30340;&#22242;&#38431;&#12289;&#25552;&#20379;&#35774;&#32622;&#33310;&#21488;&#30340;&#35821;&#22659;&#21644;&#25552;&#20379;&#28201;&#21644;&#30340;&#25552;&#31034;&#65292;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#24773;&#22659;&#25512;&#36827;&#26469;&#24341;&#20986;&#19987;&#23478;&#34892;&#20026;&#20197;&#25191;&#34892;&#26377;&#24847;&#20041;&#30340;&#35748;&#30693;&#24037;&#20316;&#12290;&#36825;&#19968;&#31574;&#30053;&#30340;&#23041;&#21147;&#22312;&#20004;&#20010;&#20363;&#23376;&#20013;&#24471;&#21040;&#20102;&#23637;&#31034;&#65292;&#19968;&#20010;&#25915;&#20987;LLM&#21709;&#24212;&#30340;&#20107;&#23454;&#24615;&#65292;&#21478;&#19968;&#20010;&#26159;&#22797;&#21046;&#20102;&#19968;&#20010;&#26368;&#36817;&#22312;&#37327;&#23376;&#20809;&#23398;&#20013;&#21457;&#34920;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) trained on a substantial corpus of human knowledge and literature productively work with a large array of facts from that corpus. Surprisingly, they are also able to re-create the behaviors of personae that are captured within the corpus. By forming teams of simulated personae, supplying contexts that set the stage, and providing gentle prompts, one can move through scenarios that elicit expert behavior to perform meaningful cognitive work. The power of this strategy is demonstrated with two examples, one attacking factuality of LLM responses and the other reproducing a very recently published result in quantum optics.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#21516;&#36136;&#21270;&#30340;&#27169;&#22411;&#65292;&#27169;&#22411;&#35780;&#20272;&#38656;&#35201;&#25552;&#20379;&#26377;&#25928;&#30340;&#35780;&#20272;&#65292;&#20197;&#21028;&#26029;&#29305;&#23450;&#27169;&#22411;&#26159;&#21542;&#22312;&#19979;&#28216;&#20351;&#29992;&#22330;&#26223;&#20013;&#21487;&#20197;&#28385;&#36275;&#22810;&#23569;&#20154;&#31867;&#38656;&#27714;&#65292;&#24182;&#19988;&#24212;&#35813;&#26681;&#25454;&#30495;&#23454;&#30340;&#31038;&#20250;&#38656;&#27714;&#26469;&#24320;&#21457;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25317;&#25265;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03100</link><description>&lt;p&gt;
&#23558;&#27169;&#22411;&#35780;&#20272;&#37325;&#26032;&#32771;&#34385;&#20026;&#32553;&#23567;&#31038;&#20250;&#25216;&#26415;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Rethinking Model Evaluation as Narrowing the Socio-Technical Gap. (arXiv:2306.03100v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03100
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21516;&#36136;&#21270;&#30340;&#27169;&#22411;&#65292;&#27169;&#22411;&#35780;&#20272;&#38656;&#35201;&#25552;&#20379;&#26377;&#25928;&#30340;&#35780;&#20272;&#65292;&#20197;&#21028;&#26029;&#29305;&#23450;&#27169;&#22411;&#26159;&#21542;&#22312;&#19979;&#28216;&#20351;&#29992;&#22330;&#26223;&#20013;&#21487;&#20197;&#28385;&#36275;&#22810;&#23569;&#20154;&#31867;&#38656;&#27714;&#65292;&#24182;&#19988;&#24212;&#35813;&#26681;&#25454;&#30495;&#23454;&#30340;&#31038;&#20250;&#38656;&#27714;&#26469;&#24320;&#21457;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25317;&#25265;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#21457;&#23637;&#32473;&#27169;&#22411;&#35780;&#20272;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#30028;&#21644;&#24037;&#19994;&#30028;&#27491;&#22312;&#21162;&#21147;&#24212;&#23545;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#25165;&#22810;&#33402;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#22859;&#65292;&#20294;&#23427;&#20204;&#20063;&#19981;&#21487;&#36991;&#20813;&#22320;&#21521;&#21516;&#36136;&#21270;&#36808;&#36827;&#65306;&#29992;&#21333;&#20010;&#24120;&#31216;&#20043;&#20026;&#8220;&#36890;&#29992;&#8221;&#30340;&#27169;&#22411;&#20026;&#19968;&#31995;&#21015;&#24212;&#29992;&#25552;&#20379;&#21160;&#21147;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#27169;&#22411;&#35780;&#20272;&#23454;&#36341;&#24517;&#39035;&#25215;&#25285;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20197;&#24212;&#23545;&#36825;&#31181;&#21516;&#36136;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#21644;&#36131;&#20219;&#65306;&#20026;&#29305;&#23450;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#30340;&#35780;&#20272;&#65292;&#21028;&#26029;&#26159;&#21542;&#20197;&#21450;&#22312;&#19979;&#28216;&#20351;&#29992;&#22330;&#26223;&#20013;&#21487;&#20197;&#36890;&#36807;&#32473;&#23450;&#27169;&#22411;&#28385;&#36275;&#22810;&#23569;&#20154;&#31867;&#38656;&#27714;&#65288;&#8220;&#31038;&#20250;&#25216;&#26415;&#24046;&#36317;&#8221;&#65289;&#12290;&#25105;&#20204;&#27762;&#21462;&#31038;&#20250;&#31185;&#23398;&#12289;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#21644;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#36328;&#23398;&#31185;&#39046;&#22495;&#30340;&#32463;&#39564;&#65292;&#25958;&#20419;&#31038;&#21306;&#24320;&#21457;&#22522;&#20110;&#30495;&#23454;&#31038;&#20250;&#38656;&#27714;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25317;&#25265;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development of generative and large language models (LLMs) poses new challenges for model evaluation that the research community and industry are grappling with. While the versatile capabilities of these models ignite excitement, they also inevitably make a leap toward homogenization: powering a wide range of applications with a single, often referred to as ``general-purpose'', model. In this position paper, we argue that model evaluation practices must take on a critical task to cope with the challenges and responsibilities brought by this homogenization: providing valid assessments for whether and how much human needs in downstream use cases can be satisfied by the given model (\textit{socio-technical gap}). By drawing on lessons from the social sciences, human-computer interaction (HCI), and the interdisciplinary field of explainable AI (XAI), we urge the community to develop evaluation methods based on real-world socio-requirements and embrace diverse evaluation methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#32511;&#33394;&#22242;&#38431;&#8221;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#29992;&#26469;&#36890;&#36807;&#32469;&#36807;GM&#20869;&#23481;&#36807;&#28388;&#22120;&#35774;&#35745;&#26377;&#30410;&#29992;&#20363;&#12290;&#23454;&#38469;&#24212;&#29992;&#21253;&#25324;&#20351;&#29992;ChatGPT&#36827;&#34892;&#33258;&#26432;&#25903;&#25345;&#22521;&#35757;&#20197;&#21450;&#20351;&#29992;Codex&#36827;&#34892;&#26377;&#32570;&#38519;&#30340;&#35299;&#20915;&#26041;&#26696;&#22521;&#35757;&#12290;</title><link>http://arxiv.org/abs/2306.03097</link><description>&lt;p&gt;
&#36229;&#36234;&#26434;&#33609;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#30340;&#8220;&#32511;&#33394;&#22242;&#38431;&#8221;&#36827;&#34892;&#26377;&#30410;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Seeing Seeds Beyond Weeds: Green Teaming Generative AI for Beneficial Uses. (arXiv:2306.03097v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#32511;&#33394;&#22242;&#38431;&#8221;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#29992;&#26469;&#36890;&#36807;&#32469;&#36807;GM&#20869;&#23481;&#36807;&#28388;&#22120;&#35774;&#35745;&#26377;&#30410;&#29992;&#20363;&#12290;&#23454;&#38469;&#24212;&#29992;&#21253;&#25324;&#20351;&#29992;ChatGPT&#36827;&#34892;&#33258;&#26432;&#25903;&#25345;&#22521;&#35757;&#20197;&#21450;&#20351;&#29992;Codex&#36827;&#34892;&#26377;&#32570;&#38519;&#30340;&#35299;&#20915;&#26041;&#26696;&#22521;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#65288;GMs&#65289;&#22914;GPT&#21644;DALL-E&#30340;&#35757;&#32451;&#30446;&#30340;&#26159;&#20026;&#20102;&#26222;&#36941;&#12289;&#24191;&#27867;&#30340;&#30446;&#30340;&#29983;&#25104;&#20869;&#23481;&#12290;GM&#20869;&#23481;&#36807;&#28388;&#22120;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#36807;&#28388;&#20986;&#22312;&#24456;&#22810;&#24773;&#20917;&#19979;&#23384;&#22312;&#21361;&#23475;&#39118;&#38505;&#30340;&#20869;&#23481;&#65292;&#20363;&#22914;&#20167;&#24680;&#35328;&#35770;&#12290;&#28982;&#32780;&#65292;&#34987;&#31105;&#27490;&#30340;&#20869;&#23481;&#24182;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#8212;&#8212;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#34987;&#31105;&#27490;&#30340;&#20869;&#23481;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#22240;&#27492;&#65292;&#24403;GMs&#36807;&#28388;&#20869;&#23481;&#26102;&#65292;&#23427;&#20204;&#19981;&#20165;&#20250;&#25490;&#38500;&#26377;&#23475;&#30340;&#29992;&#20363;&#65292;&#20063;&#20250;&#25490;&#38500;&#26377;&#30410;&#30340;&#29992;&#20363;&#12290;&#34987;&#25490;&#38500;&#30340;&#29992;&#20363;&#21453;&#26144;&#20102;GM&#20869;&#23481;&#36807;&#28388;&#20013;&#23884;&#20837;&#30340;&#20215;&#20540;&#35266;&#12290;&#26368;&#36817;&#30340;&#32418;&#38431;&#24037;&#20316;&#25552;&#20986;&#20102;&#32469;&#36807;GM&#20869;&#23481;&#36807;&#28388;&#22120;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#36896;&#20102;&#32511;&#33394;&#22242;&#38431;&#36825;&#19968;&#26415;&#35821;&#65292;&#29992;&#26469;&#25551;&#36848;&#32469;&#36807;GM&#20869;&#23481;&#36807;&#28388;&#22120;&#20026;&#26377;&#30410;&#29992;&#20363;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#23454;&#20363;&#23637;&#31034;&#20102;&#32511;&#33394;&#22242;&#38431;&#30340;&#24212;&#29992;&#65306;1&#65289;&#20351;&#29992;ChatGPT&#20316;&#20026;&#34394;&#25311;&#24739;&#32773;&#65292;&#36890;&#36807;&#27169;&#25311;&#24739;&#26377;&#33258;&#26432;&#20542;&#21521;&#30340;&#20154;&#26469;&#36827;&#34892;&#33258;&#26432;&#25903;&#25345;&#22521;&#35757;&#65307;2&#65289;&#20351;&#29992;Codex&#26377;&#24847;&#22320;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#29992;&#20110;&#22521;&#35757;&#23398;&#29983;&#36827;&#34892;&#35843;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large generative AI models (GMs) like GPT and DALL-E are trained to generate content for general, wide-ranging purposes. GM content filters are generalized to filter out content which has a risk of harm in many cases, e.g., hate speech. However, prohibited content is not always harmful -- there are instances where generating prohibited content can be beneficial. So, when GMs filter out content, they preclude beneficial use cases along with harmful ones. Which use cases are precluded reflects the values embedded in GM content filtering. Recent work on red teaming proposes methods to bypass GM content filters to generate harmful content. We coin the term green teaming to describe methods of bypassing GM content filters to design for beneficial use cases. We showcase green teaming by: 1) Using ChatGPT as a virtual patient to simulate a person experiencing suicidal ideation, for suicide support training; 2) Using Codex to intentionally generate buggy solutions to train students on debuggin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#38750;&#21442;&#25968;&#30446;&#26631;&#27169;&#22411;&#30340;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#21644;&#36138;&#24515;&#27867;&#20989;&#25945;&#23398;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03007</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Iterative Machine Teaching. (arXiv:2306.03007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#38750;&#21442;&#25968;&#30446;&#26631;&#27169;&#22411;&#30340;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#21644;&#36138;&#24515;&#27867;&#20989;&#25945;&#23398;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;(IMT)&#38382;&#39064;&#12290;&#29616;&#26377;IMT&#31639;&#27861;&#22522;&#20110;&#21442;&#25968;&#21270;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#26222;&#36941;&#30340;&#20219;&#21153;&#8212;&#8212;&#38750;&#21442;&#25968;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;(NIMT)&#65292;&#26088;&#22312;&#20197;&#36845;&#20195;&#26041;&#24335;&#21521;&#23398;&#20064;&#32773;&#25945;&#25480;&#38750;&#21442;&#25968;&#30446;&#26631;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;NIMT&#35270;&#20026;&#19968;&#20010;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#27867;&#20989;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#38543;&#26426;&#21644;&#36138;&#24515;&#27867;&#20989;&#25945;&#23398;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of Iterative Machine Teaching (IMT), where the teacher provides examples to the learner iteratively such that the learner can achieve fast convergence to a target model. However, existing IMT algorithms are solely based on parameterized families of target models. They mainly focus on convergence in the parameter space, resulting in difficulty when the target models are defined to be functions without dependency on parameters. To address such a limitation, we study a more general task -Nonparametric Iterative Machine Teaching (NIMT), which aims to teach nonparametric target models to learners in an iterative fashion. Unlike parametric IMT that merely operates in the parameter space, we cast NIMT as a functional optimization problem in the function space. To solve it, we propose both random and greedy functional teaching algorithms. We obtain the iterative teaching dimension (ITD) of the random teaching algorithm under proper assumptions, which se
&lt;/p&gt;</description></item><item><title>Time Interpret&#26159;&#19968;&#20010;&#22522;&#20110;Captum&#30340;&#27169;&#22411;&#35299;&#37322;&#24211;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#31181;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#21508;&#31181;PyTorch&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.02968</link><description>&lt;p&gt;
Time Interpret: &#19968;&#31181;&#24207;&#21015;&#25968;&#25454;&#21487;&#35299;&#37322;&#24615;&#32479;&#19968;&#27169;&#22411;&#24211;
&lt;/p&gt;
&lt;p&gt;
Time Interpret: a Unified Model Interpretability Library for Time Series. (arXiv:2306.02968v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02968
&lt;/p&gt;
&lt;p&gt;
Time Interpret&#26159;&#19968;&#20010;&#22522;&#20110;Captum&#30340;&#27169;&#22411;&#35299;&#37322;&#24211;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#31181;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#21508;&#31181;PyTorch&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#27454;&#21517;&#20026;$\texttt{time_interpret}$&#30340;&#24211;&#65292;&#23427;&#26159;&#20197;Captum&#20026;&#22522;&#30784;&#35774;&#35745;&#30340;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#35813;&#24211;&#23454;&#29616;&#20102;&#22810;&#31181;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#37322;&#20219;&#20309;Pytorch&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;$\texttt{time_interpret}$&#36824;&#25552;&#20379;&#20102;&#22810;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12289;&#21508;&#31181;PyTorch&#27169;&#22411;&#20197;&#21450;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#35813;&#24211;&#20027;&#35201;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;&#26102;&#38388;&#25968;&#25454;&#30340;&#39044;&#27979;&#65292;&#20294;&#23427;&#30340;&#26576;&#20123;&#32452;&#20214;&#20063;&#26377;&#19981;&#21516;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#35813;&#24211;&#30340;&#22522;&#26412;&#20869;&#23481;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#19982;$\texttt{time_interpret}$&#21516;&#26102;&#24320;&#21457;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce $\texttt{time_interpret}$, a library designed as an extension of Captum, with a specific focus on temporal data. As such, this library implements several feature attribution methods that can be used to explain predictions made by any Pytorch model. $\texttt{time_interpret}$ also provides several synthetic and real world time series datasets, various PyTorch models, as well as a set of methods to evaluate feature attributions. Moreover, while being primarily developed to explain predictions based on temporal data, some of its components have a different application, including for instance methods explaining predictions made by language models. In this paper, we give a general introduction of this library. We also present several previously unpublished feature attribution methods, which have been developed along with $\texttt{time_interpret}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24314;&#27169;&#21644;&#35299;&#20915;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#26694;&#26550;&#8212;&#34892;&#21160;&#28436;&#21464;Petri&#32593;&#26684;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#21487;&#20197;&#34920;&#31034;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#25152;&#26377;&#35201;&#32032;&#12290;&#32780;&#19988;&#65292;&#35813;&#27169;&#22411;&#26159;&#21487;&#25191;&#34892;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#37197;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.02910</link><description>&lt;p&gt;
&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#35299;&#20915;&#26694;&#26550;&#65306;&#34892;&#21160;&#28436;&#21464;Petri&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Action-Evolution Petri Nets: a Framework for Modeling and Solving Dynamic Task Assignment Problems. (arXiv:2306.02910v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24314;&#27169;&#21644;&#35299;&#20915;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#26694;&#26550;&#8212;&#34892;&#21160;&#28436;&#21464;Petri&#32593;&#26684;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#21487;&#20197;&#34920;&#31034;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#25152;&#26377;&#35201;&#32032;&#12290;&#32780;&#19988;&#65292;&#35813;&#27169;&#22411;&#26159;&#21487;&#25191;&#34892;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#37197;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#28041;&#21450;&#23558;&#21040;&#36798;&#30340;&#20219;&#21153;&#20998;&#37197;&#32473;&#26377;&#38480;&#30340;&#36164;&#28304;&#65292;&#20197;&#26368;&#23567;&#21270;&#20998;&#37197;&#30340;&#24635;&#25104;&#26412;&#12290;&#20026;&#20102;&#36798;&#21040;&#26368;&#20248;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#24517;&#39035;&#20808;&#23545;&#20998;&#37197;&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#12290;&#34429;&#28982;&#23384;&#22312;&#29992;&#20110;&#27169;&#25311;&#12289;&#25191;&#34892;&#21644;&#35299;&#20915;&#38382;&#39064;&#19981;&#21516;&#26041;&#38754;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#22914;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;Petri&#32593;&#26684;&#65292;&#20294;&#19981;&#23384;&#22312;&#19968;&#31181;&#38598;&#25104;&#24314;&#27169;&#25216;&#26415;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Action-Evolution Petri&#32593;&#26684;(A-E PN)&#20316;&#20026;&#19968;&#31181;&#24314;&#27169;&#21644;&#35299;&#20915;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;A-E PN&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#21487;&#20197;&#34920;&#31034;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#25152;&#26377;&#35201;&#32032;&#12290;&#27492;&#22806;&#65292;A-E PN&#27169;&#22411;&#26159;&#21487;&#25191;&#34892;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;(RL)&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#37197;&#31574;&#30053;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#24314;&#27169;&#24037;&#20316;&#12290;&#20026;&#20102;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#21407;&#22411;&#20998;&#37197;&#38382;&#39064;&#30340;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic task assignment involves assigning arriving tasks to a limited number of resources in order to minimize the overall cost of the assignments. To achieve optimal task assignment, it is necessary to model the assignment problem first. While there exist separate formalisms, specifically Markov Decision Processes and (Colored) Petri Nets, to model, execute, and solve different aspects of the problem, there is no integrated modeling technique. To address this gap, this paper proposes Action-Evolution Petri Nets (A-E PN) as a framework for modeling and solving dynamic task assignment problems. A-E PN provides a unified modeling technique that can represent all elements of dynamic task assignment problems. Moreover, A-E PN models are executable, which means they can be used to learn close-to-optimal assignment policies through Reinforcement Learning (RL) without additional modeling effort. To evaluate the framework, we define a taxonomy of archetypical assignment problems. We show for 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.02865</link><description>&lt;p&gt;
&#25235;&#20303;&#24847;&#22806;&#25910;&#33719;&#65306;&#22312;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#20215;&#20540;(arXiv:2306.02865v2 [cs.LG]&#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39640;&#36136;&#37327;&#30340; Q &#20540;&#20989;&#25968;&#22312;&#35768;&#22810;&#29616;&#20195;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (RL) &#31639;&#27861;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#35299;&#20915;&#37319;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#21644;&#31163;&#32447;&#23398;&#20064;&#25152;&#23548;&#33268;&#30340;&#20540;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#19982;&#36825;&#31181;&#26222;&#36941;&#35266;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040; Q &#20540;&#22312; RL &#35757;&#32451;&#36807;&#31243;&#30340;&#21518;&#26399;&#23454;&#38469;&#19978;&#34987;&#20302;&#20272;&#20102;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36125;&#23572;&#26364;&#26356;&#26032;&#20013;&#65292;&#24403;&#21069;&#31574;&#30053;&#20351;&#29992;&#27604;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#26356;&#20248;&#30340;&#21160;&#20316;&#26679;&#26412;&#24046;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20010;&#38271;&#26399;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#21487;&#33021;&#38459;&#30861;&#20102;&#31574;&#30053;&#23398;&#20064;&#65292;&#38477;&#20302;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#22312;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#30340;&#21516;&#26102;&#65292;&#32467;&#21512;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#21033;&#29992;&#21644;&#25506;&#32034; (BEE) &#25805;&#20316;&#31526;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21382;&#21490;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#21160;&#20316;&#21644;&#24403;&#21069;&#31574;&#30053;&#29983;&#25104;&#30340;&#21160;&#20316;&#26469;&#26356;&#26032; Q &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D-CLOSE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#20219;&#20309;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20998;&#21106;&#32423;&#21035;&#21644;&#19968;&#31181;&#32452;&#21512;&#23427;&#20204;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#26356;&#23569;&#30340;&#22122;&#38899;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.02744</link><description>&lt;p&gt;
&#36808;&#21521;&#26356;&#22909;&#30340;&#30446;&#26631;&#26816;&#27979;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards Better Explanations for Object Detection. (arXiv:2306.02744v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D-CLOSE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#20219;&#20309;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20998;&#21106;&#32423;&#21035;&#21644;&#19968;&#31181;&#32452;&#21512;&#23427;&#20204;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#26356;&#23569;&#30340;&#22122;&#38899;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#23427;&#20204;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#30340;&#20351;&#29992;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#20351;&#35299;&#37322;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#21644;&#20915;&#31574;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#21644;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#37322;&#20998;&#31867;&#20219;&#21153;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D-CLOSE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#20219;&#20309;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#23494;&#20999;&#36319;&#36394;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#19978;&#20351;&#29992;&#20102;&#22810;&#20010;&#20998;&#21106;&#32423;&#21035;&#21644;&#19968;&#31181;&#32452;&#21512;&#23427;&#20204;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;YOLOX&#27169;&#22411;&#22312;MS-COCO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;D-RISE&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#26356;&#23569;&#30340;&#22122;&#38899;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Artificial Intelligence (AI) technology have promoted their use in almost every field. The growing complexity of deep neural networks (DNNs) makes it increasingly difficult and important to explain the inner workings and decisions of the network. However, most current techniques for explaining DNNs focus mainly on interpreting classification tasks. This paper proposes a method to explain the decision for any object detection model called D-CLOSE. To closely track the model's behavior, we used multiple levels of segmentation on the image and a process to combine them. We performed tests on the MS-COCO dataset with the YOLOX model, which shows that our method outperforms D-RISE and can give a better quality and less noise explanation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#30111;&#30142;&#39044;&#27979;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#24067;&#38534;&#36842;&#30111;&#30142;&#26102;&#31354;&#21160;&#24577;&#65292;&#20026;&#30111;&#30142;&#38450;&#27835;&#21644;&#24178;&#39044;&#35774;&#35745;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.02685</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#24067;&#38534;&#36842;&#30111;&#30142;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Predicting malaria dynamics in Burundi using deep Learning Models. (arXiv:2306.02685v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02685
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#30111;&#30142;&#39044;&#27979;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#24067;&#38534;&#36842;&#30111;&#30142;&#26102;&#31354;&#21160;&#24577;&#65292;&#20026;&#30111;&#30142;&#38450;&#27835;&#21644;&#24178;&#39044;&#35774;&#35745;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30111;&#30142;&#32487;&#32493;&#22312;&#38750;&#27954;&#22823;&#38470;&#29305;&#21035;&#26159;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#22320;&#21306;&#25104;&#20026;&#20027;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#22312;&#21162;&#21147;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#24067;&#38534;&#36842;&#65292;&#30111;&#30142;&#26159;&#20027;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#24067;&#38534;&#36842;&#30111;&#30142;&#39044;&#27979;&#27169;&#22411;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#25105;&#20204;&#24314;&#31435;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#20272;&#35745;&#24067;&#38534;&#36842;&#30340;&#30111;&#30142;&#30149;&#20363;&#12290;&#21033;&#29992;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#22240;&#32032;&#22914;&#28201;&#24230;&#12289;&#38477;&#38632;&#21644;&#30456;&#23545;&#28287;&#24230;&#20197;&#21450;&#30111;&#30142;&#21382;&#21490;&#25968;&#25454;&#21644;&#20154;&#21475;&#25968;&#25454;&#65292;&#37319;&#29992;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;LSTM&#27169;&#22411;&#65292;&#23545;&#30465;&#32423;&#21644;&#22269;&#23478;&#33539;&#22260;&#20869;&#30340;&#30111;&#30142;&#24773;&#20917;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;&#26681;&#25454;&#27169;&#22411;&#32467;&#26524;&#65292;&#21487;&#20197;&#30830;&#23450;&#22312;&#19981;&#21516;&#21442;&#25968;&#35843;&#25972;&#19979;&#65292;&#22269;&#23478;&#32423;&#21035;&#30340;&#26368;&#20302;&#21644;&#26368;&#39640;&#39044;&#26399;&#30111;&#30142;&#30149;&#20363;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malaria continues to be a major public health problem on the African continent, particularly in Sub-Saharan Africa. Nonetheless, efforts are ongoing, and significant progress has been made. In Burundi, malaria is among the main public health concerns. In the literature, there are limited prediction models for Burundi. We know that such tools are much needed for interventions design. In our study, we built machine-learning based models to estimates malaria cases in Burundi. The forecast of malaria cases was carried out at province level and national scale as well. Long short term memory (LSTM) model, a type of deep learning model has been used to achieve best results using climate-change related factors such as temperature, rainfal, and relative humidity, together with malaria historical data and human population. With this model, the results showed that at country level different tuning of parameters can be used in order to determine the minimum and maximum expected malaria cases. The 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#30340;&#24341;&#20837;&#65292;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#29289;&#20307;&#24212;&#26144;&#23556;&#21040;&#19981;&#21516;&#27133;&#20301;&#30340;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02204</link><description>&lt;p&gt;
&#24490;&#29615;&#19968;&#33268;&#24615;&#39537;&#21160;&#30340;&#29289;&#20307;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cycle Consistency Driven Object Discovery. (arXiv:2306.02204v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#30340;&#24341;&#20837;&#65292;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#29289;&#20307;&#24212;&#26144;&#23556;&#21040;&#19981;&#21516;&#27133;&#20301;&#30340;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#30340;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#26550;&#26500;&#20808;&#39564;&#25110;&#36741;&#21161;&#20449;&#24687;&#65288;&#20363;&#22914;&#28145;&#24230;&#22270;&#25110;&#27969;&#22330;&#22270;&#65289;&#26469;&#25506;&#32034;&#22522;&#20110;&#27133;&#20301;&#30340;&#26041;&#27861;&#65292;&#20197;&#34920;&#31034;&#23545;&#35937;&#20026;&#31216;&#20026;&#8220;&#27133;&#20301;&#8221;&#25110;&#8220;&#23545;&#35937;&#25991;&#20214;&#8221;&#30340;&#22266;&#23450;&#22823;&#23567;&#30340;&#21521;&#37327;&#65292;&#20174;&#32780;&#20419;&#36827;&#29289;&#20307;&#21457;&#29616;&#12290; &#28982;&#32780;&#65292;&#20381;&#36182;&#20110;&#26550;&#26500;&#20808;&#39564;&#20250;&#24341;&#20837;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#25165;&#33021;&#35782;&#21035;&#27491;&#30830;&#30340;&#23545;&#35937;&#12290; &#21516;&#26679;&#65292;&#20381;&#36182;&#36741;&#21161;&#20449;&#24687;&#30340;&#26041;&#27861;&#20063;&#19981;&#22815;&#20248;&#36234;&#65292;&#22240;&#20026;&#36825;&#31181;&#20449;&#24687;&#36890;&#24120;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#24773;&#20917;&#19979;&#19981;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#23545;&#35937;&#24212;&#26144;&#23556;&#21040;&#19968;&#20010;&#19981;&#21516;&#27133;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#24418;&#24335;&#21270;&#36825;&#20010;&#32422;&#26463;&#65292;&#31216;&#20043;&#20026;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290; &#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#29289;&#20307;&#21457;&#29616;&#21644;&#23569;&#26679;&#26412;&#29289;&#20307;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches have explored slot-based methods utilizing architectural priors or auxiliary information such as depth maps or flow maps to facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. However, reliance on architectural priors introduces unreliability and requires meticulous engineering to identify the correct objects. Likewise, methods relying on auxiliary information are suboptimal as such information is often unavailable for most natural scenes. To address these limitations, we propose a method that explicitly optimizes the constraint that each object in a scene should be mapped to a distinct slot. We formalize this constraint by introducing consistency objectives which are cyclic in nature. We refer to them as the \textit{cycle-consistency} objectives. By applying these con
&lt;/p&gt;</description></item><item><title>MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.02069</link><description>&lt;p&gt;
MultiLegalPile&#65306;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile: A 689GB Multilingual Legal Corpus. (arXiv:2306.02069v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02069
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#27490;&#65292;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#27861;&#24459;&#65289;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#32780;&#19988;&#32463;&#24120;&#20165;&#38480;&#20110;&#33521;&#35821;&#12290;&#25105;&#20204;&#25972;&#29702;&#24182;&#21457;&#24067;&#20102;MultiLegalPile&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;689GB&#35821;&#26009;&#24211;&#12290;MultiLegalPile&#35821;&#26009;&#24211;&#21253;&#25324;&#21508;&#31181;&#35768;&#21487;&#35777;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#65292;&#23545;&#20110;Eurlex Resources&#21644;Legal mC4&#23376;&#38598;&#25317;&#26377;&#26356;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;RoBERTa&#27169;&#22411;&#21644;&#19968;&#20010;&#22810;&#35821;&#35328;Longformer&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#20998;&#21035;&#22312;&#27599;&#31181;&#29305;&#23450;&#35821;&#35328;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;24&#20010;&#21333;&#35821;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;LEXTREME&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;LexGLUE&#19978;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;LEXTREME&#19978;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;(SotA)&#65292;&#33521;&#35821;&#27169;&#22411;&#21017;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#20195;&#30721;&#20840;&#37096;&#37322;&#25918;&#22312;&#26368;&#24320;&#25918;&#30340;&#35768;&#21487;&#35777;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, there are few datasets available for specialized critical domains such as law and the available ones are often only for the English language. We curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. The MultiLegalPile corpus, which includes diverse legal data sources with varying licenses, allows for pretraining NLP models under fair use, with more permissive licenses for the Eurlex Resources and Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, the trained models, and all of the code under the most open possible licenses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#30340;&#24212;&#29992;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#24212;&#23545;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.02051</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#30340;&#32508;&#36848;&#65306;&#26368;&#26032;&#36827;&#23637;&#19982;&#26032;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Learning for Relation Extraction: Recent Advances and New Frontiers. (arXiv:2306.02051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#30340;&#24212;&#29992;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#24212;&#23545;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26159;&#25351;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#35782;&#21035;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20851;&#31995;&#25277;&#21462;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#22522;&#30784;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#38382;&#31572;&#21644;&#20449;&#24687;&#26816;&#32034;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#21344;&#25454;&#20102;&#20027;&#23548;&#22320;&#20301;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#38543;&#21518;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23558;&#20851;&#31995;&#25277;&#21462;&#30340;&#26368;&#26032;&#25216;&#26415;&#25512;&#21521;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#24230;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#31995;&#25277;&#21462;&#36164;&#28304;&#65292;&#21253;&#25324;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#25991;&#26412;&#34920;&#31034;&#12289;&#19978;&#19979;&#25991;&#32534;&#30721;&#21644;&#19977;&#20803;&#32452;&#39044;&#27979;&#19977;&#20010;&#26041;&#38754;&#23545;&#29616;&#26377;&#24037;&#20316;&#36827;&#34892;&#20998;&#31867;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#31995;&#25277;&#21462;&#38754;&#20020;&#30340;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#21487;&#33021;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20123;&#20855;&#26377;&#28508;&#22312;&#21069;&#26223;&#30340;&#26410;&#26469;&#26041;&#21521;&#21644;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) involves identifying the relations between entities from unstructured texts. RE serves as the foundation for many natural language processing (NLP) applications, such as knowledge graph completion, question answering, and information retrieval. In recent years, deep neural networks have dominated the field of RE and made noticeable progress. Subsequently, the large pre-trained language models (PLMs) have taken the state-of-the-art of RE to a new level. This survey provides a comprehensive review of existing deep learning techniques for RE. First, we introduce RE resources, including RE datasets and evaluation metrics. Second, we propose a new taxonomy to categorize existing works from three perspectives (text representation, context encoding, and triplet prediction). Third, we discuss several important challenges faced by RE and summarize potential techniques to tackle these challenges. Finally, we outline some promising future directions and prospects in this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.01788</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Responsible Design Patterns for Machine Learning Pipelines. (arXiv:2306.01788v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36947;&#24503;&#23454;&#36341;&#25972;&#21512;&#21040;&#20154;&#24037;&#26234;&#33021;(AI)&#24320;&#21457;&#36807;&#31243;&#20013;&#23545;&#20110;&#30830;&#20445;AI&#30340;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36127;&#36131;&#20219;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;AI&#20262;&#29702;&#28041;&#21450;&#23558;&#20262;&#29702;&#21407;&#21017;&#24212;&#29992;&#20110;AI&#31995;&#32479;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#12290;&#36825;&#23545;&#20110;&#20943;&#36731;&#19982;AI&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#20260;&#23475;&#65288;&#22914;&#31639;&#27861;&#20559;&#35265;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#65288;RDPs&#65289;&#23545;&#20110;&#30830;&#20445;&#20262;&#29702;&#21644;&#20844;&#24179;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;RDPs&#32435;&#20837;ML&#27969;&#31243;&#20013;&#65292;&#20197;&#20943;&#36731;&#39118;&#38505;&#24182;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#36890;&#36807;&#23545;AI&#20262;&#29702;&#21644;&#25968;&#25454;&#31649;&#29702;&#19987;&#23478;&#30340;&#35843;&#26597;&#30830;&#23450;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#21453;&#39304;&#30340;&#23454;&#38469;&#24773;&#20917;&#36827;&#34892;&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating ethical practices into the AI development process for artificial intelligence (AI) is essential to ensure safe, fair, and responsible operation. AI ethics involves applying ethical principles to the entire life cycle of AI systems. This is essential to mitigate potential risks and harms associated with AI, such as algorithm biases. To achieve this goal, responsible design patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee ethical and fair outcomes. In this paper, we propose a comprehensive framework incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical development of AI systems. Our framework comprises new responsible AI design patterns for ML pipelines identified through a survey of AI ethics and data management experts and validated through real-world scenarios with expert feedback. The framework guides AI developers, data scientists, and policy-makers to implement ethical practices in AI development and deploy responsibl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#21147;&#31995;&#32479;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#35299;&#20915;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01021</link><description>&lt;p&gt;
&#22522;&#20110;&#34394;&#25311;&#21147;&#30340;&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Vitual-Force Based Swarm Algorithm for Balanced Circular Bin Packing Problems. (arXiv:2306.01021v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#21147;&#31995;&#32479;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#35299;&#20915;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#28041;&#21450;&#23558;&#32473;&#23450;&#25968;&#37327;&#30340;&#21152;&#26435;&#22278;&#25918;&#32622;&#22312;&#22278;&#24418;&#23481;&#22120;&#20013;&#65292;&#20197;&#26368;&#23567;&#21270;&#21322;&#24452;&#24182;&#28385;&#36275;&#24179;&#34913;&#32422;&#26463;&#26465;&#20214;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#21147;&#31995;&#32479;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#23545;&#27599;&#20010;&#32452;&#20214;&#26045;&#21152;&#19968;&#32452;&#21147;&#65292;&#20197;&#32771;&#34385;&#32422;&#26463;&#26465;&#20214;&#24182;&#20351;&#29992;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#21407;&#29702;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#24182;&#39564;&#35777;&#20102;&#20855;&#26377;&#39640;&#36798;300&#20010;&#22278;&#30340;&#38382;&#39064;&#12290;&#25253;&#21578;&#30340;&#32467;&#26524;&#20801;&#35768;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#25991;&#29486;&#20013;&#29616;&#26377;&#32467;&#26524;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Balanced circular bin packing problems consist in positioning a given number of weighted circles in order to minimize the radius of a circular container while satisfying equilibrium constraints. These problems are NP-hard, highly constrained and dimensional. This paper describes a swarm algorithm based on a virtual-force system in order to solve balanced circular bin packing problems. In the proposed approach, a system of forces is applied to each component allowing to take into account the constraints and minimizing the objective function using the fundamental principle of dynamics. The proposed algorithm is experimented and validated on benchmarks of various balanced circular bin packing problems with up to 300 circles. The reported results allow to assess the effectiveness of the proposed approach compared to existing results from the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; ChatGPT &#22312;&#28304;&#20195;&#30721;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#23558;&#20026;&#31243;&#24207;&#21592;&#25552;&#20379;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#24110;&#21161;&#65292;&#24182;&#22312;&#20195;&#30721;&#21019;&#24314;&#12289;&#25991;&#26723;&#32534;&#20889;&#12289;&#32570;&#38519;&#26816;&#27979;&#21644;&#20195;&#30721;&#37325;&#26500;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.00597</link><description>&lt;p&gt;
ChatGPT&#22312;&#28304;&#20195;&#30721;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of ChatGPT on Source Code. (arXiv:2306.00597v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; ChatGPT &#22312;&#28304;&#20195;&#30721;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#23558;&#20026;&#31243;&#24207;&#21592;&#25552;&#20379;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#24110;&#21161;&#65292;&#24182;&#22312;&#20195;&#30721;&#21019;&#24314;&#12289;&#25991;&#26723;&#32534;&#20889;&#12289;&#32570;&#38519;&#26816;&#27979;&#21644;&#20195;&#30721;&#37325;&#26500;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29305;&#21035;&#26159; ChatGPT &#22312;&#32534;&#31243;&#12289;&#28304;&#20195;&#30721;&#20998;&#26512;&#21644;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;LLMs &#21644; ChatGPT &#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26500;&#24314;&#65292;&#24182;&#20026;&#24320;&#21457;&#20154;&#21592;&#21644;&#31243;&#24207;&#21592;&#25552;&#20379;&#22810;&#31181;&#22909;&#22788;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#24182;&#25552;&#20379;&#39640;&#24230;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#36824;&#19981;&#36275;&#20197;&#23436;&#20840;&#21462;&#20195;&#20154;&#31867;&#31243;&#24207;&#21592;&#12290;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102; LLMS &#21644; ChatGPT &#22312;&#20195;&#30721;&#21019;&#24314;&#12289;&#20195;&#30721;&#25991;&#26723;&#12289;&#32570;&#38519;&#26816;&#27979;&#12289;&#37325;&#26500;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#35813;&#35770;&#25991;&#36824;&#24314;&#35758;&#65292;&#38543;&#30528; LLM &#21644; ChatGPT &#20026;&#32534;&#31243;&#31038;&#21306;&#25552;&#20379;&#26080;&#19982;&#20262;&#27604;&#30340;&#22909;&#22788;&#65292;&#23427;&#20204;&#30340;&#20351;&#29992;&#39044;&#35745;&#23558;&#22312;&#26410;&#26469;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of Large Language Models (LLMs) and in particular ChatGPT in programming, source code analysis, and code generation. LLMs and ChatGPT are built using machine learning and artificial intelligence techniques, and they offer several benefits to developers and programmers. While these models can save time and provide highly accurate results, they are not yet advanced enough to replace human programmers entirely. The paper investigates the potential applications of LLMs and ChatGPT in various areas, such as code creation, code documentation, bug detection, refactoring, and more. The paper also suggests that the usage of LLMs and ChatGPT is expected to increase in the future as they offer unparalleled benefits to the programming community.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.18885</link><description>&lt;p&gt;
&#26631;&#20934;&#27604;&#35780;&#20998;&#26356;&#37325;&#35201;&#65306;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20934;&#21017;&#25512;&#33616;&#31995;&#32479;&#29616;&#22312;&#22312;&#24191;&#27867;&#30340;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#21033;&#29992;&#22810;&#20934;&#21017; (MC) &#35780;&#20998;&#20449;&#24687;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#21457;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;GNN&#36741;&#21161;&#35774;&#35745;MC&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#26041;&#27861;(CPA-LGC),&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#20197;&#21450;&#22797;&#26434;&#39640;&#38454;&#36830;&#25509;&#20013;&#30340;&#21327;&#20316;&#20449;&#21495;&#12290;&#26412;&#25991;&#22312;MC&#25193;&#23637;&#22270;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#29992;&#25143;-&#29289;&#21697;MC&#35780;&#20998;&#36716;&#25442;&#20026;&#25193;&#23637;&#20108;&#20998;&#22270;&#30340;MC&#25193;&#23637;&#22270;&#65292;&#20877;&#36827;&#19968;&#27493;&#23558;&#26631;&#20934;&#37325;&#35201;&#24615;&#32534;&#30721;&#21040;&#22270;&#21367;&#31215;&#36807;&#31243;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#32858;&#21512;&#26041;&#27861;&#26469;&#23558;&#29992;&#25143;&#23545;&#19981;&#21516;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#21644;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25913;&#21892;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#38656;&#35201;&#25919;&#24220;&#12289;&#31169;&#33829;&#37096;&#38376;&#12289;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#22269;&#38469;&#32452;&#32455;&#21327;&#35843;&#19968;&#33268;&#22320;&#21162;&#21147;&#65292;&#21019;&#24314;&#21487;&#25345;&#32493;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#28385;&#36275;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#29420;&#29305;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.18302</link><description>&lt;p&gt;
&#30446;&#21069;&#20026;&#27490;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
What We Know So Far: Artificial Intelligence in African Healthcare. (arXiv:2305.18302v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#21644;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25913;&#21892;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#38656;&#35201;&#25919;&#24220;&#12289;&#31169;&#33829;&#37096;&#38376;&#12289;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#22269;&#38469;&#32452;&#32455;&#21327;&#35843;&#19968;&#33268;&#22320;&#21162;&#21147;&#65292;&#21019;&#24314;&#21487;&#25345;&#32493;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#28385;&#36275;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#29420;&#29305;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#30340;&#21307;&#30103;&#20445;&#20581;&#38382;&#39064;&#21463;&#21040;&#35768;&#22810;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#36139;&#22256;&#12289;&#22522;&#30784;&#35774;&#26045;&#32570;&#20047;&#21644;&#36164;&#37329;&#19981;&#36275;&#31561;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20855;&#26377;&#28508;&#21147;&#36890;&#36807;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12289;&#20351;&#30142;&#30149;&#26356;&#26089;&#22320;&#34987;&#21457;&#29616;&#12289;&#25903;&#25345;&#20010;&#24615;&#21270;&#33647;&#29289;&#30340;&#21457;&#24067;&#26469;&#25913;&#21464;&#38750;&#27954;&#30340;&#21307;&#30103;&#20445;&#20581;&#29366;&#20917;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#30446;&#21069;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#25913;&#21892;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#30142;&#30149;&#30417;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25913;&#21892;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#25919;&#24220;&#12289;&#31169;&#33829;&#37096;&#38376;&#12289;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#22269;&#38469;&#32452;&#32455;&#21327;&#35843;&#19968;&#33268;&#22320;&#21162;&#21147;&#65292;&#21019;&#24314;&#21487;&#25345;&#32493;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#28385;&#36275;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#29420;&#29305;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare in Africa is a complex issue influenced by many factors including poverty, lack of infrastructure, and inadequate funding. However, Artificial intelligence (AI) applied to healthcare, has the potential to transform healthcare in Africa by improving the accuracy and efficiency of diagnosis, enabling earlier detection of diseases, and supporting the delivery of personalized medicine. This paper reviews the current state of how AI Algorithms can be used to improve diagnostics, treatment, and disease monitoring, as well as how AI can be used to improve access to healthcare in Africa as a low-resource setting and discusses some of the critical challenges and opportunities for its adoption. As such, there is a need for a well-coordinated effort by the governments, private sector, healthcare providers, and international organizations to create sustainable AI solutions that meet the unique needs of the African healthcare system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#38169;&#35273;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;InDL&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#33021;&#21147;&#12290;&#21033;&#29992;&#20960;&#20309;&#20809;&#23398;&#35270;&#38169;&#35273;&#65292;&#24314;&#31435;&#21487;&#27604;&#24615;&#26694;&#26550;&#29992;&#20110;&#38416;&#26126;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#21644;&#25552;&#20379;&#25913;&#36827;&#27169;&#22411;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.17716</link><description>&lt;p&gt;
InDL: &#22522;&#20110;&#35270;&#38169;&#35273;&#30340;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#26032;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
InDL: A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion. (arXiv:2305.17716v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#38169;&#35273;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;InDL&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#33021;&#21147;&#12290;&#21033;&#29992;&#20960;&#20309;&#20809;&#23398;&#35270;&#38169;&#35273;&#65292;&#24314;&#31435;&#21487;&#27604;&#24615;&#26694;&#26550;&#29992;&#20110;&#38416;&#26126;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#21644;&#25552;&#20379;&#25913;&#36827;&#27169;&#22411;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#26377;&#36259;&#39046;&#22495;&#30340;&#35270;&#38169;&#35273;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;InDL&#65292;&#26088;&#22312;&#20005;&#26684;&#27979;&#35797;&#21644;&#22522;&#20934;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#21033;&#29992;&#20845;&#20010;&#32463;&#20856;&#30340;&#20960;&#20309;&#35270;&#35273;&#38169;&#35273;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#20154;&#31867;&#21644;&#26426;&#22120;&#35270;&#35273;&#24863;&#30693;&#30340;&#21487;&#27604;&#24615;&#26694;&#26550;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#37327;&#21270;&#30340;&#34913;&#37327;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#34892;&#21160;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to evaluating deep learning models' capacity for in-diagram logic interpretation. Leveraging the intriguing realm of visual illusions, we establish a unique dataset, InDL, designed to rigorously test and benchmark these models. Deep learning has witnessed remarkable progress in domains such as computer vision and natural language processing. However, models often stumble in tasks requiring logical reasoning due to their inherent 'black box' characteristics, which obscure the decision-making process. Our work presents a new lens to understand these models better by focusing on their handling of visual illusions -- a complex interplay of perception and logic. We utilize six classic geometric optical illusions to create a comparative framework between human and machine visual perception. This methodology offers a quantifiable measure to rank models, elucidating potential weaknesses and providing actionable insights for model improvements. Our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16806</link><description>&lt;p&gt;
GPT&#26159;&#21542;&#20250;&#20135;&#29983;&#26356;&#19981;&#20934;&#30830;&#30340;&#32763;&#35793;?
&lt;/p&gt;
&lt;p&gt;
Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3&#65292;&#24050;&#32463;&#25104;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25110;&#29702;&#35299;&#20219;&#21153;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#20013;&#65292;&#24050;&#26377;&#22810;&#39033;&#30740;&#31350;&#25506;&#32034;&#21033;&#29992;few-shot&#25552;&#31034;&#26426;&#21046;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#30456;&#23545;&#36739;&#23569;&#22320;&#20851;&#27880;&#36825;&#31181;&#32763;&#35793;&#19982;&#26631;&#20934;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#29983;&#25104;&#32763;&#35793;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#20174;&#25991;&#23383;&#23545;&#40784;&#21644;&#21333;&#35843;&#24615;&#31561;&#26041;&#38754;&#65292;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#26412;&#25991;&#23383;&#31215;&#26497;&#24230;&#65292;&#21457;&#29616;GPT&#20174;&#33521;&#35821;&#65288;E-X&#65289;&#32763;&#35793;&#30340;&#25991;&#26412;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#24403;&#32763;&#35793;&#21477;&#23376;&#38271;&#24230;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#24046;&#21035;&#23601;&#23588;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating senten
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;ChipGPT&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#35774;&#35745;&#29615;&#22659;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#29983;&#25104;&#30828;&#20214;&#36923;&#36753;&#35774;&#35745;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20154;&#24037;&#35774;&#35745;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#19988;&#21487;&#33410;&#30465;&#36229;&#36807;75&#65285;&#30340;&#32534;&#30721;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.14019</link><description>&lt;p&gt;
ChipGPT: &#36828;&#31163;&#33258;&#28982;&#35821;&#35328;&#30828;&#20214;&#35774;&#35745;&#36824;&#26377;&#22810;&#36828;
&lt;/p&gt;
&lt;p&gt;
ChipGPT: How far are we from natural language hardware design. (arXiv:2305.14019v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;ChipGPT&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#35774;&#35745;&#29615;&#22659;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#29983;&#25104;&#30828;&#20214;&#36923;&#36753;&#35774;&#35745;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20154;&#24037;&#35774;&#35745;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#19988;&#21487;&#33410;&#30465;&#36229;&#36807;75&#65285;&#30340;&#32534;&#30721;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#22120;&#26234;&#33021;&#65292;&#23427;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26469;&#21327;&#21161;&#30828;&#20214;&#24037;&#31243;&#24072;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#36923;&#36753;&#35774;&#35745;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#26497;&#20339;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;LLMs&#21327;&#21161;&#30828;&#20214;&#35774;&#35745;&#36807;&#31243;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#23581;&#35797;&#28436;&#31034;&#19968;&#20010;&#33258;&#21160;&#21270;&#35774;&#35745;&#29615;&#22659;&#65292;&#35813;&#29615;&#22659;&#21033;&#29992;LLMs&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#29983;&#25104;&#30828;&#20214;&#36923;&#36753;&#35774;&#35745;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#26131;&#29992;&#19988;&#26356;&#39640;&#25928;&#30340;&#33455;&#29255;&#24320;&#21457;&#27969;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#21487;&#25193;&#23637;&#30340;&#22235;&#38454;&#27573;&#38646;&#20195;&#30721;&#36923;&#36753;&#35774;&#35745;&#26694;&#26550;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#39318;&#20808;&#65292;&#28436;&#31034;&#29256;&#26412;ChipGPT&#36890;&#36807;&#20026;LLM&#29983;&#25104;&#25552;&#31034;&#24320;&#22987;&#65292;&#28982;&#21518;&#20135;&#29983;&#21021;&#22987;Verilog&#31243;&#24207;&#12290; &#20854;&#27425;&#65292;&#36755;&#20986;&#31649;&#29702;&#22120;&#32416;&#27491;&#21644;&#20248;&#21270;&#36825;&#20123;&#31243;&#24207;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#25910;&#38598;&#21040;&#26368;&#32456;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#12290;&#26368;&#21518;&#65292;ChipGPT&#23558;&#22312;&#27492;&#31354;&#38388;&#20013;&#25628;&#32034;&#20197;&#36873;&#25321;&#31526;&#21512;&#30446;&#26631;&#25351;&#26631;&#30340;&#26368;&#20248;&#35774;&#35745;&#12290;&#35780;&#20272;&#34920;&#26126;&#65292;&#30001;ChipGPT&#35774;&#35745;&#30340;&#36923;&#36753;&#30005;&#36335;&#30340;&#24615;&#33021;&#19982;&#20154;&#24037;&#35774;&#35745;&#30340;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#25972;&#20010;&#36807;&#31243;&#33410;&#30465;&#20102;&#36229;&#36807;75&#65285;&#30340;&#32534;&#30721;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) like ChatGPT exhibited unprecedented machine intelligence, it also shows great performance in assisting hardware engineers to realize higher-efficiency logic design via natural language interaction. To estimate the potential of the hardware design process assisted by LLMs, this work attempts to demonstrate an automated design environment that explores LLMs to generate hardware logic designs from natural language specifications. To realize a more accessible and efficient chip development flow, we present a scalable four-stage zero-code logic design framework based on LLMs without retraining or finetuning. At first, the demo, ChipGPT, begins by generating prompts for the LLM, which then produces initial Verilog programs. Second, an output manager corrects and optimizes these programs before collecting them into the final design space. Eventually, ChipGPT will search through this space to select the optimal design under the target metrics. The evaluation sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#37325;&#22609;&#37096;&#20998;&#36890;&#36947;&#20026;&#25209;&#22788;&#29702;&#32500;&#24230;&#24182;&#23558;&#36890;&#36947;&#20998;&#32452;&#65292;&#20197;&#22686;&#21152;&#31354;&#38388;&#35821;&#20041;&#20998;&#24067;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#20132;&#21449;&#32500;&#24230;&#20132;&#20114;&#32858;&#21512;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#30340;&#36755;&#20986;&#29305;&#24449;&#12290;&#23454;&#39564;&#34920;&#26126;EMA&#21487;&#20197;&#39640;&#25928;&#22320;&#20248;&#21270;&#24615;&#33021;&#65292;&#27604;&#20043;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.13563</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#39640;&#25928;&#20132;&#21449;&#31354;&#38388;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-Scale Attention Module with Cross-Spatial Learning. (arXiv:2305.13563v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#37325;&#22609;&#37096;&#20998;&#36890;&#36947;&#20026;&#25209;&#22788;&#29702;&#32500;&#24230;&#24182;&#23558;&#36890;&#36947;&#20998;&#32452;&#65292;&#20197;&#22686;&#21152;&#31354;&#38388;&#35821;&#20041;&#20998;&#24067;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#20132;&#21449;&#32500;&#24230;&#20132;&#20114;&#32858;&#21512;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#30340;&#36755;&#20986;&#29305;&#24449;&#12290;&#23454;&#39564;&#34920;&#26126;EMA&#21487;&#20197;&#39640;&#25928;&#22320;&#20248;&#21270;&#24615;&#33021;&#65292;&#27604;&#20043;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#65288;EMA&#65289;&#27169;&#22359;&#65292;&#26088;&#22312;&#20445;&#30041;&#27599;&#20010;&#36890;&#36947;&#30340;&#20449;&#24687;&#21644;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#35813;&#27169;&#22359;&#23558;&#37096;&#20998;&#36890;&#36947;&#37325;&#22609;&#20026;&#25209;&#22788;&#29702;&#32500;&#24230;&#65292;&#24182;&#23558;&#36890;&#36947;&#20998;&#32452;&#25104;&#22810;&#20010;&#23376;&#29305;&#24449;&#65292;&#20174;&#32780;&#20351;&#31354;&#38388;&#35821;&#20041;&#29305;&#24449;&#22312;&#27599;&#20010;&#29305;&#24449;&#32452;&#20013;&#20998;&#24067;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22359;&#36890;&#36807;&#20132;&#21449;&#32500;&#24230;&#20132;&#20114;&#36827;&#19968;&#27493;&#32858;&#21512;&#20102;&#20004;&#20010;&#24182;&#34892;&#20998;&#25903;&#30340;&#36755;&#20986;&#29305;&#24449;&#65292;&#20197;&#25429;&#25417;&#20687;&#32032;&#32423;&#21035;&#30340;&#25104;&#23545;&#20851;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EMA&#21487;&#20197;&#22312;&#22810;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#27604;&#20043;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remarkable effectiveness of the channel or spatial attention mechanisms for producing more discernible feature representation are illustrated in various computer vision tasks. However, modeling the cross-channel relationships with channel dimensionality reduction may bring side effect in extracting deep visual representations. In this paper, a novel efficient multi-scale attention (EMA) module is proposed. Focusing on retaining the information on per channel and decreasing the computational overhead, we reshape the partly channels into the batch dimensions and group the channel dimensions into multiple sub-features which make the spatial semantic features well-distributed inside each feature group. Specifically, apart from encoding the global information to re-calibrate the channel-wise weight in each parallel branch, the output features of the two parallel branches are further aggregated by a cross-dimension interaction for capturing pixel-level pairwise relationship. We conduct exten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#957;&#20351;&#24471;&#24615;&#33021;&#26356;&#20339;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;GP&#38598;&#25104;&#25928;&#26524;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#25439;&#22833;&#39046;&#22495;&#30340;&#29289;&#29702;&#23646;&#24615;&#30340;&#25237;&#31080;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10748</link><description>&lt;p&gt;
&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#29702;&#35299;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics Inspired Approaches Towards Understanding Gaussian Processes. (arXiv:2305.10748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#957;&#20351;&#24471;&#24615;&#33021;&#26356;&#20339;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;GP&#38598;&#25104;&#25928;&#26524;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#25439;&#22833;&#39046;&#22495;&#30340;&#29289;&#29702;&#23646;&#24615;&#30340;&#25237;&#31080;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#26680;&#21487;&#20197;&#23558;&#20808;&#39564;&#26377;&#20851;&#28508;&#22312;&#20989;&#25968;&#30340;&#20449;&#24565;&#32435;&#20837;&#39640;&#26031;&#36807;&#31243;(GP)&#20013;&#20197;&#24418;&#25104;&#24402;&#32435;&#20559;&#32622;&#65292;&#20294;&#38500;&#20102;&#20869;&#26680;&#36873;&#25321;&#22806;&#65292;GP&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20173;&#28982;&#24456;&#38590;&#29702;&#35299;&#12290;&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#23545;GP&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#28436;&#31034;&#20102;Matern&#20869;&#26680;&#30340;&#957;&#36830;&#32493;&#24615;&#65292;&#24182;&#27010;&#36848;&#20102;&#26799;&#24230;&#22330;&#20851;&#38190;&#28857;&#30340;&#28798;&#21464;&#29702;&#35770;&#26041;&#38754;&#12290;&#36890;&#36807;&#23558;&#957;&#30452;&#25509;&#21253;&#21547;&#22312;Matern&#20869;&#26680;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#22312;&#25991;&#29486;&#20013;&#957;&#30340;&#20856;&#22411;&#20540;&#22686;&#21152;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#20294;&#20854;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#38750;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20107;&#20808;&#35780;&#20272;GP&#38598;&#21512;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;&#25439;&#22833;&#26223;&#35266;&#29289;&#29702;&#23646;&#24615;&#30340;&#21508;&#31181;&#25237;&#31080;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#22312;&#22810;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;GP&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#27169;&#22411;&#36873;&#25321;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior beliefs about the latent function to shape inductive biases can be incorporated into a Gaussian Process (GP) via the kernel. However, beyond kernel choices, the decision-making process of GP models remains poorly understood. In this work, we contribute an analysis of the loss landscape for GP models using methods from physics. We demonstrate $\nu$-continuity for Matern kernels and outline aspects of catastrophe theory at critical points in the loss landscape. By directly including $\nu$ in the hyperparameter optimisation for Matern kernels, we find that typical values of $\nu$ are far from optimal in terms of performance, yet prevail in the literature due to the increased computational speed. We also provide an a priori method for evaluating the effect of GP ensembles and discuss various voting approaches based on physical properties of the loss landscape. The utility of these approaches is demonstrated for various synthetic and real datasets. Our findings provide an enhanced und
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#31216;&#20854;&#20026;Gap Filler (GaFi)&#27969;&#31243;&#24182;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.10118</link><description>&lt;p&gt;
&#26550;&#36215;&#26725;&#26753;&#65306;&#36890;&#36807;&#21518;&#22788;&#29702;&#25216;&#26415;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques. (arXiv:2305.10118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#31216;&#20854;&#20026;Gap Filler (GaFi)&#27969;&#31243;&#24182;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#21644;&#27880;&#37322;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29983;&#25104;&#26367;&#20195;&#25110;&#22686;&#24378;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20854;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#38543;&#21518;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#65306;&#21160;&#24577;&#26679;&#26412;&#36807;&#28388;&#65292;&#21160;&#24577;&#25968;&#25454;&#38598;&#22238;&#25910;&#21644;&#25193;&#23637;&#25216;&#24039;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220; Gap Filler (GaFi)&#8221;&#30340;&#27969;&#31243;&#65292;&#22312;&#26368;&#20339;&#21644;&#21327;&#35843;&#30340;&#26041;&#24335;&#19979;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring and annotating suitable datasets for training deep learning models is challenging. This often results in tedious and time-consuming efforts that can hinder research progress. However, generative models have emerged as a promising solution for generating synthetic datasets that can replace or augment real-world data. Despite this, the effectiveness of synthetic data is limited by their inability to fully capture the complexity and diversity of real-world data. To address this issue, we explore the use of Generative Adversarial Networks to generate synthetic datasets for training classifiers that are subsequently evaluated on real-world images. To improve the quality and diversity of the synthetic dataset, we propose three novel post-processing techniques: Dynamic Sample Filtering, Dynamic Dataset Recycle, and Expansion Trick. In addition, we introduce a pipeline called Gap Filler (GaFi), which applies these techniques in an optimal and coordinated manner to maximise classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#25105;&#30417;&#30563;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#35299;&#20915;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10071</link><description>&lt;p&gt;
&#20919;&#21551;&#21160;&#38382;&#39064;&#65306;&#26080;&#30417;&#30563;&#30340;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cold PAWS: Unsupervised class discovery and the cold-start problem. (arXiv:2305.10071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#25105;&#30417;&#30563;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#35299;&#20915;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#38598;&#24120;&#24120;&#26159;&#19968;&#39033;&#33392;&#33510;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#30740;&#31350;&#34920;&#26126;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#26631;&#31614;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#20851;&#27880;&#22914;&#20309;&#36873;&#25321;&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#36827;&#34892;&#26631;&#35760;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#39318;&#27425;&#36873;&#25321;&#20449;&#24687;&#22270;&#20687;&#23376;&#38598;&#36827;&#34892;&#26631;&#35760;&#30340;&#25361;&#25112;&#65292;&#21363;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20960;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;CIFAR10&#12289;Imagenette&#12289;DeepWeeds&#21644;EuroSAT&#65289;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#24403;&#20351;&#29992;&#25105;&#20204;&#30340;&#26631;&#31614;&#36873;&#25321;&#31574;&#30053;&#26102;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#22343;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#22312;d&#26041;&#38754;&#33719;&#24471;&#20102;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
In many machine learning applications, labeling datasets can be an arduous and time-consuming task. Although research has shown that semi-supervised learning techniques can achieve high accuracy with very few labels within the field of computer vision, little attention has been given to how images within a dataset should be selected for labeling. In this paper, we propose a novel approach based on well-established self-supervised learning, clustering, and manifold learning techniques that address this challenge of selecting an informative image subset to label in the first instance, which is known as the cold-start or unsupervised selective labelling problem. We test our approach using several publicly available datasets, namely CIFAR10, Imagenette, DeepWeeds, and EuroSAT, and observe improved performance with both supervised and semi-supervised learning strategies when our label selection strategy is used, in comparison to random sampling. We also obtain superior performance for the d
&lt;/p&gt;</description></item><item><title>BIMT&#26041;&#27861;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#65292;&#24182;&#19988;&#33021;&#22815;&#30452;&#25509;&#23637;&#31034;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.08746</link><description>&lt;p&gt;
&#35265;&#35777;&#23601;&#26159;&#20449;&#20208;&#65306;&#33041;&#21551;&#21457;&#27169;&#22359;&#21270;&#35757;&#32451;&#20419;&#36827;&#26426;&#29702;&#35808;&#37322;
&lt;/p&gt;
&lt;p&gt;
Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability. (arXiv:2305.08746v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08746
&lt;/p&gt;
&lt;p&gt;
BIMT&#26041;&#27861;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#65292;&#24182;&#19988;&#33021;&#22815;&#30452;&#25509;&#23637;&#31034;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33041;&#21551;&#21457;&#27169;&#22359;&#21270;&#35757;&#32451;&#65288;Brain-Inspired Modular Training, BIMT&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#12290;BIMT&#20174;&#22823;&#33041;&#21463;&#21551;&#21457;&#65292;&#23558;&#31070;&#32463;&#20803;&#23884;&#20837;&#21040;&#20960;&#20309;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#25104;&#26412;&#19982;&#31070;&#32463;&#20803;&#36830;&#25509;&#38271;&#24230;&#25104;&#27491;&#27604;&#30340;&#26041;&#24335;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;BIMT&#21487;&#20197;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#21457;&#29616;&#26377;&#29992;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25581;&#31034;&#20102;&#31526;&#21495;&#20844;&#24335;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#12289;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#36793;&#30028;&#21644;&#20998;&#31867;&#29305;&#24449;&#65292;&#20197;&#21450;&#31639;&#27861;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#12290;&#30452;&#25509;&#30524;&#30555;&#30475;&#21040;&#27169;&#22359;&#30340;&#33021;&#21147;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#65292;&#20363;&#22914;&#25506;&#38024;&#65292;&#24178;&#39044;&#25110;&#20957;&#35270;&#25152;&#26377;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08040</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#27744;&#21270;&#30340;&#21487;&#35777;&#26126;&#22810;&#23454;&#20363;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26032;&#22411;&#24212;&#29992;&#65292;&#29992;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#65292;&#20854;&#20013;&#23558;&#21333;&#20010;&#31867;&#26631;&#31614;&#20998;&#37197;&#32473;&#19968;&#32452;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#24739;&#32773;&#30340;&#22810;&#20010;CT&#25195;&#25551;&#30340;&#22810;&#20010;2D&#20999;&#29255;&#65289;&#12290;&#25105;&#20204;&#22312;DAM&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;MIL&#20013;&#34987;&#24573;&#30053;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#21363;&#21253;&#22823;&#23567;&#36807;&#22823;&#65292;&#26080;&#27861;&#22312;&#21453;&#21521;&#20256;&#25773;&#26102;&#21152;&#36733;&#21040;GPU&#20869;&#23384;&#20013;&#65292;&#36825;&#26159;MIL&#26631;&#20934;&#27744;&#21270;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20851;&#20110;&#27719;&#32858;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#26500;&#36896;&#20026;&#22810;&#32423;&#32452;&#21512;&#20989;&#25968;&#12290;&#36890;&#36807;&#32508;&#21512;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#21644;&#38750;&#20984;&#26497;&#23567;&#26368;&#22823;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19988;&#21487;&#35777;&#26126;&#30340;&#22810;&#23454;&#20363;DAM&#65288;MIDAM&#65289;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#26368;&#22823;&#27744;&#21270;&#25110;&#38543;&#26426;&#27880;&#24847;&#21147;&#27744;&#21270;&#65292;&#20165;&#23545;&#27599;&#20010;&#21253;&#23545;&#24212;&#30340;&#23454;&#20363;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#26469;&#35745;&#31639; sto&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;15.27%&#12290;</title><link>http://arxiv.org/abs/2305.05228</link><description>&lt;p&gt;
&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#25552;&#21319;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic Embedded Deep Neural Network: A Generic Approach to Boost Multi-Label Image Classification Performance. (arXiv:2305.05228v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;15.27%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411;&#22312;&#20122;&#39532;&#36874;&#29983;&#20135;&#21151;&#33021;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#22522;&#20110;&#35270;&#35273;&#30340;&#26631;&#31614;&#39044;&#27979;&#65292;&#20174;&#26102;&#23578;&#23646;&#24615;&#26816;&#27979;&#21040;&#21697;&#29260;&#35782;&#21035;&#12290;&#23454;&#29616;&#36825;&#20123;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#37326;&#22806;&#35270;&#35273;&#32972;&#26223;&#20449;&#21495;&#65292;&#20854;&#20013;&#21253;&#21547;&#28151;&#28102;&#27169;&#22411;&#30340;&#26080;&#20851;&#20687;&#32032;&#65292;&#20351;&#27169;&#22411;&#38590;&#20197;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#24182;&#26681;&#25454;&#35813;&#29305;&#23450;&#21306;&#22495;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#21033;&#29992;&#23450;&#20301;&#24341;&#23548;&#65292;&#20197;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25152;&#26377;&#26631;&#31614;&#30340;AUC&#24471;&#20998;&#30340;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#20026;15.27%&#12290;&#26680;&#24515;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#28041;&#21450;&#23545;Instagram&#26102;&#23578;&#26381;&#35013;&#30340;&#22810;&#26631;&#31614;&#26102;&#23578;&#23646;&#24615;&#20998;&#31867;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained multi-label classification models have broad applications in Amazon production features, such as visual based label predictions ranging from fashion attribute detection to brand recognition. One challenge to achieve satisfactory performance for those classification tasks in real world is the wild visual background signal that contains irrelevant pixels which confuses model to focus onto the region of interest and make prediction upon the specific region. In this paper, we introduce a generic semantic- embedding deep neural network to apply the spatial awareness semantic feature incorporating a channel- wise attention based model to leverage the localization guidance to boost model performance for multi- label prediction. We observed an Avg.relative improvement of 15.27% in terms of AUC score across all labels compared to the baseline approach. Core experiment and ablation studies involve multi-label fashion attribute classification performed on Instagram fashion apparels' 
&lt;/p&gt;</description></item><item><title>FishRecGAN&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#30699;&#27491;&#40060;&#30524;&#22270;&#20687;&#24182;&#21516;&#26102;&#26657;&#20934;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#12290;&#20854;&#24555;&#36895;&#26657;&#27491;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#20998;&#36776;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#25668;&#20687;&#26426;&#22411;&#30417;&#25511;&#35774;&#22791;&#20013;&#30340;&#24658;&#23450;&#26631;&#23450;&#65292;&#24182;&#20351;&#29992;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;</title><link>http://arxiv.org/abs/2305.05222</link><description>&lt;p&gt;
FishRecGAN&#65306;&#29992;&#20110;&#40060;&#30524;&#22270;&#20687;&#30699;&#27491;&#21644;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#26631;&#23450;&#30340;&#31471;&#21040;&#31471;GAN&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration. (arXiv:2305.05222v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05222
&lt;/p&gt;
&lt;p&gt;
FishRecGAN&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#30699;&#27491;&#40060;&#30524;&#22270;&#20687;&#24182;&#21516;&#26102;&#26657;&#20934;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#12290;&#20854;&#24555;&#36895;&#26657;&#27491;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#20998;&#36776;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#25668;&#20687;&#26426;&#22411;&#30417;&#25511;&#35774;&#22791;&#20013;&#30340;&#24658;&#23450;&#26631;&#23450;&#65292;&#24182;&#20351;&#29992;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#30699;&#27491;&#40060;&#30524;&#22270;&#20687;&#24182;&#21516;&#26102;&#26657;&#20934;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#20351;&#29992;Pix2Pix GAN&#21644;Wasserstein GAN&#65288;W-Pix2PixGAN&#65289;&#24320;&#21457;&#30340;Quick Image Rectification&#27169;&#22359;&#65292;&#20197;&#21450;&#20351;&#29992;CNN&#26550;&#26500;&#30340;Calibration&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#24555;&#36895;&#26657;&#27491;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#20998;&#36776;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#25668;&#20687;&#26426;&#22411;&#30417;&#25511;&#35774;&#22791;&#20013;&#30340;&#24658;&#23450;&#26631;&#23450;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26631;&#23450;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;Quick Image Rectification&#27169;&#22359;&#20013;&#36755;&#20986;&#30340;&#30452;&#32447;&#29305;&#24449;&#20316;&#20026;&#25351;&#23548;&#26679;&#26412;&#20256;&#36882;&#32473;Calibration&#27169;&#22359;&#65292;&#20197;&#23398;&#20064;&#26657;&#27491;&#21069;&#21518;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#36879;&#35270;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#34920;&#29616;&#20986;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an end-to-end deep learning approach to rectify fisheye images and simultaneously calibrate camera intrinsic and distortion parameters. Our method consists of two parts: a Quick Image Rectification Module developed with a Pix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with a CNN architecture. Our Quick Rectification Network performs robust rectification with good resolution, making it suitable for constant calibration in camera-based surveillance equipment. To achieve high-quality calibration, we use the straightened output from the Quick Rectification Module as a guidance-like semantic feature map for the Calibration Module to learn the geometric relationship between the straightened feature and the distorted feature. We train and validate our method with a large synthesized dataset labeled with well-simulated parameters applied to a perspective image dataset. Our solution has achieved robust performance in high-resolution with a significant PSNR v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23545;&#26102;&#23578;&#26381;&#35013;&#30340;&#22270;&#20687;&#23646;&#24615;&#20998;&#31867;&#30340;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04379</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#23578;&#26816;&#27979;&#30340;&#19981;&#24179;&#34913;&#26631;&#31614;&#26679;&#26412;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Data Efficient Training with Imbalanced Label Sample Distribution for Fashion Detection. (arXiv:2305.04379v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23545;&#26102;&#23578;&#26381;&#35013;&#30340;&#22270;&#20687;&#23646;&#24615;&#20998;&#31867;&#30340;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#20110;&#35270;&#35273;&#30340;&#26631;&#31614;&#39044;&#27979;&#20197;&#21450;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#31867;&#12290;&#23454;&#29616;&#36825;&#20123;&#20219;&#21153;&#30340;&#19968;&#20010;&#20027;&#35201;&#38590;&#28857;&#26159;&#25968;&#25454;&#20998;&#24067;&#30340;&#26174;&#33879;&#19981;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#39640;&#25928;&#27169;&#22411;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38024;&#23545;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#26102;&#23578;&#26381;&#35013;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#23646;&#24615;&#20998;&#31867;&#65292;&#24182;&#19988;&#32467;&#26524;&#34920;&#26126;&#65292;&#26032;&#30340;&#21152;&#26435;&#30446;&#26631;&#20989;&#25968;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label classification models have a wide range of applications in E-commerce, including visual-based label predictions and language-based sentiment classifications. A major challenge in achieving satisfactory performance for these tasks in the real world is the notable imbalance in data distribution. For instance, in fashion attribute detection, there may be only six 'puff sleeve' clothes among 1000 products in most E-commerce fashion catalogs. To address this issue, we explore more data-efficient model training techniques rather than acquiring a huge amount of annotations to collect sufficient samples, which is neither economic nor scalable. In this paper, we propose a state-of-the-art weighted objective function to boost the performance of deep neural networks (DNNs) for multi-label classification with long-tailed data distribution. Our experiments involve image-based attribute classification of fashion apparels, and the results demonstrate favorable performance for the new weig
&lt;/p&gt;</description></item><item><title>SI-LSTM&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.03506</link><description>&lt;p&gt;
SI-LSTM: &#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#35828;&#35805;&#20154;&#28151;&#21512;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention for Emotion Recognition in Conversation. (arXiv:2305.03506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03506
&lt;/p&gt;
&lt;p&gt;
SI-LSTM&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#30340;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#23545;&#20110;&#26234;&#33021;&#21307;&#30103;&#12289;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#21644;&#32842;&#22825;&#21382;&#21490;&#35266;&#28857;&#25366;&#25496;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35828;&#35805;&#20154;&#20449;&#24687;&#22686;&#24378;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;SI-LSTM&#65289;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation~(ERC) across modalities is of vital importance for a variety of applications, including intelligent healthcare, artificial intelligence for conversation, and opinion mining over chat history. The crux of ERC is to model both cross-modality and cross-time interactions throughout the conversation. Previous methods have made progress in learning the time series information of conversation while lacking the ability to trace down the different emotional states of each speaker in a conversation. In this paper, we propose a recurrent structure called Speaker Information Enhanced Long-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states of the distinct speaker can be tracked in a sequential way to enhance the learning of the emotion in conversation. Further, to improve the learning of multimodal features in ERC, we utilize a cross-modal attention component to fuse the features between different modalities and model the interaction of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#31639;&#27861;&#21644;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00169</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#35777;&#25454;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Evidential Real-Time Multi-Mode Fault Diagnosis Approach Based on Broad Learning System. (arXiv:2305.00169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#31639;&#27861;&#21644;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22810;&#31181;&#24037;&#20917;&#34920;&#29616;&#20986;&#30340;&#38750;&#39640;&#26031;&#12289;&#22810;&#27169;&#24577;&#21644;&#20013;&#24515;&#28418;&#31227;&#29305;&#24449;&#65292;&#25925;&#38556;&#35786;&#26029;&#26159;&#24037;&#19994;&#30028;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#30446;&#21069;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#20294;&#23427;&#20204;&#22312;&#36830;&#32493;&#25925;&#38556;&#20998;&#31867;&#21644;&#25925;&#38556;&#20998;&#31867;&#22120;&#21442;&#25968;&#26356;&#26032;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#22810;&#31181;&#25805;&#20316;&#27169;&#24335;&#21644;&#23454;&#26102;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#23454;&#29616;&#24037;&#19994;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26159;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35777;&#25454;&#25512;&#29702;&#65288;ER&#65289;&#31639;&#27861;&#26469;&#34701;&#21512;&#20449;&#24687;&#24182;&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#22522;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#36825;&#20123;&#22522;&#20998;&#31867;&#22120;&#20351;&#29992;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#65288;BLS&#65289;&#24320;&#21457;&#65292;&#20197;&#25552;&#39640;&#33391;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#26102;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault diagnosis is a crucial area of research in the industry due to diverse operating conditions that exhibit non-Gaussian, multi-mode, and center-drift characteristics. Currently, data-driven approaches are the main focus in the field, but they pose challenges for continuous fault classification and parameter updates of fault classifiers, particularly in multiple operating modes and real-time settings. Therefore, a pressing issue is to achieve real-time multi-mode fault diagnosis for industrial systems. To address this problem, this paper proposes a novel approach that utilizes an evidence reasoning (ER) algorithm to fuse information and merge outputs from different base classifiers. These base classifiers are developed using a broad learning system (BLS) to improve good fault diagnosis performance. Moreover, in this approach, the pseudo-label learning method is employed to update model parameters in real-time. To demonstrate the effectiveness of the proposed approach, we perform exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120; ImageCaptioner$^2$ &#65292;&#29992;&#20110;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.04874</link><description>&lt;p&gt;
ImageCaptioner$^2$: &#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#35780;&#20272;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment. (arXiv:2304.04874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120; ImageCaptioner$^2$ &#65292;&#29992;&#20110;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#23398;&#20064;&#31995;&#32479;&#37117;&#20250;&#21463;&#21040;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#36825;&#36890;&#24120;&#26469;&#33258;&#25968;&#25454;&#12289;&#27169;&#22411;&#25110;&#20004;&#32773;&#12290;&#34913;&#37327;&#21644;&#37327;&#21270;&#20559;&#24046;&#21450;&#20854;&#26469;&#28304;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#22312;&#21253;&#25324;&#35270;&#35273;&#20449;&#21495;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#20559;&#24046;&#35780;&#20272;&#25351;&#26631;&#65292;&#31216;&#20026; ImageCaptioner$^2$&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20165;&#22522;&#20110;&#29983;&#25104;&#30340;&#23383;&#24149;&#35780;&#20272;&#22270;&#20687;&#23383;&#24149;&#31639;&#27861;&#19981;&#21516;&#65292;ImageCaptioner$^2$&#22312;&#27979;&#37327;&#20559;&#24046;&#26102;&#32771;&#34385;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20844;&#24335;&#26469;&#20316;&#20026;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26469;&#27979;&#37327;&#29983;&#25104;&#23383;&#24149;&#30340;&#20559;&#24046;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most pre-trained learning systems are known to suffer from bias, which typically emerges from the data, the model, or both. Measuring and quantifying bias and its sources is a challenging task and has been extensively studied in image captioning. Despite the significant effort in this direction, we observed that existing metrics lack consistency in the inclusion of the visual signal. In this paper, we introduce a new bias assessment metric, dubbed $ImageCaptioner^2$, for image captioning. Instead of measuring the absolute bias in the model or the data, $ImageCaptioner^2$ pay more attention to the bias introduced by the model w.r.t the data bias, termed bias amplification. Unlike the existing methods, which only evaluate the image captioning algorithms based on the generated captions only, $ImageCaptioner^2$ incorporates the image while measuring the bias. In addition, we design a formulation for measuring the bias of generated captions as prompt-based image captioning instead of using 
&lt;/p&gt;</description></item><item><title>oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17612</link><description>&lt;p&gt;
oBERTa: &#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#21644;&#21098;&#26525;&#26469;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17612
&lt;/p&gt;
&lt;p&gt;
oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;oBERTa&#35821;&#35328;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23427;&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#19994;&#32773;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;3.8&#21040;24.3&#20493;&#30340;&#26356;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;oBERTa&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#21098;&#26525;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#24037;&#20316;&#65292;&#24182;&#21033;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#65292;&#24182;&#25913;&#36827;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20197;&#22312;&#24191;&#27867;&#30340;&#20256;&#36882;&#20219;&#21153;&#19978;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#29983;&#25104;oBERTa&#26102;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;RoBERTa&#19982;BERT&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26399;&#38388;&#21098;&#26525;&#26041;&#38754;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#24494;&#35843;&#26399;&#38388;&#19981;&#22826;&#36866;&#21512;&#21387;&#32553;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;oBERTa&#22312;&#19971;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#30340;&#20351;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#36827;&#30340;&#21387;&#32553;&#25216;&#26415;&#20351;&#24471;&#32463;&#36807;&#21098;&#26525;&#30340;oBERTa&#27169;&#22411;&#33021;&#22815;&#21305;&#37197;BERTBASE&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;SQUAD V1.1&#38382;&#31572;&#25968;&#25454;&#30340;Prune OFA Large&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11249</link><description>&lt;p&gt;
&#20160;&#20040;&#35753;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65311;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#32416;&#32544;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#26469;&#33258;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#38024;&#23545;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#22312;&#26576;&#20123;&#29305;&#24449;&#30340;&#35268;&#33539;&#21010;&#20998;&#19979;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#25509;&#21463;&#20302;&#37327;&#23376;&#32416;&#32544;&#26102;&#65292;&#29305;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25165;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#35813;&#25968;&#25454;&#20998;&#24067;&#12290;&#20316;&#20026;&#26412;&#32467;&#26524;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25968;&#25454;&#20998;&#24067;&#36866;&#21512;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545;&#24191;&#27867;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#32416;&#32544;&#23558;&#40723;&#21169;&#24418;&#24335;&#25512;&#29702;&#30340;&#29289;&#29702;&#24037;&#20855;&#26469;&#36827;&#19968;&#27493;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;(STG-NRDE)&#65292;&#36890;&#36807;&#20004;&#20010;NRDE&#36827;&#34892;&#26102;&#31354;&#22788;&#29702;&#24182;&#32452;&#21512;&#36215;&#26469;&#26500;&#25104;&#19968;&#20010;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;27&#20010;&#22522;&#32447;&#27169;&#22411;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.10909</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Rough Differential Equations for Traffic Forecasting. (arXiv:2303.10909v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;(STG-NRDE)&#65292;&#36890;&#36807;&#20004;&#20010;NRDE&#36827;&#34892;&#26102;&#31354;&#22788;&#29702;&#24182;&#32452;&#21512;&#36215;&#26469;&#26500;&#25104;&#19968;&#20010;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;27&#20010;&#22522;&#32447;&#27169;&#22411;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#26102;&#31354;&#20219;&#21153;&#20043;&#19968;&#12290;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#26159;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#26102;&#31354;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#65288;STG-NRDE&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#65288;NRDE&#65289;&#30340;&#23545;&#25968;&#31614;&#21517;&#21464;&#25442;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#25442;&#20026;&#36739;&#30701;&#30340;&#29305;&#24449;&#21521;&#37327;&#24207;&#21015;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#24212;&#29992;&#20110;&#26102;&#31354;&#22788;&#29702;&#12290;&#25105;&#20204;&#23558;&#20004;&#31181;NRDE&#35774;&#35745;&#25104;&#19968;&#20010;&#26694;&#26550;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#23454;&#39564;&#25968;&#25454;&#38598;&#21253;&#25324;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;27&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;STG-NRDE&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20986;&#26368;&#20339;&#20934;&#30830;&#24615;&#65292;&#32988;&#36807;&#36825;27&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is one of the most popular spatio-temporal tasks in the field of machine learning. A prevalent approach in the field is to combine graph convolutional networks and recurrent neural networks for the spatio-temporal processing. There has been fierce competition and many novel methods have been proposed. In this paper, we present the method of spatio-temporal graph neural rough differential equation (STG-NRDE). Neural rough differential equations (NRDEs) are a breakthrough concept for processing time-series data. Their main concept is to use the log-signature transform to convert a time-series sample into a relatively shorter series of feature vectors. We extend the concept and design two NRDEs: one for the temporal processing and the other for the spatial processing. After that, we combine them into a single framework. We conduct experiments with 6 benchmark datasets and 27 baselines. STG-NRDE shows the best accuracy in all cases, outperforming all those 27 baselines 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23436;&#20840;&#26410;&#30693;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20027;&#21160;&#39034;&#24207;&#20551;&#35774;&#26816;&#39564;&#65292;&#35813;&#26041;&#27861;&#22312;&#26377;&#38480;&#21644;&#26080;&#38480;&#26102;&#38388;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#19982;Chernoff&#26816;&#39564;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10623</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#26410;&#30693;&#29615;&#22659;&#20013;&#20027;&#21160;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Active hypothesis testing in unknown environments using recurrent neural networks and model free reinforcement learning. (arXiv:2303.10623v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10623
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23436;&#20840;&#26410;&#30693;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20027;&#21160;&#39034;&#24207;&#20551;&#35774;&#26816;&#39564;&#65292;&#35813;&#26041;&#27861;&#22312;&#26377;&#38480;&#21644;&#26080;&#38480;&#26102;&#38388;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#19982;Chernoff&#26816;&#39564;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23436;&#20840;&#26410;&#30693;&#30340;&#29615;&#22659;&#20013;&#30340;&#20027;&#21160;&#39034;&#24207;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20808;&#39564;&#27010;&#29575;&#12289;&#34892;&#21160;&#21644;&#35266;&#23519;&#38598;&#12289;&#20197;&#21450;&#35266;&#23519;&#29983;&#25104;&#36807;&#31243;&#19981;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20219;&#20309;&#29615;&#22659;&#65292;&#21363;&#20351;&#23427;&#26377;&#36830;&#32493;&#30340;&#35266;&#23519;&#25110;&#34892;&#21160;&#65292;&#24182;&#22312;&#26377;&#38480;&#21644;&#26080;&#38480;&#26102;&#38388;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#19982; Chernoff &#26816;&#39564;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#26080;&#27861;&#35775;&#38382;&#29615;&#22659;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
A combination of deep reinforcement learning and supervised learning is proposed for the problem of active sequential hypothesis testing in completely unknown environments. We make no assumptions about the prior probability, the action and observation sets, and the observation generating process. Our method can be used in any environment even if it has continuous observations or actions, and performs competitively and sometimes better than the Chernoff test, in both finite and infinite horizon problems, despite not having access to the environment dynamics.
&lt;/p&gt;</description></item><item><title>LIDA&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19982;&#35821;&#27861;&#26080;&#20851;&#30340;&#21487;&#35270;&#21270;&#21644;&#20449;&#24687;&#22270;&#34920;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.02927</link><description>&lt;p&gt;
LIDA&#65306;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19982;&#35821;&#27861;&#26080;&#20851;&#30340;&#21487;&#35270;&#21270;&#19982;&#20449;&#24687;&#22270;&#34920;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models. (arXiv:2303.02927v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02927
&lt;/p&gt;
&lt;p&gt;
LIDA&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19982;&#35821;&#27861;&#26080;&#20851;&#30340;&#21487;&#35270;&#21270;&#21644;&#20449;&#24687;&#22270;&#34920;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#29992;&#25143;&#33258;&#21160;&#21019;&#24314;&#21487;&#35270;&#21270;&#30340;&#31995;&#32479;&#24517;&#39035;&#35299;&#20915;&#22810;&#20010;&#23376;&#20219;&#21153;&#8212;&#8212;&#29702;&#35299;&#25968;&#25454;&#30340;&#35821;&#20041;&#12289;&#21015;&#20030;&#30456;&#20851;&#30340;&#21487;&#35270;&#21270;&#30446;&#26631;&#20197;&#21450;&#29983;&#25104;&#21487;&#35270;&#21270;&#35268;&#33539;&#12290;&#26412;&#25991;&#23558;&#21487;&#35270;&#21270;&#29983;&#25104;&#35270;&#20026;&#19968;&#20010;&#22810;&#38454;&#27573;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#35748;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#20363;&#22914;ChatGPT/GPT-4&#65289;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;IGM&#65289;&#30340;&#33391;&#22909;&#32534;&#37197;&#30340;&#31649;&#36947;&#36866;&#21512;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LIDA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#29992;&#20110;&#29983;&#25104;&#19982;&#35821;&#27861;&#26080;&#20851;&#30340;&#21487;&#35270;&#21270;&#21644;&#20449;&#24687;&#22270;&#34920;&#30340;&#24037;&#20855;&#12290;LIDA&#30001;4&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;SUMMARIZER&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#23500;&#20294;&#32039;&#20945;&#30340;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#12289;GOAL EXPLORER&#22312;&#32473;&#23450;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21015;&#20030;&#21487;&#35270;&#21270;&#30446;&#26631;&#12289;VISGENERATOR&#29983;&#25104;&#12289;&#25913;&#36827;&#12289;&#25191;&#34892;&#21644;&#36807;&#28388;&#21487;&#35270;&#21270;&#20195;&#30721;&#65292;&#20197;&#21450;INFOGRAPHER&#20351;&#29992;IGM&#29983;&#25104;&#25968;&#25454;&#24544;&#23454;&#30340;&#39118;&#26684;&#21270;&#22270;&#24418;&#12290;LIDA&#25552;&#20379;&#20102;&#19968;&#20010;Python API&#21644;&#28151;&#21512;&#29992;&#25143;&#30028;&#38754;&#65288;&#30452;&#25509;&#25805;&#20316;&#21644;&#22810;&#27169;&#24577;&#25991;&#26412;&#36755;&#20837;&#65289;&#65292;&#20379;&#29992;&#25143;&#25351;&#23450;&#25968;&#25454;&#21644;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LIDA&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#26377;&#24847;&#20041;&#19988;&#32654;&#35266;&#30340;&#21487;&#35270;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) such as ChatGPT/GPT-4 and image generation models (IGMs) are suitable to addressing these tasks. We present LIDA, a novel tool for generating grammar-agnostic visualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER that converts data into a rich but compact natural language summary, a GOAL EXPLORER that enumerates visualization goals given the data, a VISGENERATOR that generates, refines, executes and filters visualization code and an INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA provides a python api, and a hybrid user interface (direct manipulation and mu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLGD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#30340;&#33976;&#39311;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#36845;&#20195;&#20998;&#24067;&#21305;&#37197;&#26469;&#22788;&#29702;&#21516;&#27493;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02278</link><description>&lt;p&gt;
&#22522;&#20110;&#26412;&#22320;&#20840;&#23616;&#33976;&#39311;&#30340;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#34394;&#25311;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Virtual Learning on Heterogeneous Data with Local-global Distillation. (arXiv:2303.02278v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02278
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLGD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#30340;&#33976;&#39311;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#36845;&#20195;&#20998;&#24067;&#21305;&#37197;&#26469;&#22788;&#29702;&#21516;&#27493;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#32852;&#37030;&#23398;&#20064;&#24050;&#25104;&#20026;&#20998;&#24067;&#24335;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#65292;&#20294;&#22312;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#26102;&#65292;&#20854;&#24615;&#33021;&#23481;&#26131;&#20986;&#29616;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#32852;&#37030;&#23398;&#20064;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#21516;&#27493;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#31561;&#25361;&#25112;&#12290;&#36817;&#26469;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#24050;&#34987;&#30740;&#31350;&#65292;&#20197;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20445;&#30041;&#26412;&#22320;&#31169;&#26377;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#24615;&#33021;&#30340;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;FL&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#20351;&#29992;&#33976;&#39311;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#20250;&#25918;&#22823;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#26412;&#22320;&#20840;&#23616;&#33976;&#39311;&#30340;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#34394;&#25311;&#23398;&#20064;&#65288;FedLGD&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#34394;&#25311;&#25968;&#25454;&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#32452;&#21512;&#21019;&#24314;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#22788;&#29702;&#21516;&#27493;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#20998;&#24067;&#21305;&#37197;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#20174;&#20840;&#23616;&#27169;&#22411;&#20013;&#33719;&#21462;&#30693;&#35782;&#24182;&#36890;&#36807;&#27169;&#22411;&#21453;&#39304;&#26469;&#20849;&#21516;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite Federated Learning (FL)'s trend for learning machine learning models in a distributed manner, it is susceptible to performance drops when training on heterogeneous data. In addition, FL inevitability faces the challenges of synchronization, efficiency, and privacy. Recently, dataset distillation has been explored in order to improve the efficiency and scalability of FL by creating a smaller, synthetic dataset that retains the performance of a model trained on the local private datasets. We discover that using distilled local datasets can amplify the heterogeneity issue in FL. To address this, we propose a new method, called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FedLGD), which trains FL using a smaller synthetic dataset (referred as virtual data) created through a combination of local and global dataset distillation. Specifically, to handle synchronization and class imbalance, we propose iterative distribution matching to allow clients 
&lt;/p&gt;</description></item><item><title>&#36866;&#29992;&#20110;&#24314;&#31435;&#39044;&#27979;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#30103;&#39046;&#22495;&#21644;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#30340;&#21464;&#21270;&#21644;&#20256;&#36755;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2303.01513</link><description>&lt;p&gt;
&#23398;&#20064;&#26426;&#22120;&#22312;&#21307;&#30103;&#21450;&#20854;&#20182;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning machines for health and beyond. (arXiv:2303.01513v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01513
&lt;/p&gt;
&lt;p&gt;
&#36866;&#29992;&#20110;&#24314;&#31435;&#39044;&#27979;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#30103;&#39046;&#22495;&#21644;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#30340;&#21464;&#21270;&#21644;&#20256;&#36755;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#25928;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#25797;&#38271;&#35782;&#21035;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#24320;&#21457;&#24448;&#24448;&#20572;&#30041;&#22312;&#21457;&#34920;&#35770;&#25991;&#12289;&#27010;&#24565;&#39564;&#35777;&#25110;&#36890;&#36807;&#26576;&#31181;&#37096;&#32626;&#27169;&#24335;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#39046;&#22495;&#37324;&#65292;&#27169;&#22411;&#30340;&#24739;&#32773;&#20154;&#21475;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#27169;&#22411;&#30340;&#32500;&#25252;&#21644;&#30417;&#25511;&#26159;&#30830;&#20445;&#20854;&#38271;&#26399;&#23433;&#20840;&#26377;&#25928;&#20351;&#29992;&#30340;&#20851;&#38190;&#12290;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26159;&#26377;&#25928;&#22320;&#35757;&#32451;&#20197;&#22312;&#21487;&#29992;&#25968;&#25454;&#38598;&#20013;&#23547;&#25214;&#27169;&#24335;&#30340;&#65292;&#22240;&#27492;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#19981;&#20250;&#22312;&#21457;&#34920;&#25110;&#37096;&#32626;&#26102;&#36798;&#21040;&#23792;&#20540;&#21518;&#22266;&#23450;&#19981;&#21464;&#12290;&#30456;&#21453;&#65292;&#25968;&#25454;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#21464;&#21270;&#32780;&#20135;&#29983;&#21464;&#21270;&#65292;&#32780;&#24403;&#27169;&#22411;&#34987;&#36816;&#24448;&#26032;&#30340;&#22320;&#26041;&#20379;&#26032;&#30340;&#20154;&#32676;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques are effective for building predictive models because they are good at identifying patterns in large datasets. Development of a model for complex real life problems often stops at the point of publication, proof of concept or when made accessible through some mode of deployment. However, a model in the medical domain risks becoming obsolete as soon as patient demographic changes. The maintenance and monitoring of predictive models post-publication is crucial to guarantee their safe and effective long term use. As machine learning techniques are effectively trained to look for patterns in available datasets, the performance of a model for complex real life problems will not peak and remain fixed at the point of publication or even point of deployment. Rather, data changes over time, and they also changed when models are transported to new places to be used by new demography.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DPT&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#33021;&#23454;&#29616;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;SOTA&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#19968;&#20123;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10586</link><description>&lt;p&gt;
&#20998;&#24067;&#27169;&#22411;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#22312;&#23569;&#37327;&#26631;&#31614;&#19978;&#20114;&#30456;&#21463;&#30410;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels. (arXiv:2302.10586v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DPT&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#33021;&#23454;&#29616;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;SOTA&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#19968;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#36827;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#8212;&#8212;&#21452;&#20266;&#35757;&#32451;&#65288;DPT&#65289;&#65292;&#35813;&#31574;&#30053;&#24314;&#31435;&#22312;&#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#19978;&#12290;DPT&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#20351;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#20266;&#26631;&#31614;&#65307;&#20351;&#29992;&#36825;&#20123;&#20266;&#26631;&#31614;&#35757;&#32451;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20197;&#29983;&#25104;&#20266;&#22270;&#20687;&#65307;&#24182;&#20351;&#29992;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;&#22270;&#20687;&#28151;&#21512;&#37325;&#26032;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;DPT&#22987;&#32456;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#29983;&#25104;&#21644;&#20998;&#31867;&#30340;SOTA&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#25110;&#20004;&#20010;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;ImageNet 256x256&#19978;&#65292;DPT&#30340;Fr\'echet Inception Distance&#65288;FID&#65289;&#24471;&#20998;&#20998;&#21035;&#20026;3.08&#25110;2.52&#65292;&#36229;&#36807;&#20102;&#20855;&#26377;&#23436;&#25972;&#26631;&#31614;&#30340;&#24378;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;IDDPM&#65292;CDM&#65292;ADM&#21644;LDM&#65289;&#12290;&#27492;&#22806;&#65292;DPT&#22312;ImageNet&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#31454;&#20105;&#24615;&#30340;&#21322;&#30417;&#30563;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#39030;&#32423;1&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called dual pseudo training (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images. Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\'echet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet 256x256, surpassing strong diffusion models with full labels, such as IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification tasks, achieving top-1 accuracies o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#26041;&#26696;Adap-$\tau$&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#33410;&#27599;&#20010;&#29992;&#25143;-&#27599;&#20010;&#29289;&#21697;&#23545;&#30340;&#23884;&#20837;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#24819;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#26041;&#27861;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04775</link><description>&lt;p&gt;
Adap-$\tau$:&#33258;&#36866;&#24212;&#35843;&#25972;&#23884;&#20837;&#30340;&#24133;&#24230;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Adap-$\tau$: Adaptively Modulating Embedding Magnitude for Recommendation. (arXiv:2302.04775v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#26041;&#26696;Adap-$\tau$&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#33410;&#27599;&#20010;&#29992;&#25143;-&#27599;&#20010;&#29289;&#21697;&#23545;&#30340;&#23884;&#20837;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#24819;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#26041;&#27861;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#24615;&#33021;&#36824;&#19981;&#38169;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#19968;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#8212;&#8212;&#23884;&#20837;&#24133;&#24230;&#27809;&#26377;&#26126;&#30830;&#35843;&#33410;&#65292;&#36825;&#21487;&#33021;&#21152;&#21095;&#27969;&#34892;&#24230;&#20559;&#35265;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#20570;&#20986;&#22909;&#30340;&#25512;&#33616;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#21033;&#29992;&#23884;&#20837;&#24402;&#19968;&#21270;&#26469;&#25512;&#33616;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;/&#29289;&#21697;&#23884;&#20837;&#24402;&#19968;&#21270;&#20026;&#29305;&#23450;&#20540;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#35777;&#35266;&#23519;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#24179;&#22343;9&#65285;&#65289;&#12290;&#34429;&#28982;&#36825;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#65292;&#20294;&#25105;&#20204;&#20063;&#25581;&#31034;&#20102;&#22312;&#25512;&#33616;&#20013;&#24212;&#29992;&#24402;&#19968;&#21270;&#30340;&#20005;&#37325;&#23616;&#38480;&#24615;&#8212;&#8212;&#24615;&#33021;&#39640;&#24230;&#25935;&#24863;&#20110;&#25511;&#21046;&#26631;&#20934;&#21270;&#23884;&#20837;&#27604;&#20363;&#30340;&#28201;&#24230;&#964;&#30340;&#36873;&#25321;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#24402;&#19968;&#21270;&#30340;&#20248;&#28857;&#24182;&#36991;&#20813;&#20854;&#23616;&#38480;&#24615;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#33258;&#36866;&#24212;&#35774;&#32622;&#36866;&#24403;&#30340;&#964;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;&#25512;&#33616;&#20013;&#24402;&#19968;&#21270;&#25805;&#20316;&#19982;&#20559;&#24046;-&#26041;&#24046;&#25240;&#34935;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#21517;&#20026;Adap-$\tau$&#65292;&#23427;&#21160;&#24577;&#35843;&#33410;&#27599;&#20010;&#29992;&#25143;-&#27599;&#20010;&#29289;&#21697;&#23545;&#30340;&#23884;&#20837;&#24133;&#24230;&#65292;&#26088;&#22312;&#23454;&#29616;&#29702;&#24819;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;Adap-$\tau$&#22987;&#32456;&#20248;&#20110;&#24378;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the great successes of embedding-based methods in recommender systems. Despite their decent performance, we argue one potential limitation of these methods -- the embedding magnitude has not been explicitly modulated, which may aggravate popularity bias and training instability, hindering the model from making a good recommendation. It motivates us to leverage the embedding normalization in recommendation. By normalizing user/item embeddings to a specific value, we empirically observe impressive performance gains (9\% on average) on four real-world datasets. Although encouraging, we also reveal a serious limitation when applying normalization in recommendation -- the performance is highly sensitive to the choice of the temperature $\tau$ which controls the scale of the normalized embeddings.  To fully foster the merits of the normalization while circumvent its limitation, this work studied on how to adaptively set the proper $\tau$. Towards this end, we firs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#26469;&#20445;&#25252;&#20154;&#31867;&#21019;&#36896;&#30340;&#33402;&#26415;&#21697;&#65292;&#23545;&#25239;&#20405;&#26435;&#32773;&#21033;&#29992;&#26410;&#32463;&#25480;&#26435;&#30340;&#32472;&#30011;&#35757;&#32451;DMs&#29983;&#25104;&#31867;&#20284;&#39118;&#26684;&#30340;&#26032;&#39062;&#32472;&#30011;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04578</link><description>&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#36215;&#21040;&#20102;&#31215;&#26497;&#20316;&#29992;&#65306;&#36890;&#36807;&#23545;&#25239;&#26679;&#26412;&#38450;&#27490;&#25193;&#25955;&#27169;&#22411;&#27169;&#20223;&#32472;&#30011;
&lt;/p&gt;
&lt;p&gt;
Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples. (arXiv:2302.04578v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#26469;&#20445;&#25252;&#20154;&#31867;&#21019;&#36896;&#30340;&#33402;&#26415;&#21697;&#65292;&#23545;&#25239;&#20405;&#26435;&#32773;&#21033;&#29992;&#26410;&#32463;&#25480;&#26435;&#30340;&#32472;&#30011;&#35757;&#32451;DMs&#29983;&#25104;&#31867;&#20284;&#39118;&#26684;&#30340;&#26032;&#39062;&#32472;&#30011;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#33402;&#26415;&#39046;&#22495;&#25472;&#36215;&#20102;&#19968;&#32929;&#28909;&#28526;&#65292;&#20294;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#26032;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#21363;&#20405;&#26435;&#32773;&#21033;&#29992;&#26410;&#32463;&#25480;&#26435;&#30340;&#32472;&#30011;&#35757;&#32451;DMs&#29983;&#25104;&#31867;&#20284;&#39118;&#26684;&#30340;&#26032;&#39062;&#32472;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#26032;&#20852;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#24182;&#25552;&#20986;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20445;&#25252;&#20154;&#31867;&#21019;&#36896;&#30340;&#33402;&#26415;&#21697;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#23450;&#20041;&#21644;&#35780;&#20272;DMs&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;AdvDM&#65292;&#23427;&#36890;&#36807;&#23545;&#20174;DMs&#30340;&#21453;&#21521;&#36807;&#31243;&#20013;&#25277;&#26679;&#30340;&#19981;&#21516;&#28508;&#21464;&#37327;&#36827;&#34892;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#30340;&#23545;&#25239;&#26679;&#26412;&#36827;&#34892;&#20248;&#21270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#21487;&#20197;&#26377;&#25928;&#22320;&#38459;&#30861;DMs&#25552;&#21462;&#23427;&#20204;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#20026;&#20445;&#25252;&#20154;&#31867;&#33402;&#26415;&#23478;&#29256;&#26435;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#65292;&#20197;&#23545;&#25239;&#35013;&#22791;&#26377;DMs&#30340;&#20405;&#26435;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs to generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs. Then, based on this framework, we design a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. Extensive experiments show that the generated adversarial examples can effectively hinder DMs from extracting their features. Therefore, our method can be a powerful tool for human artists to protect their copyright against infringers equipped with DM-based AI-for-A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.02209</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#30340;&#38142;&#36335;&#39044;&#27979;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Link Prediction via Relational Weisfeiler-Leman. (arXiv:2302.02209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#27169;&#22411;&#12290;&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#31616;&#21333;&#22270;&#19978;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20294;&#23545;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#30340;&#29702;&#35299;&#20173;&#28982;&#19981;&#23436;&#25972;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#31995;&#32479;&#24615;&#30340;&#29702;&#35299;&#65292;&#20197;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#31561;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#12289;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#27169;&#22411;&#65292;&#24182;&#35299;&#38145;&#20102;&#19968;&#31995;&#21015;&#20854;&#20182;&#27169;&#22411;&#12290;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#65292;&#34920;&#24449;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#20998;&#26512;&#34987;&#25193;&#23637;&#20197;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#25429;&#25417;&#30340;&#20989;&#25968;&#31867;&#36827;&#34892;&#31934;&#30830;&#36923;&#36753;&#25551;&#36848;&#12290;&#25552;&#20986;&#30340;&#29702;&#35770;&#21457;&#29616;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#65292;&#24182;&#24471;&#21040;&#20102;&#32463;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.01567</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Online Error Detection in Cyber-Physical Systems. (arXiv:2302.01567v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#24615;&#26159;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#20027;&#35201;&#30340;&#35774;&#35745;&#26631;&#20934;&#20043;&#19968;&#12290;&#36825;&#26159;&#30001;&#20110;CPS&#20013;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#20204;&#30340;&#22833;&#25928;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;CPS&#20013;&#20351;&#29992;&#24378;&#22823;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#26426;&#21046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#20256;&#32479;&#30340;&#23481;&#38169;&#26041;&#27861;&#21253;&#25324;&#20887;&#20313;&#26102;&#38388;&#12289;&#30828;&#20214;&#12289;&#20449;&#24687;&#21644;/&#25110;&#36719;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38500;&#20102;&#20302;&#38169;&#35823;&#35206;&#30422;&#29575;&#22806;&#65292;&#36824;&#20250;&#24102;&#26469;&#26497;&#22823;&#30340;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability is one of the major design criteria in Cyber-Physical Systems (CPSs). This is because of the existence of some critical applications in CPSs and their failure is catastrophic. Therefore, employing strong error detection and correction mechanisms in CPSs is inevitable. CPSs are composed of a variety of units, including sensors, networks, and microcontrollers. Each of these units is probable to be in a faulty state at any time and the occurred fault can result in erroneous output. The fault may cause the units of CPS to malfunction and eventually crash. Traditional fault-tolerant approaches include redundancy time, hardware, information, and/or software. However, these approaches impose significant overheads besides their low error coverage, which limits their applicability. In addition, the interval between error occurrence and detection is too long in these approaches. In this paper, based on Deep Reinforcement Learning (DRL), a new error detection approach is proposed that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#30340;&#24178;&#25200;&#24615;&#12290;&#20182;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#26080;&#20851;&#20449;&#24687;&#30340;&#31639;&#26415;&#25512;&#29702;&#25968;&#25454;&#38598;GSM-IC&#26469;&#34913;&#37327;&#36825;&#31181;&#21487;&#24178;&#25200;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21253;&#21547;&#26080;&#20851;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#20250;&#24613;&#21095;&#19979;&#38477;&#65292;&#20294;&#20351;&#29992;&#33258;&#25105;&#19968;&#33268;&#24615;&#36827;&#34892;&#35299;&#30721;&#24182;&#28155;&#21152;&#19968;&#20010;&#25351;&#20196;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2302.00093</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#26080;&#20851;&#19978;&#19979;&#25991;&#30340;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Easily Distracted by Irrelevant Context. (arXiv:2302.00093v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#30340;&#24178;&#25200;&#24615;&#12290;&#20182;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#26080;&#20851;&#20449;&#24687;&#30340;&#31639;&#26415;&#25512;&#29702;&#25968;&#25454;&#38598;GSM-IC&#26469;&#34913;&#37327;&#36825;&#31181;&#21487;&#24178;&#25200;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21253;&#21547;&#26080;&#20851;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#20250;&#24613;&#21095;&#19979;&#38477;&#65292;&#20294;&#20351;&#29992;&#33258;&#25105;&#19968;&#33268;&#24615;&#36827;&#34892;&#35299;&#30721;&#24182;&#28155;&#21152;&#19968;&#20010;&#25351;&#20196;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#23427;&#20204;&#20027;&#35201;&#22312;&#25152;&#26377;&#36755;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#37117;&#19982;&#35299;&#20915;&#20219;&#21153;&#30456;&#20851;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#24178;&#25200;&#24615;&#65292;&#21363;&#19981;&#30456;&#20851;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#20934;&#30830;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#26080;&#20851;&#20449;&#24687;&#30340;&#31639;&#26415;&#25512;&#29702;&#25968;&#25454;&#38598;GSM-IC&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#34913;&#37327;&#26368;&#23574;&#31471;&#30340;&#25552;&#31034;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#24178;&#25200;&#24615;&#65292;&#21457;&#29616;&#24403;&#21253;&#21547;&#26080;&#20851;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#20960;&#31181;&#32531;&#35299;&#36825;&#31181;&#19981;&#36275;&#30340;&#26041;&#27861;&#65292;&#22914;&#20351;&#29992;&#33258;&#25105;&#19968;&#33268;&#24615;&#36827;&#34892;&#35299;&#30721;&#65292;&#24182;&#22312;&#25552;&#31034;&#20013;&#28155;&#21152;&#19968;&#26465;&#25351;&#20196;&#65292;&#21578;&#35785;&#35821;&#35328;&#27169;&#22411;&#24573;&#30053;&#26080;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.13757</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#25928;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Toward Efficient Gradient-Based Value Estimation. (arXiv:2301.13757v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#23450;&#24615;&#65292;&#20294;&#36890;&#24120;&#27604;&#26102;&#38388;&#24046;&#24322;&#65288;TD&#65289;&#23398;&#20064;&#26041;&#27861;&#24930;&#24471;&#22810;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#32531;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#34920;&#26126;&#22343;&#26041;&#36125;&#23572;&#26364;&#35823;&#24046;&#65288;MSBE&#65289;&#26159;&#19968;&#31181;&#30149;&#24577;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#40657;&#22622;&#30697;&#38453;&#20855;&#26377;&#36739;&#22823;&#30340;&#26465;&#20214;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;MSBE&#30340;&#19981;&#33391;&#26465;&#20214;&#23545;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26080;&#25209;&#22788;&#29702;&#36817;&#31471;&#26041;&#27861;&#65292;&#23427;&#36817;&#20284;&#36981;&#24490;&#39640;&#26031;&#29275;&#39039;&#26041;&#21521;&#65292;&#24182;&#22312;&#21442;&#25968;&#21270;&#26041;&#38754;&#28176;&#36817;&#40065;&#26834;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#31639;&#27861;&#31216;&#20026;RANS&#65292;&#23427;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based methods for value estimation in reinforcement learning have favorable stability properties, but they are typically much slower than Temporal Difference (TD) learning methods. We study the root causes of this slowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned loss function in the sense that its Hessian has large condition-number. To resolve the adverse effect of poor conditioning of MSBE on gradient based methods, we propose a low complexity batch-free proximal method that approximately follows the Gauss-Newton direction and is asymptotically robust to parameterization. Our main algorithm, called RANS, is efficient in the sense that it is significantly faster than the residual gradient methods while having almost the same computational complexity, and is competitive with TD on the classic problems that we tested.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#32593;&#32476;&#36739;&#20302;&#23618;&#26131;&#35745;&#31639;&#34394;&#20551;&#29305;&#24449;&#65292;&#20351;&#24471;&#26356;&#39640;&#23618;&#30340;&#32593;&#32476;&#25552;&#21462;&#21644;&#21033;&#29992;&#26356;&#20016;&#23500;&#12289;&#26356;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31616;&#21333;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2301.13293</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#31579;&#36873;&#20811;&#26381;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31616;&#21333;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Overcoming Simplicity Bias in Deep Networks using a Feature Sieve. (arXiv:2301.13293v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13293
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#32593;&#32476;&#36739;&#20302;&#23618;&#26131;&#35745;&#31639;&#34394;&#20551;&#29305;&#24449;&#65292;&#20351;&#24471;&#26356;&#39640;&#23618;&#30340;&#32593;&#32476;&#25552;&#21462;&#21644;&#21033;&#29992;&#26356;&#20016;&#23500;&#12289;&#26356;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31616;&#21333;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#20559;&#24046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#20381;&#36182;&#31616;&#21333;&#19988;&#39044;&#27979;&#24615;&#36739;&#24369;&#29305;&#24449;&#30340;&#20196;&#20154;&#25285;&#24551;&#30340;&#36235;&#21183;&#65292;&#20174;&#32780;&#25490;&#38500;&#26356;&#24378;&#12289;&#26356;&#22797;&#26434;&#30340;&#29305;&#24449;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#21644;&#34394;&#20551;&#29305;&#24449;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#20102;&#20559;&#21521;&#24615;&#12289;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#12289;&#24178;&#39044;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#31616;&#21333;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29305;&#24449;&#31579;&#36873;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#33258;&#21160;&#35782;&#21035;&#21644;&#25233;&#21046;&#32593;&#32476;&#36739;&#20302;&#23618;&#30340;&#26131;&#35745;&#31639;&#34394;&#20551;&#29305;&#24449;&#65292;&#20174;&#32780;&#35753;&#26356;&#39640;&#23618;&#30340;&#32593;&#32476;&#25552;&#21462;&#21644;&#21033;&#29992;&#26356;&#20016;&#23500;&#12289;&#26356;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25511;&#21046;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#19978;&#26377;&#20851;&#26377;&#25928;&#29305;&#24449;&#19981;&#21516;&#21387;&#21046;&#21644;&#22686;&#24378;&#30340;&#20855;&#20307;&#35777;&#25454;&#65292;&#24182;&#22312;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#21435;&#20559;&#24046;&#22522;&#20934;&#27979;&#35797;&#20013;&#25253;&#21578;&#20102;&#26174;&#33879;&#24615;&#25552;&#39640;&#65288;Imagenet-A&#30456;&#23545;&#22686;&#30410;11.4&#65285;&#65307;BAR 3.2&#65285;&#31561;&#65289;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#19981;&#20381;&#36182;&#34394;&#20551;&#23646;&#24615;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplicity bias is the concerning tendency of deep networks to over-depend on simple, weakly predictive features, to the exclusion of stronger, more complex features. This is exacerbated in real-world applications by limited training data and spurious feature-label correlations, leading to biased, incorrect predictions. We propose a direct, interventional method for addressing simplicity bias in DNNs, which we call the feature sieve. We aim to automatically identify and suppress easily-computable spurious features in lower layers of the network, thereby allowing the higher network levels to extract and utilize richer, more meaningful representations. We provide concrete evidence of this differential suppression &amp; enhancement of relevant features on both controlled datasets and real-world images, and report substantial gains on many real-world debiasing benchmarks (11.4% relative gain on Imagenet-A; 3.2% on BAR, etc). Crucially, we do not depend on prior knowledge of spurious attributes
&lt;/p&gt;</description></item><item><title>AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12132</link><description>&lt;p&gt;
AutoPEFT&#65306;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#33258;&#21160;&#37197;&#32622;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12132
&lt;/p&gt;
&lt;p&gt;
AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19987;&#38376;&#30340;&#24494;&#35843;&#29992;&#20110;&#19979;&#28216;NLP&#20219;&#21153;&#65292;&#20294;&#36825;&#26679;&#30340;&#36807;&#31243;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#26368;&#36817;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#36890;&#36807;&#26356;&#26032;&#27604;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#65288;FFT&#65289;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;PEFT&#37197;&#32622;&#26041;&#38754;&#20570;&#20986;&#26126;&#26234;&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#19981;&#23481;&#26131;&#30340;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#29978;&#33267;&#26159;PEFT&#27169;&#22359;&#25554;&#20837;&#30340;&#22270;&#23618;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#25163;&#21160;&#35774;&#35745;&#37197;&#32622;&#24456;&#21487;&#33021;&#22312;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#26041;&#38754;&#26159;&#27425;&#20248;&#30340;&#12290;&#21463;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoPEFT&#26469;&#33258;&#21160;&#36873;&#25321;PEFT&#37197;&#32622;&#65306;&#39318;&#20808;&#35774;&#35745;&#20855;&#26377;&#22810;&#20010;&#20195;&#34920;&#24615;PEFT&#27169;&#22359;&#30340;&#34920;&#36798;&#37197;&#32622;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#21518;&#20351;&#29992;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#20302;&#25104;&#26412;&#30340;&#35774;&#32622;&#65292;&#20174;&#32780;&#21457;&#29616;&#20248;&#21270;&#20219;&#21153;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#30340;Pareto&#20248;&#21270;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#19978;&#35780;&#20272;&#20102;AutoPEFT&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#25163;&#21160;&#35774;&#35745;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained language models are widely used in downstream NLP tasks via task-specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating a much smaller number of parameters compared to full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: we first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimisation in a low-cost setup, we then disc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20855;&#26377;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#65292;&#36890;&#36807;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;</title><link>http://arxiv.org/abs/2301.11526</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#26377;&#30028;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30452;&#25509;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Parameterization of Lipschitz-Bounded Deep Networks. (arXiv:2301.11526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20855;&#26377;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#65292;&#36890;&#36807;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#26041;&#24335;&#65288;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#20855;&#26377;&#26377;&#38480;&#28789;&#25935;&#24230;&#30340;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#12290;&#19982;SDP&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;"&#30452;&#25509;"&#21442;&#25968;&#21270;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#30340;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed Lipschitz bounds, i.e. limited sensitivity to perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program (SDP), which does not scale to large models. In contrast to the SDP approach, we provide a ``direct'' parameterization, i.e. a smooth mapping from $\mathbb R^N$ onto the set of weights of Lipschitz-bounded networks. This enables training via standard gradient methods, without any computationally intensive projections or barrier terms. The new parameterization can equivalently be thought of as either a new layer type (the \textit{sandwich layer}), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. Finally, the comprehensive set of experiments on image classification shows that sandwich layers outperform previous approaches on
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#21017;&#21270;&#22343;&#34913;&#65292;&#21487;&#20197;&#23558;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#25277;&#35937;&#20986;&#26469;&#24182;&#20316;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2301.09159</link><description>&lt;p&gt;
&#20174;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25277;&#35937;&#20986;&#19981;&#23436;&#32654;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Abstracting Imperfect Information Away from Two-Player Zero-Sum Games. (arXiv:2301.09159v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09159
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#22343;&#34913;&#65292;&#21487;&#20197;&#23558;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#25277;&#35937;&#20986;&#26469;&#24182;&#20316;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Nayyar&#31561;&#20154;&#22312;&#20854;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#20013;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#28216;&#25103;&#36807;&#31243;&#20013;&#35753;&#29609;&#23478;&#20844;&#24320;&#23459;&#24067;&#20854;&#31574;&#30053;&#65292;&#19981;&#23436;&#32654;&#20449;&#24687;&#21487;&#20197;&#34987;&#20174;&#20849;&#21516;&#25928;&#30410;&#28216;&#25103;&#20013;&#25277;&#35937;&#20986;&#26469;&#12290;&#36825;&#20010;&#35265;&#35299;&#26159;&#25903;&#25745;&#20849;&#21516;&#25928;&#30410;&#28216;&#25103;&#21512;&#29702;&#30340;&#27714;&#35299;&#22120;&#21644;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23558;&#21516;&#26679;&#30340;&#35265;&#35299;&#31616;&#21333;&#24212;&#29992;&#20110;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#20855;&#26377;&#20844;&#24320;&#31574;&#30053;&#23459;&#24067;&#30340;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#21487;&#33021;&#19982;&#21407;&#22987;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#19981;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#21512;&#29702;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#38656;&#35201;&#22797;&#26434;&#30340;&#39069;&#22806;&#26426;&#21046;&#65292;&#20854;&#20855;&#26377;&#19981;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23637;&#31034;&#26576;&#20123;&#27491;&#21017;&#21270;&#22343;&#34913;&#19981;&#20855;&#26377;&#19978;&#36848;&#30340;&#19981;&#23545;&#24212;&#38382;&#39064;&#65292;&#22240;&#27492;&#65292;&#35745;&#31639;&#23427;&#20204;&#21487;&#20197;&#34987;&#35270;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#12290;&#22240;&#20026;&#36825;&#20123;&#27491;&#21017;&#21270;&#22343;&#34913;&#21487;&#20197;&#34987;&#26080;&#38480;&#25509;&#36817;&#32435;&#20160;&#22343;&#34913;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In their seminal work, Nayyar et al. (2013) showed that imperfect information can be abstracted away from common-payoff games by having players publicly announce their policies as they play. This insight underpins sound solvers and decision-time planning algorithms for common-payoff games. Unfortunately, a naive application of the same insight to two-player zero-sum games fails because Nash equilibria of the game with public policy announcements may not correspond to Nash equilibria of the original game. As a consequence, existing sound decision-time planning algorithms require complicated additional mechanisms that have unappealing properties. The main contribution of this work is showing that certain regularized equilibria do not possess the aforementioned non-correspondence problem -- thus, computing them can be treated as perfect-information problems. Because these regularized equilibria can be made arbitrarily close to Nash equilibria, our result opens the door to a new perspectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#24335;&#27169;&#25311;&#29615;&#22659;&#30340;&#20154;&#26426;&#21327;&#21516;&#20307;&#29616;&#26234;&#33021;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#26032;&#24179;&#21488;&#65292;&#20197;&#21450;&#20154;&#31867;&#19987;&#23478;&#25351;&#23548;&#19979;&#26426;&#22120;&#20154;&#23398;&#20064;&#34920;&#29616;&#30340;&#25552;&#39640;&#21644;&#20154;&#20307;&#31034;&#33539;&#23545;&#25163;&#26415;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2301.00452</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#20114;&#24335;&#27169;&#25311;&#29615;&#22659;&#30340;&#20154;&#26426;&#21327;&#21516;&#20307;&#29616;&#26234;&#33021;&#22312;&#22806;&#31185;&#25163;&#26415;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning. (arXiv:2301.00452v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#24335;&#27169;&#25311;&#29615;&#22659;&#30340;&#20154;&#26426;&#21327;&#21516;&#20307;&#29616;&#26234;&#33021;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#26032;&#24179;&#21488;&#65292;&#20197;&#21450;&#20154;&#31867;&#19987;&#23478;&#25351;&#23548;&#19979;&#26426;&#22120;&#20154;&#23398;&#20064;&#34920;&#29616;&#30340;&#25552;&#39640;&#21644;&#20154;&#20307;&#31034;&#33539;&#23545;&#25163;&#26415;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#22806;&#31185;&#25163;&#26415;&#26426;&#22120;&#20154;&#33258;&#21160;&#21270;&#24050;&#32463;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#39044;&#26399;&#20854;&#28508;&#21147;&#33021;&#22815;&#24800;&#21450;&#22806;&#31185;&#21307;&#29983;&#12289;&#25252;&#22763;&#21644;&#24739;&#32773;&#12290;&#26368;&#36817;&#65292;&#20855;&#26377;&#20307;&#29616;&#26234;&#33021;&#30340;&#23398;&#20064;&#33539;&#24335;&#23637;&#31034;&#20102;&#23398;&#20064;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#30340;&#33391;&#22909;&#25511;&#21046;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#20307;&#29616;&#26234;&#33021;&#27169;&#25311;&#22120;&#22312;&#20419;&#36827;&#30456;&#20851;&#30740;&#31350;&#26041;&#38754;&#21457;&#25381;&#20102;&#26680;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22806;&#31185;&#25163;&#26415;&#26426;&#22120;&#20154;&#24320;&#28304;&#27169;&#25311;&#22120;&#20173;&#26410;&#36275;&#22815;&#25903;&#25345;&#36890;&#36807;&#29289;&#29702;&#36755;&#20837;&#35774;&#22791;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#65292;&#36825;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#23545;&#20154;&#20307;&#31034;&#33539;&#22914;&#20309;&#24433;&#21709;&#31574;&#30053;&#23398;&#20064;&#30340;&#26377;&#25928;&#35843;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#24335;&#27169;&#25311;&#24179;&#21488;&#30340;&#20154;&#26426;&#21327;&#21516;&#20307;&#29616;&#26234;&#33021;&#65292;&#29992;&#20110;&#22806;&#31185;&#25163;&#26415;&#26426;&#22120;&#20154;&#30340;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;&#20808;&#21069;&#21457;&#24067;&#30340;SurRoL&#27169;&#25311;&#22120;&#30340;&#24179;&#21488;&#65292;&#24182;&#19982;&#20854;&#20182;&#24037;&#31243;&#24072;&#19968;&#36215;&#24320;&#21457;&#20102;&#20960;&#20010;&#26032;&#21151;&#33021;&#65292;&#20197;&#20801;&#35768;&#36890;&#36807;&#36755;&#20837;&#35774;&#22791;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20154;&#31867;&#19987;&#23478;&#30340;&#25351;&#23548;&#19979;&#65292;&#26426;&#22120;&#20154;&#23398;&#20064;&#34920;&#29616;&#30340;&#25552;&#39640;&#65292;&#24182;&#30830;&#23450;&#20102;&#19981;&#21516;&#31867;&#22411;&#20154;&#20307;&#31034;&#33539;&#23545;&#25163;&#26415;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25511;&#21046;&#31574;&#30053;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical robot automation has attracted increasing research interest over the past decade, expecting its potential to benefit surgeons, nurses and patients. Recently, the learning paradigm of embodied intelligence has demonstrated promising ability to learn good control policies for various complex tasks, where embodied AI simulators play an essential role to facilitate relevant research. However, existing open-sourced simulators for surgical robot are still not sufficiently supporting human interactions through physical input devices, which further limits effective investigations on how the human demonstrations would affect policy learning. In this work, we study human-in-the-loop embodied intelligence with a new interactive simulation platform for surgical robot learning. Specifically, we establish our platform based on our previously released SurRoL simulator with several new features co-developed to allow high-quality human interaction via an input device. We showcase the improveme
&lt;/p&gt;</description></item><item><title>HiTSKT&#26159;&#19968;&#31181;&#20998;&#23618;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#20250;&#35805;&#24863;&#30693;&#30693;&#35782;&#36861;&#36394;&#65292;&#33021;&#22815;&#25429;&#25417;&#23398;&#29983;&#19981;&#21516;&#20250;&#35805;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23398;&#20064;&#25216;&#33021;&#32423;&#21035;&#34920;&#31034;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#22522;&#32447;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2212.12139</link><description>&lt;p&gt;
HiTSKT&#65306;&#19968;&#31181;&#29992;&#20110;&#20250;&#35805;&#24863;&#30693;&#30693;&#35782;&#36861;&#36394;&#30340;&#20998;&#23618;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
HiTSKT: A Hierarchical Transformer Model for Session-Aware Knowledge Tracing. (arXiv:2212.12139v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12139
&lt;/p&gt;
&lt;p&gt;
HiTSKT&#26159;&#19968;&#31181;&#20998;&#23618;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#20250;&#35805;&#24863;&#30693;&#30693;&#35782;&#36861;&#36394;&#65292;&#33021;&#22815;&#25429;&#25417;&#23398;&#29983;&#19981;&#21516;&#20250;&#35805;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23398;&#20064;&#25216;&#33021;&#32423;&#21035;&#34920;&#31034;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#22522;&#32447;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36319;&#36394;(KT)&#26088;&#22312;&#21033;&#29992;&#23398;&#29983;&#30340;&#23398;&#20064;&#21382;&#21490;&#26469;&#20272;&#35745;&#20182;&#20204;&#22312;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25216;&#33021;&#19978;&#30340;&#25484;&#25569;&#27700;&#24179;&#65292;&#20174;&#32780;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#30456;&#24212;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#20316;&#20026;&#20026;&#22312;&#32447;&#25945;&#32946;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#30340;&#37325;&#35201;&#26041;&#24335;&#65292;KT&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23398;&#29983;&#30340;&#23398;&#20064;&#21382;&#21490;&#26159;&#30001;&#19968;&#32452;&#38598;&#20013;&#30340;&#38382;&#39064;&#31572;&#26696;&#32452;&#25104;&#30340;&#65292;&#27599;&#20010;&#38382;&#39064;&#38598;&#34987;&#31216;&#20026;&#19968;&#20010;&#20250;&#35805;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29420;&#31435;&#31572;&#26696;&#30340;&#24207;&#21015;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23398;&#29983;&#30340;&#23398;&#20064;&#21160;&#24577;&#21487;&#20197;&#22312;&#36825;&#20123;&#20250;&#35805;&#20013;&#20869;&#37096;&#21644;&#36328;&#20250;&#35805;&#20043;&#38388;&#38750;&#24120;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#20250;&#35805;&#20869;&#37096;&#21644;&#36328;&#20250;&#35805;&#27169;&#25311;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#21160;&#24577;&#23545;&#20110;&#22788;&#29702;KT&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;KT&#27169;&#22411;&#23558;&#23398;&#29983;&#30340;&#23398;&#20064;&#35760;&#24405;&#35270;&#20026;&#21333;&#20010;&#36830;&#32493;&#24207;&#21015;&#65292;&#32780;&#27809;&#26377;&#25429;&#25417;&#23398;&#29983;&#30693;&#35782;&#29366;&#24577;&#30340;&#20250;&#35805;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;Transformer&#27169;&#22411;&#65292;&#31216;&#20026;HiTSKT&#65292;&#20197;&#20998;&#23618;&#30340;&#26041;&#24335;&#34920;&#31034;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#65292;&#20225;&#19994;&#32423;&#34920;&#31034;&#21644;&#25216;&#33021;&#32423;&#34920;&#31034;&#65292;&#20998;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;HiTSKT&#39318;&#20808;&#21033;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#19981;&#21516;&#20250;&#35805;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#28982;&#21518;&#22522;&#20110;&#20250;&#35805;&#32423;&#21035;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#33021;&#32423;&#21035;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HiTSKT&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge tracing (KT) aims to leverage students' learning histories to estimate their mastery levels on a set of pre-defined skills, based on which the corresponding future performance can be accurately predicted. As an important way of providing personalized experience for online education, KT has gained increased attention in recent years. In practice, a student's learning history comprises answers to sets of massed questions, each known as a session, rather than merely being a sequence of independent answers. Theoretically, within and across these sessions, students' learning dynamics can be very different. Therefore, how to effectively model the dynamics of students' knowledge states within and across the sessions is crucial for handling the KT problem. Most existing KT models treat student's learning records as a single continuing sequence, without capturing the sessional shift of students' knowledge state. To address the above issue, we propose a novel hierarchical transformer m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;VISOR&#20197;&#34913;&#37327;&#29983;&#25104;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;&#24403;&#21069;T2I&#27169;&#22411;&#23613;&#31649;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#20854;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#31354;&#38388;&#35859;&#35789;&#21644;&#22330;&#26223;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2212.10015</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Spatial Relationships in Text-to-Image Generation. (arXiv:2212.10015v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;VISOR&#20197;&#34913;&#37327;&#29983;&#25104;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;&#24403;&#21069;T2I&#27169;&#22411;&#23613;&#31649;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#20854;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#31354;&#38388;&#35859;&#35789;&#21644;&#22330;&#26223;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#29702;&#35299;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#23545;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#22270;&#20687;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#26159;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#36924;&#30495;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#38752;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;T2I&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;VISOR&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#25429;&#25417;&#20102;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#31354;&#38388;&#20851;&#31995;&#22312;&#22270;&#20687;&#20013;&#26159;&#21542;&#20934;&#30830;&#29983;&#25104;&#12290;&#20026;&#20102;&#22522;&#20934;&#29616;&#26377;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#25551;&#36848;&#20004;&#20010;&#23545;&#35937;&#21450;&#23427;&#20204;&#20043;&#38388;&#31354;&#38388;&#20851;&#31995;&#30340;&#21477;&#23376;&#25968;&#25454;&#38598;SR2D&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#35780;&#20272;&#27969;&#31243;&#26469;&#35782;&#21035;&#29289;&#20307;&#21450;&#20854;&#31354;&#38388;&#20851;&#31995;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#35780;&#20272;T2I&#27169;&#22411;&#26102;&#37319;&#29992;&#23427;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#20063;&#23601;&#26159;&#23613;&#31649;&#26368;&#26032;&#30340;T2I&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#23427;&#20204;&#29983;&#25104;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#31354;&#38388;&#35859;&#35789;&#65288;&#22914;'&#22312;&#21069;&#38754;'&#21644;'&#22312;&#21518;&#38754;'&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#22330;&#26223;&#30340;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#20063;&#26377;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial understanding is a fundamental aspect of computer vision and integral for human-level reasoning about images, making it an important component for grounded language understanding. While recent text-to-image synthesis (T2I) models have shown unprecedented improvements in photorealism, it is unclear whether they have reliable spatial understanding capabilities. We investigate the ability of T2I models to generate correct spatial relationships among objects and present VISOR, an evaluation metric that captures how accurately the spatial relationship described in text is generated in the image. To benchmark existing models, we introduce a dataset, SR2D, that contains sentences describing two objects and the spatial relationship between them. We construct an automated evaluation pipeline to recognize objects and their spatial relationships, and employ it in a large-scale evaluation of T2I models. Our experiments reveal a surprising finding that, although state-of-the-art T2I models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01692</link><description>&lt;p&gt;
&#22312;&#22330;&#23398;&#20064;&#32773;&#33021;&#21542;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#25512;&#29702;&#27010;&#24565;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22330;&#23398;&#20064;&#32773;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#20182;&#20204;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#22914;&#26631;&#31614;&#30340;&#24773;&#24863;&#65292;&#32780;&#19981;&#26159;&#22312;&#36755;&#20837;&#20013;&#25214;&#21040;&#26032;&#30340;&#20851;&#32852;&#24615;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#23569;&#26679;&#26412;&#35780;&#20272;&#35774;&#32622;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#30340;&#22312;&#22330;&#28436;&#31034;&#26080;&#27861;&#21306;&#20998;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#24182;&#19981;&#21576;&#29616;&#36229;&#36234;&#26292;&#38706;&#20110;&#26032;&#20219;&#21153;&#20998;&#24067;&#30340;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#12290;&#25105;&#20204;&#20174;&#27880;&#37322;&#35299;&#37322;&#20013;&#25552;&#21462;&#20102;&#19968;&#32452;&#36825;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#23637;&#31034;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#33719;&#24471;&#22810;&#23569;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.  To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in f
&lt;/p&gt;</description></item><item><title>SmoothQuant&#26159;&#19968;&#31181;&#35757;&#32451;&#26080;&#38656;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;8&#20301;&#26435;&#37325;&#12289;8&#20301;&#28608;&#27963;&#65288;W8A8&#65289;&#37327;&#21270;&#12290;SmoothQuant&#36890;&#36807;&#25968;&#23398;&#31561;&#25928;&#36716;&#25442;&#23558;&#37327;&#21270;&#38590;&#24230;&#20174;&#28608;&#27963;&#31227;&#21040;&#26435;&#37325;&#65292;&#20351;&#24471;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;INT8&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#65292;&#20855;&#26377;&#26368;&#39640;1.56&#20493;&#21152;&#36895;&#21644;2&#20493;&#20869;&#23384;&#20943;&#23569;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.10438</link><description>&lt;p&gt;
SmoothQuant&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#39640;&#25928;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. (arXiv:2211.10438v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10438
&lt;/p&gt;
&lt;p&gt;
SmoothQuant&#26159;&#19968;&#31181;&#35757;&#32451;&#26080;&#38656;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;8&#20301;&#26435;&#37325;&#12289;8&#20301;&#28608;&#27963;&#65288;W8A8&#65289;&#37327;&#21270;&#12290;SmoothQuant&#36890;&#36807;&#25968;&#23398;&#31561;&#25928;&#36716;&#25442;&#23558;&#37327;&#21270;&#38590;&#24230;&#20174;&#28608;&#27963;&#31227;&#21040;&#26435;&#37325;&#65292;&#20351;&#24471;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;INT8&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#65292;&#20855;&#26377;&#26368;&#39640;1.56&#20493;&#21152;&#36895;&#21644;2&#20493;&#20869;&#23384;&#20943;&#23569;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21644;&#20869;&#23384;&#12290;&#37327;&#21270;&#21487;&#20197;&#20943;&#23569;&#20869;&#23384;&#24182;&#21152;&#36895;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22312;&#20445;&#25345;&#31934;&#24230;&#21644;&#30828;&#20214;&#25928;&#29575;&#30340;&#21516;&#26102;&#32500;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothQuant&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#12289;&#20445;&#25345;&#31934;&#24230;&#21644;&#36890;&#29992;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;LLMs&#30340;8&#20301;&#26435;&#37325;&#12289;8&#20301;&#28608;&#27963;&#65288;W8A8&#65289;&#37327;&#21270;&#12290;&#22522;&#20110;&#26435;&#37325;&#26131;&#20110;&#37327;&#21270;&#32780;&#28608;&#27963;&#19981;&#26131;&#37327;&#21270;&#30340;&#20107;&#23454;&#65292;SmoothQuant&#36890;&#36807;&#25968;&#23398;&#31561;&#25928;&#36716;&#25442;&#23558;&#37327;&#21270;&#38590;&#24230;&#20174;&#28608;&#27963;&#31227;&#33267;&#26435;&#37325;&#65292;&#36890;&#36807;&#31163;&#32447;&#24179;&#28369;&#28608;&#27963;&#30340;&#24322;&#24120;&#20540;&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;SmoothQuant&#20351;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;INT8&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;OPT&#12289;BLOOM&#12289;GLM&#12289;MT-NLG&#21644;LLaMA&#31995;&#21015;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;LLMs&#30340;&#26368;&#39640;1.56&#20493;&#21152;&#36895;&#21644;2&#20493;&#20869;&#23384;&#20943;&#23569;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#20250;&#26377;&#31934;&#24230;&#25439;&#22833;&#12290;SmoothQuant&#21487;&#20197;&#20026;530B LLM&#25552;&#20379;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM wi
&lt;/p&gt;</description></item><item><title>&#26412;&#23454;&#35777;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#19981;&#21516;&#20998;&#24067;&#20559;&#31227;&#19979;&#36229;&#36807;20,000&#20010;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#65288;&#22914;Adam&#21644;Adagrad&#65289;&#21450;&#21487;&#20197;&#20943;&#23569;&#26102;&#38388;&#30456;&#20851;&#29305;&#24449;&#30340;&#20248;&#21270;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.08583</link><description>&lt;p&gt;
&#20851;&#20110;&#20248;&#21270;&#22120;&#36873;&#25321;&#25552;&#39640;&#27169;&#22411;&#23545;&#20998;&#24067;&#20284;&#28982;&#23567;&#21464;&#21270;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical Study on Optimizer Selection for Out-of-Distribution Generalization. (arXiv:2211.08583v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#23454;&#35777;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#19981;&#21516;&#20998;&#24067;&#20559;&#31227;&#19979;&#36229;&#36807;20,000&#20010;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#65288;&#22914;Adam&#21644;Adagrad&#65289;&#21450;&#21487;&#20197;&#20943;&#23569;&#26102;&#38388;&#30456;&#20851;&#29305;&#24449;&#30340;&#20248;&#21270;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#31245;&#26377;&#19981;&#21516;&#26102;&#65292;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#20102;&#24456;&#22810;&#26377;&#21069;&#36884;&#30340;&#24037;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23545;&#20248;&#21270;&#22120;&#21450;&#20854;&#22312;&#20998;&#24067;&#22806;&#27867;&#21270;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#23578;&#26410;&#36827;&#34892;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#19979;&#65292;&#20351;&#29992;DomainBed&#65292;WILDS&#21644;Backgrounds Challenge&#20998;&#21035;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#30740;&#31350;&#19981;&#21516;&#31867;&#22411;&#20559;&#31227;&#65288;&#21363;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#21464;&#21270;&#65289;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#33539;&#22260;&#25628;&#32034;&#24182;&#27979;&#35797;&#36229;&#36807;20,000&#20010;&#27169;&#22411;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#65288;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65292;&#25105;&#20204;&#30456;&#20449;&#23545;&#23454;&#36341;&#32773;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;i) &#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914; Adam &#21644; Adagrad&#65289;&#20855;&#26377;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#12290;ii) &#24403;&#20559;&#31227;&#26377;&#24456;&#24378;&#30340;&#26102;&#38388;&#23616;&#37096;&#24615;&#26102;&#65292;&#33021;&#22815;&#20943;&#23569;&#26102;&#38388;&#30456;&#20851;&#29305;&#24449;&#30340;&#20248;&#21270;&#22120;&#20855;&#26377;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#65292;&#32780;&#24403;&#20559;&#31227;&#26377;&#24378;&#30340;&#26102;&#38388;&#25972;&#20307;&#24615;&#26102;&#21017;&#27809;&#26377;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep learning systems do not generalize well when the test data distribution is slightly different to the training data distribution. While much promising work has been accomplished to address this fragility, a systematic study of the role of optimizers and their out-of-distribution generalization performance has not been undertaken. In this study, we examine the performance of popular first-order optimizers for different classes of distributional shift under empirical risk minimization and invariant risk minimization. We address this question for image and text classification using DomainBed, WILDS, and Backgrounds Challenge as testbeds for studying different types of shifts -- namely correlation and diversity shift. We search over a wide range of hyperparameters and examine classification accuracy (in-distribution and out-of-distribution) for over 20,000 models. We arrive at the following findings, which we expect to be helpful for practitioners: i) adaptive optimizers (e.g., 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#35777;&#20102;&#26597;&#35810;&#20844;&#24179;&#25351;&#26631;&#21487;&#33021;&#20250;&#27844;&#38706;&#21463;&#20445;&#25252;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2211.02139</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#28369;&#25935;&#24863;&#24230;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#20559;&#24046;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity. (arXiv:2211.02139v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#35777;&#20102;&#26597;&#35810;&#20844;&#24179;&#25351;&#26631;&#21487;&#33021;&#20250;&#27844;&#38706;&#21463;&#20445;&#25252;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#27861;&#35268;&#31105;&#27490;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;&#21463;&#20445;&#25252;&#23646;&#24615;&#65288;&#24615;&#21035;&#65292;&#31181;&#26063;&#31561;&#65289;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#22312;&#19981;&#30693;&#36947;&#20182;&#20204;&#30340;&#21463;&#20445;&#25252;&#32452;&#30340;&#24773;&#20917;&#19979;&#23545;&#20154;&#32676;&#36827;&#34892;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26426;&#26500;&#36890;&#24120;&#37319;&#29992;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#65288;&#19981;&#21487;&#35775;&#38382;&#21463;&#20445;&#25252;&#23646;&#24615;&#35757;&#32451;&#27169;&#22411;&#65289;&#21644;&#21512;&#35268;&#22242;&#38431;&#65288;&#21487;&#33021;&#20840;&#38754;&#35775;&#38382;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#23457;&#35745;&#30446;&#30340;&#65289;&#20043;&#38388;&#30340;&#20998;&#31163;&#12290;&#20294;&#26159;&#65292;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#21487;&#33021;&#34987;&#20801;&#35768;&#36890;&#36807;&#26597;&#35810;&#21512;&#35268;&#22242;&#38431;&#33719;&#21462;&#32452;&#20844;&#24179;&#24615;&#25351;&#26631;&#26469;&#27979;&#35797;&#20854;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#39318;&#20808;&#35777;&#26126;&#20102;&#20165;&#20165;&#26597;&#35810;&#20844;&#24179;&#25351;&#26631;&#65288;&#20363;&#22914;&#32479;&#35745;&#24179;&#31561;&#21644;&#24179;&#31561;&#36180;&#29575;&#65289;&#21487;&#33021;&#20250;&#27844;&#38706;&#20010;&#20154;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#32473;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#24635;&#26159;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#26597;&#35810;&#20174;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#30446;&#26631;&#20010;&#20307;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#12290;&#25105;&#20204;&#29305;&#21035;&#23637;&#31034;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23427;&#21487;&#20197;&#24179;&#28369;&#22320;&#20943;&#23567;&#25935;&#24863;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regulations prohibit model developers from accessing protected attributes (gender, race, etc.), often resulting in fairness assessments on populations without knowing their protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset for auditing purposes). However, the model developers might be allowed to test their models for bias by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. In particular, we show that one can rec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;&#30340;PUC&#27010;&#24565;&#65292;&#20026;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2210.13954</link><description>&lt;p&gt;
&#25105;&#19981;&#24819;&#35828;&#65306;&#22312;&#21487;&#36873;&#20010;&#20154;&#25968;&#25454;&#27169;&#22411;&#20013;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;
&lt;/p&gt;
&lt;p&gt;
I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data. (arXiv:2210.13954v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;&#30340;PUC&#27010;&#24565;&#65292;&#20026;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#65292;&#36825;&#22312;&#29616;&#20195;&#20445;&#38505;&#23450;&#20215;&#27169;&#22411;&#20013;&#24456;&#24120;&#35265;&#12290;&#19968;&#20123;&#29992;&#25143;&#21516;&#24847;&#20351;&#29992;&#20182;&#20204;&#30340;&#25968;&#25454;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#21453;&#23545;&#24182;&#20445;&#25345;&#20854;&#25968;&#25454;&#26410;&#20844;&#24320;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#20915;&#23450;&#26412;&#36523;&#21487;&#20197;&#34987;&#35270;&#20026;&#20449;&#24687;&#65292;&#24212;&#35813;&#21463;&#21040;&#20445;&#25252;&#65292;&#20197;&#23562;&#37325;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#24341;&#21457;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#30830;&#20445;&#20445;&#25252;&#20854;&#20010;&#20154;&#25968;&#25454;&#30340;&#29992;&#25143;&#19981;&#20250;&#22240;&#27492;&#21463;&#21040;&#20219;&#20309;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#20165;&#20351;&#29992;&#33719;&#24471;&#31215;&#26497;&#29992;&#25143;&#21516;&#24847;&#30340;&#20449;&#24687;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20445;&#25252;&#35201;&#27714;&#30340;&#27491;&#24335;&#21270;&#12290;&#36825;&#25490;&#38500;&#20102;&#20316;&#20986;&#20849;&#20139;&#25968;&#25454;&#19982;&#21542;&#20915;&#23450;&#25152;&#21253;&#21547;&#30340;&#38544;&#21547;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Protected User Consent (PUC)&#27010;&#24565;&#65292;&#36825;&#26159;&#25105;&#20204;&#35777;&#26126;&#22312;&#20445;&#25252;&#35201;&#27714;&#19979;&#25439;&#22833;&#26368;&#23567;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine machine learning models in a setup where individuals have the choice to share optional personal information with a decision-making system, as seen in modern insurance pricing models. Some users consent to their data being used whereas others object and keep their data undisclosed. In this work, we show that the decision not to share data can be considered as information in itself that should be protected to respect users' privacy. This observation raises the overlooked problem of how to ensure that users who protect their personal data do not suffer any disadvantages as a result. To address this problem, we formalize protection requirements for models which only use the information for which active user consent was obtained. This excludes implicit information contained in the decision to share data or not. We offer the first solution to this problem by proposing the notion of Protected User Consent (PUC), which we prove to be loss-optimal under our protection requirement. To
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#38376;&#25511;&#26426;&#21046;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#21035;&#24314;&#27169;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#21644;&#20020;&#24202;&#31508;&#35760;&#34920;&#24449;&#65292;&#36827;&#32780;&#36890;&#36807;&#20132;&#26367;&#27880;&#24847;&#21147;&#26426;&#21046;&#25972;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.12156</link><description>&lt;p&gt;
&#36890;&#36807;&#24314;&#27169;&#19981;&#35268;&#21017;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Predictions by Irregular Multimodal Electronic Health Records Modeling. (arXiv:2210.12156v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#38376;&#25511;&#26426;&#21046;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#21035;&#24314;&#27169;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#21644;&#20020;&#24202;&#31508;&#35760;&#34920;&#24449;&#65292;&#36827;&#32780;&#36890;&#36807;&#20132;&#26367;&#27880;&#24847;&#21147;&#26426;&#21046;&#25972;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405; (EHRs) &#36890;&#24120;&#21253;&#21547;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#21644;&#38271;&#30340;&#20020;&#24202;&#31508;&#35760;&#24207;&#21015;&#65292;&#36825;&#20123;&#25968;&#25454;&#37117;&#26159;&#22312;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#20869;&#37319;&#38598;&#30340;&#12290;&#22914;&#20309;&#22788;&#29702;&#27599;&#20010;&#27169;&#24577;&#20013;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#22810;&#27169;&#24577;&#34920;&#31034;&#20013;&#20197;&#25913;&#36827;&#21307;&#30103;&#39044;&#27979;&#65292;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#20197;&#19979;&#20004;&#31181;&#26041;&#24335;&#22788;&#29702;&#21333;&#20010;&#27169;&#24577;&#20013;&#30340;&#19981;&#35268;&#21017;&#24615;&#65306;(1)&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#21160;&#24577;&#22320;&#23558;&#25163;&#24037;&#21046;&#20316;&#30340;&#22635;&#20805;&#23884;&#20837;&#24335;&#34920;&#24449;&#19982;&#23398;&#20064;&#21040;&#30340;&#25554;&#20540;&#23884;&#20837;&#24335;&#34920;&#24449;&#30456;&#32467;&#21512;&#26469;&#24314;&#27169;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;(2)&#36890;&#36807;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#19968;&#31995;&#21015;&#20020;&#24202;&#31508;&#35760;&#34920;&#24449;&#36716;&#25442;&#25104;&#22810;&#21464;&#37327;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#35299;&#20915;&#19981;&#35268;&#21017;&#24615;&#12290;&#23545;&#20110;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#36328;&#36234;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#24443;&#24213;&#23545;&#22810;&#27169;&#24577;&#19981;&#35268;&#21017;&#24615;&#36827;&#34892;&#24314;&#27169;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health conditions among patients in intensive care units (ICUs) are monitored via electronic health records (EHRs), composed of numerical time series and lengthy clinical note sequences, both taken at irregular time intervals. Dealing with such irregularity in every modality, and integrating irregularity into multimodal representations to improve medical predictions, is a challenging problem. Our method first addresses irregularity in each single modality by (1) modeling irregular time series by dynamically incorporating hand-crafted imputation embeddings into learned interpolation embeddings via a gating mechanism, and (2) casting a series of clinical note representations as multivariate irregular time series and tackling irregularity via a time attention mechanism. We further integrate irregularity in multimodal fusion with an interleaved attention mechanism across temporal steps. To the best of our knowledge, this is the first work to thoroughly model irregularity in multimodalities
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23376;&#38598;&#23618;&#65288;&#25163;&#26415;&#24494;&#35843;&#65289;&#22312;&#36866;&#24212;&#20998;&#24067;&#20559;&#31227;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#36824;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#22312;&#29702;&#24819;&#29615;&#22659;&#19979;&#65292;&#25163;&#26415;&#24494;&#35843;&#21487;&#20197;&#20248;&#20110;&#20840;&#23618;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2210.11466</link><description>&lt;p&gt;
&#25163;&#26415;&#24494;&#35843;&#25552;&#39640;&#20102;&#36866;&#24212;&#20998;&#24067;&#20559;&#31227;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. (arXiv:2210.11466v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23376;&#38598;&#23618;&#65288;&#25163;&#26415;&#24494;&#35843;&#65289;&#22312;&#36866;&#24212;&#20998;&#24067;&#20559;&#31227;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#36824;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#22312;&#29702;&#24819;&#29615;&#22659;&#19979;&#65292;&#25163;&#26415;&#24494;&#35843;&#21487;&#20197;&#20248;&#20110;&#20840;&#23618;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#21518;&#20960;&#23618;&#65292;&#20445;&#30041;&#24050;&#23398;&#29305;&#24449;&#21516;&#26102;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26377;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23376;&#38598;&#23618;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#25163;&#26415;&#24494;&#35843;&#65289;&#21487;&#20197;&#36798;&#21040;&#19982;&#25110;&#20248;&#20110;&#24120;&#29992;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#19988;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#20559;&#31227;&#24433;&#21709;&#30528;&#33021;&#22815;&#24494;&#35843;&#30340;&#23618;&#25968;&#12290;&#25105;&#20204;&#22312;&#19971;&#20010;&#30495;&#23454;&#25968;&#25454;&#20219;&#21153;&#20013;&#31995;&#32479;&#39564;&#35777;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#29702;&#24819;&#29615;&#22659;&#19979;&#65292;&#25163;&#26415;&#24494;&#35843;&#21487;&#20197;&#32988;&#36807;&#20840;&#23618;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach to transfer learning under distribution shift is to fine-tune the last few layers of a pre-trained model, preserving learned features while also adapting to the new task. This paper shows that in such settings, selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms commonly used fine-tuning approaches. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best. We validate our findings systematically across seven real-world data tasks spanning three types of distribution shifts. Theoretically, we prove that for two-layer neural networks in an idealized setting, first-layer tuning can outperform fine-tuning all layers. Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#23545;&#35805;&#27169;&#25311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#24102;&#27880;&#37322;&#30340;&#31034;&#20363;&#33258;&#21160;&#21019;&#24314;&#22823;&#37327;&#23545;&#35805;&#25968;&#25454;&#65292;&#27604;&#20247;&#21253;&#26356;&#21152;&#25104;&#26412;&#25928;&#30410;&#21644;&#33410;&#30465;&#26102;&#38388;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#20351;&#29992;&#27169;&#25311;&#23545;&#35805;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.04185</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21487;&#25511;&#23545;&#35805;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Controllable Dialogue Simulation with In-Context Learning. (arXiv:2210.04185v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#23545;&#35805;&#27169;&#25311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#24102;&#27880;&#37322;&#30340;&#31034;&#20363;&#33258;&#21160;&#21019;&#24314;&#22823;&#37327;&#23545;&#35805;&#25968;&#25454;&#65292;&#27604;&#20247;&#21253;&#26356;&#21152;&#25104;&#26412;&#25928;&#30410;&#21644;&#33410;&#30465;&#26102;&#38388;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#20351;&#29992;&#27169;&#25311;&#23545;&#35805;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#24102;&#27880;&#37322;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#36890;&#36807;&#20247;&#21253;&#21019;&#24314;&#65292;&#36153;&#26102;&#36153;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Dialogic &#30340;&#26032;&#22411;&#23545;&#35805;&#27169;&#25311;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#22823;&#23610;&#24230;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#21019;&#24314;&#25968;&#25454;&#38598;&#12290;&#22312;&#23569;&#37327;&#24102;&#27880;&#37322;&#30340;&#23545;&#35805;&#31034;&#20363;&#30340;&#21551;&#21457;&#19979;&#65292;Dialogic &#33258;&#21160;&#36873;&#25321;&#19978;&#19979;&#25991;&#20013;&#30340;&#31034;&#20363;&#65292;&#20419;&#20351; GPT-3 &#25511;&#21046;&#29983;&#25104;&#26032;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#25193;&#23637;&#23569;&#37327;&#30340;&#23545;&#35805;&#25968;&#25454;&#65292;&#20960;&#20046;&#27809;&#26377;&#20154;&#31867;&#20171;&#20837;&#21644;&#21442;&#25968;&#26356;&#26032;&#65292;&#22240;&#27492;&#27604;&#20247;&#21253;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#33410;&#30465;&#26102;&#38388;&#12290;&#22522;&#20110; MultiWOZ &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#65292;&#20351;&#29992;&#27169;&#25311;&#23545;&#35805;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#29978;&#33267;&#27604;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#20154;&#24037;&#29983;&#25104;&#30340;&#23545;&#35805;&#26356;&#22909;&#65292;&#20165;&#20351;&#29992; 85 &#26465;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building dialogue systems requires a large corpus of annotated dialogues. Such datasets are usually created via crowdsourcing, which is expensive and time-consuming. In this paper, we propose \textsc{Dialogic}, a novel dialogue simulation method based on large language model in-context learning to automate dataset creation. Seeded with a few annotated dialogues, \textsc{Dialogic} automatically selects in-context examples for demonstration and prompts GPT-3 to generate new dialogues and annotations in a controllable way. Our method can rapidly expand a small set of dialogue data with minimum or zero \textit{human involvement} and \textit{parameter update} and is thus much more cost-efficient and time-saving than crowdsourcing. Experimental results on the MultiWOZ dataset demonstrate that training a model on the simulated dialogues leads to even better performance than using the same amount of human-generated dialogues under the challenging low-resource settings, with as few as 85 dialog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#26041;&#27861;&#65288;TED&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#20219;&#21153;&#24863;&#30693;&#30340;&#28388;&#27874;&#22120;&#26469;&#23545;&#40784;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#36873;&#25321;&#26377;&#29992;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#30693;&#35782;&#24046;&#36317;&#65292;&#20351;&#23398;&#29983;&#27169;&#22411;&#26356;&#22909;&#22320;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.01351</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Less is More: Task-aware Layer-wise Distillation for Language Model Compression. (arXiv:2210.01351v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#26041;&#27861;&#65288;TED&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#20219;&#21153;&#24863;&#30693;&#30340;&#28388;&#27874;&#22120;&#26469;&#23545;&#40784;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#36873;&#25321;&#26377;&#29992;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#30693;&#35782;&#24046;&#36317;&#65292;&#20351;&#23398;&#29983;&#27169;&#22411;&#26356;&#22909;&#22320;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#33976;&#39311;&#26159;&#23558;&#22823;&#27169;&#22411;&#65288;&#21363;&#25945;&#24072;&#27169;&#22411;&#65289;&#21387;&#32553;&#20026;&#23567;&#27169;&#22411;&#65288;&#21363;&#23398;&#29983;&#27169;&#22411;&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23398;&#29983;&#36890;&#36807;&#22312;&#27599;&#20010;&#20013;&#38388;&#23618;&#27169;&#20223;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#26469;&#20174;&#25945;&#24072;&#20013;&#33976;&#39311;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20998;&#23618;&#33976;&#39311;&#20063;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;&#30001;&#20110;&#23398;&#29983;&#30340;&#27169;&#22411;&#23481;&#37327;&#27604;&#25945;&#24072;&#23567;&#65292;&#23427;&#36890;&#24120;&#20250;&#20986;&#29616;&#27424;&#25311;&#21512;;&#27492;&#22806;&#65292;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#21253;&#21547;&#20102;&#23398;&#29983;&#26410;&#24517;&#38656;&#35201;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#65288;TED&#65289;&#26041;&#27861;&#12290;TED&#35774;&#35745;&#20219;&#21153;&#24863;&#30693;&#28388;&#27874;&#22120;&#26469;&#23545;&#40784;&#27599;&#19968;&#23618;&#30340;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#36825;&#20123;&#28388;&#27874;&#22120;&#20174;&#38544;&#34255;&#34920;&#31034;&#20013;&#36873;&#25321;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;TED&#20943;&#23569;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#24182;&#24110;&#21161;&#23398;&#29983;&#26356;&#22909;&#22320;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;TED&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#25110;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layer-wise distillation is a powerful tool to compress large models (i.e. teacher models) into small ones (i.e., student models). The student distills knowledge from the teacher by mimicking the hidden representations of the teacher at every intermediate layer. However, layer-wise distillation is difficult. Since the student has a smaller model capacity than the teacher, it is often under-fitted. Furthermore, the hidden representations of the teacher contain redundant information that the student does not necessarily need for the target task's learning. To address these challenges, we propose a novel Task-aware layEr-wise Distillation (TED). TED designs task-aware filters to align the hidden representations of the student and the teacher at each layer. The filters select the knowledge that is useful for the target task from the hidden representations. As such, TED reduces the knowledge gap between the two models and helps the student to fit better on the target task. We evaluate TED in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#23398;&#20064;&#20219;&#21153;&#65306;&#27169;&#22411;&#38142;&#25509;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#40657;&#30418;&#27169;&#22411;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#27169;&#22411;&#38142;&#25509;&#65292;&#25226;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#12290;&#25552;&#20986;&#20102;&#25903;&#25345;&#38142;&#25509;&#19981;&#21516;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#35843;&#24230;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.13883</link><description>&lt;p&gt;
MLink&#65306;&#22810;&#20010;&#39046;&#22495;&#30340;&#40657;&#30418;&#27169;&#22411;&#38142;&#25509;&#23454;&#29616;&#21327;&#21516;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
MLink: Linking Black-Box Models from Multiple Domains for Collaborative Inference. (arXiv:2209.13883v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#23398;&#20064;&#20219;&#21153;&#65306;&#27169;&#22411;&#38142;&#25509;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#40657;&#30418;&#27169;&#22411;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#27169;&#22411;&#38142;&#25509;&#65292;&#25226;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#12290;&#25552;&#20986;&#20102;&#25903;&#25345;&#38142;&#25509;&#19981;&#21516;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#35843;&#24230;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#25512;&#29702;&#30340;&#25104;&#26412;&#25928;&#30410;&#23545;&#20110;&#26102;&#24310;&#25935;&#24863;&#30340;&#20219;&#21153;&#21644;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#22256;&#22659;&#26159;&#65306;&#20026;&#20102;&#25552;&#20379;&#22797;&#26434;&#30340;&#26234;&#33021;&#26381;&#21153;&#65288;&#22914;&#26234;&#33021;&#22478;&#24066;&#65289;&#65292;&#25105;&#20204;&#38656;&#35201;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#20294;&#25104;&#26412;&#39044;&#31639;&#65288;&#22914;GPU&#20869;&#23384;&#65289;&#19981;&#36275;&#20197;&#36816;&#34892;&#25152;&#26377;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#22522;&#30784;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#20219;&#21153;&#65306;&#27169;&#22411;&#38142;&#25509;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#23427;&#20204;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65288;&#31216;&#20026;&#27169;&#22411;&#38142;&#25509;&#65289;&#26469;&#36830;&#25509;&#19981;&#21516;&#40657;&#30418;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25903;&#25345;&#38142;&#25509;&#24322;&#26500;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27169;&#22411;&#38142;&#25509;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#38142;&#25509;&#30340;&#36866;&#24212;&#21644;&#32858;&#21512;&#26041;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#38142;&#25509;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35843;&#24230;&#31639;&#27861;&#65292;&#21517;&#20026;MLink&#12290;&#36890;&#36807;&#21551;&#29992;&#21327;&#20316;&#22810;&#27169;&#22411;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cost efficiency of model inference is critical to real-world machine learning (ML) applications, especially for delay-sensitive tasks and resource-limited devices. A typical dilemma is: in order to provide complex intelligent services (e.g. smart city), we need inference results of multiple ML models, but the cost budget (e.g. GPU memory) is not enough to run all of them. In this work, we study underlying relationships among black-box ML models and propose a novel learning task: model linking, which aims to bridge the knowledge of different black-box models by learning mappings (dubbed model links) between their output spaces. We propose the design of model links which supports linking heterogeneous black-box ML models. Also, in order to address the distribution discrepancy challenge, we present adaptation and aggregation methods of model links. Based on our proposed model links, we developed a scheduling algorithm, named MLink. Through collaborative multi-model inference enabled b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#36755;&#20837;&#36807;&#28388;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#21644;&#36755;&#20837;&#36807;&#28388;&#22120;&#30340;&#20551;&#35774;&#22797;&#26434;&#24615;&#36827;&#34892;&#29702;&#35770;&#27604;&#36739;&#65292;&#20174;&#32780;&#20102;&#35299;&#20248;&#21270;&#28508;&#21147;&#12290;&#35813;&#26694;&#26550;&#20943;&#23569;&#20887;&#20313;&#65292;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#65292;&#24182;&#22312;f&#20540;&#12289;&#25512;&#29702;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.13873</link><description>&lt;p&gt;
InFi&#65306;&#31227;&#21160;&#31471;&#25512;&#29702;&#30340;&#36164;&#28304;&#39640;&#25928;&#24615;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#31471;&#21040;&#31471;&#36755;&#20837;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
InFi: End-to-End Learning to Filter Input for Resource-Efficiency in Mobile-Centric Inference. (arXiv:2209.13873v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#36755;&#20837;&#36807;&#28388;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#21644;&#36755;&#20837;&#36807;&#28388;&#22120;&#30340;&#20551;&#35774;&#22797;&#26434;&#24615;&#36827;&#34892;&#29702;&#35770;&#27604;&#36739;&#65292;&#20174;&#32780;&#20102;&#35299;&#20248;&#21270;&#28508;&#21147;&#12290;&#35813;&#26694;&#26550;&#20943;&#23569;&#20887;&#20313;&#65292;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#65292;&#24182;&#22312;f&#20540;&#12289;&#25512;&#29702;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#31471;AI&#24212;&#29992;&#23545;&#27169;&#22411;&#25512;&#29702;&#30340;&#36164;&#28304;&#39640;&#25928;&#24615;&#26377;&#24456;&#39640;&#30340;&#35201;&#27714;&#12290;&#36755;&#20837;&#36807;&#28388;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#28040;&#38500;&#20887;&#20313;&#65292;&#20174;&#32780;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#20026;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#37327;&#36523;&#23450;&#21046;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30041;&#19979;&#20102;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#26410;&#35299;&#31572;&#65306;&#65288;1&#65289;&#25512;&#29702;&#24037;&#20316;&#37327;&#30340;&#29702;&#35770;&#21487;&#36807;&#28388;&#24615;&#65292;&#20197;&#25351;&#23548;&#36755;&#20837;&#36807;&#28388;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#20174;&#32780;&#36991;&#20813;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#35797;&#38169;&#25104;&#26412;&#65307;&#65288;2&#65289;&#29305;&#24449;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#21306;&#20998;&#24230;&#65292;&#20197;&#20351;&#36755;&#20837;&#36807;&#28388;&#23545;&#22810;&#26679;&#21270;&#25512;&#29702;&#20219;&#21153;&#21644;&#36755;&#20837;&#20869;&#23481;&#26222;&#36941;&#26377;&#25928;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24418;&#24335;&#21270;&#36755;&#20837;&#36807;&#28388;&#38382;&#39064;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#27604;&#36739;&#25512;&#29702;&#27169;&#22411;&#21644;&#36755;&#20837;&#36807;&#28388;&#22120;&#30340;&#20551;&#35774;&#22797;&#26434;&#24615;&#65292;&#20197;&#20102;&#35299;&#20248;&#21270;&#28508;&#21147;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#36755;&#20837;&#36807;&#28388;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;f&#20540;&#12289;&#25512;&#29702;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#36229;&#36234;&#20102;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile-centric AI applications have high requirements for resource-efficiency of model inference. Input filtering is a promising approach to eliminate the redundancy so as to reduce the cost of inference. Previous efforts have tailored effective solutions for many applications, but left two essential questions unanswered: (1) theoretical filterability of an inference workload to guide the application of input filtering techniques, thereby avoiding the trial-and-error cost for resource-constrained mobile applications; (2) robust discriminability of feature embedding to allow input filtering to be widely effective for diverse inference tasks and input content. To answer them, we first formalize the input filtering problem and theoretically compare the hypothesis complexity of inference models and input filters to understand the optimization potential. Then we propose the first end-to-end learnable input filtering framework that covers most state-of-the-art methods and surpasses them in f
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#65292;&#20174;&#21018;&#20307;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.11355</link><description>&lt;p&gt;
&#20174;&#21018;&#20307;&#22270;&#20687;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65288;&#26410;&#30693;&#36136;&#37327;&#20998;&#24067;&#65289;
&lt;/p&gt;
&lt;p&gt;
Learning to predict 3D rotational dynamics from images of a rigid body with unknown mass distribution. (arXiv:2209.11355v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#65292;&#20174;&#21018;&#20307;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#24403;&#20302;&#32500;&#24230;&#27979;&#37327;&#19981;&#21487;&#29992;&#26102;&#65292;&#20250;&#26377;&#33258;&#30001;&#26059;&#36716;&#30340;3D&#21018;&#20307;&#30340;&#22270;&#20687;&#35266;&#23519;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#25968;&#25454;&#30340;&#39640;&#32500;&#25968;&#38459;&#27490;&#20102;&#20351;&#29992;&#32463;&#20856;&#20272;&#35745;&#25216;&#26415;&#26469;&#23398;&#20064;&#21160;&#24577;&#12290;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#20063;&#21463;&#38480;&#20110;&#19968;&#20010;&#21018;&#20307;&#22270;&#20687;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#65292;&#32780;&#36136;&#37327;&#20998;&#24067;&#19982;&#21021;&#22987;&#35282;&#36895;&#24230;&#19968;&#36215;&#20915;&#23450;&#21018;&#20307;&#26059;&#36716;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#20272;&#35745;&#21644;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#65292;&#35813;&#27969;&#31243;&#23558;&#21333;&#20010;&#22270;&#20687;&#26144;&#23556;&#21040;&#19982; $\mathbf{SO}(3)$ &#21516;&#32986;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#65292;&#20174;&#28508;&#22312;&#23545;&#20013;&#35745;&#31639;&#35282;&#36895;&#24230;&#65292;&#24182;&#20351;&#29992;Hamilton&#36816;&#21160;&#26041;&#31243;&#39044;&#27979;&#26410;&#26469;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#26059;&#36716;&#21018;&#20307;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world settings, image observations of freely rotating 3D rigid bodies, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics. The usefulness of standard deep learning methods is also limited because an image of a rigid body reveals nothing about the distribution of mass inside the body, which, together with initial angular velocity, is what determines how the body will rotate. We present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to $\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion. We demonstrate the efficacy of our approach on new rotating rigid-body datasets of sequenc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#27169;&#22411;&#21270;&#25277;&#35937;&#30446;&#26631;&#65292;&#20197;&#38477;&#20302;&#34892;&#21160;&#39044;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;&#35270;&#35273;&#34920;&#24449;&#26469;&#25551;&#36848;&#21160;&#20316;&#21644;&#30446;&#26631;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#25277;&#35937;&#30446;&#26631;&#20026;&#19968;&#20010;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#21487;&#22312;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05044</link><description>&lt;p&gt;
&#24314;&#27169;&#25277;&#35937;&#30446;&#26631;&#39044;&#27979;&#19979;&#19968;&#27493;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Predicting the Next Action by Modeling the Abstract Goal. (arXiv:2209.05044v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#27169;&#22411;&#21270;&#25277;&#35937;&#30446;&#26631;&#65292;&#20197;&#38477;&#20302;&#34892;&#21160;&#39044;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;&#35270;&#35273;&#34920;&#24449;&#26469;&#25551;&#36848;&#21160;&#20316;&#21644;&#30446;&#26631;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#25277;&#35937;&#30446;&#26631;&#20026;&#19968;&#20010;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#21487;&#22312;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20154;&#31867;&#21160;&#20316;&#30340;&#38382;&#39064;&#20855;&#26377;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#26159;&#65292;&#22914;&#26524;&#25105;&#20204;&#26377;&#20851;&#20110;&#21160;&#20316;&#23454;&#29616;&#30446;&#26631;&#30340;&#24863;&#30693;&#65292;&#21487;&#20197;&#38477;&#20302;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#65292;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#26469;&#20943;&#23569;&#26410;&#26469;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#35270;&#35273;&#34920;&#24449;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21160;&#20316;&#21644;&#30446;&#26631;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#25277;&#35937;&#30446;&#26631;&#30340;&#26032;&#27010;&#24565;&#65292;&#20854;&#21462;&#20915;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#29305;&#24449;&#24207;&#21015;&#65292;&#29992;&#20110;&#34892;&#21160;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25277;&#35937;&#30446;&#26631;&#35774;&#35745;&#20026;&#19968;&#20010;&#20998;&#24067;&#65292;&#20854;&#21442;&#25968;&#26159;&#20351;&#29992;&#21464;&#20998;&#36882;&#24402;&#32593;&#32476;&#20272;&#35745;&#30340;&#12290;&#25105;&#20204;&#23545;&#19979;&#19968;&#20010;&#21160;&#20316;&#36827;&#34892;&#22810;&#27425;&#37319;&#26679;&#65292;&#24182;&#24341;&#20837;&#30446;&#26631;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#30830;&#23450;&#20174;&#25277;&#35937;&#30446;&#26631;&#24471;&#20986;&#30340;&#26368;&#20339;&#20505;&#36873;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of anticipating human actions is an inherently uncertain one. However, we can reduce this uncertainty if we have a sense of the goal that the actor is trying to achieve. Here, we present an action anticipation model that leverages goal information for the purpose of reducing the uncertainty in future predictions. Since we do not possess goal information or the observed actions during inference, we resort to visual representation to encapsulate information about both actions and goals. Through this, we derive a novel concept called abstract goal which is conditioned on observed sequences of visual features for action anticipation. We design the abstract goal as a distribution whose parameters are estimated using a variational recurrent network. We sample multiple candidates for the next action and introduce a goal consistency measure to determine the best candidate that follows from the abstract goal. Our method obtains impressive results on the very challenging Epic-Kitchen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#28385;&#36275;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20998;&#37197;&#29289;&#21697;&#32473;&#24179;&#21488;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#26469;&#36817;&#20284;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#21516;&#26102;&#23454;&#29616;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.09951</link><description>&lt;p&gt;
&#20108;&#20998;&#22270;&#21305;&#37197;&#20013;&#30340;&#19981;&#21516;&#32676;&#20307;&#20844;&#24179;&#24615;&#19979;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#8212;&#8212;&#19968;&#31181;&#36817;&#20284;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Individual fairness under Varied Notions of Group Fairness in Bipartite Matching -- One Framework to Approximate Them Al. (arXiv:2208.09951v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28385;&#36275;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20998;&#37197;&#29289;&#21697;&#32473;&#24179;&#21488;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#26469;&#36817;&#20284;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#21516;&#26102;&#23454;&#29616;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28385;&#36275;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20998;&#37197;&#29289;&#21697;&#32473;&#24179;&#21488;&#30340;&#38382;&#39064;&#12290;&#27599;&#20010;&#29289;&#21697;&#37117;&#19982;&#26576;&#20123;&#32676;&#20307;&#30456;&#20851;&#32852;&#65292;&#24182;&#19988;&#23545;&#24179;&#21488;&#26377;&#20248;&#20808;&#39034;&#24207;&#12290;&#27599;&#20010;&#24179;&#21488;&#36890;&#36807;&#25351;&#23450;&#27599;&#20010;&#32676;&#20307;&#21487;&#20197;&#19982;&#20043;&#21305;&#37197;&#30340;&#29289;&#21697;&#25968;&#37327;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#26469;&#25191;&#34892;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#23613;&#31649;&#21487;&#33021;&#23384;&#22312;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#22810;&#20010;&#26368;&#20248;&#35299;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#20998;&#24067;&#26469;&#23454;&#29616;&#8220;&#38543;&#26426;&#20010;&#20307;&#20844;&#24179;&#24615;&#8221;&#65292;&#20351;&#24471;&#27599;&#20010;&#29289;&#21697;&#34987;&#21305;&#37197;&#21040;&#20854;&#21069;&#20960;&#20010;&#36873;&#25321;&#20043;&#19968;&#30340;&#21512;&#29702;&#27010;&#29575;&#12290;&#24403;&#27599;&#20010;&#29289;&#21697;&#21487;&#20197;&#23646;&#20110;&#22810;&#20010;&#32676;&#20307;&#26102;&#65292;&#21363;&#20351;&#25152;&#26377;&#32676;&#20307;&#19979;&#38480;&#22343;&#20026;0&#19988;&#27809;&#26377;&#20010;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#23547;&#25214;&#26368;&#22823;&#22823;&#23567;&#32676;&#20307;&#20844;&#24179;&#21305;&#37197;&#30340;&#38382;&#39064;&#20063;&#26159;NP-&#38590;&#30340;&#12290;&#23545;&#20110;&#19968;&#20849;$n$&#20010;&#29289;&#21697;&#65292;&#24403;&#19968;&#20010;&#29289;&#21697;&#26368;&#22810;&#23646;&#20110;$\Delta$&#20010;&#32676;&#20307;&#65292;&#24182;&#19988;&#25152;&#26377;&#32676;&#20307;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#37117;&#26159;&#24120;&#25968;&#26102;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;$O(\Delta \log n)$&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#37117;&#26159;&#21306;&#38388;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#30340;&#20010;&#20307;&#20844;&#24179;&#21305;&#37197;&#30340;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#36817;&#20284;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#21516;&#26102;&#23454;&#29616;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of assigning items to platforms while satisfying group and individual fairness constraints. Each item is associated with certain groups and has a preference ordering over platforms. Each platform enforces group fairness by specifying an upper and a lower bound on the number of items that can be matched to it from each group. Although there may be multiple optimal solutions that satisfy the group fairness constraints, we aim to achieve `probabilistic individual fairness' by computing a distribution over `group fair' matchings such that each item has a reasonable probability of being matched to one of its top choices. When each item can belong to multiple groups, the problem of finding a maximum size group-fair matching is NP-hard even when all the group lower bounds are 0, and there are no individual fairness constraints. Given a total of $n$ items, we achieve a $O(\Delta \log n)$ approximation algorithm when an item can belong to at most $\Delta$ groups, and all
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.13872</link><description>&lt;p&gt;
&#21518;&#39564;&#27010;&#24565;&#35299;&#37322;&#20309;&#26102;&#21487;&#35782;&#21035;&#65311;
&lt;/p&gt;
&lt;p&gt;
When are Post-hoc Conceptual Explanations Identifiable?. (arXiv:2206.13872v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23884;&#20837;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#27010;&#24565;&#35299;&#37322;&#26469;&#29702;&#35299;&#21644;&#20998;&#35299;&#65292;&#36825;&#31181;&#38656;&#27714;&#22312;&#35299;&#37322;&#20013;&#19981;&#21253;&#21547;&#26377;&#25928;&#27010;&#24565;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26174;&#33879;&#12290;&#20026;&#20102;&#25552;&#20379;&#21518;&#39564;&#35299;&#37322;&#65292;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#20250;&#22312;&#24050;&#35757;&#32451;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#37322;&#24615;&#24378;&#30340;&#27010;&#24565;&#65292;&#20363;&#22914;&#29289;&#20307;&#24418;&#29366;&#25110;&#39068;&#33394;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#35748;&#20026;&#27010;&#24565;&#21457;&#29616;&#24212;&#35813;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#34987;&#35777;&#26126;&#22320;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20316;&#20026;&#19968;&#20010;&#36215;&#28857;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#23558;&#27010;&#24565;&#21457;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#20363;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65289;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#38750;&#39640;&#26031;&#20998;&#24067;&#30340;&#29420;&#31435;&#27010;&#24565;&#26469;&#38416;&#26126;&#36825;&#19968;&#28857;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#21487;&#35777;&#26126;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest in understanding and factorizing learned embedding spaces through conceptual explanations is steadily growing. When no human concept labels are available, concept discovery methods search trained embedding spaces for interpretable concepts like object shape or color that can be used to provide post-hoc explanations for decisions. Unlike previous work, we argue that concept discovery should be identifiable, meaning that a number of known concepts can be provably recovered to guarantee reliability of the explanations. As a starting point, we explicitly make the connection between concept discovery and classical methods like Principal Component Analysis and Independent Component Analysis by showing that they can recover independent concepts with non-Gaussian distributions. For dependent concepts, we propose two novel approaches that exploit functional compositionality properties of image-generating processes. Our provably identifiable concept discovery methods substantially outpe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20840;&#23616;&#19978;&#19979;&#25991;&#35270;&#35273;Transformer (GC ViT) &#26550;&#26500;&#65292;&#21033;&#29992;&#20840;&#23616;&#19978;&#19979;&#25991;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26631;&#20934;&#30340;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#23545;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#31354;&#38388;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#26377;&#25928;&#32780;&#39640;&#25928;&#30340;&#24314;&#27169;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;ViTs&#20013;&#32570;&#20047;&#24402;&#32435;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.09959</link><description>&lt;p&gt;
&#20840;&#23616;&#19978;&#19979;&#25991;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
Global Context Vision Transformers. (arXiv:2206.09959v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20840;&#23616;&#19978;&#19979;&#25991;&#35270;&#35273;Transformer (GC ViT) &#26550;&#26500;&#65292;&#21033;&#29992;&#20840;&#23616;&#19978;&#19979;&#25991;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26631;&#20934;&#30340;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#23545;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#31354;&#38388;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#26377;&#25928;&#32780;&#39640;&#25928;&#30340;&#24314;&#27169;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;ViTs&#20013;&#32570;&#20047;&#24402;&#32435;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8212;&#8212;&#20840;&#23616;&#19978;&#19979;&#25991;&#35270;&#35273;Transformer (GC ViT), &#21487;&#20197;&#22686;&#24378;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20840;&#23616;&#19978;&#19979;&#25991;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26631;&#20934;&#30340;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#65292;&#23545;&#38271;&#36317;&#31163;&#21644;&#30701;&#36317;&#31163;&#31354;&#38388;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#26377;&#25928;&#32780;&#39640;&#25928;&#30340;&#24314;&#27169;&#65292;&#26080;&#38656;&#36827;&#34892;&#20687;&#35745;&#31639;&#27880;&#24847;&#21147;&#25513;&#30721;&#25110;&#31227;&#21160;&#26412;&#22320;&#31383;&#21475;&#36825;&#26679;&#30340;&#26114;&#36149;&#25805;&#20316;&#12290;&#24182;&#19988;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;ViTs&#20013;&#32570;&#20047;&#24402;&#32435;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#26550;&#26500;&#20013;&#20351;&#29992;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#34701;&#21512;&#21453;&#21521;&#27531;&#24046;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;GC ViT&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20998;&#31867;&#65292;GC ViT&#30340;51M&#12289;90M&#21644;201M&#21442;&#25968;&#21464;&#20307;&#22312;224&#20687;&#32032;&#20998;&#36776;&#29575;&#19979;&#37117;&#33021;&#22815;&#36798;&#21040;84.3%&#12289;85.0%&#21644;85.7%&#30340;Top-1&#31934;&#24230;&#65292;&#32780;&#19988;&#26080;&#38656;&#20219;&#20309;&#39044;&#35757;&#32451;&#65292;&#22240;&#27492;&#36229;&#36234;&#20102;CNN-based Conv&#31561;&#20808;&#21069;&#30340;&#33402;&#26415;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and 85.7% Top-1 accuracy, respectively, at 224 image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based Conv
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#30340;&#26222;&#21450;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#26368;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#20294;&#20063;&#21361;&#21450;&#20102;MLaaS&#25552;&#20379;&#21830;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#22791;&#30456;&#21516;&#34892;&#20026;&#30340;&#27169;&#22411;&#21103;&#26412;&#65292;&#26412;&#25991;&#38024;&#23545;&#27169;&#22411;&#31363;&#21462;&#30340;&#25915;&#20987;&#21644;&#30456;&#24212;&#30340;&#23545;&#31574;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;</title><link>http://arxiv.org/abs/2206.08451</link><description>&lt;p&gt;
&#25105;&#30693;&#36947;&#20320;&#21435;&#24180;&#35757;&#32451;&#20102;&#20160;&#20040;&#65306;&#20851;&#20110;&#31363;&#21462;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#38450;&#24481;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences. (arXiv:2206.08451v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08451
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#30340;&#26222;&#21450;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#26368;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#20294;&#20063;&#21361;&#21450;&#20102;MLaaS&#25552;&#20379;&#21830;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#22791;&#30456;&#21516;&#34892;&#20026;&#30340;&#27169;&#22411;&#21103;&#26412;&#65292;&#26412;&#25991;&#38024;&#23545;&#27169;&#22411;&#31363;&#21462;&#30340;&#25915;&#20987;&#21644;&#30456;&#24212;&#30340;&#23545;&#31574;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#30340;&#33539; paradigm&#65292;&#36890;&#36807;&#25353;&#38656;&#20184;&#36153;&#30340;&#21407;&#21017;&#65292;&#29978;&#33267;&#21487;&#20197;&#20026;&#23458;&#25143;&#25552;&#20379;&#26368;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#36825;&#20351;&#29992;&#25143;&#21487;&#20197;&#36991;&#20813;&#32791;&#26102;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#35775;&#38382;&#65288;&#20854;&#39044;&#27979;&#30340;&#65289;&#27169;&#22411;&#65292;MLaaS &#25552;&#20379;&#21830;&#21361;&#21450;&#20854;&#30693;&#35782;&#20135;&#26435;&#65292;&#22914;&#25935;&#24863;&#30340;&#35757;&#32451;&#25968;&#25454;&#12289;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#25110;&#23398;&#20064;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#20165;&#39044;&#27979;&#26631;&#31614;&#21019;&#24314;&#20855;&#26377;&#65288;&#20960;&#20046;&#65289;&#30456;&#21516;&#34892;&#20026;&#30340;&#27169;&#22411;&#21103;&#26412;&#12290;&#34429;&#28982;&#25551;&#36848;&#20102;&#35768;&#22810;&#36825;&#31181;&#25915;&#20987;&#30340;&#21464;&#20307;&#65292;&#20294;&#21482;&#25552;&#20986;&#20102;&#20998;&#25955;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#28041;&#21450;&#23396;&#31435;&#30340;&#23041;&#32961;&#12290;&#36825;&#25552;&#20986;&#20102;&#23545;&#27169;&#22411;&#31363;&#21462;&#39046;&#22495;&#36827;&#34892;&#24443;&#24213;&#31995;&#32479;&#21270;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#20840;&#38754;&#20102;&#35299;&#20026;&#20160;&#20040;&#36825;&#20123;&#25915;&#20987;&#25104;&#21151;&#20197;&#21450;&#22914;&#20309;&#20840;&#38754;&#22320;&#36827;&#34892;&#38450;&#24481;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#39033;&#32508;&#21512;&#24615;&#35843;&#26597;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#28085;&#30422;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#21644;&#30456;&#24212;&#30340;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-as-a-Service (MLaaS) has become a widespread paradigm, making even the most complex machine learning models available for clients via e.g. a pay-per-query principle. This allows users to avoid time-consuming processes of data collection, hyperparameter tuning, and model training. However, by giving their customers access to the (predictions of their) models, MLaaS providers endanger their intellectual property, such as sensitive training data, optimised hyperparameters, or learned model parameters. Adversaries can create a copy of the model with (almost) identical behavior using the the prediction labels only. While many variants of this attack have been described, only scattered defence strategies have been proposed, addressing isolated threats. This raises the necessity for a thorough systematisation of the field of model stealing, to arrive at a comprehensive understanding why these attacks are successful, and how they could be holistically defended against. We addr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#21644;&#31354;&#38388;&#32422;&#26463;&#30340;&#22810;&#20219;&#21153;&#32593;&#32476;&#65292;&#21517;&#20026;CSLSL&#65292;&#29992;&#20110;&#20154;&#31867;&#27969;&#21160;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.05731</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#21644;&#31354;&#38388;&#32422;&#26463;&#30340;&#22810;&#20219;&#21153;&#32593;&#32476;&#30340;&#20154;&#31867;&#27969;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Human Mobility Prediction with Causal and Spatial-constrained Multi-task Network. (arXiv:2206.05731v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#21644;&#31354;&#38388;&#32422;&#26463;&#30340;&#22810;&#20219;&#21153;&#32593;&#32476;&#65292;&#21517;&#20026;CSLSL&#65292;&#29992;&#20110;&#20154;&#31867;&#27969;&#21160;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20154;&#31867;&#27969;&#21160;&#27169;&#22411;&#26377;&#21161;&#20110;&#29702;&#35299;&#20154;&#20204;&#22312;&#22478;&#24066;&#20013;&#22914;&#20309;&#33719;&#21462;&#36164;&#28304;&#65292;&#19982;&#20182;&#20154;&#25509;&#35302;&#65292;&#22240;&#27492;&#26377;&#21161;&#20110;&#22478;&#24066;&#35268;&#21010;&#12289;&#27969;&#34892;&#30149;&#25511;&#21046;&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#24191;&#21578;&#31561;&#21508;&#31181;&#24212;&#29992;&#12290;&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#39044;&#27979;&#26159;&#20010;&#20154;&#27969;&#21160;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#20915;&#23450;&#24615;&#20219;&#21153;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#25110;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#24182;&#27809;&#26377;&#32473;&#20010;&#20154;&#26053;&#34892;&#20915;&#31574;&#30340;&#36923;&#36753;&#21644;&#20154;&#21475;&#38598;&#20307;&#34892;&#20026;&#30340;&#20877;&#29616;&#24102;&#26469;&#36275;&#22815;&#30340;&#37325;&#35270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#21644;&#31354;&#38388;&#32422;&#26463;&#30340;&#38271;&#30701;&#26399;&#23398;&#20064;&#22120;(CSLSL)&#65292;&#29992;&#20110;&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#39044;&#27979;&#12290;CSLSL&#21033;&#29992;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22240;&#26524;&#32467;&#26500;&#26126;&#30830;&#22320;&#24314;&#27169; "\textit{when$\rightarrow$what$\rightarrow$where}" &#25110; "\textit{ time$\rightarrow$activity$\rightarrow$location}" &#20915;&#31574;&#36923;&#36753;&#12290;&#25105;&#20204;&#25509;&#19979;&#26469;&#25552;&#20986;&#19968;&#20010;&#31354;&#38388;&#32422;&#26463;&#30340;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#23558;&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#39044;&#27979;&#19982;&#22478;&#24066;&#29615;&#22659;&#30340;&#31354;&#38388;&#37197;&#32622;&#21327;&#35843;&#36215;&#26469;&#12290;&#20004;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling human mobility helps to understand how people are accessing resources and physically contacting with each other in cities, and thus contributes to various applications such as urban planning, epidemic control, and location-based advertisement. Next location prediction is one decisive task in individual human mobility modeling and is usually viewed as sequence modeling, solved with Markov or RNN-based methods. However, the existing models paid little attention to the logic of individual travel decisions and the reproducibility of the collective behavior of population. To this end, we propose a Causal and Spatial-constrained Long and Short-term Learner (CSLSL) for next location prediction. CSLSL utilizes a causal structure based on multi-task learning to explicitly model the "\textit{when$\rightarrow$what$\rightarrow$where}", a.k.a. "\textit{time$\rightarrow$activity$\rightarrow$location}" decision logic. We next propose a spatial-constrained loss function as an auxiliary task, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#24402;&#32435;&#23398;&#20064;&#22120;&#65288;NSIL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#27010;&#24565;&#65292;&#24182;&#23398;&#20064;&#23558;&#28508;&#22312;&#27010;&#24565;&#26144;&#23556;&#21040;&#30446;&#26631;&#26631;&#31614;&#30340;&#31526;&#21495;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2205.12735</link><description>&lt;p&gt;
&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;Answer Set Programs&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Learning of Answer Set Programs from Raw Data. (arXiv:2205.12735v7 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#24402;&#32435;&#23398;&#20064;&#22120;&#65288;NSIL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#27010;&#24565;&#65292;&#24182;&#23398;&#20064;&#23558;&#28508;&#22312;&#27010;&#24565;&#26144;&#23556;&#21040;&#30446;&#26631;&#26631;&#31614;&#30340;&#31526;&#21495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#32456;&#30446;&#26631;&#20043;&#19968;&#26159;&#21327;&#21161;&#20154;&#31867;&#36827;&#34892;&#22797;&#26434;&#30340;&#20915;&#31574;&#12290;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26088;&#22312;&#23558;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35299;&#37322;&#24615;&#19982;&#28145;&#24230;&#23398;&#20064;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#26159;&#23454;&#29616;&#27492;&#30446;&#26631;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#26500;&#24314;&#31526;&#21495;&#30693;&#35782;&#65292;&#32780;&#22312;&#32771;&#34385;&#31471;&#21040;&#31471;&#35757;&#32451;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#20165;&#38480;&#20110;&#23398;&#20064;&#30830;&#23450;&#30340;&#31243;&#24207;&#65292;&#35201;&#20040;&#20165;&#38480;&#20110;&#35757;&#32451;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#24402;&#32435;&#23398;&#20064;&#22120;&#65288;NSIL&#65289;&#65292;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#27010;&#24565;&#65292;&#24182;&#23398;&#20064;&#23558;&#28508;&#22312;&#27010;&#24565;&#26144;&#23556;&#21040;&#30446;&#26631;&#26631;&#31614;&#30340;&#31526;&#21495;&#30693;&#35782;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#21644;&#31526;&#21495;&#32452;&#20214;&#30340;&#24615;&#33021;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#31526;&#21495;&#30693;&#35782;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#38382;&#39064;&#22495;&#19978;&#35780;&#20272;&#20102;NSIL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the ultimate goals of Artificial Intelligence is to assist humans in complex decision making. A promising direction for achieving this goal is Neuro-Symbolic AI, which aims to combine the interpretability of symbolic techniques with the ability of deep learning to learn from raw data. However, most current approaches require manually engineered symbolic knowledge, and where end-to-end training is considered, such approaches are either restricted to learning definite programs, or are restricted to training binary neural networks. In this paper, we introduce Neuro-Symbolic Inductive Learner (NSIL), an approach that trains a general neural network to extract latent concepts from raw data, whilst learning symbolic knowledge that maps latent concepts to target labels. The novelty of our approach is a method for biasing the learning of symbolic knowledge, based on the in-training performance of both neural and symbolic components. We evaluate NSIL on three problem domains of different
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#8212;&#8212;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#36864;&#28779;&#65288;CMA-MAE&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36807;&#26089;&#22320;&#25918;&#24323;&#30446;&#26631;&#20197;&#36827;&#34892;&#25506;&#32034;&#12289;&#38590;&#20197;&#25506;&#32034;&#24179;&#22374;&#30446;&#26631;&#20197;&#21450;&#20302;&#20998;&#36776;&#29575;&#26723;&#26696;&#24615;&#33021;&#24046;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.10752</link><description>&lt;p&gt;
&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#36864;&#28779;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Covariance Matrix Adaptation MAP-Annealing. (arXiv:2205.10752v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#8212;&#8212;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#36864;&#28779;&#65288;CMA-MAE&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36807;&#26089;&#22320;&#25918;&#24323;&#30446;&#26631;&#20197;&#36827;&#34892;&#25506;&#32034;&#12289;&#38590;&#20197;&#25506;&#32034;&#24179;&#22374;&#30446;&#26631;&#20197;&#21450;&#20302;&#20998;&#36776;&#29575;&#26723;&#26696;&#24615;&#33021;&#24046;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#23547;&#25214;&#26368;&#39640;&#36136;&#37327;&#30340;&#21333;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#20248;&#21270;&#31639;&#27861;&#65292;&#20363;&#22914;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP-&#31934;&#33521;&#65288;CMA-ME&#65289;&#65292;&#23547;&#25214;&#19968;&#32452;&#26082;&#22312;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#39640;&#36136;&#37327;&#12289;&#21448;&#22312;&#29305;&#23450;&#24230;&#37327;&#20989;&#25968;&#26041;&#38754;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#12290;&#20294;&#26159;CMA-ME&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#30340;&#38480;&#21046;&#65306;&#36807;&#26089;&#22320;&#25918;&#24323;&#30446;&#26631;&#20197;&#36827;&#34892;&#25506;&#32034;&#12289;&#38590;&#20197;&#25506;&#32034;&#24179;&#22374;&#30446;&#26631;&#20197;&#21450;&#20302;&#20998;&#36776;&#29575;&#26723;&#26696;&#24615;&#33021;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#65292;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#36864;&#28779;&#65288;CMA-MAE&#65289;&#65292;&#20197;&#35299;&#20915;&#25152;&#26377;&#19977;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27599;&#20010;&#38480;&#21046;&#30340;&#26032;&#31639;&#27861;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25903;&#25745;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;CMA-MAE&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-objective optimization algorithms search for the single highest-quality solution with respect to an objective. Quality diversity (QD) optimization algorithms, such as Covariance Matrix Adaptation MAP-Elites (CMA-ME), search for a collection of solutions that are both high-quality with respect to an objective and diverse with respect to specified measure functions. However, CMA-ME suffers from three major limitations highlighted by the QD community: prematurely abandoning the objective in favor of exploration, struggling to explore flat objectives, and having poor performance for low-resolution archives. We propose a new quality diversity algorithm, Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), that addresses all three limitations. We provide theoretical justifications for the new algorithm with respect to each limitation. Our theory informs our experiments, which support the theory and show that CMA-MAE achieves state-of-the-art performance and robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#20262;&#29702;&#20445;&#35777;&#35770;&#35777;&#27169;&#24335;&#65292;&#29992;&#20110;&#30830;&#20445;&#29305;&#23450;&#30340;AI/AS&#22312;&#36816;&#34892;&#26102;&#22312;&#20262;&#29702;&#19978;&#26159;&#21487;&#25509;&#21463;&#30340;&#12290;&#35813;&#27169;&#24335;&#37319;&#29992;&#20102;&#20844;&#27491;&#12289;&#34892;&#21892;&#12289;&#19981;&#20260;&#23475;&#12289;&#23562;&#37325;&#20154;&#30340;&#33258;&#20027;&#26435;&#12289;&#36879;&#26126;&#24230;&#31561;&#20262;&#29702;&#21407;&#21017;&#65292;&#32553;&#20889;&#20026;PRAISE&#12290;</title><link>http://arxiv.org/abs/2203.15370</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#27835;&#31995;&#32479;&#30340;&#22522;&#20110;&#21407;&#21017;&#30340;&#20262;&#29702;&#20445;&#35777;&#35770;&#35777;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
A Principles-based Ethics Assurance Argument Pattern for AI and Autonomous Systems. (arXiv:2203.15370v4 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#20262;&#29702;&#20445;&#35777;&#35770;&#35777;&#27169;&#24335;&#65292;&#29992;&#20110;&#30830;&#20445;&#29305;&#23450;&#30340;AI/AS&#22312;&#36816;&#34892;&#26102;&#22312;&#20262;&#29702;&#19978;&#26159;&#21487;&#25509;&#21463;&#30340;&#12290;&#35813;&#27169;&#24335;&#37319;&#29992;&#20102;&#20844;&#27491;&#12289;&#34892;&#21892;&#12289;&#19981;&#20260;&#23475;&#12289;&#23562;&#37325;&#20154;&#30340;&#33258;&#20027;&#26435;&#12289;&#36879;&#26126;&#24230;&#31561;&#20262;&#29702;&#21407;&#21017;&#65292;&#32553;&#20889;&#20026;PRAISE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#35777;&#26696;&#20363;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#35770;&#35777;&#65292;&#36890;&#24120;&#30001;&#23433;&#20840;&#24037;&#31243;&#24072;&#21046;&#20316;&#65292;&#20197;&#20256;&#36798;&#23545;&#20110;&#20851;&#38190;&#25110;&#22797;&#26434;&#31995;&#32479;&#65288;&#20363;&#22914;&#39134;&#26426;&#65289;&#22312;&#20854;&#26082;&#23450;&#29615;&#22659;&#20013;&#20855;&#26377;&#36275;&#22815;&#23433;&#20840;&#24615;&#30340;&#20449;&#24515;&#12290;&#20445;&#35777;&#26696;&#20363;&#32463;&#24120;&#29992;&#20110;&#31995;&#32479;&#30340;&#31532;&#19977;&#26041;&#23457;&#25209;&#12290;&#22312;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#27835;&#31995;&#32479;&#65288;AI/AS&#65289;&#30740;&#31350;&#31038;&#21306;&#20013;&#65292;&#19968;&#20010;&#26032;&#20852;&#30340;&#21629;&#39064;&#26159;&#20351;&#29992;&#20445;&#35777;&#26696;&#20363;&#22312;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#26102;&#65292;&#30830;&#20445;&#29305;&#23450;&#30340;AI/AS&#22312;&#20262;&#29702;&#19978;&#26159;&#21487;&#25509;&#21463;&#30340;&#12290;&#26412;&#25991;&#22823;&#22823;&#21457;&#23637;&#20102;&#36825;&#20010;&#21629;&#39064;&#24182;&#20855;&#20307;&#38416;&#36848;&#20102;&#23427;&#12290;&#23427;&#23558;&#20445;&#35777;&#26696;&#20363;&#26041;&#27861;&#19982;&#19968;&#32452;&#20262;&#29702;&#21407;&#21017;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#20262;&#29702;&#20445;&#35777;&#35770;&#35777;&#27169;&#24335;&#12290;&#36825;&#20123;&#21407;&#21017;&#21253;&#25324;&#20844;&#27491;&#12289;&#34892;&#21892;&#12289;&#19981;&#20260;&#23475;&#12289;&#23562;&#37325;&#20154;&#30340;&#33258;&#20027;&#26435;&#21644;&#36879;&#26126;&#24230;&#21407;&#21017;&#12290;&#35813;&#35770;&#35777;&#27169;&#24335;&#20197;&#32553;&#20889;PRAISE&#21629;&#21517;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35813;&#35770;&#35777;&#27169;&#24335;&#30340;&#30446;&#30340;&#21644;&#20855;&#20307;&#23454;&#26045;&#26041;&#27861;&#65292;&#24182;&#36880;&#19968;&#35299;&#37322;&#20102;&#20854;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
An assurance case is a structured argument, typically produced by safety engineers, to communicate confidence that a critical or complex system, such as an aircraft, will be acceptably safe within its intended context. Assurance cases often inform third party approval of a system. One emerging proposition within the trustworthy AI and autonomous systems (AI/AS) research community is to use assurance cases to instil justified confidence that specific AI/AS will be ethically acceptable when operational in well-defined contexts. This paper substantially develops the proposition and makes it concrete. It brings together the assurance case methodology with a set of ethical principles to structure a principles-based ethics assurance argument pattern. The principles are justice, beneficence, non-maleficence, and respect for human autonomy, with the principle of transparency playing a supporting role. The argument pattern, shortened to the acronym PRAISE, is described. The objective of the pro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#23545;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#21160;&#20316;&#29305;&#24449;&#21644;&#35266;&#27979;&#29305;&#24449;&#20043;&#38388;&#35821;&#20041;&#20851;&#31995;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#32852;&#21512;&#22788;&#29702;&#35266;&#23519;&#29305;&#24449;&#21644;&#21160;&#20316;&#29305;&#24449;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26550;&#26500;&#21487;&#20197;&#23398;&#20064;&#30452;&#35273;&#31574;&#30053;&#65292;&#24182;&#19988;&#36825;&#26679;&#30340;&#20195;&#29702;&#19982;&#20154;&#31867;&#21327;&#20316;&#32780;&#26080;&#38656;&#25509;&#21463;&#20154;&#31867;&#25968;&#25454;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2201.12658</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#20316;&#29305;&#24449;&#23398;&#20064;&#30452;&#35273;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Intuitive Policies Using Action Features. (arXiv:2201.12658v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#23545;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#21160;&#20316;&#29305;&#24449;&#21644;&#35266;&#27979;&#29305;&#24449;&#20043;&#38388;&#35821;&#20041;&#20851;&#31995;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#32852;&#21512;&#22788;&#29702;&#35266;&#23519;&#29305;&#24449;&#21644;&#21160;&#20316;&#29305;&#24449;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26550;&#26500;&#21487;&#20197;&#23398;&#20064;&#30452;&#35273;&#31574;&#30053;&#65292;&#24182;&#19988;&#36825;&#26679;&#30340;&#20195;&#29702;&#19982;&#20154;&#31867;&#21327;&#20316;&#32780;&#26080;&#38656;&#25509;&#21463;&#20154;&#31867;&#25968;&#25454;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#26159;&#20351;AI&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#21160;&#20316;&#29305;&#24449;&#21644;&#35266;&#27979;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#20154;&#31867;&#20197;&#39640;&#24230;&#30452;&#35273;&#30340;&#26041;&#24335;&#21033;&#29992;&#36825;&#20123;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#23545;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#36825;&#20123;&#35821;&#20041;&#20851;&#31995;&#30340;&#20542;&#21521;&#30340;&#24433;&#21709;&#12290;&#22312;&#19968;&#20010;&#31243;&#24207;&#29983;&#25104;&#30340;&#21327;&#20316;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#32852;&#21512;&#22788;&#29702;&#35266;&#23519;&#29305;&#24449;&#21644;&#21160;&#20316;&#29305;&#24449;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26550;&#26500;&#20855;&#26377;&#26356;&#22909;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#21487;&#20197;&#23398;&#20064;&#30452;&#35273;&#31574;&#30053;&#12290;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35780;&#20272;&#21644;&#22330;&#26223;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24471;&#21040;&#30340;&#31574;&#30053;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#26679;&#30340;&#20195;&#29702;&#19982;&#20154;&#31867;&#21327;&#20316;&#32780;&#26080;&#38656;&#25509;&#21463;&#20219;&#20309;&#20154;&#31867;&#25968;&#25454;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
An unaddressed challenge in multi-agent coordination is to enable AI agents to exploit the semantic relationships between the features of actions and the features of observations. Humans take advantage of these relationships in highly intuitive ways. For instance, in the absence of a shared language, we might point to the object we desire or hold up our fingers to indicate how many objects we want. To address this challenge, we investigate the effect of network architecture on the propensity of learning algorithms to exploit these semantic relationships. Across a procedurally generated coordination task, we find that attention-based architectures that jointly process a featurized representation of observations and actions have a better inductive bias for learning intuitive policies. Through fine-grained evaluation and scenario analysis, we show that the resulting policies are human-interpretable. Moreover, such agents coordinate with people without training on any human data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Deep Serial Number (DSN) &#30340;&#27700;&#21360;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#12290;&#35813;&#31639;&#27861;&#22312;DNN&#20013;&#23454;&#29616;&#24207;&#21015;&#21495;&#23884;&#20837;&#65292;&#21482;&#26377;&#36755;&#20837;&#26377;&#25928;&#24207;&#21015;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;DNN&#25165;&#33021;&#27491;&#30830;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2011.08960</link><description>&lt;p&gt;
&#28145;&#24230;&#24207;&#21015;&#21495;&#65306;&#29992;&#20110; DNN &#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#35745;&#31639;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Deep Serial Number: Computational Watermarking for DNN Intellectual Property Protection. (arXiv:2011.08960v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.08960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Deep Serial Number (DSN) &#30340;&#27700;&#21360;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#12290;&#35813;&#31639;&#27861;&#22312;DNN&#20013;&#23454;&#29616;&#24207;&#21015;&#21495;&#23884;&#20837;&#65292;&#21482;&#26377;&#36755;&#20837;&#26377;&#25928;&#24207;&#21015;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;DNN&#25165;&#33021;&#27491;&#30830;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35774;&#35745;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27700;&#21360;&#31639;&#27861; Deep Serial Number&#65288;DSN&#65289;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#22312;DNN&#20013;&#24341;&#20837;&#26631;&#35782;&#20449;&#21495;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;DNN&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#38459;&#27490;&#20102;&#25915;&#20987;&#32773;&#20351;&#29992;&#31363;&#21462;&#30340;&#32593;&#32476;&#12290;&#20511;&#37492;&#24207;&#21015;&#21495;&#22312;&#20445;&#25252;&#20256;&#32479;&#36719;&#20214;&#30693;&#35782;&#20135;&#26435;&#26041;&#38754;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;DNN&#20013;&#23454;&#29616;&#24207;&#21015;&#21495;&#23884;&#20837;&#30340;&#31532;&#19968;&#20010;&#23454;&#29616;&#12290;&#20026;&#27492;&#65292;DSN&#34987;&#38598;&#25104;&#21040;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#20013;&#65292;&#20854;&#20013;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#31169;&#26377;&#30340;&#25945;&#24072;DNN&#12290;&#38543;&#21518;&#65292;&#20854;&#30693;&#35782;&#34987;&#25552;&#28860;&#24182;&#20256;&#25480;&#32473;&#19968;&#31995;&#21015;&#23450;&#21046;&#30340;&#23398;&#29983;DNN&#12290;&#27599;&#20010;&#23458;&#25143;DNN&#20165;&#22312;&#36755;&#20837;&#26377;&#25928;&#24207;&#21015;&#21495;&#30340;&#24773;&#20917;&#19979;&#25165;&#33021;&#27491;&#30830;&#24037;&#20316;&#12290;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DSN&#22312;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#20351;&#29992;&#30340;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#21407;&#22987;DNN&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present DSN (Deep Serial Number), a simple yet effective watermarking algorithm designed specifically for deep neural networks (DNNs). Unlike traditional methods that incorporate identification signals into DNNs, our approach explores a novel Intellectual Property (IP) protection mechanism for DNNs, effectively thwarting adversaries from using stolen networks. Inspired by the success of serial numbers in safeguarding conventional software IP, we propose the first implementation of serial number embedding within DNNs. To achieve this, DSN is integrated into a knowledge distillation framework, in which a private teacher DNN is initially trained. Subsequently, its knowledge is distilled and imparted to a series of customized student DNNs. Each customer DNN functions correctly only upon input of a valid serial number. Experimental results across various applications demonstrate DSN's efficacy in preventing unauthorized usage without compromising the original DNN performan
&lt;/p&gt;</description></item></channel></rss>