<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#25968;&#23398;&#38382;&#39064;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#40723;&#21169;&#31526;&#21495;&#21270;&#25512;&#29702;&#19982;&#25968;&#20540;&#31572;&#26696;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2308.01906</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#31526;&#21495;&#21270;&#25968;&#23398;&#38382;&#39064;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning in Large Language Models Through Symbolic Math Word Problems. (arXiv:2308.01906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#25968;&#23398;&#38382;&#39064;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#40723;&#21169;&#31526;&#21495;&#21270;&#25512;&#29702;&#19982;&#25968;&#20540;&#31572;&#26696;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#35299;&#20915;&#20960;&#20046;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26041;&#24335;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#22810;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#25968;&#23398;&#38382;&#39064;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#22240;&#20026;&#31526;&#21495;&#34920;&#36798;&#26159;&#23545;&#25968;&#20540;&#31572;&#26696;&#30340;&#8220;&#31616;&#26126;&#35299;&#37322;&#8221;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#20351;&#29992;&#20102;SVAMP&#25968;&#25454;&#38598;&#30340;&#31526;&#21495;&#21270;&#29256;&#26412;&#65292;&#24182;&#21457;&#29616;GPT-3&#30340;davinci-002&#27169;&#22411;&#22312;&#31526;&#21495;&#21270;&#25968;&#23398;&#38382;&#39064;&#19978;&#20063;&#20855;&#26377;&#33391;&#22909;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#19981;&#20165;&#32771;&#34385;&#20934;&#30830;&#29575;&#65292;&#36824;&#35780;&#20272;&#26368;&#32456;&#31572;&#26696;&#21644;&#25512;&#29702;&#32467;&#26524;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#25968;&#20540;&#21644;&#31526;&#21495;&#21270;&#31572;&#26696;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#40723;&#21169;&#31526;&#21495;&#21270;&#25512;&#29702;&#19982;&#25968;&#20540;&#31572;&#26696;&#20445;&#25345;&#19968;&#33268;&#65292;&#20174;&#32780;&#20351;LLM&#33021;&#22815;&#25552;&#20379;&#31616;&#26126;&#19988;&#21487;&#39564;&#35777;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a "concise explanation" of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21487;&#21464;&#24418;&#21367;&#31215;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21487;&#21464;&#24418;&#26680;&#21367;&#31215;&#20316;&#20026;&#19968;&#27425;&#36890;&#36807;&#30340;&#23436;&#21892;&#27169;&#22359;&#30340;&#26377;&#25928;&#26550;&#26500;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#21464;&#24418;&#21367;&#31215;&#22312;&#28145;&#24230;&#23436;&#25104;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01905</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#28145;&#24230;&#23436;&#25104;&#30340;&#21487;&#21464;&#24418;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Revisiting Deformable Convolution for Depth Completion. (arXiv:2308.01905v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21487;&#21464;&#24418;&#21367;&#31215;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21487;&#21464;&#24418;&#26680;&#21367;&#31215;&#20316;&#20026;&#19968;&#27425;&#36890;&#36807;&#30340;&#23436;&#21892;&#27169;&#22359;&#30340;&#26377;&#25928;&#26550;&#26500;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#21464;&#24418;&#21367;&#31215;&#22312;&#28145;&#24230;&#23436;&#25104;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23436;&#25104;&#26088;&#22312;&#20174;&#31232;&#30095;&#28145;&#24230;&#22270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23494;&#38598;&#28145;&#24230;&#22270;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;RGB&#22270;&#20687;&#20316;&#20026;&#24341;&#23548;&#65292;&#24182;&#24341;&#20837;&#36845;&#20195;&#30340;&#31354;&#38388;&#20256;&#25773;&#26469;&#23436;&#21892;&#20272;&#35745;&#30340;&#31895;&#31961;&#28145;&#24230;&#22270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20256;&#25773;&#23436;&#21892;&#26041;&#27861;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#65292;&#24182;&#19988;&#23384;&#22312;&#22266;&#23450;&#30340;&#24863;&#21463;&#37326;&#65292;&#21487;&#33021;&#21253;&#21547;&#19982;&#38750;&#24120;&#31232;&#30095;&#36755;&#20837;&#26080;&#20851;&#21644;&#26080;&#29992;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21487;&#21464;&#24418;&#21367;&#31215;&#30340;&#24605;&#24819;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#21487;&#21464;&#24418;&#26680;&#21367;&#31215;&#20316;&#20026;&#19968;&#27425;&#36890;&#36807;&#30340;&#23436;&#21892;&#27169;&#22359;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#21487;&#21464;&#24418;&#21367;&#31215;&#30340;&#21151;&#33021;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#28145;&#24230;&#23436;&#25104;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#21508;&#31181;&#20195;&#34920;&#24615;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19982;&#20197;&#24448;&#24037;&#20316;&#19981;&#21516;&#65292;&#21487;&#21464;&#24418;&#21367;&#31215;&#22312;&#28145;&#24230;&#23436;&#25104;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years. Previous work usually employs RGB images as guidance, and introduces iterative spatial propagation to refine estimated coarse depth maps. However, most of the propagation refinement methods require several iterations and suffer from a fixed receptive field, which may contain irrelevant and useless information with very sparse input. In this paper, we address these two challenges simultaneously by revisiting the idea of deformable convolution. We propose an effective architecture that leverages deformable kernel convolution as a single-pass refinement module, and empirically demonstrate its superiority. To better understand the function of deformable convolution and exploit it for depth completion, we further systematically investigate a variety of representative strategies. Our study reveals that, different from prior work, deformable convol
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#22312;arXiv&#19978;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#37327;&#21270;&#20102;&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#26368;&#32456;&#34987;&#21360;&#21047;&#22312;&#21516;&#34892;&#35780;&#23457;&#30340;&#26399;&#21002;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#21360;&#26412;&#21644;&#26368;&#32456;&#21457;&#34920;&#29256;&#26412;&#30340;&#23545;&#24212;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#26144;&#23556;&#26041;&#27861;&#20351;&#29992;BERT&#12290;</title><link>http://arxiv.org/abs/2308.01899</link><description>&lt;p&gt;
&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#23454;&#38469;&#19978;&#34987;&#21360;&#21047;&#20102;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#65306;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#22312;arXiv&#19978;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How many preprints have actually been printed and why: a case study of computer science preprints on arXiv. (arXiv:2308.01899v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#22312;arXiv&#19978;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#37327;&#21270;&#20102;&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#26368;&#32456;&#34987;&#21360;&#21047;&#22312;&#21516;&#34892;&#35780;&#23457;&#30340;&#26399;&#21002;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#21360;&#26412;&#21644;&#26368;&#32456;&#21457;&#34920;&#29256;&#26412;&#30340;&#23545;&#24212;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#26144;&#23556;&#26041;&#27861;&#20351;&#29992;BERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#21360;&#26412;&#22312;&#23398;&#26415;&#30028;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#27491;&#24335;&#25552;&#20132;&#21040;&#26399;&#21002;&#25110;&#20250;&#35758;&#20043;&#21069;&#23558;&#20182;&#20204;&#30340;&#25163;&#31295;&#21457;&#24067;&#21040;&#39044;&#21360;&#26412;&#26381;&#21153;&#22120;&#19978;&#30340;&#21407;&#22240;&#26377;&#24456;&#22810;&#65292;&#20294;&#39044;&#21360;&#26412;&#30340;&#20351;&#29992;&#20063;&#24341;&#21457;&#20102;&#19981;&#23569;&#20105;&#35758;&#65292;&#23588;&#20854;&#26159;&#19982;&#20248;&#20808;&#26435;&#30340;&#22768;&#26126;&#26377;&#20851;&#12290;&#26412;&#25991;&#23545;2008&#24180;&#33267;2017&#24180;&#26399;&#38388;&#25552;&#20132;&#21040;arXiv&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#39044;&#21360;&#26412;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#37327;&#21270;&#26368;&#32456;&#26377;&#22810;&#23569;&#39044;&#21360;&#26412;&#22312;&#21516;&#34892;&#35780;&#23457;&#30340;&#22330;&#21512;&#20013;&#34987;&#21360;&#21047;&#12290;&#22312;&#36825;&#20123;&#24050;&#21457;&#34920;&#30340;&#25163;&#31295;&#20013;&#65292;&#26377;&#20123;&#20197;&#19981;&#21516;&#30340;&#26631;&#39064;&#21457;&#34920;&#65292;&#19988;&#26410;&#26356;&#26032;arXiv&#19978;&#30340;&#39044;&#21360;&#26412;&#12290;&#23545;&#20110;&#36825;&#20123;&#25163;&#31295;&#65292;&#20256;&#32479;&#30340;&#27169;&#31946;&#21305;&#37197;&#26041;&#27861;&#26080;&#27861;&#23558;&#39044;&#21360;&#26412;&#19982;&#26368;&#32456;&#21457;&#34920;&#29256;&#26412;&#23545;&#24212;&#36215;&#26469;&#12290;&#37492;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30340;&#26144;&#23556;&#26041;&#27861;&#65292;&#21033;&#29992;Transformers&#20013;&#30340;Bidirectional Encoder Representations (BERT)&#12290;&#21033;&#29992;&#36825;&#31181;&#26032;&#30340;&#26144;&#23556;&#26041;&#27861;&#21644;&#22810;&#31181;&#25968;&#25454;&#26469;&#28304;...
&lt;/p&gt;
&lt;p&gt;
Preprints play an increasingly critical role in academic communities. There are many reasons driving researchers to post their manuscripts to preprint servers before formal submission to journals or conferences, but the use of preprints has also sparked considerable controversy, especially surrounding the claim of priority. In this paper, a case study of computer science preprints submitted to arXiv from 2008 to 2017 is conducted to quantify how many preprints have eventually been printed in peer-reviewed venues. Among those published manuscripts, some are published under different titles and without an update to their preprints on arXiv. In the case of these manuscripts, the traditional fuzzy matching method is incapable of mapping the preprint to the final published version. In view of this issue, we introduce a semantics-based mapping method with the employment of Bidirectional Encoder Representations from Transformers (BERT). With this new mapping method and a plurality of data sou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25913;&#36827;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#21644;&#23384;&#20648;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33988;&#27700;&#27744;&#25277;&#26679;&#21644;&#20854;&#20182;&#26367;&#20195;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#23547;&#25214;&#26368;&#20339;&#23384;&#20648;&#26679;&#26412;&#25968;&#37327;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.01895</link><description>&lt;p&gt;
&#25913;&#36827;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#21644;&#23384;&#20648;&#20197;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning. (arXiv:2308.01895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25913;&#36827;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#21644;&#23384;&#20648;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33988;&#27700;&#27744;&#25277;&#26679;&#21644;&#20854;&#20182;&#26367;&#20195;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#23547;&#25214;&#26368;&#20339;&#23384;&#20648;&#26679;&#26412;&#25968;&#37327;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#20351;&#28145;&#24230;&#23398;&#20064;&#32773;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#26410;&#30693;&#38271;&#24230;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#20250;&#36951;&#24536;&#20197;&#21069;&#30340;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22238;&#25918;&#65292;&#21363;&#23558;&#23569;&#37327;&#20808;&#21069;&#30340;&#32463;&#39564;&#23384;&#20648;&#22312;&#20869;&#23384;&#20013;&#65292;&#24182;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#26102;&#37325;&#26032;&#25773;&#25918;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#22312;&#36873;&#25321;&#26368;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#23384;&#20648;&#20197;&#21450;&#30830;&#23450;&#25152;&#38656;&#23384;&#20648;&#26679;&#26412;&#30340;&#26368;&#20339;&#25968;&#37327;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;&#24120;&#29992;&#30340;&#33988;&#27700;&#27744;&#25277;&#26679;&#19982;&#21508;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25214;&#21040;&#26368;&#20339;&#30340;&#23384;&#20648;&#26679;&#26412;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning seeks to enable deep learners to train on a series of tasks of unknown length without suffering from the catastrophic forgetting of previous tasks. One effective solution is replay, which involves storing few previous experiences in memory and replaying them when learning the current task. However, there is still room for improvement when it comes to selecting the most informative samples for storage and determining the optimal number of samples to be stored. This study aims to address these issues with a novel comparison of the commonly used reservoir sampling to various alternative population strategies and providing a novel detailed analysis of how to find the optimal number of stored samples.
&lt;/p&gt;</description></item><item><title>"Thespian"&#26159;&#19968;&#31181;&#22810;&#35282;&#33394;&#30340;&#25991;&#26412;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#20855;&#26377;&#23398;&#20064;&#27169;&#20223;&#22810;&#20010;&#35282;&#33394;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26580;&#24615;&#25552;&#31034;&#36827;&#34892;&#25351;&#23548;&#12290;&#35813;&#26694;&#26550;&#36824;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#20197;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#26032;&#35282;&#33394;&#65292;&#24182;&#22312;&#22810;&#35282;&#33394;&#23398;&#20064;&#21644;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.01872</link><description>&lt;p&gt;
Thespian: &#22810;&#35282;&#33394;&#25991;&#26412;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Thespian: Multi-Character Text Role-Playing Game Agents. (arXiv:2308.01872v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01872
&lt;/p&gt;
&lt;p&gt;
"Thespian"&#26159;&#19968;&#31181;&#22810;&#35282;&#33394;&#30340;&#25991;&#26412;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#20855;&#26377;&#23398;&#20064;&#27169;&#20223;&#22810;&#20010;&#35282;&#33394;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26580;&#24615;&#25552;&#31034;&#36827;&#34892;&#25351;&#23548;&#12290;&#35813;&#26694;&#26550;&#36824;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#20197;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#26032;&#35282;&#33394;&#65292;&#24182;&#22312;&#22810;&#35282;&#33394;&#23398;&#20064;&#21644;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20882;&#38505;&#28216;&#25103;&#21644;&#25991;&#26412;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#26159;&#24378;&#21270;&#23398;&#20064;&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#25991;&#26412;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#26159;&#24320;&#25918;&#24335;&#29615;&#22659;&#65292;&#26234;&#33021;&#20307;&#24517;&#39035;&#24544;&#23454;&#22320;&#25198;&#28436;&#29305;&#23450;&#35282;&#33394;&#12290;&#25991;&#31456;&#32771;&#34385;&#21040;&#20102;&#23383;&#31526;&#21644;&#28436;&#21592;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#28436;&#21592;&#26234;&#33021;&#20307;&#33021;&#22815;&#25198;&#28436;&#22810;&#20010;&#35282;&#33394;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"Thespian"&#30340;&#26694;&#26550;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#27169;&#20223;&#22810;&#20010;&#35282;&#33394;&#65292;&#24182;&#20351;&#29992;&#26580;&#24615;&#25552;&#31034;&#26469;&#25351;&#23548;&#20854;&#22312;&#20219;&#20309;&#26102;&#20505;&#25198;&#28436;&#29305;&#23450;&#35282;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#27880;&#24847;&#26426;&#21046;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#20197;&#23569;&#37327;&#31034;&#20363;&#30340;&#26041;&#24335;&#23398;&#20064;&#22522;&#20110;&#20808;&#21069;&#23398;&#20064;&#30340;&#35282;&#33394;&#30340;&#26032;&#35282;&#33394;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#22312;&#22810;&#35282;&#33394;&#23398;&#20064;&#21644;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-adventure games and text role-playing games are grand challenges for reinforcement learning game playing agents. Text role-playing games are open-ended environments where an agent must faithfully play a particular character. We consider the distinction between characters and actors, where an actor agent has the ability to play multiple characters. We present a framework we call a thespian agent that can learn to emulate multiple characters along with a soft prompt that can be used to direct it as to which character to play at any time. We further describe an attention mechanism that allows the agent to learn new characters that are based on previously learned characters in a few-shot fashion. We show that our agent outperforms the state of the art agent framework in multi-character learning and few-shot learning.
&lt;/p&gt;</description></item><item><title>ClassEval&#26159;&#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#22312;&#36825;&#19968;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#12290;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.01861</link><description>&lt;p&gt;
ClassEval: &#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation. (arXiv:2308.01861v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01861
&lt;/p&gt;
&lt;p&gt;
ClassEval&#26159;&#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#22312;&#36825;&#19968;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#12290;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20195;&#30721;&#29983;&#25104;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#21363;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#25163;&#21160;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;ClassEval&#65292;&#20854;&#20013;&#21253;&#21547;100&#20010;&#31867;&#32423;Python&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24635;&#20849;&#32791;&#26102;&#32422;500&#20154;&#23567;&#26102;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23545;11&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#30740;&#31350;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#29616;&#26377;&#30340;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#35201;&#36828;&#20302;&#20110;&#29420;&#31435;&#30340;&#26041;&#27861;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65288;&#22914;HumanEval&#65289;&#65307;&#32780;&#26041;&#27861;&#32423;&#30340;&#32534;&#30721;&#33021;&#21147;&#19981;&#33021;&#31561;&#21516;&#22320;&#21453;&#26144;LLMs&#22312;&#31867;&#32423;&#32534;&#30721;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#20173;&#28982;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#65292;&#32780;&#20108;&#32447;&#27169;&#22411;&#21253;&#25324;Instruct-Starcoder&#65292;Instruct-Codegen&#21644;Wizardcoder&#22312;&#24615;&#33021;&#19978;&#38750;&#24120;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#36830;&#36143;&#37319;&#26679;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#26399;&#20154;&#31867;&#21160;&#20316;&#29983;&#25104;&#20013;&#30340;&#36830;&#36143;&#24615;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01850</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#37319;&#26679;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#38271;&#26399;&#20154;&#31867;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling. (arXiv:2308.01850v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#36830;&#36143;&#37319;&#26679;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#26399;&#20154;&#31867;&#21160;&#20316;&#29983;&#25104;&#20013;&#30340;&#36830;&#36143;&#24615;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#21160;&#20316;&#30340;&#29983;&#25104;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#38480;&#20110;&#29983;&#25104;&#19982;&#25551;&#36848;&#21333;&#20010;&#21160;&#20316;&#30340;&#21333;&#20010;&#21477;&#23376;&#30456;&#23545;&#24212;&#30340;&#30701;&#26399;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#24403;&#19968;&#20010;&#25991;&#26412;&#27969;&#25551;&#36848;&#19968;&#31995;&#21015;&#36830;&#32493;&#30340;&#21160;&#20316;&#26102;&#65292;&#19982;&#27599;&#20010;&#21477;&#23376;&#30456;&#23545;&#24212;&#30340;&#29983;&#25104;&#30340;&#21160;&#20316;&#21487;&#33021;&#19981;&#26159;&#36830;&#36143;&#30340;&#12290;&#29616;&#26377;&#30340;&#38271;&#26399;&#21160;&#20316;&#29983;&#25104;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#19981;&#33021;&#30452;&#25509;&#29983;&#25104;&#36830;&#36143;&#30340;&#21160;&#20316;&#65292;&#38656;&#35201;&#20854;&#20182;&#25805;&#20316;&#65288;&#22914;&#25554;&#20540;&#65289;&#26469;&#22788;&#29702;&#29983;&#25104;&#30340;&#21160;&#20316;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#21518;&#32493;&#21160;&#20316;&#65292;&#32780;&#19981;&#32771;&#34385;&#26410;&#26469;&#21160;&#20316;&#23545;&#20808;&#21069;&#21160;&#20316;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36807;&#21435;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#20004;&#31181;&#21487;&#36873;&#30340;&#36830;&#36143;&#37319;&#26679;&#26041;&#27861;&#65306;&#36807;&#21435;&#20462;&#34917;&#37319;&#26679;&#21644;&#32452;&#21512;&#36716;&#25442;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-motion generation has gained increasing attention, but most existing methods are limited to generating short-term motions that correspond to a single sentence describing a single action. However, when a text stream describes a sequence of continuous motions, the generated motions corresponding to each sentence may not be coherently linked. Existing long-term motion generation methods face two main issues. Firstly, they cannot directly generate coherent motions and require additional operations such as interpolation to process the generated actions. Secondly, they generate subsequent actions in an autoregressive manner without considering the influence of future actions on previous ones. To address these issues, we propose a novel approach that utilizes a past-conditioned diffusion model with two optional coherent sampling methods: Past Inpainting Sampling and Compositional Transition Sampling. Past Inpainting Sampling completes subsequent motions by treating previous motions as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;URET&#30340;&#36890;&#29992;&#40065;&#26834;&#24615;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#35813;&#24037;&#20855;&#21253;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#31867;&#22411;&#21644;&#20219;&#21153;&#39046;&#22495;&#19979;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#20197;&#30830;&#20445;&#20851;&#38190;AI&#20219;&#21153;&#30340;&#23433;&#20840;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01840</link><description>&lt;p&gt;
URET: &#36890;&#29992;&#40065;&#26834;&#24615;&#35780;&#20272;&#24037;&#20855;&#21253;&#65288;&#29992;&#20110;&#36867;&#36991;&#25915;&#20987;&#65289;
&lt;/p&gt;
&lt;p&gt;
URET: Universal Robustness Evaluation Toolkit (for Evasion). (arXiv:2308.01840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;URET&#30340;&#36890;&#29992;&#40065;&#26834;&#24615;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#35813;&#24037;&#20855;&#21253;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#31867;&#22411;&#21644;&#20219;&#21153;&#39046;&#22495;&#19979;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#20197;&#30830;&#20445;&#20851;&#38190;AI&#20219;&#21153;&#30340;&#23433;&#20840;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36867;&#36991;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#25152;&#31034;&#12290;&#20805;&#20998;&#20102;&#35299;&#36825;&#31181;&#25915;&#20987;&#23545;&#20110;&#30830;&#20445;&#20851;&#38190;AI&#20219;&#21153;&#30340;&#23433;&#20840;&#21644;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36867;&#36991;&#25915;&#20987;&#24456;&#38590;&#23545;&#22823;&#22810;&#25968;AI&#31995;&#32479;&#36827;&#34892;&#37096;&#32626;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#38598;&#20013;&#22312;&#22270;&#20687;&#39046;&#22495;&#24182;&#20855;&#26377;&#23569;&#25968;&#32422;&#26463;&#12290;&#19982;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#20854;&#20182;&#36755;&#20837;&#31867;&#22411;&#19981;&#21516;&#65292;&#22270;&#20687;&#30001;&#22343;&#21248;&#30340;&#12289;&#25968;&#20540;&#30340;&#12289;&#36830;&#32493;&#30340;&#21644;&#29420;&#31435;&#30340;&#29305;&#24449;&#32452;&#25104;&#12290;&#27492;&#22806;&#65292;&#26576;&#20123;&#36755;&#20837;&#31867;&#22411;&#21253;&#21547;&#39069;&#22806;&#30340;&#35821;&#20041;&#21644;&#21151;&#33021;&#32422;&#26463;&#65292;&#24517;&#39035;&#36981;&#23432;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#36755;&#20837;&#31867;&#22411;&#21644;&#20219;&#21153;&#39046;&#22495;&#26080;&#20851;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#21644;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#36755;&#20837;&#36716;&#25442;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#21457;&#29616;&#19968;&#31995;&#21015;&#36716;&#25442;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#31526;&#21512;&#35821;&#20041;&#21644;&#21151;&#33021;&#35201;&#27714;&#30340;&#27491;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24739;&#32773;&#37319;&#35775;&#21644;&#20020;&#24202;&#25551;&#36848;&#39044;&#27979;&#31934;&#31070;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#35780;&#20998;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#19982;&#20154;&#31867;&#20020;&#24202;&#35780;&#20272;&#32773;&#30340;&#32467;&#26524;&#30456;&#36817;&#65292;&#25581;&#31034;&#20102;&#36890;&#29992;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01834</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Capability of Large Language Models to Measure Psychiatric Functioning. (arXiv:2308.01834v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24739;&#32773;&#37319;&#35775;&#21644;&#20020;&#24202;&#25551;&#36848;&#39044;&#27979;&#31934;&#31070;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#35780;&#20998;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#19982;&#20154;&#31867;&#20020;&#24202;&#35780;&#20272;&#32773;&#30340;&#32467;&#26524;&#30456;&#36817;&#65292;&#25581;&#31034;&#20102;&#36890;&#29992;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#21307;&#23398;&#30693;&#35782;&#65288;Med-PaLM 2&#65289;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27809;&#26377;&#32463;&#36807;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24739;&#32773;&#37319;&#35775;&#21644;&#20020;&#24202;&#25551;&#36848;&#26469;&#39044;&#27979;&#31934;&#31070;&#21151;&#33021;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#26469;&#25552;&#21462;&#20272;&#35745;&#30340;&#20020;&#24202;&#35780;&#20998;&#21644;&#35786;&#26029;&#65292;&#20998;&#26512;&#20102;145&#20363;&#25233;&#37057;&#30151;&#21644;115&#20363;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#35780;&#20272;&#20197;&#21450;46&#20363;&#20020;&#24202;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Med-PaLM 2&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#31934;&#31070;&#30142;&#30149;&#20013;&#35780;&#20272;&#31934;&#31070;&#21151;&#33021;&#65292;&#20854;&#20013;&#23545;&#22522;&#20110;&#26631;&#20934;&#35780;&#20272;&#30340;&#25233;&#37057;&#30151;&#35780;&#20998;&#30340;&#39044;&#27979;&#34920;&#29616;&#26368;&#20339;&#65288;&#20934;&#30830;&#29575;&#33539;&#22260;= 0.80-0.84&#65289;&#65292;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#22312;&#32479;&#35745;&#19978;&#19982;&#20154;&#31867;&#20020;&#24202;&#35780;&#20272;&#32773;&#30340;&#32467;&#26524;&#26080;&#27861;&#21306;&#20998;&#65288;t(1,144) = 1.20&#65292;p = 0.23&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#36890;&#29992;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#31934;&#31070;&#29366;&#20917;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current work investigates the capability of Large language models (LLMs) that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2) to predict psychiatric functioning from patient interviews and clinical descriptions without being trained to do so. To assess this, n = 145 depression and n =115 PTSD assessments and n = 46 clinical case studies across high prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma and stress, Addictive disorders) were analyzed using prompts to extract estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is capable of assessing psychiatric functioning across a range of psychiatric conditions with the strongest performance being the prediction of depression scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which were statistically indistinguishable from human clinical raters t(1,144) = 1.20; p = 0.23. Results show the potential for general clinical language models to f
&lt;/p&gt;</description></item><item><title>&#26790;&#22659;&#21644;&#24819;&#35937;&#19981;&#20165;&#20165;&#26159;&#24863;&#35273;&#36755;&#20837;&#30340;&#20877;&#29616;&#65292;&#23427;&#20204;&#23545;&#20110;&#22609;&#36896;&#22823;&#33041;&#30382;&#23618;&#30340;&#35821;&#20041;&#34920;&#31034;&#21516;&#26679;&#37325;&#35201;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#34394;&#25311;&#32463;&#39564;&#26469;&#24433;&#21709;&#21644;&#32452;&#32455;&#36825;&#20123;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01830</link><description>&lt;p&gt;
&#36229;&#36234;&#24863;&#35273;&#30340;&#23398;&#20064;: &#26790;&#22914;&#20309;&#32452;&#32455;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning beyond sensations: how dreams organize neuronal representations. (arXiv:2308.01830v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01830
&lt;/p&gt;
&lt;p&gt;
&#26790;&#22659;&#21644;&#24819;&#35937;&#19981;&#20165;&#20165;&#26159;&#24863;&#35273;&#36755;&#20837;&#30340;&#20877;&#29616;&#65292;&#23427;&#20204;&#23545;&#20110;&#22609;&#36896;&#22823;&#33041;&#30382;&#23618;&#30340;&#35821;&#20041;&#34920;&#31034;&#21516;&#26679;&#37325;&#35201;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#34394;&#25311;&#32463;&#39564;&#26469;&#24433;&#21709;&#21644;&#32452;&#32455;&#36825;&#20123;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#24863;&#35273;&#30382;&#23618;&#20013;&#30340;&#35821;&#20041;&#34920;&#31034;&#26159;&#31283;&#20581;&#32780;&#28789;&#27963;&#34892;&#20026;&#30340;&#22522;&#30784;&#12290;&#36825;&#20123;&#34920;&#31034;&#26159;&#22312;&#21457;&#32946;&#36807;&#31243;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#33719;&#24471;&#30340;&#65292;&#24182;&#22312;&#29983;&#29289;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#25345;&#32493;&#32500;&#25345;&#12290;&#39044;&#27979;&#24615;&#23398;&#20064;&#29702;&#35770;&#35748;&#20026;&#65292;&#36825;&#20123;&#34920;&#31034;&#26159;&#36890;&#36807;&#39044;&#27979;&#25110;&#37325;&#24314;&#24863;&#35273;&#36755;&#20837;&#32780;&#20135;&#29983;&#30340;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22823;&#33041;&#20250;&#20135;&#29983;&#34394;&#25311;&#32463;&#39564;&#65292;&#20363;&#22914;&#22312;&#24819;&#35937;&#21644;&#26790;&#22659;&#20013;&#65292;&#36229;&#36234;&#20808;&#21069;&#32463;&#21382;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#34394;&#25311;&#32463;&#39564;&#21487;&#33021;&#19982;&#23454;&#38469;&#24863;&#35273;&#36755;&#20837;&#21516;&#26679;&#37325;&#35201;&#65292;&#33021;&#22815;&#22609;&#36896;&#30382;&#23618;&#34920;&#31034;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36890;&#36807;&#29983;&#25104;&#34394;&#25311;&#32463;&#39564;&#26469;&#32452;&#32455;&#34920;&#31034;&#30340;&#20004;&#20010;&#20114;&#34917;&#23398;&#20064;&#21407;&#21017;&#12290;&#39318;&#20808;&#65292;&#8220;&#23545;&#25239;&#24615;&#26790;&#24819;&#8221;&#25552;&#20986;&#21019;&#36896;&#24615;&#26790;&#24819;&#25903;&#25345;&#20102;&#22312;&#30382;&#23618;&#20013;&#23454;&#29616;&#23545;&#25239;&#24615;&#23398;&#20064;&#65292;&#20854;&#20013;&#21453;&#39304;&#21644;&#21069;&#39304;&#36335;&#24452;&#21442;&#19982;&#21040;&#30456;&#20114;&#27450;&#39575;&#30340;&#26377;&#30410;&#28216;&#25103;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic representations in higher sensory cortices form the basis for robust, yet flexible behavior. These representations are acquired over the course of development in an unsupervised fashion and continuously maintained over an organism's lifespan. Predictive learning theories propose that these representations emerge from predicting or reconstructing sensory inputs. However, brains are known to generate virtual experiences, such as during imagination and dreaming, that go beyond previously experienced inputs. Here, we suggest that virtual experiences may be just as relevant as actual sensory inputs in shaping cortical representations.In particular, we discuss two complementary learning principles that organize representations through the generation of virtual experiences. First, "adversarial dreaming" proposes that creative dreams support a cortical implementation of adversarial learning in which feedback and feedforward pathways engage in a productive game of trying to fool each o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38024;&#23545;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#19981;&#20844;&#24179;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#22256;&#38590;&#23545;&#25239;&#26679;&#26412;&#25366;&#25496;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01823</link><description>&lt;p&gt;
&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#22256;&#38590;&#23545;&#25239;&#26679;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Hard Adversarial Example Mining for Improving Robust Fairness. (arXiv:2308.01823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38024;&#23545;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#19981;&#20844;&#24179;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#22256;&#38590;&#23545;&#25239;&#26679;&#26412;&#25366;&#25496;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AE&#65289;&#40065;&#26834;&#24615;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#35266;&#23519;&#21457;&#29616;&#65292;&#36825;&#20010;&#38480;&#21046;&#21487;&#33021;&#26159;&#30001;&#20110;&#20005;&#37325;&#30340;&#23545;&#25239;&#32622;&#20449;&#36807;&#25311;&#21512;&#65292;&#21363;&#26576;&#20123;&#20855;&#26377;&#36807;&#24230;&#33258;&#20449;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HAM&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#22256;&#38590;&#23545;&#25239;&#26679;&#26412;&#25366;&#25496;&#12290;HAM&#38598;&#20013;&#20110;&#20197;&#36866;&#24212;&#24615;&#30340;&#26041;&#24335;&#25366;&#25496;&#22256;&#38590;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#21516;&#26102;&#20002;&#24323;&#23481;&#26131;&#30340;&#26679;&#26412;&#12290;&#20855;&#20307;&#22320;&#65292;HAM&#26681;&#25454;&#35745;&#31639;&#25439;&#22833;&#20540;&#26102;&#38656;&#35201;&#31359;&#36807;&#20915;&#31574;&#36793;&#30028;&#30340;&#27493;&#38271;&#26469;&#35782;&#21035;&#22256;&#38590;&#30340;AE&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#26089;&#26399;&#20002;&#24323;&#26426;&#21046;&#26469;&#22312;AE&#29983;&#25104;&#30340;&#21021;&#26399;&#20002;&#24323;&#23481;&#26131;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#20351;&#24471;&#32593;&#32476;&#26356;&#21152;&#40065;&#26834;&#21644;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training (AT) is widely considered the state-of-the-art technique for improving the robustness of deep neural networks (DNNs) against adversarial examples (AE). Nevertheless, recent studies have revealed that adversarially trained models are prone to unfairness problems, restricting their applicability. In this paper, we empirically observe that this limitation may be attributed to serious adversarial confidence overfitting, i.e., certain adversarial examples with overconfidence. To alleviate this problem, we propose HAM, a straightforward yet effective framework via adaptive Hard Adversarial example Mining.HAM concentrates on mining hard adversarial examples while discarding the easy ones in an adaptive fashion. Specifically, HAM identifies hard AEs in terms of their step sizes needed to cross the decision boundary when calculating loss value. Besides, an early-dropping mechanism is incorporated to discard the easy examples at the initial stages of AE generation, resulting
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20840;&#23616;&#32441;&#29702;&#19982;&#23616;&#37096;&#34917;&#19969;&#20449;&#24687;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#21644;&#35745;&#31639;&#22270;&#20687;&#32423;&#32441;&#29702;&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01813</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#32441;&#29702;&#30456;&#32467;&#21512;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks Fused with Textures for Image Classification. (arXiv:2308.01813v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20840;&#23616;&#32441;&#29702;&#19982;&#23616;&#37096;&#34917;&#19969;&#20449;&#24687;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#21644;&#35745;&#31639;&#22270;&#20687;&#32423;&#32441;&#29702;&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20122;&#31867;&#21035;&#20043;&#38388;&#35270;&#35273;&#24046;&#24322;&#36739;&#23567;&#65292;&#20294;&#26159;&#31867;&#20869;&#21464;&#21270;&#36739;&#22823;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20840;&#23616;&#32441;&#29702;&#19982;&#22522;&#20110;&#23616;&#37096;&#34917;&#19969;&#20449;&#24687;&#30340;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#31532;&#19968;&#20010;&#27493;&#39588;&#20174;&#21508;&#31181;&#22266;&#23450;&#22823;&#23567;&#30340;&#38750;&#37325;&#21472;&#34917;&#19969;&#20013;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#36827;&#34892;&#29305;&#24449;&#32534;&#30721;&#12290;&#21478;&#19968;&#20010;&#27493;&#39588;&#20351;&#29992;&#23616;&#37096;&#20108;&#20540;&#27169;&#24335;&#65288;LBP&#65289;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#35745;&#31639;&#22270;&#20687;&#32423;&#32441;&#29702;&#12290;&#20004;&#20010;&#27969;&#30340;&#20248;&#28857;&#34987;&#25972;&#21512;&#21040;&#34920;&#31034;&#39640;&#25928;&#29305;&#24449;&#21521;&#37327;&#30340;&#26041;&#27861;&#20013;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#20195;&#34920;&#20154;&#33080;&#12289;&#30382;&#32932;&#30149;&#21464;&#12289;&#39135;&#29289;&#30424;&#23376;&#12289;&#28023;&#27915;&#29983;&#29289;&#31561;&#30340;&#20843;&#20010;&#25968;&#25454;&#38598;&#19978;&#32463;&#36807;&#22235;&#20010;&#26631;&#20934;&#39592;&#24178;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27979;&#35797;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained image classification (FGIC) is a challenging task in computer vision for due to small visual differences among inter-subcategories, but, large intra-class variations. Deep learning methods have achieved remarkable success in solving FGIC. In this paper, we propose a fusion approach to address FGIC by combining global texture with local patch-based information. The first pipeline extracts deep features from various fixed-size non-overlapping patches and encodes features by sequential modelling using the long short-term memory (LSTM). Another path computes image-level textures at multiple scales using the local binary patterns (LBP). The advantages of both streams are integrated to represent an efficient feature vector for image classification. The method is tested on eight datasets representing the human faces, skin lesions, food dishes, marine lives, etc. using four standard backbone CNNs. Our method has attained better classification accuracy over existing methods with no
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#21644;&#22823;&#37327;&#29992;&#25143;&#28857;&#20987;&#26085;&#24535;&#65292;&#23398;&#20064;&#26356;&#24191;&#20041;&#21644;&#26377;&#25928;&#30340;&#29305;&#24449;&#21644;&#23454;&#20363;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01737</link><description>&lt;p&gt;
MAP: &#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction. (arXiv:2308.01737v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01737
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#21644;&#22823;&#37327;&#29992;&#25143;&#28857;&#20987;&#26085;&#24535;&#65292;&#23398;&#20064;&#26356;&#24191;&#20041;&#21644;&#26377;&#25928;&#30340;&#29305;&#24449;&#21644;&#23454;&#20363;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20010;&#24615;&#21270;&#22312;&#32447;&#26381;&#21153;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#21644;&#30740;&#31350;&#12290;CTR&#39044;&#27979;&#30340;&#26368;&#31361;&#20986;&#29305;&#28857;&#26159;&#20854;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#26684;&#24335;&#21644;&#24222;&#22823;&#32780;&#26085;&#30410;&#22686;&#38271;&#30340;&#25968;&#25454;&#37327;&#12290;&#31070;&#32463;&#27169;&#22411;&#30340;&#22823;&#23481;&#37327;&#26377;&#21161;&#20110;&#22312;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#19979;&#28040;&#21270;&#22914;&#27492;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#20294;&#26159;&#23427;&#20204;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;1&#27604;&#29305;&#30340;&#28857;&#20987;&#20449;&#21495;&#19981;&#36275;&#20197;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#21151;&#33021;&#24378;&#22823;&#30340;&#29305;&#24449;&#21644;&#23454;&#20363;&#34920;&#31034;&#12290;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#25552;&#20379;&#20102;&#26356;&#26377;&#21069;&#26223;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22823;&#37327;&#29992;&#25143;&#28857;&#20987;&#26085;&#24535;&#24182;&#23398;&#20064;&#26356;&#24191;&#20041;&#21644;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;CTR&#39044;&#27979;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#24403;&#21069;&#22312;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#20165;&#20165;&#26159;&#21021;&#27493;&#21644;&#22522;&#30784;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread application of personalized online services, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume. The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since the 1-bit click signal is not sufficient to guide the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs, and learn more generalized and effective representations. However, self-supervised learning for CTR prediction is still an open question, since current works on this line are only preliminary and rudimentary. To this end, we propose a Model-agnostic pretrain
&lt;/p&gt;</description></item><item><title>&#36817;&#21313;&#24180;&#26469;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#33258;&#32452;&#32455;&#20010;&#20154;&#30693;&#35782;&#21161;&#25163;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#20844;&#21496;&#35760;&#24518;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#21644;&#32467;&#26524;&#24635;&#32467;&#20102;&#30693;&#35782;&#22270;&#26500;&#24314;&#12289;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#21644;&#30693;&#35782;&#24037;&#20316;&#25903;&#25345;&#31561;&#39046;&#22495;&#30340;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#24037;&#19994;&#20351;&#29992;&#26696;&#20363;&#12290;&#36825;&#20123;&#36129;&#29486;&#26159;&#26032;&#21457;&#23637;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2308.01732</link><description>&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#25105;&#20204;&#37096;&#38376;&#20851;&#20110;&#33258;&#32452;&#32455;&#20010;&#20154;&#30693;&#35782;&#21161;&#25163;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#20844;&#21496;&#35760;&#24518;&#20013;&#30340;&#30740;&#31350;&#30340;&#22238;&#39038;&#24615;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Self-organizing Personal Knowledge Assistants in Evolving Corporate Memories. (arXiv:2308.01732v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01732
&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#33258;&#32452;&#32455;&#20010;&#20154;&#30693;&#35782;&#21161;&#25163;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#20844;&#21496;&#35760;&#24518;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#21644;&#32467;&#26524;&#24635;&#32467;&#20102;&#30693;&#35782;&#22270;&#26500;&#24314;&#12289;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#21644;&#30693;&#35782;&#24037;&#20316;&#25903;&#25345;&#31561;&#39046;&#22495;&#30340;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#24037;&#19994;&#20351;&#29992;&#26696;&#20363;&#12290;&#36825;&#20123;&#36129;&#29486;&#26159;&#26032;&#21457;&#23637;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#24615;&#22320;&#27010;&#36848;&#20102;&#25105;&#20204;&#37096;&#38376;&#36817;&#21313;&#24180;&#26469;&#20851;&#20110;&#33258;&#32452;&#32455;&#20010;&#20154;&#30693;&#35782;&#21161;&#25163;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#20844;&#21496;&#35760;&#24518;&#20013;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#24120;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#24182;&#24120;&#24120;&#19982;&#30740;&#31350;&#21644;&#24037;&#19994;&#21512;&#20316;&#20249;&#20276;&#36827;&#34892;&#36328;&#23398;&#31185;&#21512;&#20316;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#36807;&#21435;&#30340;&#23454;&#39564;&#21644;&#32467;&#26524;&#65292;&#21253;&#25324;&#20225;&#19994;&#21644;&#20010;&#20154;&#29615;&#22659;&#20013;&#21508;&#31181;&#30693;&#35782;&#22270;&#26500;&#24314;&#30340;&#26041;&#24335;&#65292;Managed Forgetting&#21644;(Self-organizing) Context Spaces&#20316;&#20026;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;(PIM)&#21644;&#30693;&#35782;&#24037;&#20316;&#25903;&#25345;&#30340;&#26032;&#26041;&#27861;&#12290;&#36807;&#21435;&#30340;&#32467;&#26524;&#32467;&#21512;&#20102;&#30456;&#20851;&#24037;&#20316;&#30340;&#27010;&#36848;&#21644;&#25105;&#20204;&#36804;&#20170;&#26410;&#21457;&#34920;&#30340;&#19968;&#20123;&#26368;&#26032;&#21457;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25105;&#20204;&#30456;&#20851;&#30340;&#24037;&#19994;&#20351;&#29992;&#26696;&#20363;&#65292;&#21253;&#25324;&#23545;CoMem&#30340;&#35814;&#32454;&#20171;&#32461;&#65292;&#23427;&#26159;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#30740;&#31350;&#30340;&#20844;&#21496;&#35760;&#24518;&#65292;&#24050;&#32463;&#25237;&#20837;&#20351;&#29992;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25361;&#25112;&#12290;&#35768;&#22810;&#36129;&#29486;&#20165;&#20165;&#26159;&#26032;&#21457;&#23637;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a retrospective overview of a decade of research in our department towards self-organizing personal knowledge assistants in evolving corporate memories. Our research is typically inspired by real-world problems and often conducted in interdisciplinary collaborations with research and industry partners. We summarize past experiments and results comprising topics like various ways of knowledge graph construction in corporate and personal settings, Managed Forgetting and (Self-organizing) Context Spaces as a novel approach to Personal Information Management (PIM) and knowledge work support. Past results are complemented by an overview of related work and some of our latest findings not published so far. Last, we give an overview of our related industry use cases including a detailed look into CoMem, a Corporate Memory based on our presented research already in productive use and providing challenges for further research. Many contributions are only first steps in new d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26412;&#22320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#19982;&#26412;&#22320;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#21307;&#23398;&#20219;&#21153;&#12290;&#20316;&#32773;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#30142;&#30149;&#20195;&#30721;&#26469;&#23637;&#31034;&#20854;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#21033;&#29992;LLaMA&#27169;&#22411;&#30456;&#36739;&#20110;BERT&#39118;&#26684;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#32780;&#19988;&#65292;LLaMA&#27169;&#22411;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#29305;&#21035;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.01727</link><description>&lt;p&gt;
&#38024;&#23545;&#22797;&#26434;&#32467;&#26500;&#21270;&#21307;&#23398;&#20219;&#21153;&#30340;&#26412;&#22320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Local Large Language Models for Complex Structured Medical Tasks. (arXiv:2308.01727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26412;&#22320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#19982;&#26412;&#22320;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#21307;&#23398;&#20219;&#21153;&#12290;&#20316;&#32773;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#30142;&#30149;&#20195;&#30721;&#26469;&#23637;&#31034;&#20854;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#21033;&#29992;LLaMA&#27169;&#22411;&#30456;&#36739;&#20110;BERT&#39118;&#26684;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#32780;&#19988;&#65292;LLaMA&#27169;&#22411;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#29305;&#21035;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#19982;&#26412;&#22320;&#35757;&#32451;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20316;&#32773;&#36890;&#36807;&#20174;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#30142;&#30149;&#20195;&#30721;&#26469;&#23637;&#31034;&#20182;&#20204;&#30340;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#26412;&#22320;LLMs&#65292;&#21487;&#20197;&#36827;&#34892;&#38024;&#23545;&#29305;&#23450;&#29983;&#25104;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#24182;&#25552;&#20379;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#20316;&#32773;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;15&#19975;&#20010;&#26410;&#32534;&#36753;&#30340;&#22806;&#31185;&#30149;&#29702;&#25253;&#21578;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22806;&#35266;&#25551;&#36848;&#12289;&#26368;&#32456;&#35786;&#26029;&#21644;&#30142;&#30149;&#20195;&#30721;&#12290;&#20182;&#20204;&#35757;&#32451;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#21253;&#25324;LLaMA&#12289;BERT&#21644;LongFormer&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLaMA&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#26126;&#26174;&#20248;&#20110;BERT&#39118;&#26684;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#31934;&#30830;&#24615;&#22823;&#24133;&#38477;&#20302;&#12290;LLaMA&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#29305;&#21035;&#20986;&#33394;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22788;&#29702;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex, domain-specific tasks. Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports. The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and provide structured outputs. The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final diagnoses, and condition codes. They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance. The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, even with extremely reduced precision. The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-la
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#34588;&#34562;&#23616;&#37096;&#30456;&#20301;&#37327;&#21270;&#29305;&#24449;&#36873;&#25321;&#23545;RGB-D&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#24182;&#19982;&#20854;&#20182;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.01700</link><description>&lt;p&gt;
&#34588;&#34562;&#23616;&#37096;&#30456;&#20301;&#37327;&#21270;&#29305;&#24449;&#36873;&#25321;&#23545;RGB-D&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Bees Local Phase Quantization Feature Selection for RGB-D Facial Expressions Recognition. (arXiv:2308.01700v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#34588;&#34562;&#23616;&#37096;&#30456;&#20301;&#37327;&#21270;&#29305;&#24449;&#36873;&#25321;&#23545;RGB-D&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#24182;&#19982;&#20854;&#20182;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#21487;&#20197;&#23450;&#20041;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#29289;&#21551;&#21457;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#12290;&#34588;&#34562;&#31639;&#27861;&#22312;&#29305;&#24449;&#36873;&#25321;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23616;&#37096;&#30456;&#20301;&#37327;&#21270;&#26159;&#19968;&#31181;&#22312;&#28145;&#24230;&#22270;&#20687;&#19978;&#20855;&#26377;&#20248;&#31168;&#24615;&#33021;&#30340;&#39057;&#22495;&#29305;&#24449;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20174;&#20234;&#26391;Kinect&#38754;&#37096;&#25968;&#25454;&#24211;&#65288;IKFDB&#65289;&#20013;&#25552;&#21462;RGB&#65288;&#24425;&#33394;&#65289;&#21644;&#28145;&#24230;&#22270;&#20687;&#30340;&#23616;&#37096;&#30456;&#20301;&#37327;&#21270;&#29305;&#24449;&#65292;&#34588;&#34562;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#24212;&#29992;&#20110;&#36873;&#25321;&#26368;&#32456;&#20998;&#31867;&#20219;&#21153;&#25152;&#38656;&#30340;&#29305;&#24449;&#25968;&#37327;&#12290;IKFDB&#20351;&#29992;Kinect&#20256;&#24863;&#22120;V.2&#36827;&#34892;&#35760;&#24405;&#65292;&#21253;&#25324;&#38754;&#37096;&#21644;&#38754;&#37096;&#24494;&#34920;&#24773;&#35782;&#21035;&#30340;&#24425;&#33394;&#21644;&#28145;&#24230;&#22270;&#20687;&#12290;&#22312;&#26368;&#32456;&#39564;&#35777;&#20013;&#65292;&#20351;&#29992;&#20102;&#20116;&#31181;&#38754;&#37096;&#34920;&#24773;&#65306;&#24868;&#24594;&#65292;&#21916;&#24742;&#65292;&#24778;&#35766;&#65292;&#21388;&#24694;&#21644;&#24656;&#24807;&#12290;&#25552;&#20986;&#30340;&#34588;&#34562;&#23616;&#37096;&#30456;&#20301;&#37327;&#21270;&#26041;&#27861;&#19982;&#31890;&#23376;&#32676;&#20248;&#21270;&#12289;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;Lasso&#21644;&#20165;&#23616;&#37096;&#30456;&#20301;&#37327;&#21270;&#29305;&#24449;&#22312;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;K&#26368;&#36817;&#37051;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection could be defined as an optimization problem and solved by bio-inspired algorithms. Bees Algorithm (BA) shows decent performance in feature selection optimization tasks. On the other hand, Local Phase Quantization (LPQ) is a frequency domain feature which has excellent performance on Depth images. Here, after extracting LPQ features out of RGB (colour) and Depth images from the Iranian Kinect Face Database (IKFDB), the Bees feature selection algorithm applies to select the desired number of features for final classification tasks. IKFDB is recorded with Kinect sensor V.2 and contains colour and depth images for facial and facial micro-expressions recognition purposes. Here five facial expressions of Anger, Joy, Surprise, Disgust and Fear are used for final validation. The proposed Bees LPQ method is compared with Particle Swarm Optimization (PSO) LPQ, PCA LPQ, Lasso LPQ, and just LPQ features for classification tasks with Support Vector Machines (SVM), K-Nearest Neighb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LCPS&#65292;&#31532;&#19968;&#20010;LiDAR-&#30456;&#26426;&#20840;&#26223;&#20998;&#21106;&#32593;&#32476;&#65292;&#36890;&#36807;&#24322;&#27493;&#34917;&#20607;&#20687;&#32032;&#23545;&#40784;&#12289;&#35821;&#20041;&#24863;&#30693;&#21306;&#22495;&#23545;&#40784;&#21644;&#28857;&#21040;&#20307;&#32032;&#29305;&#24449;&#20256;&#25773;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20840;&#26223;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01686</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#19968;&#33268;&#24615;&#21644;&#35821;&#20041;&#24863;&#30693;&#23545;&#40784;&#30340;LiDAR-&#30456;&#26426;&#20840;&#26223;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment. (arXiv:2308.01686v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LCPS&#65292;&#31532;&#19968;&#20010;LiDAR-&#30456;&#26426;&#20840;&#26223;&#20998;&#21106;&#32593;&#32476;&#65292;&#36890;&#36807;&#24322;&#27493;&#34917;&#20607;&#20687;&#32032;&#23545;&#40784;&#12289;&#35821;&#20041;&#24863;&#30693;&#21306;&#22495;&#23545;&#40784;&#21644;&#28857;&#21040;&#20307;&#32032;&#29305;&#24449;&#20256;&#25773;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20840;&#26223;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#20840;&#26223;&#20998;&#21106;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24863;&#30693;&#20219;&#21153;&#65292;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22270;&#20687;&#21487;&#20197;&#25552;&#20379;&#20016;&#23500;&#30340;&#32441;&#29702;&#12289;&#39068;&#33394;&#21644;&#36776;&#21035;&#20449;&#24687;&#65292;&#21487;&#20197;&#20026;LiDAR&#25968;&#25454;&#25552;&#20379;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#30340;&#34701;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LCPS&#65292;&#31532;&#19968;&#20010;LiDAR-&#30456;&#26426;&#20840;&#26223;&#20998;&#21106;&#32593;&#32476;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;LiDAR-&#30456;&#26426;&#34701;&#21512;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;1&#65289;&#19968;&#20010;&#24322;&#27493;&#34917;&#20607;&#20687;&#32032;&#23545;&#40784;&#65288;ACPA&#65289;&#27169;&#22359;&#65292;&#26657;&#27491;&#20256;&#24863;&#22120;&#20043;&#38388;&#24322;&#27493;&#38382;&#39064;&#23548;&#33268;&#30340;&#22352;&#26631;&#38169;&#20301;&#65307;2&#65289;&#19968;&#20010;&#35821;&#20041;&#24863;&#30693;&#21306;&#22495;&#23545;&#40784;&#65288;SARA&#65289;&#27169;&#22359;&#65292;&#23558;&#19968;&#23545;&#19968;&#30340;&#28857;-&#20687;&#32032;&#26144;&#23556;&#25193;&#23637;&#21040;&#19968;&#23545;&#22810;&#30340;&#35821;&#20041;&#20851;&#31995;&#65307;3&#65289;&#19968;&#20010;&#28857;&#21040;&#20307;&#32032;&#29305;&#24449;&#20256;&#25773;&#65288;PVP&#65289;&#27169;&#22359;&#65292;&#25972;&#21512;&#20960;&#20309;&#21644;&#35821;&#20041;&#34701;&#21512;&#20449;&#24687;&#23545;&#25972;&#20010;&#28857;&#20113;&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#34701;&#21512;&#31574;&#30053;&#30456;&#23545;&#20110;&#20165;&#20351;&#29992;LiDAR&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#25552;&#21319;&#20102;&#32422;6.9%&#30340;PQ&#12290;
&lt;/p&gt;
&lt;p&gt;
3D panoptic segmentation is a challenging perception task that requires both semantic segmentation and instance segmentation. In this task, we notice that images could provide rich texture, color, and discriminative information, which can complement LiDAR data for evident performance improvement, but their fusion remains a challenging problem. To this end, we propose LCPS, the first LiDAR-Camera Panoptic Segmentation network. In our approach, we conduct LiDAR-Camera fusion in three stages: 1) an Asynchronous Compensation Pixel Alignment (ACPA) module that calibrates the coordinate misalignment caused by asynchronous problems between sensors; 2) a Semantic-Aware Region Alignment (SARA) module that extends the one-to-one point-pixel mapping to one-to-many semantic relations; 3) a Point-to-Voxel feature Propagation (PVP) module that integrates both geometric and semantic fusion information for the entire point cloud. Our fusion strategy improves about 6.9% PQ performance over the LiDAR-on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38142;&#36335;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#24213;&#23618;&#20551;&#35774;&#21644;&#25216;&#26415;&#32454;&#33410;&#23545;&#35299;&#37322;&#30340;&#36136;&#37327;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.01682</link><description>&lt;p&gt;
&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38142;&#36335;&#39044;&#27979;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Evaluating Link Prediction Explanations for Graph Neural Networks. (arXiv:2308.01682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38142;&#36335;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#24213;&#23618;&#20551;&#35774;&#21644;&#25216;&#26415;&#32454;&#33410;&#23545;&#35299;&#37322;&#30340;&#36136;&#37327;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#65288;GML&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#27604;&#22914;&#33410;&#28857;/&#22270;&#20998;&#31867;&#21644;&#38142;&#36335;&#39044;&#27979;&#12290;&#20026;GML&#27169;&#22411;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#22522;&#30784;&#30340;&#20219;&#21153;&#65292;&#20294;&#23545;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;&#30340;&#35299;&#37322;&#39564;&#35777;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#37327;&#25351;&#26631;&#26469;&#35780;&#20272;&#38142;&#36335;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#26080;&#35770;&#26159;&#21542;&#26377;&#22522;&#20934;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20808;&#36827;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#24213;&#23618;&#20551;&#35774;&#21644;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#29305;&#23450;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#27604;&#22914;&#33410;&#28857;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#36873;&#25321;&#65292;&#22914;&#20309;&#24433;&#21709;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Machine Learning (GML) has numerous applications, such as node/graph classification and link prediction, in real-world domains. Providing human-understandable explanations for GML models is a challenging yet fundamental task to foster their adoption, but validating explanations for link prediction models has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01681</link><description>&lt;p&gt;
NBIAS: &#29992;&#20110;&#25991;&#26412;&#20013;&#20559;&#35265;&#35782;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#23384;&#22312;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#20351;&#29992;&#26102;&#20135;&#29983;&#20542;&#26012;&#30340;&#35299;&#37322;&#21644;&#32467;&#26524;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20250;&#25345;&#32493;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#12289;&#27495;&#35270;&#25110;&#20854;&#20182;&#24418;&#24335;&#30340;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;&#22312;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31639;&#27861;&#26368;&#32456;&#20250;&#20570;&#20986;&#19981;&#24179;&#31561;&#24433;&#21709;&#26576;&#20010;&#32676;&#20307;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#23545;&#25968;&#25454;&#30340;&#20844;&#24179;&#21644;&#36947;&#24503;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#32780;&#24378;&#22823;&#30340;&#26694;&#26550;"NBIAS"&#65292;&#23427;&#21253;&#25324;&#25968;&#25454;&#23618;&#12289;&#35821;&#26009;&#24211;&#26500;&#24314;&#12289;&#27169;&#22411;&#24320;&#21457;&#23618;&#21644;&#35780;&#20272;&#23618;&#12290;&#25968;&#25454;&#38598;&#30001;&#20174;&#21508;&#20010;&#39046;&#22495;&#25910;&#38598;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#65292;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#38376;&#25143;&#32593;&#31449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#20010;&#21807;&#19968;&#30340;&#21629;&#21517;&#23454;&#20307;&#33021;&#22815;&#35782;&#21035;&#20986;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data ends up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework \textsc{Nbias} that consists of a data layer, corpus contruction, model development layer and an evaluation layer. The dataset is constructed by collecting diverse data from various fields, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity. In the assessment procedure, we incorporate a blend of quantitative and qualitative evaluations to gauge the effectiveness of our models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MARLIM&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#38543;&#26426;&#38656;&#27714;&#21644;&#20132;&#36135;&#26102;&#38388;&#30340;&#21333;&#23618;&#22810;&#20135;&#21697;&#20379;&#24212;&#38142;&#30340;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.01649</link><description>&lt;p&gt;
MARLIM: &#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MARLIM: Multi-Agent Reinforcement Learning for Inventory Management. (arXiv:2308.01649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MARLIM&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#38543;&#26426;&#38656;&#27714;&#21644;&#20132;&#36135;&#26102;&#38388;&#30340;&#21333;&#23618;&#22810;&#20135;&#21697;&#20379;&#24212;&#38142;&#30340;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20379;&#24212;&#38142;&#34892;&#19994;&#20013;&#65292;&#36890;&#36807;&#20248;&#21270;&#34917;&#36135;&#20915;&#31574;&#26469;&#32500;&#25345;&#20135;&#21697;&#20379;&#38656;&#24179;&#34913;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MARLIM&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#38543;&#26426;&#38656;&#27714;&#21644;&#20132;&#36135;&#26102;&#38388;&#30340;&#21333;&#23618;&#22810;&#20135;&#21697;&#20379;&#24212;&#38142;&#30340;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#36890;&#36807;&#21333;&#20010;&#25110;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#29615;&#22659;&#20013;&#24320;&#21457;&#25511;&#21046;&#22120;&#12290;&#23545;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining a balance between the supply and demand of products by optimizing replenishment decisions is one of the most important challenges in the supply chain industry. This paper presents a novel reinforcement learning framework called MARLIM, to address the inventory management problem for a single-echelon multi-products supply chain with stochastic demands and lead-times. Within this context, controllers are developed through single or multiple agents in a cooperative setting. Numerical experiments on real data demonstrate the benefits of reinforcement learning methods over traditional baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21097;&#20313;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#39118;&#38459;&#24615;&#33021;&#12290;&#36890;&#36807;&#23398;&#20064;&#34917;&#20607;&#25200;&#21160;&#30340;&#21097;&#20313;&#37096;&#20998;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#32423;&#32852;PID&#25511;&#21046;&#22120;&#30340;&#31616;&#21333;&#24615;&#21644;&#26041;&#20415;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#23545;&#39118;&#25200;&#21160;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01648</link><description>&lt;p&gt;
&#20351;&#29992;&#21097;&#20313;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#32423;&#32852;PID&#25511;&#21046;&#30340;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#39118;&#38459;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Wind Resistance Performance of Cascaded PID Controlled Quadcopters using Residual Reinforcement Learning. (arXiv:2308.01648v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21097;&#20313;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#39118;&#38459;&#24615;&#33021;&#12290;&#36890;&#36807;&#23398;&#20064;&#34917;&#20607;&#25200;&#21160;&#30340;&#21097;&#20313;&#37096;&#20998;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#32423;&#32852;PID&#25511;&#21046;&#22120;&#30340;&#31616;&#21333;&#24615;&#21644;&#26041;&#20415;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#23545;&#39118;&#25200;&#21160;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#38459;&#25511;&#21046;&#26159;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#65292;&#29992;&#20110;&#32500;&#25345;&#20854;&#20301;&#32622;&#65292;&#36991;&#20813;&#20559;&#31163;&#30446;&#26631;&#20301;&#32622;&#24182;&#38450;&#27490;&#19982;&#38556;&#30861;&#29289;&#30896;&#25758;&#12290;&#20256;&#32479;&#19978;&#65292;&#32423;&#32852;PID&#25511;&#21046;&#22120;&#34987;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#25511;&#21046;&#65292;&#22240;&#20026;&#23427;&#31616;&#21333;&#26131;&#29992;&#65292;&#21487;&#20197;&#35843;&#33410;&#20854;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#39118;&#25200;&#21160;&#34920;&#29616;&#36739;&#24369;&#65292;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#24456;&#23481;&#26131;&#20559;&#31163;&#30446;&#26631;&#20301;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21097;&#20313;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#39118;&#38459;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#20165;&#23398;&#20064;&#34917;&#20607;&#25200;&#21160;&#30340;&#21097;&#20313;&#37096;&#20998;&#65292;&#25105;&#20204;&#21487;&#20197;&#32487;&#32493;&#20351;&#29992;&#32423;&#32852;PID&#25511;&#21046;&#22120;&#20316;&#20026;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#22522;&#26412;&#25511;&#21046;&#22120;&#65292;&#21516;&#26102;&#25552;&#39640;&#20854;&#25239;&#39118;&#25200;&#21160;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36991;&#20813;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#24847;&#22806;&#22368;&#27585;&#21644;&#30772;&#22351;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23454;&#38469;&#30828;&#20214;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#12290;&#25511;&#21046;&#22120;&#20165;&#22312;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#20110;&#30446;&#26631;&#30828;&#20214;&#65292;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind resistance control is an essential feature for quadcopters to maintain their position to avoid deviation from target position and prevent collisions with obstacles. Conventionally, cascaded PID controller is used for the control of quadcopters for its simplicity and ease of tuning its parameters. However, it is weak against wind disturbances and the quadcopter can easily deviate from target position. In this work, we propose a residual reinforcement learning based approach to build a wind resistance controller of a quadcopter. By learning only the residual that compensates the disturbance, we can continue using the cascaded PID controller as the base controller of the quadcopter but improve its performance against wind disturbances. To avoid unexpected crashes and destructions of quadcopters, our method does not require real hardware for data collection and training. The controller is trained only on a simulator and directly applied to the target hardware without extra finetuning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#20114;&#20132;&#38169;&#24212;&#29992;&#20110;&#20070;&#31821;&#23553;&#38754;&#35774;&#35745;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#26631;&#39064;&#33719;&#24471;&#22810;&#20010;&#36873;&#39033;&#24182;&#21033;&#29992;&#37492;&#21035;&#22120;&#36873;&#25321;&#26368;&#20339;&#22270;&#20687;&#65292;&#34920;&#29616;&#20986;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#21644;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.01626</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#20114;&#20132;&#38169;&#24212;&#29992;&#20110;&#20070;&#31821;&#23553;&#38754;&#35774;&#35745;&#30340;&#21019;&#24847;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Interleaving GANs with knowledge graphs to support design creativity for book covers. (arXiv:2308.01626v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#20114;&#20132;&#38169;&#24212;&#29992;&#20110;&#20070;&#31821;&#23553;&#38754;&#35774;&#35745;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#26631;&#39064;&#33719;&#24471;&#22810;&#20010;&#36873;&#39033;&#24182;&#21033;&#29992;&#37492;&#21035;&#22120;&#36873;&#25321;&#26368;&#20339;&#22270;&#20687;&#65292;&#34920;&#29616;&#20986;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#21644;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20070;&#31821;&#23553;&#38754;&#30340;&#21560;&#24341;&#21147;&#23545;&#20110;&#19968;&#26412;&#20070;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24212;&#29992;&#20110;&#20070;&#31821;&#23553;&#38754;&#39046;&#22495;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#27861;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29983;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;GANs&#30456;&#20114;&#20132;&#38169;&#65292;&#25913;&#21464;&#36755;&#20837;&#26631;&#39064;&#20197;&#33719;&#24471;&#32473;&#23450;&#26631;&#39064;&#30340;&#22810;&#20010;&#21487;&#33021;&#36873;&#39033;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;&#29983;&#25104;&#22120;&#30340;&#22686;&#24378;&#36755;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#35757;&#32451;&#38454;&#27573;&#33719;&#24471;&#30340;&#37492;&#21035;&#22120;&#36873;&#25321;&#20351;&#29992;&#26032;&#26631;&#39064;&#29983;&#25104;&#30340;&#26368;&#20339;&#22270;&#20687;&#12290;&#30456;&#27604;&#20165;&#20351;&#29992;GANs&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#20070;&#31821;&#23553;&#38754;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#30693;&#35782;&#22270;&#35889;&#19982;&#20070;&#31821;&#20316;&#32773;&#25110;&#32534;&#36753;&#30456;&#27604;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
An attractive book cover is important for the success of a book. In this paper, we apply Generative Adversarial Networks (GANs) to the book covers domain, using different methods for training in order to obtain better generated images. We interleave GANs with knowledge graphs to alter the input title to obtain multiple possible options for any given title, which are then used as an augmented input to the generator. Finally, we use the discriminator obtained during the training phase to select the best images generated with new titles. Our method performed better at generating book covers than previous attempts, and the knowledge graph gives better options to the book author or editor compared to using GANs alone.
&lt;/p&gt;</description></item><item><title>ReIDTrack&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36816;&#21160;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#21644;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#24615;&#33021;&#30340;&#26816;&#27979;&#21644;&#22806;&#35266;&#27169;&#22411;&#65292;&#22823;&#24133;&#24230;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#22312;MOTS&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.01622</link><description>&lt;p&gt;
ReIDTrack: &#26080;&#38656;&#36816;&#21160;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ReIDTrack: Multi-Object Track and Segmentation Without Motion. (arXiv:2308.01622v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01622
&lt;/p&gt;
&lt;p&gt;
ReIDTrack&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36816;&#21160;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#21644;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#24615;&#33021;&#30340;&#26816;&#27979;&#21644;&#22806;&#35266;&#27169;&#22411;&#65292;&#22823;&#24133;&#24230;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#22312;MOTS&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20027;&#23548;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#21644;&#20998;&#21106;&#65288;MOTS&#65289;&#26041;&#27861;&#20027;&#35201;&#36981;&#24490;&#30340;&#26159;&#36319;&#36394;-&#26816;&#27979;&#33539;&#24335;&#12290;&#22522;&#20110;Transformer&#30340;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#35299;&#20915;&#26041;&#26696;&#20026;MOT&#21644;MOTS&#24102;&#26469;&#20102;&#19968;&#20123;&#24819;&#27861;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#22312;&#20027;&#35201;&#30340;MOT&#21644;MOTS&#22522;&#20934;&#20013;&#36798;&#21040;&#26032;&#30340;&#26368;&#39640;&#24615;&#33021;&#12290;&#26816;&#27979;&#21644;&#20851;&#32852;&#26159;&#36319;&#36394;-&#26816;&#27979;&#33539;&#24335;&#30340;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#12290;&#20851;&#32852;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#36816;&#21160;&#21644;&#22806;&#35266;&#20449;&#24687;&#30340;&#32452;&#21512;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#36817;&#21457;&#23637;&#65292;&#26816;&#27979;&#21644;&#22806;&#35266;&#27169;&#22411;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#24555;&#36895;&#25552;&#21319;&#12290;&#36825;&#20123;&#36235;&#21183;&#20351;&#25105;&#20204;&#32771;&#34385;&#26159;&#21542;&#21487;&#20197;&#20165;&#22522;&#20110;&#39640;&#24615;&#33021;&#30340;&#26816;&#27979;&#21644;&#22806;&#35266;&#27169;&#22411;&#23454;&#29616;&#26032;&#30340;&#26368;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;CBNetV2&#20316;&#20026;&#26816;&#27979;&#27169;&#22411;&#12289;Swin-B&#20316;&#20026;&#26816;&#27979;&#27169;&#22411;&#12289;MoCo-v2&#20316;&#20026;&#33258;&#30417;&#30563;&#22806;&#35266;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;MOTS&#22522;&#20934;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, dominant Multi-object tracking (MOT) and segmentation (MOTS) methods mainly follow the tracking-by-detection paradigm. Transformer-based end-to-end (E2E) solutions bring some ideas to MOT and MOTS, but they cannot achieve a new state-of-the-art (SOTA) performance in major MOT and MOTS benchmarks. Detection and association are two main modules of the tracking-by-detection paradigm. Association techniques mainly depend on the combination of motion and appearance information. As deep learning has been recently developed, the performance of the detection and appearance model is rapidly improved. These trends made us consider whether we can achieve SOTA based on only high-performance detection and appearance model. Our paper mainly focuses on exploring this direction based on CBNetV2 with Swin-B as a detection model and MoCo-v2 as a self-supervised appearance model. Motion information and IoU mapping were removed during the association. Our method wins 1st place on the MOTS
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#20107;&#23454;&#39564;&#35777;&#35780;&#20272;DNNs&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2308.01614</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#39564;&#35777;&#35780;&#20272;DNNs&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Assessing Systematic Weaknesses of DNNs using Counterfactuals. (arXiv:2308.01614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01614
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#39564;&#35777;&#35780;&#20272;DNNs&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;DNNs&#36827;&#20837;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#23545;&#36825;&#31181;&#27169;&#22411;&#30340;&#27979;&#35797;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#19968;&#20010;&#26041;&#21521;&#26159;&#23547;&#25214;&#21644;&#35782;&#21035;&#31995;&#32479;&#24615;&#24369;&#28857;&#65292;&#36825;&#20123;&#24369;&#28857;&#20351;&#22522;&#20110;&#24179;&#22343;&#24615;&#33021;&#20540;&#30340;&#23433;&#20840;&#20551;&#35774;&#22788;&#20110;&#21361;&#38505;&#20043;&#20013;&#12290;&#36825;&#20123;&#24369;&#28857;&#21487;&#20197;&#34920;&#29616;&#20026;&#65288;&#35821;&#20041;&#19978;&#36830;&#36143;&#30340;&#65289;&#23376;&#38598;&#25110;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#21306;&#22495;&#65292;&#22312;&#36825;&#20123;&#21306;&#22495;&#20013;&#65292;DNN&#30340;&#24615;&#33021;&#27604;&#39044;&#26399;&#30340;&#24179;&#22343;&#24615;&#33021;&#35201;&#24046;&#12290;&#28982;&#32780;&#65292;&#23558;&#35266;&#23519;&#21040;&#30340;&#20302;&#24615;&#33021;&#24402;&#22240;&#20110;&#25551;&#36848;&#35813;&#23376;&#38598;&#30340;&#29305;&#23450;&#35821;&#20041;&#29305;&#24449;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#20363;&#22914;&#65292;&#19982;&#20854;&#20182;&#65288;&#26410;&#32771;&#34385;&#65289;&#23646;&#24615;&#30456;&#20851;&#30340;&#25968;&#25454;&#19981;&#22343;&#21248;&#24615;&#21487;&#33021;&#20250;&#25197;&#26354;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#25152;&#26377;&#65288;&#21487;&#29992;&#65289;&#23646;&#24615;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#21463;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#35745;&#31639;&#25104;&#26412;&#20302;&#30340;&#31639;&#27861;&#26469;&#39564;&#35777;&#29616;&#26377;&#23376;&#38598;&#30340;&#35821;&#20041;&#24402;&#22240;&#65292;&#21363;&#26816;&#26597;...
&lt;/p&gt;
&lt;p&gt;
With the advancement of DNNs into safety-critical applications, testing approaches for such models have gained more attention. A current direction is the search for and identification of systematic weaknesses that put safety assumptions based on average performance values at risk. Such weaknesses can take on the form of (semantically coherent) subsets or areas in the input space where a DNN performs systematically worse than its expected average. However, it is non-trivial to attribute the reason for such observed low performances to the specific semantic features that describe the subset. For instance, inhomogeneities within the data w.r.t. other (non-considered) attributes might distort results. However, taking into account all (available) attributes and their interaction is often computationally highly expensive. Inspired by counterfactual explanations, we propose an effective and computationally cheap algorithm to validate the semantic attribution of existing subsets, i.e., to chec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#38382;&#39064;&#25551;&#36848;&#20013;&#25552;&#21462;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#20351;&#32422;&#26463;&#32534;&#31243;&#26356;&#26131;&#20110;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01589</link><description>&lt;p&gt;
&#22307;&#26479;2.0&#65306;&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;&#32422;&#26463;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Holy Grail 2.0: From Natural Language to Constraint Models. (arXiv:2308.01589v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#38382;&#39064;&#25551;&#36848;&#20013;&#25552;&#21462;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#20351;&#32422;&#26463;&#32534;&#31243;&#26356;&#26131;&#20110;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
27&#24180;&#21069;&#65292;E. Freuder&#24378;&#35843;&#20102;&#8220;&#32422;&#26463;&#32534;&#31243;&#20195;&#34920;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#23545;&#20110;&#32534;&#31243;&#30340;&#19968;&#20010;&#26368;&#25509;&#36817;&#22307;&#26479;&#30340;&#26041;&#27861;&#65306;&#29992;&#25143;&#38472;&#36848;&#38382;&#39064;&#65292;&#35745;&#31639;&#26426;&#35299;&#20915;&#38382;&#39064;&#8221;&#12290;&#22914;&#20170;&#65292;CP&#29992;&#25143;&#25317;&#26377;&#24378;&#22823;&#30340;&#24314;&#27169;&#24037;&#20855;&#65288;&#22914;Minizinc&#21644;CPMpy&#65289;&#65292;&#21487;&#20197;&#29992;&#23427;&#20204;&#26469;&#34920;&#36798;&#38382;&#39064;&#65292;&#28982;&#21518;&#35753;&#27714;&#35299;&#22120;&#23436;&#25104;&#21097;&#19979;&#30340;&#24037;&#20316;&#65292;&#31163;&#30446;&#26631;&#26356;&#36817;&#20102;&#12290;&#28982;&#32780;&#65292;&#36825;&#20173;&#28982;&#35201;&#27714;CP&#29992;&#25143;&#20102;&#35299;&#24418;&#24335;&#21270;&#26041;&#27861;&#24182;&#36981;&#23432;&#23427;&#12290;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#25165;&#33021;&#26377;&#25928;&#22320;&#23545;&#32452;&#21512;&#38382;&#39064;&#24314;&#27169;&#12290;&#25152;&#26377;&#36825;&#20123;&#38480;&#21046;&#20102;CP&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#38382;&#39064;&#25551;&#36848;&#20013;&#25552;&#21462;&#27169;&#22411;&#30340;&#21487;&#33021;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20248;&#21270;&#65288;NL4OPT&#65289;&#25361;&#25112;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#35299;&#30340;&#25552;&#31034;&#24212;&#29992;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Twenty-seven years ago, E. Freuder highlighted that "Constraint programming represents one of the closest approaches computer science has yet made to the Holy Grail of programming: the user states the problem, the computer solves it". Nowadays, CP users have great modeling tools available (like Minizinc and CPMpy), allowing them to formulate the problem and then let a solver do the rest of the job, getting closer to the stated goal. However, this still requires the CP user to know the formalism and respect it. Another significant challenge lies in the expertise required to effectively model combinatorial problems. All this limits the wider adoption of CP. In this position paper, we investigate a possible approach to leverage pre-trained Large Language Models to extract models from textual problem descriptions. More specifically, we take inspiration from the Natural Language Processing for Optimization (NL4OPT) challenge and present early results with a decomposition-based prompting app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ULTS&#24211;&#30340;&#24555;&#36895;&#23454;&#29616;&#19982;&#32479;&#19968;&#35780;&#20272;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2308.01578</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Representation Learning for Time Series: A Review. (arXiv:2308.01578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ULTS&#24211;&#30340;&#24555;&#36895;&#23454;&#29616;&#19982;&#32479;&#19968;&#35780;&#20272;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#23545;&#27599;&#20010;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#12290;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#35828;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#38750;&#24120;&#20851;&#38190;&#65292;&#22240;&#20026;&#20854;&#22797;&#26434;&#29305;&#24615;&#21644;&#32570;&#20047;&#19982;&#20854;&#20182;&#25968;&#25454;&#24418;&#24577;&#30456;&#27604;&#30340;&#35270;&#35273;&#25552;&#31034;&#23548;&#33268;&#20102;&#26631;&#27880;&#22256;&#22659;&#12290;&#36817;&#24180;&#26469;&#65292;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#20998;&#26512;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;&#29616;&#26377;&#24555;&#36895;&#21457;&#23637;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#25991;&#29486;&#32508;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#21644;&#26631;&#20934;&#21270;&#30340;&#24211;&#65292;&#21517;&#20026;ULTS&#65288;&#21363;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#65289;&#65292;&#20197;&#20415;&#23545;&#21508;&#31181;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#23454;&#29616;&#21644;&#32479;&#19968;&#35780;&#20272;&#12290;&#36890;&#36807;ULTS&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;&#20102;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning approaches aim to learn discriminative feature representations from unlabeled data, without the requirement of annotating every sample. Enabling unsupervised representation learning is extremely crucial for time series data, due to its unique annotation bottleneck caused by its complex characteristics and lack of visual cues compared with other data modalities. In recent years, unsupervised representation learning techniques have advanced rapidly in various domains. However, there is a lack of systematic analysis of unsupervised representation learning approaches for time series. To fill the gap, we conduct a comprehensive literature review of existing rapidly evolving unsupervised representation learning approaches for time series. Moreover, we also develop a unified and standardized library, named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast implementations and unified evaluations on various models. With ULTS, we empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#21152;&#36895;&#36816;&#21160;&#35268;&#21010;&#20248;&#21270;&#36807;&#31243;&#12290;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#24182;&#21487;&#20197;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2308.01557</link><description>&lt;p&gt;
&#36816;&#21160;&#35268;&#21010;&#25193;&#25955;&#65306;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. (arXiv:2308.01557v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#21152;&#36895;&#36816;&#21160;&#35268;&#21010;&#20248;&#21270;&#36807;&#31243;&#12290;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#24182;&#21487;&#20197;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36712;&#36857;&#20998;&#24067;&#30340;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#21152;&#24555;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#20248;&#21270;&#12290;&#22312;&#32473;&#23450;&#20808;&#21069;&#25104;&#21151;&#30340;&#35268;&#21010;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#36712;&#36857;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#26032;&#35268;&#21010;&#38382;&#39064;&#30340;&#20808;&#39564;&#30693;&#35782;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#36825;&#31181;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#24341;&#23548;&#30340;&#26041;&#27861;&#12290;&#21487;&#20197;&#36890;&#36807;&#20174;&#20808;&#39564;&#30693;&#35782;&#20013;&#37319;&#26679;&#21021;&#22987;&#21270;&#65292;&#25110;&#32773;&#22312;&#26368;&#22823;&#21518;&#39564;&#20248;&#21270;&#30340;&#36807;&#31243;&#20013;&#20351;&#29992;&#20808;&#39564;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#21435;&#22122;&#36807;&#31243;&#65292;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#36825;&#23545;&#20110;&#22823;&#37327;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#38750;&#24120;&#36866;&#29992;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;-&#36816;&#21160;&#35268;&#21010;&#25193;&#25955;&#19982;&#20960;&#31181;&#22522;&#20934;&#26041;&#21457;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multimodality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#36712;&#36947;&#20132;&#36890;&#20840;&#29699;&#36816;&#36755;&#33021;&#21147;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#24182;&#21033;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23545;&#36712;&#36947;&#20132;&#36890;&#32593;&#32476;&#30340;&#36816;&#36755;&#33021;&#21147;&#39118;&#38505;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.01556</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#36712;&#36947;&#20132;&#36890;&#20840;&#29699;&#36816;&#36755;&#33021;&#21147;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Global Transport Capacity Risk Prediction Method for Rail Transit Based on Gaussian Bayesian Network. (arXiv:2308.01556v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#36712;&#36947;&#20132;&#36890;&#20840;&#29699;&#36816;&#36755;&#33021;&#21147;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#24182;&#21033;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23545;&#36712;&#36947;&#20132;&#36890;&#32593;&#32476;&#30340;&#36816;&#36755;&#33021;&#21147;&#39118;&#38505;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36712;&#36947;&#20132;&#36890;&#32593;&#32476;&#25215;&#36733;&#33021;&#21147;&#19982;&#20056;&#23458;&#27969;&#37327;&#38656;&#27714;&#20043;&#38388;&#19981;&#21305;&#37197;&#25152;&#24341;&#36215;&#30340;&#36816;&#36755;&#33021;&#21147;&#39118;&#38505;&#39044;&#27979;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#36712;&#36947;&#20132;&#36890;&#32593;&#32476;&#36816;&#36755;&#33021;&#21147;&#39118;&#38505;&#21487;&#35299;&#37322;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21253;&#25324;&#36712;&#36947;&#20132;&#36890;&#32593;&#32476;&#12289;&#21015;&#36710;&#27969;&#21644;&#20056;&#23458;&#27969;&#30340;&#19977;&#23618;&#32467;&#26500;&#30340;&#36712;&#36947;&#20132;&#36890;&#31995;&#32479;&#20223;&#30495;&#27169;&#22411;&#33719;&#21462;&#39044;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36947;&#20132;&#36890;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;MLE&#65288;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65289;&#26041;&#27861;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#21442;&#25968;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20223;&#30495;&#23454;&#20363;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming at the prediction problem of transport capacity risk caused by the mismatch between the carrying capacity of rail transit network and passenger flow demand, this paper proposes an explainable prediction method of rail transit network transport capacity risk based on linear Gaussian Bayesian network. This method obtains the training data of the prediction model based on the simulation model of the rail transit system with a three-layer structure including rail transit network, train flow and passenger flow. A Bayesian network structure construction method based on the topology of the rail transit network is proposed, and the MLE (Maximum Likelihood Estimation) method is used to realize the parameter learning of the Bayesian network. Finally, the effectiveness of the proposed method is verified by simulation examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;OpenAI&#30340;ChatGPT&#38598;&#25104;&#21040;&#20855;&#36523;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23558;ChatGPT&#20998;&#37197;&#19981;&#21516;&#35282;&#33394;&#24182;&#19982;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;98%&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20102;ChatGPT&#22312;&#29702;&#35299;&#21644;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01552</link><description>&lt;p&gt;
InterAct: &#25506;&#32034;&#23558;ChatGPT&#20316;&#20026;&#21512;&#20316;&#20195;&#29702;&#20154;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent. (arXiv:2308.01552v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;OpenAI&#30340;ChatGPT&#38598;&#25104;&#21040;&#20855;&#36523;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23558;ChatGPT&#20998;&#37197;&#19981;&#21516;&#35282;&#33394;&#24182;&#19982;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;98%&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20102;ChatGPT&#22312;&#29702;&#35299;&#21644;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;OpenAI&#30340;ChatGPT&#38598;&#25104;&#21040;&#20855;&#36523;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#35780;&#20272;&#20854;&#23545;&#20132;&#20114;&#20915;&#31574;&#22522;&#20934;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;InterAct&#36825;&#19968;&#27010;&#24565;&#65292;&#23558;&#20854;&#31867;&#27604;&#20110;&#20154;&#20204;&#26681;&#25454;&#33258;&#24049;&#29420;&#29305;&#30340;&#20248;&#21183;&#25198;&#28436;&#35282;&#33394;&#30340;&#27010;&#24565;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#32473;ChatGPT&#25552;&#20379;&#21508;&#31181;&#25552;&#31034;&#65292;&#23558;&#20854;&#20998;&#37197;&#20026;&#20687;&#26816;&#26597;&#21592;&#21644;&#20998;&#31867;&#21592;&#36825;&#26679;&#30340;&#22810;&#31181;&#35282;&#33394;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#19982;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;AlfWorld&#20013;&#23637;&#31034;&#20102;98%&#30340;&#26174;&#33879;&#25104;&#21151;&#29575;&#65292;AlfWorld&#26159;&#19968;&#20010;&#27169;&#25311;&#23478;&#24237;&#29615;&#22659;&#20013;&#21253;&#21547;6&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#33391;&#22909;&#30340;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;ChatGPT&#22312;&#29702;&#35299;&#21644;&#39640;&#25928;&#22320;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#20219;&#21153;&#35268;&#21010;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper delves into the integration of OpenAI's ChatGPT into embodied agent systems, evaluating its influence on interactive decision-making benchmark. Drawing a parallel to the concept of people assuming roles according to their unique strengths, we introduce InterAct. In this approach, we feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model. Our research shows a remarkable success rate of 98% in AlfWorld, which consists of 6 different tasks in a simulated household environment, emphasizing the significance of proficient prompt engineering. The results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#30340;&#36991;&#38556;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#31163;&#32447;&#35757;&#32451;&#31574;&#30053;&#21644;&#25910;&#38598;&#19987;&#23478;&#32463;&#39564;&#30340;&#36890;&#29992;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#22870;&#21169;&#12290;&#36890;&#36807;&#20808;&#36827;&#30340;&#20223;&#30495;&#21644;&#30495;&#23454;&#29289;&#29702;&#24314;&#27169;&#65292;&#32553;&#23567;&#20102;&#20223;&#30495;&#19982;&#30495;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#65292;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#37117;&#33021;&#21462;&#24471;&#30456;&#21516;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01551</link><description>&lt;p&gt;
&#22522;&#20110;&#31163;&#32447;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#30340;&#36991;&#38556;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Avoidance Navigation Based on Offline Pre-Training Reinforcement Learning. (arXiv:2308.01551v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#30340;&#36991;&#38556;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#31163;&#32447;&#35757;&#32451;&#31574;&#30053;&#21644;&#25910;&#38598;&#19987;&#23478;&#32463;&#39564;&#30340;&#36890;&#29992;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#22870;&#21169;&#12290;&#36890;&#36807;&#20808;&#36827;&#30340;&#20223;&#30495;&#21644;&#30495;&#23454;&#29289;&#29702;&#24314;&#27169;&#65292;&#32553;&#23567;&#20102;&#20223;&#30495;&#19982;&#30495;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#65292;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#37117;&#33021;&#21462;&#24471;&#30456;&#21516;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#36991;&#38556;&#23548;&#33322;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#26144;&#23556;&#21040;&#25511;&#21046;&#21464;&#37327;&#65292;&#24182;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31163;&#32447;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21152;&#36895;&#26089;&#26399;&#38454;&#27573;&#30340;&#38543;&#26426;&#25506;&#32034;&#65292;&#24182;&#25910;&#38598;&#20102;&#29992;&#20110;&#31163;&#32447;&#35757;&#32451;&#30340;&#21253;&#21547;&#19987;&#23478;&#32463;&#39564;&#30340;&#36890;&#29992;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#20110;&#20854;&#20182;&#23548;&#33322;&#35757;&#32451;&#24037;&#20316;&#20855;&#26377;&#19968;&#23450;&#30340;&#24847;&#20041;&#12290;&#39044;&#35757;&#32451;&#21644;&#20248;&#20808;&#30340;&#19987;&#23478;&#32463;&#39564;&#34987;&#25552;&#20986;&#26469;&#20943;&#23569;80&#65285;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#24050;&#32463;&#39564;&#35777;&#21487;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;2&#20493;&#12290;&#36890;&#36807;&#20808;&#36827;&#30340;&#20223;&#30495;Gazebo&#21644;&#30495;&#23454;&#29289;&#29702;&#24314;&#27169;&#20197;&#21450;&#21160;&#24577;&#26041;&#31243;&#65292;&#32553;&#23567;&#20102;&#20223;&#30495;&#19982;&#30495;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#36208;&#24266;&#29615;&#22659;&#20013;&#35757;&#32451;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#25928;&#26524;&#12290;&#19982;&#20256;&#32479;&#30340;&#23548;&#33322;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#35748;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#19981;&#21516;&#24773;&#22659;&#65292;&#24182;&#21462;&#24471;&#30456;&#21516;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a Pre-Training Deep Reinforcement Learning(DRL) for avoidance navigation without map for mobile robots which map raw sensor data to control variable and navigate in an unknown environment. The efficient offline training strategy is proposed to speed up the inefficient random explorations in early stage and we also collect a universal dataset including expert experience for offline training, which is of some significance for other navigation training work. The pre-training and prioritized expert experience are proposed to reduce 80\% training time and has been verified to improve the 2 times reward of DRL. The advanced simulation gazebo with real physical modelling and dynamic equations reduce the gap between sim-to-real. We train our model a corridor environment, and evaluate the model in different environment getting the same effect. Compared to traditional method navigation, we can confirm the trained model can be directly applied into different scenarios and have
&lt;/p&gt;</description></item><item><title>MusicLDM&#36890;&#36807;&#31283;&#23450;&#25193;&#25955;&#21644;AudioLDM&#26550;&#26500;&#65292;&#32467;&#21512;&#37325;&#26032;&#35757;&#32451;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;Hifi-GAN&#22768;&#30721;&#22120;&#65292;&#20197;&#35299;&#20915;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33410;&#22863;&#21516;&#27493;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#22686;&#24191;&#65292;&#25552;&#39640;&#26032;&#39062;&#24615;&#24182;&#36991;&#20813;&#25220;&#34989;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01546</link><description>&lt;p&gt;
MusicLDM&#65306;&#20351;&#29992;&#33410;&#22863;&#21516;&#27493;&#30340;&#28151;&#21512;&#31574;&#30053;&#22686;&#24378;&#25991;&#26412;&#36716;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#26032;&#39062;&#24615;
&lt;/p&gt;
&lt;p&gt;
MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies. (arXiv:2308.01546v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01546
&lt;/p&gt;
&lt;p&gt;
MusicLDM&#36890;&#36807;&#31283;&#23450;&#25193;&#25955;&#21644;AudioLDM&#26550;&#26500;&#65292;&#32467;&#21512;&#37325;&#26032;&#35757;&#32451;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;Hifi-GAN&#22768;&#30721;&#22120;&#65292;&#20197;&#35299;&#20915;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33410;&#22863;&#21516;&#27493;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#22686;&#24191;&#65292;&#25552;&#39640;&#26032;&#39062;&#24615;&#24182;&#36991;&#20813;&#25220;&#34989;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#36328;&#27169;&#24577;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38899;&#20048;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#20197;&#21450;&#19982;&#29256;&#26435;&#21644;&#25220;&#34989;&#30456;&#20851;&#30340;&#25935;&#24863;&#38382;&#39064;&#65292;&#29983;&#25104;&#38899;&#20048;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#38899;&#39057;&#65292;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#27169;&#22411;MusicLDM&#65292;&#23558;&#31283;&#23450;&#25193;&#25955;&#21644;AudioLDM&#26550;&#26500;&#36866;&#24212;&#21040;&#38899;&#20048;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;&#38899;&#20048;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;(CLAP)&#21644;Hifi-GAN&#22768;&#30721;&#22120;&#36825;&#20123;MusicLDM&#30340;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#24182;&#36991;&#20813;&#25220;&#34989;&#65292;&#25105;&#20204;&#21033;&#29992;&#33410;&#25293;&#36861;&#36394;&#27169;&#22411;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24191;&#28151;&#21512;&#31574;&#30053;&#65306;&#33410;&#22863;&#21516;&#27493;&#38899;&#39057;&#28151;&#21512;&#21644;&#33410;&#22863;&#21516;&#27493;&#28508;&#22312;&#28151;&#21512;&#65292;&#20998;&#21035;&#36890;&#36807;&#30452;&#25509;&#37325;&#26032;&#32452;&#21512;&#35757;&#32451;&#38899;&#39057;&#25110;&#36890;&#36807;&#28508;&#22312;&#23884;&#20837;&#31354;&#38388;&#37325;&#26032;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;AI&#22686;&#24378;&#30340;&#19978;&#37319;&#26679;&#20316;&#20026;&#35774;&#35745;&#36741;&#21161;&#24037;&#20855;&#65292;&#22312;2D&#28216;&#25103;&#20851;&#21345;&#35774;&#35745;&#20013;&#23454;&#29616;&#21327;&#21516;&#21019;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23558;&#20154;&#24037;&#38477;&#20302;&#20998;&#36776;&#29575;&#30340;&#20851;&#21345;&#29255;&#27573;&#19978;&#37319;&#26679;&#65292;&#24182;&#20026;&#27492;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#19978;&#37319;&#26679;&#24182;&#22788;&#29702;&#19981;&#22826;&#24120;&#35265;&#22270;&#22359;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#32463;&#36807;&#19982;&#35774;&#35745;&#24072;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#35774;&#35745;&#24072;&#20204;&#21916;&#27426;&#36825;&#20010;&#24037;&#20855;&#30340;&#20849;&#21516;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#35748;&#20026;&#23427;&#20855;&#26377;&#28508;&#21147;&#25512;&#21160;&#26356;&#22810;&#30340;&#24320;&#21457;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.01543</link><description>&lt;p&gt;
Lode Enhancer: &#36890;&#36807;&#25193;&#23637;&#20419;&#36827;&#20851;&#21345;&#30340;&#21327;&#21516;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Lode Enhancer: Level Co-creation Through Scaling. (arXiv:2308.01543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;AI&#22686;&#24378;&#30340;&#19978;&#37319;&#26679;&#20316;&#20026;&#35774;&#35745;&#36741;&#21161;&#24037;&#20855;&#65292;&#22312;2D&#28216;&#25103;&#20851;&#21345;&#35774;&#35745;&#20013;&#23454;&#29616;&#21327;&#21516;&#21019;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23558;&#20154;&#24037;&#38477;&#20302;&#20998;&#36776;&#29575;&#30340;&#20851;&#21345;&#29255;&#27573;&#19978;&#37319;&#26679;&#65292;&#24182;&#20026;&#27492;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#19978;&#37319;&#26679;&#24182;&#22788;&#29702;&#19981;&#22826;&#24120;&#35265;&#22270;&#22359;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#32463;&#36807;&#19982;&#35774;&#35745;&#24072;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#35774;&#35745;&#24072;&#20204;&#21916;&#27426;&#36825;&#20010;&#24037;&#20855;&#30340;&#20849;&#21516;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#35748;&#20026;&#23427;&#20855;&#26377;&#28508;&#21147;&#25512;&#21160;&#26356;&#22810;&#30340;&#24320;&#21457;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#21019;&#24314;2D&#28216;&#25103;&#20851;&#21345;&#26102;&#65292;&#20351;&#29992;AI&#22686;&#24378;&#30340;&#19978;&#37319;&#26679;&#20316;&#20026;&#35774;&#35745;&#36741;&#21161;&#24037;&#20855;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#26469;&#33258;&#35868;&#39064;&#24179;&#21488;&#28216;&#25103;Lode Runner&#30340;&#20154;&#24037;&#38477;&#20302;&#20998;&#36776;&#29575;&#30340;&#20851;&#21345;&#29255;&#27573;&#36827;&#34892;&#19978;&#37319;&#26679;&#12290;&#35757;&#32451;&#24471;&#21040;&#30340;&#32593;&#32476;&#34987;&#25972;&#21512;&#21040;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#32534;&#36753;&#22120;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#20197;3&#20010;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#65288;4x4&#12289;8x8&#21644;16x16&#65289;&#21019;&#24314;&#21644;&#32534;&#36753;&#20851;&#21345;&#12290;&#22312;&#20219;&#20309;&#20998;&#36776;&#29575;&#19978;&#30340;&#32534;&#36753;&#37117;&#20250;&#31435;&#21363;&#20256;&#36755;&#21040;&#20854;&#20182;&#20998;&#36776;&#29575;&#19978;&#12290;&#30001;&#20110;&#19978;&#37319;&#26679;&#38656;&#35201;&#21457;&#26126;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#19979;&#21487;&#33021;&#19981;&#23384;&#22312;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#22797;&#21046;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#19981;&#20165;&#33021;&#22815;&#23398;&#20064;&#19978;&#37319;&#26679;&#65292;&#36824;&#33021;&#22815;&#26356;&#39640;&#20248;&#20808;&#32423;&#22320;&#22788;&#29702;&#19981;&#22826;&#24120;&#35265;&#30340;&#22270;&#22359;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#24037;&#20855;&#30340;&#28508;&#21147;&#24182;&#25351;&#23548;&#36827;&#19968;&#27493;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23450;&#24615;&#30740;&#31350;&#65292;&#19982;3&#20301;&#35774;&#35745;&#24072;&#21327;&#20316;&#65292;&#20102;&#35299;&#20182;&#20204;&#22914;&#20309;&#20351;&#29992;&#23427;&#12290;&#35774;&#35745;&#24072;&#20139;&#21463;&#19982;&#36825;&#20010;&#24037;&#20855;&#30340;&#20849;&#21516;&#35774;&#35745;&#36807;&#31243;&#65292;&#21916;&#27426;&#23427;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore AI-powered upscaling as a design assistance tool in the context of creating 2D game levels. Deep neural networks are used to upscale artificially downscaled patches of levels from the puzzle platformer game Lode Runner. The trained networks are incorporated into a web-based editor, where the user can create and edit levels at three different levels of resolution: 4x4, 8x8, and 16x16. An edit at any resolution instantly transfers to the other resolutions. As upscaling requires inventing features that might not be present at lower resolutions, we train neural networks to reproduce these features. We introduce a neural network architecture that is capable of not only learning upscaling but also giving higher priority to less frequent tiles. To investigate the potential of this tool and guide further development, we conduct a qualitative study with 3 designers to understand how they use it. Designers enjoyed co-designing with the tool, liked its underlying concept, and provided 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#24207;&#31995;&#32479;&#20013;&#30340;&#26032;&#20852;&#26234;&#33021;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#29289;&#29702;&#23398;&#25506;&#32034;&#23398;&#20064;&#26426;&#21046;&#21644;&#29289;&#29702;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#27492;&#20026;&#25351;&#23548;&#21407;&#21017;&#35774;&#35745;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.01538</link><description>&lt;p&gt;
&#38750;&#24179;&#34913;&#29289;&#29702;&#23398;&#65306;&#20174;&#33258;&#26059;&#29627;&#29827;&#21040;&#26426;&#22120;&#21644;&#31070;&#32463;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-equilibrium physics: from spin glasses to machine and neural learning. (arXiv:2308.01538v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#24207;&#31995;&#32479;&#20013;&#30340;&#26032;&#20852;&#26234;&#33021;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#29289;&#29702;&#23398;&#25506;&#32034;&#23398;&#20064;&#26426;&#21046;&#21644;&#29289;&#29702;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#27492;&#20026;&#25351;&#23548;&#21407;&#21017;&#35774;&#35745;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#24207;&#22810;&#20307;&#31995;&#32479;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#34920;&#29616;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#26032;&#20852;&#29616;&#35937;&#12290;&#36825;&#20123;&#22797;&#26434;&#34892;&#20026;&#21487;&#20197;&#29992;&#20110;&#38169;&#35823;&#20462;&#27491;&#12289;&#23398;&#20064;&#21644;&#20248;&#21270;&#31561;&#21508;&#31181;&#20449;&#24687;&#22788;&#29702;&#20219;&#21153;&#12290;&#23613;&#31649;&#21033;&#29992;&#36825;&#20123;&#31995;&#32479;&#36827;&#34892;&#26234;&#33021;&#20219;&#21153;&#30340;&#32463;&#39564;&#25104;&#26524;&#26174;&#33879;&#65292;&#20294;&#20854;&#20986;&#29616;&#30340;&#26234;&#33021;&#34892;&#20026;&#30340;&#22522;&#26412;&#21407;&#21017;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#30693;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#32479;&#35745;&#29289;&#29702;&#23398;&#26469;&#34920;&#24449;&#26080;&#24207;&#31995;&#32479;&#20013;&#30340;&#36825;&#31181;&#26032;&#20852;&#26234;&#33021;&#12290;&#25105;&#20204;&#26681;&#25454;&#23398;&#20064;&#26426;&#21046;&#65288;&#38271;&#26399;&#35760;&#24518; vs. &#24037;&#20316;&#35760;&#24518;&#65289;&#21644;&#23398;&#20064;&#21160;&#21147;&#23398;&#65288;&#20154;&#24037; vs. &#33258;&#28982;&#65289;&#36825;&#20004;&#20010;&#26041;&#38754;&#21046;&#23450;&#20102;&#25105;&#20204;&#22312;&#35770;&#25991;&#20013;&#30340;&#21162;&#21147;&#30340;&#36335;&#32447;&#22270;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23398;&#20064;&#26426;&#21046;&#21644;&#29289;&#29702;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#20851;&#31995;&#21487;&#20197;&#20316;&#20026;&#35774;&#35745;&#26234;&#33021;&#31995;&#32479;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#23545;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#20852;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#33021;&#22815;&#25193;&#23637;&#25105;&#20204;&#24403;&#21069;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disordered many-body systems exhibit a wide range of emergent phenomena across different scales. These complex behaviors can be utilized for various information processing tasks such as error correction, learning, and optimization. Despite the empirical success of utilizing these systems for intelligent tasks, the underlying principles that govern their emergent intelligent behaviors remain largely unknown. In this thesis, we aim to characterize such emergent intelligence in disordered systems through statistical physics. We chart a roadmap for our efforts in this thesis based on two axes: learning mechanisms (long-term memory vs. working memory) and learning dynamics (artificial vs. natural). Throughout our journey, we uncover relationships between learning mechanisms and physical dynamics that could serve as guiding principles for designing intelligent systems. We hope that our investigation into the emergent intelligence of seemingly disparate learning systems can expand our current
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#32593;&#32476;&#27010;&#24565;&#30340;&#37327;&#23376;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;QMARL&#65289;&#31639;&#27861;&#65292;&#30446;&#26631;&#26159;&#24212;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#21442;&#25968;&#21033;&#29992;&#21644;&#25910;&#25947;&#22256;&#38590;&#38382;&#39064;&#12290;QMARL&#22312;&#21487;&#25193;&#23637;&#24615;&#12289;&#21442;&#25968;&#21033;&#29992;&#25928;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#22870;&#21169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#30340;&#20219;&#21153;&#31934;&#24230;&#26469;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#12290;&#21478;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#20215;&#20540;&#27979;&#37327;&#65288;PVM&#65289;&#30340;&#21487;&#25193;&#23637;&#25216;&#26415;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#31995;&#32479;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.01519</link><description>&lt;p&gt;
&#37327;&#23376;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#20027;&#31227;&#21160;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Quantum Multi-Agent Reinforcement Learning for Autonomous Mobility Cooperation. (arXiv:2308.01519v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#32593;&#32476;&#27010;&#24565;&#30340;&#37327;&#23376;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;QMARL&#65289;&#31639;&#27861;&#65292;&#30446;&#26631;&#26159;&#24212;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#21442;&#25968;&#21033;&#29992;&#21644;&#25910;&#25947;&#22256;&#38590;&#38382;&#39064;&#12290;QMARL&#22312;&#21487;&#25193;&#23637;&#24615;&#12289;&#21442;&#25968;&#21033;&#29992;&#25928;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#22870;&#21169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#30340;&#20219;&#21153;&#31934;&#24230;&#26469;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#12290;&#21478;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#20215;&#20540;&#27979;&#37327;&#65288;PVM&#65289;&#30340;&#21487;&#25193;&#23637;&#25216;&#26415;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#31995;&#32479;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;4.0&#38761;&#21629;&#20013;&#65292;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#21512;&#20316;&#33258;&#20027;&#31227;&#21160;&#31995;&#32479;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;MARL&#31639;&#27861;&#23384;&#22312;&#30528;&#22823;&#37327;&#21442;&#25968;&#20351;&#29992;&#21644;&#25910;&#25947;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#32593;&#32476;&#27010;&#24565;&#30340;&#37327;&#23376;MARL&#65288;QMARL&#65289;&#31639;&#27861;&#65292;&#36825;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#26159;&#26377;&#30410;&#30340;&#65292;&#20197;&#24212;&#23545;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;QMARL&#22312;&#21442;&#25968;&#21033;&#29992;&#25928;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;&#26041;&#38754;&#20063;&#20855;&#26377;&#30410;&#22788;&#65292;&#36825;&#26159;&#30001;&#20110;&#37327;&#23376;&#38712;&#26435;&#30340;&#21407;&#22240;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;QMARL&#20013;&#30340;&#22870;&#21169;&#23450;&#20041;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#30340;&#20219;&#21153;&#31934;&#24230;&#65292;&#22240;&#27492;&#21487;&#20197;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#20215;&#20540;&#27979;&#37327;&#65288;PVM&#65289;&#30340;&#21487;&#25193;&#23637;&#25216;&#26415;&#12290;&#22522;&#20110;PVM&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;QMARL&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#21160;&#20316;&#26469;&#23454;&#29616;&#26368;&#39640;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
For Industry 4.0 Revolution, cooperative autonomous mobility systems are widely used based on multi-agent reinforcement learning (MARL). However, the MARL-based algorithms suffer from huge parameter utilization and convergence difficulties with many agents. To tackle these problems, a quantum MARL (QMARL) algorithm based on the concept of actor-critic network is proposed, which is beneficial in terms of scalability, to deal with the limitations in the noisy intermediate-scale quantum (NISQ) era. Additionally, our QMARL is also beneficial in terms of efficient parameter utilization and fast convergence due to quantum supremacy. Note that the reward in our QMARL is defined as task precision over computation time in multiple agents, thus, multi-agent cooperation can be realized. For further improvement, an additional technique for scalability is proposed, which is called projection value measure (PVM). Based on PVM, our proposed QMARL can achieve the highest reward, by reducing the action
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24863;&#30693;&#21644;&#26410;&#26469;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38544;&#24335;&#21344;&#29992;&#27969;&#22330;&#26469;&#34920;&#31034;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23545;&#21608;&#22260;&#29615;&#22659;&#30340;&#24863;&#30693;&#24182;&#39044;&#27979;&#20854;&#20182;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#21644;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01471</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#38544;&#24335;&#21344;&#29992;&#27969;&#22330;&#24212;&#29992;&#20110;&#24863;&#30693;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving. (arXiv:2308.01471v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24863;&#30693;&#21644;&#26410;&#26469;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38544;&#24335;&#21344;&#29992;&#27969;&#22330;&#26469;&#34920;&#31034;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23545;&#21608;&#22260;&#29615;&#22659;&#30340;&#24863;&#30693;&#24182;&#39044;&#27979;&#20854;&#20182;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#21644;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24517;&#39035;&#33021;&#22815;&#24863;&#30693;&#21608;&#22260;&#29615;&#22659;&#24182;&#39044;&#27979;&#20854;&#20182;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#65292;&#28982;&#21518;&#23545;&#26816;&#27979;&#21040;&#30340;&#30446;&#26631;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#65292;&#35201;&#20040;&#39044;&#27979;&#25972;&#20010;&#22330;&#26223;&#30340;&#23494;&#38598;&#21344;&#29992;&#21644;&#27969;&#26684;&#12290;&#21069;&#32773;&#30001;&#20110;&#25928;&#29575;&#21407;&#22240;&#38656;&#35201;&#20445;&#25345;&#26816;&#27979;&#25968;&#37327;&#36739;&#23569;&#65292;&#20174;&#32780;&#29306;&#29298;&#20102;&#23545;&#35937;&#30340;&#22238;&#24518;&#29575;&#65292;&#23384;&#22312;&#23433;&#20840;&#38382;&#39064;&#12290;&#21518;&#32773;&#30001;&#20110;&#36755;&#20986;&#26684;&#30340;&#39640;&#32500;&#24615;&#21644;&#23436;&#20840;&#21367;&#31215;&#32593;&#32476;&#22266;&#26377;&#30340;&#26377;&#38480;&#24863;&#21463;&#37326;&#32780;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#39044;&#27979;&#21487;&#33021;&#27704;&#36828;&#19981;&#20250;&#34987;&#36816;&#21160;&#35268;&#21010;&#22120;&#26597;&#35810;&#30340;&#21306;&#22495;&#25110;&#23545;&#35937;&#12290;&#37492;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24863;&#30693;&#21644;&#26410;&#26469;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#34920;&#31034;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#21344;&#29992;&#21644;&#27969;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#30452;&#25509;&#34987;&#36816;&#21160;&#35268;&#21010;&#22120;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
A self-driving vehicle (SDV) must be able to perceive its surroundings and predict the future behavior of other traffic participants. Existing works either perform object detection followed by trajectory forecasting of the detected objects, or predict dense occupancy and flow grids for the whole scene. The former poses a safety concern as the number of detections needs to be kept low for efficiency reasons, sacrificing object recall. The latter is computationally expensive due to the high-dimensionality of the output grid, and suffers from the limited receptive field inherent to fully convolutional networks. Furthermore, both approaches employ many computational resources predicting areas or objects that might never be queried by the motion planner. This motivates our unified approach to perception and future prediction that implicitly represents occupancy and flow over time with a single neural network. Our method avoids unnecessary computation, as it can be directly queried by the mo
&lt;/p&gt;</description></item><item><title>VertexSerum&#26159;&#19968;&#31181;&#38024;&#23545;&#38142;&#36335;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27602;&#21270;&#25915;&#20987;&#65292;&#36890;&#36807;&#25918;&#22823;&#38142;&#25509;&#36830;&#25509;&#24615;&#27844;&#28431;&#26469;&#22686;&#21152;&#22270;&#38142;&#25509;&#31363;&#21462;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23884;&#20837;&#21040;&#38142;&#25509;&#26816;&#27979;&#32593;&#32476;&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;VertexSerum&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;GNN&#32467;&#26500;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;9.8&#65285;&#30340;AUC&#20998;&#25968;&#65292;&#19988;&#22312;&#40657;&#30418;&#21644;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#22343;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01469</link><description>&lt;p&gt;
VertexSerum: &#38024;&#23545;&#38142;&#36335;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27602;&#21270;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
VertexSerum: Poisoning Graph Neural Networks for Link Inference. (arXiv:2308.01469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01469
&lt;/p&gt;
&lt;p&gt;
VertexSerum&#26159;&#19968;&#31181;&#38024;&#23545;&#38142;&#36335;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27602;&#21270;&#25915;&#20987;&#65292;&#36890;&#36807;&#25918;&#22823;&#38142;&#25509;&#36830;&#25509;&#24615;&#27844;&#28431;&#26469;&#22686;&#21152;&#22270;&#38142;&#25509;&#31363;&#21462;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23884;&#20837;&#21040;&#38142;&#25509;&#26816;&#27979;&#32593;&#32476;&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;VertexSerum&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;GNN&#32467;&#26500;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;9.8&#65285;&#30340;AUC&#20998;&#25968;&#65292;&#19988;&#22312;&#40657;&#30418;&#21644;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#22343;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21033;&#29992;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#65288;&#22914;&#31038;&#20132;&#20998;&#26512;&#21644;&#27450;&#35784;&#26816;&#27979;&#65289;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22270;&#30340;&#38142;&#25509;&#65288;&#20363;&#22914;&#31038;&#20132;&#20851;&#31995;&#21644;&#20132;&#26131;&#21382;&#21490;&#65289;&#26159;&#25935;&#24863;&#21644;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;GNNs&#26102;&#20250;&#24341;&#36215;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VertexSerum&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27602;&#21270;&#25915;&#20987;&#65292;&#36890;&#36807;&#25918;&#22823;&#38142;&#25509;&#36830;&#25509;&#24615;&#27844;&#28431;&#26469;&#22686;&#21152;&#22270;&#38142;&#25509;&#31363;&#21462;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#26356;&#20934;&#30830;&#22320;&#25512;&#26029;&#33410;&#28857;&#37051;&#25509;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23884;&#20837;&#21040;&#38142;&#25509;&#26816;&#27979;&#32593;&#32476;&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;GNN&#32467;&#26500;&#19978;&#65292;VertexSerum&#30340;AUC&#20998;&#25968;&#24179;&#22343;&#25552;&#39640;&#20102;9.8&#65285;&#65292;&#26174;&#33879;&#20248;&#20110;SOTA&#38142;&#36335;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#36824;&#39564;&#35777;&#20102;VertexSerum&#22312;&#40657;&#30418;&#21644;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have brought superb performance to various applications utilizing graph structural data, such as social analysis and fraud detection. The graph links, e.g., social relationships and transaction history, are sensitive and valuable information, which raises privacy concerns when using GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel graph poisoning attack that increases the effectiveness of graph link stealing by amplifying the link connectivity leakage. To infer node adjacency more accurately, we propose an attention mechanism that can be embedded into the link detection network. Our experiments demonstrate that VertexSerum significantly outperforms the SOTA link inference attack, improving the AUC scores by an average of $9.8\%$ across four real-world datasets and three different GNN structures. Furthermore, our experiments reveal the effectiveness of VertexSerum in both black-box and online learning settings, further validating its a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20845;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#36817;&#20284;&#23460;&#20869;&#27745;&#26579;&#29289;&#27987;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#29366;&#24577;&#31354;&#38388;&#27010;&#24565;&#12289;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#20998;&#35299;&#25216;&#26415;&#12290;&#36890;&#36807;&#22312;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#19968;&#26635;&#21830;&#19994;&#24314;&#31569;&#20013;&#20116;&#20010;&#21150;&#20844;&#23460;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.01438</link><description>&lt;p&gt;
&#23460;&#20869;&#31354;&#27668;&#36136;&#37327;&#36817;&#20284;&#30340;&#26032;&#39062;&#22522;&#20110;&#29289;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations. (arXiv:2308.01438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20845;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#36817;&#20284;&#23460;&#20869;&#27745;&#26579;&#29289;&#27987;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#29366;&#24577;&#31354;&#38388;&#27010;&#24565;&#12289;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#20998;&#35299;&#25216;&#26415;&#12290;&#36890;&#36807;&#22312;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#19968;&#26635;&#21830;&#19994;&#24314;&#31569;&#20013;&#20116;&#20010;&#21150;&#20844;&#23460;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#26412;&#20302;&#24265;&#30340;&#20256;&#24863;&#22120;&#33021;&#22815;&#23454;&#26102;&#25429;&#25417;&#21040;&#19981;&#21516;&#27745;&#26579;&#29289;&#27987;&#24230;&#12289;&#23460;&#20869;/&#23460;&#22806;&#28287;&#24230;&#21644;&#28201;&#24230;&#31561;&#19982;&#31354;&#27668;&#36136;&#37327;&#30456;&#20851;&#30340;&#22810;&#31181;&#27169;&#24577;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#25552;&#21069;&#36817;&#20284;&#23460;&#20869;&#31354;&#27668;&#36136;&#37327;&#12290;&#27627;&#26080;&#30097;&#38382;&#65292;&#20934;&#30830;&#30340;&#23460;&#20869;&#31354;&#27668;&#36136;&#37327;&#36817;&#20284;&#26377;&#21161;&#20110;&#25552;&#20379;&#20581;&#24247;&#30340;&#23460;&#20869;&#29615;&#22659;&#65292;&#20248;&#21270;&#30456;&#20851;&#33021;&#32791;&#65292;&#24182;&#25552;&#20379;&#20154;&#20307;&#33298;&#36866;&#24230;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#25152;&#35859;&#30340;&#38382;&#39064;&#29289;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#36817;&#20284;&#23460;&#20869;&#27745;&#26579;&#29289;&#27987;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21253;&#25324;&#29289;&#29702;&#23398;&#20013;&#30340;&#29366;&#24577;&#31354;&#38388;&#27010;&#24565;&#12289;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#20998;&#35299;&#25216;&#26415;&#30340;&#24039;&#22937;&#32452;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#20174;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#19968;&#26635;&#21830;&#19994;&#24314;&#31569;&#20013;&#20116;&#20010;&#21150;&#20844;&#23460;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36739;&#20026;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cost-effective sensors are capable of real-time capturing a variety of air quality-related modalities from different pollutant concentrations to indoor/outdoor humidity and temperature. Machine learning (ML) models are capable of performing air-quality "ahead-of-time" approximations. Undoubtedly, accurate indoor air quality approximation significantly helps provide a healthy indoor environment, optimize associated energy consumption, and offer human comfort. However, it is crucial to design an ML architecture to capture the domain knowledge, so-called problem physics. In this study, we propose six novel physics-based ML models for accurate indoor pollutant concentration approximations. The proposed models include an adroit combination of state-space concepts in physics, Gated Recurrent Units, and Decomposition techniques. The proposed models were illustrated using data collected from five offices in a commercial building in California. The proposed models are shown to be less complex, 
&lt;/p&gt;</description></item><item><title>ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01423</link><description>&lt;p&gt;
ChatMOF: &#19968;&#31181;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;
&lt;/p&gt;
&lt;p&gt;
ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01423
&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#20010;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#65288;MOFs&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;gpt-3.5-turbo&#65289;&#65292;ChatMOF&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#21018;&#24615;&#32467;&#26500;&#21270;&#26597;&#35810;&#30340;&#38656;&#27714;&#12290;&#35813;&#31995;&#32479;&#30001;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#65288;&#21363;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#65289;&#32452;&#25104;&#65292;&#24418;&#25104;&#19968;&#20010;&#24378;&#22823;&#30340;&#27969;&#27700;&#32447;&#65292;&#31649;&#29702;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#12290;&#35813;&#30740;&#31350;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#26410;&#26469;&#21457;&#23637;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#19982;&#37329;&#34701;&#19987;&#23478;&#30340;&#23545;&#35805;&#21644;&#21453;&#39304;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29983;&#25104;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#20934;&#30830;&#12289;&#30456;&#20851;&#21644;&#37329;&#34701;&#39118;&#26684;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.01415</link><description>&lt;p&gt;
&#19968;&#20010;&#26377;&#25928;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#29992;&#20110;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25351;&#20196;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model. (arXiv:2308.01415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#19982;&#37329;&#34701;&#19987;&#23478;&#30340;&#23545;&#35805;&#21644;&#21453;&#39304;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29983;&#25104;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#20934;&#30830;&#12289;&#30456;&#20851;&#21644;&#37329;&#34701;&#39118;&#26684;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21021;&#26399;&#38454;&#27573;&#65292;&#20026;&#37329;&#34701;&#30456;&#20851;&#20219;&#21153;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#38750;&#24120;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#27700;&#32447;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#24341;&#21457;&#20102;&#19968;&#20010;AI&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#19987;&#23478;&#20043;&#38388;&#30340;&#23545;&#35805;&#65292;&#24182;&#34701;&#20837;&#20102;&#20154;&#24037;&#37329;&#34701;&#19987;&#23478;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#25968;&#25454;&#38598;&#12290;&#35813;&#27969;&#27700;&#32447;&#20135;&#29983;&#20102;&#19968;&#20010;&#30001;103k&#20010;&#22810;&#36718;&#23545;&#35805;&#32452;&#25104;&#30340;&#31283;&#23450;&#30340;&#25351;&#20196;&#31934;&#35843;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#37319;&#29992;&#22806;&#37096;&#30340;GPT-4&#20316;&#20026;&#35780;&#21028;&#32773;&#65292;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26377;&#24076;&#26395;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#20934;&#30830;&#12289;&#30456;&#20851;&#21644;&#37329;&#34701;&#39118;&#26684;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#32780;&#20026;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the beginning era of large language model, it is quite critical to generate a high-quality financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model's performance by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models, and thus providing a powerful tool for applications within the financial sector.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#35774;&#35745;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;HouYi&#21450;&#20854;&#23545;&#24212;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#23398;&#26415;&#35770;&#25991;&#25968;&#25454;&#38598;REAP&#12290;HouYi&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#23398;&#26415;&#35770;&#25991;&#27573;&#33853;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#19982;ChatGPT&#30456;&#24403;&#65292;&#30053;&#20248;&#20110;Clau&#12290;</title><link>http://arxiv.org/abs/2308.01414</link><description>&lt;p&gt;
HouYi: &#19968;&#31181;&#19987;&#38376;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#30899;&#20013;&#21644;&#39046;&#22495;&#35774;&#35745;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field. (arXiv:2308.01414v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#35774;&#35745;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;HouYi&#21450;&#20854;&#23545;&#24212;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#23398;&#26415;&#35770;&#25991;&#25968;&#25454;&#38598;REAP&#12290;HouYi&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#23398;&#26415;&#35770;&#25991;&#27573;&#33853;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#19982;ChatGPT&#30456;&#24403;&#65292;&#30053;&#20248;&#20110;Clau&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20877;&#29983;&#33021;&#28304;&#23545;&#20110;&#23454;&#29616;&#30899;&#20013;&#21644;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#22914;ChatGPT&#22312;&#33258;&#21160;&#20869;&#23481;&#29983;&#25104;&#26041;&#38754;&#30340;&#25104;&#21151;&#65292;&#20351;&#24471;LLMs&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#25198;&#28436;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19987;&#38376;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#35774;&#35745;&#30340;LLM&#65292;&#20063;&#27809;&#26377;&#20219;&#20309;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;LLM&#12290;&#22240;&#27492;&#65292;&#26412;&#35770;&#25991;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;&#38750;&#21830;&#19994;&#24615;&#21487;&#20877;&#29983;&#33021;&#28304;LLM&#30740;&#31350;&#30340;&#24320;&#28304;&#21487;&#20877;&#29983;&#33021;&#28304;&#23398;&#26415;&#35770;&#25991;&#65288;REAP&#65289;&#25968;&#25454;&#38598;&#12290;REAP&#25968;&#25454;&#38598;&#36890;&#36807;&#20174;Web of Science&#25628;&#32034;1168970&#31687;&#23398;&#26415;&#25991;&#29486;&#30340;&#26631;&#39064;&#21644;&#25688;&#35201;&#36827;&#34892;&#25910;&#38598;&#12290;&#22522;&#20110;REAP&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#36890;&#29992;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;LLM&#27169;&#22411;HouYi&#12290;HouYi&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#39046;&#22495;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23398;&#26415;&#35770;&#25991;&#27573;&#33853;&#29983;&#25104;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23427;&#22312;&#29983;&#25104;&#21487;&#20877;&#29983;&#33021;&#28304;&#23398;&#26415;&#35770;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#19982;ChatGPT&#30456;&#24403;&#65292;&#22312;&#26576;&#20123;&#26041;&#38754;&#30053;&#20248;&#20110;Clau&#12290;
&lt;/p&gt;
&lt;p&gt;
Renewable energy is important for achieving carbon neutrality goal. With the great success of Large Language Models (LLMs) like ChatGPT in automatic content generation, LLMs are playing an increasingly important role. However, there has not been a specially designed LLM for renewable energy. Meanwhile, there has not been any dataset of renewable energy for training LLMs. Therefore, this paper published the first open-source Renewable Energy Academic Paper (REAP) dataset for non-commercial LLM research of renewable energy. REAP dataset is collected through searching the title and abstract of 1,168,970 academic literatures from Web of Science. Based on REAP dataset, HouYi model, the first LLM for renewable energy, is developed through finetuning general LLMs. HouYi demonstrated powerful academic paper paragraph generation ability in renewable energy field. Experiments show that its ability to generate academic papers on renewable energy is comparable to ChatGPT, slightly outperforms Clau
&lt;/p&gt;</description></item><item><title>LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01413</link><description>&lt;p&gt;
LaFiCMIL&#65306;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22823;&#25991;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01413
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#12290;&#30452;&#35266;&#19978;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26399;&#26395;&#25991;&#26412;&#20998;&#31867;&#65292;&#20316;&#20026;&#19981;&#38656;&#35201;&#20687;&#29983;&#25104;&#20219;&#21153;&#37027;&#26679;&#35768;&#22810;&#39640;&#32423;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;Transformer&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22312;&#22810;&#31867;&#21035;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#38271;&#25991;&#26412;&#25991;&#26723;&#21644;&#20854;&#20182;&#22823;&#25991;&#20214;&#30340;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#12290;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#21463;&#21040;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#30340;&#38459;&#30861;&#65306;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#27604;&#22914;BERT&#30340;512&#20010;&#26631;&#35760;&#12290;&#34429;&#28982;&#22686;&#21152;GPU&#20869;&#23384;&#21487;&#20197;&#31245;&#24494;&#25193;&#23637;&#36825;&#20010;&#38480;&#21046;&#65292;&#20294;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;GPU&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#36755;&#20837;&#38480;&#21046;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;LaFiCMIL&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have revolutionized the performance of a wide range of language tasks. Intuitively, one might expect text classification, which does not necessitate as many high-level representations as generative tasks, to be comprehensively addressed with the powerful representation capabilities of Transformers. However, in reality, there remains significant potential for enhancement, particularly in the areas of multi-class and multi-label classification of lengthy textual documents and other large files. The performance of Transformer-based models is mainly hindered by a major limitation: a restricted input length, e.g., 512 tokens for BERT. While an increase in GPU memory can marginally extend this limit, practical real-world applications often operate under constrained GPU resources. In this work, we tackle the input limit problem from the perspective of correlated multiple instance learning. The proposed approach, LaFiCMIL, serves as a versatile framework applicable to 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.01399</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#19982;&#20154;&#31867;&#22312;&#19990;&#30028;&#20013;&#30456;&#20114;&#20316;&#29992;&#65292;&#20195;&#29702;&#22120;&#38656;&#35201;&#29702;&#35299;&#20154;&#20204;&#20351;&#29992;&#30340;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#35270;&#35273;&#19990;&#30028;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#22522;&#20110;&#35821;&#35328;&#34892;&#21160;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#20195;&#29702;&#22120;&#21487;&#20197;&#36890;&#36807;&#20219;&#21153;&#22870;&#21169;&#23398;&#20064;&#25191;&#34892;&#31616;&#21333;&#30340;&#35821;&#35328;&#25351;&#20196;&#65292;&#20294;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#21487;&#20197;&#21033;&#29992;&#20256;&#36798;&#19968;&#33324;&#30693;&#35782;&#12289;&#25551;&#36848;&#19990;&#30028;&#29366;&#24577;&#12289;&#25552;&#20379;&#20114;&#21160;&#21453;&#39304;&#31561;&#22810;&#26679;&#21270;&#35821;&#35328;&#30340;&#20195;&#29702;&#22120;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#65306;&#23558;&#20250;&#34987;&#35266;&#23519;&#21040;&#20160;&#20040;&#12289;&#19990;&#30028;&#23558;&#22914;&#20309;&#36816;&#34892;&#20197;&#21450;&#21738;&#20123;&#24773;&#20917;&#23558;&#33719;&#24471;&#22870;&#21169;&#12290;&#36825;&#20010;&#35266;&#28857;&#23558;&#35821;&#35328;&#29702;&#35299;&#19982;&#26410;&#26469;&#39044;&#27979;&#32479;&#19968;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Dynalang&#65292;&#19968;&#31181;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#30340;&#20195;&#29702;&#22120;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#24819;&#20687;&#30340;&#27169;&#22411;&#22238;&#28378;&#20013;&#23398;&#20064;&#34892;&#21160;&#12290;&#19982;&#21482;&#20351;&#29992;&#35821;&#35328;&#39044;&#27979;&#21160;&#20316;&#30340;&#20256;&#32479;&#20195;&#29702;&#22120;&#19981;&#21516;&#65292;Dynalang&#36890;&#36807;&#36807;&#21435;&#30340;&#35821;&#35328;&#36824;&#21487;&#20197;&#33719;&#21462;&#20016;&#23500;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to 
&lt;/p&gt;</description></item><item><title>OpenFlamingo&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36798;&#21040;&#20102;&#23545;&#24212;&#27169;&#22411;&#24615;&#33021;&#30340;80%&#33267;89%&#12290;</title><link>http://arxiv.org/abs/2308.01390</link><description>&lt;p&gt;
OpenFlamingo: &#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. (arXiv:2308.01390v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01390
&lt;/p&gt;
&lt;p&gt;
OpenFlamingo&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36798;&#21040;&#20102;&#23545;&#24212;&#27169;&#22411;&#24615;&#33021;&#30340;80%&#33267;89%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;OpenFlamingo&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#33258;&#22238;&#24402;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;3B&#21040;9B&#12290; OpenFlamingo&#26159;&#19968;&#20010;&#25345;&#32493;&#21162;&#21147;&#30340;&#39033;&#30446;&#65292;&#26088;&#22312;&#22797;&#21046;DeepMind&#30340;Flamingo&#27169;&#22411;&#30340;&#24320;&#28304;&#29256;&#26412;&#12290;&#22312;&#19971;&#20010;&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#65292;OpenFlamingo&#27169;&#22411;&#30340;&#24615;&#33021;&#20171;&#20110;&#23545;&#24212;&#30340;Flamingo&#24615;&#33021;&#30340;80%&#33267;89%&#20043;&#38388;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21644;&#35780;&#20272;&#22871;&#20214;&#12290;&#25105;&#20204;&#22312;https://github.com/mlfoundations/open_flamingo&#19978;&#20998;&#20139;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at https://github.com/mlfoundations/open_flamingo.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CausalOps&#65292;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#27169;&#22411;&#24320;&#21457;&#21644;&#24212;&#29992;&#30340;&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#65292;&#26088;&#22312;&#25512;&#21160;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#37319;&#29992;&#22240;&#26524;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01375</link><description>&lt;p&gt;
CausalOps -- &#23454;&#29616;&#22240;&#26524;&#27010;&#29575;&#22270;&#27169;&#22411;&#23454;&#38469;&#29983;&#21629;&#21608;&#26399;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CausalOps -- Towards an Industrial Lifecycle for Causal Probabilistic Graphical Models. (arXiv:2308.01375v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01375
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CausalOps&#65292;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#27169;&#22411;&#24320;&#21457;&#21644;&#24212;&#29992;&#30340;&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#65292;&#26088;&#22312;&#25512;&#21160;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#37319;&#29992;&#22240;&#26524;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#27010;&#29575;&#22270;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#24314;&#27169;&#22240;&#26524;&#20851;&#31995;&#12290;&#38543;&#30528;&#20854;&#22312;&#27773;&#36710;&#31995;&#32479;&#23433;&#20840;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#26032;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20154;&#20204;&#23545;&#20110;&#31867;&#20284;DevOps&#21644;MLOps&#30340;&#38598;&#25104;&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#30446;&#21069;&#65292;&#32570;&#20047;&#19968;&#20010;&#36866;&#29992;&#20110;&#22240;&#26524;&#24037;&#31243;&#30340;&#32452;&#32455;&#21442;&#32771;&#27969;&#31243;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#24182;&#20419;&#36827;&#24191;&#27867;&#30340;&#24037;&#19994;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CausalOps&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#22240;&#26524;&#27169;&#22411;&#24320;&#21457;&#21644;&#24212;&#29992;&#30340;&#20840;&#26032;&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#12290;&#36890;&#36807;&#23450;&#20041;&#22240;&#26524;&#24037;&#31243;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#20851;&#38190;&#23454;&#20307;&#12289;&#20381;&#36182;&#20851;&#31995;&#21644;&#20013;&#38388;&#20135;&#29289;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19968;&#33268;&#30340;&#35789;&#27719;&#34920;&#21644;&#24037;&#20316;&#27969;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#27169;&#22411;&#30340;&#20351;&#29992;&#24773;&#22659;&#21270;&#20026;&#19981;&#21516;&#38454;&#27573;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#27010;&#36848;&#20102;&#21019;&#24314;&#21644;&#32500;&#25252;&#22240;&#26524;&#27169;&#22411;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;CausalOps&#26088;&#22312;&#25512;&#21160;&#22240;&#26524;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal probabilistic graph-based models have gained widespread utility, enabling the modeling of cause-and-effect relationships across diverse domains. With their rising adoption in new areas, such as automotive system safety and machine learning, the need for an integrated lifecycle framework akin to DevOps and MLOps has emerged. Currently, a process reference for organizations interested in employing causal engineering is missing. To address this gap and foster widespread industrial adoption, we propose CausalOps, a novel lifecycle framework for causal model development and application. By defining key entities, dependencies, and intermediate artifacts generated during causal engineering, we establish a consistent vocabulary and workflow model. This work contextualizes causal model usage across different stages and stakeholders, outlining a holistic view of creating and maintaining them. CausalOps' aim is to drive the adoption of causal methods in practical applications within intere
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#39550;&#39542;&#24322;&#36136;&#24615;&#21644;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#26469;&#22686;&#24378;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23618;&#32423;&#27169;&#22411;&#23558;&#21608;&#22260;&#20154;&#39550;&#39542;&#36710;&#36742;&#30340;&#39550;&#39542;&#34892;&#20026;&#21644;&#38271;&#26399;&#36712;&#36857;&#36827;&#34892;&#32806;&#21512;&#65292;&#20197;&#25552;&#39640;&#39550;&#39542;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01369</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#39550;&#39542;&#24322;&#36136;&#24615;&#21644;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#26469;&#22686;&#24378;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An enhanced motion planning approach by integrating driving heterogeneity and long-term trajectory prediction for automated driving systems. (arXiv:2308.01369v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#39550;&#39542;&#24322;&#36136;&#24615;&#21644;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#26469;&#22686;&#24378;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23618;&#32423;&#27169;&#22411;&#23558;&#21608;&#22260;&#20154;&#39550;&#39542;&#36710;&#36742;&#30340;&#39550;&#39542;&#34892;&#20026;&#21644;&#38271;&#26399;&#36712;&#36857;&#36827;&#34892;&#32806;&#21512;&#65292;&#20197;&#25552;&#39640;&#39550;&#39542;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#22797;&#26434;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#23548;&#33322;&#26159;&#22256;&#38590;&#30340;&#12290;&#39044;&#27979;&#21608;&#22260;&#20154;&#39550;&#39542;&#36710;&#36742;&#30340;&#39550;&#39542;&#34892;&#20026;&#26159;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#36895;&#20844;&#36335;&#21512;&#27969;&#22330;&#26223;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#22686;&#24378;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#12290;&#35813;&#22686;&#24378;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#20010;&#26041;&#38754;&#30340;&#32467;&#26524;&#65306;&#21608;&#22260;&#20154;&#39550;&#39542;&#36710;&#36742;&#30340;&#39550;&#39542;&#34892;&#20026;&#21644;&#38271;&#26399;&#36712;&#36857;&#65292;&#36890;&#36807;&#20351;&#29992;&#23618;&#32423;&#27169;&#22411;&#23558;&#23427;&#20204;&#32806;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#36816;&#21160;&#35268;&#21010;&#65292;&#20197;&#25552;&#39640;&#39550;&#39542;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Navigating automated driving systems (ADSs) through complex driving environments is difficult. Predicting the driving behavior of surrounding human-driven vehicles (HDVs) is a critical component of an ADS. This paper proposes an enhanced motion-planning approach for an ADS in a highway-merging scenario. The proposed enhanced approach utilizes the results of two aspects: the driving behavior and long-term trajectory of surrounding HDVs, which are coupled using a hierarchical model that is used for the motion planning of an ADS to improve driving safety.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#20102;&#23454;&#35777;&#32763;&#35793;&#36807;&#31243;&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#20316;&#20026;&#24314;&#27169;&#28145;&#23884;&#20837;&#24335;&#32763;&#35793;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#28608;&#21160;&#20154;&#24515;&#30340;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.01368</link><description>&lt;p&gt;
&#23454;&#35777;&#32763;&#35793;&#36807;&#31243;&#30740;&#31350;&#65306;&#36807;&#21435;&#21644;&#21487;&#33021;&#30340;&#26410;&#26469;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Empirical Translation Process Research: Past and Possible Future Perspectives. (arXiv:2308.01368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#20102;&#23454;&#35777;&#32763;&#35793;&#36807;&#31243;&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#20316;&#20026;&#24314;&#27169;&#28145;&#23884;&#20837;&#24335;&#32763;&#35793;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#28608;&#21160;&#20154;&#24515;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#22235;&#21313;&#24180;&#37324;&#65292;&#20154;&#20204;&#19968;&#30452;&#33268;&#21147;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#23454;&#35777;&#32763;&#35793;&#36807;&#31243;&#30740;&#31350;&#65288;TPR&#65289;&#30340;&#27169;&#22411;&#65292;&#20294;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#36861;&#36394;&#20102;CRITT TPR-DB&#20256;&#32479;&#20013;&#23454;&#35777;TPR&#30340;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#65288;FEP&#65289;&#21644;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#20316;&#20026;&#24314;&#27169;&#28145;&#23884;&#20837;&#24335;&#32763;&#35793;&#36807;&#31243;&#30340;&#26694;&#26550;&#12290;&#23427;&#24341;&#20837;&#20102;&#37327;&#21270;&#20851;&#32852;&#29702;&#35770;&#65288;&#30456;&#20851;&#24615;&#65292;s-mode&#65292;i-mode&#65289;&#22522;&#26412;&#27010;&#24565;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#19982;&#30417;&#25511;&#27169;&#22411;&#30340;&#20851;&#31995;&#65292;&#23558;&#20851;&#32852;&#24615;&#26368;&#22823;&#21270;&#23450;&#20026;&#26368;&#23567;&#21270;&#33258;&#30001;&#33021;&#30340;&#29305;&#20363;&#12290;FEP/AIF&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#20005;&#35880;&#30340;&#22522;&#30784;&#65292;&#21487;&#20197;&#24314;&#27169;&#19981;&#21516;&#26102;&#38388;&#32447;&#19978;&#23637;&#24320;&#30340;&#23884;&#20837;&#24335;&#32763;&#35793;&#36807;&#31243;&#30340;&#28145;&#23618;&#26102;&#38388;&#26550;&#26500;&#12290;&#36825;&#20010;&#26694;&#26550;&#20026;&#39044;&#27979;&#24615;TPR&#30340;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21069;&#26223;&#65292;&#26377;&#26395;&#20016;&#23500;&#25105;&#20204;&#23545;&#20154;&#31867;&#32763;&#35793;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#23454;&#36341;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past four decades, efforts have been made to develop and evaluate models for Empirical Translation Process Research (TPR), yet a comprehensive framework remains elusive. This article traces the evolution of empirical TPR within the CRITT TPR-DB tradition and proposes the Free Energy Principle (FEP) and Active Inference (AIF) as a framework for modeling deeply embedded translation processes. It introduces novel approaches for quantifying fundamental concepts of Relevance Theory (relevance, s-mode, i-mode), and establishes their relation to the Monitor Model, framing relevance maximization as a special case of minimizing free energy. FEP/AIF provides a mathematically rigorous foundation that enables modeling of deep temporal architectures in which embedded translation processes unfold on different timelines. This framework opens up exciting prospects for future research in predictive TPR, likely to enrich our comprehension of human translation processes, and making valuable cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23884;&#20837;&#23398;&#20064;&#30340;&#23618;&#27425;&#25506;&#32034;&#31639;&#27861;EmbeddingTree&#21644;&#30456;&#24212;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#33021;&#22815;&#35299;&#37322;&#20855;&#26377;&#35821;&#20041;&#30340;&#23454;&#20307;&#29305;&#24449;&#21644;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01329</link><description>&lt;p&gt;
EmbeddingTree&#65306;&#23884;&#20837;&#24335;&#20013;&#23454;&#20307;&#29305;&#24449;&#30340;&#23618;&#27425;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding. (arXiv:2308.01329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23884;&#20837;&#23398;&#20064;&#30340;&#23618;&#27425;&#25506;&#32034;&#31639;&#27861;EmbeddingTree&#21644;&#30456;&#24212;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#33021;&#22815;&#35299;&#37322;&#20855;&#26377;&#35821;&#20041;&#30340;&#23454;&#20307;&#29305;&#24449;&#21644;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#23398;&#20064;&#23558;&#31163;&#25955;&#30340;&#25968;&#25454;&#23454;&#20307;&#36716;&#25442;&#20026;&#36830;&#32493;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#32534;&#30721;&#20102;&#23454;&#20307;&#30340;&#29305;&#24449;/&#23646;&#24615;&#12290;&#23613;&#31649;&#26377;&#22810;&#31181;&#23884;&#20837;&#23398;&#20064;&#31639;&#27861;&#25253;&#21578;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#25237;&#20837;&#21040;&#23545;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#22914;&#20309;&#32534;&#30721;&#30340;&#32467;&#26500;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EmbeddingTree&#65292;&#19968;&#31181;&#23618;&#27425;&#23884;&#20837;&#25506;&#32034;&#31639;&#27861;&#65292;&#23558;&#23454;&#20307;&#29305;&#24449;&#30340;&#35821;&#20041;&#19982;&#36739;&#38590;&#35299;&#37322;&#30340;&#23884;&#20837;&#21521;&#37327;&#30456;&#20851;&#32852;&#12290;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;EmbeddingTree&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#25506;&#32034;&#39640;&#32500;&#23884;&#20837;&#12290;&#35813;&#24037;&#20855;&#21487;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#25968;&#25454;&#23454;&#20307;&#30340;&#24494;&#22937;&#29305;&#24449;&#65292;&#22312;&#23884;&#20837;&#35757;&#32451;&#20013;&#25191;&#34892;&#29305;&#24449;&#21435;&#22122;/&#27880;&#20837;&#65292;&#24182;&#20026;&#26410;&#35265;&#23454;&#20307;&#29983;&#25104;&#23884;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#24037;&#19994;&#35268;&#27169;&#30340;&#21830;&#25143;&#25968;&#25454;&#21644;&#20844;&#20849;&#30340;30Music&#21548;&#27468;/&#25773;&#25918;&#21015;&#34920;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#23884;&#20837;&#26469;&#35777;&#26126;EmbeddingTree&#21644;&#25105;&#20204;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding learning transforms discrete data entities into continuous numerical representations, encoding features/properties of the entities. Despite the outstanding performance reported from different embedding learning algorithms, few efforts were devoted to structurally interpreting how features are encoded in the learned embedding space. This work proposes EmbeddingTree, a hierarchical embedding exploration algorithm that relates the semantics of entity features with the less-interpretable embedding vectors. An interactive visualization tool is also developed based on EmbeddingTree to explore high-dimensional embeddings. The tool helps users discover nuance features of data entities, perform feature denoising/injecting in embedding training, and generate embeddings for unseen entities. We demonstrate the efficacy of EmbeddingTree and our visualization tool through embeddings generated for industry-scale merchant data and the public 30Music listening/playlists dataset.
&lt;/p&gt;</description></item><item><title>DeepSpeed-Chat&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#20351;&#24471;ChatGPT-like&#27169;&#22411;&#30340;RLHF&#22521;&#35757;&#26131;&#20110;&#35775;&#38382;&#65292;&#39640;&#25928;&#19988;&#32463;&#27982;&#23454;&#24800;&#12290;&#23427;&#20855;&#26377;&#26131;&#20110;&#20351;&#29992;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20307;&#39564;&#65292;&#22797;&#21046;&#20102;InstructGPT&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#38598;&#25104;&#20102;&#21508;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01320</link><description>&lt;p&gt;
DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales.
&lt;/p&gt;
&lt;p&gt;
DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. (arXiv:2308.01320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01320
&lt;/p&gt;
&lt;p&gt;
DeepSpeed-Chat&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#20351;&#24471;ChatGPT-like&#27169;&#22411;&#30340;RLHF&#22521;&#35757;&#26131;&#20110;&#35775;&#38382;&#65292;&#39640;&#25928;&#19988;&#32463;&#27982;&#23454;&#24800;&#12290;&#23427;&#20855;&#26377;&#26131;&#20110;&#20351;&#29992;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20307;&#39564;&#65292;&#22797;&#21046;&#20102;InstructGPT&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#38598;&#25104;&#20102;&#21508;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT-like&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#24102;&#26469;&#20102;&#38761;&#21629;&#65292;&#20174;&#25688;&#35201;&#21644;&#32534;&#30721;&#21040;&#32763;&#35793;&#65292;&#29978;&#33267;&#36229;&#36234;&#20102;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29615;&#22659;&#36824;&#32570;&#20047;&#19968;&#31181;&#26131;&#20110;&#35775;&#38382;&#12289;&#39640;&#25928;&#19988;&#32463;&#27982;&#23454;&#24800;&#30340;&#31471;&#21040;&#31471;RLHF&#65288;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65289;&#35757;&#32451;&#27969;&#31243;&#65292;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#35268;&#27169;&#36798;&#21040;&#25968;&#21313;&#20159;&#21442;&#25968;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSpeed-Chat&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#20351;RLHF&#22521;&#35757;&#23545;AI&#31038;&#21306;&#21487;&#29992;&#12290;DeepSpeed-Chat&#25552;&#20379;&#20102;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;ChatGPT-like&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20307;&#39564;&#65292;&#19968;&#20010;&#22797;&#21046;InstructGPT&#35757;&#32451;&#27969;&#31243;&#30340;DeepSpeed-RLHF&#27969;&#27700;&#32447;&#65292;&#20197;&#21450;&#19968;&#20010;&#38598;&#25104;&#20102;&#21508;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#20248;&#21270;&#30340;&#24378;&#22823;DeepSpeed-RLHF&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#20197;&#35757;&#32451;&#20855;&#26377;&#25968;&#21315;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-like models have revolutionized various applications in artificial intelligence, from summarization and coding to translation, matching or even surpassing human performance. However, the current landscape lacks an accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement Learning with Human Feedback) training pipeline for these powerful models, particularly when training at the scale of billions of parameters. This paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. DeepSpeed-Chat offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way. The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#39046;&#22495;&#22312;&#30142;&#30149;&#35786;&#26029;&#26041;&#38754;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20110;&#30142;&#30149;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#20854;&#22312;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.01319</link><description>&lt;p&gt;
&#36817;&#24180;&#26469;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#30142;&#30149;&#35786;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;: &#31995;&#32479;&#24615;&#35843;&#26597;&#12289;&#27604;&#36739;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges. (arXiv:2308.01319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#39046;&#22495;&#22312;&#30142;&#30149;&#35786;&#26029;&#26041;&#38754;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20110;&#30142;&#30149;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#20854;&#22312;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;(CAD)&#20316;&#20026;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#39046;&#22495;&#65292;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#30001;&#20110;&#21307;&#23398;&#35786;&#26029;&#31995;&#32479;&#30340;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#35823;&#23548;&#30340;&#21307;&#30103;&#27835;&#30103;&#65292;&#36817;&#24180;&#26469;&#24050;&#32463;&#20184;&#20986;&#20102;&#37325;&#35201;&#21162;&#21147;&#26469;&#25913;&#36827;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#24212;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#31561;&#24335;&#21487;&#33021;&#20250;&#23548;&#33268;&#20851;&#20110;&#22120;&#23448;&#31561;&#39033;&#30446;&#30340;&#38169;&#35823;&#25351;&#31034;&#12290;&#22240;&#27492;&#65292;&#20174;&#31034;&#20363;&#20013;&#23398;&#20064;&#26159;&#27169;&#24335;&#35782;&#21035;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#26426;&#22120;&#23398;&#20064;&#26377;&#26395;&#25552;&#39640;&#30142;&#30149;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#31934;&#30830;&#24615;&#65292;&#21516;&#26102;&#25903;&#25345;&#20915;&#31574;&#36807;&#31243;&#30340;&#23458;&#35266;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#20026;&#21019;&#24314;&#20248;&#38597;&#19988;&#33258;&#20027;&#30340;&#31639;&#27861;&#26469;&#20998;&#26512;&#39640;&#32500;&#21644;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#38469;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#29992;&#20110;&#26816;&#27979;&#30142;&#30149;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#32925;&#28814;&#12289;&#31958;&#23615;&#30149;&#12289;&#32925;&#33039;&#30142;&#30149;&#12289;&#30331;&#38761;&#28909;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer-aided diagnosis (CAD), a vibrant medical imaging research field, is expanding quickly. Because errors in medical diagnostic systems might lead to seriously misleading medical treatments, major efforts have been made in recent years to improve computer-aided diagnostics applications. The use of machine learning in computer-aided diagnosis is crucial. A simple equation may result in a false indication of items like organs. Therefore, learning from examples is a vital component of pattern recognition. Pattern recognition and machine learning in the biomedical area promise to increase the precision of disease detection and diagnosis. They also support the decision-making process's objectivity. Machine learning provides a practical method for creating elegant and autonomous algorithms to analyze high-dimensional and multimodal bio-medical data. This review article examines machine-learning algorithms for detecting diseases, including hepatitis, diabetes, liver disease, dengue fever
&lt;/p&gt;</description></item><item><title>FusionAD&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01006</link><description>&lt;p&gt;
FusionAD: &#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01006
&lt;/p&gt;
&lt;p&gt;
FusionAD&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20219;&#21153;&#20013;&#65292;&#26500;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#24615;&#33021;&#24050;&#25104;&#20026;&#38081;&#26495;&#19968;&#22359;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#26469;&#32852;&#21512;&#20248;&#21270;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#20173;&#28982;&#20960;&#20046;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FusionAD&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#20004;&#20010;&#26368;&#20851;&#38190;&#20256;&#24863;&#22120;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#36229;&#36234;&#24863;&#30693;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;&#22522;&#20110;&#34701;&#21512;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#19982;&#22522;&#20110;&#30456;&#26426;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;UniAD&#30456;&#27604;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#34701;&#21512;&#36741;&#21161;&#30340;&#27169;&#24577;&#24863;&#30693;&#39044;&#27979;&#21644;&#29366;&#24577;&#24863;&#30693;&#35268;&#21010;&#27169;&#22359;&#65292;&#31216;&#20026;FMSPnP&#65292;&#20805;&#20998;&#21033;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#30340;nuScenes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;FusionAD&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#22522;&#20934;&#32447;&#24179;&#22343;15%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a multi-modality multi-task neural network toward accurate and robust performance is a de-facto standard in perception task of autonomous driving. However, leveraging such data from multiple sensors to jointly optimize the prediction and planning tasks remains largely unexplored. In this paper, we present FusionAD, to the best of our knowledge, the first unified framework that fuse the information from two most critical sensors, camera and LiDAR, goes beyond perception task. Concretely, we first build a transformer based multi-modality fusion network to effectively produce fusion based features. In constrast to camera-based end-to-end method UniAD, we then establish a fusion aided modality-aware prediction and status-aware planning modules, dubbed FMSPnP that take advantages of multi-modality features. We conduct extensive experiments on commonly used benchmark nuScenes dataset, our FusionAD achieves state-of-the-art performance and surpassing baselines on average 15% on perce
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38548;&#31163;&#21644;&#35825;&#23548;&#65288;InI&#65289;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#25239;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#38548;&#31163;&#23545;&#25163;&#30340;&#35757;&#32451;&#26799;&#24230;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#20013;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#39640;&#21644;&#20934;&#30830;&#24615;&#19982;&#38450;&#31363;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#19981;&#21033;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.00958</link><description>&lt;p&gt;
&#38548;&#31163;&#21644;&#35825;&#23548;&#65306;&#38024;&#23545;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks. (arXiv:2308.00958v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00958
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38548;&#31163;&#21644;&#35825;&#23548;&#65288;InI&#65289;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#25239;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#38548;&#31163;&#23545;&#25163;&#30340;&#35757;&#32451;&#26799;&#24230;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#20013;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#39640;&#21644;&#20934;&#30830;&#24615;&#19982;&#38450;&#31363;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#19981;&#21033;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#40657;&#30418;&#26597;&#35810;&#36807;&#31243;&#22797;&#21046;&#27169;&#22411;&#21151;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#30446;&#26631;&#21463;&#23475;&#27169;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#31363;&#21462;&#38450;&#24481;&#26041;&#27861;&#36890;&#36807;&#21521;&#21463;&#23475;&#32773;&#30340;&#21518;&#39564;&#27010;&#29575;&#28155;&#21152;&#27450;&#39575;&#24615;&#25200;&#21160;&#26469;&#35823;&#23548;&#25915;&#20987;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#29616;&#22312;&#38754;&#20020;&#30528;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#39640;&#21644;&#33391;&#22909;&#20934;&#30830;&#24615;&#19982;&#38450;&#31363;&#40065;&#26834;&#24615;&#20043;&#38388;&#19981;&#21033;&#26435;&#34913;&#30340;&#38382;&#39064;&#65292;&#36825;&#25361;&#25112;&#20102;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#27169;&#22411;&#31363;&#21462;&#38450;&#24481;&#35757;&#32451;&#26694;&#26550;Isolation and Induction&#65288;InI&#65289;&#12290;InI&#19981;&#20687;&#37096;&#32626;&#36741;&#21161;&#38450;&#24481;&#27169;&#22359;&#37027;&#26679;&#24341;&#20837;&#20887;&#20313;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#26159;&#36890;&#36807;&#23558;&#23545;&#25163;&#30340;&#35757;&#32451;&#26799;&#24230;&#19982;&#39044;&#26399;&#26799;&#24230;&#38548;&#31163;&#26469;&#30452;&#25509;&#35757;&#32451;&#38450;&#24481;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the broad application of Machine Learning models as a Service (MLaaS), they are vulnerable to model stealing attacks. These attacks can replicate the model functionality by using the black-box query process without any prior knowledge of the target victim model. Existing stealing defenses add deceptive perturbations to the victim's posterior probabilities to mislead the attackers. However, these defenses are now suffering problems of high inference computational overheads and unfavorable trade-offs between benign accuracy and stealing robustness, which challenges the feasibility of deployed models in practice. To address the problems, this paper proposes Isolation and Induction (InI), a novel and effective training framework for model stealing defenses. Instead of deploying auxiliary defense modules that introduce redundant inference time, InI directly trains a defensive model by isolating the adversary's training gradient from the expected gradient, which can effectively reduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16680</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26223;&#35266;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39046;&#20808;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20063;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25991;&#29486;&#65292;&#20294;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#28041;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#26032;&#20852;&#23041;&#32961;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36131;&#20219;&#36825;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#24352;&#35814;&#23613;&#30340;&#22320;&#22270;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20123;&#21162;&#21147;&#23545;&#20110;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#39564;&#22686;&#24378;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20223;&#30495;&#25968;&#25454;&#21644;&#23454;&#39564;&#25968;&#25454;&#26469;&#20943;&#36731;&#27169;&#22411;&#24046;&#24322;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16173</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#39564;&#22686;&#24378;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#29992;&#20110;&#21452;&#26377;&#28304;&#26725;&#21464;&#25442;&#22120;&#30340;&#35843;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Modeling with Experimental Augmentation for the Modulation Strategy of the Dual-Active-Bridge Converter. (arXiv:2307.16173v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#39564;&#22686;&#24378;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20223;&#30495;&#25968;&#25454;&#21644;&#23454;&#39564;&#25968;&#25454;&#26469;&#20943;&#36731;&#27169;&#22411;&#24046;&#24322;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30005;&#21147;&#36716;&#25442;&#22120;&#30340;&#24615;&#33021;&#24314;&#27169;&#65292;&#20027;&#27969;&#26041;&#27861;&#22522;&#26412;&#19978;&#26159;&#22522;&#20110;&#30693;&#35782;&#30340;&#65292;&#30001;&#20110;&#20154;&#21147;&#36127;&#25285;&#37325;&#21644;&#24314;&#27169;&#31934;&#24230;&#20302;&#32780;&#21463;&#22256;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#36890;&#36807;&#20174;&#20223;&#30495;&#25968;&#25454;&#20013;&#33258;&#21160;&#24314;&#27169;&#26497;&#22823;&#22320;&#20943;&#36731;&#20102;&#23545;&#20154;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26410;&#24314;&#27169;&#30340;&#23492;&#29983;&#20803;&#20214;&#12289;&#19981;&#36275;&#30340;&#28909;&#30913;&#27169;&#22411;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#29615;&#22659;&#26465;&#20214;&#31561;&#21407;&#22240;&#65292;&#27169;&#22411;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#21457;&#29983;&#12290;&#36825;&#20123;&#20165;&#22522;&#20110;&#20223;&#30495;&#30340;&#19981;&#20934;&#30830;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26080;&#27861;&#20195;&#34920;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#24615;&#33021;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#30005;&#21147;&#36716;&#25442;&#22120;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#20943;&#36731;&#27169;&#22411;&#24046;&#24322;&#24182;&#22312;&#23454;&#36341;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#39564;&#22686;&#24378;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;(D2EA)&#65292;&#32467;&#21512;&#20102;&#20223;&#30495;&#25968;&#25454;&#21644;&#23454;&#39564;&#25968;&#25454;&#12290;&#22312;D2EA&#20013;&#65292;&#20223;&#30495;&#25968;&#25454;&#26088;&#22312;&#24314;&#31435;&#22522;&#26412;&#30340;&#21151;&#33021;&#26223;&#35266;&#65292;&#23454;&#39564;&#25968;&#25454;&#21017;&#19987;&#27880;&#20110;&#21305;&#37197;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;&#23454;&#20363;&#21270;&#20102;D2EA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
For the performance modeling of power converters, the mainstream approaches are essentially knowledge-based, suffering from heavy manpower burden and low modeling accuracy. Recent emerging data-driven techniques greatly relieve human reliance by automatic modeling from simulation data. However, model discrepancy may occur due to unmodeled parasitics, deficient thermal and magnetic models, unpredictable ambient conditions, etc. These inaccurate data-driven models based on pure simulation cannot represent the practical performance in physical world, hindering their applications in power converter modeling. To alleviate model discrepancy and improve accuracy in practice, this paper proposes a novel data-driven modeling with experimental augmentation (D2EA), leveraging both simulation data and experimental data. In D2EA, simulation data aims to establish basic functional landscape, and experimental data focuses on matching actual performance in real world. The D2EA approach is instantiated
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.16149</link><description>&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#30340;LSTM-DDPM&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16149
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#65288;ETD&#65289;&#21644;&#33021;&#37327;&#28040;&#32791;&#39044;&#27979;&#65288;ECF&#65289;&#26159;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#20849;&#21516;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#31995;&#32479;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;ETD&#21644;ECF&#30340;&#30456;&#20114;&#20851;&#32852;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#36755;&#20837;&#37325;&#26500;&#21644;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#21644;&#39044;&#27979;&#35823;&#24046;&#30340;&#26041;&#27861;&#30456;&#20114;&#34917;&#20805;&#65292;&#21487;&#20197;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;ETD&#21644;ECF&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#38598;&#25104;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;ETD&#24615;&#33021;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21040;&#22522;&#20934;&#26041;&#27861;&#26410;&#33021;&#26816;&#27979;&#21040;&#30340;&#33021;&#37327;&#30423;&#31363;&#25915;&#20987;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;ETD&#21644;ECF&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30896;&#25758;&#33258;&#30001;&#31354;&#38388;&#20013;&#24314;&#27169;&#20809;&#27969;&#30340;&#31574;&#30053;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#19977;&#32500;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#20809;&#27969;&#30340;&#26126;&#30830;&#34920;&#31034;&#21644;&#20809;&#27969;&#20998;&#37327;&#19982;&#22402;&#30452;&#22352;&#26631;&#20043;&#38388;&#30340;&#20108;&#27425;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.15989</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#30340;&#33258;&#30001;&#31354;&#38388;&#20809;&#27969;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Freespace Optical Flow Modeling for Automated Driving. (arXiv:2307.15989v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30896;&#25758;&#33258;&#30001;&#31354;&#38388;&#20013;&#24314;&#27169;&#20809;&#27969;&#30340;&#31574;&#30053;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#19977;&#32500;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#20809;&#27969;&#30340;&#26126;&#30830;&#34920;&#31034;&#21644;&#20809;&#27969;&#20998;&#37327;&#19982;&#22402;&#30452;&#22352;&#26631;&#20043;&#38388;&#30340;&#20108;&#27425;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#27969;&#21644;&#35270;&#24046;&#26159;&#33258;&#20027;&#39550;&#39542;&#24863;&#30693;&#20013;&#20004;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#23427;&#20204;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#22914;&#38556;&#30861;&#29289;&#21644;&#36710;&#36947;&#26816;&#27979;&#12290;"U-V-Disparity"&#30340;&#27010;&#24565;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#35752;&#65292;&#32780;&#20809;&#27969;&#30340;&#23545;&#24212;&#27010;&#24565;&#21364;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#36816;&#21160;&#20998;&#26512;&#31639;&#27861;&#36890;&#36807;&#21305;&#37197;&#20004;&#20010;&#36830;&#32493;&#35270;&#39057;&#24103;&#20043;&#38388;&#30340;&#23545;&#24212;&#28857;&#26469;&#20272;&#35745;&#20809;&#27969;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29615;&#22659;&#20449;&#24687;&#21644;&#20960;&#20309;&#32422;&#26463;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#22312;&#26234;&#33021;&#36710;&#36742;&#30340;&#30896;&#25758;&#33258;&#30001;&#31354;&#38388;&#65288;&#20063;&#31216;&#20026;&#21487;&#36890;&#34892;&#21306;&#22495;&#25110;&#31616;&#31216;&#20026;&#33258;&#30001;&#31354;&#38388;&#65289;&#20013;&#24314;&#27169;&#20809;&#27969;&#65292;&#20805;&#20998;&#21033;&#29992;&#19977;&#32500;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20809;&#27969;&#30340;&#26126;&#30830;&#34920;&#31034;&#65292;&#24182;&#25512;&#23548;&#20986;&#20809;&#27969;&#20998;&#37327;&#19982;&#22402;&#30452;&#22352;&#26631;&#20043;&#38388;&#30340;&#20108;&#27425;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical flow and disparity are two informative visual features for autonomous driving perception. They have been used for a variety of applications, such as obstacle and lane detection. The concept of "U-V-Disparity" has been widely explored in the literature, while its counterpart in optical flow has received relatively little attention. Traditional motion analysis algorithms estimate optical flow by matching correspondences between two successive video frames, which limits the full utilization of environmental information and geometric constraints. Therefore, we propose a novel strategy to model optical flow in the collision-free space (also referred to as drivable area or simply freespace) for intelligent vehicles, with the full utilization of geometry information in a 3D driving environment. We provide explicit representations of optical flow and deduce the quadratic relationship between the optical flow component and the vertical coordinate. Through extensive experiments on severa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.11363</link><description>&lt;p&gt;
&#21463;&#36974;&#34109;&#25193;&#25955;&#27169;&#22411;&#26159;&#24555;&#36895;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11363
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#30340;&#20107;&#23454;&#19978;&#25216;&#26415;&#65292;&#28982;&#32780;&#23427;&#20204;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#35813;&#25216;&#26415;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#32435;&#20837;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#20419;&#36827;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#36890;&#36807;&#36974;&#34109;&#36755;&#20837;&#22270;&#20687;&#30340;&#39640;&#27604;&#20363;&#65288;&#20363;&#22914;&#39640;&#36798;90&#65285;&#65289;&#65292;&#24182;&#21033;&#29992;&#36974;&#34109;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#26469;&#21435;&#22122;&#21487;&#35265;&#21306;&#22495;&#65292;&#20174;&#32780;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#26356;&#26174;&#33879;&#30340;&#29305;&#24449;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#36974;&#34109;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;CelebA-HQ $256 \times 256$&#20687;&#32032;&#31354;&#38388;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;&#20102;&#22522;&#20110;ViT&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;4&#20493;&#21152;&#36895;&#65292;&#24182;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#19982;&#21435;&#22122;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as the \emph{de-facto} technique for image generation, yet they entail significant computational overhead, hindering the technique's broader application in the research community. We propose a prior-based denoising training framework, the first to incorporate the pre-train and fine-tune paradigm into the diffusion model training process, which substantially improves training efficiency and shows potential in facilitating various downstream tasks. Our approach centers on masking a high proportion (e.g., up to 90\%) of the input image and employing masked denoising score matching to denoise the visible areas, thereby guiding the diffusion model to learn more salient features from training data as prior knowledge. By utilizing masked learning in a pre-training stage, we efficiently train the ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space, achieving a 4x acceleration and enhancing the quality of generated images compared to denoisin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#23545;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#23646;&#24615;&#24402;&#32435;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-3.5&#26377;&#19968;&#20123;&#22256;&#38590;&#65292;&#20294;GPT-4&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30456;&#20284;&#65292;&#38500;&#20102;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20316;&#26410;&#26469;&#30740;&#31350;&#22522;&#20934;&#30340;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06548</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24402;&#32435;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Inductive reasoning in humans and large language models. (arXiv:2306.06548v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#23545;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#23646;&#24615;&#24402;&#32435;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-3.5&#26377;&#19968;&#20123;&#22256;&#38590;&#65292;&#20294;GPT-4&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30456;&#20284;&#65292;&#38500;&#20102;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20316;&#26410;&#26469;&#30740;&#31350;&#22522;&#20934;&#30340;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#26159;&#21542;&#33021;&#20316;&#20026;&#26222;&#36890;&#26234;&#33021;&#30340;&#27169;&#22411;&#25110;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#30340;&#31243;&#24230;&#30340;&#30097;&#38382;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;GPT-3.5&#21644;GPT-4&#24212;&#29992;&#20110;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#19968;&#20010;&#32463;&#20856;&#38382;&#39064;&#65292;&#21363;&#23646;&#24615;&#24402;&#32435;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;&#20154;&#31867;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#30340;&#23646;&#24615;&#24402;&#32435;&#20219;&#21153;&#19978;&#30340;&#21028;&#26029;&#12290;&#23613;&#31649;GPT-3.5&#22312;&#25429;&#25417;&#20154;&#31867;&#34892;&#20026;&#30340;&#35768;&#22810;&#26041;&#38754;&#19978;&#26377;&#22256;&#38590;&#65292;&#20294;GPT-4&#26356;&#21152;&#25104;&#21151;&#65306;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#65292;&#23427;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#22312;&#36136;&#19978;&#30456;&#21305;&#37197;&#65292;&#21807;&#19968;&#26174;&#33879;&#30340;&#20363;&#22806;&#26159;&#20854;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#23646;&#24615;&#24402;&#32435;&#21487;&#20197;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#36827;&#34892;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive recent performance of large language models has led many to wonder to what extent they can serve as models of general intelligence or are similar to human cognition. We address this issue by applying GPT-3.5 and GPT-4 to a classic problem in human inductive reasoning known as property induction. Over two experiments, we elicit human judgments on a range of property induction tasks spanning multiple domains. Although GPT-3.5 struggles to capture many aspects of human behaviour, GPT-4 is much more successful: for the most part, its performance qualitatively matches that of humans, and the only notable exception is its failure to capture the phenomenon of premise non-monotonicity. Our work demonstrates that property induction allows for interesting comparisons between human and machine intelligence and provides two large datasets that can serve as benchmarks for future work in this vein.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#33258;&#21160;&#22320;&#21457;&#29616;&#19981;&#21516;&#30340;&#29983;&#25104;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#20123;&#29983;&#25104;&#27010;&#24565;&#21487;&#20197;&#34987;&#29992;&#20110;&#37325;&#26032;&#32452;&#21512;&#21644;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#20316;&#20026;&#19968;&#31181;&#34920;&#31034;&#29992;&#20110;&#19979;&#28216;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.05357</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#19979;&#30340;&#32452;&#21512;&#24335;&#27010;&#24565;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#33258;&#21160;&#22320;&#21457;&#29616;&#19981;&#21516;&#30340;&#29983;&#25104;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#20123;&#29983;&#25104;&#27010;&#24565;&#21487;&#20197;&#34987;&#29992;&#20110;&#37325;&#26032;&#32452;&#21512;&#21644;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#20316;&#20026;&#19968;&#31181;&#34920;&#31034;&#29992;&#20110;&#19979;&#28216;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20351;&#24471;&#22312;&#19981;&#21516;&#39046;&#22495;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21512;&#25104;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#38656;&#35201;&#29992;&#25143;&#25351;&#23450;&#20182;&#20204;&#24819;&#35201;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#30456;&#21453;&#30340;&#38382;&#39064;&#8212;&#8212;&#22312;&#32473;&#20986;&#30340;&#19981;&#21516;&#22270;&#20687;&#38598;&#21512;&#20013;&#65292;&#25105;&#20204;&#33021;&#21542;&#21457;&#29616;&#20195;&#34920;&#27599;&#20010;&#22270;&#20687;&#30340;&#29983;&#25104;&#27010;&#24565;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20174;&#19968;&#32452;&#22270;&#20687;&#20013;&#21457;&#29616;&#29983;&#25104;&#30340;&#27010;&#24565;&#65292;&#23558;&#32472;&#30011;&#20013;&#19981;&#21516;&#30340;&#33402;&#26415;&#39118;&#26684;&#65292;&#23545;&#35937;&#21644;&#29031;&#26126;&#20174;&#21416;&#25151;&#22330;&#26223;&#20013;&#20998;&#35299;&#20986;&#26469;&#65292;&#24182;&#36890;&#36807;&#32473;&#23450;&#30340;ImageNet&#22270;&#20687;&#21457;&#29616;&#22270;&#20687;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#29983;&#25104;&#27010;&#24565;&#33021;&#22815;&#20934;&#30830;&#22320;&#34920;&#31034;&#22270;&#20687;&#30340;&#20869;&#23481;&#65292;&#33021;&#22815;&#37325;&#26032;&#32452;&#21512;&#21644;&#32452;&#25104;&#20197;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#31181;&#34920;&#31034;&#26469;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem -- given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32858;&#31867;&#31639;&#27861;&#26816;&#27979;&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65288;NFT&#65289;&#20132;&#26131;&#24066;&#22330;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#30417;&#31649;&#23545;&#20943;&#23569;&#27450;&#35784;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04643</link><description>&lt;p&gt;
NFT&#24066;&#22330;&#20013;&#30340;&#24322;&#24120;&#20132;&#26131;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Abnormal Trading Detection in the NFT Market. (arXiv:2306.04643v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32858;&#31867;&#31639;&#27861;&#26816;&#27979;&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65288;NFT&#65289;&#20132;&#26131;&#24066;&#22330;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#30417;&#31649;&#23545;&#20943;&#23569;&#27450;&#35784;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65288;NFT&#65289;&#24066;&#22330;&#36817;&#24180;&#26469;&#21576;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#25454;DappRadar&#32479;&#35745;&#65292;&#26368;&#22823;&#30340;NFT&#24066;&#22330;OpenSea&#30340;&#24635;&#20132;&#26131;&#39069;&#22312;2023&#24180;2&#26376;&#36798;&#21040;&#20102;347&#20159;&#32654;&#20803;&#12290;&#28982;&#32780;&#65292;NFT&#24066;&#22330;&#22823;&#37096;&#20998;&#26159;&#26410;&#21463;&#30417;&#31649;&#30340;&#65292;&#23384;&#22312;&#30528;&#37325;&#22823;&#30340;&#27927;&#38065;&#12289;&#27450;&#35784;&#21644;&#34394;&#20551;&#20132;&#26131;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#25581;&#31034;&#24120;&#35265;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#22914;&#34394;&#25311;&#20132;&#26131;&#65292;&#36825;&#21487;&#33021;&#20250;&#35823;&#23548;&#20854;&#20182;&#20132;&#26131;&#32773;&#12290;&#25105;&#20204;&#20351;&#29992;&#24066;&#22330;&#25968;&#25454;&#20174;&#32593;&#32476;&#12289;&#36135;&#24065;&#21644;&#26102;&#38388;&#30340;&#35282;&#24230;&#35774;&#35745;&#37327;&#21270;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22522;&#20110;K&#22343;&#20540;&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20197;&#23545;&#20132;&#26131;&#32773;&#36827;&#34892;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#32858;&#31867;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#30417;&#31649;&#26469;&#20943;&#23569;&#19981;&#33391;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#33021;&#26377;&#21161;&#20110;&#37325;&#26032;&#24314;&#31435;&#20132;&#26131;&#32773;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Non-Fungible-Token (NFT) market has experienced explosive growth in recent years. According to DappRadar, the total transaction volume on OpenSea, the largest NFT marketplace, reached 34.7 billion dollars in February 2023. However, the NFT market is mostly unregulated and there are significant concerns about money laundering, fraud and wash trading. Amateur traders and retail investors comprise a significant fraction of the NFT market. Hence it is important that researchers highlight the relevant risks involved in NFT trading. In this paper, we attempt to uncover common fraudulent behaviors such as wash trading that could mislead other traders. Using market data, we design quantitative features from the network, monetary, and temporal perspectives that are fed into K-means clustering unsupervised learning algorithm to sort traders into groups. Lastly, we discuss the clustering results' significance and how regulations can reduce undesired behaviors. Our work can potentially help re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#28508;&#22312;&#22270;&#21040;&#28508;&#22312;&#25299;&#25169;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#24494;&#20998;&#30340;&#21333;&#22797;&#24418;&#27169;&#22359;&#65288;DCM&#65289;&#65292;&#23398;&#20064;&#25551;&#36848;&#25968;&#25454;&#28857;&#20043;&#38388;&#22810;&#21521;&#20132;&#20114;&#30340;&#39640;&#38454;&#21333;&#22797;&#24418;&#30340;&#31232;&#30095;&#19988;&#19981;&#35268;&#21017;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#21333;&#22797;&#24418;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#23618;&#38598;&#25104;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16174</link><description>&lt;p&gt;
&#20174;&#28508;&#22312;&#22270;&#21040;&#28508;&#22312;&#25299;&#25169;&#25512;&#26029;&#65306;&#21487;&#24494;&#20998;&#30340;&#21333;&#22797;&#24418;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module. (arXiv:2305.16174v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16174
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#28508;&#22312;&#22270;&#21040;&#28508;&#22312;&#25299;&#25169;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#24494;&#20998;&#30340;&#21333;&#22797;&#24418;&#27169;&#22359;&#65288;DCM&#65289;&#65292;&#23398;&#20064;&#25551;&#36848;&#25968;&#25454;&#28857;&#20043;&#38388;&#22810;&#21521;&#20132;&#20114;&#30340;&#39640;&#38454;&#21333;&#22797;&#24418;&#30340;&#31232;&#30095;&#19988;&#19981;&#35268;&#21017;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#21333;&#22797;&#24418;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#23618;&#38598;&#25104;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#22270;&#25512;&#26029;&#65288;LGI&#65289;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#26469;&#20943;&#23569;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23545;&#32473;&#23450;&#22270;&#25299;&#25169;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;LGI&#26041;&#27861;&#20551;&#35774;&#23384;&#22312;&#65288;&#22122;&#22768;&#12289;&#19981;&#23436;&#25972;&#12289;&#21487;&#25913;&#36827;&#30340;...&#65289;&#36755;&#20837;&#22270;&#26469;&#37325;&#26032;&#36830;&#25509;&#65292;&#24182;&#19988;&#21482;&#33021;&#23398;&#20064;&#24120;&#35268;&#30340;&#22270;&#25299;&#25169;&#12290;&#22312;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#65288;TDL&#65289;&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#23398;&#20064;&#25551;&#36848;&#25968;&#25454;&#28857;&#20043;&#38388;&#22810;&#21521;&#20132;&#20114;&#30340;&#39640;&#38454;&#21333;&#22797;&#24418;&#65288;&#20855;&#26377;&#31232;&#30095;&#19988;&#19981;&#35268;&#21017;&#30340;&#25299;&#25169;&#32467;&#26500;&#65289;&#30340;&#28508;&#22312;&#25299;&#25169;&#25512;&#26029;&#65288;LTI&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#24494;&#20998;&#30340;&#21333;&#22797;&#24418;&#27169;&#22359;&#65288;DCM&#65289;&#65292;&#19968;&#31181;&#35745;&#31639;&#22797;&#26434;&#20013;&#21333;&#20803;&#27010;&#29575;&#30340;&#26032;&#22411;&#21487;&#23398;&#20064;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;DCM&#19982;&#21333;&#22797;&#24418;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#23618;&#38598;&#25104;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#20004;&#27493;&#25512;&#26029;&#36807;&#31243;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#23545;&#36755;&#20837;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#21333;&#20803;&#36827;&#34892;&#35814;&#23613;&#25628;&#32034;&#65292;&#20174;&#32780;&#20445;&#25345;&#21487;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#39640;&#38454;&#25299;&#25169;&#25512;&#26029;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks (GNNs) on a given graph topology by dynamically learning it. However, most of LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake of the success of Topological Deep Learning (TDL), we study Latent Topology Inference (LTI) for learning higher-order cell complexes (with sparse and not regular topology) describing multi-way interactions between data points. To this aim, we introduce the Differentiable Cell Complex Module (DCM), a novel learnable function that computes cell probabilities in the complex to improve the downstream task. We show how to integrate DCM with cell complex message passing networks layers and train it in a end-to-end fashion, thanks to a two-step inference procedure that avoids an exhaustive search across all possible cells in the input, thus maintaining scalability. Our model is tested on several ho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LaDI-VTON&#65292;&#19968;&#31181;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#25991;&#26412;&#21453;&#28436;&#32452;&#20214;&#22686;&#24378;&#34394;&#25311;&#35797;&#31359;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#36136;&#37327;&#36824;&#21407;&#21830;&#24215;&#26381;&#35013;&#30340;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2305.13501</link><description>&lt;p&gt;
LaDI-VTON: &#28508;&#22312;&#25193;&#25955;&#25991;&#26412;&#21453;&#28436;&#22686;&#24378;&#34394;&#25311;&#35797;&#31359;
&lt;/p&gt;
&lt;p&gt;
LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On. (arXiv:2305.13501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LaDI-VTON&#65292;&#19968;&#31181;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#25991;&#26412;&#21453;&#28436;&#32452;&#20214;&#22686;&#24378;&#34394;&#25311;&#35797;&#31359;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#36136;&#37327;&#36824;&#21407;&#21830;&#24215;&#26381;&#35013;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#21644;&#20803;&#23431;&#23449;&#36825;&#20004;&#20010;&#19981;&#26029;&#21457;&#23637;&#21464;&#21270;&#30340;&#39046;&#22495;&#32487;&#32493;&#23547;&#27714;&#21019;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#28040;&#36153;&#32773;&#20307;&#39564;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#32593;&#32476;&#33021;&#22815;&#21019;&#36896;&#20986;&#38750;&#24120;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#34394;&#25311;&#35797;&#31359;&#65292;&#23601;&#26159;&#29983;&#25104;&#19968;&#20010;&#30446;&#26631;&#27169;&#22411;&#31359;&#30528;&#21830;&#24215;&#20013;&#30340;&#26576;&#20214;&#26381;&#35013;&#30340;&#26032;&#22270;&#29255;&#65292;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;LaDI-VTON&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#25991;&#26412;&#21453;&#28436;&#22686;&#24378;&#30340;&#34394;&#25311;&#35797;&#31359;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20381;&#36182;&#20110;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#19968;&#20010;&#26032;&#30340;&#38468;&#21152;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36339;&#36807;&#36830;&#25509;&#26469;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#65292;&#20445;&#25345;&#27169;&#22411;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20445;&#25345;&#21830;&#24215;&#26381;&#35013;&#30340;&#36136;&#22320;&#21644;&#32454;&#33410;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#21453;&#28436;&#32452;&#20214;&#65292;&#21487;&#20197;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;&#28508;&#22312;&#25193;&#25955;&#31354;&#38388;&#65292;&#23454;&#29616;&#26381;&#35013;&#30340;&#39640;&#36136;&#37327;&#32454;&#33410;&#36824;&#21407;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly evolving fields of e-commerce and metaverse continue to seek innovative approaches to enhance the consumer experience. At the same time, recent advancements in the development of diffusion models have enabled generative networks to create remarkably realistic images. In this context, image-based virtual try-on, which consists in generating a novel image of a target model wearing a given in-shop garment, has yet to capitalize on the potential of these powerful generative solutions. This work introduces LaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for the Virtual Try-ON task. The proposed architecture relies on a latent diffusion model extended with a novel additional autoencoder module that exploits learnable skip connections to enhance the generation process preserving the model's characteristics. To effectively maintain the texture and details of the in-shop garment, we propose a textual inversion component that can map the visual features of the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.10406</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Classification. (arXiv:2305.10406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26032;&#22411;&#25193;&#23637;&#65292;&#31216;&#20026;&#21464;&#20998;&#20998;&#31867; (VC)&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#65292;&#31867;&#20284;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#35777;&#25454;&#19979;&#30028; (ELBO) &#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;VC&#27169;&#22411;&#20801;&#35768;&#22312;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#29305;&#21035;&#26159;&#31867;&#26465;&#20214;&#28508;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#22312;&#29616;&#25104;&#30340;softmax&#20998;&#31867;&#22120;&#20013;&#20570;&#20986;&#30340;&#38544;&#24335;&#20551;&#35774;&#12290;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25913;&#21892;&#20102;&#20854;&#20182;&#33391;&#22909;&#29305;&#24615;&#65292;&#22914;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#22495;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;Mlinear&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#33410;&#36890;&#36947;&#29420;&#31435;&#24615;&#21644;&#36890;&#36947;&#20381;&#36182;&#24615;&#23646;&#24615;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04800</link><description>&lt;p&gt;
Mlinear:&#37325;&#26032;&#24605;&#32771;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mlinear: Rethink the Linear Model for Time-series Forecasting. (arXiv:2305.04800v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04800
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;Mlinear&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#33410;&#36890;&#36947;&#29420;&#31435;&#24615;&#21644;&#36890;&#36947;&#20381;&#36182;&#24615;&#23646;&#24615;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36234;&#26469;&#36234;&#20851;&#27880;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24615;&#36136;&#65292;&#20363;&#22914;&#36890;&#36947;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#21644;&#36890;&#36947;&#20381;&#36182;&#24615;&#65288;CD&#65289;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#35774;&#35745;&#22797;&#26434;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#29420;&#30340;CI&#25110;CD&#19978;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#36825;&#20004;&#20010;&#30456;&#21453;&#30340;&#23646;&#24615;&#20197;&#23454;&#29616;&#21327;&#21516;&#25928;&#24212;&#30340;&#25361;&#25112;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;CI&#21644;CD&#30340;&#30456;&#21453;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36804;&#20170;&#26410;&#33021;&#26377;&#25928;&#22238;&#31572;&#30340;&#23454;&#38469;&#38382;&#39064;&#65292;&#21363;&#8220;&#22914;&#20309;&#26377;&#25928;&#22320;&#28151;&#21512;&#26102;&#38388;&#24207;&#21015;&#30340;CI&#21644;CD&#23646;&#24615;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mlinear&#65288;MIX-Linear&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#32447;&#24615;&#23618;&#30340;&#26041;&#27861;&#12290;Mlinear&#30340;&#35774;&#35745;&#29702;&#24565;&#20027;&#35201;&#21253;&#25324;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#22522;&#20110;&#21160;&#24577;&#35843;&#33410;CI&#21644;CD&#23646;&#24615;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recently, significant advancements have been made in time-series forecasting research, with an increasing focus on analyzing the nature of time-series data, e.g, channel-independence (CI) and channel-dependence (CD), rather than solely focusing on designing sophisticated forecasting models. However, current research has primarily focused on either CI or CD in isolation, and the challenge of effectively combining these two opposing properties to achieve a synergistic effect remains an unresolved issue. In this paper, we carefully examine the opposing properties of CI and CD, and raise a practical question that has not been effectively answered, e.g.,"How to effectively mix the CI and CD properties of time series to achieve better predictive performance?" To answer this question, we propose Mlinear (MIX-Linear), a simple yet effective method based mainly on linear layers. The design philosophy of Mlinear mainly includes two aspects:(1) dynamically tuning the CI and CD properties based on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#22312;&#20845;&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#21487;&#22312;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;GPT-4&#27169;&#22411;&#26102;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02897</link><description>&lt;p&gt;
&#33258;&#21160;&#21457;&#29616;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
An automatically discovered chain-of-thought prompt generalizes to novel models and datasets. (arXiv:2305.02897v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#22312;&#20845;&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#21487;&#22312;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;GPT-4&#27169;&#22411;&#26102;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#33021;&#21147;&#26377;&#26395;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20808;&#21069;&#27169;&#22411;&#25152;&#21046;&#23450;&#30340;&#25552;&#31034;&#31574;&#30053;&#22914;&#20309;&#36866;&#29992;&#20110;&#26032;&#27169;&#22411;&#21644;&#19981;&#21516;&#25968;&#25454;&#38598;&#20173;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#23567;&#22411;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#65288;zero-shot prompts&#65289;&#30340;&#24615;&#33021;&#65292;&#20197;&#35825;&#23548;CoT&#25512;&#29702;&#65292;&#22312;6&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;LLM&#65288;davinci-002&#65292;davinci-003&#65292;GPT-3.5-turbo&#65292;GPT-4&#65292;Flan-T5-xxl&#21644;Cohere command-xlarge&#65289;&#19978;&#19982;&#21253;&#25324;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#30340;&#20845;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#28151;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#22312;&#23454;&#39564;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#26102;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergent chain-of-thought (CoT) reasoning capabilities promise to improve performance and explainability of large language models (LLMs). However, uncertainties remain about how prompting strategies formulated for previous model generations generalize to new model generations and different datasets. In this small-scale study we compare the performance of a range of zero-shot prompts for inducing CoT reasoning across six recently released LLMs (davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a mixture of six question-answering datasets, including datasets from scientific and medical domains. We find that a CoT prompt that was previously discovered through automated prompt discovery shows robust performance across experimental conditions and produces best results when applied to the state-of-the-art model GPT-4.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#23454;&#39564;&#35777;&#26126;&#25237;&#24433;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12289;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#21644;&#36719;&#26368;&#22823;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23481;&#37327;&#24046;&#24322;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21450;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11098</link><description>&lt;p&gt;
&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A closer look at the training dynamics of knowledge distillation. (arXiv:2303.11098v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#23454;&#39564;&#35777;&#26126;&#25237;&#24433;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12289;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#21644;&#36719;&#26368;&#22823;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23481;&#37327;&#24046;&#24322;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21450;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#23558;&#30693;&#35782;&#33976;&#39311;&#20316;&#20026;&#20989;&#25968;&#21305;&#37197;&#21644;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#39564;&#35777;&#19977;&#20010;&#37325;&#35201;&#35774;&#35745;&#20915;&#31574;&#65292;&#21363;&#26631;&#20934;&#21270;&#12289;&#36719;&#26368;&#22823;&#20989;&#25968;&#21644;&#25237;&#24433;&#23618;&#20316;&#20026;&#20851;&#38190;&#35201;&#32032;&#65292;&#25105;&#20204;&#26377;&#29702;&#35770;&#22320;&#26174;&#31034;&#20986;&#25237;&#24433;&#22120;&#38544;&#21547;&#22320;&#32534;&#30721;&#20102;&#20851;&#20110;&#36807;&#21435;&#26679;&#26412;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#23398;&#29983;&#25552;&#20379;&#20102;&#20851;&#32852;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#19982;&#25237;&#24433;&#22120;&#30340;&#35757;&#32451;&#21160;&#24577;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#21487;&#33021;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36719;&#26368;&#22823;&#20989;&#25968;&#21487;&#20197;&#29992;&#26469;&#35299;&#20915;&#20219;&#20309;&#26174;&#33879;&#23481;&#37327;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#25110;&#20248;&#20110;&#20854;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classificati
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23384;&#27963;&#30340;&#29983;&#29289;&#21487;&#20197;&#23398;&#20064;&#21040;&#20248;&#20110; L\'evy walks &#30340;&#35269;&#39135;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#35269;&#39135;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06050</link><description>&lt;p&gt;
&#26368;&#20248;&#35269;&#39135;&#31574;&#30053;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#24182;&#19988;&#20248;&#20110; L\'evy walks&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal foraging strategies can be learned and outperform L\'evy walks. (arXiv:2303.06050v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06050
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23384;&#27963;&#30340;&#29983;&#29289;&#21487;&#20197;&#23398;&#20064;&#21040;&#20248;&#20110; L\'evy walks &#30340;&#35269;&#39135;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#35269;&#39135;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
L\'evy walks &#21644;&#20854;&#20182;&#20105;&#35758;&#35770;&#30340;&#26368;&#20248;&#35269;&#39135;&#27169;&#22411;&#25104;&#21151;&#22320;&#34987;&#29992;&#20110;&#25551;&#36848;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#65292;&#21560;&#24341;&#20102;&#32463;&#27982;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#29983;&#24577;&#23398;&#12289;&#36827;&#21270;&#29983;&#29289;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20173;&#19981;&#28165;&#26970;&#21738;&#20123;&#31574;&#30053;&#21487;&#20197;&#26368;&#22823;&#21270;&#35269;&#39135;&#25928;&#29575;&#65292;&#36825;&#20123;&#31574;&#30053;&#26159;&#21542;&#21487;&#20197;&#34987;&#29983;&#29289;&#23398;&#20064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#35269;&#39135;&#32773;&#24314;&#27169;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#31561;&#21516;&#20110;&#20248;&#21270;&#35269;&#39135;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#23398;&#20064;&#20102;&#20248;&#20110;&#24050;&#30693;&#31574;&#30053;&#22914; L\'evy walks &#30340;&#35269;&#39135;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
L\'evy walks and other theoretical models of optimal foraging have been successfully used to describe real-world scenarios, attracting attention in several fields such as economy, physics, ecology, and evolutionary biology. However, it remains unclear in most cases which strategies maximize foraging efficiency and whether such strategies can be learned by living organisms. To address these questions, we model foragers as reinforcement learning agents. We first prove theoretically that maximizing rewards in our reinforcement learning model is equivalent to optimizing foraging efficiency. We then show with numerical experiments that our agents learn foraging strategies which outperform the efficiency of known strategies such as L\'evy walks.
&lt;/p&gt;</description></item><item><title>SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05118</link><description>&lt;p&gt;
SLCA: &#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05118
&lt;/p&gt;
&lt;p&gt;
SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#22312;&#23398;&#20064;&#39034;&#24207;&#21040;&#36798;&#30340;&#25968;&#25454;&#20013;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#29616;&#26377;&#24037;&#20316;&#37117;&#24314;&#31435;&#22312;&#20174;&#22836;&#23398;&#20064;&#30340;&#21069;&#25552;&#19979;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#21162;&#21147;&#24050;&#32463;&#33268;&#21147;&#20110;&#34701;&#20837;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#27599;&#20010;&#22686;&#37327;&#20219;&#21153;&#20013;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#24182;&#23558;&#20851;&#38190;&#25361;&#25112;&#24402;&#22240;&#20110;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#35266;&#23519;&#21040;&#22312;&#34920;&#24449;&#23618;&#27425;&#19978;&#36873;&#25321;&#24615;&#38477;&#20302;&#23398;&#20064;&#29575;&#20960;&#20046;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;&#65288;SLCA&#65289;&#65292;&#36890;&#36807;&#24314;&#27169;&#31867;&#21035;&#20998;&#24067;&#24182;&#22312;&#20107;&#21518;&#23545;&#40784;&#20998;&#31867;&#23618;&#27425;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20998;&#31867;&#23618;&#27425;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SLCA&#22312;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of continual learning is to improve the performance of recognition models in learning sequentially arrived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training. However, how to adaptively exploit the pre-trained knowledge for each incremental task while maintaining its generalizability remains an open question. In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selectively reducing the learning rate can almost resolve this issue in the representation layer, we propose a simple but extremely effective approach named Slow Learner with Classifier Alignment (SLCA), which further improves the classification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fashion. Across a variety o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#21033;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#29399;&#30340;&#35270;&#35273;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.06267</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26377;&#21161;&#20110;&#21333;&#27169;&#24577;&#65306;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#30340;&#20132;&#21449;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models. (arXiv:2301.06267v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06267
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#21033;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#29399;&#30340;&#35270;&#35273;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#26159;&#26234;&#33021;&#20195;&#29702;&#30340;&#26680;&#24515;&#35201;&#32032;&#65292;&#20063;&#34987;&#31216;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#20351;&#29992;&#26469;&#33258;&#21333;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#26679;&#26412;&#65292;&#20294;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#19981;&#36275;&#20197;&#25551;&#36848;&#25972;&#20010;&#27010;&#24565;&#31867;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20351;&#29992;&#36328;&#27169;&#24577;&#20449;&#24687;&#39640;&#25928;&#22320;&#23398;&#20064;&#26032;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38405;&#35835;&#20851;&#20110;&#29399;&#24182;&#21548;&#23427;&#20204;&#21536;&#21483;&#30340;&#22768;&#38899;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;&#35270;&#35273;&#29399;&#20998;&#31867;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26159;&#22266;&#26377;&#30340;&#36328;&#27169;&#24577;&#30340;&#29305;&#24615;&#65292;&#23558;&#19981;&#21516;&#30340;&#27169;&#24577;&#26144;&#23556;&#21040;&#30456;&#21516;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#36328;&#36234;&#19981;&#21516;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#31867;&#21517;&#37325;&#26032;&#29992;&#20316;&#39069;&#22806;&#30340;&#19968;&#27425;&#24615;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26497;&#20854;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better ${\bf visual}$ dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for visi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22320;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#20219;&#21153;&#12290;&#36890;&#36807;&#26500;&#24314;&#23616;&#37096;&#24230;&#37327;&#22320;&#22270;&#21644;&#20840;&#23616;&#25299;&#25169;&#22320;&#22270;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#21051;&#30011;&#31354;&#38388;&#24863;&#30693;&#21644;&#23548;&#33322;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.04385</link><description>&lt;p&gt;
BEVBert: &#29992;&#20110;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#30340;&#22810;&#27169;&#24577;&#22320;&#22270;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BEVBert: Multimodal Map Pre-training for Language-guided Navigation. (arXiv:2212.04385v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22320;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#20219;&#21153;&#12290;&#36890;&#36807;&#26500;&#24314;&#23616;&#37096;&#24230;&#37327;&#22320;&#22270;&#21644;&#20840;&#23616;&#25299;&#25169;&#22320;&#22270;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#21051;&#30011;&#31354;&#38388;&#24863;&#30693;&#21644;&#23548;&#33322;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#24050;&#32463;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#37319;&#29992;&#31163;&#25955;&#30340;&#20840;&#26223;&#22270;&#26469;&#23398;&#20064;&#35270;&#35273;-&#25991;&#26412;&#20851;&#32852;&#12290;&#36825;&#35201;&#27714;&#27169;&#22411;&#38544;&#24335;&#22320;&#20851;&#32852;&#20840;&#26223;&#22270;&#20013;&#30340;&#19981;&#23436;&#25972;&#12289;&#37325;&#22797;&#30340;&#35266;&#23519;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#21040;&#26234;&#33021;&#20307;&#30340;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22320;&#22270;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#29992;&#20110;VLN&#20013;&#30340;&#31354;&#38388;&#24863;&#30693;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23616;&#37096;&#24230;&#37327;&#22320;&#22270;&#65292;&#26126;&#30830;&#22320;&#27719;&#32858;&#19981;&#23436;&#25972;&#30340;&#35266;&#23519;&#25968;&#25454;&#24182;&#28040;&#38500;&#37325;&#22797;&#65292;&#21516;&#26102;&#22312;&#19968;&#20010;&#20840;&#23616;&#25299;&#25169;&#22320;&#22270;&#20013;&#24314;&#27169;&#23548;&#33322;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#28151;&#21512;&#35774;&#35745;&#21487;&#20197;&#24179;&#34913;VLN&#23545;&#30701;&#26399;&#25512;&#29702;&#21644;&#38271;&#26399;&#35268;&#21010;&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#28151;&#21512;&#22320;&#22270;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#26694;&#26550;&#26469;&#23398;&#20064;&#22810;&#27169;&#24577;&#22320;&#22270;&#34920;&#31034;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31354;&#38388;&#24863;&#30693;&#36328;&#27169;&#24577;&#25512;&#29702;&#65292;&#26377;&#21161;&#20110;&#35821;&#35328;&#23548;&#21521;&#23548;&#33322;&#30446;&#26631;&#30340;&#23454;&#29616;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-training has shown promising results on the vision-and-language navigation (VLN) task. However, most existing pre-training methods employ discrete panoramas to learn visual-textual associations. This requires the model to implicitly correlate incomplete, duplicate observations within the panoramas, which may impair an agent's spatial understanding. Thus, we propose a new map-based pre-training paradigm that is spatial-aware for use in VLN. Concretely, we build a local metric map to explicitly aggregate incomplete observations and remove duplicates, while modeling navigation dependency in a global topological map. This hybrid design can balance the demand of VLN for both short-term reasoning and long-term planning. Then, based on the hybrid map, we devise a pre-training framework to learn a multimodal map representation, which enhances spatial-aware cross-modal reasoning thereby facilitating the language-guided navigation goal. Extensive experiments demonstrate the effec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22320;&#38663;&#21160;&#35889;&#30340;&#23450;&#20041;&#29305;&#24449;&#65292;&#20197;&#36741;&#21161;&#22320;&#38663;&#21160;&#36873;&#25321;&#12290;&#23427;&#32467;&#21512;&#20102;&#26426;&#22120;&#21457;&#29616;&#30340;&#28508;&#22312;&#29305;&#24449;&#21644;&#20256;&#32479;&#24378;&#24230;&#27979;&#37327;&#65292;&#36890;&#36807;&#32858;&#31867;&#20998;&#26512;&#36873;&#25321;&#20195;&#34920;&#24615;&#30340;&#22320;&#38663;&#21160;&#35760;&#24405;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.03188</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#22320;&#38663;&#21160;&#35889;&#30340;&#32858;&#31867;&#21644;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection. (arXiv:2212.03188v2 [physics.geo-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22320;&#38663;&#21160;&#35889;&#30340;&#23450;&#20041;&#29305;&#24449;&#65292;&#20197;&#36741;&#21161;&#22320;&#38663;&#21160;&#36873;&#25321;&#12290;&#23427;&#32467;&#21512;&#20102;&#26426;&#22120;&#21457;&#29616;&#30340;&#28508;&#22312;&#29305;&#24449;&#21644;&#20256;&#32479;&#24378;&#24230;&#27979;&#37327;&#65292;&#36890;&#36807;&#32858;&#31867;&#20998;&#26512;&#36873;&#25321;&#20195;&#34920;&#24615;&#30340;&#22320;&#38663;&#21160;&#35760;&#24405;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#24212;&#29992;&#31185;&#23398;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24207;&#21015;&#25968;&#25454;&#30340;&#32858;&#31867;&#20998;&#26512;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#20173;&#28982;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22320;&#38663;&#21160;&#35889;&#30340;&#23450;&#20041;&#29305;&#24449;&#65292;&#20063;&#31216;&#20026;&#28508;&#22312;&#29305;&#24449;&#65292;&#20197;&#36741;&#21161;&#22320;&#38663;&#21160;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#28508;&#22312;&#29305;&#24449;&#26159;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#20302;&#32500;&#24230;&#26426;&#22120;&#21457;&#29616;&#30340;&#35889;&#29305;&#24449;&#12290;&#26426;&#22120;&#21457;&#29616;&#30340;&#28508;&#22312;&#29305;&#24449;&#21487;&#20197;&#19982;&#20256;&#32479;&#23450;&#20041;&#30340;&#24378;&#24230;&#27979;&#37327;&#30456;&#32467;&#21512;&#65292;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#65292;&#20174;&#22823;&#37327;&#30340;&#22320;&#38663;&#21160;&#26679;&#26412;&#20013;&#36873;&#25321;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#23376;&#38598;&#12290;&#39640;&#25928;&#30340;&#22320;&#38663;&#21160;&#36873;&#25321;&#30340;&#30446;&#26631;&#26159;&#36873;&#25321;&#20195;&#34920;&#24615;&#30340;&#35760;&#24405;&#65292;&#20197;&#27010;&#29575;&#24615;&#22320;&#21453;&#26144;&#32467;&#26500;&#22312;&#20854;&#23551;&#21629;&#21608;&#26399;&#20869;&#23558;&#32463;&#21382;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19977;&#20010;&#31034;&#20363;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#21512;&#25104;&#21644;&#29616;&#22330;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering analysis of sequence data continues to address many applications in engineering design, aided with the rapid growth of machine learning in applied science. This paper presents an unsupervised machine learning algorithm to extract defining characteristics of earthquake ground-motion spectra, also called latent features, to aid in ground-motion selection (GMS). In this context, a latent feature is a low-dimensional machine-discovered spectral characteristic learned through nonlinear relationships of a neural network autoencoder. Machine discovered latent features can be combined with traditionally defined intensity measures and clustering can be performed to select a representative subgroup from a large ground-motion suite. The objective of efficient GMS is to choose characteristic records representative of what the structure will probabilistically experience in its lifetime. Three examples are presented to validate this approach, including the use of synthetic and field recor
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;Nothigattu&#12289;Shah&#21644;Procaccia&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#32858;&#21512;&#35780;&#23457;&#20154;&#30340;&#26144;&#23556;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#36127;&#38754;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.02144</link><description>&lt;p&gt;
&#26080;&#25439;&#22833;&#21363;&#26080;&#21327;&#35758;&#65306;&#23398;&#20064;&#19982;&#21516;&#34892;&#35780;&#23457;&#30340;&#31038;&#20250;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
No Agreement Without Loss: Learning and Social Choice in Peer Review. (arXiv:2211.02144v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02144
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;Nothigattu&#12289;Shah&#21644;Procaccia&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#32858;&#21512;&#35780;&#23457;&#20154;&#30340;&#26144;&#23556;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#36127;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21516;&#34892;&#35780;&#23457;&#31995;&#32479;&#20013;&#65292;&#35780;&#23457;&#20154;&#32463;&#24120;&#34987;&#35201;&#27714;&#35780;&#20272;&#25552;&#20132;&#31295;&#20214;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#22914;&#25216;&#26415;&#36136;&#37327;&#25110;&#26032;&#39062;&#24615;&#12290;&#32473;&#23450;&#27599;&#20010;&#39044;&#23450;&#20041;&#29305;&#24449;&#30340;&#35780;&#20998;&#65292;&#35780;&#23457;&#20154;&#24517;&#39035;&#25552;&#20379;&#19968;&#20010;&#25972;&#20307;&#30340;&#23450;&#37327;&#24314;&#35758;&#12290;&#21487;&#20197;&#20551;&#35774;&#27599;&#20010;&#35780;&#23457;&#20154;&#23545;&#29305;&#24449;&#38598;&#21512;&#21040;&#24314;&#35758;&#30340;&#26144;&#23556;&#37117;&#26377;&#33258;&#24049;&#30340;&#30475;&#27861;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#35780;&#23457;&#20154;&#24515;&#20013;&#26377;&#30528;&#19981;&#21516;&#30340;&#26144;&#23556;&#12290;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#27604;&#20363;&#20559;&#24046;&#30340;&#20219;&#36873;&#24615;&#22240;&#32032;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#30001;Noothigattu&#12289;Shah&#21644;Procaccia&#24341;&#20837;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#35813;&#26694;&#26550;&#22312;AAAI2022&#20250;&#35758;&#30340;&#32452;&#32455;&#32773;&#20013;&#24212;&#29992;&#12290;Noothigattu&#12289;Shah&#21644;Procaccia&#25552;&#20986;&#36890;&#36807;&#26368;&#23567;&#21270;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#26469;&#32858;&#21512;&#35780;&#23457;&#20154;&#30340;&#26144;&#23556;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20013;&#30340;&#20844;&#29702;&#24615;&#36136;&#12290;&#25105;&#20204;&#23545;&#20182;&#20204;&#30340;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#32467;&#26524;&#21644;&#20551;&#35774;&#36827;&#34892;&#20102;&#25361;&#25112;&#65292;&#24182;&#25253;&#21578;&#20102;&#19968;&#20123;&#36127;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In peer review systems, reviewers are often asked to evaluate various features of submissions, such as technical quality or novelty. A score is given to each of the predefined features and based on these the reviewer has to provide an overall quantitative recommendation. It may be assumed that each reviewer has her own mapping from the set of features to a recommendation, and that different reviewers have different mappings in mind. This introduces an element of arbitrariness known as commensuration bias. In this paper we discuss a framework, introduced by Noothigattu, Shah and Procaccia, and then applied by the organizers of the AAAI 2022 conference. Noothigattu, Shah and Procaccia proposed to aggregate reviewer's mapping by minimizing certain loss functions, and studied axiomatic properties of this approach, in the sense of social choice theory. We challenge several of the results and assumptions used in their work and report a number of negative results. On the one hand, we study a 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#65292;&#38024;&#23545;&#25512;&#33616;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26041;&#27861;&#21644;&#24212;&#29992;&#26469;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13619</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#22522;&#30784;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fairness in Recommendation: Foundations, Methods and Applications. (arXiv:2205.13619v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13619
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#65292;&#38024;&#23545;&#25512;&#33616;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26041;&#27861;&#21644;&#24212;&#29992;&#26469;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26368;&#26222;&#36941;&#30340;&#24212;&#29992;&#20043;&#19968;&#65292;&#25512;&#33616;&#31995;&#32479;&#22312;&#36741;&#21161;&#20154;&#31867;&#20915;&#31574;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#21644;&#24179;&#21488;&#30340;&#21033;&#30410;&#19982;&#29983;&#25104;&#30340;&#25512;&#33616;&#32467;&#26524;&#30340;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#39640;&#24230;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#21463;&#21040;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#21066;&#24369;&#31995;&#32479;&#30340;&#21487;&#20449;&#36182;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#33616;&#35774;&#32622;&#20013;&#35299;&#20915;&#28508;&#22312;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#32771;&#34385;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#28041;&#21450;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#30456;&#23545;&#38646;&#25955;&#19988;&#32570;&#20047;&#31995;&#32479;&#21270;&#25972;&#29702;&#65292;&#22240;&#27492;&#23545;&#20110;&#26032;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#38590;&#20197;&#28145;&#20837;&#39046;&#22495;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#23545;&#25512;&#33616;&#20013;&#29616;&#26377;&#20844;&#24179;&#24615;&#20316;&#21697;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35760;&#24518;&#25511;&#21046;&#21644;&#21518;&#32487;&#29305;&#24449;&#20004;&#20010;&#26694;&#26550;&#30340;&#25972;&#21512;&#65292;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#31574;&#30053;&#37325;&#29992;&#30340;&#20248;&#38597;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.03110</link><description>&lt;p&gt;
&#21518;&#32487;&#29305;&#24449;&#31070;&#32463;&#35760;&#24518;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Successor Feature Neural Episodic Control. (arXiv:2111.03110v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35760;&#24518;&#25511;&#21046;&#21644;&#21518;&#32487;&#29305;&#24449;&#20004;&#20010;&#26694;&#26550;&#30340;&#25972;&#21512;&#65292;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#31574;&#30053;&#37325;&#29992;&#30340;&#20248;&#38597;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#26159;&#26500;&#24314;&#26234;&#33021;&#20195;&#29702;&#65292;&#23637;&#31034;&#31867;&#20284;&#20110;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#24555;&#36895;&#23398;&#20064;&#21644;&#28789;&#27963;&#30340;&#25216;&#33021;&#36716;&#31227;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#20915;&#36825;&#20123;&#30446;&#26631;&#30340;&#20004;&#20010;&#26694;&#26550;&#30340;&#25972;&#21512;&#65306;&#35760;&#24518;&#25511;&#21046;&#21644;&#21518;&#32487;&#29305;&#24449;&#12290;&#35760;&#24518;&#25511;&#21046;&#26159;&#19968;&#31181;&#21463;&#35748;&#30693;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#24773;&#33410;&#35760;&#24518;&#65292;&#21363;&#20195;&#29702;&#30340;&#32463;&#39564;&#30340;&#22522;&#20110;&#23454;&#20363;&#30340;&#20869;&#23384;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#21518;&#32487;&#29305;&#24449;&#21644;&#24191;&#20041;&#31574;&#30053;&#25913;&#36827;&#65288;SF&amp;amp;GPI&#65289;&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#20855;&#26377;&#19981;&#21516;&#22870;&#21169;&#20989;&#25968;&#30340;&#21518;&#32493;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#22320;&#37325;&#29992;&#20808;&#21069;&#23398;&#21040;&#30340;&#31574;&#30053;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#20998;&#21035;&#22312;&#22823;&#22823;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#20248;&#38597;&#22320;&#37325;&#29992;&#20808;&#21069;&#23398;&#21040;&#30340;&#31574;&#30053;&#26041;&#38754;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20854;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
A longstanding goal in reinforcement learning is to build intelligent agents that show fast learning and a flexible transfer of skills akin to humans and animals. This paper investigates the integration of two frameworks for tackling those goals: episodic control and successor features. Episodic control is a cognitively inspired approach relying on episodic memory, an instance-based memory model of an agent's experiences. Meanwhile, successor features and generalized policy improvement (SF&amp;GPI) is a meta and transfer learning framework allowing to learn policies for tasks that can be efficiently reused for later tasks which have a different reward function. Individually, these two techniques have shown impressive results in vastly improving sample efficiency and the elegant reuse of previously learned policies. Thus, we outline a combination of both approaches in a single reinforcement learning framework and empirically illustrate its benefits.
&lt;/p&gt;</description></item><item><title>ROME&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;NAS&#26041;&#27861;&#65292;&#36890;&#36807;&#25299;&#25169;&#35299;&#32806;&#21644;&#26799;&#24230;&#32047;&#31215;&#35299;&#20915;&#20102;&#21333;&#36335;&#24452;DARTS&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2011.11233</link><description>&lt;p&gt;
ROME: &#36890;&#36807;&#25299;&#25169;&#35299;&#32806;&#21644;&#26799;&#24230;&#32047;&#31215;&#23454;&#29616;&#40065;&#26834;&#30340;&#20869;&#23384;&#39640;&#25928;NAS
&lt;/p&gt;
&lt;p&gt;
ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation. (arXiv:2011.11233v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.11233
&lt;/p&gt;
&lt;p&gt;
ROME&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;NAS&#26041;&#27861;&#65292;&#36890;&#36807;&#25299;&#25169;&#35299;&#32806;&#21644;&#26799;&#24230;&#32047;&#31215;&#35299;&#20915;&#20102;&#21333;&#36335;&#24452;DARTS&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#25972;&#20010;&#36229;&#32593;&#32476;&#23384;&#25918;&#22312;&#20869;&#23384;&#20013;&#65292;&#23427;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#20869;&#23384;&#24320;&#38144;&#30340;&#38480;&#21046;&#12290;&#36825;&#23601;&#26159;&#21333;&#36335;&#24452;DARTS&#30340;&#20248;&#21183;&#25152;&#22312;&#65292;&#23427;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#21482;&#36873;&#25321;&#19968;&#20010;&#21333;&#36335;&#24452;&#23376;&#27169;&#22411;&#12290;&#34429;&#28982;&#23427;&#23545;&#20869;&#23384;&#21451;&#22909;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#36335;&#24452;DARTS&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#36825;&#19968;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23427;&#20063;&#20250;&#20986;&#29616;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;&#20687;DARTS&#19968;&#26679;&#65292;&#23427;&#20250;&#23548;&#20986;&#22826;&#22810;&#26080;&#21442;&#25968;&#25805;&#20316;&#65292;&#20363;&#22914;&#36339;&#36291;&#36830;&#25509;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#25299;&#25169;&#25628;&#32034;&#21644;&#25805;&#20316;&#25628;&#32034;&#35299;&#32806;&#65292;&#20351;&#25628;&#32034;&#21644;&#35780;&#20272;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;Gumbel-Top2&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#26799;&#24230;&#32047;&#31215;&#26469;&#22686;&#24378;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21452;&#23618;&#20248;&#21270;&#12290;&#25105;&#20204;&#23545;ROME&#36827;&#34892;&#20102;&#22823;&#37327;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Albeit being a prevalent architecture searching approach, differentiable architecture search (DARTS) is largely hindered by its substantial memory cost since the entire supernet resides in the memory. This is where the single-path DARTS comes in, which only chooses a single-path submodel at each step. While being memory-friendly, it also comes with low computational costs. Nonetheless, we discover a critical issue of single-path DARTS that has not been primarily noticed. Namely, it also suffers from severe performance collapse since too many parameter-free operations like skip connections are derived, just like DARTS does. In this paper, we propose a new algorithm called RObustifying Memory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology search from the operation search to make searching and evaluation consistent. We then adopt Gumbel-Top2 reparameterization and gradient accumulation to robustify the unwieldy bi-level optimization. We verify ROME extensively acr
&lt;/p&gt;</description></item></channel></rss>