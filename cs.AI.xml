<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17342</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#31574;&#30053;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24191;&#20041;&#25915;&#20987;&#24418;&#24335;&#21644;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#30740;&#31350;&#30452;&#25509;&#25200;&#21160;&#21463;&#23475;&#32773;&#30340;&#29366;&#24577;/&#21160;&#20316;&#25110;&#22522;&#30784;&#36716;&#31227;&#21160;&#24577;&#20197;&#23637;&#31034;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#33030;&#24369;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#30452;&#25509;&#25805;&#32437;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21478;&#19968;&#31181;&#24120;&#35265;&#19988;&#29616;&#23454;&#30340;&#25915;&#20987;&#35774;&#32622;&#65306;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#30340;&#35774;&#32622;&#20013;&#65292;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#21463;&#23475;&#20195;&#29702;$\nu$&#34987;&#25915;&#20987;&#32773;&#25511;&#21046;&#21478;&#19968;&#20010;&#20195;&#29702;$\alpha$&#20197;&#25932;&#23545;&#26041;&#24335;&#34892;&#21160;&#65292;&#20351;&#29992;&#8220;&#23545;&#25239;&#31574;&#30053;&#8221;&#23545;&#21463;&#23475;&#20195;&#29702;&#36827;&#34892;&#25915;&#20987;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#25915;&#20987;&#27169;&#22411;&#32771;&#34385;&#20102;&#36825;&#31181;&#35774;&#32622;&#65292;&#20294;&#20182;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#25915;&#20987;&#32773;&#21487;&#20197;&#36935;&#21040;&#25269;&#25239;&#65292;&#22240;&#27492;&#21482;&#33021;&#37096;&#20998;&#25511;&#21046;&#20195;&#29702;$\alpha$&#65292;&#21516;&#26102;&#24341;&#20837;&#21487;&#23519;&#35273;&#30340;&#8220;&#24322;&#24120;&#8221;&#34892;&#20026;&#65292;&#36825;&#20123;&#34892;&#20026;&#24456;&#23481;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#24182;&#19988;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#23545;&#25239;&#31574;&#30053;&#30340;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#21270;&#30340;&#25915;&#20987;&#24418;&#24335;&#65292;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#22312;&#20309;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#25511;&#21046;&#20195;&#29702;$\alpha$&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\nu$ is exploited by an attacker who controls another agent $\alpha$ to act adversarially against the victim using an \textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#33258;&#21160;&#21270;&#35270;&#39057;&#20998;&#26512;&#31995;&#32479;&#65292;&#22312;&#27700;&#19979;&#33337;&#20307;&#26816;&#27979;&#20013;&#23454;&#29616;&#22810;&#26631;&#31614;&#35270;&#39057;&#20998;&#31867;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#21464;&#25442;&#22120;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#33719;&#36830;&#32493;&#35270;&#39057;&#24103;&#20013;&#30340;&#26102;&#31354;&#27880;&#24847;&#12290;</title><link>http://arxiv.org/abs/2305.17338</link><description>&lt;p&gt;
&#27700;&#19979;&#33337;&#20307;&#26816;&#27979;&#30340;&#22810;&#26631;&#31614;&#35270;&#39057;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-label Video Classification for Underwater Ship Inspection. (arXiv:2305.17338v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#33258;&#21160;&#21270;&#35270;&#39057;&#20998;&#26512;&#31995;&#32479;&#65292;&#22312;&#27700;&#19979;&#33337;&#20307;&#26816;&#27979;&#20013;&#23454;&#29616;&#22810;&#26631;&#31614;&#35270;&#39057;&#20998;&#31867;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#21464;&#25442;&#22120;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#33719;&#36830;&#32493;&#35270;&#39057;&#24103;&#20013;&#30340;&#26102;&#31354;&#27880;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#65292;&#33337;&#20307;&#26816;&#27979;&#65292;&#21253;&#25324;&#22806;&#37096;&#28034;&#23618;&#30340;&#26816;&#26597;&#65292;&#32570;&#38519;&#30340;&#26816;&#27979;&#20197;&#21450;&#20854;&#20182;&#31867;&#22411;&#30340;&#22806;&#37096;&#38477;&#35299;&#22914;&#33104;&#34432;&#21644;&#28023;&#27915;&#29983;&#38271;&#36890;&#36807;&#36828;&#31243;&#25805;&#20316;&#36710;&#65288;ROVs&#65289;&#22312;&#27700;&#19979;&#36827;&#34892;&#12290;&#26816;&#27979;&#36807;&#31243;&#21253;&#25324;&#23545;&#35270;&#39057;&#30340;&#25163;&#21160;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#33258;&#21160;&#21270;&#35270;&#39057;&#20998;&#26512;&#31995;&#32479;&#65292;&#20197;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#25152;&#20165;&#32771;&#34385;&#27700;&#19979;&#33337;&#20307;&#35270;&#39057;&#26816;&#27979;&#21333;&#20010;&#24103;&#30340;&#31354;&#38388;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#25506;&#32034;&#28155;&#21152;&#26102;&#38388;&#20449;&#24687;&#21644;&#20998;&#26512;&#22522;&#20110;&#24103;&#30340;&#20998;&#31867;&#22120;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26631;&#31614;&#35270;&#39057;&#20998;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#21464;&#25442;&#22120;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#33719;&#36830;&#32493;&#35270;&#39057;&#24103;&#20013;&#30340;&#26102;&#31354;&#27880;&#24847;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26041;&#27861;&#24050;&#32463;&#23637;&#29616;&#20986;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#24182;&#21487;&#20316;&#20026;&#26410;&#26469;&#27700;&#19979;&#33337;&#20307;&#26816;&#39564;&#30740;&#31350;&#21644;&#21457;&#23637;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today ship hull inspection including the examination of the external coating, detection of defects, and other types of external degradation such as corrosion and marine growth is conducted underwater by means of Remotely Operated Vehicles (ROVs). The inspection process consists of a manual video analysis which is a time-consuming and labor-intensive process. To address this, we propose an automatic video analysis system using deep learning and computer vision to improve upon existing methods that only consider spatial information on individual frames in underwater ship hull video inspection. By exploring the benefits of adding temporal information and analyzing frame-based classifiers, we propose a multi-label video classification model that exploits the self-attention mechanism of transformers to capture spatiotemporal attention in consecutive video frames. Our proposed method has demonstrated promising results and can serve as a benchmark for future research and development in underw
&lt;/p&gt;</description></item><item><title>&#26032;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#24179;&#22343;F1&#20998;&#25968;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;EL&#27169;&#22411;8.51&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.17337</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Diverse-Modal Entity Linking with Generative Models. (arXiv:2305.17337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17337
&lt;/p&gt;
&lt;p&gt;
&#26032;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#24179;&#22343;F1&#20998;&#25968;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;EL&#27169;&#22411;8.51&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21487;&#20197;&#29992;&#19981;&#21516;&#30340;&#26684;&#24335;&#26469;&#34920;&#36798;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#34920;&#26684;&#20013;&#30340;&#21015;&#21517;&#21644;&#21333;&#20803;&#26684;&#20540;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#27169;&#22411;&#22312;&#27599;&#31181;&#27169;&#24335;&#37197;&#32622;&#19978;&#37117;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#20165;&#25991;&#26412;EL&#12289;&#35270;&#35273;&#23450;&#20301;&#25110;&#27169;&#24335;&#38142;&#25509;&#65292;&#20294;&#20026;&#22810;&#31181;&#27169;&#24335;&#37197;&#32622;&#35774;&#35745;&#32479;&#19968;&#27169;&#22411;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#23558;&#21508;&#31181;&#27169;&#24577;&#37197;&#32622;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#20174;&#29616;&#26377;EL&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#34920;&#26684;&#19977;&#31181;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;EL&#65288;DMEL&#65289;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#35299;&#20915;DMEL&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;GDMM&#65289;&#65292;&#36981;&#24490;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#33539;&#20363;&#12290;&#23558;\Model&#29992;&#20016;&#23500;&#30340;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#19981;&#20445;&#23384;&#25972;&#20010;KB&#36827;&#34892;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#20026;DMEL&#26500;&#24314;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;&#24494;&#35843;GDMM&#21487;&#20197;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;DMEL&#22522;&#32447;&#65292;&#22312;&#24179;&#22343;F1&#20998;&#25968;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;EL&#27169;&#22411;8.51&#20998;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#23558;&#22810;&#27169;&#24577;&#23454;&#20307;&#32763;&#35793;&#21644;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#21183;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding, or schema linking, it is more challenging to design a unified model for diverse modality configurations. To bring various modality configurations together, we constructed a benchmark for diverse-modal EL (DMEL) from existing EL datasets, covering all three modalities including text, image, and table. To approach the DMEL task, we proposed a generative diverse-modal model (GDMM) following a multimodal-encoder-decoder paradigm. Pre-training \Model with rich corpora builds a solid foundation for DMEL without storing the entire KB for inference. Fine-tuning GDMM builds a stronger DMEL baseline, outperforming state-of-the-art task-specific EL models by 8.51 F1 score on average. Additionally, extensive error analyses are conducted to highlight the challenges of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.17330</link><description>&lt;p&gt;
MADiff&#65306;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#22312;&#21253;&#25324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#20869;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20854;&#20013;&#31574;&#30053;&#36890;&#36807;&#22312;&#22312;&#32447;&#35780;&#20272;&#20013;&#20135;&#29983;&#36712;&#36857;&#26469;&#36827;&#34892;&#35268;&#21010;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#26174;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;DM&#22914;&#20309;&#22312;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#20013;&#25805;&#20316;&#65292;&#20854;&#20013;&#20195;&#29702;&#21830;&#24456;&#38590;&#22312;&#29420;&#31435;&#24314;&#27169;&#27599;&#20010;&#20195;&#29702;&#21830;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22242;&#38431;&#21512;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MADiff&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MADiff&#26159;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#25193;&#25955;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#22797;&#26434;&#21327;&#35843;&#24314;&#27169;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MADiff&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#26694;&#26550;&#65292;&#23427;&#26082;&#21487;&#20197;&#34892;&#20026;&#20026;&#20998;&#25955;&#30340;&#25919;&#31574;&#65292;&#21448;&#21487;&#20197;&#20026;&#38598;&#20013;&#25511;&#21046;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#25163;&#24314;&#27169;&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
&lt;/p&gt;</description></item><item><title>&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17328</link><description>&lt;p&gt;
&#12298;Zero-TPrune: &#22522;&#20110;&#39044;&#35757;&#32451;Transformers&#20851;&#27880;&#22270;&#30340;&#38646;&#23556;&#20987;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17328
&lt;/p&gt;
&lt;p&gt;
&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;Transformer&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#27169;&#22411;&#30340;&#20307;&#31215;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#25512;&#29702;&#25104;&#26412;&#21017;&#38543;&#36755;&#20837;&#24207;&#21015;&#20013;&#20196;&#29260;&#25968;&#37327;&#30340;&#24179;&#26041;&#25552;&#39640;&#12290;&#20196;&#29260;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#20852;&#35299;&#20915;&#26041;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#26131;&#20110;&#22312;&#21508;&#31181;Transformer&#25903;&#25345;&#30340;&#27169;&#22411;&#19978;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#38656;&#35201;&#22312;&#21098;&#26525;&#21518;&#25110;&#26399;&#38388;&#36827;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25506;&#35752;&#20102;&#27809;&#26377;&#24494;&#35843;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;Transformer&#30340;&#21098;&#26525;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Zero-TPrune&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#26082;&#32771;&#34385;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21448;&#32771;&#34385;&#30456;&#20284;&#24615;&#26469;&#25191;&#34892;&#20196;&#29260;&#21098;&#26525;&#12290;Zero-TPrune&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#20026;&#20196;&#29260;&#29983;&#25104;&#19968;&#20010;&#37325;&#35201;&#24615;&#25490;&#21517;&#24182;&#31227;&#38500;&#20449;&#24687;&#36739;&#23569;&#30340;&#20196;&#29260;&#12290;&#27880;&#24847;&#30697;&#38453;&#21487;&#29992;&#20110;&#25512;&#26029;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployment of Transformer models on the edge is increasingly challenging due to the exponentially growing model size and inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require a computationally-expensive fine-tuning process after or during pruning, which is not desirable in many cases. Some recent works explore pruning of off-the-shelf pre-trained Transformers without fine-tuning. However, they only take the importance of tokens into consideration. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. Zero-TPrune leverages the attention graph of pre-trained Transformer models to produce an importance rank for tokens and removes the less informative tokens. The attention matrix can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;</title><link>http://arxiv.org/abs/2305.17326</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;KL&#25955;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Kernel-SSL
&lt;/p&gt;
&lt;p&gt;
Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#23558;&#19968;&#20010;&#27491;&#38170;&#28857;&#26679;&#26412;&#19982;&#35768;&#22810;&#36127;&#26679;&#26412;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23436;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#30456;&#21453;&#65292;&#38750;&#23545;&#27604;&#23398;&#20064;&#65292;&#20363;&#22914;BYOL&#12289;SimSiam&#21644;Barlow Twins&#31561;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#20351;&#29992;&#36127;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;SSL&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#29616;&#26377;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Kernel-SSL&#65292;&#30452;&#25509;&#20248;&#21270;RKHS&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Kernel-SSL&#22312;&#32447;&#24615;&#35780;&#20272;&#35774;&#32622;&#19979;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#22823;&#24133;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36827;&#34892;100&#20010;epoch&#30340;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;SimCLR&#34920;&#29616;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36947;&#24503;&#26426;&#22120;&#39033;&#30446;&#20013;&#25552;&#20986;&#30340;&#32447;&#24615;&#27169;&#22411;&#32858;&#21512;&#26426;&#21046;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#23384;&#22312;&#31574;&#30053;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23569;&#25968;&#32676;&#20307;&#26159;&#21542;&#20250;&#33719;&#32988;&#30340;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.17319</link><description>&lt;p&gt;
&#36947;&#24503;&#26426;&#22120;&#36824;&#26159;&#22810;&#25968;&#27966;&#30340;&#26292;&#25919;?
&lt;/p&gt;
&lt;p&gt;
Moral Machine or Tyranny of the Majority?. (arXiv:2305.17319v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36947;&#24503;&#26426;&#22120;&#39033;&#30446;&#20013;&#25552;&#20986;&#30340;&#32447;&#24615;&#27169;&#22411;&#32858;&#21512;&#26426;&#21046;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#23384;&#22312;&#31574;&#30053;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23569;&#25968;&#32676;&#20307;&#26159;&#21542;&#20250;&#33719;&#32988;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#37325;&#35201;&#39046;&#22495;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#35810;&#38382;&#36825;&#20123;&#31995;&#32479;&#22312;&#20262;&#29702;&#19978;&#20855;&#26377;&#20105;&#35758;&#30340;&#24773;&#20917;&#19979;&#24212;&#35813;&#22914;&#20309;&#34892;&#21160;&#65292;&#21363;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#36830;&#20154;&#31867;&#26412;&#36523;&#20063;&#27809;&#26377;&#19968;&#33268;&#20849;&#35782;&#12290;&#22312;&#36947;&#24503;&#26426;&#22120;&#39033;&#30446;&#20013;&#65292;&#30740;&#31350;&#21592;&#20204;&#20247;&#21253;&#22238;&#31572;&#20102;&#20851;&#20110;&#33258;&#20027;&#36710;&#36742;&#30340;"&#25318;&#36710;"&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;Noothigattu&#31561;&#20154;(2018)&#25552;&#20986;&#20102;&#25512;&#26029;&#32447;&#24615;&#20989;&#25968;&#26469;&#36817;&#20284;&#27599;&#20010;&#20010;&#20307;&#30340;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#23545;&#25972;&#20010;&#20154;&#32676;&#30340;&#21442;&#25968;&#21462;&#24179;&#22343;&#20540;&#26469;&#32858;&#21512;&#36825;&#20123;&#32447;&#24615;&#27169;&#22411;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#36825;&#31181;&#24179;&#22343;&#26426;&#21046;&#65292;&#22312;&#23384;&#22312;&#31574;&#30053;&#25928;&#24212;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#26041;&#38754;&#36827;&#34892;&#20102;&#37325;&#28857;&#35752;&#35770;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#20154;&#21475;&#20998;&#20026;&#20004;&#32452;&#65292;&#23569;&#25968;&#27966;&#30340;&#945; &lt;0.5&#12290;&#20026;&#31616;&#21270;&#20998;&#26512;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32676;&#20307;&#20869;&#20559;&#22909;&#26159;&#21516;&#36136;&#30340;&#26497;&#31471;&#24773;&#20917;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#23569;&#25968;&#32676;&#20307;&#33719;&#32988;&#30340;&#26377;&#20105;&#35758;&#24773;&#20917;&#30340;&#27604;&#20363;&#65292;&#36827;&#34892;&#20998;&#26512;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
With Artificial Intelligence systems increasingly applied in consequential domains, researchers have begun to ask how these systems ought to act in ethically charged situations where even humans lack consensus. In the Moral Machine project, researchers crowdsourced answers to "Trolley Problems" concerning autonomous vehicles. Subsequently, Noothigattu et al. (2018) proposed inferring linear functions that approximate each individual's preferences and aggregating these linear models by averaging parameters across the population. In this paper, we examine this averaging mechanism, focusing on fairness concerns in the presence of strategic effects. We investigate a simple setting where the population consists of two groups, with the minority constituting an {\alpha} &lt; 0.5 share of the population. To simplify the analysis, we consider the extreme case in which within-group preferences are homogeneous. Focusing on the fraction of contested cases where the minority group prevails, we make th
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#20302;&#33021;&#35265;&#24230;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24863;&#30693;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;3D&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#8220;REDFormer&#8221;&#65292;&#36890;&#36807;&#40479;&#30640;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#36827;&#34892;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#19988;&#30456;&#36739;&#20110;&#29616;&#26377;&#27169;&#22411;&#26356;&#32463;&#27982;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.17318</link><description>&lt;p&gt;
&#38647;&#36798;&#29031;&#20142;&#40657;&#26263;&#65306;&#36890;&#36807;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20302;&#33021;&#35265;&#24230;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Radar Enlighten the Dark: Enhancing Low-Visibility Perception for Automated Vehicles with Camera-Radar Fusion. (arXiv:2305.17318v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17318
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20302;&#33021;&#35265;&#24230;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24863;&#30693;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;3D&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#8220;REDFormer&#8221;&#65292;&#36890;&#36807;&#40479;&#30640;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#36827;&#34892;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#19988;&#30456;&#36739;&#20110;&#29616;&#26377;&#27169;&#22411;&#26356;&#32463;&#27982;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#34701;&#21512;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#30340;&#39550;&#39542;&#26465;&#20214;&#19979;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24863;&#30693;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#24694;&#21155;&#30340;&#22825;&#27668;&#21644;&#20302;&#20809;&#29031;&#26465;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#20256;&#24863;&#22120;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#36710;&#36742;&#23433;&#20840;&#38754;&#20020;&#28508;&#22312;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;3D&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#8220;REDFormer&#8221;&#65292;&#21033;&#29992;&#40479;&#30640;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#30340;&#20415;&#21033;&#21644;&#32463;&#27982;&#23454;&#29992;&#24615;&#26469;&#35299;&#20915;&#20302;&#33021;&#35265;&#24230;&#38382;&#39064;&#12290;&#22312;&#20351;&#29992;&#22810;&#38647;&#36798;&#28857;&#20113;&#12289;&#22825;&#27668;&#20449;&#24687;&#21644;&#26102;&#38388;&#25968;&#25454;&#30340;nuScenes&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20998;&#31867;&#21644;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#27169;&#22411;&#32452;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#23545;&#24212;&#23545;&#19978;&#36848;&#38382;&#39064;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sensor fusion is a crucial augmentation technique for improving the accuracy and reliability of perception systems for automated vehicles under diverse driving conditions. However, adverse weather and low-light conditions remain challenging, where sensor performance degrades significantly, exposing vehicle safety to potential risks. Advanced sensors such as LiDARs can help mitigate the issue but with extremely high marginal costs. In this paper, we propose a novel transformer-based 3D object detection model "REDFormer" to tackle low visibility conditions, exploiting the power of a more practical and cost-effective solution by leveraging bird's-eye-view camera-radar fusion. Using the nuScenes dataset with multi-radar point clouds, weather information, and time-of-day data, our model outperforms state-of-the-art (SOTA) models on classification and detection accuracy. Finally, we provide extensive ablation studies of each model component on their contributions to address the above-mention
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;NeQA&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21453;&#21521;&#32553;&#25918;&#12289;U&#22411;&#32553;&#25918;&#25110;&#27491;&#21521;&#32553;&#25918;&#65292;&#35299;&#20915;NeQA&#20381;&#36182;&#20110;&#38382;&#31572;&#21644;&#21542;&#23450;&#29702;&#35299;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#20854;&#32553;&#25918;&#36235;&#21183;&#30001;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#32553;&#25918;&#36235;&#21183;&#32452;&#21512;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.17311</link><description>&lt;p&gt;
&#36229;&#36234;&#27491;&#21521;&#32553;&#25918;&#65306;&#21542;&#23450;&#35821;&#23545;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#36235;&#21183;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models. (arXiv:2305.17311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;NeQA&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21453;&#21521;&#32553;&#25918;&#12289;U&#22411;&#32553;&#25918;&#25110;&#27491;&#21521;&#32553;&#25918;&#65292;&#35299;&#20915;NeQA&#20381;&#36182;&#20110;&#38382;&#31572;&#21644;&#21542;&#23450;&#29702;&#35299;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#20854;&#32553;&#25918;&#36235;&#21183;&#30001;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#32553;&#25918;&#36235;&#21183;&#32452;&#21512;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#27491;&#21521;&#32553;&#25918;&#65292;&#22312;&#22823;&#23567;&#12289;&#35745;&#31639;&#25110;&#25968;&#25454;&#26041;&#38754;&#25193;&#23637;&#27169;&#22411;&#20250;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#38382;&#21477;&#30340;&#25968;&#25454;&#38598;NeQA&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#19981;&#20250;&#34920;&#29616;&#20986;&#31616;&#21333;&#30340;&#27491;&#21521;&#32553;&#25918;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#20219;&#21153;&#21487;&#20197;&#34920;&#29616;&#20986;&#21453;&#21521;&#32553;&#25918;&#12289;U&#24418;&#32553;&#25918;&#25110;&#27491;&#21521;&#32553;&#25918;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#25552;&#31034;&#26041;&#27861;&#25110;&#27169;&#22411;&#26063;&#32676;&#26102;&#65292;&#36825;&#19977;&#31181;&#32553;&#25918;&#36235;&#21183;&#20250;&#25353;&#29031;&#36825;&#20010;&#39034;&#24207;&#21457;&#29983;&#36716;&#21464;&#12290;&#25105;&#20204;&#20551;&#35774;&#35299;&#20915;NeQA&#20381;&#36182;&#20110;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#38382;&#31572;&#65288;&#20219;&#21153;1&#65289;&#21644;&#21542;&#23450;&#29702;&#35299;&#65288;&#20219;&#21153;2&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#20219;&#21153;1&#20855;&#26377;&#32447;&#24615;&#32553;&#25918;&#65292;&#32780;&#20219;&#21153;2&#20855;&#26377;S&#24418;&#32553;&#25918;&#65292;&#24182;&#20855;&#26377;&#19968;&#20010;&#32039;&#24613;&#30340;&#36716;&#25240;&#28857;&#65292;&#23558;&#36825;&#20004;&#20010;&#32553;&#25918;&#36235;&#21183;&#32452;&#21512;&#36215;&#26469;&#21363;&#21487;&#24471;&#20986;&#26368;&#32456;&#30340;NeQA&#32553;&#25918;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#32553;&#25918;&#36235;&#21183;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to exhibit positive scaling, where performance improves as models are scaled up in terms of size, compute, or data. In this work, we introduce NeQA, a dataset consisting of questions with negation in which language models do not exhibit straightforward positive scaling. We show that this task can exhibit inverse scaling, U-shaped scaling, or positive scaling, and the three scaling trends shift in this order as we use more powerful prompting methods or model families. We hypothesize that solving NeQA depends on two subtasks: question answering (task 1) and negation understanding (task 2). We find that task 1 has linear scaling, while task 2 has sigmoid-shaped scaling with an emergent transition point, and composing these two scaling trends yields the final scaling trend of NeQA. Our work reveals and provides a way to analyze the complex scaling trends of language models.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21576;&#29616;&#20102;&#22312;&#22810;&#20010;&#25968;&#23383;&#24179;&#21488;&#19978;&#36816;&#34892;&#30340;&#35748;&#30693;&#26426;&#22120;&#20154;&#30340;&#26550;&#26500;&#25361;&#25112;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#38656;&#35201;&#20855;&#22791;&#36866;&#24212;&#24615;&#21644;&#35748;&#30693;&#33021;&#21147;&#65292;&#20197;&#36866;&#24212;&#24179;&#21488;&#30340;&#19981;&#21516;&#29305;&#24615;&#21644;&#19982;&#20154;&#31867;/&#36719;&#20214;&#20195;&#29702;&#30340;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.17308</link><description>&lt;p&gt;
&#26397;&#30528;&#35748;&#30693;&#26426;&#22120;&#20154;&#65306;&#26550;&#26500;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Towards Cognitive Bots: Architectural Research Challenges. (arXiv:2305.17308v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17308
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21576;&#29616;&#20102;&#22312;&#22810;&#20010;&#25968;&#23383;&#24179;&#21488;&#19978;&#36816;&#34892;&#30340;&#35748;&#30693;&#26426;&#22120;&#20154;&#30340;&#26550;&#26500;&#25361;&#25112;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#38656;&#35201;&#20855;&#22791;&#36866;&#24212;&#24615;&#21644;&#35748;&#30693;&#33021;&#21147;&#65292;&#20197;&#36866;&#24212;&#24179;&#21488;&#30340;&#19981;&#21516;&#29305;&#24615;&#21644;&#19982;&#20154;&#31867;/&#36719;&#20214;&#20195;&#29702;&#30340;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#34394;&#25311;&#25968;&#23383;&#24179;&#21488;&#19978;&#36816;&#34892;&#30340;&#36719;&#20214;&#26426;&#22120;&#20154;&#24517;&#39035;&#29702;&#35299;&#24179;&#21488;&#30340;&#21151;&#33021;&#24182;&#20687;&#20154;&#31867;&#29992;&#25143;&#19968;&#26679;&#34892;&#20107;&#12290;&#24179;&#21488;&#30340;&#21151;&#33021;&#25110;&#29305;&#28857;&#22240;&#24212;&#29992;&#24179;&#21488;&#25110;&#29983;&#21629;&#21608;&#26399;&#32780;&#24322;&#65292;&#22240;&#27492;&#38656;&#35201;&#36825;&#20123;&#26426;&#22120;&#20154;&#20855;&#22791;&#36866;&#24212;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#24179;&#21488;&#19978;&#30340;&#26426;&#22120;&#20154;&#21487;&#20197;&#19982;&#20154;&#31867;&#25110;&#20854;&#20182;&#36719;&#20214;&#20195;&#29702;&#21512;&#20316;&#24037;&#20316;&#25110;&#23398;&#20064;&#29305;&#23450;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#35821;&#35328;&#22788;&#29702;&#21644;&#39044;&#27979;&#20043;&#22806;&#65292;&#29616;&#20170;&#30340;&#26426;&#22120;&#20154;&#65292;&#29305;&#21035;&#26159;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36828;&#26410;&#36798;&#21040;&#22797;&#26434;&#21830;&#19994;&#20449;&#24687;&#31995;&#32479;&#20869;&#30340;&#20154;&#31867;&#29992;&#25143;&#34892;&#20026;&#27700;&#24179;&#12290;&#23427;&#20204;&#32570;&#20047;&#22312;&#36825;&#31181;&#34394;&#25311;&#29615;&#22659;&#20013;&#24863;&#30693;&#21644;&#34892;&#21160;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#24320;&#21457;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#38382;&#39064;&#24182;&#30740;&#31350;&#20102;&#26500;&#24605;&#36719;&#20214;&#26426;&#22120;&#20154;&#26550;&#26500;&#30340;&#20551;&#35774;&#65292;&#38024;&#23545;&#24320;&#21457;&#20855;&#26377;&#22797;&#26434;&#35748;&#30693;&#33021;&#21147;&#30340;&#26426;&#22120;&#20154;&#30340;&#26174;&#33879;&#26550;&#26500;&#30740;&#31350;&#25361;&#25112;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software bots operating in multiple virtual digital platforms must understand the platforms' affordances and behave like human users. Platform affordances or features differ from one application platform to another or through a life cycle, requiring such bots to be adaptable. Moreover, bots in such platforms could cooperate with humans or other software agents for work or to learn specific behavior patterns. However, present-day bots, particularly chatbots, other than language processing and prediction, are far from reaching a human user's behavior level within complex business information systems. They lack the cognitive capabilities to sense and act in such virtual environments, rendering their development a challenge to artificial general intelligence research. In this study, we problematize and investigate assumptions in conceptualizing software bot architecture by directing attention to significant architectural research challenges in developing cognitive bots endowed with complex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Chain-of-Thought Hub &#30340;&#24320;&#28304;&#35780;&#20272;&#22871;&#20214;&#65292;&#30446;&#30340;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#26159;&#20026;&#20102;&#36861;&#36394;LLMs&#36827;&#23637;&#32780;&#32534;&#21046;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#22522;&#20934;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#25512;&#29702;&#33021;&#21147;&#30456;&#20851;&#65292;&#32780; Claude-v1.3 &#26159;&#36804;&#20170;&#20026;&#27490;&#25512;&#29702;&#33021;&#21147;&#26368;&#24378;&#30340;LLM&#12290;</title><link>http://arxiv.org/abs/2305.17306</link><description>&lt;p&gt;
&#8220;Chain-of-Thought Hub: &#36830;&#32493;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#34920;&#29616;&#30340;&#21162;&#21147;&#8221;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance. (arXiv:2305.17306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Chain-of-Thought Hub &#30340;&#24320;&#28304;&#35780;&#20272;&#22871;&#20214;&#65292;&#30446;&#30340;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#26159;&#20026;&#20102;&#36861;&#36394;LLMs&#36827;&#23637;&#32780;&#32534;&#21046;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#22522;&#20934;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#25512;&#29702;&#33021;&#21147;&#30456;&#20851;&#65292;&#32780; Claude-v1.3 &#26159;&#36804;&#20170;&#20026;&#27490;&#25512;&#29702;&#33021;&#21147;&#26368;&#24378;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#20294;&#20063;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; Chain-of-Thought Hub&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20043;&#25152;&#20197;&#23545;&#36825;&#20010;&#35774;&#32622;&#24863;&#20852;&#36259;&#65292;&#26159;&#22240;&#20026; (1) &#20174; GPT &#21644; PaLM &#27169;&#22411;&#23478;&#26063;&#30340;&#34892;&#20026;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22797;&#26434;&#30340;&#25512;&#29702;&#24456;&#21487;&#33021;&#26159;&#19968;&#20010;&#26356;&#24369;&#21644;&#26356;&#24378;&#30340;LLMs&#20043;&#38388;&#30340;&#20851;&#38190;&#21306;&#21035;&#65307; (2) &#25105;&#20204;&#39044;&#35265;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#25104;&#20026;&#19979;&#19968;&#20195;&#35745;&#31639;&#24179;&#21488;&#65292;&#24182;&#20419;&#36827;&#22522;&#20110;LLM&#30340;&#26032;&#24212;&#29992;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#36825;&#33258;&#28982;&#38656;&#35201;&#22522;&#30784;&#27169;&#22411;&#25191;&#34892;&#24120;&#24120;&#28041;&#21450;&#35821;&#35328;&#21644;&#36923;&#36753;&#25805;&#20316;&#32452;&#21512;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#32534;&#21046;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#22522;&#20934;&#65292;&#20197;&#36319;&#36394;LLMs&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30446;&#21069;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;(1) &#27169;&#22411;&#35268;&#27169;&#26174;&#28982;&#19982;&#25512;&#29702;&#33021;&#21147;&#30456;&#20851;&#65307;(2) &#25130;&#33267;2023&#24180;5&#26376;&#65292;Claude-v1.3 &#26159;&#36804;&#20170;&#20026;&#27490;&#25512;&#29702;&#33021;&#21147;&#26368;&#24378;&#30340;LLM &#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21019;&#36896;&#26356;&#21152;&#31283;&#20581;&#12289;&#39640;&#25928;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#21457;&#29616;&#37325;&#22797;&#23376;&#30005;&#36335;&#21644;&#20998;&#26512;&#26524;&#34631;&#30340;&#33322;&#21521;&#26041;&#21521;&#30005;&#36335;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#36830;&#25509;&#27169;&#24335;&#21644;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#22914;&#20309;&#36827;&#19968;&#27493;&#25193;&#23637;&#29616;&#26377;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.17300</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#21019;&#36896;&#36830;&#25509;&#32452;&#38480;&#21046;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#24378;&#22823;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#36866;&#24212;&#24615;&#24378;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence. (arXiv:2305.17300v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17300
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21019;&#36896;&#26356;&#21152;&#31283;&#20581;&#12289;&#39640;&#25928;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#21457;&#29616;&#37325;&#22797;&#23376;&#30005;&#36335;&#21644;&#20998;&#26512;&#26524;&#34631;&#30340;&#33322;&#21521;&#26041;&#21521;&#30005;&#36335;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#36830;&#25509;&#27169;&#24335;&#21644;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#22914;&#20309;&#36827;&#19968;&#27493;&#25193;&#23637;&#29616;&#26377;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36793;&#32536;&#19978;&#30340;&#39640;&#25928;&#23398;&#20064;&#65288;&#23454;&#29616;&#36866;&#24212;&#24615;&#24378;&#12289;&#20302;&#22797;&#26434;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65289;&#20173;&#28982;&#26159;&#22269;&#38450;&#21644;&#21830;&#19994;&#24212;&#29992;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#25105;&#20204;&#26500;&#24819;&#20102;&#20351;&#29992;&#22823;&#22411;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25429;&#33719;&#31070;&#32463;&#20803;&#21644;&#31361;&#35302;&#36830;&#25509;&#30340;&#22823;&#33041;&#22320;&#22270;&#65292;&#26469;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#22312;&#35813;&#27969;&#31243;&#32467;&#26500;&#20869;&#36861;&#27714;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the progress in deep learning networks, efficient learning at the edge (enabling adaptable, low-complexity machine learning solutions) remains a critical need for defense and commercial applications. We envision a pipeline to utilize large neuroimaging datasets, including maps of the brain which capture neuron and synapse connectivity, to improve machine learning approaches. We have pursued different approaches within this pipeline structure. First, as a demonstration of data-driven discovery, the team has developed a technique for discovery of repeated subcircuits, or motifs. These were incorporated into a neural architecture search approach to evolve network architectures. Second, we have conducted analysis of the heading direction circuit in the fruit fly, which performs fusion of visual and angular velocity features, to explore augmenting existing computational models with new insight. Our team discovered a novel pattern of connectivity, implemented a new model, and demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21307;&#30103;&#24212;&#29992;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#26641;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#29992;&#23427;&#26469;&#30830;&#23450;&#26641;&#30340;&#31283;&#23450;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#31283;&#23450;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#31283;&#23450;&#24615;&#12289;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.17299</link><description>&lt;p&gt;
&#25552;&#39640;&#20915;&#31574;&#26641;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Stability in Decision Tree Models. (arXiv:2305.17299v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21307;&#30103;&#24212;&#29992;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#26641;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#29992;&#23427;&#26469;&#30830;&#23450;&#26641;&#30340;&#31283;&#23450;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#31283;&#23450;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#31283;&#23450;&#24615;&#12289;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#32467;&#26500;&#26131;&#20110;&#29702;&#35299;&#65292;&#20915;&#31574;&#26641;&#36890;&#24120;&#22312;&#38656;&#35201;&#21487;&#35299;&#37322;&#24615;&#30340;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#36817;&#26399;&#30340;&#24037;&#20316;&#38598;&#20013;&#20110;&#25913;&#36827;&#20915;&#31574;&#26641;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#39044;&#27979;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65307;&#28982;&#32780;&#65292;&#20854;&#19981;&#31283;&#23450;&#24615;&#34429;&#28982;&#26377;&#20805;&#20998;&#30340;&#35760;&#24405;&#65292;&#20294;&#21364;&#24471;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#38469;&#30340;&#21307;&#30103;&#24212;&#29992;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#21270;&#20915;&#31574;&#26641;&#27169;&#22411;&#30340;&#19968;&#23567;&#27493;&#12290;&#30001;&#20110;&#31283;&#23450;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#22312;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#26641;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#30830;&#23450;&#26641;&#30340;&#31283;&#23450;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#31283;&#23450;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#24182;&#35843;&#26597;&#20102;&#20915;&#31574;&#26641;&#27169;&#22411;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#65292;&#21253;&#25324;&#22312;&#31283;&#23450;&#24615;&#12289;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20845;&#20010;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#25152;&#25552;&#35758;&#26041;&#27861;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to their inherently interpretable structure, decision trees are commonly used in applications where interpretability is essential. Recent work has focused on improving various aspects of decision trees, including their predictive power and robustness; however, their instability, albeit well-documented, has been addressed to a lesser extent. In this paper, we take a step towards the stabilization of decision tree models through the lens of real-world health care applications due to the relevance of stability and interpretability in this space. We introduce a new distance metric for decision trees and use it to determine a tree's level of stability. We propose a novel methodology to train stable decision trees and investigate the existence of trade-offs that are inherent to decision tree models - including between stability, predictive power, and interpretability. We demonstrate the value of the proposed methodology through an extensive quantitative and qualitative analysis of six 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21547;&#26377;&#19981;&#30830;&#23450;&#24615;&#38556;&#30861;&#29289;&#30340;&#38750;&#20984;&#29615;&#22659;&#20013;&#30340;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36890;&#36807;&#39118;&#38505;&#36718;&#24275;&#30340;&#27010;&#24565;&#23558;&#39118;&#38505;&#26377;&#30028;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17291</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#38750;&#20984;&#29615;&#22659;&#20013;&#30340;&#20984;&#39118;&#38505;&#26377;&#30028;&#36830;&#32493;&#26102;&#38388;&#36712;&#36857;&#35268;&#21010;&#21644;&#31649;&#36947;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Convex Risk Bounded Continuous-Time Trajectory Planning and Tube Design in Uncertain Nonconvex Environments. (arXiv:2305.17291v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21547;&#26377;&#19981;&#30830;&#23450;&#24615;&#38556;&#30861;&#29289;&#30340;&#38750;&#20984;&#29615;&#22659;&#20013;&#30340;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36890;&#36807;&#39118;&#38505;&#36718;&#24275;&#30340;&#27010;&#24565;&#23558;&#39118;&#38505;&#26377;&#30028;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21547;&#26377;&#20855;&#26377;&#27010;&#29575;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#20960;&#20309;&#24418;&#29366;&#30340;&#38556;&#30861;&#29289;&#30340;&#19981;&#30830;&#23450;&#38750;&#20984;&#38745;&#24577;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#24102;&#26377;&#26377;&#30028;&#39118;&#38505;&#30340;&#36712;&#36857;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23547;&#25214;&#35268;&#21010;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#35777;&#26377;&#30028;&#39118;&#38505;&#30340;&#36830;&#32493;&#26102;&#38388;&#36712;&#36857;&#12290;&#39118;&#38505;&#34987;&#23450;&#20041;&#20026;&#19982;&#19981;&#30830;&#23450;&#38556;&#30861;&#29289;&#30896;&#25758;&#30340;&#27010;&#29575;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#39118;&#38505;&#26377;&#30028;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#35201;&#20040;&#20165;&#38480;&#20110;&#39640;&#26031;&#19981;&#30830;&#23450;&#24615;&#21644;&#20984;&#38556;&#30861;&#29289;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#38656;&#35201;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#21644;&#26102;&#38388;&#31163;&#25955;&#21270;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#39118;&#38505;&#26377;&#30028;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#39118;&#38505;&#36718;&#24275;&#30340;&#27010;&#24565;&#23558;&#39118;&#38505;&#26377;&#30028;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#39118;&#38505;&#36718;&#24275;&#26159;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#25152;&#26377;&#20855;&#26377;&#20445;&#35777;&#26377;&#30028;&#39118;&#38505;&#30340;&#28857;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the trajectory planning problem in uncertain nonconvex static and dynamic environments that contain obstacles with probabilistic location, size, and geometry. To address this problem, we provide a risk bounded trajectory planning method that looks for continuous-time trajectories with guaranteed bounded risk over the planning time horizon. Risk is defined as the probability of collision with uncertain obstacles. Existing approaches to address risk bounded trajectory planning problems either are limited to Gaussian uncertainties and convex obstacles or rely on sampling-based methods that need uncertainty samples and time discretization. To address the risk bounded trajectory planning problem, we leverage the notion of risk contours to transform the risk bounded planning problem into a deterministic optimization problem. Risk contours are the set of all points in the uncertain environment with guaranteed bounded risk. The obtained deterministic optimization is, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#27493;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#25991;&#26723;&#35299;&#26512;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#21033;&#29992;&#28304;-&#30446;&#26631;&#23545;&#40784;&#24182;&#32422;&#26463;&#35299;&#30721;&#20197;&#20445;&#35777;&#37325;&#21472;&#31383;&#21475;&#30340;&#21516;&#27493;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#22312;AMR 3.0&#30340;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#20102;&#39640;&#36136;&#37327;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17273</link><description>&lt;p&gt;
&#28369;&#21160;&#12289;&#32422;&#26463;&#12289;&#35299;&#26512;&#12289;&#37325;&#22797;&#65306;&#36866;&#29992;&#20110;&#25991;&#26723; AMR &#35299;&#26512;&#30340;&#21516;&#27493;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Slide, Constrain, Parse, Repeat: Synchronous SlidingWindows for Document AMR Parsing. (arXiv:2305.17273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#27493;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#25991;&#26723;&#35299;&#26512;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#21033;&#29992;&#28304;-&#30446;&#26631;&#23545;&#40784;&#24182;&#32422;&#26463;&#35299;&#30721;&#20197;&#20445;&#35777;&#37325;&#21472;&#31383;&#21475;&#30340;&#21516;&#27493;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#22312;AMR 3.0&#30340;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#20102;&#39640;&#36136;&#37327;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#36229;&#36807;Transformer&#36755;&#20837;&#31383;&#21475;&#22823;&#23567;&#30340;&#19978;&#19979;&#25991;&#30340;&#20248;&#32654;&#26041;&#24335;&#65292;&#20363;&#22914;&#22788;&#29702;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26723;&#35299;&#26512;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#36716;&#31227;&#21477;&#27861;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#22312;&#28304;&#21644;&#30446;&#26631;&#20043;&#38388;&#23454;&#29616;&#21516;&#27493;&#28369;&#21160;&#31383;&#21475;&#26469;&#23454;&#29616;&#35299;&#26512;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26426;&#26500;&#21270;BART&#19978;&#25193;&#23637;&#26469;&#24320;&#21457;&#25991;&#26723;&#32423;AMR&#30340;oracle&#21644;&#35299;&#26512;&#22120;&#65292;&#20197;&#21033;&#29992;&#28304;-&#30446;&#26631;&#23545;&#40784;&#24182;&#32422;&#26463;&#35299;&#30721;&#20197;&#20445;&#35777;&#37325;&#21472;&#31383;&#21475;&#30340;&#21516;&#27493;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;3.0&#35821;&#26009;&#24211;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;oracle&#21644;&#35299;&#26512;&#22120;&#12290;&#22312;AMR 3.0&#30340;&#22810;&#21477;&#23376;&#24320;&#21457;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36716;&#31227;oracle&#20165;&#20002;&#22833;&#20102;8&#65285;&#30340;&#37329;&#21477;&#38469;&#38142;&#25509;&#65292;&#23613;&#31649;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#20135;&#29983;&#20102;&#19968;&#20010;&#20855;&#26377;&#21487;&#31649;&#29702;&#20869;&#23384;&#35201;&#27714;&#30340;&#39640;&#36136;&#37327;&#25991;&#26723;&#32423;&#35299;&#26512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sliding window approach provides an elegant way to handle contexts of sizes larger than the Transformer's input window, for tasks like language modeling. Here we extend this approach to the sequence-to-sequence task of document parsing. For this, we exploit recent progress in transition-based parsing to implement a parser with synchronous sliding windows over source and target. We develop an oracle and a parser for document-level AMR by expanding on Structured-BART such that it leverages source-target alignments and constrains decoding to guarantee synchronicity and consistency across overlapping windows. We evaluate our oracle and parser using the Abstract Meaning Representation (AMR) parsing 3.0 corpus. On the Multi-Sentence development set of AMR 3.0, we show that our transition oracle loses only 8\% of the gold cross-sentential links despite using a sliding window. In practice, this approach also results in a high-quality document-level parser with manageable memory requirement
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#21253;&#25324;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#33180;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17271</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#26041;&#27861;&#23454;&#29616;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss. (arXiv:2305.17271v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#21253;&#25324;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#33180;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36947;&#26816;&#27979;&#26159;&#36710;&#36742;&#23450;&#20301;&#30340;&#20851;&#38190;&#65292;&#26159;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#21644;&#35768;&#22810;&#26234;&#33021;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#21644;&#32858;&#21512;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#36710;&#36947;&#32447;&#21644;&#22270;&#20687;&#20013;&#20854;&#20182;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#24182;&#25552;&#21319;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#27169;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#22312;&#32454;&#35843;&#20998;&#21106;&#38454;&#27573;&#20013;&#65292;&#36830;&#32493;&#30340;&#22270;&#20687;&#24103;&#34987;&#29992;&#20316;&#36755;&#20837;&#65292;
&lt;/p&gt;
&lt;p&gt;
Lane detection is crucial for vehicle localization which makes it the foundation for automated driving and many intelligent and advanced driving assistant systems. Available vision-based lane detection methods do not make full use of the valuable features and aggregate contextual information, especially the interrelationships between lane lines and other regions of the images in continuous frames. To fill this research gap and upgrade lane detection performance, this paper proposes a pipeline consisting of self pre-training with masked sequential autoencoders and fine-tuning with customized PolyLoss for the end-to-end neural network models using multi-continuous image frames. The masked sequential autoencoders are adopted to pre-train the neural network models with reconstructing the missing pixels from a random masked image as the objective. Then, in the fine-tuning segmentation phase where lane detection segmentation is performed, the continuous image frames are served as the inputs,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31867;&#27604;&#25512;&#29702;&#33021;&#21542;&#23454;&#29616;&#23545;&#21487;&#32452;&#21512;&#35270;&#35273;&#21050;&#28608;&#25104;&#20998;&#30340;&#19978;&#19979;&#25991;&#20869;&#32452;&#21512;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#31867;&#27604;&#25512;&#29702;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550; Im-Promptu&#12290;&#20351;&#29992; Im-Promptu &#21487;&#20197;&#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#32452;&#21512;&#27700;&#24179;&#30340;&#20195;&#29702;&#65292;&#21253;&#25324;&#30690;&#37327;&#34920;&#31034;&#12289;&#34917;&#19969;&#34920;&#31034;&#21644;&#29289;&#20307;&#27133;&#12290;</title><link>http://arxiv.org/abs/2305.17262</link><description>&lt;p&gt;
Im-Promptu: &#20174;&#22270;&#20687;&#25552;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Im-Promptu: In-Context Composition from Image Prompts. (arXiv:2305.17262v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31867;&#27604;&#25512;&#29702;&#33021;&#21542;&#23454;&#29616;&#23545;&#21487;&#32452;&#21512;&#35270;&#35273;&#21050;&#28608;&#25104;&#20998;&#30340;&#19978;&#19979;&#25991;&#20869;&#32452;&#21512;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#31867;&#27604;&#25512;&#29702;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550; Im-Promptu&#12290;&#20351;&#29992; Im-Promptu &#21487;&#20197;&#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#32452;&#21512;&#27700;&#24179;&#30340;&#20195;&#29702;&#65292;&#21253;&#25324;&#30690;&#37327;&#34920;&#31034;&#12289;&#34917;&#19969;&#34920;&#31034;&#21644;&#29289;&#20307;&#27133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#28436;&#31034;&#20013;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#36825;&#31181;&#38544;&#21547;&#30340;&#20219;&#21153;&#29702;&#35299;&#34920;&#26126;&#65292;&#21333;&#35789;&#20196;&#29260;&#19978;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#33021;&#22312;&#31867;&#27604;&#25512;&#29702;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#31867;&#27604;&#25512;&#29702;&#26159;&#21542;&#33021;&#23454;&#29616;&#23545;&#21487;&#32452;&#21512;&#35270;&#35273;&#21050;&#28608;&#25104;&#20998;&#30340;&#19978;&#19979;&#25991;&#20869;&#32452;&#21512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#20197;&#27979;&#35797;&#35270;&#35273;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#30340;&#27867;&#21270;&#23646;&#24615;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#22522;&#20110;&#31867;&#27604;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#30340;&#27010;&#24565;&#65292;&#24182;&#29992;&#23427;&#26469;&#35774;&#35745;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; Im-Promptu&#12290;&#34429;&#28982;&#35821;&#35328;&#30340;&#25152;&#38656;&#20196;&#29260;&#31890;&#24230;&#24050;&#32463;&#24471;&#21040;&#20102;&#20805;&#20998;&#35777;&#23454;&#65292;&#20294;&#29992;&#20110;&#23454;&#29616;&#35270;&#35273;&#21050;&#28608;&#20869;&#19978;&#19979;&#25991;&#27867;&#21270;&#30340;&#36866;&#24403;&#32452;&#21512;&#31890;&#24230;&#36890;&#24120;&#26410;&#32463;&#25351;&#23450;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992; Im-Promptu &#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#32452;&#21512;&#27700;&#24179;&#30340;&#20195;&#29702;&#65292;&#21253;&#25324;&#30690;&#37327;&#34920;&#31034;&#12289;&#34917;&#19969;&#34920;&#31034;&#21644;&#29289;&#20307;&#27133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are few-shot learners that can solve diverse tasks from a handful of demonstrations. This implicit understanding of tasks suggests that the attention mechanisms over word tokens may play a role in analogical reasoning. In this work, we investigate whether analogical reasoning can enable in-context composition over composable elements of visual stimuli. First, we introduce a suite of three benchmarks to test the generalization properties of a visual in-context learner. We formalize the notion of an analogy-based in-context learner and use it to design a meta-learning framework called Im-Promptu. Whereas the requisite token granularity for language is well established, the appropriate compositional granularity for enabling in-context generalization in visual stimuli is usually unspecified. To this end, we use Im-Promptu to train multiple agents with different levels of compositionality, including vector representations, patch representations, and object slots. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20154;&#31867;&#23454;&#39564;&#30740;&#31350;&#20102;&#24418;&#24335;&#35268;&#33539;&#26159;&#21542;&#21487;&#34987;&#20154;&#29702;&#35299;&#21644;&#29992;&#20110;&#26816;&#26597;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#35828;&#26126;&#26377;&#25928;&#24615;&#12289;&#34987;&#35797;&#29087;&#24713;&#24230;&#21644;&#25945;&#32946;&#27700;&#24179;&#26159;&#24433;&#21709;&#39564;&#35777;&#27491;&#30830;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21442;&#19982;&#32773;&#23384;&#22312;&#30830;&#35748;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.17258</link><description>&lt;p&gt;
STL&#65306;&#29992;&#20110;&#31995;&#32479;&#39564;&#35777;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26840;&#25163;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
STL: Surprisingly Tricky Logic (for System Validation). (arXiv:2305.17258v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20154;&#31867;&#23454;&#39564;&#30740;&#31350;&#20102;&#24418;&#24335;&#35268;&#33539;&#26159;&#21542;&#21487;&#34987;&#20154;&#29702;&#35299;&#21644;&#29992;&#20110;&#26816;&#26597;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#35828;&#26126;&#26377;&#25928;&#24615;&#12289;&#34987;&#35797;&#29087;&#24713;&#24230;&#21644;&#25945;&#32946;&#27700;&#24179;&#26159;&#24433;&#21709;&#39564;&#35777;&#27491;&#30830;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21442;&#19982;&#32773;&#23384;&#22312;&#30830;&#35748;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21457;&#23637;&#24418;&#24335;&#21270;&#26041;&#27861;&#25216;&#26415;&#20197;&#25351;&#23450;&#25110;&#23398;&#20064;&#33258;&#20027;&#31995;&#32479;&#34892;&#20026;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#22522;&#20110;&#36825;&#26679;&#19968;&#31181;&#20449;&#24565;&#65292;&#21363;&#24418;&#24335;&#21270;&#35828;&#26126;&#23545;&#20110;&#26816;&#26597;&#31995;&#32479;&#26102;&#20154;&#31867;&#26159;&#21487;&#35299;&#37322;&#21644;&#26377;&#29992;&#30340;&#12290;&#23613;&#31649;&#36825;&#31181;&#20551;&#35774;&#32463;&#24120;&#34987;&#26029;&#35328;&#65292;&#20294;&#24456;&#23569;&#34987;&#27979;&#35797;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#31867;&#23454;&#39564;&#65288;N = 62&#65289;&#65292;&#28151;&#21512;&#20102;&#20043;&#21069;&#29087;&#24713;&#24418;&#24335;&#21270;&#26041;&#27861;&#20197;&#21450;&#19981;&#29087;&#24713;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;&#20154;&#65292;&#35201;&#27714;&#20182;&#20204;&#39564;&#35777;&#19968;&#32452;STL&#32422;&#26463;&#26159;&#21542;&#33021;&#35753;&#19968;&#20010;&#20195;&#29702;&#20154;&#22312;&#32593;&#26684;&#19990;&#30028;&#30340;&#26071;&#24092;&#20105;&#22842;&#29615;&#22659;&#20013;&#36991;&#20813;&#21361;&#38505;&#24182;&#23436;&#25104;&#20219;&#21153;&#12290;&#39564;&#35777;&#20934;&#30830;&#24230;&#20026;$45\%\pm20\%$&#65288;&#24179;&#22343;$\pm$&#26631;&#20934;&#20559;&#24046;&#65289;&#12290;&#20107;&#23454;&#19978;&#30340;&#35828;&#26126;&#26377;&#25928;&#24615;&#12289;&#34987;&#35797;&#29087;&#24713;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;&#31243;&#24230;&#20197;&#21450;&#34987;&#35797;&#30340;&#25945;&#32946;&#27700;&#24179;&#26159;&#20915;&#23450;&#39564;&#35777;&#27491;&#30830;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#21442;&#19982;&#32773;&#34920;&#29616;&#20986;&#30830;&#35748;&#20559;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#26377;&#25928;&#35828;&#26126;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#32780;&#23545;&#26080;&#25928;&#35828;&#26126;&#30340;&#20934;&#30830;&#24615;&#21017;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much of the recent work developing formal methods techniques to specify or learn the behavior of autonomous systems is predicated on a belief that formal specifications are interpretable and useful for humans when checking systems. Though frequently asserted, this assumption is rarely tested. We performed a human experiment (N = 62) with a mix of people who were and were not familiar with formal methods beforehand, asking them to validate whether a set of signal temporal logic (STL) constraints would keep an agent out of harm and allow it to complete a task in a gridworld capture-the-flag setting. Validation accuracy was $45\% \pm 20\%$ (mean $\pm$ standard deviation). The ground-truth validity of a specification, subjects' familiarity with formal methods, and subjects' level of education were found to be significant factors in determining validation correctness. Participants exhibited an affirmation bias, causing significantly increased accuracy on valid specifications, but significan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17256</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26159;&#25042;&#24816;&#30340;&#23398;&#20064;&#32773;&#65306;&#20998;&#26512;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning. (arXiv:2305.17256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20854;&#20013;LLM&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;-&#26631;&#31614;&#23545;&#65288;&#25552;&#31034;&#65289;&#30340;&#26465;&#20214;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#25105;&#20204;&#23545;&#24433;&#21709;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31283;&#20581;&#24615;&#30340;&#22240;&#32032;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;LLM&#23545;&#25552;&#31034;&#20869;&#25463;&#24452;&#25110;&#20551;&#30456;&#20851;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#24357;&#34917;&#36825;&#19968;&#30693;&#35782;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#31867;&#21644;&#25277;&#21462;&#20219;&#21153;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLM&#26159;&#8220;&#25042;&#24816;&#23398;&#20064;&#32773;&#8221;&#30340;&#20107;&#23454;&#65292;&#23427;&#24448;&#24448;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#26469;&#33719;&#21462;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#21363;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are "lazy learners" that tend to exploit shortcuts in prompts for downstream tasks. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#22870;&#21169;&#30340;&#20219;&#21153;&#38388;&#36827;&#34892;&#34892;&#20026;&#36801;&#31227;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#19968;&#20123;&#38543;&#26426;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#33021;&#22815;&#26263;&#21547;&#38271;&#26399;&#29615;&#22659;&#21160;&#24577;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#30340;&#35268;&#21010;&#25216;&#26415;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17250</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#30340;&#33258;&#30417;&#30563;&#22686;&#24378;&#23398;&#20064;&#23454;&#29616;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Reinforcement Learning that Transfers using Random Features. (arXiv:2305.17250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17250
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#22870;&#21169;&#30340;&#20219;&#21153;&#38388;&#36827;&#34892;&#34892;&#20026;&#36801;&#31227;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#19968;&#20123;&#38543;&#26426;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#33021;&#22815;&#26263;&#21547;&#38271;&#26399;&#29615;&#22659;&#21160;&#24577;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#30340;&#35268;&#21010;&#25216;&#26415;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35299;&#20915;&#20855;&#26377;&#39640;&#32500;&#35266;&#27979;&#21644;&#38271;&#26399;&#20915;&#31574;&#26041;&#26696;&#30340;&#21333;&#20219;&#21153;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#38590;&#20197;&#27178;&#36328;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21017;&#23398;&#20064;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#33258;&#28982;&#22320;&#23454;&#29616;&#20102;&#19981;&#21516;&#22870;&#21169;&#20989;&#25968;&#30340;&#36801;&#31227;&#65292;&#20294;&#30001;&#20110;&#35823;&#24046;&#30340;&#32047;&#31215;&#32780;&#38590;&#20197;&#36866;&#24212;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;&#20026;&#20102;&#23454;&#29616;&#20004;&#32773;&#20860;&#39038;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22312;&#20855;&#26377;&#19981;&#21516;&#22870;&#21169;&#30340;&#20219;&#21153;&#38388;&#36827;&#34892;&#34892;&#20026;&#36801;&#31227;&#65292;&#21516;&#26102;&#36991;&#24320;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#33258;&#30001;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#29992;&#19968;&#20123;&#38543;&#26426;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#33021;&#22815;&#26263;&#21547;&#38271;&#26399;&#29615;&#22659;&#21160;&#24577;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#30340;&#35268;&#21010;&#25216;&#26415;&#65288;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65289;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-free reinforcement learning algorithms have exhibited great potential in solving single-task sequential decision-making problems with high-dimensional observations and long horizons, but are known to be hard to generalize across tasks. Model-based RL, on the other hand, learns task-agnostic models of the world that naturally enables transfer across different reward functions, but struggles to scale to complex environments due to the compounding error. To get the best of both worlds, we propose a self-supervised reinforcement learning method that enables the transfer of behaviors across tasks with different rewards, while circumventing the challenges of model-based RL. In particular, we show self-supervised pre-training of model-free reinforcement learning with a number of random features as rewards allows implicit modeling of long-horizon environment dynamics. Then, planning techniques like model-predictive control using these implicit models enable fast adaptation to problems wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; Vision Transformer &#21387;&#32553;&#26041;&#27861;&#65292;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#19978;&#36827;&#34892;&#20102;&#26032;&#30340;&#25506;&#31350;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17235</link><description>&lt;p&gt;
COMCAT&#65306;&#39640;&#25928;&#21387;&#32553;&#21644;&#33258;&#23450;&#20041;&#27880;&#24847;&#21147;&#35270;&#35273;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models. (arXiv:2305.17235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; Vision Transformer &#21387;&#32553;&#26041;&#27861;&#65292;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#19978;&#36827;&#34892;&#20102;&#26032;&#30340;&#25506;&#31350;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#20363;&#22914;Vision Transformer&#65288;ViT&#65289;&#21450;&#20854;&#21464;&#20307;&#65292;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#20852;&#30340;&#26550;&#26500;&#23384;&#22312;&#30528;&#27169;&#22411;&#23610;&#23544;&#22823;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#31350;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#20197;&#20016;&#23500;&#33719;&#21462;&#32039;&#20945;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#24037;&#20855;&#38598;&#12290;&#22522;&#20110;&#23545;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#30340;&#26032;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;ViT&#21387;&#32553;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#26041;&#27861;&#12290;&#22312;ImageNet&#19978;&#23545;DeiT-small&#21644;DeiT-base&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#21363;&#20351;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#27604;&#29616;&#26377;&#26041;&#27861;&#39640;0.45&#65285;&#21644;0.76&#65285;&#30340;top-1&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods. For compressing DeiT-small and DeiT-base models on ImageNet, our proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters. Our fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.17225</link><description>&lt;p&gt;
&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Component Analysis. (arXiv:2305.17225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#30340;&#30446;&#26631;&#26159;&#20174;&#28151;&#21512;&#35266;&#27979;&#21040;&#30340;&#21464;&#37327;&#20013;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#32780;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#24378;&#30456;&#20851;&#24615;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#20197;&#21450;&#32534;&#30721;&#23427;&#20204;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26410;&#30693;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65292;&#31216;&#20026;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#12290;CauCA&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;ICA&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#23545;&#28508;&#22312;&#25104;&#20998;&#20043;&#38388;&#30340;&#22240;&#26524;&#20381;&#36182;&#24314;&#27169;&#65292;&#20063;&#26159;CRL&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;&#19982;CRL&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#65292;&#20165;&#20851;&#27880;&#20110;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#12290;&#25152;&#26377;&#20851;&#20110;CauCA&#22238;&#25910;&#22522;&#30784;&#30495;&#30456;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;CRL&#65292;&#32780;&#21487;&#33021;&#24615;&#32467;&#26524;&#21487;&#20197;&#20316;&#20026;&#25193;&#23637;CRL&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#23558;&#20174;&#23545;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#23454;&#26045;&#19981;&#21516;&#31867;&#22411;&#24178;&#39044;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#24449;CauCA&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#25216;&#26415;&#20013;&#25552;&#31034;&#25968;&#37327;&#23545;&#24494;&#35843;&#24615;&#33021;&#21644;&#33258;&#25105;&#20851;&#27880;&#25805;&#20316;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;Prompt Condensation&#65288;PC&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#23558;&#25552;&#31034;&#25968;&#37327;&#20943;&#23569;&#32422;70&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17223</link><description>&lt;p&gt;
&#25105;&#20204;&#30495;&#30340;&#38656;&#35201;&#22823;&#37327;&#30340;&#35270;&#35273;&#25552;&#31034;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do We Really Need a Large Number of Visual Prompts?. (arXiv:2305.17223v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#25216;&#26415;&#20013;&#25552;&#31034;&#25968;&#37327;&#23545;&#24494;&#35843;&#24615;&#33021;&#21644;&#33258;&#25105;&#20851;&#27880;&#25805;&#20316;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;Prompt Condensation&#65288;PC&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#23558;&#25552;&#31034;&#25968;&#37327;&#20943;&#23569;&#32422;70&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#19978;&#36866;&#24212;&#27169;&#22411;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#21487;&#35270;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#23558;&#21487;&#23398;&#20064;&#25552;&#31034;&#21152;&#21040;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#19982;&#20840;&#32593;&#32476;&#21442;&#25968;&#30340;&#35757;&#32451;&#30456;&#27604;&#65292;&#26174;&#31034;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;VPT&#22686;&#21152;&#20102;&#36755;&#20837;&#26631;&#35760;&#30340;&#25968;&#37327;&#65292;&#23548;&#33268;&#39069;&#22806;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25552;&#31034;&#25968;&#37327;&#23545;&#35270;&#35273;&#21464;&#25442;&#22120;&#20307;&#31995;&#32467;&#26500;&#20013;&#24494;&#35843;&#24615;&#33021;&#21644;&#33258;&#25105;&#20851;&#27880;&#25805;&#20316;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#34920;&#26126;&#28155;&#21152;&#26356;&#22810;&#25552;&#31034;&#19981;&#20250;&#23548;&#33268;&#32447;&#24615;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Prompt Condensation&#65288;PC&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#38450;&#27490;&#20351;&#29992;&#23569;&#37327;&#25552;&#31034;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#22312;FGVC&#21644;VTAB-1k&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#25552;&#31034;&#25968;&#37327;&#20943;&#23569;&#32422;70&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to increasing interest in adapting models on resource-constrained edges, parameter-efficient transfer learning has been widely explored. Among various methods, Visual Prompt Tuning (VPT), prepending learnable prompts to input space, shows competitive fine-tuning performance compared to training of full network parameters. However, VPT increases the number of input tokens, resulting in additional computational overhead. In this paper, we analyze the impact of the number of prompts on fine-tuning performance and self-attention operation in a vision transformer architecture. Through theoretical and empirical analysis we show that adding more prompts does not lead to linear performance improvement. Further, we propose a Prompt Condensation (PC) technique that aims to prevent performance degradation from using a small number of prompts. We validate our methods on FGVC and VTAB-1k tasks and show that our approach reduces the number of prompts by ~70% while maintaining accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#35774;&#32622;&#21644;&#26032;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26032;&#31639;&#27861;FedSQL&#21644;Lorar&#20248;&#20110;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#32622;&#30340;&#24378;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.17221</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#65306;&#20219;&#21153;&#24418;&#24335;&#65292;&#35780;&#20272;&#35774;&#32622;&#21450;&#26032;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms. (arXiv:2305.17221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#35774;&#32622;&#21644;&#26032;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26032;&#31639;&#27861;FedSQL&#21644;Lorar&#20248;&#20110;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#32622;&#30340;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#38024;&#23545;&#35821;&#20041;&#35299;&#26512;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#20854;&#35821;&#20041;&#20998;&#26512;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#32852;&#37030;&#23398;&#20064;&#27169;&#24335;&#23545;&#20110;&#37027;&#20123;&#27809;&#26377;&#36275;&#22815;&#35757;&#32451;&#25968;&#25454;&#26469;&#24320;&#21457;&#19968;&#20010;&#25968;&#25454;&#39269;&#39295;&#30340;&#31070;&#32463;&#35821;&#20041;&#20998;&#26512;&#22120;&#30340;&#23458;&#25143;&#31471;&#23588;&#20854;&#26377;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#35774;&#32622;&#26469;&#30740;&#31350;&#36825;&#20010;&#20219;&#21153;&#65292;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#21333;&#22495;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#20316;&#20026;&#23458;&#25143;&#31471;&#26469;&#24418;&#25104;&#19968;&#20010;&#29616;&#23454;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#29616;&#23454;&#35774;&#32622;&#20013;&#23458;&#25143;&#32676;&#30340;&#24322;&#36136;&#24615;&#24456;&#39640;&#65292;&#26631;&#20934;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#25152;&#20197;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;LOss Reduction Adjusted Re-weighting (Lorar)&#26469;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#65292;&#35813;&#26426;&#21046;&#22522;&#20110;&#23458;&#25143;&#31471;&#27599;&#36718;&#35757;&#32451;&#25439;&#22833;&#30340;&#20943;&#23569;&#24773;&#20917;&#26469;&#35843;&#33410;&#27599;&#20010;&#23458;&#25143;&#31471;&#23545;&#20110;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30452;&#35273;&#26159;&#65292;&#25439;&#22833;&#20943;&#23569;&#30340;&#36234;&#22810;&#65292;&#23458;&#25143;&#31471;&#31163;&#20840;&#23616;&#26368;&#20248;&#35299;&#23601;&#36234;&#36828;&#65292;&#20854;&#23545;&#27169;&#22411;&#26356;&#26032;&#30340;&#36129;&#29486;&#23601;&#24212;&#35813;&#36234;&#39640;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#24322;&#26500;&#25991;&#26412;&#21040;SQL FL&#35774;&#32622;&#30340;&#26032;&#30340;FL&#31639;&#27861;FedSQL&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FedSQL&#21644;Lorar&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;FL&#35774;&#32622;&#20013;&#30340;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a new task of federated learning (FL) for semantic parsing, where multiple clients collaboratively train one global model without sharing their semantic parsing data. By leveraging data from multiple clients, the FL paradigm can be especially beneficial for clients that have little training data to develop a data-hungry neural semantic parser on their own. We propose an evaluation setup to study this task, where we re-purpose widely-used single-domain text-to-SQL datasets as clients to form a realistic heterogeneous FL setting and collaboratively train a global model. As standard FL algorithms suffer from the high client heterogeneity in our realistic setup, we further propose a novel LOss Reduction Adjusted Re-weighting (Lorar) mechanism to mitigate the performance degradation, which adjusts each client's contribution to the global model update based on its training loss reduction during each round. Our intuition is that the larger the loss reduction, the further aw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#19990;&#30028;&#29366;&#24577;&#34920;&#31034;&#21644;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#24211;&#26469;&#24314;&#27169;&#35268;&#21010;&#20013;&#19990;&#30028;&#29366;&#24577;&#21644;&#26356;&#26032;&#30340;&#27491;&#24335;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.17208</link><description>&lt;p&gt;
&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#35268;&#21010;&#30340;&#33539;&#30068;&#34920;&#36798;&#35821;&#35328;&#21644;&#35745;&#31639;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Categorical Representation Language and Computational System for Knowledge-Based Planning. (arXiv:2305.17208v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#19990;&#30028;&#29366;&#24577;&#34920;&#31034;&#21644;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#24211;&#26469;&#24314;&#27169;&#35268;&#21010;&#20013;&#19990;&#30028;&#29366;&#24577;&#21644;&#26356;&#26032;&#30340;&#27491;&#24335;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#19968;&#38454;&#36923;&#36753;&#30340;&#32463;&#20856;&#35268;&#21010;&#34920;&#36798;&#35821;&#35328;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24314;&#27169;&#21644;&#35299;&#20915;&#35268;&#21010;&#38382;&#39064;&#65292;&#20294;&#26159;&#22312;&#22797;&#26434;&#30340;&#35268;&#21010;&#22330;&#26223;&#20013;&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#21040;&#38544;&#21547;&#30340;&#21069;&#25552;&#21644;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#21644;&#36716;&#25442;&#35268;&#21010;&#36807;&#31243;&#20013;&#19990;&#30028;&#29366;&#24577;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;C&#38598;&#21644;&#21452;&#25512;&#25104;&#37325;&#20889;&#65288;DPO&#65289;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#34920;&#31034;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#36825;&#20123;&#30693;&#35782;&#25903;&#25345;&#21508;&#20010;&#23618;&#27425;&#30340;&#39046;&#22495;&#25277;&#35937;&#12290;&#23427;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#26412;&#20307;&#23398;&#34920;&#31034;&#35859;&#35789;&#30340;&#35821;&#20041;&#65292;&#24182;&#22312;&#29366;&#24577;&#36716;&#25442;&#26102;&#20445;&#25345;&#35821;&#20041;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#24211;&#26469;&#24314;&#27169;&#35268;&#21010;&#20013;&#30340;&#19990;&#30028;&#29366;&#24577;&#21644;&#26356;&#26032;&#25552;&#20379;&#20102;&#27491;&#24335;&#30340;&#35821;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#33539;&#30068;&#35770;&#34920;&#31034;&#19982;&#32463;&#20856;&#35268;&#21010;&#34920;&#31034;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical planning representation languages based on first-order logic have been extensively used to model and solve planning problems, but they struggle to capture implicit preconditions and effects that arise in complex planning scenarios. To address this problem, we propose an alternative approach to representing and transforming world states during planning. Based on the category-theoretic concepts of $\mathsf{C}$-sets and double-pushout rewriting (DPO), our proposed representation can effectively handle structured knowledge about world states that support domain abstractions at all levels. It formalizes the semantics of predicates according to a user-provided ontology and preserves the semantics when transitioning between world states. This method provides a formal semantics for using knowledge graphs and relational databases to model world states and updates in planning. In this paper, we compare our category-theoretic representation with the classical planning representation. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#27779;&#23572;&#29595;&#38144;&#21806;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17201</link><description>&lt;p&gt;
&#22522;&#20110; Trend &#21644; Seasonality &#20998;&#35299;&#21644; LightGBM &#30340;&#38144;&#21806;&#39044;&#27979;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Sales Forecasting using Trend and Seasonality Decomposition with LightGBM. (arXiv:2305.17201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#27779;&#23572;&#29595;&#38144;&#21806;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#27779;&#23572;&#29595;&#21644;&#20122;&#39532;&#36874;&#31561;&#22823;&#22411;&#38646;&#21806;&#21830;&#38144;&#21806;&#39044;&#27979;&#30340;&#38590;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#20998;&#32452;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;MAPE&#65288;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65289;&#22312;&#27979;&#35797;&#38598;&#19978;&#21487;&#36798; 4.49%&#12290;
&lt;/p&gt;
&lt;p&gt;
Retail sales forecasting presents a significant challenge for large retailers such as Walmart and Amazon, due to the vast assortment of products, geographical location heterogeneity, seasonality, and external factors including weather, local economic conditions, and geopolitical events. Various methods have been employed to tackle this challenge, including traditional time series models, machine learning models, and neural network mechanisms, but the difficulty persists. Categorizing data into relevant groups has been shown to improve sales forecast accuracy as time series from different categories may exhibit distinct patterns. In this paper, we propose a new measure to indicate the unique impacts of the trend and seasonality components on a time series and suggest grouping time series based on this measure. We apply this approach to Walmart sales data from 01/29/2011 to 05/22/2016 and generate sales forecasts from 05/23/2016 to 06/19/2016. Our experiments show that the proposed strat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;MOMA-PPO&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#24182;&#20248;&#21270;&#26234;&#33021;&#20307;&#30340;&#25919;&#31574;&#65292;&#35299;&#20915;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#21644;&#31574;&#30053;&#24494;&#35843;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.17198</link><description>&lt;p&gt;
&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21327;&#35843;&#38382;&#39064;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem. (arXiv:2305.17198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17198
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;MOMA-PPO&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#24182;&#20248;&#21270;&#26234;&#33021;&#20307;&#30340;&#25919;&#31574;&#65292;&#35299;&#20915;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#21644;&#31574;&#30053;&#24494;&#35843;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22810;&#20010;&#26234;&#33021;&#20307;&#36827;&#34892;&#21327;&#35843;&#26159;&#19968;&#39033;&#37325;&#35201;&#38382;&#39064;&#65292;&#20855;&#26377;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#21338;&#24328;&#35770;&#12289;&#32463;&#27982;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26159;&#22312;&#32447;&#30340;&#65292;&#22240;&#27492;&#22312;&#25910;&#38598;&#26032;&#30340;&#20132;&#20114;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#25110;&#21361;&#38505;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#34429;&#28982;&#36825;&#20123;&#31639;&#27861;&#24212;&#35813;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#20294;&#36825;&#26679;&#20570;&#20250;&#24341;&#36215;&#31163;&#32447;&#21327;&#35843;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#24418;&#24335;&#21270;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#65288;SA&#65289;&#21644;&#31574;&#30053;&#24494;&#35843;&#65288;SFT&#65289;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#36825;&#26159;&#24403;&#21069;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#24494;&#35843;&#31574;&#30053;&#30340;&#21516;&#26102;&#25910;&#25947;&#20110;&#19968;&#20010;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;Model-based Offline Multi-Agent Proximal Policy Optimization&#65288;MOMA-PPO&#65289;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training multiple agents to coordinate is an important problem with applications in robotics, game theory, economics, and social sciences. However, most existing Multi-Agent Reinforcement Learning (MARL) methods are online and thus impractical for real-world applications in which collecting new interactions is costly or dangerous. While these algorithms should leverage offline data when available, doing so gives rise to the offline coordination problem. Specifically, we identify and formalize the strategy agreement (SA) and the strategy fine-tuning (SFT) challenges, two coordination issues at which current offline MARL algorithms fail. To address this setback, we propose a simple model-based approach that generates synthetic interaction data and enables agents to converge on a strategy while fine-tuning their policies accordingly. Our resulting method, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO), outperforms the prevalent learning methods in challenging offl
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#30693;&#35782;&#24037;&#31243;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24110;&#21161;&#35835;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#24182;&#24314;&#31435;&#30452;&#35273;&#12290;</title><link>http://arxiv.org/abs/2305.17196</link><description>&lt;p&gt;
&#30693;&#35782;&#24037;&#31243;&#20837;&#38376;
&lt;/p&gt;
&lt;p&gt;
A Knowledge Engineering Primer. (arXiv:2305.17196v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17196
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#30693;&#35782;&#24037;&#31243;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24110;&#21161;&#35835;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#24182;&#24314;&#31435;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#30340;&#30446;&#30340;&#26159;&#20197;&#31616;&#27905;&#32780;&#32508;&#21512;&#30340;&#26041;&#24335;&#20171;&#32461;&#30693;&#35782;&#24037;&#31243;&#30340;&#20027;&#39064;&#65292;&#20197;&#22521;&#20859;&#35835;&#32773;&#23545;&#35813;&#39046;&#22495;&#30340;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this primer is to introduce the subject of knowledge engineering in a concise but synthetic way to develop the reader's intuition about the area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#29992;&#20110;&#20174;&#24773;&#25253;&#20027;&#20307;&#30340;&#21333;&#24352;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#19968;&#36830;&#20018;&#22797;&#26434;&#30340;&#21382;&#21490;&#21644;&#26410;&#26469;&#20107;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#21482;&#26377;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#21040;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17195</link><description>&lt;p&gt;
&#36890;&#36807;&#24819;&#35937;&#36807;&#21435;&#26469;&#25512;&#26029;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Inferring the Future by Imagining the Past. (arXiv:2305.17195v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#29992;&#20110;&#20174;&#24773;&#25253;&#20027;&#20307;&#30340;&#21333;&#24352;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#19968;&#36830;&#20018;&#22797;&#26434;&#30340;&#21382;&#21490;&#21644;&#26410;&#26469;&#20107;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#21482;&#26377;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#21040;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28459;&#30011;&#20070;&#20013;&#30340;&#21333;&#19968;&#30011;&#38754;&#33021;&#22815;&#23637;&#29616;&#20154;&#29289;&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#26469;&#33258;&#20309;&#22788;&#12289;&#21160;&#26426;&#20197;&#21450;&#21487;&#33021;&#21457;&#29983;&#30340;&#20107;&#24773;&#65292;&#36825;&#21551;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#20174;&#24773;&#25253;&#20027;&#20307;&#30340;&#21333;&#24352;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#19968;&#36830;&#20018;&#22797;&#26434;&#30340;&#21382;&#21490;&#21644;&#26410;&#26469;&#20107;&#20214;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#30340;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#65292;&#25552;&#20379;&#19968;&#31181;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#29992;&#20110;&#36827;&#34892;&#27492;&#31867;&#25512;&#26029;&#12290;&#24314;&#31435;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#33945;&#29305;&#21345;&#32599;&#36335;&#24452;&#36861;&#36394;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#35768;&#22810;&#29702;&#24565;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#21482;&#26377;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#21040;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#23427;&#20063;&#34920;&#26126;&#20102;&#19968;&#23450;&#30340;&#35748;&#30693;&#21512;&#29702;&#24615;&#65292;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#21305;&#37197;&#20808;&#21069;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#30340;&#20154;&#31867;&#30452;&#35273;&#30340;&#20154;&#31867;&#21463;&#35797;&#32773;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A single panel of a comic book can say a lot: it shows not only where characters currently are, but also where they came from, what their motivations are, and what might happen next. More generally, humans can often infer a complex sequence of past and future events from a *single snapshot image* of an intelligent agent.  Building on recent work in cognitive science, we offer a Monte Carlo algorithm for making such inferences. Drawing a connection to Monte Carlo path tracing in computer graphics, we borrow ideas that help us dramatically improve upon prior work in sample efficiency. This allows us to scale to a wide variety of challenging inference problems with only a handful of samples. It also suggests some degree of cognitive plausibility, and indeed we present human subject studies showing that our algorithm matches human intuitions in a variety of domains that previous methods could not scale to.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#23545;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#36827;&#34892;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#21152;&#36895;&#25506;&#32034;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.17193</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#20998;&#26512;&#65306;&#22312;&#27809;&#26377;&#22522;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#29983;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
AI-based analysis of super-resolution microscopy: Biological discovery in the absence of ground truth. (arXiv:2305.17193v1 [q-bio.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17193
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#23545;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#36827;&#34892;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#21152;&#36895;&#25506;&#32034;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#30340;&#32435;&#31859;&#32423;&#20998;&#36776;&#29575;&#29616;&#24050;&#20351;&#33639;&#20809;&#20998;&#23376;&#23450;&#20301;&#24037;&#20855;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#25972;&#20010;&#32454;&#32990;&#32467;&#26500;&#29983;&#29289;&#23398;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#25968;&#25454;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#32780;&#26032;&#29983;&#29289;&#23398;&#26412;&#36523;&#27809;&#26377;&#34987;&#21457;&#29616;&#36807;&#65292;&#20063;&#27809;&#26377;&#22522;&#20934;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#22312;&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#20854;&#28508;&#21147;&#65292;&#20197;&#23454;&#29616;&#23545;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#30340;&#21152;&#36895;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nanoscale resolution of super-resolution microscopy has now enabled the use of fluorescent based molecular localization tools to study whole cell structural biology. Machine learning based analysis of super-resolution data offers tremendous potential for discovery of new biology, that by definition is not known and lacks ground truth. Herein, we describe the application of weakly supervised learning paradigms to super-resolution microscopy and its potential to enable the accelerated exploration of the molecular architecture of subcellular macromolecules and organelles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;ProGroTrack&#65292;&#22312;&#22522;&#20110;&#26816;&#27979;&#30340;&#36319;&#36394;(DBT)&#26694;&#26550;&#20013;&#23558;YOLO&#21644;ByteTrack&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#36319;&#36394;&#20102;&#32454;&#32990;&#20869;&#34507;&#30333;&#36136;&#32435;&#31859;&#32467;&#26500;&#30340;&#29983;&#38271;&#34892;&#20026;&#12290;&#20854;&#20013;&#65292;&#37319;&#29992;YOLOv5&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#25581;&#31034;&#20102;iPAK4&#34507;&#30333;&#36136;&#32420;&#32500;&#30340;&#20004;&#20010;&#19981;&#21516;&#29983;&#38271;&#38454;&#27573;&#65292;&#20026;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.17183</link><description>&lt;p&gt;
ProGroTrack: &#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#19979;&#30340;&#32454;&#32990;&#20869;&#34507;&#30333;&#36136;&#22686;&#38271;&#21160;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
ProGroTrack: Deep Learning-Assisted Tracking of Intracellular Protein Growth Dynamics. (arXiv:2305.17183v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;ProGroTrack&#65292;&#22312;&#22522;&#20110;&#26816;&#27979;&#30340;&#36319;&#36394;(DBT)&#26694;&#26550;&#20013;&#23558;YOLO&#21644;ByteTrack&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#36319;&#36394;&#20102;&#32454;&#32990;&#20869;&#34507;&#30333;&#36136;&#32435;&#31859;&#32467;&#26500;&#30340;&#29983;&#38271;&#34892;&#20026;&#12290;&#20854;&#20013;&#65292;&#37319;&#29992;YOLOv5&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#25581;&#31034;&#20102;iPAK4&#34507;&#30333;&#36136;&#32420;&#32500;&#30340;&#20004;&#20010;&#19981;&#21516;&#29983;&#38271;&#38454;&#27573;&#65292;&#20026;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#36319;&#36394;&#32454;&#32990;&#21644;&#20122;&#32454;&#32990;&#32467;&#26500;&#20197;&#21450;&#23427;&#20204;&#30340;&#21160;&#24577;&#22312;&#29702;&#35299;&#29983;&#29289;&#31995;&#32479;&#30340;&#22522;&#26412;&#26426;&#21046;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;ProGroTrack&#65292;&#22312;&#22522;&#20110;&#26816;&#27979;&#30340;&#36319;&#36394;(DBT)&#26694;&#26550;&#20013;&#23558;You Only Look Once (YOLO) &#21644;ByteTrack&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#36319;&#36394;&#32454;&#32990;&#20869;&#34507;&#30333;&#36136;&#32435;&#31859;&#32467;&#26500;&#12290;&#20197;iPAK4&#34507;&#30333;&#32420;&#32500;&#20026;&#20195;&#34920;&#26696;&#20363;&#65292;&#25105;&#20204;&#23545;YOLOv5&#21644;YOLOv8&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;YOLOv5&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;YOLOv5x&#23454;&#29616;&#20102;0.839&#30340;mAP50&#21644;0.819&#30340;F-score&#12290;&#20026;&#36827;&#19968;&#27493;&#20248;&#21270;&#26816;&#27979;&#33021;&#21147;&#65292;&#25105;&#20204;&#37319;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#27169;&#22411;&#25913;&#36827;&#65292;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#33719;&#24471;&#20102;&#25552;&#39640;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#24212;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#26469;&#36319;&#36394;iPAK4&#34507;&#30333;&#36136;&#32420;&#32500;&#30340;&#29983;&#38271;&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#29983;&#38271;&#38454;&#27573;&#65292;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate tracking of cellular and subcellular structures, along with their dynamics, plays a pivotal role in understanding the underlying mechanisms of biological systems. This paper presents a novel approach, ProGroTrack, that combines the You Only Look Once (YOLO) and ByteTrack algorithms within the detection-based tracking (DBT) framework to track intracellular protein nanostructures. Focusing on iPAK4 protein fibers as a representative case study, we conducted a comprehensive evaluation of YOLOv5 and YOLOv8 models, revealing the superior performance of YOLOv5 on our dataset. Notably, YOLOv5x achieved an impressive mAP50 of 0.839 and F-score of 0.819. To further optimize detection capabilities, we incorporated semi-supervised learning for model improvement, resulting in enhanced performances in all metrics. Subsequently, we successfully applied our approach to track the growth behavior of iPAK4 protein fibers, revealing their two distinct growth phases consistent with a previously r
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;NMT&#20013;&#30340;&#22797;&#21046;&#38382;&#39064;&#36890;&#24120;&#21457;&#29983;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#19988;&#20250;&#30452;&#25509;&#22797;&#21046;&#36755;&#20837;&#21477;&#23376;&#30340;&#37096;&#20998;&#20316;&#20026;&#32763;&#35793;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;&#26469;&#32531;&#35299;&#35813;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#31181;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17182</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#22797;&#21046;&#38382;&#39064;&#65306;&#20855;&#26377;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
On the Copying Problem of Unsupervised NMT: A Training Schedule with a Language Discriminator Loss. (arXiv:2305.17182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17182
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;NMT&#20013;&#30340;&#22797;&#21046;&#38382;&#39064;&#36890;&#24120;&#21457;&#29983;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#19988;&#20250;&#30452;&#25509;&#22797;&#21046;&#36755;&#20837;&#21477;&#23376;&#30340;&#37096;&#20998;&#20316;&#20026;&#32763;&#35793;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;&#26469;&#32531;&#35299;&#35813;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#31181;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24050;&#22312;&#35768;&#22810;&#35821;&#31181;&#38388;&#24471;&#21040;&#25104;&#21151;&#65292;&#20294;&#22797;&#21046;&#38382;&#39064;&#65288;&#21363;&#23558;&#36755;&#20837;&#21477;&#23376;&#30340;&#26576;&#20123;&#37096;&#20998;&#30452;&#25509;&#22797;&#21046;&#20316;&#20026;&#32763;&#35793;&#65289;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#24456;&#24120;&#35265;&#65292;&#23588;&#20854;&#28041;&#21450;&#20302;&#36164;&#28304;&#35821;&#31181;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#38382;&#39064;&#19982;&#22312;&#32447;&#22238;&#35793;&#65288;BT&#65289;&#26399;&#38388;&#20986;&#29616;&#30340;&#39044;&#26399;&#22797;&#21046;&#34892;&#20026;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35757;&#32451;&#35745;&#21010;&#65292;&#23427;&#21253;&#21547;&#20102;&#19968;&#20010;&#35821;&#35328;&#37492;&#21035;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#35813;&#25439;&#22833;&#26045;&#21152;&#32422;&#26463;&#20110;&#20013;&#38388;&#32763;&#35793;&#65292;&#20197;&#20351;&#32763;&#35793;&#26159;&#25152;&#38656;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#35821;&#35328;&#23545;&#12289; &#21253;&#25324;&#30456;&#20284;&#21644;&#36828;&#36317;&#31163;&#12289;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#22797;&#21046;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although unsupervised neural machine translation (UNMT) has achieved success in many language pairs, the copying problem, i.e., directly copying some parts of the input sentence as the translation, is common among distant language pairs, especially when low-resource languages are involved. We find this issue is closely related to an unexpected copying behavior during online back-translation (BT). In this work, we propose a simple but effective training schedule that incorporates a language discriminator loss. The loss imposes constraints on the intermediate translation so that the translation is in the desired language. By conducting extensive experiments on different language pairs, including similar and distant, high and low-resource languages, we find that our method alleviates the copying problem, thus improving the translation performance on low-resource languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ETSE&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32974;&#20799;&#20581;&#24247;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#29992;&#22810;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;7&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17156</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36229;&#21442;&#25968;&#35843;&#25972;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#32974;&#20799;&#20581;&#24247;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Improved Model Ensembled of Different Hyper-parameter Tuned Machine Learning Algorithms for Fetal Health Prediction. (arXiv:2305.17156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ETSE&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32974;&#20799;&#20581;&#24247;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#29992;&#22810;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;7&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32974;&#20799;&#20581;&#24247;&#23545;&#20110;&#23381;&#26399;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20250;&#24433;&#21709;&#21040;&#27597;&#20146;&#21644;&#32974;&#20799;&#30340;&#20581;&#24247;&#12290;&#30417;&#27979;&#32974;&#20799;&#20581;&#24247;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#36895;&#24230;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ETSE&#30340;&#26426;&#22120;&#23398;&#20064;&#23450;&#21046;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;ExtraTrees&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32974;&#20799;&#20581;&#24247;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#31163;&#32676;&#20540;&#21076;&#38500;&#12289;&#32570;&#22833;&#20540;&#34917;&#20840;&#12289;&#25968;&#25454;&#26631;&#20934;&#21270;&#21644;&#25968;&#25454;&#25277;&#26679;&#31561;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#23454;&#29616;&#20102;7&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;XGBoost&#12289;LGBM&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;ExtraTrees&#21644;K&#36817;&#37051;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#36807;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#37319;&#29992;&#36229;&#21442;&#25968;&#35843;&#25972;&#36827;&#34892;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fetal health is a critical concern during pregnancy as it can impact the well-being of both the mother and the baby. Regular monitoring and timely interventions are necessary to ensure the best possible outcomes. While there are various methods to monitor fetal health in the mother's womb, the use of artificial intelligence (AI) can improve the accuracy, efficiency, and speed of diagnosis. In this study, we propose a robust ensemble model called ensemble of tuned Support Vector Machine and ExtraTrees (ETSE) for predicting fetal health. Initially, we employed various data preprocessing techniques such as outlier rejection, missing value imputation, data standardization, and data sampling. Then, seven machine learning (ML) classifiers including Support Vector Machine (SVM), XGBoost (XGB), Light Gradient Boosting Machine (LGBM), Decision Tree (DT), Random Forest (RF), ExtraTrees (ET), and K-Neighbors were implemented. These models were evaluated and then optimized by hyperparameter tuning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#31283;&#23450;&#24615;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21152;&#20837;&#20102;&#30828;&#24615;&#32422;&#26463;&#26469;&#20445;&#35777;&#20854;&#26435;&#37325;&#31283;&#23450;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#38271;&#26399;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17155</link><description>&lt;p&gt;
&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability of implicit neural networks for long-term forecasting in dynamical systems. (arXiv:2305.17155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#31283;&#23450;&#24615;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21152;&#20837;&#20102;&#30828;&#24615;&#32422;&#26463;&#26469;&#20445;&#35777;&#20854;&#26435;&#37325;&#31283;&#23450;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#38271;&#26399;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#29289;&#29702;&#20449;&#21495;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#30740;&#31350;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20026;&#20102;&#35268;&#36991;&#20256;&#32479;&#27714;&#35299;&#22120;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#20204;&#37117;&#22522;&#20110;&#33258;&#22238;&#24402;&#26041;&#27861;&#24182;&#23637;&#31034;&#20986;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#21463;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#29305;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#33258;&#22238;&#24402;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#26681;&#25454;&#25968;&#20540;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#29702;&#35770;&#26469;&#20445;&#35777;&#32593;&#32476;&#39044;&#27979;&#30340;&#31283;&#23450;&#24615;&#12290;&#23427;&#23548;&#33268;&#25105;&#20204;&#23545;&#20854;&#26435;&#37325;&#28155;&#21152;&#20102;&#30828;&#24615;&#32422;&#26463;&#65292;&#24182;&#22312;&#28508;&#31354;&#38388;&#20013;&#20256;&#25773;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#31283;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#36755;&#36816;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38271;&#26399;&#39044;&#27979;&#19978;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting physical signals in long time range is among the most challenging tasks in Partial Differential Equations (PDEs) research. To circumvent limitations of traditional solvers, many different Deep Learning methods have been proposed. They are all based on auto-regressive methods and exhibit stability issues. Drawing inspiration from the stability property of implicit numerical schemes, we introduce a stable auto-regressive implicit neural network. We develop a theory based on the stability definition of schemes to ensure the stability in forecasting of this network. It leads us to introduce hard constraints on its weights and propagate the dynamics in the latent space. Our experimental results validate our stability property, and show improved results at long-term forecasting for two transports PDEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#23545;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36817;&#20284;&#20984;&#24615;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.17154</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On convex conceptual regions in deep network representations. (arXiv:2305.17154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#23545;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36817;&#20284;&#20984;&#24615;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#23545;&#40784;&#30340;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;Gardenfors&#30340;&#27010;&#24565;&#31354;&#38388;&#26159;&#29702;&#35299;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#20010;&#37325;&#35201;&#26694;&#26550;&#12290;&#22312;&#27010;&#24565;&#31354;&#38388;&#20013;&#65292;&#23545;&#35937;&#21306;&#22495;&#30340;&#20984;&#24615;&#34987;&#35748;&#20026;&#26159;&#20419;&#36827;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27010;&#24565;&#21306;&#22495;&#30340;&#20984;&#24615;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#32452;&#29992;&#20110;&#27979;&#37327;&#37319;&#26679;&#25968;&#25454;&#20013;&#20984;&#24615;&#30340;&#24037;&#20855;&#65292;&#24182;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#23618;&#34920;&#31034;&#20013;&#30340;&#20984;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20984;&#24615;&#23545;&#20110;&#22522;&#26412;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26159;&#31283;&#20581;&#30340;&#65292;&#22240;&#27492;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#36136;&#37327;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#36817;&#20284;&#20984;&#24615;&#22312;&#31070;&#32463;&#34920;&#31034;&#20013;&#24191;&#27867;&#23384;&#22312;&#20110;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#20154;&#31867;&#27963;&#21160;&#12289;&#25991;&#26412;&#21644;&#33041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current study of human-machine alignment aims at understanding the geometry of latent spaces and the correspondence to human representations. G\"ardenfors' conceptual spaces is a prominent framework for understanding human representations. Convexity of object regions in conceptual spaces is argued to promote generalizability, few-shot learning, and intersubject alignment. Based on these insights, we investigate the notion of convexity of concept regions in machine-learned latent spaces. We develop a set of tools for measuring convexity in sampled data and evaluate emergent convexity in layered representations of state-of-the-art deep networks. We show that convexity is robust to basic re-parametrization, hence, meaningful as a quality of machine-learned latent spaces. We find that approximate convexity is pervasive in neural representations in multiple application domains, including models of images, audio, human activity, text, and brain data. We measure convexity separately for l
&lt;/p&gt;</description></item><item><title>mldr.resampling&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#26088;&#22312;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17152</link><description>&lt;p&gt;
mldr.resampling: &#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#31639;&#27861;&#26377;&#25928;&#30340;&#21442;&#32771;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
mldr.resampling: Efficient Reference Implementations of Multilabel Resampling Algorithms. (arXiv:2305.17152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17152
&lt;/p&gt;
&lt;p&gt;
mldr.resampling&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#26088;&#22312;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#37319;&#26679;&#31639;&#27861;&#26159;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#24773;&#20917;&#30340;&#26377;&#29992;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#24517;&#39035;&#22788;&#29702;&#22810;&#26631;&#31614;&#25968;&#25454;&#20013;&#30340;&#22855;&#24322;&#24615;&#65292;&#20363;&#22914;&#21516;&#19968;&#23454;&#20363;&#20013;&#39057;&#32321;&#21644;&#19981;&#39057;&#32321;&#26631;&#31614;&#30340;&#20986;&#29616;&#12290;&#36825;&#31687;&#21407;&#21019;&#36719;&#20214;&#21457;&#34920;&#20171;&#32461;&#20102; mldr.resampling&#65292;&#36825;&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;&#20102;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#24378;&#35843;&#25928;&#29575;&#65292;&#22240;&#20026;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#32791;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resampling algorithms are a useful approach to deal with imbalanced learning in multilabel scenarios. These methods have to deal with singularities in the multilabel data, such as the occurrence of frequent and infrequent labels in the same instance. Implementations of these methods are sometimes limited to the pseudocode provided by their authors in a paper. This Original Software Publication presents mldr.resampling, a software package that provides reference implementations for eleven multilabel resampling methods, with an emphasis on efficiency since these algorithms are usually time-consuming.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35786;&#26029;&#26102;&#31354;&#21464;&#25442;&#22120;&#65288;DFStrans&#65289;&#65292;&#20854;&#21033;&#29992;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#26102;&#31354;&#20381;&#36182;&#24615;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#21462;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.17149</link><description>&lt;p&gt;
&#20855;&#26377;&#31934;&#30830;&#32534;&#30721;&#30340;&#35786;&#26029;&#26102;&#31354;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Spatio-temporal Transformer with Faithful Encoding. (arXiv:2305.17149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35786;&#26029;&#26102;&#31354;&#21464;&#25442;&#22120;&#65288;DFStrans&#65289;&#65292;&#20854;&#21033;&#29992;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#26102;&#31354;&#20381;&#36182;&#24615;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#21462;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#24403;&#22522;&#26412;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20855;&#26377;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#26102;&#30340;&#24322;&#24120;&#35786;&#26029;&#20219;&#21153;&#12290;&#20854;&#20013;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#26159;&#20174;&#25551;&#36848;&#26102;&#38388;&#21644;&#31354;&#38388;&#25351;&#25968;&#20043;&#38388;&#39640;&#38454;&#20132;&#20114;&#30340;&#20381;&#36182;&#24352;&#37327;&#20013;&#25552;&#21462;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#30417;&#30563;&#20381;&#36182;&#21457;&#29616;&#65292;&#20854;&#20013;&#26102;&#31354;&#20381;&#36182;&#24615;&#34987;&#20316;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#21103;&#20135;&#21697;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;ST&#21464;&#25442;&#22120;&#20013;&#20351;&#29992;&#30340;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#22312;&#25429;&#25417;&#26356;&#39640;&#39057;&#29575;&#65288;&#30701;&#26102;&#38388;&#23610;&#24230;&#65289;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#29702;&#35770;&#20445;&#35777;&#30340;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#21457;&#29616;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#26041;&#21521;&#19978;&#25552;&#20379;&#26131;&#20110;&#28040;&#21270;&#30340;&#35786;&#26029;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;DFStrans&#65288;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#35786;&#26029;&#26102;&#31354;&#21464;&#25442;&#22120;&#65289;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the task of anomaly diagnosis when the underlying data generation process has a complex spatio-temporal (ST) dependency. The key technical challenge is to extract actionable insights from the dependency tensor characterizing high-order interactions among temporal and spatial indices. We formalize the problem as supervised dependency discovery, where the ST dependency is learned as a side product of multivariate time-series classification. We show that temporal positional encoding used in existing ST transformer works has a serious limitation in capturing higher frequencies (short time scales). We propose a new positional encoding with a theoretical guarantee, based on discrete Fourier transform. We also propose a new ST dependency discovery framework, which can provide readily consumable diagnostic information in both spatial and temporal directions. Finally, we demonstrate the utility of the proposed model, DFStrans (Diagnostic Fourier-based Spatio-temporal Transf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.17147</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24322;&#36136;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#23558;&#20854;&#19982;&#19968;&#31181;&#21516;&#36136;&#30340;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#24182;&#38656;&#35201;&#20154;&#31867;&#39564;&#35777;&#65292;&#20294;&#32570;&#20047;&#23545;&#23545;&#40784;&#25152;&#38656;&#26041;&#38754;&#21644;&#28145;&#24230;&#30340;&#20849;&#35782;&#20197;&#21450;&#36896;&#25104;&#30340;&#20154;&#31867;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#65288;1&#65289;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#21333;&#20010;&#20154;&#31867;&#20559;&#35265;&#65292;&#24182;&#19988;&#65288;2&#65289;&#20801;&#35768;&#35780;&#20272;&#38024;&#23545;&#21508;&#31181;&#30446;&#26631;&#20540;&#30340;&#24322;&#36136;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#20195;&#34920;&#20102;&#20195;&#29702;&#20154;&#25191;&#34892;&#26368;&#33021;&#28385;&#36275;&#30446;&#26631;&#20215;&#20540;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#37327;&#21270;&#26159;&#36890;&#36807;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#36827;&#34892;&#30340;&#65292;&#35813;&#26694;&#26550;&#23558;&#20215;&#20540;&#31354;&#38388;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#27169;&#22411;&#30340;&#20215;&#20540;&#21512;&#29702;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;A2EHV&#26041;&#27861;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2305.17144</link><description>&lt;p&gt;
Minecraft&#20013;&#30340;&#24189;&#28789;&#65306;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30693;&#35782;&#21644;&#35760;&#24518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory. (arXiv:2305.17144v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Minecraft&#29609;&#27861;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#25104;&#20026;&#24320;&#21457;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#26234;&#33021;&#20307;&#30340;&#20016;&#23500;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#30446;&#26631;&#19978;&#65292;&#20363;&#22914;&#27969;&#34892;&#30340;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#65292;&#24182;&#19988;&#36824;&#27809;&#26377;&#26174;&#31034;&#20986;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#30340;&#30446;&#21069;&#26368;&#39640;&#25104;&#21151;&#29575;&#21482;&#26377;&#32422;20&#65285;&#65292;&#20984;&#26174;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25511;&#21046;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Ghost in the Minecraft (GITM)&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21019;&#24314;Minecraft&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;&#36825;&#20123;&#20855;&#22791;LLM&#20013;&#30340;&#36923;&#36753;&#21644;&#24120;&#35782;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#29087;&#32451;&#22320;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "ObtainDiamond" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "ObtainDiamond" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;CTDE&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;MAPPO&#31639;&#27861;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20915;&#31574;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26435;&#37325;&#35843;&#24230;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#21327;&#20316;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.17141</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#19982;&#21327;&#20316;&#20915;&#31574;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Research on Multi-Agent Communication and Collaborative Decision-Making Based on Deep Reinforcement Learning. (arXiv:2305.17141v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;CTDE&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;MAPPO&#31639;&#27861;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20915;&#31574;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26435;&#37325;&#35843;&#24230;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#21327;&#20316;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#20026;&#20102;&#20811;&#26381;&#21644;&#32531;&#35299;&#29615;&#22659;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#20027;&#27969;&#26041;&#27861;&#26159;&#37319;&#29992;&#38598;&#20013;&#24335;&#35757;&#32451;&#20998;&#25955;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#26694;&#26550;&#12290;&#26412;&#25991;&#22522;&#20110;CTDE&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;MAPPO&#65289;&#31639;&#27861;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20915;&#31574;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#26435;&#37325;&#35843;&#24230;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26426;&#21046;&#12290;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#26469;&#32531;&#35299;&#30001;&#26412;&#22320;&#35266;&#27979;&#24341;&#36215;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#21327;&#21161;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#20915;&#31574;&#12290;&#20855;&#20307;&#26041;&#27861;&#26159;&#22312;&#31574;&#30053;&#32593;&#32476;&#37096;&#20998;&#24341;&#20837;&#19968;&#20010;&#36890;&#20449;&#27169;&#22359;&#12290;&#36890;&#20449;&#27169;&#22359;&#30001;&#26435;&#37325;&#29983;&#25104;&#22120;&#12289;&#26435;&#37325;&#35843;&#24230;&#22120;&#12289;&#20449;&#24687;&#32534;&#30721;&#22120;&#12289;&#20449;&#24687;&#35299;&#30721;&#22120;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#32452;&#25104;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#21327;&#20316;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a multi-agent environment, In order to overcome and alleviate the non-stationarity of the multi-agent environment, the mainstream method is to adopt the framework of Centralized Training Decentralized Execution (CTDE). This thesis is based on the framework of CTDE, and studies the cooperative decision-making of multi-agent based on the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm for multi-agent proximal policy optimization. In order to alleviate the non-stationarity of the multi-agent environment, a multi-agent communication mechanism based on weight scheduling and attention module is introduced. Different agents can alleviate the non-stationarity caused by local observations through information exchange between agents, assisting in the collaborative decision-making of agents. The specific method is to introduce a communication module in the policy network part. The communication module is composed of a weight generator, a weight scheduler, a message encoder, a messag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#29992;&#25143;&#33021;&#21542;&#32771;&#34385;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#24182;&#36827;&#34892;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#22312;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#36890;&#36807;&#20132;&#20114;&#24335;&#31995;&#32479;&#25552;&#20986;&#21644;&#23436;&#21892;&#35299;&#20915;&#26041;&#26696;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2305.17140</link><description>&lt;p&gt;
&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#20132;&#20114;&#27169;&#22411;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Interactive Model Expansion in an Observable Environment. (arXiv:2305.17140v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#29992;&#25143;&#33021;&#21542;&#32771;&#34385;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#24182;&#36827;&#34892;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#22312;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#36890;&#36807;&#20132;&#20114;&#24335;&#31995;&#32479;&#25552;&#20986;&#21644;&#23436;&#21892;&#35299;&#20915;&#26041;&#26696;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#21487;&#20197;&#29702;&#35299;&#20026;&#23547;&#25214;&#19968;&#31181;&#29366;&#24577;&#65292;&#23427;&#25193;&#23637;&#20102;&#22266;&#23450;&#30340;&#37096;&#20998;&#29366;&#24577;&#65292;&#21363;"&#29615;&#22659;"&#65292;&#21516;&#26102;&#28385;&#36275;&#19968;&#23450;&#30340;&#24418;&#24335;&#21270;&#35268;&#23450;&#26465;&#20214;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#24037;&#31243;&#12289;&#27861;&#24459;&#25110;&#32463;&#27982;&#23398;&#20013;&#37117;&#23384;&#22312;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#19968;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#65292;&#29992;&#25143;&#22312;&#25628;&#32034;&#24320;&#22987;&#26102;&#24182;&#19981;&#30693;&#36947;&#26377;&#20851;&#29615;&#22659;&#30340;&#26576;&#20123;&#20449;&#24687;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#32771;&#34385;&#26576;&#20123;&#26242;&#23450;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#23545;&#36825;&#20123;&#26410;&#30693;&#20449;&#24687;&#36827;&#34892;&#20102;&#26263;&#31034;&#24615;&#20551;&#35774;&#12290;&#20026;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#36866;&#23452;&#24615;&#65292;&#36825;&#20123;&#20551;&#35774;&#24517;&#39035;&#36890;&#36807;&#23545;&#29615;&#22659;&#30340;&#35266;&#23519;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20551;&#35774;&#65292;&#38500;&#20102;&#35299;&#20915;&#26041;&#26696;&#26500;&#25104;&#30340;&#30693;&#35782;&#22806;&#65292;&#25105;&#20204;&#36824;&#26377;&#20851;&#20110;&#29615;&#22659;&#36890;&#29992;&#23450;&#24459;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#20855;&#26377;&#36275;&#22815;&#39564;&#35777;&#20107;&#23454;&#30340;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20445;&#35777;&#23436;&#25972;&#12289;&#36866;&#24403;&#35299;&#20915;&#26041;&#26696;&#30340;&#23384;&#22312;&#24615;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#20197;&#21327;&#21161;&#29992;&#25143;&#36827;&#34892;&#25628;&#32034;&#36807;&#31243;&#65292;&#31995;&#32479;&#26681;&#25454;&#29992;&#25143;&#34892;&#21160;&#30340;&#35266;&#23519;&#32467;&#26524;&#25552;&#20986;&#21644;&#23436;&#21892;&#35299;&#20915;&#26041;&#26696;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many practical problems can be understood as the search for a state of affairs that extends a fixed partial state of affairs, the \emph{environment}, while satisfying certain conditions that are formally specified. Such problems are found in, e.g., engineering, law or economics.  We study this class of problems in a context where some of the relevant information about the environment is not known by the user at the start of the search. During the search, the user may consider tentative solutions that make implicit hypotheses about these unknowns. To ensure that the solution is appropriate, these hypotheses must be verified by observing the environment. Furthermore, we assume that, in addition to knowledge of what constitutes a solution, knowledge of general laws of the environment is also present. We formally define partial solutions with enough verified facts to guarantee the existence of complete and appropriate solutions.  Additionally, we propose an interactive system to assist the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;"&#22240;&#26524;&#31354;&#38388;"&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20197;&#26607;&#23572;&#33707;&#25096;&#32599;&#22827;&#30340;&#27010;&#29575;&#27979;&#24230;&#20844;&#29702;&#21270;&#20026;&#36215;&#28857;&#65292;&#23454;&#29616;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#20844;&#29702;&#21270;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.17139</link><description>&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#30340;&#27979;&#24230;&#35770;&#20844;&#29702;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Measure-Theoretic Axiomatisation of Causality. (arXiv:2305.17139v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;"&#22240;&#26524;&#31354;&#38388;"&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20197;&#26607;&#23572;&#33707;&#25096;&#32599;&#22827;&#30340;&#27010;&#29575;&#27979;&#24230;&#20844;&#29702;&#21270;&#20026;&#36215;&#28857;&#65292;&#23454;&#29616;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#20844;&#29702;&#21270;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#26159;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#26680;&#24515;&#27010;&#24565;&#65292;&#20294;&#20173;&#28982;&#27809;&#26377;&#26222;&#36941;&#35748;&#21487;&#30340;&#22240;&#26524;&#20851;&#31995;&#20844;&#29702;&#21270;&#12290;&#25105;&#20204;&#23558;&#22240;&#26524;&#20851;&#31995;&#35270;&#20026;&#27010;&#29575;&#29702;&#35770;&#30340;&#25193;&#23637;&#65292;&#24182;&#20316;&#20026;&#30740;&#31350;&#22312;&#31995;&#32479;&#19978;&#24178;&#39044;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#35758;&#20197;&#26607;&#23572;&#33707;&#25096;&#32599;&#22827;&#30340;&#27010;&#29575;&#27979;&#24230;&#20844;&#29702;&#21270;&#20316;&#20026;&#22240;&#26524;&#20851;&#31995;&#20844;&#29702;&#21270;&#30340;&#36215;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;"&#22240;&#26524;&#31354;&#38388;"&#30340;&#27010;&#24565;&#65292;&#21253;&#25324;&#19968;&#20010;&#27010;&#29575;&#31354;&#38388;&#21644;&#31216;&#20026;"&#22240;&#26524;&#26680;"&#30340;&#36716;&#31227;&#27010;&#29575;&#26680;&#30340;&#38598;&#21512;&#65292;&#29992;&#26469;&#32534;&#30721;&#35813;&#31354;&#38388;&#30340;&#22240;&#26524;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#19981;&#20165;&#22312;&#27979;&#24230;&#35770;&#19978;&#20005;&#26684;&#22320;&#22522;&#20110;&#65292;&#36824;&#25581;&#31034;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#38271;&#26399;&#38480;&#21046;&#65292;&#20363;&#22914;&#65292;&#24490;&#29615;&#12289;&#28508;&#22312;&#21464;&#37327;&#21644;&#38543;&#26426;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causality is a central concept in a wide range of research areas, yet there is still no universally agreed axiomatisation of causality. We view causality both as an extension of probability theory and as a study of \textit{what happens when one intervenes on a system}, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a \textit{causal space}, consisting of a probability space along with a collection of transition probability kernels, called \textit{causal kernels}, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#26234;&#33021;&#36710;&#36742;&#31995;&#32479;&#20013;&#38598;&#25104;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#38754;&#25351;&#21335;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#20854;&#23545;&#35821;&#38899;&#12289;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#21644;&#19982;&#20262;&#29702;&#36947;&#24503;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.17137</link><description>&lt;p&gt;
&#26234;&#33021;&#36710;&#36742;&#31995;&#32479;&#20013;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems. (arXiv:2305.17137v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#26234;&#33021;&#36710;&#36742;&#31995;&#32479;&#20013;&#38598;&#25104;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#38754;&#25351;&#21335;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#20854;&#23545;&#35821;&#38899;&#12289;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#21644;&#19982;&#20262;&#29702;&#36947;&#24503;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20840;&#38754;&#25351;&#21335;&#65292;&#20026;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#36710;&#36742;&#29615;&#22659;&#20013;&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#28508;&#22312;&#24212;&#29992;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#27934;&#35265;&#12290;&#38543;&#30528;&#27773;&#36710;&#34892;&#19994;&#36880;&#28176;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26377;&#28508;&#21147;&#22312;&#29992;&#25143;&#20132;&#20114;&#26041;&#38754;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#65292;&#25552;&#20379;&#26356;&#27785;&#28024;&#12289;&#30452;&#35266;&#12289;&#20010;&#24615;&#21270;&#30340;&#36710;&#20869;&#20307;&#39564;&#12290;&#25105;&#20204;&#25552;&#20379;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#22312;&#27773;&#36710;&#39046;&#22495;&#20013;&#30340;&#24403;&#21069;&#24212;&#29992;&#27010;&#36848;&#65292;&#37325;&#28857;&#24378;&#35843;&#35821;&#38899;&#12289;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;&#38543;&#21518;&#25105;&#20204;&#27010;&#36848;&#20102;&#20851;&#38190;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#39046;&#22495;&#36866;&#24212;&#24615;&#12289;&#23545;&#40784;&#12289;&#22810;&#27169;&#24577;&#38598;&#25104;&#31561;&#65292;&#20197;&#21450;&#35299;&#20915;&#19982;&#20262;&#29702;&#36947;&#24503;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;&#36890;&#36807;&#20419;&#36827;&#21327;&#20316;&#21644;&#35299;&#20915;&#36825;&#20123;&#30740;&#31350;&#39046;&#22495;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#25104;&#20026;&#26234;&#33021;&#36710;&#36742;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22686;&#24378;&#39550;&#39542;&#21592;&#21644;&#20056;&#23458;&#30340;&#23433;&#20840;&#12289;&#33298;&#36866;&#21644;&#20415;&#21033;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to serve as a comprehensive guide for researchers and practitioners, offering insights into the current state, potential applications, and future research directions for generative artificial intelligence and foundation models within the context of intelligent vehicles. As the automotive industry progressively integrates AI, generative artificial intelligence technologies hold the potential to revolutionize user interactions, delivering more immersive, intuitive, and personalised in-car experiences. We provide an overview of current applications of generative artificial intelligence in the automotive domain, emphasizing speech, audio, vision, and multimodal interactions. We subsequently outline critical future research areas, including domain adaptability, alignment, multimodal integration and others, as well as, address the challenges and risks associated with ethics. By fostering collaboration and addressing these research areas, generative artificial intelligence can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; 3T &#26041;&#27861;&#65292;&#21363;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;3T &#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#23545;&#26816;&#32034;&#20219;&#21153;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16999</link><description>&lt;p&gt;
&#19977;&#22612;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#28789;&#27963;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Three Towers: Flexible Contrastive Learning with Pretrained Image Models. (arXiv:2305.16999v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; 3T &#26041;&#27861;&#65292;&#21363;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;3T &#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#23545;&#26816;&#32034;&#20219;&#21153;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19977;&#22612;&#65288;3T&#65289;&#8221;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#32435;&#20837;&#23545;&#27604;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;&#19982;&#36890;&#24120;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23545;&#27604;&#27169;&#22411;&#19981;&#21516;&#65292;&#26368;&#36817;&#30340; LiT&#65288;Zhai &#31561;&#20154;&#65292;2022&#65289;&#34920;&#26126;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#23884;&#20837;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;LiT &#30452;&#25509;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26367;&#25442;&#22270;&#20687;&#22612;&#65292;&#25490;&#38500;&#20102;&#23545;&#22270;&#20687;&#22612;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#30340;&#20219;&#20309;&#28508;&#22312;&#22909;&#22788;&#12290;&#36890;&#36807; 3T&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#31574;&#30053;&#65292;&#20801;&#35768;&#22270;&#20687;&#22612;&#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19977;&#20010;&#22612;&#65292;&#20854;&#20013;&#21253;&#21547;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#65292;&#24182;&#40723;&#21169;&#35813;&#31532;&#19977;&#20010;&#22612;&#19982;&#20027;&#35201;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22612;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;3T &#22312;&#26816;&#32034;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110; LiT &#21644; CLIP &#39118;&#26684;&#30340;&#20174;&#22836;&#24320;&#22987;&#23545;&#27604;&#23398;&#20064;&#22522;&#32447;&#12290;&#23545;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;3T &#22312;&#20174;&#22836;&#24320;&#22987;&#22522;&#32447;&#30340;&#22522;&#30784;&#19978;&#21487;&#38752;&#22320;&#25913;&#21892;&#65292;&#34429;&#28982;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#21450; LiT&#65292;&#20294;&#20173;&#28982;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#26041;&#27861;&#20984;&#26174;&#20102;&#23558;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#27880;&#20837;&#21040;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#21033;&#29992;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Three Towers (3T), a flexible method to improve the contrastive learning of vision-language models by incorporating pretrained image classifiers. While contrastive models are usually trained from scratch, LiT (Zhai et al., 2022) has recently shown performance gains from using pretrained classifier embeddings. However, LiT directly replaces the image tower with the frozen embeddings, excluding any potential benefits of contrastively training the image tower. With 3T, we propose a more flexible strategy that allows the image tower to benefit from both pretrained embeddings and contrastive training. To achieve this, we introduce a third tower that contains the frozen pretrained embeddings, and we encourage alignment between this third tower and the main image-text towers. Empirically, 3T consistently improves over LiT and the CLIP-style from-scratch baseline for retrieval tasks. For classification, 3T reliably improves over the from-scratch baseline, and while it underperform
&lt;/p&gt;</description></item><item><title>NavGPT&#26159;&#22522;&#20110;LLM&#30340;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#25191;&#34892;&#38646;-shot&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#32423;&#35268;&#21010;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;NavGPT&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.16986</link><description>&lt;p&gt;
NavGPT: &#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#26174;&#24335;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models. (arXiv:2305.16986v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16986
&lt;/p&gt;
&lt;p&gt;
NavGPT&#26159;&#22522;&#20110;LLM&#30340;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#25191;&#34892;&#38646;-shot&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#32423;&#35268;&#21010;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;NavGPT&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20363;&#22914;ChatGPT&#21644;GPT-4&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#27169;&#22411;&#30340;&#25193;&#23637;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#36235;&#21183;&#24378;&#35843;&#20102;&#20351;&#29992;&#26080;&#38480;&#35821;&#35328;&#25968;&#25454;&#35757;&#32451;LLM&#30340;&#28508;&#21147;&#65292;&#25512;&#21160;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NavGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32431;&#31929;&#22522;&#20110;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#20026;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#25191;&#34892;&#38646;-shot&#30340;&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#65292;&#25581;&#31034;&#20102;&#23545;&#20110;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#22330;&#26223;&#19979;GPT&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;NavGPT&#23558;&#35270;&#35273;&#35266;&#23519;&#12289;&#23548;&#33322;&#21382;&#21490;&#21644;&#26410;&#26469;&#21487;&#25506;&#32034;&#26041;&#21521;&#30340;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#65292;&#25512;&#29702;&#20986;&#26234;&#33021;&#20307;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#20915;&#23450;&#22914;&#20309;&#25509;&#36817;&#30446;&#26631;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NavGPT&#21487;&#20197;&#26126;&#30830;&#22320;&#25191;&#34892;&#23548;&#33322;&#30340;&#39640;&#32423;&#35268;&#21010;&#65292;&#21253;&#25324;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#21487;&#33021;&#25104;&#20026;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#20256;&#32479;&#27969;&#31243;&#30340;&#24378;&#26377;&#21147;&#26367;&#20195;&#21697;&#65292;&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#65292;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#28304;&#22495;&#25193;&#23637;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16820</link><description>&lt;p&gt;
&#38754;&#21521;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Aligned Prefix Averaging for Domain Generalization in Abstractive Summarization. (arXiv:2305.16820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#65292;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#28304;&#22495;&#25193;&#23637;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#65292;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#12290;&#36890;&#36807;&#32473;&#23450;&#22810;&#20010;&#28304;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20026;&#27599;&#20010;&#22495;&#35757;&#32451;&#19968;&#20010;&#21069;&#32512;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#21069;&#32512;&#29983;&#25104;&#23569;&#37327;&#30446;&#26631;&#22495;&#25991;&#26723;&#30340;&#25688;&#35201;&#65292;&#35745;&#31639;&#25152;&#38656;&#30340;&#26435;&#37325;&#26469;&#24179;&#22343;&#28304;&#21069;&#32512;&#12290;&#22312;DAPA&#20013;&#65292;&#21069;&#32512;&#35843;&#25972;&#20801;&#35768;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#65292;&#21152;&#26435;&#24179;&#22343;&#20801;&#35768;&#26377;&#25928;&#22320;&#28155;&#21152;&#26032;&#30340;&#28304;&#22495;&#12290;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#25688;&#35201;&#39046;&#22495;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;DAPA&#34920;&#29616;&#20986;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#21069;&#32512;&#24179;&#22343;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization is hitherto an underexplored area applied in abstractive summarization. Moreover, most existing works on domain generalization have sophisticated training algorithms. In this paper, we propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging approach to domain generalization for abstractive summarization. Given a number of source domains, our method first trains a prefix for each one of them. These source prefixes generate summaries for a small number of target domain documents. The similarity of the generated summaries to their corresponding documents is used for calculating weights required to average source prefixes. In DAPA, prefix tuning allows for lightweight finetuning, and weight averaging allows for the computationally efficient addition of new source domains. When evaluated on four diverse summarization domains, DAPA shows comparable or better performance against the baselines, demonstrating the effectiveness of its prefix averaging
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#36890;&#36807;&#32452;&#21512;&#21367;&#31215;&#22359;&#21644;&#21464;&#24418;&#22120;&#22359;&#65292;&#20197;&#21450;&#24341;&#20837;BFIM&#21644;SFM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16342</link><description>&lt;p&gt;
InterFormer: &#28151;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#20132;&#20114;&#24335;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition. (arXiv:2305.16342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#36890;&#36807;&#32452;&#21512;&#21367;&#31215;&#22359;&#21644;&#21464;&#24418;&#22120;&#22359;&#65292;&#20197;&#21450;&#24341;&#20837;BFIM&#21644;SFM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#32780;&#35328;&#65292;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#24050;&#32463;&#35777;&#23454;&#65292;&#31616;&#21333;&#22320;&#21512;&#24182;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ASR&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#30053;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20018;&#34892;&#26550;&#26500;&#26080;&#27861;&#21453;&#26144;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21367;&#31215;&#22359;&#19982;&#21464;&#24418;&#22120;&#22359;&#20197;&#24182;&#34892;&#35774;&#35745;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#21521;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#65288;BFIM&#65289;&#21644;&#36873;&#25321;&#24615;&#34701;&#21512;&#27169;&#22359;&#65288;SFM&#65289;&#26469;&#23454;&#29616;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#12290;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;InterFormer&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;Transformer&#21644;Conformer&#27169;&#22411;&#20855;&#26377;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the other Transformer and Conformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36229;&#22823;&#35268;&#27169;&#22270;&#30340;&#22312;&#32447;&#33410;&#28857;&#20998;&#31867;&#31639;&#27861;FastONL&#65292;&#23427;&#22522;&#20110;&#24191;&#20041;&#23616;&#37096;&#25512;&#36865;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#36817;&#20284;&#36870;&#30697;&#38453;&#21015;&#24182;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;&#22270;&#26680;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#36951;&#25022;&#20540;&#21644;&#27599;&#20010;&#39044;&#27979;&#30340;&#36739;&#20302;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.16257</link><description>&lt;p&gt;
&#38754;&#21521;&#36229;&#22823;&#35268;&#27169;&#22270;&#30340;&#24555;&#36895;&#22312;&#32447;&#33410;&#28857;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Online Node Labeling for Very Large Graphs. (arXiv:2305.16257v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36229;&#22823;&#35268;&#27169;&#22270;&#30340;&#22312;&#32447;&#33410;&#28857;&#20998;&#31867;&#31639;&#27861;FastONL&#65292;&#23427;&#22522;&#20110;&#24191;&#20041;&#23616;&#37096;&#25512;&#36865;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#36817;&#20284;&#36870;&#30697;&#38453;&#21015;&#24182;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;&#22270;&#26680;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#36951;&#25022;&#20540;&#21644;&#27599;&#20010;&#39044;&#27979;&#30340;&#36739;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#23548;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#22312;&#32447;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#24403;&#21069;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22312;$\mathcal{O}(n^3)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;$\mathcal{O}(n^2)$&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#20869;&#27714;&#35299;&#22270;&#26680;&#30697;&#38453;&#30340;&#36870;&#65292;&#35201;&#20040;&#38656;&#35201;&#37319;&#26679;&#22823;&#37327;&#30340;&#38543;&#26426;&#29983;&#25104;&#26641;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#26494;&#24347;&#25216;&#26415;&#30340;&#25913;&#36827;&#31639;&#27861;&#12290;&#24403;&#36866;&#24403;&#36873;&#25321;&#21442;&#25968;&#21270;&#30340;&#22270;&#26680;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#36951;&#25022;&#20540;&#20026;$\mathcal{O}(\sqrt{n^{1+\gamma}})$&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#35813;&#26494;&#24347;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#31639;&#27861;FastONL&#65292;&#20854;&#36951;&#25022;&#20540;&#20026;$\mathcal{O}(k\sqrt{n^{1+\gamma}})$&#12290;FastONL&#30340;&#20851;&#38190;&#26159;&#19968;&#31181;&#24191;&#20041;&#23616;&#37096;&#25512;&#36865;&#26041;&#27861;&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#36817;&#20284;&#36870;&#30697;&#38453;&#21015;&#24182;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;&#22270;&#26680;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#39044;&#27979;&#30340;&#25104;&#26412;&#20026;$\mathcal{O}(\text{vol}({\mathcal{S}})\log 1/\epsilon)$
&lt;/p&gt;
&lt;p&gt;
This paper studies the online node classification problem under a transductive learning setting. Current methods either invert a graph kernel matrix with $\mathcal{O}(n^3)$ runtime and $\mathcal{O}(n^2)$ space complexity or sample a large volume of random spanning trees, thus are difficult to scale to large graphs. In this work, we propose an improvement based on the \textit{online relaxation} technique introduced by a series of works (Rakhlin et al.,2012; Rakhlin and Sridharan, 2015; 2017). We first prove an effective regret $\mathcal{O}(\sqrt{n^{1+\gamma}})$ when suitable parameterized graph kernels are chosen, then propose an approximate algorithm FastONL enjoying $\mathcal{O}(k\sqrt{n^{1+\gamma}})$ regret based on this relaxation. The key of FastONL is a \textit{generalized local push} method that effectively approximates inverse matrix columns and applies to a series of popular kernels. Furthermore, the per-prediction cost is $\mathcal{O}(\text{vol}({\mathcal{S}})\log 1/\epsilon)$
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#30830;&#20445;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#19981;&#21457;&#29983;&#30896;&#25758;&#65292;&#24182;&#20351;&#29992; ASP-MAUPF &#31995;&#32479;&#36827;&#34892;&#23454;&#39564;&#65292;&#23545;&#20854;&#36866;&#29992;&#24615;&#21644;&#29615;&#22659;&#20381;&#36182;&#24230;&#36827;&#34892;&#20102;&#35266;&#23519;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.16203</link><description>&lt;p&gt;
&#35745;&#31639;&#22810;&#26234;&#33021;&#20307;&#37096;&#20998;&#21487;&#35266;&#27979;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#36890;&#29992;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
On Computing Universal Plans for Partially Observable Multi-Agent Path Finding. (arXiv:2305.16203v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#30830;&#20445;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#19981;&#21457;&#29983;&#30896;&#25758;&#65292;&#24182;&#20351;&#29992; ASP-MAUPF &#31995;&#32479;&#36827;&#34892;&#23454;&#39564;&#65292;&#23545;&#20854;&#36866;&#29992;&#24615;&#21644;&#29615;&#22659;&#20381;&#36182;&#24230;&#36827;&#34892;&#20102;&#35266;&#23519;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#22312;&#29616;&#20170;&#24191;&#27867;&#24212;&#29992;&#20110;&#20179;&#24211;&#26426;&#22120;&#20154;&#12289;&#29289;&#27969;&#33258;&#21160;&#21270;&#12289;&#20132;&#36890;&#25511;&#21046;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#23558;&#20854;&#30475;&#20316;&#26159;&#36890;&#29992;&#35268;&#21010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35745;&#21010;&#65288;&#21448;&#31216;&#31574;&#30053;&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026; ASP-MAUPF &#30340;&#31995;&#32479;&#26469;&#35745;&#31639;&#23427;&#20204;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#22312;&#20219;&#24847;&#20108;&#32500;&#22320;&#22270;&#21644;&#26234;&#33021;&#20307;&#30446;&#26631;&#37197;&#32622;&#19979;&#65292;&#25214;&#21040;&#19968;&#20010;&#36866;&#29992;&#20110;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#36890;&#29992;&#35745;&#21010;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#20043;&#38388;&#20114;&#19981;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent routing problems have drawn significant attention nowadays due to their broad industrial applications in, e.g., warehouse robots, logistics automation, and traffic control. Conventionally, they are modelled as classical planning problems. In this paper, we argue that it is beneficial to formulate them as universal planning problems. We therefore propose universal plans, also known as policies, as the solution concepts, and implement a system called ASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for computing them. Given an arbitrary two-dimensional map and a profile of goals for the agents, the system finds a feasible universal plan for each agent that ensures no collision with others. We use the system to conduct some experiments, and make some observations on the types of goal profiles and environments that will have feasible policies, and how they may depend on agents' sensors. We also demonstrate how users can customize action preferences to c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16044</link><description>&lt;p&gt;
&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23558;&#22122;&#22768;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#26159;&#22823;&#33041;&#38750;&#20961;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24182;&#24050;&#25104;&#20026;&#31070;&#32463;&#24418;&#24577;&#26234;&#33021;&#30340;&#25903;&#26609;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#37319;&#29992;&#24102;&#26377;&#22122;&#22768;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26174;&#31034;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#29702;&#35770;&#19978;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;NDL&#20026;&#20195;&#29702;&#26799;&#24230;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#23558;&#21508;&#31181;SNN&#26550;&#26500;&#21644;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#30830;&#23450;&#24615;SNNs&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#65288;&#31216;&#20026;Cauchy&#27493;&#24577;&#65289;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#26222;&#36941;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15559</link><description>&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#30340;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#26222;&#36941;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse square Levy walk emerging universally in goal-oriented tasks. (arXiv:2305.15559v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#65288;&#31216;&#20026;Cauchy&#27493;&#24577;&#65289;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#26222;&#36941;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Levy&#27493;&#24577;&#20013;&#65292;&#27493;&#38271;&#20986;&#29616;&#39057;&#29575;&#36981;&#24490;&#24130;&#24459;&#20998;&#24067;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#29983;&#29289;&#30340;&#36801;&#31227;&#34892;&#20026;&#20013;&#35266;&#23519;&#21040;&#12290;&#35266;&#23519;&#21040;&#20102;&#25509;&#36817;&#20110;2&#30340;&#24130;&#25351;&#25968;&#30340;Levy&#27493;&#24577;&#65292;&#20294;&#20854;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26222;&#36941;&#20135;&#29983;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#65288;&#31216;&#20026;Cauchy&#27493;&#24577;&#65289;&#30340;&#27169;&#22411;&#65292;&#24182;&#30830;&#23450;&#20986;Cauchy&#27493;&#24577;&#20986;&#29616;&#26465;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#30446;&#26631;&#23548;&#21521;&#30340;&#20219;&#21153;&#20013;&#65292;Cauchy&#27493;&#24577;&#26222;&#36941;&#20986;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#26415;&#35821;&#8220;&#30446;&#26631;&#23548;&#21521;&#8221;&#65292;&#24403;&#30446;&#26631;&#26126;&#30830;&#26102;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#24335;&#23454;&#29616;&#65292;&#32780;&#26080;&#27861;&#30830;&#23450;&#21807;&#19968;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#25311;&#65292;&#19968;&#20010;&#20195;&#29702;&#35266;&#23519;&#21040;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#20174;&#27010;&#29575;&#20998;&#24067;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#24182;&#36830;&#32493;&#20272;&#35745;&#35813;&#27010;&#29575;&#20998;&#24067;&#30340;&#20013;&#24515;&#22352;&#26631;&#12290;&#20195;&#29702;&#26377;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#24182;&#21487;&#20197;&#20462;&#25913;&#35813;&#27169;&#22411;&#65292;&#20197;&#20351;&#20854;&#26356;&#31526;&#21512;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Levy walk in which the frequency of occurrence of step lengths follows a power-law distribution, can be observed in the migratory behavior of organisms at various levels. Levy walks with power exponents close to 2 are observed, and the reasons are unclear. This study aims to propose a model that universally generates inverse square Levy walks (called Cauchy walks) and to identify the conditions under which Cauchy walks appear. We demonstrate that Cauchy walks emerge universally in goal-oriented tasks. We use the term "goal-oriented" when the goal is clear, but this can be achieved in different ways, which cannot be uniquely determined. We performed a simulation in which an agent observed the data generated from a probability distribution in a two-dimensional space and successively estimated the central coordinates of that probability distribution. The agent has a model of probability distribution as a hypothesis for data-generating distribution and can modify the model such that ea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#21442;&#25968;&#25968;&#25454;&#23545;&#36710;&#36742;&#36827;&#34892;&#35780;&#20998;&#39044;&#27979;&#65292;&#22686;&#21152;&#20102;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15218</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#22312;&#36710;&#36742;&#35780;&#20998;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#21442;&#25968;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Machine Learning for Vehicle Rating Predictions Using Image, Text, and Parametric Data. (arXiv:2305.15218v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#21442;&#25968;&#25968;&#25454;&#23545;&#36710;&#36742;&#36827;&#34892;&#35780;&#20998;&#39044;&#27979;&#65292;&#22686;&#21152;&#20102;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#36710;&#36742;&#35780;&#20998;&#39044;&#27979;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#21644;&#37197;&#32622;&#22909;&#30340;&#36710;&#36742;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24335;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#20174;&#36710;&#36742;&#21442;&#25968;&#12289;&#25991;&#26412;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#39044;&#27979;&#20116;&#31181;&#36710;&#36742;&#35780;&#20998;&#65292;&#21253;&#25324;&#24635;&#20998;&#21644;&#35780;&#20215;&#20998;&#25968;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate vehicle rating prediction can facilitate designing and configuring good vehicles. This prediction allows vehicle designers and manufacturers to optimize and improve their designs in a timely manner, enhance their product performance, and effectively attract consumers. However, most of the existing data-driven methods rely on data from a single mode, e.g., text, image, or parametric data, which results in a limited and incomplete exploration of the available information. These methods lack comprehensive analyses and exploration of data from multiple modes, which probably leads to inaccurate conclusions and hinders progress in this field. To overcome this limitation, we propose a multi-modal learning model for more comprehensive and accurate vehicle rating predictions. Specifically, the model simultaneously learns features from the parametric specifications, text descriptions, and images of vehicles to predict five vehicle rating scores, including the total score, critics score,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2305.15151</link><description>&lt;p&gt;
&#30693;&#35782;&#35774;&#35745;&#65306;&#36890;&#36807;&#30693;&#35782;&#25552;&#28860;&#25512;&#21160;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement. (arXiv:2305.15151v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#65292;&#23547;&#25214;&#25240;&#21472;&#20026;&#25152;&#26399;&#26395;&#32467;&#26500;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#24050;&#32463;&#21462;&#24471;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#30053;&#20102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#37325;&#35201;&#24615;&#65292;&#26410;&#33021;&#35206;&#30422;&#24191;&#27867;&#30340;&#34507;&#30333;&#36136;&#31354;&#38388;&#65292;&#24182;&#19988;&#27809;&#26377;&#34701;&#20837;&#24120;&#35265;&#30340;&#34507;&#30333;&#36136;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#26469;&#33410;&#30465;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;CATH&#12289;TS50&#21644;TS500&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#30693;&#35782;&#35774;&#35745;&#26041;&#27861;&#22312;CATH&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;PiFold&#26041;&#27861;&#32422;9&#65285;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30693;&#35782;&#35774;&#35745;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown competitive performance in protein design that aims to find the amino acid sequence folding into the desired structure. However, most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. After witnessing the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, and TS500 datasets and our results show that our Knowledge-Design method outperforms the previous PiFold method by approximately 9\% on the CATH dataset. Specifically, Knowledge-Design is the first method that achieves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#21040;&#39564;&#35777;&#22270;&#20687;&#29983;&#25104;(NL2VI)&#26041;&#27861;&#65292;&#23558;&#33258;&#28982;&#25552;&#31034;&#36716;&#21270;&#20026;&#26356;&#36866;&#21512;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#36890;&#36807;VQA&#31639;&#27861;&#36827;&#34892;&#39564;&#35777;&#65292;&#20351;&#29983;&#25104;&#30340;&#22270;&#20687;&#26356;&#21152;&#20934;&#30830;&#21644;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#32852;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15026</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;&#21487;&#39564;&#35777;&#30340;&#22270;&#20687;&#29983;&#25104;&#65306;&#20256;&#36882;&#35270;&#35273;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transferring Visual Attributes from Natural Language to Verified Image Generation. (arXiv:2305.15026v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#21040;&#39564;&#35777;&#22270;&#20687;&#29983;&#25104;(NL2VI)&#26041;&#27861;&#65292;&#23558;&#33258;&#28982;&#25552;&#31034;&#36716;&#21270;&#20026;&#26356;&#36866;&#21512;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#36890;&#36807;VQA&#31639;&#27861;&#36827;&#34892;&#39564;&#35777;&#65292;&#20351;&#29983;&#25104;&#30340;&#22270;&#20687;&#26356;&#21152;&#20934;&#30830;&#21644;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#32852;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;(T2I)&#22312;&#29983;&#25104;&#33402;&#26415;&#21644;&#20854;&#20182;&#21019;&#36896;&#24615;&#20135;&#21697;&#26041;&#38754;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#35270;&#35273;&#24187;&#35273;&#21487;&#33021;&#26159;&#21019;&#36896;&#21147;&#21463;&#21040;&#35748;&#21487;&#30340;&#24773;&#20917;&#19979;&#30340;&#31215;&#26497;&#22240;&#32032;&#65292;&#20294;&#36825;&#20123;&#20135;&#21697;&#19981;&#36866;&#29992;&#20110;&#38656;&#35201;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#32852;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#35821;&#35328;&#20013;&#21253;&#25324;&#38750;&#35270;&#35273;&#20449;&#24687;&#20197;&#21450;&#38656;&#35201;&#20934;&#30830;&#29983;&#25104;&#30340;&#25991;&#26412;&#20803;&#32032;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#21040;&#39564;&#35777;&#22270;&#20687;&#29983;&#25104;(NL2VI)&#26041;&#27861;&#65292;&#23558;&#33258;&#28982;&#25552;&#31034;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;&#22270;&#20687;&#29983;&#25104;&#30340;&#35270;&#35273;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;T2I&#27169;&#22411;&#22522;&#20110;&#35270;&#35273;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#65292;&#20877;&#20351;&#29992;VQA&#31639;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#33258;&#28982;&#25552;&#31034;&#19982;NL2VI&#20013;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#23545;&#40784;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22522;&#30784;&#65292;&#24182;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#23383;&#24149;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text to image generation methods (T2I) are widely popular in generating art and other creative artifacts. While visual hallucinations can be a positive factor in scenarios where creativity is appreciated, such artifacts are poorly suited for cases where the generated image needs to be grounded in complex natural language without explicit visual elements. In this paper, we propose to strengthen the consistency property of T2I methods in the presence of natural complex language, which often breaks the limits of T2I methods by including non-visual information, and textual elements that require knowledge for accurate generation. To address these phenomena, we propose a Natural Language to Verified Image generation approach (NL2VI) that converts a natural prompt into a visual prompt, which is more suitable for image generation. A T2I model then generates an image for the visual prompt, which is then verified with VQA algorithms. Experimentally, aligning natural prompts with image generation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#65292;&#33021;&#22815;&#20351;&#29992;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#34920;&#36798;&#32422;&#26463;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#24182;&#20445;&#35777;&#32422;&#26463;&#26465;&#20214;&#30340;&#27010;&#29575;&#36275;&#22815;&#39640;&#12290;&#21516;&#26102;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#22810;&#26234;&#33021;&#20307;&#35774;&#32622;&#36827;&#34892;&#26368;&#20248;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.14736</link><description>&lt;p&gt;
&#36923;&#36753;&#32422;&#26463;&#19979;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26368;&#20248;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Optimal Control of Logically Constrained Partially Observable and Multi-Agent Markov Decision Processes. (arXiv:2305.14736v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#65292;&#33021;&#22815;&#20351;&#29992;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#34920;&#36798;&#32422;&#26463;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#24182;&#20445;&#35777;&#32422;&#26463;&#26465;&#20214;&#30340;&#27010;&#29575;&#36275;&#22815;&#39640;&#12290;&#21516;&#26102;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#22810;&#26234;&#33021;&#20307;&#35774;&#32622;&#36827;&#34892;&#26368;&#20248;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#31995;&#32479;&#36890;&#24120;&#20250;&#20135;&#29983;&#36923;&#36753;&#32422;&#26463;&#65292;&#20363;&#22914;&#26469;&#33258;&#23433;&#20840;&#12289;&#25805;&#20316;&#25110;&#27861;&#35268;&#35201;&#27714;&#65292;&#21487;&#20197;&#29992;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#34920;&#36798;&#36825;&#20123;&#32422;&#26463;&#12290;&#31995;&#32479;&#29366;&#24577;&#36890;&#24120;&#26159;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#65292;&#21487;&#33021;&#21253;&#21547;&#20855;&#26377;&#20849;&#21516;&#30446;&#26631;&#20294;&#19981;&#21516;&#20449;&#24687;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#22810;&#20010;&#26234;&#33021;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#65292;&#29992;&#20110;&#20855;&#26377;&#26377;&#38480;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#32422;&#26463;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#28385;&#36275;&#26102;&#38388;&#36923;&#36753;&#32422;&#26463;&#30340;&#27010;&#29575;&#36275;&#22815;&#39640;&#26102;&#26368;&#22823;&#21270;&#32047;&#31215;&#22238;&#25253;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20851;&#20110;&#36817;&#20284;&#22870;&#21169;&#26368;&#20248;&#24615;&#21644;&#32422;&#26463;&#28385;&#36275;&#30340;&#20445;&#35777;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#20855;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#35774;&#32622;&#36827;&#34892;&#26368;&#20248;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#35813;&#26041;&#27861;&#24182;&#32473;&#20986;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous systems often have logical constraints arising, for example, from safety, operational, or regulatory requirements. Such constraints can be expressed using temporal logic specifications. The system state is often partially observable. Moreover, it could encompass a team of multiple agents with a common objective but disparate information structures and constraints. In this paper, we first introduce an optimal control theory for partially observable Markov decision processes (POMDPs) with finite linear temporal logic constraints. We provide a structured methodology for synthesizing policies that maximize a cumulative reward while ensuring that the probability of satisfying a temporal logic constraint is sufficiently high. Our approach comes with guarantees on approximate reward optimality and constraint satisfaction. We then build on this approach to design an optimal control framework for logically constrained multi-agent settings with information asymmetry. We illustrate the
&lt;/p&gt;</description></item><item><title>LoReTTa&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#20013;&#36716;&#25442;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14243</link><description>&lt;p&gt;
&#20869;&#23481;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#35757;&#32451;&#19982; LoReTTa
&lt;/p&gt;
&lt;p&gt;
Training Transitive and Commutative Multimodal Transformers with LoReTTa. (arXiv:2305.14243v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14243
&lt;/p&gt;
&lt;p&gt;
LoReTTa&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#20013;&#36716;&#25442;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#65292;&#25910;&#38598;&#20004;&#20010;&#21305;&#37197;&#30340;&#24418;&#24577;A&#21644;B&#25110;B&#21644;C&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#24456;&#22256;&#38590;&#65292;&#33719;&#24471;&#21253;&#21547;&#19977;&#20010;&#23545;&#40784;&#24418;&#24577;A&#12289;B&#21644;C&#30340;&#25968;&#25454;&#38598;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoReTTa&#20197;&#24212;&#23545;&#36825;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#24314;&#27169;&#21644;&#20132;&#25442;&#24459;&#21644;&#20256;&#36882;&#24615;&#30340;&#35268;&#21017;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#20013;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#21512;&#25104;&#26174;&#30528;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting a multimodal dataset with two paired modalities A and B or B and C is difficult in practice. Obtaining a dataset with three aligned modalities A, B, and C is even more challenging. For example, some public medical datasets have only genetic sequences and microscopic images for one patient, and only genetic sequences and radiological images for another - but no dataset includes both microscopic and radiological images for the same patient. This makes it difficult to integrate and combine all modalities into a large pre-trained neural network. We introduce LoReTTa (Linking mOdalities with a tRansitive and commutativE pre-Training sTrAtegy) to address this understudied problem. Our self-supervised framework combines causal masked modeling with the rules of commutativity and transitivity to transition within and between different modalities. Thus, it can model the relation A -&gt; C with A -&gt; B -&gt; C. Given a dataset containing only the disjoint combinations (A, B) and (B, C), we sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20174;&#21477;&#32534;&#36753;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#30340;&#35821;&#35328;&#27169;&#22411;&#32416;&#38169;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;SQL&#26597;&#35810;&#34920;&#31034;&#25913;&#36827;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#20102;2.4-6.5&#65292;&#26368;&#22810;&#25552;&#39640;4.3&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.13073</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#30340;&#35821;&#35328;&#27169;&#22411;&#32416;&#38169;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL Error Correction with Language Models of Code. (arXiv:2305.13073v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20174;&#21477;&#32534;&#36753;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#30340;&#35821;&#35328;&#27169;&#22411;&#32416;&#38169;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;SQL&#26597;&#35810;&#34920;&#31034;&#25913;&#36827;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#20102;2.4-6.5&#65292;&#26368;&#22810;&#25552;&#39640;4.3&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#20173;&#19981;&#22815;&#20934;&#30830;&#20197;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#26500;&#24314;&#33258;&#21160;&#25991;&#26412;&#21040;SQL&#32416;&#38169;&#27169;&#22411;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#21333;&#35789;&#23618;&#38754;&#30340;&#32534;&#36753;&#32570;&#20047;&#19978;&#19979;&#25991;&#24182;&#19988;&#26377;&#26102;&#19981;&#26126;&#30830;&#65292;&#22240;&#27492;&#25552;&#20986;&#26500;&#24314;&#20174;&#21477;&#32534;&#36753;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#22823;&#22810;&#25968;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#19987;&#38376;&#39044;&#35757;&#32451;SQL&#65292;&#20294;&#23427;&#20204;&#29087;&#24713;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#25968;&#25454;&#32467;&#26500;&#21644;&#20854;&#25805;&#20316;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SQL&#26597;&#35810;&#34920;&#31034;&#21450;&#20854;&#32534;&#36753;&#26041;&#27861;&#65292;&#26356;&#31526;&#21512;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#30340;&#38169;&#35823;&#32416;&#38169;&#27169;&#22411;&#25552;&#39640;&#20102;&#19981;&#21516;&#35299;&#26512;&#22120;&#30340;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#20102;2.4-6.5&#65292;&#24182;&#33719;&#24471;&#20102;&#20004;&#20010;&#24378;&#22522;&#32447;&#30340;&#32477;&#23545;&#25913;&#36827;&#26368;&#22810;4.3&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/OSU-NLP-Group/Auto-SQL-Correction &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in text-to-SQL parsing, current semantic parsers are still not accurate enough for practical use. In this paper, we investigate how to build automatic text-to-SQL error correction models. Noticing that token-level edits are out of context and sometimes ambiguous, we propose building clause-level edit models instead. Besides, while most language models of code are not specifically pre-trained for SQL, they know common data structures and their operations in programming languages such as Python. Thus, we propose a novel representation for SQL queries and their edits that adheres more closely to the pre-training corpora of language models of code. Our error correction model improves the exact set match accuracy of different parsers by 2.4-6.5 and obtains up to 4.3 point absolute improvement over two strong baselines. Our code and data are available at https://github.com/OSU-NLP-Group/Auto-SQL-Correction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13060</link><description>&lt;p&gt;
&#20511;&#21161;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36139;&#27665;&#31391;&#36947;&#36335;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Road Planning for Slums via Deep Reinforcement Learning. (arXiv:2305.13060v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#30334;&#19975;&#36139;&#27665;&#31391;&#23621;&#27665;&#30001;&#20110;&#36139;&#27665;&#31391;&#20869;&#19981;&#36275;&#30340;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#32780;&#36973;&#21463;&#22478;&#24066;&#26381;&#21153;&#26080;&#27861;&#35775;&#38382;&#30340;&#22256;&#22659;&#65292;&#32780;&#36139;&#27665;&#31391;&#36947;&#36335;&#35268;&#21010;&#23545;&#22478;&#24066;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#37325;&#32452;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#65292;&#19981;&#33021;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#36139;&#27665;&#31391;&#65292;&#35201;&#20040;&#22312;&#21487;&#36798;&#24615;&#21644;&#24314;&#35774;&#25104;&#26412;&#26041;&#38754;&#20135;&#29983;&#27425;&#20248;&#30340;&#36947;&#36335;&#35268;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#33719;&#36139;&#27665;&#31391;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#36873;&#25321;&#35745;&#21010;&#36947;&#36335;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36830;&#25509;&#36139;&#27665;&#31391;&#22320;&#28857;&#30340;&#36947;&#36335;&#35268;&#21010;&#65292;&#20197;&#26368;&#23567;&#30340;&#24314;&#35774;&#25104;&#26412;&#12290;&#23545;&#19981;&#21516;&#22269;&#23478;&#30340;&#30495;&#23454;&#36139;&#27665;&#31391;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Millions of slum dwellers suffer from poor accessibility to urban services due to inadequate road infrastructure within slums, and road planning for slums is critical to the sustainable development of cities. Existing re-blocking or heuristic methods are either time-consuming which cannot generalize to different slums, or yield sub-optimal road plans in terms of accessibility and construction costs. In this paper, we present a deep reinforcement learning based approach to automatically layout roads for slums. We propose a generic graph model to capture the topological structure of a slum, and devise a novel graph neural network to select locations for the planned roads. Through masked policy optimization, our model can generate road plans that connect places in a slum at minimal construction costs. Extensive experiments on real-world slums in different countries verify the effectiveness of our model, which can significantly improve accessibility by 14.3% against existing baseline metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13030</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations. (arXiv:2305.13030v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#23545;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#29983;&#25104;&#28789;&#27963;&#21644;&#22810;&#26679;&#21270;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#65307;&#28982;&#32780;&#65292;&#22312;&#24314;&#27169;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#26102;&#65292;&#22312;&#28304;&#65288;&#21363;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65289;&#21644;&#30446;&#26631;&#65288;&#21363; RL &#30340;&#32593;&#32476;&#31354;&#38388;&#65289;&#20043;&#38388;&#23384;&#22312;&#22495;&#24046;&#24322;&#65292;&#24182;&#19988;&#28304;&#29615;&#22659;&#21442;&#25968;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30340;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892; RL &#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#32467;&#21512; RL &#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#30340;&#28436;&#31034;&#21160;&#20316;&#26469;&#22312; RL &#20013;&#21033;&#29992;&#26410;&#30693;&#28304;&#21160;&#24577;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20026;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling of real-world biological multi-agents is a fundamental problem in various scientific and engineering fields. Reinforcement learning (RL) is a powerful framework to generate flexible and diverse behaviors in cyberspace; however, when modeling real-world biological multi-agents, there is a domain gap between behaviors in the source (i.e., real-world data) and the target (i.e., cyberspace for RL), and the source environment parameters are usually unknown. In this paper, we propose a method for adaptive action supervision in RL from real-world demonstrations in multi-agent scenarios. We adopt an approach that combines RL and supervised learning by selecting actions of demonstrations in RL based on the minimum distance of dynamic time warping for utilizing the information of the unknown source dynamics. This approach can be easily applied to many existing neural network architectures and provide us with an RL model balanced between reproducibility as imitation and generalization ab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#27169;&#22411;&#30340;&#32479;&#19968;&#26426;&#22120;&#20154;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#26725;&#25509;&#20027;&#21160;&#25506;&#32034;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12240</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#23398;&#20064;&#30340;&#20027;&#21160;&#25506;&#32034;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Bridging Active Exploration and Uncertainty-Aware Deployment Using Probabilistic Ensemble Neural Network Dynamics. (arXiv:2305.12240v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#27169;&#22411;&#30340;&#32479;&#19968;&#26426;&#22120;&#20154;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#26725;&#25509;&#20027;&#21160;&#25506;&#32034;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;-based&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22240;&#20854;&#22312;&#35299;&#20915;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290; &#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#36827;&#27493;&#65292;&#35813;&#26041;&#27861;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#36890;&#36807;&#23398;&#20064;&#26410;&#30693;&#25110;&#37096;&#20998;&#24050;&#30693;&#30340;&#26426;&#22120;&#20154;&#21160;&#24577;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#25511;&#21046;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290; &#20027;&#21160;&#25506;&#32034;&#26159;&#25968;&#25454;&#26377;&#25928;&#25910;&#38598;&#21644;&#26368;&#23567;&#21270;&#20154;&#31867;&#30417;&#30563;&#30340;&#20851;&#38190;&#65292;&#23427;&#20351;&#26426;&#22120;&#20154;&#25351;&#24341;&#33258;&#24049;&#21040;&#23548;&#33268;&#26368;&#39640;&#20449;&#24687;&#22686;&#30410;&#30340;&#29366;&#24577;&#12290; &#21516;&#26679;&#65292;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#26159;&#26426;&#22120;&#20154;&#25511;&#21046;&#20013;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#20851;&#27880;&#28857;&#65292;&#22240;&#20026;&#30001;&#25152;&#23398;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;&#30340;&#19981;&#30830;&#23450;&#21160;&#20316;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#36816;&#21160;&#25110;&#22833;&#36133;&#12290; &#20294;&#26159;&#65292;&#20027;&#21160;&#25506;&#32034;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24050;&#32463;&#29420;&#31435;&#30740;&#31350;&#65292;&#24182;&#19988;&#32570;&#20047;&#26080;&#32541;&#38598;&#25104;&#23427;&#20204;&#30340;&#25991;&#29486;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26725;&#25509;&#26426;&#22120;&#20154;&#25511;&#21046;&#20013;&#30340;&#20027;&#21160;&#25506;&#32034;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#12290; &#35813;&#26694;&#26550;&#20351;&#29992;&#27010;&#29575;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#27169;&#22411;&#26469;&#25429;&#33719;&#25152;&#23398;&#31995;&#32479;&#21160;&#24577;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24341;&#23548;&#26426;&#22120;&#20154;&#22312;&#36991;&#20813;&#19981;&#30830;&#23450;&#21306;&#22495;&#30340;&#21516;&#26102;&#39640;&#25928;&#22320;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#12290; &#35813;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#32479;&#19968;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, learning-based control in robotics has gained significant attention due to its capability to address complex tasks in real-world environments. With the advances in machine learning algorithms and computational capabilities, this approach is becoming increasingly important for solving challenging control problems in robotics by learning unknown or partially known robot dynamics. Active exploration, in which a robot directs itself to states that yield the highest information gain, is essential for efficient data collection and minimizing human supervision. Similarly, uncertainty-aware deployment has been a growing concern in robotic control, as uncertain actions informed by the learned model can lead to unstable motions or failure. However, active exploration and uncertainty-aware deployment have been studied independently, and there is limited literature that seamlessly integrates them. This paper presents a unified model-based reinforcement learning framework that brid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11426</link><description>&lt;p&gt;
&#21518;&#39564;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Post Hoc Explanations of Language Models Can Improve Language Models. (arXiv:2305.11426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#20154;&#31867;&#27880;&#37322;&#30340;&#21407;&#29702;&#65288;&#20363;&#22914;&#65292;&#24605;&#32500;&#38142;&#25552;&#31034;&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#21407;&#29702;&#21152;&#20837;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#39640;&#24230;&#30340;&#20154;&#24037;&#21442;&#19982;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25918;&#22823;&#27169;&#22411;&#24615;&#33021;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#36755;&#20986;&#31216;&#20026;&#23646;&#24615;&#20998;&#25968;&#65288;&#35299;&#37322;&#65289;&#30340;&#20540;&#65292;&#29992;&#20110;&#25429;&#33719;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33258;&#21160;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#21407;&#29702;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#23646;&#24615;&#20998;&#25968;&#20013;&#33719;&#24471;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AMPLIFY&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of- Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insi
&lt;/p&gt;</description></item><item><title>&#26412;&#27425;&#35770;&#25991;&#24635;&#32467;&#20102;&#33258;&#21160;&#39550;&#39542;&#21644;&#26234;&#33021;&#36710;&#36742;&#20013;&#20851;&#20110;&#25511;&#21046;&#12289;&#35745;&#31639;&#26426;&#31995;&#32479;&#35774;&#35745;&#12289;&#36890;&#20449;&#12289;&#39640;&#31934;&#24230;&#22320;&#22270;&#12289;&#27979;&#35797;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#21457;&#23637;&#24773;&#20917;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.11239</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#21644;&#26234;&#33021;&#36710;&#36742;&#30340;&#37324;&#31243;&#30865; Part I&#65306;&#25511;&#21046;&#12289;&#35745;&#31639;&#26426;&#31995;&#32479;&#35774;&#35745;&#12289;&#36890;&#20449;&#12289;&#39640;&#31934;&#24230;&#22320;&#22270;&#12289;&#27979;&#35797;&#21644;&#20154;&#31867;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Milestones in Autonomous Driving and Intelligent Vehicles Part \uppercase\expandafter{\romannumeral1}: Control, Computing System Design, Communication, HD Map, Testing, and Human Behaviors. (arXiv:2305.11239v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#27425;&#35770;&#25991;&#24635;&#32467;&#20102;&#33258;&#21160;&#39550;&#39542;&#21644;&#26234;&#33021;&#36710;&#36742;&#20013;&#20851;&#20110;&#25511;&#21046;&#12289;&#35745;&#31639;&#26426;&#31995;&#32479;&#35774;&#35745;&#12289;&#36890;&#20449;&#12289;&#39640;&#31934;&#24230;&#22320;&#22270;&#12289;&#27979;&#35797;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#21457;&#23637;&#24773;&#20917;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;(AD)&#21644;&#26234;&#33021;&#36710;&#36742;(IV)&#30340;&#20852;&#36259;&#27491;&#22312;&#36805;&#36895;&#22686;&#38271;&#65292;&#22240;&#20026;&#23427;&#20204;&#24102;&#26469;&#30340;&#20415;&#25463;&#12289;&#23433;&#20840;&#21644;&#32463;&#27982;&#25928;&#30410;&#12290;&#34429;&#28982;&#19968;&#20123;&#35843;&#26597;&#24050;&#32463;&#22238;&#39038;&#20102;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38480;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#65292;&#24182;&#32570;&#20047;&#22312;&#26410;&#26469;&#30340;&#31995;&#32479;&#24635;&#32467;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20998;&#20026;3&#20010;&#29420;&#31435;&#30340;&#25991;&#31456;&#65292;&#31532;&#19968;&#37096;&#20998;&#26159;&#23545;AD&#21644;IV&#30340;&#24635;&#20307;&#25216;&#26415;&#36827;&#34892;&#35843;&#26597;&#65292;&#21253;&#25324;&#21382;&#21490;&#12289;&#24635;&#32467;&#37324;&#31243;&#30865;&#20197;&#21450;&#25552;&#20379;&#21069;&#30651;&#24615;&#12289;&#20262;&#29702;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#36825;&#26159;&#31532;&#20108;&#37096;&#20998;(Part I)&#30340;&#25216;&#26415;&#35843;&#26597;&#65292;&#29992;&#20110;&#23457;&#26597;IV&#20013;&#25511;&#21046;&#12289;&#35745;&#31639;&#26426;&#31995;&#32479;&#35774;&#35745;&#12289;&#36890;&#20449;&#12289;&#39640;&#28165;&#22320;&#22270;&#12289;&#27979;&#35797;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#21457;&#23637;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#31532;&#19977;&#37096;&#20998;(Part II)&#23558;&#23457;&#26597;&#24863;&#30693;&#21644;&#35268;&#21010;&#37096;&#20998;&#12290;&#26412;&#27425;&#35843;&#26597;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#20851;&#20110;AD&#21644;IV&#39046;&#22495;&#25216;&#26415;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#21644;&#26368;&#26032;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest in autonomous driving (AD) and intelligent vehicles (IVs) is growing at a rapid pace due to the convenience, safety, and economic benefits. Although a number of surveys have reviewed research achievements in this field, they are still limited in specific tasks and lack systematic summaries and research directions in the future. Our work is divided into 3 independent articles and the first part is a Survey of Surveys (SoS) for total technologies of AD and IVs that involves the history, summarizes the milestones, and provides the perspectives, ethics, and future research directions. This is the second part (Part \uppercase\expandafter{\romannumeral1} for this technical survey) to review the development of control, computing system design, communication, High Definition map (HD map), testing, and human behaviors in IVs. In addition, the third part (Part \uppercase\expandafter{\romannumeral2} for this technical survey) is to review the perception and planning sections. The objecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#30340;&#37325;&#25972;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;Fisher&#24230;&#37327;&#23450;&#20041;&#20102;&#19968;&#20010;&#30456;&#20851;&#38271;&#24230;&#20316;&#20026;&#32039;&#23494;&#30456;&#20851;&#30340;&#27010;&#29575;&#20998;&#24067;&#28857;&#20043;&#38388;&#30340;&#21487;&#20998;&#36776;&#24615;(RG)&#23610;&#24230;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#23454;&#39564;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26576;&#20010;&#31995;&#32479;&#26368;&#22823;&#29305;&#24322;&#24615;&#35266;&#23519;&#25968;&#37327;&#30340;&#20195;&#29702;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#20026;&#32473;&#23450;&#31995;&#32479;&#20934;&#22791;&#19968;&#20010;&#22312;&#19978;&#36848;&#23610;&#24230;&#19978;&#31934;&#24230;&#26377;&#38480;&#30340;&#26377;&#25928;&#27169;&#22411;&#65292;&#36825;&#20010;&#23610;&#24230;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#24403;&#21069;&#23454;&#39564;&#35013;&#32622;&#21487;&#20197;&#25506;&#27979;&#21040;&#30340;&#26368;&#22823;&#33021;&#37327;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#25552;&#20986;&#20102;&#19968;&#31181;&#21457;&#29616;&#21644;&#34920;&#24449;&#22522;&#26412;&#29289;&#29702;&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.10491</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Renormalization. (arXiv:2305.10491v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#30340;&#37325;&#25972;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;Fisher&#24230;&#37327;&#23450;&#20041;&#20102;&#19968;&#20010;&#30456;&#20851;&#38271;&#24230;&#20316;&#20026;&#32039;&#23494;&#30456;&#20851;&#30340;&#27010;&#29575;&#20998;&#24067;&#28857;&#20043;&#38388;&#30340;&#21487;&#20998;&#36776;&#24615;(RG)&#23610;&#24230;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#23454;&#39564;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26576;&#20010;&#31995;&#32479;&#26368;&#22823;&#29305;&#24322;&#24615;&#35266;&#23519;&#25968;&#37327;&#30340;&#20195;&#29702;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#20026;&#32473;&#23450;&#31995;&#32479;&#20934;&#22791;&#19968;&#20010;&#22312;&#19978;&#36848;&#23610;&#24230;&#19978;&#31934;&#24230;&#26377;&#38480;&#30340;&#26377;&#25928;&#27169;&#22411;&#65292;&#36825;&#20010;&#23610;&#24230;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#24403;&#21069;&#23454;&#39564;&#35013;&#32622;&#21487;&#20197;&#25506;&#27979;&#21040;&#30340;&#26368;&#22823;&#33021;&#37327;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#25552;&#20986;&#20102;&#19968;&#31181;&#21457;&#29616;&#21644;&#34920;&#24449;&#22522;&#26412;&#29289;&#29702;&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#30340;&#37325;&#25972;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;Fisher&#24230;&#37327;&#26469;&#23450;&#20041;&#19968;&#20010;&#30456;&#20851;&#38271;&#24230;&#65292;&#36825;&#20010;&#38271;&#24230;&#36215;&#21040;&#20102;&#32039;&#23494;&#30456;&#20851;&#30340;&#27010;&#29575;&#20998;&#24067;&#28857;&#20043;&#38388;&#30340;&#21487;&#20998;&#36776;&#24615;(RG)&#23610;&#24230;&#12290;&#36825;&#20010;RG&#23610;&#24230;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#32479;&#35745;&#25512;&#26029;&#23454;&#39564;&#20013;&#23545;&#20110;&#19968;&#20010;&#32473;&#23450;&#31995;&#32479;&#21487;&#20197;&#24471;&#21040;&#30340;&#26368;&#22823;&#29305;&#24322;&#24615;&#35266;&#23519;&#25968;&#37327;&#30340;&#20195;&#29702;&#12290;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#30340;&#20316;&#29992;&#26159;&#20026;&#32473;&#23450;&#31995;&#32479;&#20934;&#22791;&#19968;&#20010;&#22312;&#19978;&#36848;&#23610;&#24230;&#19978;&#31934;&#24230;&#26377;&#38480;&#30340;&#26377;&#25928;&#27169;&#22411;&#12290;&#22312;&#23558;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#29289;&#29702;&#31995;&#32479;&#26102;&#65292;&#36825;&#20010;&#30001;&#20449;&#24687;&#35770;&#20986;&#29616;&#30340;RG&#23610;&#24230;&#33258;&#28982;&#22320;&#34987;&#35782;&#21035;&#20026;&#24403;&#21069;&#23454;&#39564;&#35013;&#32622;&#21487;&#20197;&#25506;&#27979;&#21040;&#30340;&#26368;&#22823;&#33021;&#37327;&#65292;&#22240;&#27492;&#65292;&#36125;&#21494;&#26031;&#37325;&#25972;&#21270;&#25552;&#20986;&#20102;&#19968;&#31181;&#21457;&#29616;&#21644;&#34920;&#24449;&#22522;&#26412;&#29289;&#29702;&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we present a fully information theoretic approach to renormalization inspired by Bayesian statistical inference, which we refer to as Bayesian Renormalization. The main insight of Bayesian Renormalization is that the Fisher metric defines a correlation length that plays the role of an emergent RG scale quantifying the distinguishability between nearby points in the space of probability distributions. This RG scale can be interpreted as a proxy for the maximum number of unique observations that can be made about a given system during a statistical inference experiment. The role of the Bayesian Renormalization scheme is subsequently to prepare an effective model for a given system up to a precision which is bounded by the aforementioned scale. In applications of Bayesian Renormalization to physical systems, the emergent information theoretic scale is naturally identified with the maximum energy that can be probed by current experimental apparatus, and thus Bayesian Renormali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#21457;&#29616;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#20250;&#23545;AI&#24615;&#33021;&#34920;&#29616;&#19981;&#21033;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.10201</link><description>&lt;p&gt;
&#20154;&#20204;&#20132;&#35848;&#65292;AI&#20542;&#21548;&#65306;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;AI&#21028;&#26029;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
People Talking and AI Listening: How Stigmatizing Language in EHR Notes Affect AI Performance. (arXiv:2305.10201v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#21457;&#29616;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#20250;&#23545;AI&#24615;&#33021;&#34920;&#29616;&#19981;&#21033;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;(EHRs)&#26159;&#26399;&#26395;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;(AI)-&#39537;&#21160;&#30340;&#21307;&#30103;&#36716;&#22411;&#30340;&#37325;&#35201;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#21453;&#26144;&#22312;EHR&#31508;&#35760;&#20013;&#30340;&#20020;&#24202;&#21307;&#24072;&#20559;&#35265;&#21487;&#33021;&#20250;&#23548;&#33268;AI&#27169;&#22411;&#32487;&#25215;&#24182;&#25918;&#22823;&#36825;&#20123;&#20559;&#35265;&#65292;&#20174;&#32780;&#19981;&#26029;&#21152;&#21095;&#20581;&#24247;&#19978;&#30340;&#19981;&#24179;&#31561;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;EHR&#31508;&#35760;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;(SL)&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#19981;&#21033;&#20110;AI&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#31361;&#20986;&#20102;SL&#20316;&#20026;AI&#27169;&#22411;&#21457;&#23637;&#20013;&#31181;&#26063;&#24046;&#24322;&#30340;&#19968;&#31181;&#26469;&#28304;&#12290;&#20026;&#25506;&#32034;&#19968;&#31181;&#25805;&#20316;&#19978;&#26377;&#25928;&#30340;&#32531;&#35299;SL&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20020;&#24202;&#21307;&#29983;&#21327;&#20316;&#32593;&#32476;&#20013;SL&#29983;&#25104;&#30340;&#27169;&#24335;&#65292;&#21457;&#29616;&#20013;&#22830;&#21307;&#29983;&#23545;AI&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#24046;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21024;&#38500;&#20013;&#22830;&#20020;&#24202;&#21307;&#29983;&#25776;&#20889;&#30340;SL&#26159;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#20020;&#24202;&#21307;&#29983;&#32780;&#35328;&#65292;&#32531;&#35299;SL&#23545;AI&#24615;&#33021;&#24433;&#21709;&#30340;&#26356;&#20026;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;&#21453;&#26144;&#22312;EHR&#31508;&#35760;&#20013;&#30340;&#20020;&#24202;&#21307;&#24072;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) serve as an essential data source for the envisioned artificial intelligence (AI)-driven transformation in healthcare. However, clinician biases reflected in EHR notes can lead to AI models inheriting and amplifying these biases, perpetuating health disparities. This study investigates the impact of stigmatizing language (SL) in EHR notes on mortality prediction using a Transformer-based deep learning model and explainable AI (XAI) techniques. Our findings demonstrate that SL written by clinicians adversely affects AI performance, particularly so for black patients, highlighting SL as a source of racial disparity in AI model development. To explore an operationally efficient way to mitigate SL's impact, we investigate patterns in the generation of SL through a clinicians' collaborative network, identifying central clinicians as having a stronger impact on racial disparity in the AI model. We find that removing SL written by central clinicians is a more 
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08040</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#27744;&#21270;&#30340;&#21487;&#35777;&#26126;&#22810;&#23454;&#20363;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26032;&#22411;&#24212;&#29992;&#65292;&#29992;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#65292;&#20854;&#20013;&#23558;&#21333;&#20010;&#31867;&#26631;&#31614;&#20998;&#37197;&#32473;&#19968;&#32452;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#24739;&#32773;&#30340;&#22810;&#20010;CT&#25195;&#25551;&#30340;&#22810;&#20010;2D&#20999;&#29255;&#65289;&#12290;&#25105;&#20204;&#22312;DAM&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;MIL&#20013;&#34987;&#24573;&#30053;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#21363;&#21253;&#22823;&#23567;&#36807;&#22823;&#65292;&#26080;&#27861;&#22312;&#21453;&#21521;&#20256;&#25773;&#26102;&#21152;&#36733;&#21040;GPU&#20869;&#23384;&#20013;&#65292;&#36825;&#26159;MIL&#26631;&#20934;&#27744;&#21270;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20851;&#20110;&#27719;&#32858;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#26500;&#36896;&#20026;&#22810;&#32423;&#32452;&#21512;&#20989;&#25968;&#12290;&#36890;&#36807;&#32508;&#21512;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#21644;&#38750;&#20984;&#26497;&#23567;&#26368;&#22823;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19988;&#21487;&#35777;&#26126;&#30340;&#22810;&#23454;&#20363;DAM&#65288;MIDAM&#65289;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#26368;&#22823;&#27744;&#21270;&#25110;&#38543;&#26426;&#27880;&#24847;&#21147;&#27744;&#21270;&#65292;&#20165;&#23545;&#27599;&#20010;&#21253;&#23545;&#24212;&#30340;&#23454;&#20363;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#26469;&#35745;&#31639; sto&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
&lt;/p&gt;</description></item><item><title>&#33258;&#20027;GIS&#26159;&#19968;&#31181;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65292;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#20855;&#26377;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65306;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;</title><link>http://arxiv.org/abs/2305.06453</link><description>&lt;p&gt;
&#33258;&#20027;GIS&#65306;&#19979;&#19968;&#20195;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;GIS
&lt;/p&gt;
&lt;p&gt;
Autonomous GIS: the next-generation AI-powered GIS. (arXiv:2305.06453v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06453
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;GIS&#26159;&#19968;&#31181;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65292;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#20855;&#26377;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65306;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914; ChatGPT &#65292;&#23637;&#31034;&#20102;&#23545;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#25512;&#29702;&#12289;&#21019;&#36896;&#24615;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#32763;&#35793;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24212;&#29992;&#19982;&#25506;&#32034;&#12290;&#25105;&#20204;&#37319;&#29992;LLM&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;&#8220;&#33258;&#20027;GIS&#8221;&#30340;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65288;GIS&#65289;&#65292;&#20197;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#26469;&#35299;&#20915;&#31354;&#38388;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#24819;&#65292;&#33258;&#20027;GIS&#23558;&#38656;&#35201;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65292;&#21253;&#25324;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#20027;GIS&#30340;&#35774;&#35745;&#21407;&#21017;&#26469;&#23454;&#29616;&#36825;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65292;&#20174;&#20449;&#24687;&#20805;&#20998;&#24615;&#12289;LLM&#33021;&#21147;&#21644;&#20195;&#29702;&#26550;&#26500;&#19977;&#20010;&#26041;&#38754;&#36827;&#34892;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#31216;&#20026;LLM-Geo &#65292;&#23427;&#22312;Python&#29615;&#22659;&#20013;&#20351;&#29992;GPT-4 API&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, demonstrate a strong understanding of human natural language and have been explored and applied in various fields, including reasoning, creative writing, code generation, translation, and information retrieval. By adopting LLM as the reasoning core, we propose Autonomous GIS, an AI-powered geographic information system (GIS) that leverages the LLM's general abilities in natural language understanding, reasoning and coding for addressing spatial problems with automatic spatial data collection, analysis and visualization. We envision that autonomous GIS will need to achieve five autonomous goals including self-generating, self-organizing, self-verifying, self-executing, and self-growing. We introduce the design principles of autonomous GIS to achieve these five autonomous goals from the aspects of information sufficiency, LLM ability, and agent architecture. We developed a prototype system called LLM-Geo using GPT-4 API in a Python environme
&lt;/p&gt;</description></item><item><title>GPT-NAS&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#25552;&#20986;&#36817;&#20284;&#30340;&#26550;&#26500;&#32452;&#20214;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05351</link><description>&lt;p&gt;
GPT-NAS: &#20197;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
GPT-NAS: Neural Architecture Search with the Generative Pre-Trained Model. (arXiv:2305.05351v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05351
&lt;/p&gt;
&lt;p&gt;
GPT-NAS&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#25552;&#20986;&#36817;&#20284;&#30340;&#26550;&#26500;&#32452;&#20214;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#34429;&#28982;&#19968;&#20123;&#20154;&#24037;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;NAS&#26041;&#27861;&#20013;&#24456;&#23569;&#20986;&#29616;&#36825;&#31867;&#25104;&#26524;&#65292;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#31070;&#32463;&#26550;&#26500;&#30340;&#25628;&#32034;&#31354;&#38388;&#22826;&#22823;&#20102;&#65292;&#23548;&#33268;NAS&#31639;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;GPT-NAS&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#12290;&#22312;GPT-NAS&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#26500;&#24314;&#31070;&#32463;&#26550;&#26500;&#30340;&#22522;&#26412;&#35268;&#24459;&#12290;&#22240;&#27492;&#65292;GPT-NAS&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#20986;&#21512;&#29702;&#30340;&#26550;&#26500;&#32452;&#20214;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#24341;&#20837;&#20102;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;GPT-NAS&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has emerged as one of the effective methods to design the optimal neural network architecture automatically. Although neural architectures have achieved human-level performances in several tasks, few of them are obtained from the NAS method. The main reason is the huge search space of neural architectures, making NAS algorithms inefficient. This work presents a novel architecture search algorithm, called GPT-NAS, that optimizes neural architectures by Generative Pre-Trained (GPT) model. In GPT-NAS, we assume that a generative model pre-trained on a large-scale corpus could learn the fundamental law of building neural architectures. Therefore, GPT-NAS leverages the generative pre-trained (GPT) model to propose reasonable architecture components given the basic one. Such an approach can largely reduce the search space by introducing prior knowledge in the search process. Extensive experimental results show that our GPT-NAS method significantly outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;15.27%&#12290;</title><link>http://arxiv.org/abs/2305.05228</link><description>&lt;p&gt;
&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#25552;&#21319;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic Embedded Deep Neural Network: A Generic Approach to Boost Multi-Label Image Classification Performance. (arXiv:2305.05228v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;15.27%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411;&#22312;&#20122;&#39532;&#36874;&#29983;&#20135;&#21151;&#33021;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#22522;&#20110;&#35270;&#35273;&#30340;&#26631;&#31614;&#39044;&#27979;&#65292;&#20174;&#26102;&#23578;&#23646;&#24615;&#26816;&#27979;&#21040;&#21697;&#29260;&#35782;&#21035;&#12290;&#23454;&#29616;&#36825;&#20123;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#37326;&#22806;&#35270;&#35273;&#32972;&#26223;&#20449;&#21495;&#65292;&#20854;&#20013;&#21253;&#21547;&#28151;&#28102;&#27169;&#22411;&#30340;&#26080;&#20851;&#20687;&#32032;&#65292;&#20351;&#27169;&#22411;&#38590;&#20197;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#24182;&#26681;&#25454;&#35813;&#29305;&#23450;&#21306;&#22495;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#23884;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#31354;&#38388;&#24863;&#30693;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#36890;&#36947;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#21033;&#29992;&#23450;&#20301;&#24341;&#23548;&#65292;&#20197;&#25552;&#39640;&#22810;&#26631;&#31614;&#39044;&#27979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25152;&#26377;&#26631;&#31614;&#30340;AUC&#24471;&#20998;&#30340;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#20026;15.27%&#12290;&#26680;&#24515;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#28041;&#21450;&#23545;Instagram&#26102;&#23578;&#26381;&#35013;&#30340;&#22810;&#26631;&#31614;&#26102;&#23578;&#23646;&#24615;&#20998;&#31867;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained multi-label classification models have broad applications in Amazon production features, such as visual based label predictions ranging from fashion attribute detection to brand recognition. One challenge to achieve satisfactory performance for those classification tasks in real world is the wild visual background signal that contains irrelevant pixels which confuses model to focus onto the region of interest and make prediction upon the specific region. In this paper, we introduce a generic semantic- embedding deep neural network to apply the spatial awareness semantic feature incorporating a channel- wise attention based model to leverage the localization guidance to boost model performance for multi- label prediction. We observed an Avg.relative improvement of 15.27% in terms of AUC score across all labels compared to the baseline approach. Core experiment and ablation studies involve multi-label fashion attribute classification performed on Instagram fashion apparels' 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.03355</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#32508;&#21512;&#30740;&#31350;&#65306;&#24615;&#33021;&#12289;&#38544;&#31169;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness. (arXiv:2305.03355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#26088;&#22312;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#29305;&#24449;&#32534;&#30721;&#25104;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#26159;&#19968;&#31181;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#30456;&#20851;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#21387;&#32553;&#22270;&#20687;&#30340;&#20449;&#24687;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20174;&#23433;&#20840;&#24615;&#35282;&#24230;&#20840;&#38754;&#20998;&#26512;&#36825;&#19968;&#25216;&#26415;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23545;&#28508;&#22312;&#39118;&#38505;&#32570;&#20047;&#31995;&#32479;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#25104;&#21151;&#20351;&#29992;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26469;&#26174;&#31034;&#20173;&#28982;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#25968;&#25454;&#38598;&#21387;&#32553;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#25918;&#22823;&#31867;&#21035;&#38388;&#30340;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#12290;&#26412;&#30740;&#31350;&#20026;&#25968;&#25454;&#38598;&#21387;&#32553;&#35780;&#20272;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01788</link><description>&lt;p&gt;
&#35270;&#35273;&#19982;&#23450;&#20041;&#30456;&#36935;&#65306;&#34701;&#21512;&#35789;&#20041;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information. (arXiv:2305.01788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20934;&#30830;&#22320;&#25551;&#36848;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#30446;&#26631;&#35789;&#27491;&#30830;&#24847;&#20041;&#30340;&#22270;&#20687;&#12290;&#20197;&#24448;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#24448;&#24448;&#21463;&#21040;&#35789;&#20041;&#22810;&#20041;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#27719;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#35789;&#20041;&#23450;&#20041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#27809;&#26377;&#25552;&#20379;&#31572;&#26696;&#30340;&#35789;&#20041;&#20449;&#24687;&#26102;&#65292;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25913;&#36827;&#35789;&#20856;&#22806;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;GPT-3&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#26126;&#26174;&#25552;&#39640;&#20102;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#22312;&#35789;&#20856;&#22806;&#20363;&#23376;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples exhibiting better performance than the existing definition generation method. We will publish source 
&lt;/p&gt;</description></item><item><title>Chronosymbolic Learning&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35299;&#20915;CHC&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;288&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#35768;&#22810;&#20855;&#26377;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.01206</link><description>&lt;p&gt;
Chronosymbolic Learning: &#32467;&#21512;&#31526;&#21495;&#25512;&#29702;&#19982;&#24402;&#32435;&#23398;&#20064;&#30340;&#26377;&#25928;CHC&#27714;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chronosymbolic Learning: Efficient CHC Solving with Symbolic Reasoning and Inductive Learning. (arXiv:2305.01206v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01206
&lt;/p&gt;
&lt;p&gt;
Chronosymbolic Learning&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35299;&#20915;CHC&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;288&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#35768;&#22810;&#20855;&#26377;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CHC (Constrained Horn Clauses)&#30340;&#27714;&#35299;&#26159;&#35768;&#22810;&#39564;&#35777;&#21644;&#20998;&#26512;&#20219;&#21153;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#25968;&#25454;&#39537;&#21160;&#27861;&#22312;&#25552;&#39640;CHC&#27714;&#35299;&#25928;&#29575;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#21644;&#35843;&#25972;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#32321;&#29712;&#24037;&#20316;&#12290;&#20294;&#25968;&#25454;&#39537;&#21160;&#30340;CHC&#27714;&#35299;&#22120;&#19982;&#22522;&#20110;&#31526;&#21495;&#25512;&#29702;&#30340;&#27714;&#35299;&#22120;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;"Chronosymbolic Learning"&#65292;&#23427;&#23558;&#31526;&#21495;&#20449;&#24687;&#21644;&#25968;&#20540;&#25968;&#25454;&#28857;&#32479;&#19968;&#36215;&#26469;&#65292;&#23558;CHC&#31995;&#32479;&#39640;&#25928;&#22320;&#27714;&#35299;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Chronosymbolic Learning&#30340;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#22120;&#21644;&#19968;&#20010;BMC&#26679;&#24335;&#30340;&#25512;&#29702;&#22120;&#12290;&#23613;&#31649;&#35813;&#24037;&#20855;&#38750;&#24120;&#31616;&#21333;&#65292;&#20294;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#21147;&#21644;&#20581;&#22766;&#24615;&#12290;&#23427;&#22312;&#30001;288&#20010;&#22522;&#20934;&#27979;&#35797;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CHC&#27714;&#35299;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#21253;&#21547;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving Constrained Horn Clauses (CHCs) is a fundamental challenge behind a wide range of verification and analysis tasks. Data-driven approaches show great promise in improving CHC solving without the painstaking manual effort of creating and tuning various heuristics. However, a large performance gap exists between data-driven CHC solvers and symbolic reasoning-based solvers. In this work, we develop a simple but effective framework, "Chronosymbolic Learning", which unifies symbolic information and numerical data points to solve a CHC system efficiently. We also present a simple instance of Chronosymbolic Learning with a data-driven learner and a BMC-styled reasoner. Despite its great simplicity, experimental results show the efficacy and robustness of our tool. It outperforms state-of-the-art CHC solvers on a dataset consisting of 288 benchmarks, including many instances with non-linear integer arithmetics.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#38024;&#23545;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#24471;&#21040;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;Presto&#65289;&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14065</link><description>&lt;p&gt;
&#38754;&#21521;&#36965;&#24863;&#26102;&#24207;&#25968;&#25454;&#30340;&#36731;&#37327;&#32423;&#39044;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
Lightweight, Pre-trained Transformers for Remote Sensing Timeseries. (arXiv:2304.14065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14065
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#38024;&#23545;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#24471;&#21040;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;Presto&#65289;&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#20256;&#24863;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#31038;&#20250;&#30456;&#20851;&#24212;&#29992;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#31639;&#27861;&#30340;&#26631;&#31614;&#21487;&#33021;&#24456;&#38590;&#25110;&#19981;&#21487;&#33021;&#33719;&#24471;&#12290;&#36825;&#20010;&#25361;&#25112;&#24050;&#32463;&#25512;&#21160;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#36890;&#36807;&#36965;&#24863;&#25968;&#25454;&#35299;&#38145;&#22312;&#26631;&#35760;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#22320;&#29702;&#20301;&#32622;&#25110;&#24212;&#29992;&#39046;&#22495;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20026;&#36965;&#24863;&#25968;&#25454;&#35774;&#35745;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#35757;&#32451;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#26356;&#23567;&#12289;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Remote Sensing Transformer&#65288;Presto&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#23545;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#21487;&#27604;&#27169;&#22411;&#30456;&#27604;&#65292;Presto&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38656;&#35201;&#25968;&#37327;&#32423;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms for parsing remote sensing data have a wide range of societally relevant applications, but labels used to train these algorithms can be difficult or impossible to acquire. This challenge has spurred research into self-supervised learning for remote sensing data aiming to unlock the use of machine learning in geographies or application domains where labelled datasets are small. Current self-supervised learning approaches for remote sensing data draw significant inspiration from techniques applied to natural images. However, remote sensing data has important differences from natural images -- for example, the temporal dimension is critical for many tasks and data is collected from many complementary sensors. We show that designing models and self-supervised training techniques specifically for remote sensing data results in both smaller and more performant models. We introduce the Pretrained Remote Sensing Transformer (Presto), a transformer-based model pre-tr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#30340;AudioLDM&#12290;</title><link>http://arxiv.org/abs/2304.13731</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model. (arXiv:2304.13731v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#30340;AudioLDM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24040;&#22823;&#35268;&#27169;&#20801;&#35768;&#35768;&#22810;&#26377;&#36259;&#30340;&#23646;&#24615;&#65292;&#27604;&#22914;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#24605;&#36335;&#38142;&#30340;&#24494;&#35843;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#26174;&#30528;&#25552;&#39640;&#20102;&#38646;&#27425;&#21644;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#26679;&#19968;&#31181;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#29983;&#25104;&#20219;&#21153;&#8212;&#8212;&#30446;&#26631;&#26159;&#26681;&#25454;&#20854;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#38899;&#39057;&#12290;&#20043;&#21069;&#20851;&#20110;TTA&#30340;&#24037;&#20316;&#35201;&#20040;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#32852;&#21512;&#30340;&#25991;&#26412;-&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#35201;&#20040;&#20351;&#29992;&#19968;&#20010;&#38750;&#25351;&#20196;&#35843;&#35856;&#30340;&#27169;&#22411;&#65292;&#22914;T5&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#30340;AudioLDM&#26356;&#22909;&#30340;&#22823;&#22810;&#25968;&#25351;&#26631;&#65292;&#24182;&#22312;&#20854;&#20313;&#25351;&#26631;&#19978;&#25345;&#24179;&#65292;&#23613;&#31649;&#25105;&#20204;&#20351;&#29992;&#20102;63&#20493;&#23567;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;LDM&#65292;&#24182;&#20445;&#25345;&#25991;&#26412;&#32534;&#30721;&#22120;&#19981;&#21464;&#12290;&#36825;&#31181;&#25913;&#36827;&#21487;&#33021;&#36824;&#24402;&#22240;&#20110;&#37319;&#29992;&#22522;&#20110;&#38899;&#39057;&#21387;&#21147;&#32423;&#30340;&#28151;&#38899;&#35757;&#32451;&#38598;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM Flan-T5 as the text encoder for text-to-audio (TTA) generation -- a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmenta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11082</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19982;&#20154;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#26159;&#23545;&#40784;&#20854;&#34892;&#20026;&#65292;&#20351;&#20854;&#23545;&#20854;&#20154;&#31867;&#29992;&#25143;&#26377;&#29992;&#19988;&#26080;&#23475;&#12290;&#36825;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#65292;&#20197;&#22686;&#24378;&#25152;&#38656;&#30340;&#34892;&#20026;&#24182;&#25233;&#21046;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;(BEB)&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20010;&#20869;&#22312;&#29305;&#24449;&#21644;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#34987;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#26377;&#38480;&#27010;&#29575;&#30340;&#34892;&#20026;&#65292;&#37117;&#23384;&#22312;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#36755;&#20986;&#27492;&#34892;&#20026;&#30340;&#25552;&#31034;&#65292;&#20854;&#27010;&#29575;&#38543;&#25552;&#31034;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#20943;&#24369;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#20294;&#26410;&#23558;&#20854;&#23436;&#20840;&#28040;&#38500;&#30340;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#25269;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#31034;&#20102;&#39046;&#20808;&#30340;
&lt;/p&gt;
&lt;p&gt;
An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;</title><link>http://arxiv.org/abs/2303.15662</link><description>&lt;p&gt;
ChatGPT4PCG&#27604;&#36187;&#65306;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ChatGPT4PCG Competition: Character-like Level Generation for Science Birds. (arXiv:2303.15662v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#12290;&#26412;&#27425;&#27604;&#36187;&#30340;&#30446;&#26631;&#26159;&#35753;&#21442;&#36187;&#32773;&#36890;&#36807;&#21019;&#36896;&#24615;&#21644;&#25552;&#31034;&#24037;&#31243;&#25216;&#33021;&#65292;&#20026;ChatGPT&#21019;&#24314;&#26377;&#25928;&#30340;&#25552;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;&#20026;&#20102;&#38477;&#20302;&#21442;&#36187;&#38376;&#27099;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#38480;&#21046;&#22312;&#29983;&#25104;&#22823;&#20889;&#33521;&#25991;&#23383;&#27597;&#12290;&#21442;&#36187;&#20316;&#21697;&#30340;&#36136;&#37327;&#30001;&#20854;&#31283;&#23450;&#24615;&#21644;&#19982;&#32473;&#23450;&#23383;&#31526;&#30340;&#30456;&#20284;&#24615;&#20915;&#23450;&#12290;&#32473;&#21442;&#36187;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#26679;&#20363;&#25552;&#31034;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE Conference on Games. The objective of this competition is for participants to create effective prompts for ChatGPT--enabling it to generate Science Birds levels with high stability and character-like qualities--fully using their creativity as well as prompt engineering skills. ChatGPT is a conversational agent developed by OpenAI. Science Birds is selected as the competition platform because designing an Angry Birds-like level is not a trivial task due to the in-game gravity; the playability of the levels is determined by their stability. To lower the entry barrier to the competition, we limit the task to the generation of capitalized English alphabetical characters. Here, the quality of the generated levels is determined by their stability and similarity to the given characters. A sample prompt is provided to participants for their reference. An experiment is conducted to determine the effectiveness of its modified
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; PIQ&#65292;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#21521;&#37327;&#37327;&#21270;&#65292;&#23558;&#20854;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#35299;&#37322;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#23481;&#26131;&#35753;&#20154;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.12659</link><description>&lt;p&gt;
&#36890;&#36807;&#37327;&#21270;&#36827;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Posthoc Interpretation via Quantization. (arXiv:2303.12659v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; PIQ&#65292;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#21521;&#37327;&#37327;&#21270;&#65292;&#23558;&#20854;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#35299;&#37322;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#23481;&#26131;&#35753;&#20154;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#36890;&#36807;&#37327;&#21270;&#23454;&#29616;&#30340;&#20107;&#21518;&#35299;&#37322;&#65288;PIQ&#65289;&#8221;&#65292;&#29992;&#20110;&#35299;&#37322;&#35757;&#32451;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21521;&#37327;&#37327;&#21270;&#23558;&#20998;&#31867;&#22120;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#65292;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#12290;&#31867;&#29305;&#23450;&#30340;&#30721;&#26412;&#20316;&#20026;&#29942;&#39048;&#65292;&#36843;&#20351;&#35299;&#37322;&#32773;&#19987;&#27880;&#20110;&#20998;&#31867;&#22120;&#35748;&#20026;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#19982;&#25991;&#29486;&#20013;&#30340;&#20960;&#31181;&#20854;&#20182;&#35299;&#37322;&#26041;&#27861;&#30456;&#27604;&#65292;PIQ&#29983;&#25104;&#30340;&#35299;&#37322;&#26356;&#23481;&#26131;&#34987;&#21442;&#19982;&#25105;&#20204;&#29992;&#25143;&#30740;&#31350;&#30340;&#20154;&#25152;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new approach, called "Posthoc Interpretation via Quantization (PIQ)", for interpreting decisions made by trained classifiers. Our method utilizes vector quantization to transform the representations of a classifier into a discrete, class-specific latent space. The class-specific codebooks act as a bottleneck that forces the interpreter to focus on the parts of the input data deemed relevant by the classifier for making a prediction. We evaluated our method through quantitative and qualitative studies and found that PIQ generates interpretations that are more easily understood by participants to our user studies when compared to several other interpretation methods in the literature.
&lt;/p&gt;</description></item><item><title>CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08127</link><description>&lt;p&gt;
CB2&#65306;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08127
&lt;/p&gt;
&lt;p&gt;
CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CB2 &#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#24773;&#22659;&#19979;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010; 3D &#28216;&#25103;&#29615;&#22659;&#12289;&#19968;&#20010;&#21518;&#31471;&#26381;&#21153;&#22120;&#65292;&#21487;&#20026;&#20154;&#31867;&#26234;&#33021;&#20307;&#25552;&#20379;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#65292;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312; https://cb2.ai &#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#31995;&#32479;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31471;&#21040;&#31471;&#31574;&#30053;&#20989;&#25968;&#21644;&#20048;&#35266;&#24179;&#28369;&#34394;&#25311;&#21338;&#24328;&#31639;&#27861;&#24212;&#29992;&#20110;&#26356;&#21152;&#22797;&#26434;&#30340;&#21830;&#19994;&#28216;&#25103;&#29200;&#30707;&#25136;&#35352;&#65292;&#25552;&#20986;&#25913;&#36827;&#25216;&#26415;&#24182;&#22312;&#20154;&#26426;&#23545;&#25112;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05197</link><description>&lt;p&gt;
&#25913;&#36827;&#25216;&#26415;&#25484;&#25569;&#31574;&#30053;&#21345;&#29260;&#28216;&#25103;&#65288;&#29200;&#30707;&#25136;&#35352;&#65289;
&lt;/p&gt;
&lt;p&gt;
Mastering Strategy Card Game (Hearthstone) with Improved Techniques. (arXiv:2303.05197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31471;&#21040;&#31471;&#31574;&#30053;&#20989;&#25968;&#21644;&#20048;&#35266;&#24179;&#28369;&#34394;&#25311;&#21338;&#24328;&#31639;&#27861;&#24212;&#29992;&#20110;&#26356;&#21152;&#22797;&#26434;&#30340;&#21830;&#19994;&#28216;&#25103;&#29200;&#30707;&#25136;&#35352;&#65292;&#25552;&#20986;&#25913;&#36827;&#25216;&#26415;&#24182;&#22312;&#20154;&#26426;&#23545;&#25112;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#21345;&#29260;&#28216;&#25103;&#26159;&#19968;&#31181;&#33879;&#21517;&#30340;&#28216;&#25103;&#31867;&#22411;&#65292;&#35201;&#27714;&#26234;&#33021;&#28216;&#25103;&#29609;&#27861;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#29702;&#24819;&#27979;&#35797;&#24179;&#21488;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21512;&#24182;&#20102;&#31471;&#21040;&#31471;&#31574;&#30053;&#20989;&#25968;&#21644;&#20048;&#35266;&#24179;&#28369;&#34394;&#25311;&#21338;&#24328;&#65292;&#22312;&#12298;&#39764;&#27861;&#19982;&#39749;&#21147;&#65306;&#20195;&#30721;&#20256;&#35828;&#12299;&#31561;&#31574;&#30053;&#21345;&#29260;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#21040;&#29200;&#30707;&#25136;&#35352;&#19978;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20960;&#31181;&#25913;&#36827;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#36827;&#34892;&#26426;&#22120;&#23545;&#20154;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#36992;&#35831;&#20102;&#19968;&#20301;&#22312;&#20013;&#22269;&#23448;&#26041;&#32852;&#36187;&#20013;&#25490;&#21517;&#21069;&#21313;&#30340;&#29200;&#30707;&#25136;&#35352;&#25773;&#20027;&#65292;&#35813;&#22320;&#21306;&#30340;&#29609;&#23478;&#25968;&#37327;&#20272;&#35745;&#22312;&#25968;&#30334;&#19975;&#20154;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#23436;&#25972;&#28216;&#25103;&#65288;&#21253;&#25324;&#26500;&#24314;&#21345;&#32452;&#21644;&#23545;&#25112;&#65289;&#30340;&#20116;&#23616;&#27604;&#36187;&#20013;&#20987;&#36133;&#20102;&#20154;&#31867;&#29609;&#23478;&#65292;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strategy card game is a well-known genre that is demanding on the intelligent game-play and can be an ideal test-bench for AI. Previous work combines an end-to-end policy function and an optimistic smooth fictitious play, which shows promising performances on the strategy card game Legend of Code and Magic. In this work, we apply such algorithms to Hearthstone, a famous commercial game that is more complicated in game rules and mechanisms. We further propose several improved techniques and consequently achieve significant progress. For a machine-vs-human test we invite a Hearthstone streamer whose best rank was top 10 of the official league in China region that is estimated to be of millions of players. Our models defeat the human player in all Best-of-5 tournaments of full games (including both deck building and battle), showing a strong capability of decision making.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21160;&#24577;&#25552;&#31034;&#65288;DP&#65289;&#35843;&#25972;&#31574;&#30053;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#21160;&#24577;&#22320;&#30830;&#23450;&#19981;&#21516;&#30340;&#25552;&#31034;&#21464;&#37327;&#26469;&#25429;&#33719;&#39069;&#22806;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.02909</link><description>&lt;p&gt;
&#21160;&#24577;&#25552;&#31034;&#65306;&#29992;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Dynamic Prompting: A Unified Framework for Prompt Tuning. (arXiv:2303.02909v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21160;&#24577;&#25552;&#31034;&#65288;DP&#65289;&#35843;&#25972;&#31574;&#30053;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#21160;&#24577;&#22320;&#30830;&#23450;&#19981;&#21516;&#30340;&#25552;&#31034;&#21464;&#37327;&#26469;&#25429;&#33719;&#39069;&#22806;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#21487;&#20197;&#39640;&#25928;&#22320;&#20174;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#12289;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; (V-L)&#12290;&#28982;&#32780;&#65292;&#37319;&#29992;&#22266;&#23450;&#30340;&#36719;&#25552;&#31034;&#26469;&#19982;&#25152;&#26377;&#23454;&#20363;&#36830;&#25509;&#36755;&#20837;&#65292;&#32780;&#24573;&#30053;&#23427;&#20204;&#30340;&#22266;&#26377;&#24046;&#24322;&#65292;&#20854;&#26377;&#25928;&#24615;&#20173;&#19981;&#30830;&#23450;&#12290;&#20363;&#22914;&#25552;&#31034;&#30340;&#20301;&#32622;&#12289;&#38271;&#24230;&#21644;&#34920;&#31034;&#22312;&#19981;&#21516;&#23454;&#20363;&#21644;&#20219;&#21153;&#20013;&#30340;&#19981;&#21516;&#21464;&#37327;&#65292;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#12290;&#36825;&#20010;&#20998;&#26512;&#21457;&#29616;&#65292;&#20248;&#21270;&#25552;&#31034;&#30340;&#20301;&#32622;&#21487;&#20197;&#25429;&#33719;&#20256;&#32479;&#21069;&#32512;&#25110;&#21518;&#32512;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#26080;&#27861;&#25429;&#33719;&#30340;&#39069;&#22806;&#35821;&#20041;&#20449;&#24687;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21160;&#24577;&#25552;&#31034; (DP) &#35843;&#25972;&#31574;&#30053;&#65292;&#21487;&#20197;&#21160;&#24577;&#22320;&#30830;&#23450;&#19981;&#21516;&#30340;&#25552;&#31034;&#21464;&#37327;&#65292;&#20197;&#20248;&#21270;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been demonstrated that the art of prompt tuning is highly effective in efficiently extracting knowledge from pretrained foundation models, encompassing pretrained language models (PLMs), vision pretrained models, and vision-language (V-L) models. However, the efficacy of employing fixed soft prompts with a predetermined position for concatenation with inputs for all instances, irrespective of their inherent disparities, remains uncertain. Variables such as the position, length, and representations of prompts across diverse instances and tasks can substantially influence the performance of prompt tuning. In this context, we provide a theoretical analysis, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture. Building upon our analysis, we present a unified dynamic prompt (DP) tuning strategy that dynamically determines different factors o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#25910;&#38598;&#39640;&#19981;&#35268;&#21017;&#28508;&#33021;&#33021;&#37327;&#38754;&#19978;&#30340;&#29289;&#29702;&#26377;&#36259;&#30340;&#31283;&#24577;&#65292; &#24182;&#22312;Pd / Fe / Ir&#65288;111&#65289;&#31995;&#32479;&#20013;&#24212;&#29992;&#20110;&#35782;&#21035;&#33258;&#26059;&#32467;&#26500;&#65292;&#35266;&#23519;&#20102;&#20854;&#20013;&#19968;&#20123;&#32467;&#26500;&#30340;&#26377;&#38480;&#28201;&#24230;&#33258;&#26059;&#21160;&#21147;&#23398;&#29305;&#24615;&#21644;&#25299;&#25169;&#30005;&#33655;&#19982;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.02876</link><description>&lt;p&gt;
&#29992;&#20803;&#21551;&#21457;&#24335;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#25910;&#38598;&#22825;&#28982;&#25233;&#21046;&#24577;&#30340;&#22825;&#31354;rmion&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Metaheuristic conditional neural network for harvesting skyrmionic metastable states. (arXiv:2303.02876v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02876
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#25910;&#38598;&#39640;&#19981;&#35268;&#21017;&#28508;&#33021;&#33021;&#37327;&#38754;&#19978;&#30340;&#29289;&#29702;&#26377;&#36259;&#30340;&#31283;&#24577;&#65292; &#24182;&#22312;Pd / Fe / Ir&#65288;111&#65289;&#31995;&#32479;&#20013;&#24212;&#29992;&#20110;&#35782;&#21035;&#33258;&#26059;&#32467;&#26500;&#65292;&#35266;&#23519;&#20102;&#20854;&#20013;&#19968;&#20123;&#32467;&#26500;&#30340;&#26377;&#38480;&#28201;&#24230;&#33258;&#26059;&#21160;&#21147;&#23398;&#29305;&#24615;&#21644;&#25299;&#25169;&#30005;&#33655;&#19982;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26465;&#20214;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#39640;&#19981;&#35268;&#21017;&#28508;&#33021;&#33021;&#37327;&#38754;&#19978;&#30340;&#29289;&#29702;&#26377;&#36259;&#30340;&#31283;&#24577;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#35745;&#31639;&#30340;&#21442;&#25968;&#22522;&#30784;&#19978;&#24314;&#31435;&#30340;&#21476;&#20856;&#24494;&#35266;&#26059;&#36716;&#21704;&#23494;&#39039;&#37327;&#23545;Pd / Fe / Ir&#65288;111&#65289;&#31995;&#32479;&#20013;&#30340;&#25299;&#25169;&#30005;&#33655;$Q$&#20540;&#20174;1&#21040;$-13$&#30340;&#33258;&#26059;&#32467;&#26500;&#36827;&#34892;&#20102;&#20998;&#26512; &#12290;&#20026;&#20102;&#20419;&#36827;&#30456;&#20851;&#33258;&#26059;&#32467;&#26500;&#30340;&#25910;&#38598;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#26032;&#24320;&#21457;&#30340;&#8220;&#20219;&#24847;&#20998;&#27573;&#27169;&#22411;&#8221;&#65288;SAM&#65289;&#12290;&#24182;&#20351;&#29992;&#26377;&#38480;&#28201;&#24230;&#33258;&#26059;&#21160;&#21147;&#23398;&#27169;&#25311;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;Q&#20540;&#33539;&#22260;&#20174;$-3$&#21040;$-6$&#30340;&#33258;&#26059;&#32467;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#39640;&#36798;20K&#30340;&#28201;&#24230;&#65292;&#39044;&#27979;&#23551;&#21629;&#38271;&#36798;200ps&#20197;&#19978;&#65292;&#24403;&#36825;&#20123;&#32467;&#26500;&#34928;&#20943;&#26102;&#65292;&#26032;&#30340;&#25299;&#25169;&#26059;&#36716;&#32467;&#26500;&#23601;&#20250;&#24418;&#25104;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#33258;&#26059;&#32467;&#26500;&#30340;&#30456;&#23545;&#31283;&#23450;&#24615;&#19982;&#20854;&#25299;&#25169;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a metaheuristic conditional neural-network-based method aimed at identifying physically interesting metastable states in a potential energy surface of high rugosity. To demonstrate how this method works, we identify and analyze spin textures with topological charge $Q$ ranging from 1 to $-13$ (where antiskyrmions have $Q&lt;0$) in the Pd/Fe/Ir(111) system, which we model using a classical atomistic spin Hamiltonian based on parameters computed from density functional theory. To facilitate the harvest of relevant spin textures, we make use of the newly developed Segment Anything Model (SAM). Spin textures with $Q$ ranging from $-3$ to $-6$ are further analyzed using finite-temperature spin-dynamics simulations. We observe that for temperatures up to around 20\,K, lifetimes longer than 200\,ps are predicted, and that when these textures decay, new topological spin textures are formed. We also find that the relative stability of the spin textures depend linearly on the topological
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.13942</link><description>&lt;p&gt;
Inseq&#65306;&#19968;&#20010;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Inseq: An Interpretability Toolkit for Sequence Generation Models. (arXiv:2302.13942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36807;&#21435;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27969;&#34892;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#32780;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#19987;&#38376;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Inseq&#65292;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#20351;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#26222;&#21450;&#21270;&#12290;Inseq&#33021;&#22815;&#30452;&#35266;&#19988;&#20248;&#21270;&#22320;&#25552;&#21462;&#27969;&#34892;&#30340;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#30340;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#23427;&#26469;&#31361;&#20986;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24182;&#22312;GPT-2&#20013;&#23450;&#20301;&#20107;&#23454;&#30693;&#35782;&#12290;&#30001;&#20110;&#20854;&#25903;&#25345;&#23545;&#27604;&#29305;&#24449;&#24402;&#22240;&#31561;&#21069;&#27839;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#25509;&#21475;&#65292;&#22240;&#27492;Inseq&#21487;&#20197;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#65292;&#38598;&#20013;&#20248;&#33391;&#23454;&#36341;&#65292;&#24182;&#23454;&#29616;&#20844;&#27491;&#21644;&#21487;&#37325;&#22797;&#30340;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32858;&#31867;&#26041;&#27861;(SLAC-Time)&#65292;&#37319;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20316;&#20026;&#20195;&#29702;&#20219;&#21153;&#65292;&#19981;&#38656;&#35201;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#20855;&#26377;&#26356;&#20581;&#22766;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.13457</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#32858;&#31867;&#21253;&#21547;&#32570;&#22833;&#20540;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454; (SLAC-Time): &#24212;&#29992;&#20110;TBI&#34920;&#22411;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Self-Supervised Learning-based Approach to Clustering Multivariate Time-Series Data with Missing Values (SLAC-Time): An Application to TBI Phenotyping. (arXiv:2302.13457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32858;&#31867;&#26041;&#27861;(SLAC-Time)&#65292;&#37319;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20316;&#20026;&#20195;&#29702;&#20219;&#21153;&#65292;&#19981;&#38656;&#35201;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#20855;&#26377;&#26356;&#20581;&#22766;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20026;&#32858;&#31867;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#24456;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#32570;&#22833;&#20540;&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#27714;&#22312;&#32858;&#31867;&#20043;&#21069;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#22823;&#37327;&#35745;&#31639;&#21644;&#22122;&#22768;&#65292;&#20174;&#32780;&#23548;&#33268;&#26080;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#29992;&#20110;&#32858;&#31867;&#21253;&#21547;&#32570;&#22833;&#20540;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26041;&#27861;(SLAC-Time)&#12290;SLAC-Time&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20316;&#20026;&#20195;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#23398;&#20064;&#26356;&#20581;&#22766;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21516;&#26102;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21644;&#25152;&#23398;&#34920;&#31034;&#30340;&#32858;&#31867;&#20998;&#37197;&#12290;&#23427;&#20351;&#29992;K-means&#26041;&#27861;&#36845;&#20195;&#22320;&#23545;&#23398;&#20064;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#28982;&#21518;&#21033;&#29992;&#38543;&#21518;&#30340;&#32858;&#31867;&#20998;&#37197;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning approaches provide a promising direction for clustering multivariate time-series data. However, real-world time-series data often include missing values, and the existing approaches require imputing missing values before clustering, which may cause extensive computations and noise and result in invalid interpretations. To address these challenges, we present a Self-supervised Learning-based Approach to Clustering multivariate Time-series data with missing values (SLAC-Time). SLAC-Time is a Transformer-based clustering method that uses time-series forecasting as a proxy task for leveraging unlabeled data and learning more robust time-series representations. This method jointly learns the neural network parameters and the cluster assignments of the learned representations. It iteratively clusters the learned representations with the K-means method and then utilizes the subsequent cluster assignments as pseudo-labels to update the model parameters. To evaluate our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#25104;&#26412;&#26356;&#20302;&#12289;&#36895;&#24230;&#26356;&#24555;&#12289;&#20998;&#36776;&#29575;&#26356;&#39640;&#30340;&#21487;&#32534;&#31243;&#20809;&#24149;&#26469;&#20272;&#35745;&#26426;&#22120;&#20154;&#25152;&#22312;&#29615;&#22659;&#30340;&#38556;&#30861;&#29289;&#30340;&#20301;&#32622;&#21644;&#31227;&#21160;&#26041;&#24335;&#12290;&#20026;&#20102;&#20915;&#23450;&#20809;&#24149;&#30340;&#25918;&#32622;&#20301;&#32622;&#20197;&#20934;&#30830;&#25191;&#34892;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#22823;&#21270;&#20449;&#24687;&#22686;&#30410;&#21644;&#39564;&#35777;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#30340;&#22810;&#31181;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#32452;&#21512;&#20102;&#36825;&#20123;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;&#36895;&#24230;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.12597</link><description>&lt;p&gt;
&#21033;&#29992;&#20809;&#24149;&#30340;&#20027;&#21160;&#36895;&#24230;&#20272;&#35745;&#65306;&#33258;&#30417;&#30563;&#22810;&#33218;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Active Velocity Estimation using Light Curtains via Self-Supervised Multi-Armed Bandits. (arXiv:2302.12597v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#25104;&#26412;&#26356;&#20302;&#12289;&#36895;&#24230;&#26356;&#24555;&#12289;&#20998;&#36776;&#29575;&#26356;&#39640;&#30340;&#21487;&#32534;&#31243;&#20809;&#24149;&#26469;&#20272;&#35745;&#26426;&#22120;&#20154;&#25152;&#22312;&#29615;&#22659;&#30340;&#38556;&#30861;&#29289;&#30340;&#20301;&#32622;&#21644;&#31227;&#21160;&#26041;&#24335;&#12290;&#20026;&#20102;&#20915;&#23450;&#20809;&#24149;&#30340;&#25918;&#32622;&#20301;&#32622;&#20197;&#20934;&#30830;&#25191;&#34892;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#22823;&#21270;&#20449;&#24687;&#22686;&#30410;&#21644;&#39564;&#35777;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#30340;&#22810;&#31181;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#32452;&#21512;&#20102;&#36825;&#20123;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;&#36895;&#24230;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#21644;&#33258;&#20027;&#30340;&#23548;&#33322;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#20934;&#30830;&#22320;&#20272;&#35745;&#38556;&#30861;&#29289;&#30340;&#20301;&#32622;&#21644;&#31227;&#21160;&#26041;&#24335;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#25104;&#26412;&#26356;&#20302;&#12289;&#36895;&#24230;&#26356;&#24555;&#12289;&#20998;&#36776;&#29575;&#26356;&#39640;&#30340;&#21487;&#32534;&#31243;&#20809;&#24149;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#32780;&#38750;&#20351;&#29992;&#26114;&#36149;&#30340;&#20256;&#32479;3D&#20256;&#24863;&#22120;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#21344;&#25454;&#26629;&#26684;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#20351;&#29992;&#20809;&#24149;&#36827;&#34892;&#37096;&#20998;&#27979;&#37327;&#24182;&#26126;&#30830;&#20272;&#35745;&#22330;&#26223;&#20013;3D&#28857;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#12290;&#26368;&#22823;&#30340;&#25361;&#25112;&#26159;&#20915;&#23450;&#20809;&#24149;&#30340;&#25918;&#32622;&#20301;&#32622;&#65292;&#20197;&#20934;&#30830;&#25191;&#34892;&#27492;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#20809;&#24149;&#25918;&#32622;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20449;&#24687;&#22686;&#30410;&#21644;&#39564;&#35777;&#39044;&#27979;&#30340;&#23545;&#35937;&#20301;&#32622;&#36827;&#34892;&#25351;&#23548;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#23558;&#36825;&#20123;&#31574;&#30053;&#32452;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#22870;&#21169;&#20989;&#25968;&#65292;&#35780;&#20272;&#24403;&#21069;&#36895;&#24230;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To navigate in an environment safely and autonomously, robots must accurately estimate where obstacles are and how they move. Instead of using expensive traditional 3D sensors, we explore the use of a much cheaper, faster, and higher resolution alternative: programmable light curtains. Light curtains are a controllable depth sensor that sense only along a surface that the user selects. We adapt a probabilistic method based on particle filters and occupancy grids to explicitly estimate the position and velocity of 3D points in the scene using partial measurements made by light curtains. The central challenge is to decide where to place the light curtain to accurately perform this task. We propose multiple curtain placement strategies guided by maximizing information gain and verifying predicted object locations. Then, we combine these strategies using an online learning framework. We propose a novel self-supervised reward function that evaluates the accuracy of current velocity estimate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#26469;&#20272;&#35745;&#19987;&#23478;&#30340;&#20445;&#23432;&#27169;&#22411;&#20197;&#21450;&#19987;&#23478;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25512;&#26029;&#19987;&#19994;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07457</link><description>&lt;p&gt;
&#36890;&#36807;&#28436;&#31034;&#26469;&#29702;&#35299;&#19987;&#19994;&#25216;&#33021;&#65306;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning. (arXiv:2302.07457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07457
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#22823;&#20284;&#28982;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#26469;&#20272;&#35745;&#19987;&#23478;&#30340;&#20445;&#23432;&#27169;&#22411;&#20197;&#21450;&#19987;&#23478;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25512;&#26029;&#19987;&#19994;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;Offline IRL&#65289;&#26088;&#22312;&#20174;&#19987;&#23478;&#20195;&#29702;&#30340;&#22266;&#23450;&#26377;&#38480;&#28436;&#31034;&#20013;&#24674;&#22797;&#25903;&#25745;&#35266;&#23519;&#21040;&#30340;&#25805;&#20316;&#30340;&#22870;&#21169;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#32467;&#26500;&#12290;&#20934;&#30830;&#30340;&#19987;&#19994;&#25191;&#34892;&#20219;&#21153;&#30340;&#27169;&#22411;&#22312;&#23433;&#20840;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#65292;&#20363;&#22914;&#20020;&#24202;&#20915;&#31574;&#21644;&#33258;&#21160;&#39550;&#39542;&#12290;&#28982;&#32780;&#65292;&#19987;&#23478;&#21916;&#22909;&#38544;&#21547;&#22312;&#35266;&#23519;&#21040;&#30340;&#25805;&#20316;&#20013;&#30340;&#32467;&#26500;&#19982;&#19987;&#23478;&#23545;&#29615;&#22659;&#21160;&#24577;&#30340;&#27169;&#22411;&#65288;&#21363;&#8220;&#19990;&#30028;&#8221;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20174;&#20855;&#26377;&#26377;&#38480;&#35206;&#30422;&#33539;&#22260;&#30340;&#26377;&#38480;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#19981;&#20934;&#30830;&#19990;&#30028;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#20272;&#35745;&#30340;&#22870;&#21169;&#30340;&#19981;&#20934;&#30830;&#24615;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#20844;&#24335;&#30340;&#20272;&#35745;&#20219;&#21153;&#65292;&#20854;&#20013;&#19978;&#23618;&#26159;&#22522;&#20110;&#19987;&#23478;&#31574;&#30053;&#30340;&#20445;&#23432;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;&#19979;&#23618;&#65289;&#12290;&#31574;&#30053;&#27169;&#22411;&#26159;&#20445;&#23432;&#30340;&#65292;&#22240;&#20026;&#23427;&#22312;&#24809;&#32602;&#65288;&#24809;&#32602;&#20250;&#38543;&#30528;&#19987;&#23478;&#23545;&#19990;&#30028;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#22686;&#21152;&#65289;&#19979;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#31163;&#32447;IRL&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations from an expert agent. Accurate models of expertise in executing a task has applications in safety-sensitive applications such as clinical decision making and autonomous driving. However, the structure of an expert's preferences implicit in observed actions is closely linked to the expert's model of the environment dynamics (i.e. the ``world''). Thus, inaccurate models of the world obtained from finite data with limited coverage could compound inaccuracy in estimated rewards. To address this issue, we propose a bi-level optimization formulation of the estimation task wherein the upper level is likelihood maximization based upon a conservative model of the expert's policy (lower level). The policy model is conservative in that it maximizes reward subject to a penalty that is increasing in the uncerta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20960;&#20309;&#20195;&#25968;&#30340;&#20960;&#20309; Clifford &#20195;&#25968;&#32593;&#32476;&#65288;GCANs&#65289;&#65292;&#37319;&#29992;&#23545;&#31216;&#32676;&#21464;&#25442;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#65292;&#36890;&#36807;&#32676;&#20316;&#29992;&#23618;&#12289;&#28608;&#27963;&#21644;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#20248;&#21270;&#20960;&#20309;&#27169;&#26495;&#65292;&#25552;&#39640;&#19977;&#32500;&#21018;&#20307;&#21464;&#25442;&#21644;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06594</link><description>&lt;p&gt;
&#20960;&#20309; Clifford &#20195;&#25968;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Geometric Clifford Algebra Networks. (arXiv:2302.06594v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20960;&#20309;&#20195;&#25968;&#30340;&#20960;&#20309; Clifford &#20195;&#25968;&#32593;&#32476;&#65288;GCANs&#65289;&#65292;&#37319;&#29992;&#23545;&#31216;&#32676;&#21464;&#25442;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#65292;&#36890;&#36807;&#32676;&#20316;&#29992;&#23618;&#12289;&#28608;&#27963;&#21644;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#20248;&#21270;&#20960;&#20309;&#27169;&#26495;&#65292;&#25552;&#39640;&#19977;&#32500;&#21018;&#20307;&#21464;&#25442;&#21644;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20960;&#20309;&#65288;Clifford&#65289;&#20195;&#25968;&#30340;&#23545;&#31216;&#32676;&#21464;&#25442;&#30340;&#20960;&#20309; Clifford &#20195;&#25968;&#32593;&#32476;&#65288;GCANs&#65289;&#29992;&#20110;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#29616;&#20195;&#65288;&#22522;&#20110;&#24179;&#38754;&#30340;&#65289;&#20960;&#20309;&#20195;&#25968;&#30340;&#31934;&#39635;&#65292;&#35813;&#20195;&#25968;&#24314;&#31435;&#22312;&#20316;&#20026; $\mathrm{Pin}(p,q,r)$ &#32676;&#20803;&#32032;&#30340;&#31561;&#36317;&#26144;&#23556;&#20043;&#19978;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#32676;&#20316;&#29992;&#23618;&#30340;&#27010;&#24565;&#65292;&#23427;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#32676;&#20316;&#29992;&#32447;&#24615;&#32452;&#21512;&#29289;&#20307;&#21464;&#25442;&#65292;&#37197;&#21512;&#26032;&#30340;&#28608;&#27963;&#21644;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#36825;&#20123;&#23618;&#20316;&#20026;&#21487;&#35843;&#25972;&#30340;&#8220;&#20960;&#20309;&#27169;&#26495;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26469;&#20248;&#21270;&#12290;&#29702;&#35770;&#19978;&#30340;&#20248;&#21183;&#22312;&#19977;&#32500;&#21018;&#20307;&#21464;&#25442;&#21644;&#22823;&#35268;&#27169;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#24314;&#27169;&#20013;&#24471;&#21040;&#20102;&#24378;&#28872;&#20307;&#29616;&#65292;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Geometric Clifford Algebra Networks (GCANs) for modeling dynamical systems. GCANs are based on symmetry group transformations using geometric (Clifford) algebras. We first review the quintessence of modern (plane-based) geometric algebra, which builds on isometries encoded as elements of the $\mathrm{Pin}(p,q,r)$ group. We then propose the concept of group action layers, which linearly combine object transformations using pre-specified group actions. Together with a new activation and normalization scheme, these layers serve as adjustable $\textit{geometric templates}$ that can be refined via gradient descent. Theoretical advantages are strongly reflected in the modeling of three-dimensional rigid body transformations as well as large-scale fluid dynamics simulations, showing significantly improved performance over traditional methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20851;&#20110;&#21028;&#21035;&#24335;&#19982;&#29983;&#25104;&#24335;&#20998;&#31867;&#22120;&#30340;&#32463;&#20856;&#20027;&#39064;&#65292;&#21033;&#29992;&#22810;&#31867;$\mathcal{H}$-&#19968;&#33268;&#24615;&#19979;&#30028;&#65292;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#22810;&#31867;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#26679;&#26412;&#35201;&#27714;&#27604;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#22810;&#20102;$O(\log n)$&#12290;</title><link>http://arxiv.org/abs/2302.02334</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#21028;&#21035;&#24335;&#20998;&#31867;&#22120;&#19982;&#29983;&#25104;&#24335;&#20998;&#31867;&#22120;&#65306;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Discriminative vs. Generative Classifiers: Theory and Implications. (arXiv:2302.02334v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20851;&#20110;&#21028;&#21035;&#24335;&#19982;&#29983;&#25104;&#24335;&#20998;&#31867;&#22120;&#30340;&#32463;&#20856;&#20027;&#39064;&#65292;&#21033;&#29992;&#22810;&#31867;$\mathcal{H}$-&#19968;&#33268;&#24615;&#19979;&#30028;&#65292;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#22810;&#31867;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#26679;&#26412;&#35201;&#27714;&#27604;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#22810;&#20102;$O(\log n)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#28145;&#24230;&#27169;&#22411;&#39044;&#20808;&#22312;&#22823;&#35268;&#27169;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#32447;&#24615;&#35780;&#20272;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#20923;&#32467;&#65292;&#24182;&#21333;&#29420;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#26377;&#21560;&#24341;&#21147;&#30340;&#36716;&#31227;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#32447;&#24615;&#35780;&#20272;&#20013;&#30340;&#20998;&#31867;&#22120;&#65292;&#38500;&#20102;&#40664;&#35748;&#30340;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#21463;&#21040;&#26420;&#32032;&#36125;&#21494;&#26031;&#30340;&#32479;&#35745;&#25928;&#29575;&#21551;&#21457;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;&#21028;&#21035;&#24335;&#19982;&#29983;&#25104;&#24335;&#20998;&#31867;&#22120;&#30340;&#32463;&#20856;&#20027;&#39064;&#12290;&#29702;&#35770;&#19978;&#65292;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#20195;&#29702;&#25439;&#22833;&#32780;&#19981;&#26159;0-1&#25439;&#22833;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#23558;&#32463;&#20856;&#32467;&#26524;&#20174;&#20108;&#20803;&#24773;&#20917;&#25512;&#24191;&#21040;&#22810;&#31867;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#22810;&#31867;&#26420;&#32032;&#36125;&#21494;&#26031;&#38656;&#35201;$O(\log n)$&#20010;&#26679;&#26412;&#26469;&#25509;&#36817;&#20854;&#28176;&#36817;&#35823;&#24046;&#65292;&#32780;&#30456;&#24212;&#30340;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#38656;&#35201;$O(n)$&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;$n$&#26159;&#29305;&#24449;&#32500;&#24230;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#31867;$\mathcal{H}$-&#19968;&#33268;&#24615;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large-scale deep model pre-trained on massive labeled or unlabeled data transfers well to downstream tasks. Linear evaluation freezes parameters in the pre-trained model and trains a linear classifier separately, which is efficient and attractive for transfer. However, little work has investigated the classifier in linear evaluation except for the default logistic regression. Inspired by the statistical efficiency of naive Bayes, the paper revisits the classical topic on discriminative vs. generative classifiers. Theoretically, the paper considers the surrogate loss instead of the zero-one loss in analyses and generalizes the classical results from binary cases to multiclass ones. We show that, under mild assumptions, multiclass naive Bayes requires $O(\log n)$ samples to approach its asymptotic error while the corresponding multiclass logistic regression requires $O(n)$ samples, where $n$ is the feature dimension. To establish it, we present a multiclass $\mathcal{H}$-consistency bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38543;&#26426;&#38598;&#25104;&#31639;&#27861;&#22312;&#23545;&#25239;&#25915;&#20987;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861; BARRE&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#24481;&#24378;&#22823;&#30340; $\ell_\infty$ &#33539;&#22260;&#20869;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2302.01375</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#25200;&#21160;&#30340;&#38543;&#26426;&#38598;&#25104;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Randomized Ensembles to Adversarial Perturbations. (arXiv:2302.01375v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38543;&#26426;&#38598;&#25104;&#31639;&#27861;&#22312;&#23545;&#25239;&#25915;&#20987;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861; BARRE&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#24481;&#24378;&#22823;&#30340; $\ell_\infty$ &#33539;&#22260;&#20869;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#38598;&#25104;&#20998;&#31867;&#22120; (Randomized ensemble classifiers, RECs) &#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#20256;&#32479;&#38598;&#25104;&#26041;&#27861;&#30340;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26500;&#36896; RECs &#30340;&#26041;&#27861;&#27604;&#26368;&#21021;&#22768;&#31216;&#30340;&#26356;&#33030;&#24369;&#65292;&#23545;&#20854;&#21151;&#25928;&#20135;&#29983;&#20102;&#37325;&#22823;&#24576;&#30097;&#65292;&#24182;&#24341;&#21457;&#20102;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#65292;&#20363;&#22914;&#65306;&#8220;RECs &#20309;&#26102;&#26377;&#25928;&#65311;&#8221;&#65292;&#8220;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#26159;&#20160;&#20040;&#65311;&#8221;&#65292;&#8220;&#25105;&#20204;&#22914;&#20309;&#35757;&#32451;&#23427;&#20204;&#65311;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#25506;&#32034; RECs &#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#22522;&#26412;&#32467;&#26524;&#65292;&#20363;&#22914; RECs &#30340;&#29702;&#35770;&#38480;&#21046;&#12289;&#20351;&#29992;&#23427;&#20204;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#26465;&#20214;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#31639;&#27861; (BARRE) &#29992;&#20110;&#35757;&#32451;&#24378;&#40065;&#26834;&#30340; RECs&#65292;&#24182;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#65292;&#35777;&#26126;&#20102;&#20854;&#23545;&#25239;&#24378; $\ell_\infty$ &#33539;&#22260;&#20869;&#30340;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized ensemble classifiers (RECs), where one classifier is randomly selected during inference, have emerged as an attractive alternative to traditional ensembling methods for realizing adversarially robust classifiers with limited compute requirements. However, recent works have shown that existing methods for constructing RECs are more vulnerable than initially claimed, casting major doubts on their efficacy and prompting fundamental questions such as: "When are RECs useful?", "What are their limits?", and "How do we train them?". In this work, we first demystify RECs as we derive fundamental results regarding their theoretical limits, necessary and sufficient conditions for them to be useful, and more. Leveraging this new understanding, we propose a new boosting algorithm (BARRE) for training robust RECs, and empirically demonstrate its effectiveness at defending against strong $\ell_\infty$ norm-bounded adversaries across various network architectures and datasets. Our code can
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#33976;&#39311;&#25216;&#26415;&#65292;&#22312;&#22768;&#23398;&#21644;&#35821;&#35328;&#32423;&#21035;&#19978;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22522;&#20110;CIF&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#30456;&#36739;&#21407;&#22987;&#27169;&#22411;&#65292;&#22312;AISHELL-1&#21644;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;15%&#21644;9%&#30340;&#30456;&#23545;&#35823;&#24046;&#29575;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2301.13003</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#33976;&#39311;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22522;&#20110;CIF&#30340;&#35821;&#38899;&#35782;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation. (arXiv:2301.13003v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#33976;&#39311;&#25216;&#26415;&#65292;&#22312;&#22768;&#23398;&#21644;&#35821;&#35328;&#32423;&#21035;&#19978;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22522;&#20110;CIF&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#30456;&#36739;&#21407;&#22987;&#27169;&#22411;&#65292;&#22312;AISHELL-1&#21644;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;15%&#21644;9%&#30340;&#30456;&#23545;&#35823;&#24046;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#21033;&#29992;PLMs&#26469;&#22686;&#24378;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20063;&#25104;&#20026;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;PLMs&#32467;&#26500;&#19981;&#28789;&#27963;&#21644;PLMs&#21033;&#29992;&#19981;&#20805;&#20998;&#31561;&#38382;&#39064;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779;&#28798;&#65288;CIF&#65289;&#22522;&#30784;&#19978;&#29992;&#23618;&#27425;&#21270;&#30693;&#35782;&#33976;&#39311;&#65288;HKD&#65289;&#12290;&#20026;&#20102;&#23558;PLMs&#30340;&#30693;&#35782;&#36716;&#31227;&#33267;ASR&#27169;&#22411;&#65292;HKD&#20351;&#29992;&#20132;&#21449;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;&#22768;&#23398;&#32423;&#21035;&#23545;&#27604;&#25439;&#22833;&#20197;&#21450;&#35821;&#35328;&#32423;&#21035;&#30340;&#30693;&#35782;&#33976;&#39311;&#21644;&#22238;&#24402;&#25439;&#22833;&#12290;&#19982;&#21407;&#22987;&#30340;CIF&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;AISHELL-1&#21644;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;15&#65285;&#21644;9&#65285;&#30340;&#30456;&#23545;&#35823;&#24046;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained language models (PLMs) have shown great potential in natural language processing tasks. Leveraging the capabilities of PLMs to enhance automatic speech recognition (ASR) systems has also emerged as a promising research direction. However, previous works may be limited by the inflexible structures of PLMs and the insufficient utilization of PLMs. To alleviate these problems, we propose the hierarchical knowledge distillation (HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer knowledge from PLMs to the ASR models, HKD employs cross-modal knowledge distillation with contrastive loss at the acoustic level and knowledge distillation with regression loss at the linguistic level. Compared with the original CIF-based model, our method achieves 15% and 9% relative error rate reduction on the AISHELL-1 and LibriSpeech datasets, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24102;&#26377;&#30690;&#37327;&#37327;&#21270;&#27169;&#22411;&#30340;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35782;&#21035;&#19987;&#23478;&#36712;&#36857;&#30340;&#23376;&#30446;&#26631;&#24182;&#24314;&#31435;&#30690;&#37327;&#37327;&#21270;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#23376;&#30446;&#26631;&#32423;&#21035;&#30340;&#35268;&#21010;&#65292;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#12289;&#38271;&#36828;&#30340;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12962</link><description>&lt;p&gt;
&#24102;&#26377;&#30690;&#37327;&#37327;&#21270;&#27169;&#22411;&#30340;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Imitation Learning with Vector Quantized Models. (arXiv:2301.12962v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24102;&#26377;&#30690;&#37327;&#37327;&#21270;&#27169;&#22411;&#30340;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35782;&#21035;&#19987;&#23478;&#36712;&#36857;&#30340;&#23376;&#30446;&#26631;&#24182;&#24314;&#31435;&#30690;&#37327;&#37327;&#21270;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#23376;&#30446;&#26631;&#32423;&#21035;&#30340;&#35268;&#21010;&#65292;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#12289;&#38271;&#36828;&#30340;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#22810;&#32423;&#25277;&#35937;&#30340;&#34892;&#21160;&#35268;&#21010;&#33021;&#21147;&#21487;&#20197;&#20351;&#26234;&#33021;&#20307;&#26377;&#25928;&#22320;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#20302;&#32423;&#35268;&#21010;&#27169;&#22411;&#21644;&#39640;&#32423;&#35268;&#21010;&#27169;&#22411;&#24182;&#24314;&#31435;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#32852;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#36755;&#20837;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20174;&#19987;&#23478;&#36712;&#36857;&#20013;&#35782;&#21035;&#23376;&#30446;&#26631;&#65292;&#36890;&#36807;&#23558;&#22870;&#21169;&#30340;&#22823;&#23567;&#19982;&#22312;&#32473;&#23450;&#29366;&#24577;&#21644;&#36873;&#25321;&#30340;&#23376;&#30446;&#26631;&#19979;&#21487;&#39044;&#27979;&#30340;&#20302;&#32423;&#34892;&#21160;&#30456;&#32852;&#31995;&#26469;&#23454;&#29616;&#35782;&#21035;&#12290;&#25105;&#20204;&#38024;&#23545;&#25152;&#35782;&#21035;&#30340;&#23376;&#30446;&#26631;&#24314;&#31435;&#30690;&#37327;&#37327;&#21270;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#23376;&#30446;&#26631;&#32423;&#21035;&#30340;&#35268;&#21010;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#31639;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#12289;&#38271;&#36828;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#35268;&#21010;&#65292;&#25152;&#20197;&#22312;&#35757;&#32451;&#38598;&#20013;&#27604;&#29616;&#26377;&#36712;&#36857;&#25214;&#21040;&#20102;&#26356;&#22909;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to plan actions on multiple levels of abstraction enables intelligent agents to solve complex tasks effectively. However, learning the models for both low and high-level planning from demonstrations has proven challenging, especially with higher-dimensional inputs. To address this issue, we propose to use reinforcement learning to identify subgoals in expert trajectories by associating the magnitude of the rewards with the predictability of low-level actions given the state and the chosen subgoal. We build a vector-quantized generative model for the identified subgoals to perform subgoal-level planning. In experiments, the algorithm excels at solving complex, long-horizon decision-making problems outperforming state-of-the-art. Because of its ability to plan, our algorithm can find better trajectories than the ones in the training set
&lt;/p&gt;</description></item><item><title>CAROL&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#27169;&#22411;&#21644;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#65292;&#23398;&#20064;&#20986;&#30340;&#31574;&#30053;&#20855;&#26377;&#26426;&#22120;&#21487;&#35777;&#26126;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#35748;&#35777;&#24615;&#33021;&#21644;&#21487;&#27604;&#36739;&#30340;&#23545;&#25239;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11374</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#27169;&#22411;&#30340;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Certifiably Robust Reinforcement Learning through Model-Based Abstract Interpretation. (arXiv:2301.11374v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11374
&lt;/p&gt;
&lt;p&gt;
CAROL&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#27169;&#22411;&#21644;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#65292;&#23398;&#20064;&#20986;&#30340;&#31574;&#30053;&#20855;&#26377;&#26426;&#22120;&#21487;&#35777;&#26126;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#35748;&#35777;&#24615;&#33021;&#21644;&#21487;&#27604;&#36739;&#30340;&#23545;&#25239;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; CAROL&#65292;&#20854;&#23398;&#20064;&#20986;&#30340;&#31574;&#30053;&#24102;&#26377;&#21487;&#35777;&#26126;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#35777;&#20070;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#29615;&#22659;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#27599;&#20010;&#23398;&#20064;&#36845;&#20195;&#20013;&#20351;&#29992;&#35813;&#27169;&#22411;&#21644;&#22806;&#37096;&#25277;&#35937;&#35299;&#37322;&#22120;&#26500;&#24314;&#21487;&#24494;&#20998;&#30340;&#35777;&#26126;&#40065;&#26834;&#24615;&#20449;&#21495;&#20197;&#24341;&#23548;&#23398;&#20064;&#65292;&#30452;&#21040;&#25910;&#25947;&#26102;&#65292;&#35813;&#25277;&#35937;&#35299;&#37322;&#23601;&#30452;&#25509;&#23548;&#33268;&#20102;&#21487;&#38752;&#24615;&#35777;&#20070;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#65292;&#30028;&#23450;&#20102; CAROL &#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#32047;&#31215;&#22870;&#21169;&#12290;&#25105;&#20204;&#36824;&#22312;&#22235;&#20010;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340; MuJoCo &#29615;&#22659;&#19978;&#23545; CAROL &#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;CAROL &#23398;&#20064;&#20986;&#30340;&#31574;&#30053;&#19982;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;&#23637;&#31034;&#20986;&#26174;&#33879;&#22686;&#24378;&#30340;&#35748;&#35777;&#24615;&#33021;&#19979;&#38480;&#21644;&#21487;&#27604;&#36739;&#30340;&#23545;&#25239;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a reinforcement learning (RL) framework in which the learned policy comes with a machine-checkable certificate of provable adversarial robustness. Our approach, called CAROL, learns a model of the environment. In each learning iteration, it uses the current version of this model and an external abstract interpreter to construct a differentiable signal for provable robustness. This signal is used to guide learning, and the abstract interpretation used to construct it directly leads to the robustness certificate returned at convergence. We give a theoretical analysis that bounds the worst-case accumulative reward of CAROL. We also experimentally evaluate CAROL on four MuJoCo environments with continuous state and action spaces. On these tasks, CAROL learns policies that, when contrasted with policies from the state-of-the-art robust RL algorithms, exhibit: (i) markedly enhanced certified performance lower bounds; and (ii) comparable performance under empirical adversarial atta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.10886</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21644;&#36866;&#24212;&#24615;&#30340;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;AIRS&#21487;&#20197;&#26681;&#25454;&#23454;&#26102;&#20272;&#35745;&#30340;&#20219;&#21153;&#22238;&#25253;&#20174;&#39044;&#23450;&#20041;&#30340;&#20989;&#25968;&#38598;&#20013;&#36873;&#25321;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#21487;&#38752;&#30340;&#25506;&#32034;&#28608;&#21169;&#24182;&#35299;&#20915;&#20559;&#32622;&#30446;&#26631;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#22810;&#31181;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#30340;&#39640;&#25928;&#21487;&#38752;&#23454;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#23558;AIRS&#24212;&#29992;&#22312;MiniGrid&#12289;Procgen&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#30340;&#22810;&#39033;&#20219;&#21153;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#22823;&#37327;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;AIRS&#21487;&#20197;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
&lt;/p&gt;</description></item><item><title>DIFFormer&#26159;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#34701;&#21512;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#65292;&#23548;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#33021;&#22815;&#25581;&#31034;&#30495;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2301.09474</link><description>&lt;p&gt;
DIFFormer&#65306;&#36890;&#36807;&#21463;&#33021;&#37327;&#38480;&#21046;&#30340;&#25193;&#25955;&#24341;&#20986;&#30340;&#21487;&#25193;&#23637;&#65288;&#22270;&#24418;&#65289;Transformer
&lt;/p&gt;
&lt;p&gt;
DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion. (arXiv:2301.09474v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09474
&lt;/p&gt;
&lt;p&gt;
DIFFormer&#26159;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#34701;&#21512;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#65292;&#23548;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#33021;&#22815;&#25581;&#31034;&#30495;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#29983;&#25104;&#24120;&#24120;&#28041;&#21450;&#23454;&#20363;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20381;&#36182;&#65292;&#36829;&#21453;&#20102;&#26631;&#20934;&#23398;&#20064;&#33539;&#24335;&#30340;IID&#25968;&#25454;&#20551;&#35774;&#65292;&#20174;&#32780;&#23545;&#25581;&#31034;&#20960;&#20309;&#32467;&#26500;&#20197;&#23398;&#20064;&#25152;&#38656;&#35201;&#30340;&#23454;&#20363;&#34920;&#31034;&#24418;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#19968;&#25209;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#32534;&#30721;&#20026;&#36880;&#28176;&#34701;&#21512;&#20102;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#12290;&#25193;&#25955;&#36807;&#31243;&#21463;&#38480;&#20110;&#22522;&#20110;&#21512;&#29702;&#33021;&#37327;&#20989;&#25968;&#30340;&#19979;&#38477;&#26631;&#20934;&#65292;&#35813;&#20989;&#25968;&#34920;&#24449;&#20102;&#28508;&#22312;&#32467;&#26500;&#19978;&#23454;&#20363;&#34920;&#31034;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#35880;&#30340;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#26263;&#31034;&#20102;&#20219;&#24847;&#23454;&#20363;&#23545;&#20043;&#38388;&#30340;&#26368;&#20248;&#25193;&#25955;&#24378;&#24230;&#30340;&#38381;&#21512;&#24418;&#24335;&#20272;&#35745;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#30340;&#20135;&#29983;&#65306;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#29256;&#26412;&#65306;&#19968;&#20010;&#31616;&#21333;&#29256;&#26412;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#38754;&#20020;&#30528;&#31105;&#24524;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t.~a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22343;&#22330;&#25511;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#21363;&#20351;&#26234;&#33021;&#20307;&#20849;&#20139;&#19968;&#20010;&#38750;&#21487;&#20998;&#20840;&#23616;&#29366;&#24577;&#65292;&#20063;&#33021;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#29992;&#24615;&#21644;&#36817;&#20284;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.06889</link><description>&lt;p&gt;
&#22522;&#20110;&#22343;&#22330;&#25511;&#21046;&#30340;&#38750;&#21487;&#20998;&#20849;&#20139;&#20840;&#23616;&#29366;&#24577;&#19979;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State. (arXiv:2301.06889v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22343;&#22330;&#25511;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#21363;&#20351;&#26234;&#33021;&#20307;&#20849;&#20139;&#19968;&#20010;&#38750;&#21487;&#20998;&#20840;&#23616;&#29366;&#24577;&#65292;&#20063;&#33021;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#29992;&#24615;&#21644;&#36817;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#22330;&#25511;&#21046;&#26159;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#24378;&#22823;&#30340;&#36817;&#20284;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22343;&#22330;&#25511;&#21046;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#22312;&#32473;&#23450;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#23616;&#37096;&#29366;&#24577;&#21644;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#30340;&#19979;&#19968;&#20010;&#65288;&#23616;&#37096;&#65289;&#29366;&#24577;&#20250;&#20114;&#30456;&#29420;&#31435;&#22320;&#28436;&#21464;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#26234;&#33021;&#20307;&#20849;&#20139;&#19968;&#20010;&#20840;&#23616;&#29366;&#24577;&#30340;MARL&#22330;&#26223;&#20013;&#65292;MFC&#20173;&#28982;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#22909;&#30340;&#36817;&#20284;&#24037;&#20855;&#65292;&#21069;&#25552;&#26159;&#26234;&#33021;&#20307;&#30340;&#23616;&#37096;&#29366;&#24577;&#20173;&#20855;&#26377;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;&#25105;&#20204;&#20551;&#35774;&#20840;&#23616;&#29366;&#24577;&#26159;&#38750;&#21487;&#20998;&#30340;&#65292;&#21363;&#19981;&#33021;&#23558;&#23427;&#34920;&#31034;&#20026;&#26234;&#33021;&#20307;&#30340;&#23616;&#37096;&#29366;&#24577;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#23558;&#36817;&#20284;&#35823;&#24046;&#35745;&#31639;&#20026;$\mathcal{O}(e)$&#65292;&#20854;&#20013;$e=\frac{1}{\sqrt{N}}\left[\sqrt{|\mathcal{X}|} +\sqrt{|\mathcal{U}|}\right]$&#65292;&#20195;&#34920;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#26415;&#35821;&#20026;$N$&#65292;$|\mathcal{X}|, |\mathcal{U}|$ &#34920;&#31034;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mean Field Control (MFC) is a powerful approximation tool to solve large-scale Multi-Agent Reinforcement Learning (MARL) problems. However, the success of MFC relies on the presumption that given the local states and actions of all the agents, the next (local) states of the agents evolve conditionally independent of each other. Here we demonstrate that even in a MARL setting where agents share a common global state in addition to their local states evolving conditionally independently (thus introducing a correlation between the state transition processes of individual agents), the MFC can still be applied as a good approximation tool. The global state is assumed to be non-decomposable i.e., it cannot be expressed as a collection of local states of the agents. We compute the approximation error as $\mathcal{O}(e)$ where $e=\frac{1}{\sqrt{N}}\left[\sqrt{|\mathcal{X}|} +\sqrt{|\mathcal{U}|}\right]$. The size of the agent population is denoted by the term $N$, and $|\mathcal{X}|, |\mathcal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#26696;&#26469;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#30340;&#36825;&#20123;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#27169;&#22411;&#30340;&#35821;&#29992;&#33021;&#21147;&#20173;&#38656;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2301.05149</link><description>&lt;p&gt;
&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#25913;&#36827;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models. (arXiv:2301.05149v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#26696;&#26469;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#30340;&#36825;&#20123;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#27169;&#22411;&#30340;&#35821;&#29992;&#33021;&#21147;&#20173;&#38656;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#24037;&#20316;&#36890;&#36807;&#20026;&#20154;&#31867;&#35774;&#35745;&#30340;&#24515;&#29702;&#27979;&#35797;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#23613;&#31649;&#36825;&#20123;&#30740;&#31350;&#26377;&#21161;&#20110;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#33324;&#33021;&#21147;&#65292;&#20294;&#24182;&#19981;&#33021;&#20445;&#35777;&#19968;&#20010;&#25317;&#26377;&#36275;&#22815;&#33021;&#21147;&#36890;&#36807;&#36825;&#20123;&#27979;&#35797;&#30340;&#27169;&#22411;&#23454;&#38469;&#19978;&#20250;&#22312;&#25191;&#34892;&#23454;&#38469;&#20219;&#21153;&#26102;&#20351;&#29992;&#36825;&#20123;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#20154;&#31867;&#24335;&#35748;&#30693;&#33021;&#21147;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#26469;&#25191;&#34892;&#20219;&#21153;&#12290;&#36825;&#20123;&#33021;&#21147;&#21253;&#25324;&#65306;(i) &#24555;&#36895;&#29983;&#25104;&#33391;&#22909;&#30340;&#20505;&#36873;&#35805;&#35821;&#30340;&#33021;&#21147; (&#25628;&#32034;&#33021;&#21147;)&#65307;(ii) &#39044;&#27979;&#21548;&#32773;&#22914;&#20309;&#29702;&#35299;&#36825;&#20123;&#35805;&#35821;&#65292;&#24182;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35805;&#35821; (&#35821;&#29992;&#33021;&#21147;)&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#26041;&#26696;&#65292;&#20197;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#36825;&#20123;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#27492;&#26041;&#26696;&#24212;&#29992;&#20110;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#38382;&#39064;&#20013;&#30340;&#21508;&#31181;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35821;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests would actually use those capabilities in performing real-life tasks. In this work, we formulate task-oriented cognitive capabilities, which are human-like cognitive capabilities that language models leverage to perform tasks. These capabilities are (i) the ability to quickly generate good candidate utterances (the search capability) (ii) the ability to predict how a listener interprets those utterances and choose the most appropriate one (the pragmatic capability). We design an evaluation scheme for comparing these capabilities of a language model with those of a human. Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; InPars-v2&#65292;&#20351;&#29992;&#24320;&#28304; LLMs &#21644;&#24378;&#22823;&#20877;&#25490;&#24207;&#22120;&#29983;&#25104;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#20013;&#35757;&#32451;&#30340;&#21512;&#25104;&#26597;&#35810;-&#25991;&#26723;&#23545;&#65292;&#21487;&#22312; BEIR &#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#26032;&#30340;&#26368;&#22909;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.01820</link><description>&lt;p&gt;
InPars-v2: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#39640;&#25928;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. (arXiv:2301.01820v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; InPars-v2&#65292;&#20351;&#29992;&#24320;&#28304; LLMs &#21644;&#24378;&#22823;&#20877;&#25490;&#24207;&#22120;&#29983;&#25104;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#20013;&#35757;&#32451;&#30340;&#21512;&#25104;&#26597;&#35810;-&#25991;&#26723;&#23545;&#65292;&#21487;&#22312; BEIR &#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#26032;&#30340;&#26368;&#22909;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;InPars &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#39640;&#25928;&#29983;&#25104;&#30456;&#20851;&#26597;&#35810;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#65292;&#35825;&#23548; LLM &#29983;&#25104;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#26597;&#35810;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#29983;&#25104;&#21512;&#25104;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#65292;&#29992;&#20110;&#35757;&#32451;&#26816;&#32034;&#22120;&#12290;&#28982;&#32780;&#65292;InPars &#21644; Promptagator &#31561;&#26041;&#27861;&#20381;&#36182;&#20110; GPT-3 &#21644; FLAN &#31561;&#19987;&#26377; LLMs &#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; InPars-v2&#65292;&#35813;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#20351;&#29992;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340; LLM &#21644;&#29616;&#26377;&#30340;&#24378;&#22823;&#20877;&#25490;&#24207;&#22120;&#26469;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#30340;&#21512;&#25104;&#26597;&#35810;-&#25991;&#26723;&#23545;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340; BM25 &#26816;&#32034;&#31649;&#36947;&#65292;&#22312;&#32463;&#36807;&#30001; InPars-v2 &#25968;&#25454;&#24494;&#35843;&#30340; monoT5 &#20877;&#25490;&#24207;&#22120;&#20043;&#21518;&#65292;&#20415;&#21487;&#22312; BEIR &#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#26032;&#30340;&#26368;&#22909;&#32467;&#26524;&#12290;&#20026;&#20102;&#35753;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#25552;&#39640;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#24494;&#35843;&#27169;&#22411;&#65306;https://github.com/zetaalphavector/inPars/tree/master/tpu&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu
&lt;/p&gt;</description></item><item><title>SERENGETI&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;517&#31181;&#38750;&#27954;&#35821;&#35328;&#21644;&#35821;&#35328;&#26041;&#35328;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2212.10785</link><description>&lt;p&gt;
SERENGETI&#65306;&#20026;&#38750;&#27954;&#32780;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SERENGETI: Massively Multilingual Language Models for Africa. (arXiv:2212.10785v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10785
&lt;/p&gt;
&lt;p&gt;
SERENGETI&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;517&#31181;&#38750;&#27954;&#35821;&#35328;&#21644;&#35821;&#35328;&#26041;&#35328;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;mPLM&#65289;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#26377;&#20215;&#20540;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#21487;&#25512;&#21160;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#30446;&#21069;&#65292;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#20165;&#35206;&#30422;&#20102;&#22823;&#32422;2,000&#31181;&#38750;&#27954;&#35821;&#35328;&#20013;&#30340;&#32422;31&#31181;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;SERENGETI&#65292;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;517&#31181;&#38750;&#27954;&#35821;&#35328;&#21644;&#35821;&#35328;&#26041;&#35328;&#65292;&#20197;&#25913;&#21892;&#36825;&#31181;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;20&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26032;&#22411;&#27169;&#22411;&#22312;&#20843;&#20010;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#23558;&#20854;&#19982;&#35206;&#30422;4-23&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;4&#20010;mPLM&#36827;&#34892;&#27604;&#36739;&#12290;SERENGETI&#22312;&#20843;&#20010;&#20219;&#21153;&#20013;&#30340;11&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#23454;&#29616;&#20102;82.27&#30340;&#24179;&#22343;F_1&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27169;&#22411;&#38169;&#35823;&#20998;&#26512;&#65292;&#20197;&#25506;&#31350;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#24212;&#29992;&#27169;&#22411;&#26102;&#35821;&#35328;&#31995;&#35889;&#21644;&#35821;&#35328;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#21521;&#20844;&#20247;&#21457;&#24067;&#25105;&#20204;&#30340;&#30740;&#31350;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research.\footnote{\href{https://github.com/UBC-NLP/serengeti}{https://g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25286;&#21368;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;Lego-MT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22810;&#35821;&#35328;&#21333;&#20307;&#27169;&#22411;&#22312;&#21442;&#25968;&#24178;&#25200;&#21644;&#20302;&#25928;&#25512;&#23548;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20855;&#26377;10&#20493;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#22312;&#25928;&#29575;&#21644;&#34920;&#29616;&#26041;&#38754;&#37117;&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.10551</link><description>&lt;p&gt;
Lego-MT: &#36208;&#21521;&#21487;&#25286;&#21368;&#30340;&#39640;&#24230;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lego-MT: Towards Detachable Models in Massively Multilingual Machine Translation. (arXiv:2212.10551v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25286;&#21368;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;Lego-MT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22810;&#35821;&#35328;&#21333;&#20307;&#27169;&#22411;&#22312;&#21442;&#25968;&#24178;&#25200;&#21644;&#20302;&#25928;&#25512;&#23548;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20855;&#26377;10&#20493;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#22312;&#25928;&#29575;&#21644;&#34920;&#29616;&#26041;&#38754;&#37117;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#36866;&#29992;&#20110;&#22810;&#20010;&#35821;&#35328;&#26041;&#21521;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;MNMT&#21333;&#20307;&#27169;&#22411;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;:&#35821;&#35328;&#20043;&#38388;&#30340;&#21442;&#25968;&#24178;&#25200;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#20302;&#25928;&#25512;&#29702;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#30340;&#22810;&#36335;&#24452;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#27599;&#31181;&#35821;&#35328;(&#25110;&#35821;&#35328;&#32452;)&#20998;&#37197;&#32473;&#25903;&#25345;&#21363;&#25554;&#21363;&#29992;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#21333;&#29420;&#20998;&#25903;&#65292;&#24320;&#21457;&#20986;&#21487;&#25286;&#21368;&#27169;&#22411;&#12290;&#20026;&#20102;&#28385;&#36275;&#22312;&#32479;&#19968;&#31354;&#38388;&#20013;&#20026;&#25152;&#26377;&#35821;&#35328;&#23398;&#20064;&#34920;&#31034;&#30340;&#38656;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#35757;&#32451;&#37197;&#26041;&#65292;&#20197;&#27492;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#21487;&#25286;&#21368;&#27169;&#22411;&#65292;Lego-MT&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#27491;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#20174;OPUS&#25910;&#38598;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;433&#31181;&#35821;&#35328;&#21644;13&#20159;&#20010;&#24179;&#34892;&#25968;&#25454;&#30340;&#32763;&#35793;&#22522;&#20934;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21442;&#25968;&#20026;12&#20159;&#30340;Lego-MT&#24102;&#26469;&#20102;3.2&#20010;spBLEU&#30340;&#24179;&#22343;&#22686;&#30410;&#12290;&#23427;&#29978;&#33267;&#32988;&#36807;&#20102;&#21442;&#25968;&#20026;120&#20159;&#30340;M2M-100&#12290;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#37197;&#26041;&#27604;&#24182;&#34892;&#35757;&#32451;&#25552;&#36895;&#20102;28.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. Existing monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models. In this paper, we revisit the classic multi-way structures and develop a detachable model by assigning each language (or group of languages) to an individual branch that supports plug-and-play training and inference. To address the needs of learning representations for all languages in a unified space, we propose a novel efficient training recipe, upon which we build an effective detachable model, Lego-MT. For a fair comparison, we collect data from OPUS and build a translation benchmark covering 433 languages and 1.3B parallel data. Experiments show that Lego-MT with 1.2B parameters brings an average gain of 3.2 spBLEU. It even outperforms M2M-100 with 12B parameters. The proposed training recipe brings a 28.2$\times$ speedup over the co
&lt;/p&gt;</description></item><item><title>MaRCo&#26159;&#19968;&#31181;&#25490;&#27602;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#19987;&#23478;&#21644;&#21453;&#19987;&#23478;&#27169;&#22411;&#23545;&#25991;&#26412;&#36827;&#34892;&#21487;&#25511;&#30340;&#37325;&#20889;&#21644;&#20462;&#35746;&#65292;&#36866;&#29992;&#20110;&#28040;&#38500;&#24494;&#22937;&#30340;&#26377;&#23475;&#20449;&#24687;&#65292;&#19988;&#22312;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#22343;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10543</link><description>&lt;p&gt;
&#29992;MaRCo&#28040;&#38500;&#26377;&#23475;&#25991;&#26412;&#65306;&#19987;&#23478;&#21644;&#21453;&#19987;&#23478;&#21487;&#25511;&#20462;&#35746;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts. (arXiv:2212.10543v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10543
&lt;/p&gt;
&lt;p&gt;
MaRCo&#26159;&#19968;&#31181;&#25490;&#27602;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#19987;&#23478;&#21644;&#21453;&#19987;&#23478;&#27169;&#22411;&#23545;&#25991;&#26412;&#36827;&#34892;&#21487;&#25511;&#30340;&#37325;&#20889;&#21644;&#20462;&#35746;&#65292;&#36866;&#29992;&#20110;&#28040;&#38500;&#24494;&#22937;&#30340;&#26377;&#23475;&#20449;&#24687;&#65292;&#19988;&#22312;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#22343;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25490;&#27602;&#20855;&#26377;&#20943;&#36731;&#26377;&#23475;&#24615;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#26469;&#28040;&#38500;&#20882;&#29359;&#24615;&#30340;&#21547;&#20041;&#65292;&#20294;&#24494;&#22937;&#30340;&#26377;&#23475;&#24615;&#20173;&#28982;&#24456;&#38590;&#22788;&#29702;&#12290;&#26412;&#25991;&#24341;&#20837;MaRCo&#65292;&#19968;&#31181;&#25490;&#27602;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#21487;&#25511;&#29983;&#25104;&#21644;&#25991;&#26412;&#37325;&#20889;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#19987;&#23478;&#20135;&#21697;&#21644;&#21453;&#19987;&#23478;&#20135;&#21697;&#12290;MaRCo&#20351;&#29992;&#38750;&#26377;&#23475;LM&#65288;&#19987;&#23478;&#65289;&#21644;&#26377;&#23475;LM&#65288;&#21453;&#19987;&#23478;&#65289;&#19979;&#30340;&#21487;&#33021;&#24615;&#26469;&#26597;&#25214;&#20505;&#36873;&#21333;&#35789;&#20197;&#36827;&#34892;&#25513;&#30422;&#21644;&#21487;&#33021;&#26367;&#25442;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#24494;&#22937;&#26377;&#23475;&#24615;&#21644;&#24494;&#25915;&#20987;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#23427;&#19981;&#20165;&#22312;&#33258;&#21160;&#24230;&#37327;&#19978;&#20248;&#20110;&#22522;&#32447;&#65292;&#32780;&#19988;MaRCo&#30340;&#37325;&#20889;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#26356;&#21463;&#27426;&#36814;&#12290;&#23427;&#23545;&#24494;&#22937;&#30340;&#26377;&#23475;&#24615;&#24773;&#20917;&#30340;&#36866;&#29992;&#24615;&#23588;&#20854;&#26377;&#21069;&#36884;&#65292;&#20026;&#35299;&#20915;&#26085;&#30410;&#38590;&#20197;&#25417;&#25720;&#30340;&#22312;&#32447;&#20167;&#24680;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo's rewrites are preferred 2.1 $\times$ more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10511</link><description>&lt;p&gt;
&#20309;&#26102;&#19981;&#20449;&#20219;&#35821;&#35328;&#27169;&#22411;&#65306;&#25506;&#32034;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#35760;&#24518;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#36825;&#26263;&#31034;&#20102;&#20165;&#20381;&#38752;&#20854;&#21442;&#25968;&#26469;&#32534;&#30721;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#22312;PopQA&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#30693;&#35782;&#25506;&#27979;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#38271;&#23614;&#20013;&#65292;&#25193;&#23637;&#35268;&#27169;&#26080;&#27861;&#26126;&#26174;&#25913;&#21892;&#35760;&#24518;&#23454;&#38469;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32988;&#36807;&#32423;&#21035;&#22823;&#24471;&#22810;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26410;&#32463;&#21327;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#39640;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#22312;&#38656;&#35201;&#26102;&#26816;&#32034;&#38750;&#21442;&#25968;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Detailed Outline Control(DOC) &#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#26469;&#25552;&#39640;&#29983;&#25104;&#38271;&#31687;&#25925;&#20107;&#26102;&#30340;&#24773;&#33410;&#36830;&#36143;&#24615;&#21644;&#22823;&#32434;&#30456;&#20851;&#24615;&#65292;&#20154;&#31867;&#35780;&#20272;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#36866;&#29992;&#20110;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2212.10077</link><description>&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30340;&#22823;&#32434;&#25511;&#21046;&#25552;&#21319;&#38271;&#31687;&#25925;&#20107;&#36830;&#36143;&#24615;
&lt;/p&gt;
&lt;p&gt;
DOC: Improving Long Story Coherence With Detailed Outline Control. (arXiv:2212.10077v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Detailed Outline Control(DOC) &#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#26469;&#25552;&#39640;&#29983;&#25104;&#38271;&#31687;&#25925;&#20107;&#26102;&#30340;&#24773;&#33410;&#36830;&#36143;&#24615;&#21644;&#22823;&#32434;&#30456;&#20851;&#24615;&#65292;&#20154;&#31867;&#35780;&#20272;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#36866;&#29992;&#20110;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; Detailed Outline Control(DOC)&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#25968;&#21315;&#23383;&#38271;&#30340;&#25925;&#20107;&#26102;&#30340;&#38271;&#31243;&#24773;&#33410;&#36830;&#36143;&#24615;&#12290;DOC&#30001;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#32452;&#25104;&#65306;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#12290;&#35814;&#32454;&#22823;&#32434;&#21019;&#24314;&#19968;&#20010;&#26356;&#35814;&#32454;&#12289;&#23618;&#27425;&#21270;&#30340;&#22823;&#32434;&#65292;&#23558;&#21019;&#36896;&#24615;&#36127;&#25285;&#20174;&#20027;&#35201;&#36215;&#33609;&#36807;&#31243;&#36716;&#31227;&#21040;&#35268;&#21010;&#38454;&#27573;&#12290;&#35814;&#32454;&#25511;&#21046;&#22120;&#36890;&#36807;&#25511;&#21046;&#25925;&#20107;&#27573;&#33853;&#19982;&#22823;&#32434;&#32454;&#33410;&#23545;&#40784;&#65292;&#30830;&#20445;&#26356;&#35814;&#32454;&#30340;&#22823;&#32434;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20173;&#28982;&#34987;&#23562;&#37325;&#12290;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#25925;&#20107;&#30340;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;DOC&#22312;&#24773;&#33410;&#36830;&#36143;&#24615;(22.5% &#32477;&#23545;&#22686;&#30410;)&#12289;&#22823;&#32434;&#30456;&#20851;&#24615;(28.2%)&#21644;&#36259;&#21619;&#24615;(20.7%)&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#24378;&#22823;&#30340;Re3&#22522;&#32447;(Yang&#31561;&#20154;&#65292;2022)&#12290;&#20154;&#20204;&#36824;&#35780;&#20215;DOC&#22312;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#26041;&#38754;&#26356;&#26131;&#20110;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Detailed Outline Control (DOC) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. DOC consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more detailed, hierarchically structured outline, shifting creative burden from the main drafting procedure to the planning stage. The detailed controller ensures the more detailed outline is still respected during generation by controlling story passages to align with outline details. In human evaluations of automatically generated stories, DOC substantially outperforms a strong Re3 baseline (Yang et al., 2022) on plot coherence (22.5% absolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans also judged DOC to be much more controllable in an interactive generation setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;BLOOM&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#65292;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#35821;&#35328;&#19978;&#65292;&#24182;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25552;&#31034;&#24615;&#33021;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2212.09535</link><description>&lt;p&gt;
BLOOM+1&#65306;&#20026;&#38646;&#26679;&#26412;&#25552;&#31034;&#28155;&#21152;&#35821;&#35328;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting. (arXiv:2212.09535v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;BLOOM&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#65292;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#35821;&#35328;&#19978;&#65292;&#24182;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25552;&#31034;&#24615;&#33021;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLOOM&#27169;&#22411;&#26159;&#19968;&#20010;&#22823;&#22411;&#20844;&#24320;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#20854;&#39044;&#35757;&#32451;&#20165;&#38480;&#20110;46&#31181;&#35821;&#35328;&#12290;&#20026;&#20102;&#23558;BLOOM&#30340;&#22909;&#22788;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#36807;&#39640;&#30340;&#25104;&#26412;&#65292;&#26377;&#24517;&#35201;&#23558;BLOOM&#36866;&#24212;&#21040;&#26032;&#30340;&#35821;&#35328;&#19978;&#12290;&#26412;&#25991;&#23558;&#29616;&#26377;&#30340;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#24212;&#29992;&#20110;BLOOM&#65292;&#24182;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#23545;&#20854;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#36866;&#24212;&#23545;&#20110;&#25552;&#39640;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#26159;&#26377;&#25928;&#30340;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#24615;&#33021;&#19981;&#20250;&#21463;&#21040;&#35821;&#35328;&#29305;&#23450;&#24615;&#30340;&#26174;&#30528;&#24433;&#21709;&#65292;&#22914;&#20070;&#20889;&#31995;&#32479;&#12290;&#23427;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;&#25105;&#20204;&#36824;&#21521;BLOOMZ&#28155;&#21152;&#20102;&#26032;&#35821;&#35328;&#65292;&#36825;&#26159;BLOOM&#30340;&#22810;&#20219;&#21153;&#24494;&#35843;&#29256;&#26412;&#65292;&#33021;&#22815;&#36319;&#38543;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35757;&#32451;&#38376;&#30340;&#22810;&#26041;&#38754;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#21069;&#32512;&#30340;&#24178;&#39044;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#26041;&#38754;&#32452;&#21512;&#30340;&#25511;&#21046;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.09387</link><description>&lt;p&gt;
&#22810;&#26041;&#38754;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#25193;&#23637;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Extensible Plug-and-Play Method for Multi-Aspect Controllable Text Generation. (arXiv:2212.09387v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35757;&#32451;&#38376;&#30340;&#22810;&#26041;&#38754;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#21069;&#32512;&#30340;&#24178;&#39044;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#26041;&#38754;&#32452;&#21512;&#30340;&#25511;&#21046;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25511;&#21046;&#29983;&#25104;&#25991;&#26412;&#30340;&#22810;&#20010;&#26041;&#38754;&#65288;&#22914;&#24773;&#24863;&#12289;&#20027;&#39064;&#21644;&#20851;&#38190;&#35789;&#65289;&#30340;&#22810;&#26041;&#38754;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#22522;&#20110;&#21442;&#25968;&#26377;&#25928;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#22914;&#21069;&#32512;&#35843;&#25972;&#65292;&#21487;&#20197;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#23454;&#29616;&#22810;&#26041;&#38754;&#25511;&#21046;&#65292;&#20294;&#22810;&#20010;&#21069;&#32512;&#30340;&#30456;&#20114;&#24178;&#25200;&#23548;&#33268;&#20102;&#32422;&#26463;&#30340;&#26174;&#33879;&#24694;&#21270;&#65292;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#26041;&#38754;&#32452;&#21512;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#24178;&#25200;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#19979;&#38480;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#24178;&#25200;&#38543;&#25554;&#20837;&#21069;&#32512;&#30340;&#23618;&#25968;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#35757;&#32451;&#38376;&#26469;&#35268;&#33539;&#21069;&#32512;&#30340;&#24178;&#39044;&#65292;&#20197;&#25233;&#21046;&#19981;&#26029;&#22686;&#38271;&#30340;&#24178;&#25200;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#36830;&#25509;&#30456;&#24212;&#30340;&#25554;&#20214;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#26041;&#38754;&#32452;&#21512;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#21487;&#20197;&#20302;&#25104;&#26412;&#22320;&#25193;&#23637;&#26032;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#21508;&#31181;&#25554;&#20214;&#33021;&#22815;&#28789;&#27963;&#38598;&#25104;&#65292;&#20197;&#23545;&#24212;&#19981;&#21516;&#30340;&#26041;&#38754;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21487;&#25511;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, multi-aspect controllable text generation that controls the generated text in multiple aspects (e.g., sentiment, topic, and keywords) has attracted increasing attention. Although methods based on parameter efficient tuning like prefix-tuning could achieve multi-aspect controlling in a plug-and-play way, the mutual interference of multiple prefixes leads to significant degeneration of constraints and limits their extensibility to training-time unseen aspect combinations. In this work, we provide a theoretical lower bound for the interference and empirically found that the interference grows with the number of layers where prefixes are inserted. Based on these analyses, we propose using trainable gates to normalize the intervention of prefixes to restrain the growing interference. As a result, controlling training-time unseen combinations of aspects can be realized by simply concatenating corresponding plugins such that new constraints can be extended at a lower cost. In additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20960;&#20309;&#23398;&#35282;&#24230;&#22312;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#21457;&#29616;&#65292;&#23545;&#27604;&#23398;&#20064;&#24102;&#26469;&#20102;&#21508;&#21521;&#21516;&#24615;&#65292;&#24182;&#39537;&#21160;&#21516;&#19968;&#21477;&#23376;&#20013;&#26631;&#35760;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#25910;&#25947;&#21040;&#30456;&#20284;&#30340;&#20301;&#32622;&#12290;&#23545;&#20110;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#65292;"&#34394;&#20551;&#30340;&#24773;&#22659;&#21270;"&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#32780;&#23545;&#20110;&#21151;&#33021;&#24615;&#26631;&#35760;&#21017;&#34987;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2212.09170</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#21508;&#21521;&#21516;&#24615;&#12289;&#24773;&#22659;&#21270;&#21644;&#23398;&#20064;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning. (arXiv:2212.09170v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20960;&#20309;&#23398;&#35282;&#24230;&#22312;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#21457;&#29616;&#65292;&#23545;&#27604;&#23398;&#20064;&#24102;&#26469;&#20102;&#21508;&#21521;&#21516;&#24615;&#65292;&#24182;&#39537;&#21160;&#21516;&#19968;&#21477;&#23376;&#20013;&#26631;&#35760;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#25910;&#25947;&#21040;&#30456;&#20284;&#30340;&#20301;&#32622;&#12290;&#23545;&#20110;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#65292;"&#34394;&#20551;&#30340;&#24773;&#22659;&#21270;"&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#32780;&#23545;&#20110;&#21151;&#33021;&#24615;&#26631;&#35760;&#21017;&#34987;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#32435;&#20837;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#22312;&#35768;&#22810;&#21477;&#23376;&#32423;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#26412;&#25991;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#12289;&#24773;&#22659;&#21270;&#21644;&#23398;&#20064;&#21160;&#24577;&#30340;&#35270;&#35282;&#26469;&#21078;&#26512;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#35774;&#35745;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#25351;&#23548;&#12290;&#20316;&#32773;&#36890;&#36807;&#34920;&#31034;&#21464;&#25442;&#30340;&#20960;&#20309;&#23398;&#26469;&#35299;&#37322;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#21151;&#65292;&#24182;&#23637;&#31034;&#23545;&#27604;&#23398;&#20064;&#22914;&#20309;&#24102;&#26469;&#21508;&#21521;&#21516;&#24615;&#24182;&#23548;&#33268;&#21516;&#19968;&#21477;&#23376;&#20013;&#30340;&#26631;&#35760;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#25910;&#25947;&#21040;&#30456;&#20284;&#30340;&#20301;&#32622;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#65292;"&#34394;&#20551;&#30340;&#24773;&#22659;&#21270;"&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#32780;&#23545;&#20110;&#21151;&#33021;&#24615;&#26631;&#35760;&#21017;&#34987;&#22686;&#24378;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23884;&#20837;&#31354;&#38388;&#26397;&#21521;&#21407;&#28857;&#24182;&#26356;&#22909;&#22320;&#23450;&#20041;&#20102;&#26356;&#22810;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide future designs of sentence representation learning methods by taking a closer look at contrastive SRL through the lens of isotropy, contextualization and learning dynamics. We interpret its successes through the geometry of the representation shifts and show that contrastive learning brings isotropy, and drives high intra-sentence similarity: when in the same sentence, tokens converge to similar positions in the semantic space. We also find that what we formalize as "spurious contextualization" is mitigated for semantically meaningful tokens, while augmented for functional ones. We find that the embedding space is directed towards the origin during training, with more areas now better define
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36974;&#30422;&#24335;&#20915;&#31574;&#39044;&#27979; (MaskDP) &#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#21487;&#25193;&#23637;&#30340;&#22686;&#24378;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#20013;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#22823;&#35268;&#27169;&#22810;&#26679;&#30340;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#19988;&#38646;&#26679;&#26412;&#36716;&#31227;&#33267;&#26032;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.12740</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#21644;&#21487;&#25512;&#24191;&#20915;&#31574;&#21046;&#23450;&#30340;&#36974;&#30422;&#33258;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoding for Scalable and Generalizable Decision Making. (arXiv:2211.12740v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36974;&#30422;&#24335;&#20915;&#31574;&#39044;&#27979; (MaskDP) &#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#21487;&#25193;&#23637;&#30340;&#22686;&#24378;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#20013;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#22823;&#35268;&#27169;&#22810;&#26679;&#30340;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#19988;&#38646;&#26679;&#26412;&#36716;&#31227;&#33267;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#23398;&#20064;&#21487;&#25193;&#23637;&#30340;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31867;&#20284;&#20110;&#24403;&#21069;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22810;&#26679;&#30340;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36974;&#30422;&#24335;&#20915;&#31574;&#39044;&#27979; (MaskDP) &#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#12290;&#22312; MaskDP &#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120; (MAE) &#22788;&#29702;&#29366;&#24577;-&#21160;&#20316;&#36712;&#36857;&#65292;&#38543;&#26426;&#36974;&#30422;&#29366;&#24577;&#21644;&#25805;&#20316;&#26631;&#35760;&#24182;&#37325;&#24314;&#32570;&#22833;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#27169;&#22411;&#38656;&#35201;&#25512;&#26029;&#20986;&#36974;&#25377;&#30340;&#29366;&#24577;&#21644;&#25805;&#20316;&#65292;&#24182;&#25552;&#21462;&#20851;&#20110;&#21160;&#24577;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36974;&#30422;&#19981;&#21516;&#27604;&#20363;&#30340;&#36755;&#20837;&#24207;&#21015;&#26174;&#33879;&#26377;&#21161;&#20110;&#23398;&#20064;&#19968;&#20010;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25512;&#24191;&#21040;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616; MaskDP &#27169;&#22411;&#33719;&#24471;&#20102;&#38646;&#26679;&#26412;&#36716;&#31227;&#21040;&#26032;&#30340; BC &#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#21333;&#19968;&#21644;&#22810;&#20010;&#30446;&#26631;&#21040;&#36798;&#20219;&#21153;&#65292;&#24182;&#19988;&#21487;&#20197;&#38646;&#26679;&#26412;&#36827;&#34892;&#36830;&#32493;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are interested in learning scalable agents for reinforcement learning that can learn from large-scale, diverse sequential data similar to current large vision and language models. To this end, this paper presents masked decision prediction (MaskDP), a simple and scalable self-supervised pretraining method for reinforcement learning (RL) and behavioral cloning (BC). In our MaskDP approach, we employ a masked autoencoder (MAE) to state-action trajectories, wherein we randomly mask state and action tokens and reconstruct the missing data. By doing so, the model is required to infer masked-out states and actions and extract information about dynamics. We find that masking different proportions of the input sequence significantly helps with learning a better model that generalizes well to multiple downstream tasks. In our empirical study, we find that a MaskDP model gains the capability of zero-shot transfer to new BC tasks, such as single and multiple goal reaching, and it can zero-shot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#36136;&#32852;&#36187;&#35757;&#32451;&#65288;HLT&#65289;&#30340;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20026;&#35299;&#20915;&#24322;&#36136;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#20351;&#29992;&#31574;&#30053;&#27744;&#21644;&#36229;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#24322;&#36136;&#26234;&#33021;&#20307;&#30340;&#21512;&#20316;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.11616</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#32852;&#36187;&#35757;&#32451;&#23454;&#29616;&#24322;&#36136;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Heterogeneous Agent Cooperation via Multiagent League Training. (arXiv:2211.11616v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#36136;&#32852;&#36187;&#35757;&#32451;&#65288;HLT&#65289;&#30340;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20026;&#35299;&#20915;&#24322;&#36136;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#20351;&#29992;&#31574;&#30053;&#27744;&#21644;&#36229;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#24322;&#36136;&#26234;&#33021;&#20307;&#30340;&#21512;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35768;&#22810;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21253;&#25324;&#22810;&#31181;&#33021;&#21147;&#21644;&#21151;&#33021;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#26679;&#30340;&#24322;&#36136;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#29992;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#19982;&#21516;&#36136;&#31995;&#32479;&#30456;&#27604;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#38750;&#31283;&#24577;&#38382;&#39064;&#21644;&#31574;&#30053;&#29256;&#26412;&#36845;&#20195;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#36136;&#32852;&#36187;&#35757;&#32451;&#65288;HLT&#65289;&#30340;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#24322;&#36136;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;HLT&#36319;&#36394;&#20195;&#29702;&#20154;&#22312;&#35757;&#32451;&#26399;&#38388;&#25506;&#32034;&#30340;&#19968;&#32452;&#31574;&#30053;&#65292;&#25910;&#38598;&#24322;&#36136;&#31574;&#30053;&#32852;&#30431;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#36229;&#32593;&#32476;&#65292;&#20197;&#22686;&#21152;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#22312;&#19982;&#20855;&#26377;&#19981;&#21516;&#32423;&#21035;&#30340;&#21512;&#20316;&#25216;&#33021;&#30340;&#38431;&#21451;&#21512;&#20316;&#26102;&#36827;&#34892;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#24322;&#36136;&#22522;&#20934;&#20219;&#21153;&#26469;&#35777;&#26126;HLT&#33021;&#22815;&#25552;&#39640;&#21327;&#20316;&#24322;&#36136;&#20219;&#21153;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many multiagent systems in the real world include multiple types of agents with different abilities and functionality. Such heterogeneous multiagent systems have significant practical advantages. However, they also come with challenges compared with homogeneous systems for multiagent reinforcement learning, such as the non-stationary problem and the policy version iteration issue. This work proposes a general-purpose reinforcement learning algorithm named Heterogeneous League Training (HLT) to address heterogeneous multiagent problems. HLT keeps track of a pool of policies that agents have explored during training, gathering a league of heterogeneous policies to facilitate future policy optimization. Moreover, a hyper-network is introduced to increase the diversity of agent behaviors when collaborating with teammates having different levels of cooperation skills. We use heterogeneous benchmark tasks to demonstrate that (1) HLT promotes the success rate in cooperative heterogeneous task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;</title><link>http://arxiv.org/abs/2211.08486</link><description>&lt;p&gt;
&#38646;&#20559;&#32622;&#26631;&#37327;&#19981;&#21464;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Scalar Invariant Networks with Zero Bias. (arXiv:2211.08486v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26435;&#37325;&#19968;&#26679;&#65292;&#20559;&#32622;&#39033;&#20063;&#26159;&#35768;&#22810;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;(&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;)&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#20154;&#20204;&#35748;&#20026;&#20559;&#24046;&#33021;&#26377;&#25928;&#22320;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#33021;&#21147;&#26469;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#25105;&#20204;&#20174;&#31532;&#19968;&#21407;&#29702;&#32771;&#34385;&#22270;&#20687;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#20998;&#24067;&#20197;&#21450;&#27169;&#22411;&#24212;&#20855;&#26377;&#30340;&#19968;&#20123;&#26399;&#26395;&#29305;&#24615;&#65292;&#21017;&#20559;&#24046;&#21487;&#20197;&#23436;&#20840;&#24573;&#30053;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21487;&#33021;&#19982;&#24102;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31216;&#20026;&#26631;&#37327;(&#20056;&#27861;)&#19981;&#21464;&#24615;&#30340;&#33391;&#22909;&#23646;&#24615;&#65292;&#36825;&#20351;&#24471;&#24403;&#25913;&#21464;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#27604;&#24230;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26631;&#37327;&#19981;&#21464;&#24615;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#8230;
&lt;/p&gt;
&lt;p&gt;
Just like weights, bias terms are the learnable parameters of many popular machine learning models, including neural networks. Biases are believed to effectively increase the representational power of neural networks to solve a wide range of tasks in computer vision. However, we argue that if we consider the intrinsic distribution of images in the input space as well as some desired properties a model should have from the first principles, biases can be completely ignored in addressing many image-related tasks, such as image classification. Our observation indicates that zero-bias neural networks could perform comparably to neural networks with bias at least on practical image classification tasks. In addition, we prove that zero-bias neural networks possess a nice property called scalar (multiplication) invariance, which allows the prediction of neural networks remains the same when altering the contrast of the input image. We then extend scalar invariance to more general cases that a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SSL4EO-S12&#30340;&#22823;&#35268;&#27169;&#12289;&#20840;&#29699;&#12289;&#22810;&#27169;&#24577;&#12289;&#22810;&#23395;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#23545;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#24615;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#30340;&#27169;&#22411;&#65292;&#22312;&#35813;&#39046;&#22495;&#20855;&#26377;&#24456;&#39640;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2211.07044</link><description>&lt;p&gt;
SSL4EO-S12:&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22320;&#29699;&#35266;&#27979;&#20013;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#12289;&#22810;&#26102;&#30456;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation. (arXiv:2211.07044v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SSL4EO-S12&#30340;&#22823;&#35268;&#27169;&#12289;&#20840;&#29699;&#12289;&#22810;&#27169;&#24577;&#12289;&#22810;&#23395;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#23545;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#24615;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#30340;&#27169;&#22411;&#65292;&#22312;&#35813;&#39046;&#22495;&#20855;&#26377;&#24456;&#39640;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26377;&#26395;&#29983;&#25104;&#34920;&#29616;&#21147;&#24378;&#30340;&#34920;&#24449;&#65292;&#32780;&#22320;&#29699;&#35266;&#27979;&#39046;&#22495;&#30340;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#37117;&#22522;&#20110;ImageNet&#25110;&#20013;&#31561;&#35268;&#27169;&#30340;&#26631;&#35760;&#30340;&#36965;&#24863;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#38598;SSL4EO-S12&#65292;&#20174;&#27431;&#31354;&#23616;Sentinel-1&#21644;-2&#21355;&#26143;&#20219;&#21153;&#20013;&#35013;&#37197;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#29699;&#12289;&#22810;&#27169;&#24577;&#21644;&#22810;&#23395;&#33410;&#30340;&#21355;&#26143;&#22270;&#20687;&#35821;&#26009;&#24211;&#12290;&#23545;&#20110;EO&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;SSL4EO-S12&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20960;&#31181;&#26041;&#27861;&#65306;MoCo-v2&#65292;DINO&#65292;MAE&#21644;data2vec&#12290;&#32467;&#26524;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#25509;&#36817;&#25110;&#36229;&#36807;&#20102;&#30417;&#30563;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23545;SSL4EO-S12&#30340;&#39044;&#35757;&#32451;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#22312;https://github.com/zhu-xlab/SSL4EO-S12&#19978;&#20844;&#24320;&#20102;&#25968;&#25454;&#38598;&#12289;&#30456;&#20851;&#28304;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pre-training bears potential to generate expressive representations without human annotation. Most pre-training in Earth observation (EO) are based on ImageNet or medium-size, labeled remote sensing (RS) datasets. We share an unlabeled RS dataset SSL4EO-S12 (Self-Supervised Learning for Earth Observation - Sentinel-1/2) to assemble a large-scale, global, multimodal, and multi-seasonal corpus of satellite imagery from the ESA Sentinel-1 \&amp; -2 satellite missions. For EO applications we demonstrate SSL4EO-S12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec. Resulting models yield downstream performance close to, or surpassing accuracy measures of supervised learning. In addition, pre-training on SSL4EO-S12 excels compared to existing datasets. We make openly available the dataset, related source code, and pre-trained models at https://github.com/zhu-xlab/SSL4EO-S12.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;lilGym&#65292;&#23427;&#30001;2661&#20010;&#39640;&#24230;&#32452;&#21512;&#30340;&#20154;&#31867;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#21644;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#27880;&#37322;&#21487;&#25191;&#34892;Python&#31243;&#24207;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#22870;&#21169;&#35745;&#31639;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;lilGym&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.01994</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#65306;lilGym
&lt;/p&gt;
&lt;p&gt;
lilGym: Natural Language Visual Reasoning with Reinforcement Learning. (arXiv:2211.01994v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;lilGym&#65292;&#23427;&#30001;2661&#20010;&#39640;&#24230;&#32452;&#21512;&#30340;&#20154;&#31867;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#21644;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#27880;&#37322;&#21487;&#25191;&#34892;Python&#31243;&#24207;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#22870;&#21169;&#35745;&#31639;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;lilGym&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#20851;&#35821;&#35328;&#26465;&#20214;&#19979;&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#35273;&#29615;&#22659;&#19979;&#30340;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;lilGym&#12290;lilGym&#22522;&#20110;2661&#20010;&#39640;&#24230;&#32452;&#21512;&#30340;&#20154;&#31867;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#65292;&#36825;&#20123;&#38472;&#36848;&#26159;&#22522;&#20110;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#30340;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27599;&#31181;&#21487;&#33021;&#30340;&#19990;&#30028;&#29366;&#24577;&#19979;&#65292;&#36890;&#36807;&#20026;&#25152;&#26377;&#35821;&#21477;&#27880;&#37322;&#21487;&#25191;&#34892;&#30340;Python&#31243;&#24207;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#22870;&#21169;&#35745;&#31639;&#12290;&#27599;&#20010;&#35821;&#21477;&#37117;&#19982;&#22810;&#20010;&#36215;&#22987;&#29366;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#37197;&#23545;&#65292;&#20197;&#24418;&#25104;&#25968;&#21315;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#23398;&#20064;&#26426;&#21046;&#36827;&#34892;&#20102;lilGym&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;lilGym&#24418;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;lilGym&#21487;&#20197;&#22312; https://lil.nlp.cornell.edu/lilgym/ &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present lilGym, a new benchmark for language-conditioned reinforcement learning in visual environments. lilGym is based on 2,661 highly-compositional human-written natural language statements grounded in an interactive visual environment. We introduce a new approach for exact reward computation in every possible world state by annotating all statements with executable Python programs. Each statement is paired with multiple start states and reward functions to form thousands of distinct Markov Decision Processes of varying difficulty. We experiment with lilGym with different models and learning regimes. Our results and analysis show that while existing methods are able to achieve non-trivial performance, lilGym forms a challenging open problem. lilGym is available at https://lil.nlp.cornell.edu/lilgym/.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#33521;&#35821;&#25552;&#31034;&#19979;&#65292;&#23545;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33521;&#35821;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20165;&#20986;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20219;&#21153;&#27867;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#24494;&#35843;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21508;&#31181;&#38646;-shot&#32467;&#26524;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.01786</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#36328;&#35821;&#35328;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Crosslingual Generalization through Multitask Finetuning. (arXiv:2211.01786v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#33521;&#35821;&#25552;&#31034;&#19979;&#65292;&#23545;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33521;&#35821;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20165;&#20986;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20219;&#21153;&#27867;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#24494;&#35843;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21508;&#31181;&#38646;-shot&#32467;&#26524;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#22810;&#20219;&#21153;&#24494;&#35843;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#25512;&#24191;&#21040;&#26032;&#30340;&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;MTF&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#33521;&#35821;&#25968;&#25454;&#21644;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#23558;MTF&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;BLOOM&#21644;mT5&#27169;&#22411;&#31995;&#21015;&#65292;&#29983;&#25104;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;&#21464;&#20307;BLOOMZ&#21644;mT0&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#33521;&#35821;&#25552;&#31034;&#19979;&#65292;&#23545;&#22823;&#22411;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33521;&#35821;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20165;&#20986;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20219;&#21153;&#27867;&#21270;&#12290;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#24494;&#35843;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21508;&#31181;&#38646;-shot&#32467;&#26524;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20123;&#26426;&#22120;&#32763;&#35793;&#25552;&#31034;&#19978;&#35757;&#32451;&#21487;&#20197;&#22312;&#21508;&#33258;&#35821;&#35328;&#20013;&#26356;&#22909;&#22320;&#23436;&#25104;&#20154;&#20889;&#30340;&#25552;&#31034;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;m
&lt;/p&gt;
&lt;p&gt;
Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38899;&#39057;&#35270;&#35273;&#20851;&#27880;&#30340;&#35270;&#35273;&#24863;&#30693;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#24110;&#21161;&#35299;&#20915;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#27495;&#20041;&#22768;&#38899;&#38382;&#39064;&#12290;&#22312;AudioCaps&#19978;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.16428</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38899;&#39057;&#35270;&#35273;&#20851;&#27880;&#30340;&#35270;&#35273;&#24863;&#30693;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention. (arXiv:2210.16428v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38899;&#39057;&#35270;&#35273;&#20851;&#27880;&#30340;&#35270;&#35273;&#24863;&#30693;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#24110;&#21161;&#35299;&#20915;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#27495;&#20041;&#22768;&#38899;&#38382;&#39064;&#12290;&#22312;AudioCaps&#19978;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26088;&#22312;&#20135;&#29983;&#38899;&#39057;&#29255;&#27573;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23545;&#35937;&#20250;&#20135;&#29983;&#30456;&#20284;&#30340;&#22768;&#38899;&#12290;&#22914;&#20309;&#20934;&#30830;&#35782;&#21035;&#27169;&#31946;&#30340;&#22768;&#38899;&#26159;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#21463;&#21040;&#20154;&#31867;&#22266;&#26377;&#22810;&#27169;&#24577;&#24863;&#30693;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#35270;&#35273;&#24863;&#30693;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#65292;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#26469;&#24110;&#21161;&#25551;&#36848;&#27169;&#31946;&#30340;&#22768;&#38899;&#23545;&#35937;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29616;&#25104;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#35270;&#39057;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#32467;&#21512;&#21040;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#31995;&#32479;&#20013;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#20114;&#34917;&#30340;&#38899;&#39057;-&#35270;&#35273;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38899;&#39057;-&#35270;&#35273;&#20851;&#27880;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#33258;&#36866;&#24212;&#22320;&#25972;&#21512;&#38899;&#39057;&#21644;&#35270;&#35273;&#19978;&#19979;&#25991;&#65292;&#24182;&#28040;&#38500;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#22312;AudioCaps&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio captioning aims to generate text descriptions of audio clips. In the real world, many objects produce similar sounds. How to accurately recognize ambiguous sounds is a major challenge for audio captioning. In this work, inspired by inherent human multimodal perception, we propose visually-aware audio captioning, which makes use of visual information to help the description of ambiguous sounding objects. Specifically, we introduce an off-the-shelf visual encoder to extract video features and incorporate the visual features into an audio captioning system. Furthermore, to better exploit complementary audio-visual contexts, we propose an audio-visual attention mechanism that adaptively integrates audio and visual context and removes the redundant information in the latent space. Experimental results on AudioCaps, the largest audio captioning dataset, show that our proposed method achieves state-of-the-art results on machine translation metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#30701;&#32534;&#36753;&#36335;&#24452;&#30340;&#20132;&#21449;&#25805;&#20316;&#31526;&#65292;&#29992;&#20197;&#35299;&#20915;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#20013;&#30340;&#25490;&#21015;&#38382;&#39064;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#31361;&#21464;&#30340;&#26399;&#26395;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.14016</link><description>&lt;p&gt;
&#26368;&#30701;&#32534;&#36753;&#36335;&#24452;&#20132;&#21449;&#65306;&#28436;&#21270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#25490;&#21015;&#38382;&#39064;&#30340;&#29702;&#35770;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Shortest Edit Path Crossover: A Theory-driven Solution to the Permutation Problem in Evolutionary Neural Architecture Search. (arXiv:2210.14016v4 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#30701;&#32534;&#36753;&#36335;&#24452;&#30340;&#20132;&#21449;&#25805;&#20316;&#31526;&#65292;&#29992;&#20197;&#35299;&#20915;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#20013;&#30340;&#25490;&#21015;&#38382;&#39064;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#31361;&#21464;&#30340;&#26399;&#26395;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#31181;&#32676;&#30340;&#25628;&#32034;&#24050;&#32463;&#25104;&#20026;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#21487;&#33021;&#26367;&#20195;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#19968;&#20010;&#36873;&#25321;&#12290;&#23613;&#31649;&#23427;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#22312;&#29702;&#35770;&#19978;&#23578;&#26410;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;&#34429;&#28982;&#20256;&#32479;&#30340;&#22522;&#20110;&#31181;&#32676;&#30340;&#25628;&#32034;&#26041;&#27861;&#22914;&#28436;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#20174;&#20132;&#21449;&#25805;&#20316;&#20013;&#27762;&#21462;&#20102;&#24456;&#22810;&#21147;&#37327;&#65292;&#20294;&#22312;NAS&#20013;&#24456;&#38590;&#21033;&#29992;&#23427;&#20204;&#12290;&#20027;&#35201;&#38556;&#30861;&#34987;&#35748;&#20026;&#26159;&#25490;&#21015;&#38382;&#39064;&#65306;&#22312;&#20256;&#32479;&#30340;&#22270;&#34920;&#31034;&#20013;&#65292;&#22522;&#22240;&#22411;&#21644;&#34920;&#29616;&#22411;&#20043;&#38388;&#30340;&#26144;&#23556;&#26159;&#19968;&#23545;&#22810;&#30340;&#65292;&#23548;&#33268;&#26631;&#20934;&#20132;&#21449;&#20135;&#29983;&#30772;&#22351;&#24615;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#23545;&#40657;&#30418;NAS&#20013;&#31361;&#21464;&#12289;&#20132;&#21449;&#21644;RL&#34892;&#20026;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#22522;&#20110;&#22270;&#31354;&#38388;&#20013;&#30340;&#26368;&#30701;&#32534;&#36753;&#36335;&#24452;&#65288;SEP&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#21449;&#25805;&#20316;&#31526;&#12290;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;SEP&#20132;&#21449;&#21487;&#20197;&#20811;&#26381;&#25490;&#21015;&#38382;&#39064;&#65292;&#22240;&#27492;&#19982;&#31361;&#21464;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#26399;&#26395;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Population-based search has recently emerged as a possible alternative to Reinforcement Learning (RL) for black-box neural architecture search (NAS). It performs well in practice even though it is not theoretically well understood. In particular, whereas traditional population-based search methods such as evolutionary algorithms (EAs) draw much power from crossover operations, it is difficult to take advantage of them in NAS. The main obstacle is believed to be the permutation problem: The mapping between genotype and phenotype in traditional graph representations is many-to-one, leading to a disruptive effect of standard crossover. This paper presents the first theoretical analysis of the behaviors of mutation, crossover and RL in black-box NAS, and proposes a new crossover operator based on the shortest edit path (SEP) in graph space. The SEP crossover is shown theoretically to overcome the permutation problem, and as a result, have a better expected improvement compared to mutation,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;motif&#30340;GNN&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;motif&#30340;&#35302;&#21457;&#22120;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#25552;&#39640;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#25928;&#26524;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.13710</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#30340;GNN&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs. (arXiv:2210.13710v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;motif&#30340;GNN&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;motif&#30340;&#35302;&#21457;&#22120;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#25552;&#39640;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#25928;&#26524;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#34920;&#31034;&#33021;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#29289;&#22522;&#22240;&#39044;&#27979;&#12289;&#31038;&#20132;&#25512;&#33616;&#31561;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;GNN&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#29992;&#24694;&#24847;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#24456;&#23481;&#26131;&#34987;&#20462;&#34917;&#21518;&#30340;&#26679;&#26412;&#27450;&#39575;&#12290;&#22823;&#22810;&#25968;&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#20351;&#29992;&#30340;&#35302;&#21457;&#22120;&#35201;&#20040;&#26159;&#38543;&#26426;&#29983;&#25104;&#30340;&#23376;&#22270;&#65288;&#20363;&#22914; erd\H{o}s-r\'enyi &#21518;&#38376;&#65289;&#65292;&#20197;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#65292;&#35201;&#20040;&#26159;&#22522;&#20110;&#26799;&#24230;&#30340;&#29983;&#25104;&#23376;&#22270;&#65288;&#20363;&#22914;&#22270;Trojan&#25915;&#20987;&#65289;&#65292;&#20197;&#20351;&#25915;&#20987;&#26356;&#21152;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#29702;&#35299;&#35302;&#21457;&#22120;&#32467;&#26500;&#19982;&#21518;&#38376;&#25915;&#20987;&#25928;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#21364;&#22312;&#24403;&#21069;&#25991;&#29486;&#20013;&#34987;&#24573;&#30053;&#20102;&#12290;&#22312;&#22270;&#20013;&#65292;&#37325;&#22797;&#24615;&#21644;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#23376;&#22270;&#65288;motif&#65289;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20174;motif&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#35302;&#21457;&#22120;&#30340;&#35774;&#35745;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;motif&#30340;GNN&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;motif&#30340;&#35302;&#21457;&#22120;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;motif&#20449;&#24687;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#38544;&#34109;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) with a powerful representation capability has been widely applied to various areas, such as biological gene prediction, social recommendation, etc. Recent works have exposed that GNN is vulnerable to the backdoor attack, i.e., models trained with maliciously crafted training samples are easily fooled by patched samples. Most of the proposed studies launch the backdoor attack using a trigger that either is the randomly generated subgraph (e.g., erd\H{o}s-r\'enyi backdoor) for less computational burden, or the gradient-based generative subgraph (e.g., graph trojaning attack) to enable a more effective attack. However, the interpretation of how is the trigger structure and the effect of the backdoor attack related has been overlooked in the current literature. Motifs, recurrent and statistically significant sub-graphs in graphs, contain rich structure information. In this paper, we are rethinking the trigger from the perspective of motifs, and propose a motif-ba
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28304;&#20869;&#26679;&#24335;&#22686;&#24378;&#65288;ISSA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#25513;&#27169;&#22122;&#22768;&#32534;&#30721;&#22120;&#38543;&#26426;&#21270;&#26679;&#24335;&#21644;&#20869;&#23481;&#32452;&#21512;&#65292;ISSA&#26377;&#25928;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#24182;&#20943;&#23569;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#39550;&#39542;&#22330;&#26223;&#35821;&#20041;&#20998;&#21106;&#20013;&#33719;&#24471;&#20102;&#39640;&#36798;12.4&#65285;&#30340;mIoU&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.10175</link><description>&lt;p&gt;
&#22522;&#20110;&#28304;&#20869;&#26679;&#24335;&#22686;&#24378;&#30340;&#39046;&#22495;&#27867;&#21270;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Intra-Source Style Augmentation for Improved Domain Generalization. (arXiv:2210.10175v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28304;&#20869;&#26679;&#24335;&#22686;&#24378;&#65288;ISSA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#25513;&#27169;&#22122;&#22768;&#32534;&#30721;&#22120;&#38543;&#26426;&#21270;&#26679;&#24335;&#21644;&#20869;&#23481;&#32452;&#21512;&#65292;ISSA&#26377;&#25928;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#24182;&#20943;&#23569;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#39550;&#39542;&#22330;&#26223;&#35821;&#20041;&#20998;&#21106;&#20013;&#33719;&#24471;&#20102;&#39640;&#36798;12.4&#65285;&#30340;mIoU&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#39046;&#22495;&#20559;&#31227;&#65292;&#27604;&#22914;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28304;&#20869;&#26679;&#24335;&#22686;&#24378;&#65288;ISSA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#22411;&#30340;StyleGAN2&#21453;&#28436;&#25513;&#27169;&#22122;&#22768;&#32534;&#30721;&#22120;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22122;&#22768;&#39044;&#27979;&#23398;&#20064;&#24544;&#23454;&#37325;&#24314;&#22270;&#20687;&#24182;&#20445;&#30041;&#20854;&#35821;&#20041;&#24067;&#23616;&#12290;&#20272;&#35745;&#22122;&#22768;&#30340;&#38543;&#26426;&#25513;&#34109;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26679;&#24335;&#28151;&#21512;&#33021;&#21147;&#65292;&#21363;&#23427;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#22270;&#20687;&#35821;&#20041;&#24067;&#23616;&#30340;&#24773;&#20917;&#19979;&#25913;&#21464;&#20840;&#23616;&#22806;&#35266;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#25513;&#27169;&#22122;&#22768;&#32534;&#30721;&#22120;&#26469;&#38543;&#26426;&#21270;&#35757;&#32451;&#38598;&#20013;&#30340;&#26679;&#24335;&#21644;&#20869;&#23481;&#32452;&#21512;&#65292;ISSA&#26377;&#25928;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#24182;&#20943;&#23569;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#32467;&#26524;&#65292;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#39550;&#39542;&#22330;&#26223;&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#39640;&#36798;12.4&#65285;&#30340;mIoU&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an intra-source style augmentation (ISSA) method to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image preserving its semantic layout through noise prediction. Random masking of the estimated noise enables the style mixing capability of our model, i.e. it allows to alter the global appearance without affecting the semantic layout of an image. Using the proposed masked noise encoder to randomize style and content combinations in the training set, ISSA effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to $12.4\%$ mIoU improvements on driving-scene semantic segmentation under different type
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#20540;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65288;STaSy&#65289;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#25216;&#26415;&#21644;&#24494;&#35843;&#31574;&#30053;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.04018</link><description>&lt;p&gt;
STaSy&#65306;&#22522;&#20110;&#20998;&#20540;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
STaSy: Score-based Tabular data Synthesis. (arXiv:2210.04018v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04018
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#20540;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65288;STaSy&#65289;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#25216;&#26415;&#21644;&#24494;&#35843;&#31574;&#30053;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20174;&#32479;&#35745;&#26041;&#27861;&#21040;&#28145;&#24230;&#29983;&#25104;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#34920;&#26684;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#24182;&#19981;&#24635;&#26159;&#25104;&#21151;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#20998;&#20540;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#65288;STaSy&#65289;&#30340;&#26032;&#27169;&#22411;&#21450;&#20854;&#22522;&#20110;&#20998;&#20540;&#29983;&#25104;&#24314;&#27169;&#33539;&#20363;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#23613;&#31649;&#22522;&#20110;&#20998;&#20540;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#35299;&#20915;&#20102;&#35768;&#22810;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#20294;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35757;&#32451;&#31574;&#30053;&#21253;&#25324;&#33258;&#36866;&#24212;&#23398;&#20064;&#25216;&#26415;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#31283;&#23450;&#21435;&#22122;&#20998;&#20540;&#21305;&#37197;&#35757;&#32451;&#36827;&#19968;&#27493;&#22686;&#21152;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#37319;&#26679;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#26102;&#38388;&#19977;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#22312;o
&lt;/p&gt;
&lt;p&gt;
Tabular data synthesis is a long-standing research topic in machine learning. Many different methods have been proposed over the past decades, ranging from statistical methods to deep generative methods. However, it has not always been successful due to the complicated nature of real-world tabular data. In this paper, we present a new model named Score-based Tabular data Synthesis (STaSy) and its training strategy based on the paradigm of score-based generative modeling. Despite the fact that score-based generative models have resolved many issues in generative models, there still exists room for improvement in tabular data synthesis. Our proposed training strategy includes a self-paced learning technique and a fine-tuning strategy, which further increases the sampling quality and diversity by stabilizing the denoising score matching training. Furthermore, we also conduct rigorous experimental studies in terms of the generative task trilemma: sampling quality, diversity, and time. In o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#24335;&#8212;&#8212;&#22810;&#27169;&#24577;&#25552;&#31034;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;VIMA&#65292;&#21487;&#20197;&#22788;&#29702;&#25552;&#31034;&#24182;&#33258;&#22238;&#24402;&#22320;&#36755;&#20986;&#30005;&#26426;&#21160;&#20316;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#20219;&#21153;&#31867;&#22411;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#33021;&#22815;&#38646;&#26679;&#26412;&#27867;&#21270;&#21040;&#26032;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#36825;&#23545;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#20855;&#26377;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2210.03094</link><description>&lt;p&gt;
VIMA&#65306;&#22810;&#27169;&#24577;&#25552;&#31034;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
VIMA: General Robot Manipulation with Multimodal Prompts. (arXiv:2210.03094v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#24335;&#8212;&#8212;&#22810;&#27169;&#24577;&#25552;&#31034;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;VIMA&#65292;&#21487;&#20197;&#22788;&#29702;&#25552;&#31034;&#24182;&#33258;&#22238;&#24402;&#22320;&#36755;&#20986;&#30005;&#26426;&#21160;&#20316;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#20219;&#21153;&#31867;&#22411;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#33021;&#22815;&#38646;&#26679;&#26412;&#27867;&#21270;&#21040;&#26032;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#36825;&#23545;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#20855;&#26377;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#27169;&#24335;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25104;&#21151;&#33539;&#20363;&#65292;&#22312;&#27492;&#27169;&#24335;&#19979;&#65292;&#21333;&#20010;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25353;&#29031;&#36755;&#20837;&#25552;&#31034;&#25191;&#34892;&#20219;&#20309;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#20154;&#24037;&#31243;&#20013;&#65292;&#20219;&#21153;&#35268;&#33539;&#30340;&#24418;&#24335;&#22810;&#31181;&#22810;&#26679;&#65292;&#20363;&#22914;&#65292;&#27169;&#20223;&#21333;&#27425;&#28436;&#31034;&#12289;&#36981;&#24490;&#35821;&#35328;&#25351;&#20196;&#21644;&#36798;&#21040;&#35270;&#35273;&#30446;&#26631;&#31561;&#12290;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#30001;&#19987;&#38376;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#24191;&#27867;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#34920;&#36798;&#65292;&#20132;&#38169;&#25991;&#26412;&#21644;&#35270;&#35273;&#20196;&#29260;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#20223;&#30495;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#31243;&#24207;&#29983;&#25104;&#30340;&#26700;&#38754;&#20219;&#21153;&#65292;&#20855;&#26377;&#22810;&#27169;&#24577;&#25552;&#31034;&#65292;60&#19975;&#20010;&#19987;&#23478;&#36712;&#36857;&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#22235;&#32423;&#35780;&#20272;&#21327;&#35758;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#24191;&#20041;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;VIMA&#65292;&#35813;&#20195;&#29702;&#22788;&#29702;&#36825;&#20123;&#25552;&#31034;&#24182;&#33258;&#22238;&#24402;&#22320;&#36755;&#20986;&#30005;&#26426;&#21160;&#20316;&#12290;VIMA&#20855;&#26377;&#19968;&#22871;&#37197;&#26041;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#20219;&#21153;&#31867;&#22411;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#21253;&#25324;&#26410;&#35265;&#36807;&#30340;&#27169;&#24577;&#32452;&#21512;&#65292;&#29978;&#33267;&#21487;&#20197;&#38646;&#26679;&#26412;&#27867;&#21270;&#21040;&#26032;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26694;&#26550;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#35789;&#27719;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#20449;&#24687;&#23481;&#37327;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#22312;&#25991;&#26723;&#26816;&#32034;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.02068</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#38750;&#21442;&#25968;&#21270;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Decoding for Generative Retrieval. (arXiv:2210.02068v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#35789;&#27719;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#20449;&#24687;&#23481;&#37327;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#22312;&#25991;&#26723;&#26816;&#32034;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#20854;&#27169;&#22411;&#21442;&#25968;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#65292;&#27809;&#26377;&#22806;&#37096;&#23384;&#20648;&#22120;&#65292;&#20854;&#20449;&#24687;&#23481;&#37327;&#21463;&#21040;&#38480;&#21046;&#24182;&#19988;&#26159;&#22266;&#23450;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#35299;&#30721;&#26041;&#27861;&#65288;Np Decoding&#65289;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#20013;&#12290;Np Decoding&#20351;&#29992;&#38750;&#21442;&#25968;&#21270;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#27719;&#23884;&#20837;&#65288;&#22806;&#37096;&#23384;&#20648;&#22120;&#65289;&#65292;&#32780;&#19981;&#26159;&#20316;&#20026;&#35299;&#30721;&#22120;&#35789;&#27719;&#23884;&#20837;&#30340;&#24120;&#35268;&#35789;&#27719;&#23884;&#20837;&#12290;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#35789;&#27719;&#23884;&#20837;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#21033;&#29992;&#21442;&#25968;&#31354;&#38388;&#21644;&#38750;&#21442;&#25968;&#31354;&#38388;&#12290;&#22312;9&#20010;&#25968;&#25454;&#38598;&#65288;8&#20010;&#21333;&#36339;&#21644;1&#20010;&#22810;&#36339;&#65289;&#30340;&#25991;&#26723;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#23558;Np Decoding&#24212;&#29992;&#20110;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;Np Decoding&#20855;&#26377;&#25968;&#25454;&#21644;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generative retrieval model depends solely on the information encoded in its model parameters without external memory, its information capacity is limited and fixed. To overcome the limitation, we propose Nonparametric Decoding (Np Decoding) which can be applied to existing generative retrieval models. Np Decoding uses nonparametric contextualized vocab embeddings (external memory) rather than vanilla vocab embeddings as decoder vocab embeddings. By leveraging the contextualized vocab embeddings, the generative retrieval model is able to utilize both the parametric and nonparametric space. Evaluation over 9 datasets (8 single-hop and 1 multi-hop) in the document retrieval task shows that applying Np Decoding to generative retrieval models significantly improves the performance. We also show that Np Decoding is dataand parameter-efficient, and shows high performance in the zero-shot setting.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#24207;&#21015;&#31070;&#32463;&#35299;&#30721;&#22120;CRISP&#65292;&#21487;&#20197;&#29992;&#20110;&#26497;&#21270;&#32534;&#30721;&#26063;&#65292;&#30456;&#27604;&#36830;&#32493;&#21462;&#28040;(SC)&#35793;&#30721;&#22120;&#65292;CRISP&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#21487;&#36731;&#26494;&#22320;&#25299;&#23637;&#33267;&#26497;&#21270;&#35843;&#25972;&#21367;&#31215;&#65288;PAC&#65289;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2210.00313</link><description>&lt;p&gt;
&#22522;&#20110;&#35838;&#31243;&#30340;&#24207;&#21015;&#31070;&#32463;&#35299;&#30721;&#22120;&#29992;&#20110;&#26497;&#21270;&#32534;&#30721;&#26063;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CRISP: Curriculum based Sequential Neural Decoders for Polar Code Family. (arXiv:2210.00313v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#24207;&#21015;&#31070;&#32463;&#35299;&#30721;&#22120;CRISP&#65292;&#21487;&#20197;&#29992;&#20110;&#26497;&#21270;&#32534;&#30721;&#26063;&#65292;&#30456;&#27604;&#36830;&#32493;&#21462;&#28040;(SC)&#35793;&#30721;&#22120;&#65292;CRISP&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#21487;&#36731;&#26494;&#22320;&#25299;&#23637;&#33267;&#26497;&#21270;&#35843;&#25972;&#21367;&#31215;&#65288;PAC&#65289;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#32534;&#30721;&#26159;&#21487;&#38752;&#36890;&#20449;&#30340;&#26368;&#26032;&#35268;&#33539;&#65288;5G&#65289;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#65292;&#20294;&#22312;&#30701;&#22359;&#38271;&#24230;&#33539;&#22260;&#20869;&#35774;&#35745;&#26082;&#39640;&#25928;&#21448;&#21487;&#38752;&#30340;&#26497;&#21270;&#35793;&#30721;&#22120;&#20173;&#26377;&#31354;&#38388;&#12290;&#21463;&#25968;&#25454;&#39537;&#21160;&#20449;&#36947;&#35793;&#30721;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#24207;&#21015;&#31070;&#32463;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#26497;&#21270;&#32534;&#30721;&#65288;CRISP&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21463;&#20449;&#24687;&#29702;&#35770;&#21551;&#21457;&#30340;&#26377;&#21407;&#21017;&#30340;&#35838;&#31243;&#26469;&#35757;&#32451;CRISP&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;Polar&#65288;32,16&#65289;&#21644;Polar&#65288;64,22&#65289;&#20195;&#30721;&#19978;&#20248;&#20110;&#36830;&#32493;&#21462;&#28040;(SC)&#35793;&#30721;&#22120;&#24182;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340; &#21487;&#38752;&#24615;&#33021;&#12290;&#25152;&#25552;&#35758;&#30340;&#35838;&#31243;&#36873;&#25321;&#23545;&#20110;&#23454;&#29616;CRISP&#30340;&#20934;&#30830;&#24615;&#22686;&#30410;&#33267;&#20851;&#37325;&#35201;&#65292;&#27491;&#22914;&#25105;&#20204;&#36890;&#36807;&#19982;&#20854;&#20182;&#35838;&#31243;&#30340;&#27604;&#36739;&#25152;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CRISP&#21487;&#20197;&#36731;&#26494;&#22320;&#25193;&#23637;&#21040;&#26497;&#21270;&#35843;&#25972;&#21367;&#31215;&#65288;PAC&#65289;&#20195;&#30721;&#65292;&#20854;&#20013;&#29616;&#26377;SC&#35299;&#30721;&#22120;&#24615;&#33021;&#25439;&#22833;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polar codes are widely used state-of-the-art codes for reliable communication that have recently been included in the 5th generation wireless standards (5G). However, there remains room for the design of polar decoders that are both efficient and reliable in the short blocklength regime. Motivated by recent successes of data-driven channel decoders, we introduce a novel $\textbf{C}$ur$\textbf{RI}$culum based $\textbf{S}$equential neural decoder for $\textbf{P}$olar codes (CRISP). We design a principled curriculum, guided by information-theoretic insights, to train CRISP and show that it outperforms the successive-cancellation (SC) decoder and attains near-optimal reliability performance on the Polar(32,16) and Polar(64,22) codes. The choice of the proposed curriculum is critical in achieving the accuracy gains of CRISP, as we show by comparing against other curricula. More notably, CRISP can be readily extended to Polarization-Adjusted-Convolutional (PAC) codes, where existing SC decod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#21644;&#21333;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#30340;FIGO&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;Pix2Pix&#27169;&#22411;&#23545;&#20302;&#36136;&#37327;&#25351;&#32441;&#22270;&#20687;&#36827;&#34892;&#22686;&#24378;&#65292;&#22312;&#35782;&#21035;&#29615;&#33410;&#20013;&#20351;&#29992;&#19968;&#27425;&#23398;&#20064;&#36827;&#34892;&#25351;&#32441;&#20449;&#24687;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.05615</link><description>&lt;p&gt;
FIGO:&#20351;&#29992;GAN&#21644;&#21333;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#30340;&#22686;&#24378;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FIGO: Enhanced Fingerprint Identification Approach Using GAN and One Shot Learning Techniques. (arXiv:2208.05615v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#21644;&#21333;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#30340;FIGO&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;Pix2Pix&#27169;&#22411;&#23545;&#20302;&#36136;&#37327;&#25351;&#32441;&#22270;&#20687;&#36827;&#34892;&#22686;&#24378;&#65292;&#22312;&#35782;&#21035;&#29615;&#33410;&#20013;&#20351;&#29992;&#19968;&#27425;&#23398;&#20064;&#36827;&#34892;&#25351;&#32441;&#20449;&#24687;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#32441;&#35777;&#25454;&#22312;&#21009;&#20390;&#35843;&#26597;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#33258;&#21160;&#21270;&#25351;&#32441;&#35782;&#21035;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#21333;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#65288;FIGO&#65289;&#65292;&#21253;&#21547;&#25351;&#32441;&#22686;&#24378;&#21644;&#25351;&#32441;&#35782;&#21035;&#20004;&#20010;&#29615;&#33410;&#12290;&#37319;&#29992;Pix2Pix&#27169;&#22411;&#30452;&#25509;&#22312;&#25351;&#32441;&#22686;&#24378;&#29615;&#33410;&#23558;&#20302;&#36136;&#37327;&#25351;&#32441;&#22270;&#20687;&#36716;&#25442;&#20026;&#39640;&#20687;&#32032;&#27700;&#24179;&#25351;&#32441;&#22270;&#20687;&#65292;&#20351;&#29992;&#19968;&#27425;&#23398;&#20064;&#25216;&#26415;&#21644;&#23402;&#29983;&#32593;&#32476;&#26550;&#26500;&#22312;&#35782;&#21035;&#29615;&#33410;&#20013;&#36827;&#34892;&#25351;&#32441;&#20449;&#24687;&#30340;&#20998;&#31867;&#21644;&#27604;&#23545;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fingerprint evidence plays an important role in a criminal investigation for the identification of individuals. Although various techniques have been proposed for fingerprint classification and feature extraction, automated fingerprint identification of fingerprints is still in its earliest stage. The performance of traditional \textit{Automatic Fingerprint Identification System} (AFIS) depends on the presence of valid minutiae points and still requires human expert assistance in feature extraction and identification stages. Based on this motivation, we propose a Fingerprint Identification approach based on Generative adversarial network and One-shot learning techniques (FIGO). Our solution contains two components: fingerprint enhancement tier and fingerprint identification tier. First, we propose a Pix2Pix model to transform low-quality fingerprint images to a higher level of fingerprint images pixel by pixel directly in the fingerprint enhancement tier. With the proposed enhancement 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24515;&#33039;&#30142;&#30149;&#26465;&#20214;&#23398;&#20064;&#20840;&#26223;&#24515;&#30005;&#22270;&#34920;&#31034;&#30340;&#22810;&#35270;&#35282;ECG&#21512;&#25104;&#26041;&#27861;ME-GAN&#65292;&#21033;&#29992;&#26032;&#30340;&#28151;&#21512;&#24402;&#19968;&#21270;&#26041;&#27861;&#23558;&#30142;&#30149;&#20449;&#24687;&#27880;&#20837;&#21040;&#21512;&#36866;&#20301;&#32622;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#21644;&#30142;&#30149;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.10670</link><description>&lt;p&gt;
ME-GAN&#65306;&#22522;&#20110;&#24515;&#33039;&#30142;&#30149;&#26465;&#20214;&#23398;&#20064;&#20840;&#26223;&#24515;&#30005;&#22270;&#34920;&#31034;&#30340;&#22810;&#35270;&#35282;ECG&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
ME-GAN: Learning Panoptic Electrocardio Representations for Multi-view ECG Synthesis Conditioned on Heart Diseases. (arXiv:2207.10670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24515;&#33039;&#30142;&#30149;&#26465;&#20214;&#23398;&#20064;&#20840;&#26223;&#24515;&#30005;&#22270;&#34920;&#31034;&#30340;&#22810;&#35270;&#35282;ECG&#21512;&#25104;&#26041;&#27861;ME-GAN&#65292;&#21033;&#29992;&#26032;&#30340;&#28151;&#21512;&#24402;&#19968;&#21270;&#26041;&#27861;&#23558;&#30142;&#30149;&#20449;&#24687;&#27880;&#20837;&#21040;&#21512;&#36866;&#20301;&#32622;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#21644;&#30142;&#30149;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;( ECG)&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#65292;&#29992;&#20110;&#26816;&#27979;&#24515;&#33039;&#30142;&#30149;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35774;&#35745;&#20102;ECG&#20998;&#26512;&#27169;&#22411;(&#22914;&#20998;&#31867;&#22120;)&#26469;&#36741;&#21161;&#35786;&#26029;&#12290;&#20316;&#20026;&#19968;&#20010;&#19978;&#28216;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#24314;&#31435;&#29983;&#25104;&#27169;&#22411;&#26469;&#21512;&#25104;ECG&#25968;&#25454;&#65292;&#36825;&#26377;&#21033;&#20110;&#25552;&#20379;&#35757;&#32451;&#26679;&#26412;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#27880;&#37322;&#20943;&#23569;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;ECG&#29983;&#25104;&#26041;&#27861;&#24448;&#24448;&#26082;&#27809;&#26377;&#21512;&#25104;&#22810;&#35270;&#22270;&#25968;&#25454;&#65292;&#20063;&#27809;&#26377;&#22788;&#29702;&#24515;&#33039;&#30142;&#30149;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20197;&#30142;&#30149;&#20026;&#23548;&#21521;&#30340;&#22810;&#35270;&#35282;ECG&#21512;&#25104;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;ME-GAN&#65292;&#23427;&#33719;&#24471;&#20102;&#22522;&#20110;&#24515;&#33039;&#30142;&#30149;&#26465;&#20214;&#30340;&#20840;&#26223;&#24515;&#30005;&#22270;&#34920;&#31034;&#65292;&#24182;&#23558;&#36825;&#20123;&#34920;&#31034;&#25237;&#24433;&#21040;&#22810;&#20010;&#26631;&#20934;&#35270;&#22270;&#19978;&#20197;&#20135;&#29983;ECG&#20449;&#21495;&#12290;&#30001;&#20110;&#24515;&#33039;&#30142;&#30149;&#30340;ECG&#34920;&#29616;&#36890;&#24120;&#23616;&#37096;&#21270;&#22312;&#29305;&#23450;&#30340;&#27874;&#24418;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#28151;&#21512;&#24402;&#19968;&#21270;"&#26041;&#27861;&#65292;&#20197;&#23558;&#30142;&#30149;&#20449;&#24687;&#31934;&#30830;&#22320;&#27880;&#20837;&#21040;&#21512;&#36866;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) is a widely used non-invasive diagnostic tool for heart diseases. Many studies have devised ECG analysis models (e.g., classifiers) to assist diagnosis. As an upstream task, researches have built generative models to synthesize ECG data, which are beneficial to providing training samples, privacy protection, and annotation reduction. However, previous generative methods for ECG often neither synthesized multi-view data, nor dealt with heart disease conditions. In this paper, we propose a novel disease-aware generative adversarial network for multi-view ECG synthesis called ME-GAN, which attains panoptic electrocardio representations conditioned on heart diseases and projects the representations onto multiple standard views to yield ECG signals. Since ECG manifestations of heart diseases are often localized in specific waveforms, we propose a new "mixup normalization" to inject disease information precisely into suitable locations. In addition, we propose a view 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#27169;&#22411;&#39537;&#21160;&#30340;RL&#26694;&#26550;&#65292;&#23558;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#21160;&#21147;&#23398;&#23398;&#20064;&#20998;&#31163;&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#21644;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#20934;&#30830;&#24314;&#27169;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2206.14244</link><description>&lt;p&gt;
&#21487;&#25513;&#34109;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Masked World Models for Visual Control. (arXiv:2206.14244v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#27169;&#22411;&#39537;&#21160;&#30340;RL&#26694;&#26550;&#65292;&#23558;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#21160;&#21147;&#23398;&#23398;&#20064;&#20998;&#31163;&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#21644;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#20934;&#30830;&#24314;&#27169;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26377;&#28508;&#21147;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#35270;&#35273;&#35266;&#27979;&#20013;&#23454;&#29616;&#26679;&#26412;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#21333;&#19968;&#27169;&#22411;&#26469;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#21644;&#21160;&#21147;&#23398;&#65292;&#20351;&#24471;&#20934;&#30830;&#24314;&#27169;&#26426;&#22120;&#20154;&#19982;&#23567;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#35270;&#35273;&#27169;&#22411;&#39537;&#21160;&#30340;RL&#26694;&#26550;&#65292;&#23558;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#21160;&#21147;&#23398;&#23398;&#20064;&#20998;&#31163;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#21367;&#31215;&#23618;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#26469;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#65292;&#20197;&#22312;&#32473;&#23450;&#25513;&#34109;&#21367;&#31215;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#37325;&#26500;&#20687;&#32032;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#25805;&#20316;&#33258;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32534;&#30721;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#22870;&#21169;&#39044;&#27979;&#30446;&#26631;&#26469;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#29615;&#22659;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#22312;&#32447;&#26679;&#26412;&#19981;&#26029;&#26356;&#26032;&#33258;&#32534;&#30721;&#22120;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#31163;&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling a
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;200&#20010;AI&#27835;&#29702;&#25351;&#21335;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;&#23545;&#36825;&#20123;&#25991;&#26723;&#20869;&#23481;&#21644;&#24615;&#36136;&#30340;&#21487;&#35270;&#21270;&#65292;&#26088;&#22312;&#20102;&#35299;&#21508;&#26426;&#26500;&#38388;AI&#20262;&#29702;&#21407;&#21017;&#23384;&#22312;&#30340;&#20849;&#35782;&#21644;&#30456;&#20284;&#28857;&#65292;&#20026;&#26410;&#26469;&#30340;&#27861;&#35268;&#36777;&#35770;&#25552;&#20379;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2206.11922</link><description>&lt;p&gt;
&#12298;&#20840;&#29699;AI&#20262;&#29702;&#65306;200&#20010;AI&#27835;&#29702;&#25351;&#21335;&#21644;&#24314;&#35758;&#30340;&#32508;&#36848;&#12299;
&lt;/p&gt;
&lt;p&gt;
Worldwide AI Ethics: a review of 200 guidelines and recommendations for AI governance. (arXiv:2206.11922v5 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;200&#20010;AI&#27835;&#29702;&#25351;&#21335;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;&#23545;&#36825;&#20123;&#25991;&#26723;&#20869;&#23481;&#21644;&#24615;&#36136;&#30340;&#21487;&#35270;&#21270;&#65292;&#26088;&#22312;&#20102;&#35299;&#21508;&#26426;&#26500;&#38388;AI&#20262;&#29702;&#21407;&#21017;&#23384;&#22312;&#30340;&#20849;&#35782;&#21644;&#30456;&#20284;&#28857;&#65292;&#20026;&#26410;&#26469;&#30340;&#27861;&#35268;&#36777;&#35770;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#19968;&#20123;&#32452;&#32455;&#24050;&#32463;&#21046;&#23450;&#20102;&#25991;&#20214;&#65292;&#26088;&#22312;&#35268;&#33539;&#21644;&#25512;&#21160;&#25105;&#20204;&#26368;&#36817;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;AI&#25216;&#26415;&#30340;&#25351;&#21335;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#19968;&#20123;&#39046;&#22495;&#30340;&#20803;&#20998;&#26512;&#21644;&#25209;&#21028;&#24615;&#35780;&#35770;&#22806;&#65292;&#36825;&#20123;&#25991;&#20214;&#20013;&#21576;&#29616;&#30340;&#23436;&#25972;&#24605;&#24819;&#20809;&#35889;&#23578;&#26410;&#24471;&#21040;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#25193;&#23637;&#36807;&#21435;&#30740;&#31350;&#20154;&#21592;&#25152;&#20570;&#30340;&#24037;&#20316;&#65292;&#21019;&#24314;&#19968;&#20010;&#26356;&#22909;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#20197;&#20102;&#35299;&#21508;&#26426;&#26500;&#25152;&#20513;&#23548;&#21407;&#21017;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#20849;&#35782;&#25110;&#30456;&#20284;&#20043;&#22788;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21457;&#26410;&#26469;&#30340;&#27861;&#35268;&#36777;&#35770;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;200&#20221;&#25991;&#26723;&#26679;&#26412;&#30340;&#26041;&#27861;&#35770;&#32467;&#26524;&#36827;&#34892;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#30340;&#24819;&#27861;&#21644;&#38382;&#39064;&#65292;&#20197;&#25351;&#23548;&#30740;&#31350;&#30340;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, several organizations have produced documents intended to standardize, in the normative sense, and promote guidance to our recent and rapid AI development. However, the full spectrum of ideas presented in these documents has not yet been analyzed, except for a few meta-analyses and critical reviews of the field. In this work, we seek to expand on the work done by past researchers and create a tool for better data visualization of the contents and nature of these documents, to understand whether there is consensus or similarity between the principles espoused by various institutions, which may inspire debates on future regulations. We also provide some preliminary thoughts and questions that could guide the continuity of the research through a critical analysis of the results acquired by our methodology into a sample size of 200 documents.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13743</link><description>&lt;p&gt;
&#24102;&#26377;&#20559;&#22909;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#31639;&#27861;&#24178;&#39044;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Personalized Algorithmic Recourse with Preference Elicitation. (arXiv:2205.13743v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13743
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#24178;&#39044;&#65288;AR&#65289;&#30340;&#38382;&#39064;&#26159;&#35745;&#31639;&#29992;&#25143;&#25191;&#34892;&#19968;&#31995;&#21015;&#25805;&#20316;&#20197;&#39072;&#35206;&#19981;&#33391;&#26426;&#22120;&#20915;&#31574;&#30340;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#30340;&#25805;&#20316;&#24207;&#21015;&#19981;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#23454;&#26045;&#25552;&#20986;&#36807;&#39640;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;AR&#26041;&#27861;&#37117;&#20551;&#35774;&#25152;&#26377;&#29992;&#25143;&#30340;&#25805;&#20316;&#25104;&#26412;&#30456;&#21516;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#21521;&#26576;&#20123;&#29992;&#25143;&#25512;&#33616;&#26114;&#36149;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEAR&#65292;&#36825;&#26159;&#19968;&#31181;&#39318;&#20010;&#21487;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#65292;&#20197;&#28385;&#36275;&#20219;&#20309;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;PEAR&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#21521;&#30446;&#26631;&#29992;&#25143;&#21457;&#20986;&#36873;&#25321;&#38598;&#26597;&#35810;&#26469;&#36845;&#20195;&#22320;&#25913;&#21892;&#23545;&#25805;&#20316;&#25104;&#26412;&#30340;&#20272;&#35745;&#20540;&#12290;&#36825;&#20123;&#26597;&#35810;&#30340;&#35745;&#31639;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#26469;&#35745;&#31639;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#25104;&#26412;&#20272;&#35745;&#21644;&#29992;&#25143;&#21709;&#24212;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#12290;PEAR&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#23454;&#29616;AR&#20219;&#21153;&#25152;&#38656;&#36798;&#25104;&#30446;&#26631;&#30340;&#20559;&#22909;&#65292;&#20197;&#21450;&#25191;&#34892;&#27599;&#20010;&#25805;&#20316;&#25152;&#28041;&#21450;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;AR&#20219;&#21153;&#26469;&#35780;&#20272;PEAR&#65292;&#24182;&#26174;&#31034;&#20854;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#26356;&#20026;&#32463;&#27982;&#23454;&#29992;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Rein
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102; Masked Graph Modeling &#22312;&#22270;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33945;&#29256;&#30340;&#22270;&#24314;&#27169;&#65288;MGM&#65289;&#20316;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#20010;&#21407;&#20808;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#20248;&#21183;&#20316;&#29992;&#12290;&#22312;&#33719;&#24471;&#20102;&#29702;&#35770;&#21450;&#23454;&#35777;&#35777;&#25454;&#30340;&#25903;&#25345;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MaskGAE&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.10053</link><description>&lt;p&gt;
&#25506;&#31350;&#22270;&#33258;&#32534;&#30721;&#22120;&#20013; Masked Graph Modeling &#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders. (arXiv:2205.10053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102; Masked Graph Modeling &#22312;&#22270;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33945;&#29256;&#30340;&#22270;&#24314;&#27169;&#65288;MGM&#65289;&#20316;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#20010;&#21407;&#20808;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#20248;&#21183;&#20316;&#29992;&#12290;&#22312;&#33719;&#24471;&#20102;&#29702;&#35770;&#21450;&#23454;&#35777;&#35777;&#25454;&#30340;&#25903;&#25345;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MaskGAE&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#31181;&#39047;&#20855;&#28508;&#21147;&#30340;&#31574;&#30053;&#34987;&#31216;&#20026; Masked Autoencoding&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#22270;&#33258;&#32534;&#30721;&#22120;&#20013;&#22914;&#20309;&#23454;&#29616; Masking&#65292;&#29702;&#35770;&#19978;&#20173;&#28982;&#23384;&#22312;&#32570;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; Masked Graph Autoencoder &#65288;MaskGAE&#65289;&#65292;&#23427;&#26159;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;&#30456;&#36739;&#20110;&#20854;&#23427;&#26222;&#36890;&#30340; GAEs&#65292;MaskGAE &#37319;&#29992;&#22522;&#20110;&#33945;&#29256;&#30340;&#22270;&#24314;&#27169;&#65288;Masked Graph Modeling&#65292;MGM&#65289;&#20316;&#20026;&#19968;&#20010;&#21407;&#20808;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#8220;&#25513;&#34109;&#8221;&#19968;&#20010;&#37096;&#20998;&#36793;&#32536;&#65292;&#20197;&#37096;&#20998;&#21487;&#35270;&#12289;&#38750;&#25513;&#34109;&#30340;&#22270;&#32467;&#26500;&#65292;&#35797;&#22270;&#37325;&#26500;&#32570;&#22833;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21450;&#23454;&#35777;&#35777;&#25454;&#65292;&#20840;&#38754;&#35777;&#26126;&#20102;&#27492;&#39044;&#27979;&#20219;&#21153;&#20248;&#21183;&#30340;&#20316;&#29992;&#65292;&#20197;&#25506;&#31350; MGM &#23545; GAEs &#30340;&#36755;&#20986;&#34920;&#31034;&#30340;&#25913;&#21892;&#20316;&#29992;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102; GAEs &#19982;&#23545;&#27604;&#23398;&#20064;&#30340;&#32039;&#23494;&#20851;&#31995;&#65292;&#34920;&#26126; MGM &#26126;&#26174;&#25913;&#21892;&#20102; GAEs &#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#12290;&#22312;&#32463;&#39564;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102; MaskGAE &#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19979;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; GAEs&#12290;&#26412;&#30740;&#31350;&#20026;&#25506;&#31350; Masked Graph Modeling &#22312;&#22270;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#37325;&#35201;&#24615;&#25552;&#20379;&#20102;&#21551;&#31034;&#65292;&#24182;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last years have witnessed the emergence of a promising self-supervised learning strategy, referred to as masked autoencoding. However, there is a lack of theoretical understanding of how masking matters on graph autoencoders (GAEs). In this work, we present masked graph autoencoder (MaskGAE), a self-supervised learning framework for graph-structured data. Different from standard GAEs, MaskGAE adopts masked graph modeling (MGM) as a principled pretext task - masking a portion of edges and attempting to reconstruct the missing part with partially visible, unmasked graph structure. To understand whether MGM can help GAEs learn better representations, we provide both theoretical and empirical evidence to comprehensively justify the benefits of this pretext task. Theoretically, we establish close connections between GAEs and contrastive learning, showing that MGM significantly improves the self-supervised learning scheme of GAEs. Empirically, we conduct extensive experiments on a variet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#33258;&#21160;&#20135;&#29983;&#21407;&#22987;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#25552;&#20986;&#32452;&#21512;&#27010;&#24565;&#26144;&#23556;&#65288;CompMap&#65289;&#26694;&#26550;&#26469;&#30740;&#31350;&#65292;&#35748;&#20026;&#22914;&#26524;&#27169;&#22411;&#30830;&#23454;&#20135;&#29983;&#20102;&#21407;&#22987;&#27010;&#24565;&#65292;&#20854;&#28608;&#27963;&#24212;&#31526;&#21512;&#20808;&#21069;&#20154;&#31867;&#23545;&#21407;&#22987;&#27010;&#24565;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.17271</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#21040;&#32452;&#21512;&#21407;&#22987;&#27010;&#24565;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?. (arXiv:2203.17271v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.17271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#33258;&#21160;&#20135;&#29983;&#21407;&#22987;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#25552;&#20986;&#32452;&#21512;&#27010;&#24565;&#26144;&#23556;&#65288;CompMap&#65289;&#26694;&#26550;&#26469;&#30740;&#31350;&#65292;&#35748;&#20026;&#22914;&#26524;&#27169;&#22411;&#30830;&#23454;&#20135;&#29983;&#20102;&#21407;&#22987;&#27010;&#24565;&#65292;&#20854;&#28608;&#27963;&#24212;&#31526;&#21512;&#20808;&#21069;&#20154;&#31867;&#23545;&#21407;&#22987;&#27010;&#24565;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#21644;&#38646;&#26679;&#26412;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#35768;&#22810;VL&#27169;&#22411;&#26159;&#22312;&#20114;&#32852;&#32593;&#19978;&#26080;&#26631;&#27880;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#23545;&#19978;&#39044;&#35757;&#32451;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#39044;&#35757;&#32451;&#30340;VL&#27169;&#22411;&#26159;&#21542;&#33258;&#21160;&#20135;&#29983;&#21407;&#22987;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#39068;&#33394;&#12289;&#24418;&#29366;&#25110;&#29289;&#20307;&#37096;&#20998;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#26694;&#26550;&#65292;&#32452;&#21512;&#27010;&#24565;&#26144;&#23556;&#65288;CompMap&#65289;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;CompMap&#39318;&#20808;&#35831;&#27714;VL&#27169;&#22411;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#21407;&#22987;&#27010;&#24565;&#28608;&#27963;&#65292;&#28982;&#21518;&#23398;&#20064;&#26500;&#24314;&#19968;&#20010;&#32452;&#21512;&#27169;&#22411;&#65292;&#23558;&#21407;&#22987;&#27010;&#24565;&#28608;&#27963;&#65288;&#20363;&#22914;&#40657;&#33394;&#23614;&#24052;&#25110;&#32418;&#33394;&#32709;&#33152;&#30340;&#21487;&#33021;&#24615;&#65289;&#26144;&#23556;&#21040;&#32452;&#21512;&#27010;&#24565;&#65288;&#20363;&#22914;&#32418;&#32709;&#40657;&#40479;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20174;&#30495;&#23454;&#30340;&#21407;&#22987;&#27010;&#24565;&#31283;&#23450;&#22320;&#23398;&#20064;&#21040;&#32452;&#21512;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#22914;&#26524;&#39044;&#35757;&#32451;&#30340;VL&#27169;&#22411;&#30830;&#23454;&#20135;&#29983;&#20102;&#21407;&#22987;&#27010;&#24565;&#65292;&#21017;&#20854;&#21407;&#22987;&#27010;&#24565;&#28608;&#27963;...
&lt;/p&gt;
&lt;p&gt;
Vision-language (VL) pretrained models have achieved impressive performance on multimodal reasoning and zero-shot recognition tasks. Many of these VL models are pretrained on unlabeled image and caption pairs from the internet. In this paper, we study whether representations of primitive concepts--such as colors, shapes, or the attributes of object parts--emerge automatically within these pretrained VL models. We propose a two-step framework, Compositional Concept Mapping (CompMap), to investigate this. CompMap first asks a VL model to generate primitive concept activations with text prompts, and then learns to construct a composition model that maps the primitive concept activations (e.g. the likelihood of black tail or red wing) to composite concepts (e.g. a red-winged blackbird). We show that a composition model can be reliably learn from ground truth primitive concepts. We thus hypothesize that if primitive concepts indeed emerge in a VL pretrained model, its primitive concept acti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#24418;&#24335;&#30340;&#25968;&#25454;&#26377;&#25928;&#12289;&#22522;&#20110;&#29305;&#24449;&#30340;&#21160;&#24577;&#35828;&#35805;&#32773;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35328;&#35821;&#38556;&#30861;&#21644;&#32769;&#24180;&#20154;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2203.14593</link><description>&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#24555;&#36895;&#35828;&#35805;&#32773;&#36866;&#24212;&#22312;&#35328;&#35821;&#38556;&#30861;&#21644;&#32769;&#24180;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition. (arXiv:2203.14593v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#24418;&#24335;&#30340;&#25968;&#25454;&#26377;&#25928;&#12289;&#22522;&#20110;&#29305;&#24449;&#30340;&#21160;&#24577;&#35828;&#35805;&#32773;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35328;&#35821;&#38556;&#30861;&#21644;&#32769;&#24180;&#20154;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20934;&#30830;&#35782;&#21035;&#35328;&#35821;&#38556;&#30861;&#21644;&#32769;&#24180;&#20154;&#30340;&#35821;&#38899;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#21475;&#38899;&#25110;&#24615;&#21035;&#24341;&#36215;&#30340;&#35828;&#35805;&#32773;&#32423;&#21035;&#30340;&#24322;&#36136;&#24615;&#65292;&#19982;&#24180;&#40836;&#21644;&#35328;&#35821;&#38556;&#30861;&#30456;&#32467;&#21512;&#65292;&#20351;&#36825;&#20123;&#35828;&#35805;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#24456;&#22823;&#12290;&#35828;&#35805;&#32773;&#32423;&#21035;&#25968;&#25454;&#30340;&#31232;&#32570;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#23494;&#38598;&#22411;&#27169;&#22411;&#30340;&#35828;&#35805;&#32773;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#24418;&#24335;&#30340;&#25968;&#25454;&#26377;&#25928;&#12289;&#22522;&#20110;&#29305;&#24449;&#30340;&#21160;&#24577;&#35828;&#35805;&#32773;&#36866;&#24212;&#26041;&#27861;&#65306;&#26041;&#24046;&#27491;&#21017;&#21270;&#39057;&#35889;&#22522;&#20934;&#23884;&#20837;&#65288;SVR&#65289;&#21644;&#35889;&#29305;&#24449;&#39537;&#21160;&#30340;f-LHUC&#36716;&#25442;&#12290;&#22312;UASpeech&#35328;&#35821;&#38556;&#30861;&#21644;DementiaBank Pitt&#32769;&#24180;&#20154;&#35821;&#38899;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#35828;&#35805;&#32773;&#36866;&#24212;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#22320;&#20248;&#20110;&#22522;&#32447;iVector&#33258;&#36866;&#24212;&#28151;&#21512;DNN/TDNN&#21644;E2E Conformer&#31995;&#32479;&#65292;&#32479;&#35745;&#26174;&#33879;&#22320;&#20943;&#23569;WER 2.48%-2.85%&#65288;&#32477;&#23545;&#20540;&#65289;&#65288;7.92%-8.06%&#30456;&#20851;&#65289;&#65292;&#19982;&#31163;&#32447;&#27169;&#22411;&#22522;&#30784;LHUC&#36866;&#24212;&#20998;&#21035;&#20943;&#23567;1.82%&#65288;&#30456;&#23545;&#20540;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate recognition of dysarthric and elderly speech remain challenging tasks to date. Speaker-level heterogeneity attributed to accent or gender, when aggregated with age and speech impairment, create large diversity among these speakers. Scarcity of speaker-level data limits the practical use of data-intensive model based speaker adaptation methods. To this end, this paper proposes two novel forms of data-efficient, feature-based on-the-fly speaker adaptation methods: variance-regularized spectral basis embedding (SVR) and spectral feature driven f-LHUC transforms. Experiments conducted on UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest the proposed on-the-fly speaker adaptation approaches consistently outperform baseline iVector adapted hybrid DNN/TDNN and E2E Conformer systems by statistically significant WER reduction of 2.48%-2.85% absolute (7.92%-8.06% relative), and offline model based LHUC adaptation by 1.82% absolute (5.63% relative) respectively.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;&#30456;&#20851;&#29305;&#24449;&#23545;&#29992;&#20110;&#39044;&#27979;&#30707;&#27833;&#20844;&#21496;&#32929;&#31080;&#30340;LSTM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28155;&#21152;&#19982;&#30707;&#27833;&#32929;&#31080;&#30456;&#20851;&#30340;&#29305;&#24449;&#24182;&#19981;&#20250;&#25552;&#39640;LSTM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#27492;&#24212;&#35880;&#24910;&#20381;&#38752;LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#24066;&#22330;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2201.00350</link><description>&lt;p&gt;
LSTM&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#30707;&#27833;&#20844;&#21496;&#32929;&#31080;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#30456;&#20851;&#29305;&#24449;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Interpretability of LSTM Models for Predicting Oil Company Stocks: impacts of correlated features. (arXiv:2201.00350v3 [q-fin.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.00350
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;&#30456;&#20851;&#29305;&#24449;&#23545;&#29992;&#20110;&#39044;&#27979;&#30707;&#27833;&#20844;&#21496;&#32929;&#31080;&#30340;LSTM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28155;&#21152;&#19982;&#30707;&#27833;&#32929;&#31080;&#30456;&#20851;&#30340;&#29305;&#24449;&#24182;&#19981;&#20250;&#25552;&#39640;LSTM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#27492;&#24212;&#35880;&#24910;&#20381;&#38752;LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#24066;&#22330;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30707;&#27833;&#20844;&#21496;&#26159;&#20840;&#29699;&#26368;&#22823;&#30340;&#20844;&#21496;&#20043;&#19968;&#65292;&#30001;&#20110;&#19982;&#40644;&#37329;&#12289;&#21407;&#27833;&#21644;&#32654;&#20803;&#30456;&#20851;&#65292;&#20854;&#32463;&#27982;&#25351;&#26631;&#23545;&#20840;&#29699;&#32463;&#27982;&#21644;&#24066;&#22330;&#26377;&#30528;&#24040;&#22823;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#30456;&#20851;&#29305;&#24449;&#23545;&#29992;&#20110;&#39044;&#27979;&#30707;&#27833;&#20844;&#21496;&#32929;&#31080;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26631;&#20934;&#30340;LSTM&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#30456;&#20851;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#24433;&#21709;&#24066;&#22330;&#30340;&#22810;&#20010;&#22240;&#32032;&#65292;&#22914;&#21407;&#27833;&#20215;&#26684;&#12289;&#40644;&#37329;&#20215;&#26684;&#21644;&#32654;&#20803;&#65292;&#26469;&#25552;&#39640;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28155;&#21152;&#19982;&#30707;&#27833;&#32929;&#31080;&#30456;&#20851;&#30340;&#29305;&#24449;&#24182;&#19981;&#20250;&#25552;&#39640;LSTM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;LSTM&#27169;&#22411;&#22312;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#26041;&#38754;&#21487;&#33021;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#20854;&#21487;&#35299;&#37322;&#24615;&#21487;&#33021;&#26377;&#38480;&#12290;&#22312;&#20165;&#20381;&#38752;LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#24066;&#22330;&#20915;&#31574;&#26102;&#24212;&#26684;&#22806;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oil companies are among the largest companies in the world whose economic indicators in the global stock market have a great impact on the world economy and market due to their relation to gold, crude oil, and the dollar. This study investigates the impact of correlated features on the interpretability of Long Short-Term Memory (LSTM) models for predicting oil company stocks. To achieve this, we designed a Standard Long Short-Term Memory (LSTM) network and trained it using various correlated datasets. Our approach aims to improve the accuracy of stock price prediction by considering the multiple factors affecting the market, such as crude oil prices, gold prices, and the US dollar. The results demonstrate that adding a feature correlated with oil stocks does not improve the interpretability of LSTM models. These findings suggest that while LSTM models may be effective in predicting stock prices, their interpretability may be limited. Caution should be exercised when relying solely on L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#12289;&#21487;&#22609;&#24615;&#21644;&#20803;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#33719;&#21462;&#26032;&#30340;&#35748;&#30693;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.08588</link><description>&lt;p&gt;
&#12298;&#20351;&#29992;&#36827;&#21270;&#12289;&#21487;&#22609;&#24615;&#21644;&#20803;&#20803;&#23398;&#20064;&#23398;&#20064;&#33719;&#21462;&#26032;&#35748;&#30693;&#20219;&#21153;&#12299;&#65288;arXiv:2112.08588v10 [cs.NE] &#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Learning to acquire novel cognitive tasks with evolution, plasticity and meta-meta-learning. (arXiv:2112.08588v10 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#12289;&#21487;&#22609;&#24615;&#21644;&#20803;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#33719;&#21462;&#26032;&#30340;&#35748;&#30693;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#26631;&#24535;&#26159;&#33021;&#22815;&#33258;&#20027;&#22320;&#23398;&#20064;&#26032;&#30340;&#28789;&#27963;&#30340;&#35748;&#30693;&#34892;&#20026;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#36866;&#24403;&#30340;&#34892;&#20026;&#19981;&#20165;&#21462;&#20915;&#20110;&#30452;&#25509;&#30340;&#21050;&#28608;&#65288;&#22914;&#31616;&#21333;&#30340;&#21453;&#23556;&#24615;&#21050;&#28608;-&#21453;&#24212;&#20851;&#32852;&#65289;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#24517;&#39035;&#36275;&#22815;&#33719;&#21462;&#12289;&#23384;&#20648;&#21644;&#22788;&#29702;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#34429;&#28982;&#35768;&#22810;&#20803;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#35774;&#35745;&#20986;&#33258;&#20027;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#20294;&#26159;&#35748;&#30693;&#20219;&#21153;&#32473;&#20856;&#22411;&#30340;&#8220;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#8221;&#38382;&#39064;&#28155;&#21152;&#20102;&#21478;&#19968;&#20010;&#23398;&#20064;&#21644;&#35760;&#24518;&#30340;&#23618;&#38754;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#26694;&#26550;&#20013;&#30340;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#35748;&#30693;&#20219;&#21153;&#36827;&#21270;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#24102;&#26377;&#21487;&#22609;&#30340;&#36830;&#25509;&#21644;&#31070;&#32463;&#35843;&#33410;&#12290;&#32467;&#26524;&#24471;&#21040;&#30340;&#36827;&#21270;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20854;&#33258;&#36523;&#30340;&#36830;&#25509;&#33258;&#21160;&#20462;&#25913;&#26469;&#33719;&#21462;&#26032;&#30340;&#12289;&#20174;&#26410;&#22312;&#36827;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#36807;&#30340;&#31616;&#21333;&#35748;&#30693;&#20219;&#21153;&#65292;&#36890;&#36807;&#20854;&#36827;&#21270;&#30340;&#31070;&#32463;&#32452;&#32455;&#21644;&#21487;&#22609;&#24615;&#31995;&#32479;&#30340;&#33258;&#21457;&#36816;&#20316;&#65292;&#21482;&#22522;&#20110;&#21050;&#28608;&#21644;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#23398;&#20064;&#35748;&#30693;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hallmark of intelligence is the ability to autonomously learn new flexible, cognitive behaviors - that is, behaviors where the appropriate action depends not just on immediate stimuli (as in simple reflexive stimulus-response associations), but on contextual information that must be adequately acquired, stored and processed. While many meta-learning algorithms can design agents that autonomously learn new tasks, cognitive tasks adds another level of learning and memory to typical ``learning-to-learn'' problems. Here we evolve neural networks, endowed with plastic connections and neuromodulation, over a sizable set of simple cognitive tasks adapted from a computational neuroscience framework. The resulting evolved networks can automatically modify their own connectivity to acquire a novel simple cognitive task, never seen during evolution, from stimuli and rewards alone, through the spontaneous operation of their evolved neural organization and plasticity system. Our results emphasize
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Paddle-HeterPS&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#22810;&#23618;&#27425;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2111.10635</link><description>&lt;p&gt;
HeterPS&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;&#30340;&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments. (arXiv:2111.10635v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Paddle-HeterPS&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#22810;&#23618;&#27425;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#35768;&#22810;&#23618;&#21644;&#22823;&#37327;&#21442;&#25968;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;DNN&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#22788;&#29702;&#20855;&#26377;&#35768;&#22810;&#31232;&#30095;&#29305;&#24449;&#30340;&#22823;&#35268;&#27169;&#36755;&#20837;&#25968;&#25454;&#65292;&#36825;&#20250;&#20135;&#29983;&#39640;&#24310;&#36831;&#21644;I/O&#25104;&#26412;&#65292;&#32780;&#26576;&#20123;&#23618;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#21033;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#36164;&#28304;&#26469;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22914;CPU&#21644;GPU&#31561;&#65292;&#20063;&#21487;&#29992;&#20110;&#20998;&#24067;&#24335;&#35757;&#32451;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#22810;&#23618;&#27425;&#22320;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#23545;&#35757;&#32451;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#36890;&#36807;&#24322;&#26500;&#35745;&#31639;&#36164;&#28304;&#39640;&#25928;&#22320;&#35757;&#32451;DNN&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#26694;&#26550;Paddle-Heterogeneous Parameter Server&#65288;Paddle-HeterPS&#65289;&#65292;&#30001;&#20998;&#24067;&#24335;&#26550;&#26500;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#32452;&#25104;&#12290;&#19982;&#29616;&#26377;&#26694;&#26550;&#30456;&#27604;&#65292;Paddle-HeterPS&#30340;&#20248;&#28857;&#26377;&#19977;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) exploit many layers and a large number of parameters to achieve excellent performance. The training process of DNN models generally handles large-scale input data with many sparse features, which incurs high Input/Output (IO) cost, while some layers are compute-intensive. The training process generally exploits distributed computing resources to reduce training time. In addition, heterogeneous computing resources, e.g., CPUs, GPUs of multiple types, are available for the distributed training process. Thus, the scheduling of multiple layers to diverse computing resources is critical for the training process. To efficiently train a DNN model using the heterogeneous computing resources, we propose a distributed framework, i.e., Paddle-Heterogeneous Parameter Server (Paddle-HeterPS), composed of a distributed architecture and a Reinforcement Learning (RL)-based scheduling method. The advantages of Paddle-HeterPS are three-fold compared with existing frameworks. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.11760</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Fingerprinting Generative Adversarial Networks. (arXiv:2106.11760v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#30001;&#20110;&#21830;&#19994;GAN&#30340;&#29983;&#20135;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#65292;&#22240;&#27492;&#36843;&#20999;&#38656;&#35201;&#29256;&#26435;&#20445;&#25252;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#12290;&#25105;&#20204;&#31361;&#30772;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#25152;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21019;&#36896;&#24615;&#22320;&#20174;&#30446;&#26631;GAN&#21644;&#20998;&#31867;&#22120;&#26500;&#24314;&#19968;&#20010;&#22797;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#36825;&#20010;&#22797;&#21512;&#27169;&#22411;&#20013;&#20135;&#29983;&#25351;&#32441;&#26679;&#26412;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#29256;&#26435;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#26696;&#21551;&#21457;&#20102;&#19968;&#20123;&#20855;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#25152;&#38656;&#35201;&#30340;&#19981;&#21516;&#23433;&#20840;&#35201;&#27714;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#35777;&#26126;&#35813;&#26041;&#26696;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have been widely used in various application scenarios. Since the production of a commercial GAN requires substantial computational and human resources, the copyright protection of GANs is urgently needed. In this paper, we present the first fingerprinting scheme for the Intellectual Property (IP) protection of GANs. We break through the stealthiness and robustness bottlenecks suffered by previous fingerprinting methods for classification models being naively transferred to GANs. Specifically, we innovatively construct a composite deep learning model from the target GAN and a classifier. Then we generate fingerprint samples from this composite model, and embed them in the classifier for effective ownership verification. This scheme inspires some concrete methodologies to practically protect the modern GAN models. Theoretical analysis proves that these methods can satisfy different security requirements necessary for IP protection. We also conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#39640;&#26031;&#22122;&#22768;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#19982;&#20854;&#20182;&#24378;&#22823;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#22686;&#24378;&#22522;&#30784;&#30456;&#27604;&#65292;&#21487;&#20197;&#23558;&#40065;&#26834;&#24615;&#25552;&#39640;4.2-18.4&#65285;&#20197;&#24212;&#23545;&#26410;&#39044;&#35265;&#30340;&#22122;&#22768;&#27745;&#26579;&#12290;</title><link>http://arxiv.org/abs/2104.01231</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#39640;&#26031;&#22122;&#22768;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#29992;&#20110;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Diverse Gaussian Noise Consistency Regularization for Robustness and Uncertainty Calibration. (arXiv:2104.01231v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.01231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#39640;&#26031;&#22122;&#22768;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#19982;&#20854;&#20182;&#24378;&#22823;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#22686;&#24378;&#22522;&#30784;&#30456;&#27604;&#65292;&#21487;&#20197;&#23558;&#40065;&#26834;&#24615;&#25552;&#39640;4.2-18.4&#65285;&#20197;&#24212;&#23545;&#26410;&#39044;&#35265;&#30340;&#22122;&#22768;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19968;&#33268;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#39640;&#27700;&#24179;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;&#21508;&#31181;&#31867;&#22411;&#30340;&#25439;&#22351;&#20250;&#23548;&#33268;&#34920;&#29616;&#20005;&#37325;&#19979;&#38477;&#65292;&#36825;&#19982;&#39044;&#26399;&#30340;&#24773;&#20917;&#26377;&#25152;&#20559;&#24046;&#12290;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#21487;&#20197;&#22312;&#20986;&#29616;&#26410;&#39044;&#35265;&#21040;&#30340;&#39046;&#22495;&#20559;&#31227;&#26102;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#22270;&#20687;&#33719;&#21462;&#38454;&#27573;&#65292;&#25968;&#23383;&#22122;&#22768;&#27745;&#26579;&#32463;&#24120;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#39640;&#26031;&#22122;&#22768;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22270;&#20687;&#20998;&#31867;&#22120;&#22312;&#21508;&#31181;&#27745;&#26579;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#26412;&#22320;&#25439;&#22833;&#26223;&#35266;&#20998;&#26512;&#65292;&#25105;&#20204;&#23548;&#20986;&#30028;&#38480;&#65292;&#20197;&#28608;&#21169;&#21644;&#29702;&#35299;&#25105;&#20204;&#30340;&#39640;&#26031;&#22122;&#22768;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#30340;&#34892;&#20026;&#12290;&#30456;&#27604;&#20110;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#20854;&#20182;&#24378;&#22823;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#22686;&#24378;&#22522;&#30784;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#40065;&#26834;&#24615;&#25552;&#39640;4.2-18.4&#65285;&#20197;&#24212;&#23545;&#26410;&#39044;&#35265;&#30340;&#22122;&#22768;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks achieve high prediction accuracy when the train and test distributions coincide. In practice though, various types of corruptions occur which deviate from this setup and cause severe performance degradations. Few methods have been proposed to address generalization in the presence of unforeseen domain shifts. In particular, digital noise corruptions arise commonly in practice during the image acquisition stage and present a significant challenge for current methods. In this paper, we propose a diverse Gaussian noise consistency regularization method for improving robustness of image classifiers under a variety of corruptions while still maintaining high clean accuracy. We derive bounds to motivate and understand the behavior of our Gaussian noise consistency regularization using a local loss landscape analysis. Our approach improves robustness against unforeseen noise corruptions by 4.2-18.4% over adversarial training and other strong diverse data augmentation base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31574;&#30053;, &#19968;&#31181;&#26159;&#36880;&#27493;&#34928;&#20943;&#26412;&#22320;&#26799;&#24230;&#26435;&#37325;&#30340;&#34928;&#20943;&#27169;&#24335;, &#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#20195;&#20215;&#26368;&#23567;&#21270;&#35774;&#35745;&#30340;&#20849;&#35782;&#31639;&#27861;&#26469;&#20943;&#23569;&#27169;&#22411;&#20043;&#38388;&#30340;&#36890;&#20449;&#37327;, &#26377;&#25928;&#35299;&#20915;&#20102;&#29420;&#31435;&#23398;&#20064;&#29615;&#22659;&#24322;&#36136;&#24615;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#24320;&#38144;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2103.13026</link><description>&lt;p&gt;
&#26377;&#25928;&#36890;&#20449;&#19979;&#32852;&#37030;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26799;&#24230;&#25910;&#25947;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
The Gradient Convergence Bound of Federated Multi-Agent Reinforcement Learning with Efficient Communication. (arXiv:2103.13026v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.13026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31574;&#30053;, &#19968;&#31181;&#26159;&#36880;&#27493;&#34928;&#20943;&#26412;&#22320;&#26799;&#24230;&#26435;&#37325;&#30340;&#34928;&#20943;&#27169;&#24335;, &#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#20195;&#20215;&#26368;&#23567;&#21270;&#35774;&#35745;&#30340;&#20849;&#35782;&#31639;&#27861;&#26469;&#20943;&#23569;&#27169;&#22411;&#20043;&#38388;&#30340;&#36890;&#20449;&#37327;, &#26377;&#25928;&#35299;&#20915;&#20102;&#29420;&#31435;&#23398;&#20064;&#29615;&#22659;&#24322;&#36136;&#24615;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#24320;&#38144;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#20013;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20915;&#31574;&#30340;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#26159;&#30001;&#20110;&#29420;&#31435;&#23398;&#20064;&#29615;&#22659;&#24322;&#36136;&#24615;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#24320;&#38144;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#25910;&#25947;&#38382;&#39064;&#12290;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31574;&#30053;&#65292;&#19968;&#31181;&#26159;&#36880;&#27493;&#34928;&#20943;&#26412;&#22320;&#26799;&#24230;&#26435;&#37325;&#30340;&#34928;&#20943;&#27169;&#24335;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#20195;&#20215;&#26368;&#23567;&#21270;&#35774;&#35745;&#30340;&#20849;&#35782;&#31639;&#27861;&#26469;&#20943;&#23569;&#27169;&#22411;&#20043;&#38388;&#30340;&#36890;&#20449;&#37327;&#65292;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers independent reinforcement learning (IRL) for multi-agent collaborative decision-making in the paradigm of federated learning (FL). However, FL generates excessive communication overheads between agents and a remote central server, especially when it involves a large number of agents or iterations. Besides, due to the heterogeneity of independent learning environments, multiple agents may undergo asynchronous Markov decision processes (MDPs), which will affect the training samples and the model's convergence performance. On top of the variation-aware periodic averaging (VPA) method and the policy-based deep reinforcement learning (DRL) algorithm (i.e., proximal policy optimization (PPO)), this paper proposes two advanced optimization schemes orienting to stochastic gradient descent (SGD): 1) A decay-based scheme gradually decays the weights of a model's local gradients with the progress of successive local updates, and 2) By representing the agents as a graph, a cons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#25991;&#29486;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#22522;&#20110;&#21407;&#21017;&#19988;&#39046;&#22495;&#26080;&#20851;&#30340;&#25968;&#25454;&#24322;&#24120;&#20998;&#31867;&#27861;&#65292;&#24182;&#21576;&#29616;&#20102;&#24322;&#24120;&#31867;&#22411;&#21644;&#23376;&#31867;&#22411;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2007.15634</link><description>&lt;p&gt;
&#20851;&#20110;&#24322;&#24120;&#30340;&#24615;&#36136;&#21644;&#31867;&#22411;&#65306;&#25968;&#25454;&#20559;&#24046;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
On the Nature and Types of Anomalies: A Review of Deviations in Data. (arXiv:2007.15634v5 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.15634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#25991;&#29486;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#22522;&#20110;&#21407;&#21017;&#19988;&#39046;&#22495;&#26080;&#20851;&#30340;&#25968;&#25454;&#24322;&#24120;&#20998;&#31867;&#27861;&#65292;&#24182;&#21576;&#29616;&#20102;&#24322;&#24120;&#31867;&#22411;&#21644;&#23376;&#31867;&#22411;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26159;&#25351;&#25968;&#25454;&#38598;&#20013;&#20197;&#26576;&#31181;&#26041;&#24335;&#19981;&#23547;&#24120;&#19988;&#19981;&#31526;&#21512;&#19968;&#33324;&#27169;&#24335;&#30340;&#20107;&#20214;&#12290;&#24322;&#24120;&#30340;&#27010;&#24565;&#36890;&#24120;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#27169;&#31946;&#30340;&#21644;&#20381;&#36182;&#20110;&#20855;&#20307;&#39046;&#22495;&#30340;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24050;&#26377;&#32422;250&#24180;&#30340;&#20986;&#29256;&#29289;&#65292;&#20294;&#36804;&#20170;&#23578;&#26410;&#21457;&#34920;&#36807;&#20851;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#24322;&#24120;&#30340;&#32508;&#21512;&#21644;&#20855;&#20307;&#27010;&#36848;&#12290;&#36890;&#36807;&#24191;&#27867;&#25991;&#29486;&#32508;&#36848;&#65292;&#26412;&#30740;&#31350;&#22240;&#27492;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#22522;&#20110;&#21407;&#21017;&#19988;&#39046;&#22495;&#26080;&#20851;&#30340;&#25968;&#25454;&#24322;&#24120;&#20998;&#31867;&#27861;&#65292;&#24182;&#21576;&#29616;&#20102;&#24322;&#24120;&#31867;&#22411;&#21644;&#23376;&#31867;&#22411;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20026;&#20102;&#26126;&#30830;&#23450;&#20041;&#24322;&#24120;&#30340;&#27010;&#24565;&#21450;&#20854;&#19981;&#21516;&#30340;&#34920;&#29616;&#65292;&#35813;&#20998;&#31867;&#27861;&#36816;&#29992;&#20102;&#20116;&#20010;&#32500;&#24230;&#65306;&#25968;&#25454;&#31867;&#22411;&#12289;&#20851;&#31995;&#30340;&#22522;&#25968;&#12289;&#24322;&#24120;&#32423;&#21035;&#12289;&#25968;&#25454;&#32467;&#26500;&#21644;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#20123;&#22522;&#26412;&#19988;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#32500;&#24230;&#33258;&#28982;&#22320;&#20135;&#29983;&#20102;3&#20010;&#22823;&#32452;&#12289;9&#31181;&#22522;&#26412;&#31867;&#22411;&#21644;63&#20010;&#24322;&#24120;&#23376;&#31867;&#22411;&#12290;&#35813;&#20998;&#31867;&#27861;&#26377;&#21161;&#20110;&#35780;&#20272;&#26576;&#20010;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#23545;&#24322;&#24120;&#30340;&#21151;&#33021;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomalies are occurrences in a dataset that are in some way unusual and do not fit the general patterns. The concept of the anomaly is typically ill-defined and perceived as vague and domain-dependent. Moreover, despite some 250 years of publications on the topic, no comprehensive and concrete overviews of the different types of anomalies have hitherto been published. By means of an extensive literature review this study therefore offers the first theoretically principled and domain-independent typology of data anomalies and presents a full overview of anomaly types and subtypes. To concretely define the concept of the anomaly and its different manifestations, the typology employs five dimensions: data type, cardinality of relationship, anomaly level, data structure, and data distribution. These fundamental and data-centric dimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of anomalies. The typology facilitates the evaluation of the functional capabilities of an
&lt;/p&gt;</description></item></channel></rss>