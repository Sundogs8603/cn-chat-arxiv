<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01454</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#19968;&#31181;&#32479;&#35745;&#22240;&#26524;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#65288;SCD&#65289;&#20013;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#20316;&#20026;&#32422;&#26463;&#23884;&#20837;&#21040;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#25509;&#21463;&#65292;&#22240;&#20026;&#36825;&#23545;&#20110;&#21019;&#24314;&#19968;&#33268;&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#27169;&#22411;&#26159;&#37325;&#35201;&#30340;&#65292;&#23613;&#31649;&#35782;&#21035;&#32972;&#26223;&#30693;&#35782;&#30340;&#25361;&#25112;&#34987;&#35748;&#21487;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;LLM&#30340;&#8220;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#65288;SCP&#65289;&#8221;&#19982;SCD&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#22240;&#26524;&#25512;&#26029;&#65288;KBCI&#65289;&#30456;&#32467;&#21512;&#65292;&#23545;SCD&#36827;&#34892;&#20808;&#39564;&#30693;&#35782;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4&#21487;&#20197;&#20351;LLM-KBCI&#30340;&#36755;&#20986;&#19982;&#24102;&#26377;LLM-KBCI&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;SCD&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#65292;&#22914;&#26524;GPT-4&#32463;&#21382;&#20102;SCP&#65292;&#37027;&#20040;SCD&#30340;&#32467;&#26524;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;LLM&#19981;&#21547;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;LLM&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20854;&#32972;&#26223;&#30693;&#35782;&#26469;&#25913;&#36827;SCD&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19888</link><description>&lt;p&gt;
MambaMixer&#65306;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#21644;&#36890;&#36947;&#36873;&#25321;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19888
&lt;/p&gt;
&lt;p&gt;
MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;Transformers&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#24615;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#22823;&#35268;&#27169;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#23637;&#29616;&#20986;&#36755;&#20837;&#22823;&#23567;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#23581;&#35797;&#20026;&#22810;&#32500;&#25968;&#25454;&#35774;&#35745;&#39640;&#25928;&#26377;&#25928;&#30340;&#26550;&#26500;&#20027;&#24178;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#26159;&#25968;&#25454;&#29420;&#31435;&#30340;&#65292;&#35201;&#20040;&#26080;&#27861;&#20801;&#35768;&#36328;&#32500;&#24230;&#21644;&#20869;&#37096;&#32500;&#24230;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#23454;&#29616;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#20102;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#21463;&#21040;SSMs&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MambaMixer&#65292;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#36328;&#26631;&#35760;&#21644;&#36890;&#36947;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;</title><link>https://arxiv.org/abs/2403.19871</link><description>&lt;p&gt;
&#36890;&#36807;&#32531;&#24930;&#21464;&#21270;&#30340;&#24207;&#21015;&#23454;&#29616;&#31283;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19871
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#36138;&#23146;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#32771;&#34385;&#36890;&#36807;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#28436;&#21464;&#26469;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36890;&#36807;&#19981;&#21516;&#30340;&#25968;&#25454;&#25209;&#27425;&#26356;&#26032;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20445;&#30041;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265; - &#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#23454;&#26045;&#31616;&#26131;&#24615;&#21644;&#19982;&#29992;&#25143;&#24314;&#31435;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201; - &#36890;&#36807;&#20351;&#29992;&#21487;&#20197;&#30452;&#25509;&#32435;&#20837;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#23450;&#20041;&#23450;&#20041;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#29983;&#20135;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#27604;&#36138;&#23146;&#35757;&#32451;&#27169;&#22411;&#26356;&#24378;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19871v1 Announce Type: cross  Abstract: Retraining machine learning models remains an important task for real-world machine learning model deployment. Existing methods focus largely on greedy approaches to find the best-performing model without considering the stability of trained model structures across different retraining evolutions. In this study, we develop a mixed integer optimization algorithm that holistically considers the problem of retraining machine learning models across different data batch updates. Our method focuses on retaining consistent analytical insights - which is important to model interpretability, ease of implementation, and fostering trust with users - by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world production case study. Finally, important an
&lt;/p&gt;</description></item><item><title>MineLand&#27169;&#25311;&#22120;&#24341;&#20837;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#65292;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#22312;&#21327;&#20316;&#20013;&#22635;&#34917;&#20102;&#20449;&#24687;&#21644;&#21151;&#33021;&#38480;&#21046;&#30340;&#31354;&#30333;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#20855;&#21160;&#24577;&#21644;&#26377;&#25928;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2403.19267</link><description>&lt;p&gt;
MineLand&#65306;&#27169;&#25311;&#20855;&#26377;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#30340;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19267
&lt;/p&gt;
&lt;p&gt;
MineLand&#27169;&#25311;&#22120;&#24341;&#20837;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#65292;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#22312;&#21327;&#20316;&#20013;&#22635;&#34917;&#20102;&#20449;&#24687;&#21644;&#21151;&#33021;&#38480;&#21046;&#30340;&#31354;&#30333;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#20855;&#21160;&#24577;&#21644;&#26377;&#25928;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#20223;&#30495;&#22120;&#36890;&#24120;&#20551;&#35774;&#25317;&#26377;&#23436;&#32654;&#20449;&#24687;&#21644;&#26080;&#38480;&#21151;&#33021;&#65292;&#36825;&#38480;&#21046;&#20102;&#31038;&#20250;&#20114;&#21160;&#30340;&#29983;&#24577;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;Minecraft&#27169;&#25311;&#22120;MineLand&#65292;&#36890;&#36807;&#24341;&#20837;&#26377;&#38480;&#30340;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#20223;&#30495;&#22120;&#25903;&#25345;&#26368;&#22810;48&#20010;&#20855;&#26377;&#26377;&#38480;&#35270;&#35273;&#12289;&#21548;&#35273;&#21644;&#29615;&#22659;&#24847;&#35782;&#30340;&#26234;&#33021;&#20307;&#65292;&#36843;&#20351;&#23427;&#20204;&#31215;&#26497;&#27807;&#36890;&#21644;&#21327;&#20316;&#20197;&#28385;&#36275;&#39135;&#29289;&#21644;&#36164;&#28304;&#31561;&#29983;&#29702;&#38656;&#27714;&#12290;&#36825;&#20419;&#36827;&#20102;&#21160;&#24577;&#21644;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#20010;&#28789;&#24863;&#26469;&#33258;&#22810;&#20219;&#21153;&#22788;&#29702;&#29702;&#35770;&#30340;AI&#26234;&#33021;&#20307;&#26694;&#26550;Alex&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21327;&#35843;&#21644;&#35843;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#25311;&#22120;&#12289;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;AI&#26234;&#33021;&#20307;&#26694;&#26550;&#26377;&#21161;&#20110;&#26356;&#20855;&#29983;&#24577;&#21644;&#32454;&#33268;&#30340;&#38598;&#20307;&#34892;&#20026;&#12290;MineLand&#21644;Alex&#30340;&#28304;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/c&#20013;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19267v1 Announce Type: cross  Abstract: Conventional multi-agent simulators often assume perfect information and limitless capabilities, hindering the ecological validity of social interactions. We propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing limited multimodal senses and physical needs. Our simulator supports up to 48 agents with limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. This fosters dynamic and valid multi-agent interactions. We further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior. The source code of MineLand and Alex is openly available at https://github.com/c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19135</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21270;&#19981;&#37325;&#35201;&#30340;&#23618;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compressing Large Language Models by Streamlining the Unimportant Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#20294;&#20854;&#36866;&#29992;&#24615;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#30340;&#32039;&#20945;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#30340;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#25200;&#21160;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#23618;&#12290;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Streamline&#65292;&#21253;&#25324;&#20004;&#37096;&#20998;&#65306;&#23618;&#21098;&#26525;&#65292;&#26681;&#25454;&#30446;&#26631;&#31232;&#30095;&#24230;&#31227;&#38500;&#27169;&#22411;&#20013;&#19968;&#32452;&#36830;&#32493;&#30340;&#26368;&#19981;&#37325;&#35201;&#30340;&#23618;&#65307;&#23618;&#26367;&#25442;&#65292;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#26367;&#25442;&#34987;&#21098;&#26525;&#30340;&#23618;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#21098;&#26525;&#36896;&#25104;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#21644;&#19968;&#20010;transformer&#23618;&#31561;&#32467;&#26500;&#20316;&#20026;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19135v1 Announce Type: cross  Abstract: Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17601</link><description>&lt;p&gt;
LASIL&#65306;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#38271;&#26399;&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17601
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#22312;&#20132;&#36890;&#24037;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#21333;&#20010;&#36710;&#36742;&#34892;&#20026;&#21644;&#25972;&#20307;&#20132;&#36890;&#27969;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#19968;&#20010;&#30495;&#23454;&#30340;&#27169;&#25311;&#22120;&#65292;&#31934;&#30830;&#22797;&#21046;&#21508;&#31181;&#20132;&#36890;&#26465;&#20214;&#19979;&#30340;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#20381;&#36182;&#21551;&#21457;&#24335;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#24448;&#24448;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20132;&#36890;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#32780;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#30340;&#27169;&#25311;&#12290;&#30001;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#31283;&#23450;&#30340;&#38271;&#26399;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#20998;&#24067;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#19987;&#23478;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#22686;&#24378;&#29366;&#24577;&#24847;&#35782;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17601v1 Announce Type: new  Abstract: Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17456</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Imitating Cost-Constrained Behaviors in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#22797;&#26434;&#30340;&#35745;&#21010;&#21644;&#35843;&#24230;&#38382;&#39064;&#19968;&#30452;&#36890;&#36807;&#21508;&#31181;&#20248;&#21270;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#30340;&#27169;&#20223;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#21487;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#30340;&#34892;&#20026;&#26469;&#23398;&#20064;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#27169;&#22411;&#25110;&#30452;&#25509;&#34892;&#20026;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26080;&#38480;&#21046;&#35774;&#32622;&#19979;&#30340;&#27169;&#20223;&#65288;&#20363;&#22914;&#65292;&#36710;&#36742;&#28040;&#32791;&#30340;&#29123;&#27833;&#37327;&#27809;&#26377;&#38480;&#21046;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#19987;&#23478;&#30340;&#34892;&#20026;&#19981;&#20165;&#21463;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#30340;&#24433;&#21709;&#65292;&#36824;&#21463;&#32422;&#26463;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#33258;&#21160;&#39550;&#39542;&#36865;&#36135;&#36710;&#30340;&#20915;&#31574;&#19981;&#20165;&#21462;&#20915;&#20110;&#36335;&#24452;&#20559;&#22909;/&#22870;&#21169;&#65288;&#26681;&#25454;&#36807;&#21435;&#30340;&#38656;&#27714;&#25968;&#25454;&#65289;&#65292;&#36824;&#21462;&#20915;&#20110;&#36710;&#36742;&#20869;&#30340;&#29123;&#27833;&#21644;&#36865;&#36798;&#26102;&#38388;&#31561;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti
&lt;/p&gt;</description></item><item><title>&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15855</link><description>&lt;p&gt;
&#21021;&#22987;&#20540;&#21644;&#25299;&#25169;&#32467;&#26500;&#22312;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Initialisation and Topology Effects in Decentralised Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15855
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#23436;&#20840;&#20998;&#25955;&#24335;&#29305;&#24449;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22312;&#32593;&#32476;&#19978;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#23545;&#20010;&#20307;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#26412;&#22320;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#25968;&#25454;&#38544;&#31169;&#24615;&#65292;&#28040;&#38500;&#20102;&#21333;&#28857;&#25925;&#38556;&#21644;&#20013;&#22830;&#21327;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#19968;&#20010;&#31616;&#21270;&#30340;&#25968;&#20540;&#27169;&#22411;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#31995;&#32479;&#30340;&#26089;&#26399;&#34892;&#20026;&#65292;&#20351;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#20540;&#31574;&#30053;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#19979;&#30340;&#27604;&#20363;&#34892;&#20026;&#21644;&#29615;&#22659;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26356;&#22810;&#30740;&#31350;&#25171;&#24320;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15855v1 Announce Type: cross  Abstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for mor
&lt;/p&gt;</description></item><item><title>PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15388</link><description>&lt;p&gt;
LLaVA-PruMerge: &#33258;&#36866;&#24212;&#20196;&#29260;&#20943;&#23569;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15388
&lt;/p&gt;
&lt;p&gt;
PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;LMMs&#21253;&#25324;&#20102;&#26356;&#22797;&#26434;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22914;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#20196;&#29260;&#20943;&#23569;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#35768;&#22810;&#35270;&#35273;&#20196;&#29260;&#22312;&#31354;&#38388;&#19978;&#26159;&#20887;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PruMerge&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13101</link><description>&lt;p&gt;
AdaptSFL&#65306;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13101
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26085;&#30410;&#22797;&#26434;&#20351;&#24471;&#23558;&#20854;&#27665;&#20027;&#21270;&#21040;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#27169;&#22411;&#20998;&#21306;&#23558;&#20027;&#35201;&#35757;&#32451;&#24037;&#20316;&#36127;&#33655;&#36716;&#31227;&#21040;&#26381;&#21153;&#22120;&#19978;&#65292;&#24182;&#22312;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#23454;&#29616;&#24182;&#34892;&#35757;&#32451;&#30340;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31995;&#32479;&#20248;&#21270;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#36164;&#28304;&#21463;&#38480;&#31995;&#32479;&#19979;SFL&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;SFL&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#20998;&#21106;&#65288;MS&#65289;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#32858;&#21512;&#65288;MA&#65289;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20316;&#20026;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaptSFL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#33258;&#36866;&#24212;SFL&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#19979;&#30340;SFL&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AdaptSFL&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#23458;&#25143;&#31471;MA&#21644;MS&#65292;&#20197;&#24179;&#34913;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13101v1 Announce Type: new  Abstract: The increasing complexity of deep neural networks poses significant barriers to democratizing them to resource-limited edge devices. To address this challenge, split federated learning (SFL) has emerged as a promising solution by of floading the primary training workload to a server via model partitioning while enabling parallel training among edge devices. However, although system optimization substantially influences the performance of SFL under resource-constrained systems, the problem remains largely uncharted. In this paper, we provide a convergence analysis of SFL which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on the learning performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a novel resource-adaptive SFL framework, to expedite SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls client-side MA and MS to balance commun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13002</link><description>&lt;p&gt;
AutoTRIZ&#65306;&#21033;&#29992;TRIZ&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#21019;&#24847;
&lt;/p&gt;
&lt;p&gt;
AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#21019;&#26032;&#32773;&#22312;&#24320;&#21457;&#24605;&#32500;&#26041;&#27861;&#26041;&#38754;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#27604;&#22914;&#24418;&#24577;&#20998;&#26512;&#21644;&#31867;&#27604;&#35774;&#35745;&#65292;&#20197;&#36741;&#21161;&#24037;&#31243;&#35774;&#35745;&#21019;&#24847;&#65292;&#35299;&#20915;&#38382;&#39064;&#21644;&#25512;&#21160;&#21019;&#26032;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;TRIZ&#20316;&#20026;&#26368;&#33879;&#21517;&#30340;&#26041;&#27861;&#33073;&#39062;&#32780;&#20986;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31995;&#32479;&#21270;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;TRIZ&#36164;&#28304;&#21644;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20854;&#23545;&#29992;&#25143;&#30693;&#35782;&#12289;&#32463;&#39564;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20381;&#36182;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#24191;&#27867;&#30693;&#35782;&#21644;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;AutoTRIZ&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30683;&#30462;&#26816;&#27979;&#21644;&#27604;&#36739;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#23454;&#39564;&#26469;&#35777;&#26126;&#24182;&#35780;&#20272;AutoTRIZ&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13002v1 Announce Type: cross  Abstract: Researchers and innovators have made enormous efforts in developing ideation methods, such as morphological analysis and design-by-analogy, to aid engineering design ideation for problem solving and innovation. Among these, TRIZ stands out as the most well-known approach, widely applied for systematic innovation. However, the complexity of TRIZ resources and concepts, coupled with its reliance on users' knowledge, experience, and reasoning capabilities, limits its practicability. This paper proposes AutoTRIZ, an artificial ideation tool that leverages large language models (LLMs) to automate and enhance the TRIZ methodology. By leveraging the broad knowledge and advanced reasoning capabilities of LLMs, AutoTRIZ offers a novel approach to design automation and interpretable ideation with artificial intelligence. We demonstrate and evaluate the effectiveness of AutoTRIZ through consistency experiments in contradiction detection and compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#20419;&#36827;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#30340;&#21512;&#20316;&#65292;&#22312;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#23454;&#39564;&#20013;&#21457;&#29616;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.12482</link><description>&lt;p&gt;
&#20855;&#36523;LLM&#20195;&#29702;&#22312;&#32452;&#32455;&#22242;&#38431;&#20013;&#23398;&#20250;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Embodied LLM Agents Learn to Cooperate in Organized Teams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#20419;&#36827;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#30340;&#21512;&#20316;&#65292;&#22312;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#23454;&#39564;&#20013;&#21457;&#29616;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#25512;&#29702;&#12289;&#35268;&#21010;&#21644;&#20915;&#31574;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#30340;&#29087;&#32451;&#24230;&#12290;LLMs&#22240;&#27492;&#22312;&#22810;&#20195;&#29702;&#31995;&#32479;&#20869;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20197;&#20419;&#36827;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;LLM&#20195;&#29702;&#24448;&#24448;&#20250;&#36807;&#24230;&#25253;&#21578;&#24182;&#36981;&#20174;&#20219;&#20309;&#25351;&#20196;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22810;&#20195;&#29702;&#21512;&#20316;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#28151;&#20081;&#12290;&#21463;&#20154;&#31867;&#32452;&#32455;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21363;&#26102;&#24615;&#32452;&#32455;&#32467;&#26500;&#24378;&#21152;&#22312;LLM&#20195;&#29702;&#19978;&#65292;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20855;&#36523;LLM&#20195;&#29702;&#21644;&#20154;-&#20195;&#29702;&#21512;&#20316;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#25351;&#23450;&#39046;&#23548;&#23545;&#22242;&#38431;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#23637;&#31034;&#30340;&#39046;&#23548;&#32032;&#36136;&#21644;&#20182;&#20204;&#30340;&#33258;&#21457;&#21512;&#20316;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12482v1 Announce Type: new  Abstract: Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#21644;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#36125;&#21494;&#26031;&#32593;&#32476;&#19982;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#36827;&#20020;&#24202;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.09481</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#34920;&#26684;&#25968;&#25454;&#21644;&#25991;&#26412;&#30340;&#20020;&#24202;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Clinical Reasoning over Tabular Data and Text with Bayesian Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#21644;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#36125;&#21494;&#26031;&#32593;&#32476;&#19982;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#36827;&#20020;&#24202;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#65292;&#20294;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#26469;&#35828;&#21364;&#19981;&#22815;&#20860;&#23481;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#21017;&#20026;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#27604;&#36739;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#20197;&#29983;&#25104;&#24615;&#21644;&#21028;&#21035;&#24615;&#26041;&#24335;&#22686;&#24378;&#36125;&#21494;&#26031;&#32593;&#32476;&#19982;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#65292;&#32467;&#21512;&#27169;&#25311;&#32467;&#26524;&#20197;&#19968;&#20010;&#22522;&#30784;&#21307;&#30103;&#26696;&#20363;&#65288;&#32954;&#28814;&#35786;&#26029;&#65289;&#36827;&#34892;&#35828;&#26126;&#65292;&#24182;&#22312;&#26356;&#24191;&#27867;&#30340;&#20020;&#24202;&#32972;&#26223;&#19979;&#36827;&#34892;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09481v1 Announce Type: new  Abstract: Bayesian networks are well-suited for clinical reasoning on tabular data, but are less compatible with natural language data, for which neural networks provide a successful framework. This paper compares and discusses strategies to augment Bayesian networks with neural text representations, both in a generative and discriminative manner. This is illustrated with simulation results for a primary care use case (diagnosis of pneumonia) and discussed in a broader clinical context.
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25552;&#31034;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#33391;&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#25552;&#31034;&#26041;&#27861;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08211</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#23545;&#27604;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Contrastive Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08211
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25552;&#31034;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#33391;&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#25552;&#31034;&#26041;&#27861;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26041;&#27861;&#22312;&#22686;&#24378;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#27604;&#25552;&#31034;&#65288;CP&#65289;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;LLMs&#25552;&#20379;&#31572;&#26696;&#20043;&#21069;&#28155;&#21152;"&#35753;&#25105;&#20204;&#32473;&#20986;&#19968;&#20010;&#27491;&#30830;&#31572;&#26696;&#21644;&#19968;&#20010;&#38169;&#35823;&#31572;&#26696;"&#26469;&#28436;&#31034;LLMs&#26159;&#20307;&#38754;&#30340;&#23545;&#27604;&#25512;&#29702;&#32773;&#12290;&#23545;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38646;&#36801;&#31227;&#23545;&#27604;&#25552;&#31034;&#25552;&#21319;&#20102;&#22312;&#19968;&#31995;&#21015;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;&#37327;&#36801;&#31227;&#31034;&#20363;&#65292;&#27604;&#22914;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;35.9%&#21040;88.8%&#20197;&#21450;AQUA-RAT&#20174;41.3%&#21040;62.2%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#22823;&#22810;&#25968;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#20013;&#32988;&#36807;&#38646;&#36801;&#31227;CoT&#21644;&#23569;&#37327;&#36801;&#31227;CoT&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#26080;&#32541;&#25972;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#25110;&#32773;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08211v1 Announce Type: cross  Abstract: Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding "Let's give a correct and a wrong answer." before LLMs provide answers. Experiments on two large language models show that zero-shot contrastive prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.07743</link><description>&lt;p&gt;
&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#37197;&#22791;&#24037;&#20214;&#22788;&#29702;&#27969;&#27700;&#32447;&#65306;&#35745;&#31639;&#19982;&#24615;&#33021;&#26435;&#34913;&#30340;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07743
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#26159;&#30284;&#30151;&#35786;&#26029;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#22312;&#26174;&#24494;&#38236;&#19979;&#36827;&#34892;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#32452;&#32455;&#30149;&#29702;&#23398;&#22788;&#29702;&#36807;&#31243;&#20250;&#20135;&#29983;&#19968;&#20123;&#24037;&#20214;&#65292;&#26368;&#32456;&#20250;&#36716;&#31227;&#21040;&#29627;&#29827;&#36733;&#29627;&#29255;&#30340;&#25968;&#23383;&#21270;&#29256;&#26412;&#65292;&#21363;&#20840;&#29627;&#24187;&#28783;&#29255;&#12290;&#24037;&#20214;&#26159;&#35786;&#26029;&#26080;&#20851;&#30340;&#21306;&#22495;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CPATH&#65289;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#24037;&#20214;&#23545;&#20110;&#21487;&#38752;&#30340;&#33258;&#21160;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26041;&#26696;&#65292;&#29992;&#20110;&#26816;&#27979;&#21253;&#25324;&#25439;&#22351;&#32452;&#32455;&#12289;&#27169;&#31946;&#12289;&#35126;&#30385;&#32452;&#32455;&#12289;&#27668;&#27873;&#21644;&#22312;WSIs&#20013;&#30340;&#32452;&#32455;&#23398;&#26080;&#20851;&#34880;&#28082;&#31561;&#20116;&#31181;&#26174;&#33879;&#24037;&#20214;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#29420;&#31435;&#30340;&#20108;&#20803;DL&#27169;&#22411;&#20316;&#20026;&#19987;&#23478;&#26469;&#25429;&#25417;&#29305;&#23450;&#30340;&#24037;&#20214;&#24418;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#34701;&#21512;&#26426;&#21046;&#26469;&#38598;&#25104;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23545;&#26368;&#32456;&#30340;&#27010;&#29575;&#36827;&#34892;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07743v1 Announce Type: cross  Abstract: Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#26576;&#20123;&#21464;&#37327;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21487;&#20197;&#20351;&#24471;&#19968;&#20123;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21464;&#24471;&#21487;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#21487;&#35782;&#21035;&#24615;&#30340;&#24773;&#20917;&#19979;&#25490;&#38500;&#35266;&#27979;&#21040;&#30340;&#21151;&#33021;&#24615;&#21464;&#37327;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.04919</link><description>&lt;p&gt;
&#37492;&#21035;&#21151;&#33021;&#20381;&#36182;&#19979;&#30340;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Identifying Causal Effects Under Functional Dependencies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#26576;&#20123;&#21464;&#37327;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21487;&#20197;&#20351;&#24471;&#19968;&#20123;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21464;&#24471;&#21487;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#21487;&#35782;&#21035;&#24615;&#30340;&#24773;&#20917;&#19979;&#25490;&#38500;&#35266;&#27979;&#21040;&#30340;&#21151;&#33021;&#24615;&#21464;&#37327;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#25928;&#24212;&#30340;&#35782;&#21035;&#65292;&#21463;&#20004;&#20010;&#25913;&#36827;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#22312;&#24050;&#30693;&#22240;&#26524;&#22270;&#20013;&#26576;&#20123;&#21464;&#37327;&#26159;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;&#31532;&#19968;&#65292;&#24403;&#26576;&#20123;&#21464;&#37327;&#26159;&#21151;&#33021;&#30340;&#26102;&#65292;&#19968;&#20010;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21487;&#33021;&#21464;&#24471;&#21487;&#35782;&#21035;&#12290;&#31532;&#20108;&#65292;&#21487;&#20197;&#25490;&#38500;&#35266;&#27979;&#26576;&#20123;&#21151;&#33021;&#21464;&#37327;&#32780;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#22522;&#20110;&#19968;&#20010;&#25490;&#38500;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#20174;&#22240;&#26524;&#22270;&#20013;&#21024;&#38500;&#21151;&#33021;&#21464;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#32467;&#26524;&#22240;&#26524;&#22270;&#20013;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#21253;&#25324;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04919v1 Announce Type: new  Abstract: We study the identification of causal effects, motivated by two improvements to identifiability which can be attained if one knows that some variables in a causal graph are functionally determined by their parents (without needing to know the specific functions). First, an unidentifiable causal effect may become identifiable when certain variables are functional. Second, certain functional variables can be excluded from being observed without affecting the identifiability of a causal effect, which may significantly reduce the number of needed variables in observational data. Our results are largely based on an elimination procedure which removes functional variables from a causal graph while preserving key properties in the resulting causal graph, including the identifiability of causal effects.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.03506</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#28041;&#21450;&#24102;&#26377;&#26377;&#38480;&#36793;&#30028;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30740;&#31350;&#24212;&#35206;&#30422;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#19981;&#21516;&#31867;&#22411;&#28151;&#21512;&#25991;&#26412;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;CoAuthor&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36890;&#36807;&#20154;&#31867;&#20316;&#32773;&#21644;&#26234;&#33021;&#20889;&#20316;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#29983;&#25104;&#30340;&#22810;&#36718;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#22810;&#26679;&#21270;&#12289;&#30495;&#23454;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#27493;&#20998;&#21106;&#20026;&#22522;&#30784;&#30340;&#27969;&#31243;&#65306;(i)&#26816;&#27979;&#32473;&#23450;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#21508;&#20010;&#27573;&#33853;&#65292;&#20854;&#20013;&#27599;&#20010;&#27573;&#33853;&#21253;&#21547;&#19968;&#33268;&#20316;&#32773;&#30340;&#21477;&#23376;&#65292;&#20197;&#21450;(ii)&#20998;&#31867;&#27599;&#20010;&#30830;&#23450;&#27573;&#33853;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03506v1 Announce Type: cross  Abstract: This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20026;5G&#21644;&#19979;&#19968;&#20195;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#24320;&#21457;&#33258;&#23450;&#20041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#32593;&#32476;&#20013;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2403.02238</link><description>&lt;p&gt;
&#26397;&#21521;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#31649;&#29702;&#65306;5G&#26680;&#24515;&#32593;&#32476;&#20013;&#29992;&#20110;&#24847;&#22270;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02238
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20026;5G&#21644;&#19979;&#19968;&#20195;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#24320;&#21457;&#33258;&#23450;&#20041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#32593;&#32476;&#20013;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;ML/AI&#65289;&#19982;&#31532;&#20116;&#20195;&#65288;5G&#65289;&#32593;&#32476;&#30340;&#25972;&#21512;&#65292;&#20984;&#26174;&#20986;&#20102;&#32593;&#32476;&#26234;&#33021;&#19982;&#24403;&#21069;&#21644;&#19979;&#19968;&#20195;&#35774;&#22791;&#23545;&#36234;&#26469;&#36234;&#20005;&#26684;&#30340;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290; &#36825;&#31181;&#21521;&#26222;&#36941;&#26234;&#33021;&#30340;&#36807;&#28193;&#35201;&#27714;&#29992;&#25143;&#21644;&#32593;&#32476;&#36816;&#33829;&#21830;&#20043;&#38388;&#39640;&#24230;&#30340;&#36830;&#25509;&#24615;&#12289;&#21516;&#27493;&#24615;&#21644;&#31471;&#21040;&#31471;&#30340;&#36890;&#20449;&#65292;&#24182;&#23558;&#20026;&#23454;&#29616;&#23436;&#20840;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#20840;&#33258;&#21160;&#32593;&#32476;&#38138;&#24179;&#36947;&#36335;&#12290;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#26159;&#20943;&#23569;&#20154;&#31867;&#34892;&#20026;&#12289;&#35282;&#33394;&#21644;&#36131;&#20219;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21516;&#26102;&#36716;&#21521;&#33258;&#21160;&#21270;&#32593;&#32476;&#31649;&#29702;&#30340;&#26032;&#22411;&#25552;&#21462;&#21644;&#35299;&#37322;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#20026;5G&#21644;&#19979;&#19968;&#20195;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#24320;&#21457;&#33258;&#23450;&#20041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;LLM&#21457;&#23637;&#21644;&#38598;&#25104;&#30340;&#35265;&#35299;&#65292;&#20197;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#32593;&#32476;&#20013;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02238v1 Announce Type: cross  Abstract: The integration of Machine Learning and Artificial Intelligence (ML/AI) into fifth-generation (5G) networks has made evident the limitations of network intelligence with ever-increasing, strenuous requirements for current and next-generation devices. This transition to ubiquitous intelligence demands high connectivity, synchronicity, and end-to-end communication between users and network operators, and will pave the way towards full network automation without human intervention. Intent-based networking is a key factor in the reduction of human actions, roles, and responsibilities while shifting towards novel extraction and interpretation of automated network management. This paper presents the development of a custom Large Language Model (LLM) for 5G and next-generation intent-based networking and provides insights into future LLM developments and integrations to realize end-to-end intent-based networking for fully automated network in
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.17826</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Prediction-Powered Ranking of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#26681;&#25454;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#36827;&#34892;&#25490;&#21517;--&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#21463;&#20154;&#31867;&#20559;&#22909;&#65292;&#37027;&#20040;&#23427;&#23601;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#26469;&#24357;&#21512;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;</title><link>https://arxiv.org/abs/2402.17205</link><description>&lt;p&gt;
&#27979;&#37327;&#31070;&#32463;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;STEM&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Measuring Vision-Language STEM Skills of Neural Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#32467;&#21512;STEM&#65288;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#65289;&#30693;&#35782;&#26469;&#35299;&#20915;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#25361;&#25112;&#24615;&#38382;&#39064;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#23427;&#21253;&#25324;448&#39033;&#25216;&#33021;&#21644;1,073,146&#20010;&#36328;&#36234;&#25152;&#26377;STEM&#31185;&#30446;&#30340;&#38382;&#39064;&#12290;&#19982;&#36890;&#24120;&#20391;&#37325;&#20110;&#26816;&#39564;&#19987;&#23478;&#27700;&#24179;&#33021;&#21147;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#22522;&#30784;&#25216;&#33021;&#21644;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;GPT-3.5-Turbo&#65292;&#28155;&#21152;&#21040;&#25105;&#20204;&#30340;&#22522;&#20934;&#20013;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#36817;&#30340;&#27169;&#22411;&#36827;&#23637;&#21482;&#26377;&#21161;&#20110;&#25484;&#25569;&#25968;&#25454;&#38598;&#20013;&#38750;&#24120;&#26377;&#38480;&#25968;&#37327;&#30340;&#20302;&#24180;&#32423;&#25216;&#33021;&#65288;&#19977;&#24180;&#32423;&#20013;&#30340;2.5%&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#36828;&#27809;&#26377;&#23436;&#20840;&#25484;&#25569;&#23398;&#21069;&#25945;&#32946;&#38454;&#27573;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;Dirichlet Markov&#38142;&#21644;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#35299;&#20915;&#21407;&#26377;&#27169;&#22411;&#25429;&#25417;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.16297</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16297
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;Dirichlet Markov&#38142;&#21644;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#35299;&#20915;&#21407;&#26377;&#27169;&#22411;&#25429;&#25417;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#35745;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#22240;&#20854;&#33021;&#22815;&#25512;&#26029;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#32467;&#26500;&#21644;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#32780;&#22791;&#21463;&#37325;&#35270;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22788;&#29702;&#22024;&#26434;&#21644;&#19981;&#23436;&#25972;&#30340;&#35745;&#25968;&#25968;&#25454;&#12290;&#22312;&#36825;&#20123;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#65292;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65288;PGDSs&#65289;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#35266;&#23519;&#21040;&#30340;&#35745;&#25968;&#24207;&#21015;&#24213;&#23618;&#21160;&#24577;&#30340;&#28436;&#21464;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#30340;PGDS&#22312;&#25429;&#25417;&#24120;&#35265;&#20110;&#23454;&#38469;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;PGDS&#65292;&#20801;&#35768;&#22522;&#30784;&#36716;&#31227;&#30697;&#38453;&#38543;&#26102;&#38388;&#28436;&#21464;&#65292;&#28436;&#21464;&#30340;&#36716;&#31227;&#30697;&#38453;&#30001;&#31934;&#24515;&#35774;&#35745;&#30340;Dirichlet Markov&#38142;&#24314;&#27169;&#12290;&#21033;&#29992;Dirichlet-Multinomial-Beta&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#20849;&#36717;&#19988;&#39640;&#25928;&#30340;Gibbs&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16297v1 Announce Type: cross  Abstract: Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences. However, the state-of-the-art PGDS still falls short in capturing the time-varying transition dynamics that are commonly observed in real-world count time series. To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by sophisticatedly-designed Dirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed t
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.16075</link><description>&lt;p&gt;
&#22522;&#20110;&#25554;&#20540;&#30340;&#31574;&#30053;&#25193;&#25955;&#30340;&#34892;&#20026;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Behavioral Refinement via Interpolant-based Policy Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16075
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#36807;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26469;&#27169;&#20223;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#25317;&#26377;&#24314;&#27169;&#39640;&#32500;&#24230;&#21644;&#22810;&#27169;&#24577;&#20998;&#24067;&#33021;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23558;&#21160;&#20316;&#65288;&#25110;&#29366;&#24577;&#65289;&#20174;&#26631;&#20934;&#39640;&#26031;&#22122;&#22768;&#20013;&#25193;&#25955;&#26469;&#22609;&#36896;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35201;&#23398;&#20064;&#30340;&#30446;&#26631;&#31574;&#30053;&#36890;&#24120;&#19982;&#39640;&#26031;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#31181;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#22312;&#20351;&#29992;&#23569;&#37327;&#25193;&#25955;&#27493;&#39588;&#65288;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65289;&#21644;&#26377;&#38480;&#25968;&#25454;&#19979;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#20174;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#24320;&#22987;&#65292;&#21487;&#20197;&#20351;&#25193;&#25955;&#26041;&#27861;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#12289;&#19968;&#31181;&#26032;&#26041;&#27861;&#21644;&#23454;&#35777;&#21457;&#29616;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#28304;&#31574;&#30053;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;BRIDGER&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16075v1 Announce Type: cross  Abstract: Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochast
&lt;/p&gt;</description></item><item><title>ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15898</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Information-based Transductive Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15898
&lt;/p&gt;
&lt;p&gt;
ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20027;&#21160;&#23398;&#20064;&#25512;&#24191;&#21040;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#37319;&#26679;&#21463;&#38480;&#20110;&#21487;&#35775;&#38382;&#22495;&#30340;&#24773;&#20917;&#65292;&#32780;&#39044;&#27979;&#30446;&#26631;&#21487;&#33021;&#20301;&#20110;&#36825;&#20010;&#22495;&#20043;&#22806;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ITL&#65292;&#21363;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ITL&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26368;&#23567;&#21487;&#33021;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;ITL&#65306;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;ITL&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#36873;&#25321;&#36319;&#38543;&#31639;&#27861;&#26159;&#21542;&#20256;&#36798;&#26377;&#20851;&#20154;&#31867;&#33021;&#21147;&#30340;&#20449;&#24687;&#26159;&#23548;&#33268;&#31639;&#27861;&#21388;&#24694;&#29616;&#35937;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#31639;&#27861;&#21388;&#24694;&#8221;&#12290;</title><link>https://arxiv.org/abs/2402.15418</link><description>&lt;p&gt;
&#22768;&#35465;&#31639;&#27861;&#21388;&#24694;
&lt;/p&gt;
&lt;p&gt;
Reputational Algorithm Aversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15418
&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#36319;&#38543;&#31639;&#27861;&#26159;&#21542;&#20256;&#36798;&#26377;&#20851;&#20154;&#31867;&#33021;&#21147;&#30340;&#20449;&#24687;&#26159;&#23548;&#33268;&#31639;&#27861;&#21388;&#24694;&#29616;&#35937;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#31639;&#27861;&#21388;&#24694;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24120;&#24120;&#19981;&#24895;&#23558;&#31639;&#27861;&#20135;&#29983;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#24049;&#30340;&#20915;&#31574;&#20013;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#31639;&#27861;&#21388;&#24694;&#8221;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#31639;&#27861;&#21388;&#24694;&#26159;&#22914;&#20309;&#20135;&#29983;&#30340;&#65292;&#24403;&#36873;&#25321;&#36319;&#38543;&#31639;&#27861;&#20256;&#36798;&#26377;&#20851;&#20154;&#31867;&#33021;&#21147;&#30340;&#20449;&#24687;&#26102;&#12290;&#25105;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#24037;&#20316;&#32773;&#26681;&#25454;&#33258;&#24049;&#30340;&#31169;&#20154;&#20449;&#24687;&#21644;&#31639;&#27861;&#30340;&#20449;&#21495;&#23545;&#38543;&#26426;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#12290;&#20302;&#25216;&#33021;&#24037;&#20316;&#32773;&#25509;&#25910;&#21040;&#27604;&#31639;&#27861;&#26356;&#24046;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#24212;&#22987;&#32456;&#36981;&#24490;&#31639;&#27861;&#30340;&#20449;&#21495;&#65292;&#32780;&#39640;&#25216;&#33021;&#24037;&#20316;&#32773;&#25509;&#25910;&#21040;&#27604;&#31639;&#27861;&#26356;&#22909;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#26377;&#26102;&#24212;&#35813;&#35206;&#30422;&#31639;&#27861;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22768;&#35465;&#19978;&#30340;&#32771;&#34385;&#65292;&#20302;&#25216;&#33021;&#24037;&#20316;&#32773;&#20250;&#19981;&#21512;&#29702;&#22320;&#35206;&#30422;&#31639;&#27861;&#65292;&#20197;&#22686;&#21152;&#34987;&#35270;&#20026;&#39640;&#25216;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;&#35813;&#27169;&#22411;&#20026;&#19982;AI&#31995;&#32479;&#21487;&#33021;&#20250;&#21462;&#20195;&#35768;&#22810;&#31867;&#22411;&#30340;&#24037;&#20316;&#32773;&#30340;&#24191;&#27867;&#20851;&#27880;&#25552;&#20379;&#20102;&#23436;&#20840;&#29702;&#24615;&#30340;&#24494;&#35266;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15418v1 Announce Type: cross  Abstract: People are often reluctant to incorporate information produced by algorithms into their decisions, a phenomenon called "algorithm aversion". This paper shows how algorithm aversion arises when the choice to follow an algorithm conveys information about a human's ability. I develop a model in which workers make forecasts of a random outcome based on their own private information and an algorithm's signal. Low-skill workers receive worse information than the algorithm and hence should always follow the algorithm's signal, while high-skill workers receive better information than the algorithm and should sometimes override it. However, due to reputational concerns, low-skill workers inefficiently override the algorithm to increase the likelihood they are perceived as high-skill. The model provides a fully rational microfoundation for algorithm aversion that aligns with the broad concern that AI systems will displace many types of workers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15163</link><description>&lt;p&gt;
&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#20013;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Studying the Impact of Stochasticity on the Evaluation of Deep Neural Networks for Forest-Fire Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#29992;&#20110;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#37326;&#28779;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#20004;&#31867;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#20998;&#31867;&#30340;&#25351;&#26631;&#65292;&#35780;&#20272;&#23545;&#35266;&#23519;&#22320;&#38754;&#30495;&#30456;&#65288;GT&#65289;&#30340;&#24544;&#23454;&#24230;&#65292;&#20197;&#21450;&#36866;&#24403;&#30340;&#24471;&#20998;&#35268;&#21017;&#65292;&#27979;&#35797;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26862;&#26519;&#28779;&#28798;&#25968;&#25454;&#65292;&#31361;&#26174;&#20102;&#20256;&#32479;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#21487;&#35299;&#37322;&#30340;&#36866;&#29992;&#20110;&#38543;&#26426;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15163v1 Announce Type: cross  Abstract: This paper presents the first systematic study of the evaluation of Deep Neural Networks (DNNs) for discrete dynamical systems under stochastic assumptions, with a focus on wildfire prediction. We develop a framework to study the impact of stochasticity on two classes of evaluation metrics: classification-based metrics, which assess fidelity to observed ground truth (GT), and proper scoring rules, which test fidelity-to-statistic. Our findings reveal that evaluating for fidelity-to-statistic is a reliable alternative in highly stochastic scenarios. We extend our analysis to real-world wildfire data, highlighting limitations in traditional wildfire prediction evaluation methods, and suggest interpretable stochasticity-compatible alternatives.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.14878</link><description>&lt;p&gt;
&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#30340;&#33021;&#25928;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Energy-efficiency Limits on Training AI Systems using Learning-in-Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 &#20844;&#21578;&#31867;&#22411;: cross &#25688;&#35201;: &#20869;&#23384;&#20013;&#23398;&#20064;&#65288;LIM&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#33539;Paradigm&#65292;&#26088;&#22312;&#20811;&#26381;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#20869;&#23384;&#29942;&#39048;&#12290;&#34429;&#28982;&#35745;&#31639;&#20110;&#20869;&#23384;&#65288;CIM&#65289;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25152;&#35859;&#30340;&#20869;&#23384;&#22681;&#38382;&#39064;&#65288;&#21363;&#30001;&#20110;&#37325;&#22797;&#20869;&#23384;&#35835;&#21462;&#35775;&#38382;&#32780;&#28040;&#32791;&#30340;&#33021;&#37327;&#65289;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#20197;&#35757;&#32451;&#25152;&#38656;&#30340;&#31934;&#24230;&#37325;&#22797;&#20869;&#23384;&#20889;&#20837;&#26102;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#26356;&#26032;&#22681;&#65289;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#32771;&#34385;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#20043;&#38388;&#20256;&#36755;&#20449;&#24687;&#26102;&#25152;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#25972;&#21512;&#22681;&#65289;&#12290;LIM&#33539;&#24335;&#25552;&#20986;&#65292;&#22914;&#26524;&#29289;&#29702;&#20869;&#23384;&#30340;&#33021;&#37327;&#23631;&#38556;&#34987;&#33258;&#36866;&#24212;&#35843;&#21046;&#65292;&#20351;&#24471;&#23384;&#20648;&#22120;&#26356;&#26032;&#21644;&#25972;&#21512;&#30340;&#21160;&#24577;&#19982;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;AI&#27169;&#22411;&#30340;Lyapunov&#21160;&#24577;&#30456;&#21305;&#37197;&#65292;&#37027;&#20040;&#36825;&#20123;&#29942;&#39048;&#20063;&#21487;&#20197;&#34987;&#20811;&#26381;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;&#29992;&#19981;&#21516;LIM&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#32791;&#30340;&#26032;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 Announce Type: cross  Abstract: Learning-in-memory (LIM) is a recently proposed paradigm to overcome fundamental memory bottlenecks in training machine learning systems. While compute-in-memory (CIM) approaches can address the so-called memory-wall (i.e. energy dissipated due to repeated memory read access) they are agnostic to the energy dissipated due to repeated memory writes at the precision required for training (the update-wall), and they don't account for the energy dissipated when transferring information between short-term and long-term memories (the consolidation-wall). The LIM paradigm proposes that these bottlenecks, too, can be overcome if the energy barrier of physical memories is adaptively modulated such that the dynamics of memory updates and consolidation match the Lyapunov dynamics of gradient-descent training of an AI model. In this paper, we derive new theoretical lower bounds on energy dissipation when training AI systems using different LIM app
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.14744</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22478;&#24066;&#23621;&#27665;&#65306;&#29992;&#20110;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#30340;LLM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#28789;&#27963;&#39640;&#25928;&#30340;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#12290;LLMs&#36890;&#36807;&#39640;&#25928;&#22788;&#29702;&#35821;&#20041;&#25968;&#25454;&#24182;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#20379;&#22810;&#21151;&#33021;&#24615;, &#20811;&#26381;&#20102;&#20197;&#24448;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#36843;&#20999;&#38656;&#27714;, &#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;: &#23558;LLMs&#19982;&#20016;&#23500;&#30340;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;, &#24320;&#21457;&#21487;&#38752;&#30340;&#27963;&#21160;&#29983;&#25104;&#31574;&#30053;, &#20197;&#21450;&#25506;&#32034;LLMs&#22312;&#22478;&#24066;&#31227;&#21160;&#20013;&#30340;&#24212;&#29992;&#12290;&#20854;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20195;&#29702;&#26694;&#26550;, &#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#20010;&#20307;&#27963;&#21160;&#27169;&#24335;&#21644;&#21160;&#26426;, &#21253;&#25324;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#33258;&#27965;&#26041;&#27861;&#21644;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;, &#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14528</link><description>&lt;p&gt;
ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24573;&#35270;&#20102;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#21407;&#22987;&#34892;&#20026;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#21160;&#20316;&#32500;&#24230;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35780;&#20272;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#31181;&#21407;&#22987;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#24863;&#30693;&#29109;&#39033;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24182;&#20248;&#20808;&#22788;&#29702;&#20855;&#26377;&#39640;&#28508;&#22312;&#24433;&#21709;&#30340;&#34892;&#21160;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38450;&#27490;&#23545;&#29305;&#23450;&#21407;&#22987;&#34892;&#20026;&#36807;&#24230;&#20851;&#27880;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65292;&#22312;&#36328;7&#20010;&#39046;&#22495;&#30340;29&#20010;&#19981;&#21516;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;&#30456;&#36739;&#20110;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#65292;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14528v1 Announce Type: cross  Abstract: The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which un
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;</title><link>https://arxiv.org/abs/2402.13380</link><description>&lt;p&gt;
&#36808;&#21521;&#21464;&#21387;&#22120;&#65306;&#29992;&#21464;&#21387;&#22120;&#24443;&#24213;&#25913;&#21464;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#23481;&#37327;&#38480;&#21046;&#25209;&#37327;&#29983;&#20135;&#38382;&#39064;&#65288;CLSP&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#21033;&#29992;&#21464;&#21387;&#22120;&#26469;&#39044;&#27979;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#22788;&#29702;&#39034;&#24207;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#38750;&#24120;&#36866;&#21512;&#39044;&#27979;&#27599;&#20010;CLSP&#21608;&#26399;&#20013;&#34920;&#31034;&#29983;&#20135;&#35774;&#32622;&#20915;&#31574;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#21160;&#24577;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;CLSP&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#21518;&#22788;&#29702;&#21464;&#21387;&#22120;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;CPLEX&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13380v1 Announce Type: new  Abstract: In this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP). Our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (MIP) problem. Specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP. This problem is inherently dynamic, and we need to handle sequential decision making under constraints. We present an efficient algorithm in which CLSP solutions are learned through a transformer neural network. The proposed post-processed transformer algorithm surpasses the state-of-the-art solver, CPLEX and Long Short-Term Memory (LSTM) in solution time
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;</title><link>https://arxiv.org/abs/2402.13147</link><description>&lt;p&gt;
SubIQ: &#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13147
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#36827;&#19968;&#27493;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447; IL &#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22788;&#29702;&#20165;&#28085;&#30422;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#30340;&#19987;&#23478;&#28436;&#31034;&#30340;&#26377;&#38480;&#25903;&#25345;&#12290;&#25105;&#20204;&#32771;&#34385;&#31163;&#32447; IL&#65292;&#20854;&#20013;&#19987;&#23478;&#28436;&#31034;&#21463;&#21040;&#38480;&#21046;&#65292;&#20294;&#26159;&#30001;&#26356;&#22823;&#35268;&#27169;&#30340;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#34917;&#20805;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#29992;&#20110;&#27492;&#35774;&#32622;&#30340;&#31163;&#32447; IL &#26041;&#27861;&#22522;&#20110;&#34892;&#20026;&#20811;&#38534;&#25110;&#20998;&#24067;&#21305;&#37197;&#65292;&#20854;&#30446;&#30340;&#26159;&#23558;&#27169;&#20223;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#19982;&#19987;&#23478;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#21305;&#37197;&#12290;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#20026;&#19987;&#23478;&#28436;&#31034;&#26377;&#38480;&#65292;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#21344;&#29992;&#20998;&#24067;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#35268;&#27169;&#26356;&#22823;&#65292;&#26377;&#24456;&#39640;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13147v1 Announce Type: cross  Abstract: We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Altruistic Gradient Adjustment (AgA)&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#65292;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2402.12416</link><description>&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20013;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Aligning Individual and Collective Objectives in Multi-Agent Cooperation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12416
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Altruistic Gradient Adjustment (AgA)&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#65292;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#39046;&#22495;&#65292;&#38754;&#20020;&#30528;&#28151;&#21512;&#21160;&#26426;&#21512;&#20316;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#30683;&#30462;&#12290;&#24403;&#21069;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#22870;&#21169;&#25110;&#24341;&#20837;&#39069;&#22806;&#26426;&#21046;&#26469;&#20419;&#36827;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#30528;&#25163;&#21160;&#35774;&#35745;&#25104;&#26412;&#21644;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#30340;&#25910;&#25947;&#31243;&#24207;&#35299;&#20915;&#26041;&#26696;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23558;&#28151;&#21512;&#21160;&#26426;&#21338;&#24328;&#24314;&#27169;&#20026;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#20197;&#30740;&#31350;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Altruistic Gradient Adjustment (AgA)&#30340;&#26032;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#26032;&#39062;&#22320;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#35777;&#26126;&#65292;AgA&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#23545;&#40784;&#26435;&#37325;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12416v1 Announce Type: cross  Abstract: In the field of multi-agent learning, the challenge of mixed-motive cooperation is pronounced, given the inherent contradictions between individual and collective goals. Current research in this domain primarily focuses on incorporating domain knowledge into rewards or introducing additional mechanisms to foster cooperation. However, many of these methods suffer from the drawbacks of manual design costs and the lack of a theoretical grounding convergence procedure to the solution. To address this gap, we approach the mixed-motive game by modeling it as a differentiable game to study learning dynamics. We introduce a novel optimization method named Altruistic Gradient Adjustment (AgA) that employs gradient adjustments to novelly align individual and collective objectives. Furthermore, we provide theoretical proof that the selection of an appropriate alignment weight in AgA can accelerate convergence towards the desired solutions while e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#65292;&#23454;&#29616;&#22312;&#26032;&#39046;&#22495;&#22238;&#31572;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.12170</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Unsupervised LLM Adaptation for Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12170
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#65292;&#23454;&#29616;&#22312;&#26032;&#39046;&#22495;&#22238;&#31572;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#26679;&#21270;&#30693;&#35782;&#12290;&#25509;&#30528;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#65292;LLM&#33021;&#22815;&#36820;&#22238;&#22810;&#26679;&#38382;&#39064;&#30340;&#27491;&#30830;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#39044;&#35757;&#32451;&#30340;LLM&#35843;&#25972;&#21040;&#26032;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#22914;&#19981;&#21516;&#32452;&#32455;&#25110;&#26102;&#26399;&#65292;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#20250;&#20135;&#29983;&#24456;&#39640;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21363;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#12289;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;&#28304;&#25968;&#25454;&#65289;&#21644;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#22238;&#31572;&#20851;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#22312;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35265;&#35299;&#65307;&#65288;i&#65289;&#24494;&#35843;&#27169;&#22411;&#23637;&#31034;&#20102;&#25552;&#20379;&#27491;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12170v1 Announce Type: cross  Abstract: Large language models (LLM) learn diverse knowledge present in the large-scale training dataset via self-supervised training. Followed by instruction-tuning, LLM acquires the ability to return correct information for diverse questions. However, adapting these pre-trained LLMs to new target domains, such as different organizations or periods, for the question-answering (QA) task incurs a substantial annotation cost. To tackle this challenge, we propose a novel task, unsupervised LLM adaptation for question answering. In this task, we leverage a pre-trained LLM, a publicly available QA dataset (source data), and unlabeled documents from the target domain. Our goal is to learn LLM that can answer questions about the target domain. We introduce one synthetic and two real datasets to evaluate models fine-tuned on the source and target data, and reveal intriguing insights; (i) fine-tuned models exhibit the ability to provide correct answers 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11472</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#65306;DDIPrompt
&lt;/p&gt;
&lt;p&gt;
DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11472
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#33647;&#29289;&#20998;&#23376;&#20869;&#37096;&#21644;&#20043;&#38388;&#21407;&#23376;&#21644;&#21151;&#33021;&#22242;&#20043;&#38388;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#65288;DDI&#65289;&#26041;&#38754;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#21046;&#32422;&#65306;&#65288;1&#65289;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#20294;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#26576;&#20123;&#30456;&#20114;&#20316;&#29992;&#34987;&#24191;&#27867;&#22320;&#20302;&#20272;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#23545;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;DDI&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#65288;2&#65289;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26159;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#65292;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#24448;&#24448;&#24573;&#35270;&#25110;&#30740;&#31350;&#19981;&#36275;&#30340;&#32597;&#35265;&#20294;&#28508;&#22312;&#20851;&#38190;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DDIPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#26368;&#36817;&#22270;&#25552;&#31034;&#23398;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#33391;&#26041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11073</link><description>&lt;p&gt;
AFaCTA: &#20351;&#29992;&#21487;&#38752;&#30340;LLM&#26631;&#27880;&#32773;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#30340;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11073
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#65292;&#29992;&#20110;&#25171;&#20987;&#35823;&#23548;&#20449;&#24687;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#65292;&#21363;&#20107;&#23454;&#26680;&#26597;&#31649;&#36947;&#20013;&#30340;&#31532;&#19968;&#27493;&#65292;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#21487;&#20280;&#32553;&#24615;&#21644;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#20219;&#21153;&#23450;&#20041;&#21644;&#32034;&#36180;&#27010;&#24565;&#30340;&#19981;&#19968;&#33268;&#24615;&#20197;&#21450;&#65288;2&#65289;&#25163;&#21160;&#26631;&#27880;&#30340;&#39640;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;1&#65289;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#30456;&#20851;&#24037;&#20316;&#20013;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32858;&#28966;&#20110;&#21487;&#39564;&#35777;&#24615;&#30340;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#32479;&#19968;&#23450;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;2&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AFaCTA&#65288;&#33258;&#21160;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#26631;&#27880;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#20013;&#25552;&#20379;&#24110;&#21161;&#12290;AFaCTA&#36890;&#36807;&#27839;&#30528;&#19977;&#26465;&#39044;&#23450;&#20041;&#30340;&#25512;&#29702;&#36335;&#24452;&#20445;&#25345;&#19968;&#33268;&#24615;&#26469;&#26657;&#20934;&#20854;&#27880;&#37322;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#25919;&#27835;&#35328;&#35770;&#39046;&#22495;&#30340;&#22823;&#37327;&#35780;&#20272;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;AFaCTA&#33021;&#22815;&#39640;&#25928;&#22320;&#21327;&#21161;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11073v1 Announce Type: cross  Abstract: With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#26426;&#21327;&#21516;&#30340;&#26041;&#27861;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#29256;&#26412;&#30340;&#30456;&#21516;&#38382;&#39064;&#26469;&#25506;&#27979;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#25110;&#24187;&#35273;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#23457;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09346</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#26426;&#21327;&#21516;&#30340;&#26041;&#27861;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#26426;&#21327;&#21516;&#30340;&#26041;&#27861;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#29256;&#26412;&#30340;&#30456;&#21516;&#38382;&#39064;&#26469;&#25506;&#27979;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#25110;&#24187;&#35273;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#23457;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#29992;&#25143;&#21644;&#22330;&#26223;&#20013;&#30340;&#26222;&#21450;&#65292;&#35782;&#21035;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#20559;&#35265;&#12289;&#19981;&#19968;&#33268;&#24615;&#21644;&#24187;&#35273;&#12290;&#23613;&#31649;&#23545;&#36825;&#20123;&#38382;&#39064;&#36827;&#34892;&#23457;&#35745;&#26159;&#21487;&#21462;&#30340;&#65292;&#20294;&#24182;&#19981;&#23481;&#26131;&#35299;&#20915;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#29256;&#26412;&#30340;&#30456;&#21516;&#38382;&#39064;&#26469;&#25506;&#27979;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#26292;&#38706;&#20854;&#30693;&#35782;&#25110;&#36816;&#34892;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#34920;&#26126;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#25110;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#35201;&#22312;&#22823;&#35268;&#27169;&#19978;&#23454;&#29616;&#36825;&#31181;&#23457;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#21487;&#38752;&#19988;&#33258;&#21160;&#21270;&#30340;&#29983;&#25104;&#36825;&#20123;&#25506;&#27979;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#26426;&#21327;&#21516;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#21487;&#39564;&#35777;&#24615;&#21644;&#36879;&#26126;&#24615;&#65292;&#36991;&#20813;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#20381;&#36182;&#65292;&#24182;&#22686;&#21152;&#20102;&#31185;&#23398;&#20005;&#35880;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09346v1 Announce Type: new Abstract: As LLMs become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples include bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is desirable, it is far from being easy or solved. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose an automatic and scalable solution, where one uses a different LLM along with human-in-the-loop. This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs, and increasing scientific rigor and generalizability. Specifically, we present a novel methodology with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.09345</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#22870;&#21169;&#24314;&#27169;&#26469;&#20943;&#36731;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reward Hacking via Information-Theoretic Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20013;&#30340;&#25104;&#21151;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#38754;&#65292;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#22870;&#21169;&#24314;&#27169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#22870;&#21169;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#21644;&#40065;&#26834;&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#31216;&#20026;InfoRM&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#26469;&#36807;&#28388;&#20986;&#19981;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#36807;&#24230;&#20248;&#21270;&#19982;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23558;InfoRM&#20316;&#20026;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#36807;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.08957</link><description>&lt;p&gt;
MUSTARD&#65306;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#23398;&#25512;&#29702;&#21644;&#23450;&#29702;&#35777;&#26126;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#20005;&#26684;&#21644;&#24418;&#24335;&#21270;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#21560;&#24341;&#39046;&#22495;&#65292;&#20294;&#20173;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22914;Chain-of-Thought&#65288;CoT&#65289;&#25581;&#31034;&#20102;&#20013;&#38388;&#27493;&#39588;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36880;&#27493;&#27880;&#37322;&#38656;&#35201;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#65292;&#23548;&#33268;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#30340;&#35757;&#32451;&#27493;&#39588;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20027;&#23548;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;&#12290;MUSTARD&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#21512;&#25104;&#25968;&#25454;&#65306;&#65288;1&#65289;&#23427;&#38543;&#26426;&#36873;&#25321;&#20960;&#20010;&#25968;&#23398;&#27010;&#24565;&#20316;&#20026;&#38382;&#39064;&#30340;&#31867;&#21035;&#12290;&#65288;2&#65289;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#36873;&#23450;&#30340;&#27010;&#24565;&#25552;&#31034;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#38382;&#39064;&#21644;&#23427;&#20204;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
&lt;/p&gt;</description></item><item><title>UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07939</link><description>&lt;p&gt;
UFO: &#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#20132;&#20114;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
UFO: A UI-Focused Agent for Windows OS Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07939
&lt;/p&gt;
&lt;p&gt;
UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UFO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20102;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;UFO&#37319;&#29992;&#21452;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#31934;&#30830;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#25511;&#21046;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#21333;&#20010;&#24212;&#29992;&#31243;&#24207;&#20869;&#20197;&#21450;&#36328;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23548;&#33322;&#21644;&#25805;&#20316;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#28041;&#21450;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#21160;&#20316;&#36830;&#25509;&#65292;&#24182;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;UFO&#23558;&#33392;&#24040;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#36716;&#21464;&#20026;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#23601;&#21487;&#20197;&#23436;&#25104;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;9&#20010;&#27969;&#34892;&#30340;Windows&#24212;&#29992;&#31243;&#24207;&#19978;&#23545;UFO&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#21453;&#26144;&#29992;&#25143;&#26085;&#24120;&#20351;&#29992;&#24773;&#26223;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#24471;&#20986;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;UFO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.06955</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Training dynamics in Physics-Informed Neural Networks with feature mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26631;&#24535;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#21464;&#20307;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26469;&#33258;&#26356;&#24191;&#27867;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30740;&#31350;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#32463;&#39564;&#24615;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#26469;&#30740;&#31350;&#24102;&#26377;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;PINNs&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#26465;&#20214;&#27491;&#23450;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#20316;&#20026;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#38598;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#23454;&#29616;&#65292;&#24182;&#21463;&#30410;&#20110;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine learning approach for solving Partial Differential Equations (PDEs). Although its variants have achieved significant progress, the empirical success of utilising feature mapping from the wider Implicit Neural Representations studies has been substantially neglected. We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model. We also show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative. The empirical results reveal the efficacy of our method in diverse forward and inverse problem sets. This simple technique can be easily implemented in coordinate input networks and benefits the broad PINNs research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06104</link><description>&lt;p&gt;
&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65306;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#26126;&#30830;&#23398;&#20064;&#20989;&#25968;&#23548;&#25968;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#22238;&#24402;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#25439;&#22833;&#20989;&#25968;&#26469;&#23558;&#27169;&#22411;&#39044;&#27979;&#19982;&#27599;&#20010;&#20010;&#20307;&#25968;&#25454;&#26679;&#26412;&#30340;&#30495;&#23454;&#20540;&#23545;&#40784;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#20851;&#31995;&#30340;&#39044;&#27979;&#19981;&#22815;&#20248;&#21270;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#24037;&#20316;&#24341;&#20837;&#20102;&#26631;&#31614;&#30456;&#20284;&#24615;&#20449;&#24687;&#26469;&#25913;&#36827;&#22238;&#24402;&#26041;&#27861;&#65292;&#20294;&#22312;&#23436;&#20840;&#25429;&#25417;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FAR&#65288;&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65289;&#20316;&#20026;&#19968;&#31181;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#39046;&#22495;&#30340;&#20843;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample, which, as we show, can result in sub-optimal prediction of the relationships between the different samples. Recent research endeavors have introduced novel perspectives by incorporating label similarity information to regression. However, a notable gap persists in these approaches when it comes to fully capturing the intricacies of the underlying ground truth function. In this work, we propose FAR (Function Aligned Regression) as a arguably better and more efficient solution to fit the underlying function of ground truth by capturing functional derivatives. We demonstrate the effectiveness of the proposed method practically on 2 synthetic datasets and on 8 extensive real-world tasks from 6 b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05569</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Node Classification With Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#25104;&#23545;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21457;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#30340;&#24819;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#30340;&#21457;&#23637;&#12290;GNNs&#21644;HyperGNNs&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#25299;&#25169;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#22823;&#22810;&#25968;HyperGNNs&#21487;&#20197;&#20351;&#29992;&#24102;&#26377;&#36229;&#22270;&#30340;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;GNN&#26469;&#36817;&#20284;&#12290;&#36825;&#23548;&#33268;&#20102;WCE-GNN&#65292;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;GNN&#21644;&#19968;&#20010;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#65288;WCE&#65289;&#65292;&#29992;&#20110;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23545;&#20110;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WCE-GNN&#19981;&#20165;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
&lt;/p&gt;</description></item><item><title>EvoSeed&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#65292;&#26088;&#22312;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#23041;&#32961;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.04699</link><description>&lt;p&gt;
EvoSeed&#65306;&#25581;&#31034;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#24187;&#35273;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04699
&lt;/p&gt;
&lt;p&gt;
EvoSeed&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#65292;&#26088;&#22312;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#23041;&#32961;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#25152;&#21033;&#29992;&#65292;&#36825;&#20123;&#26679;&#26412;&#23545;&#20154;&#31867;&#24863;&#30693;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#20250;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30333;&#30418;&#24615;&#36136;&#26469;&#29983;&#25104;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#65292;&#25110;&#32773;&#25913;&#21464;&#23545;&#25239;&#26679;&#26412;&#19982;&#35757;&#32451;&#20998;&#24067;&#30340;&#20998;&#24067;&#12290;&#20026;&#20102;&#32531;&#35299;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvoSeed&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;EvoSeed&#26694;&#26550;&#20351;&#29992;&#36741;&#21161;&#25193;&#25955;&#21644;&#20998;&#31867;&#22120;&#27169;&#22411;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#36816;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;CMA-ES&#26469;&#20248;&#21270;&#23545;&#23545;&#25239;&#31181;&#23376;&#21521;&#37327;&#30340;&#25628;&#32034;&#65292;&#35813;&#21521;&#37327;&#22312;&#32463;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22788;&#29702;&#21518;&#65292;&#23548;&#33268;&#20998;&#31867;&#22120;&#27169;&#22411;&#38169;&#35823;&#20998;&#31867;&#26080;&#38480;&#21046;&#30340;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#29983;&#25104;&#30340;&#23545;&#25239;&#22270;&#20687;&#20855;&#26377;&#39640;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04062</link><description>&lt;p&gt;
&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Link Prediction with Relational Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#24050;&#32463;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#23548;&#33268;&#20102;&#20855;&#26377;&#25104;&#21151;&#24212;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#20016;&#23500;&#26223;&#35266;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#25104;&#21151;&#36716;&#31227;&#21040;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20851;&#31995;&#36229;&#36793;&#30340;&#23384;&#22312;&#20351;&#24471;&#38142;&#25509;&#39044;&#27979;&#25104;&#20026;&#22312;&#19981;&#21516;&#36873;&#25321;&#30340;k&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20219;&#21153;&#65292;&#36825;&#27604;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#22240;&#20026;&#27599;&#20010;&#20851;&#31995;&#37117;&#26159;&#20108;&#36827;&#21046;&#30340;&#65288;k=2&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#20197;&#21450;&#19968;&#20123;&#33258;&#28982;&#36923;&#36753;&#24418;&#24335;&#23545;&#29983;&#25104;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model archit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.03559</link><description>&lt;p&gt;
&#29992;&#20110;&#32422;&#26463;&#28385;&#36275;&#30340;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Projected Generative Diffusion Models for Constraint Satisfaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#39034;&#24207;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#21407;&#22987;&#22122;&#22768;&#20013;&#21512;&#25104;&#20986;&#36830;&#36143;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#36755;&#20986;&#31526;&#21512;&#29305;&#23450;&#20005;&#26684;&#26465;&#20214;&#30340;&#22330;&#26223;&#20013;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#24544;&#23454;&#22320;&#36981;&#24490;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#26412;&#25991;&#22312;&#21463;&#38480;&#21046;&#30340;&#32422;&#26463;&#31867;&#21035;&#19979;&#65292;&#23545;PGDM&#33021;&#22815;&#20174;&#21487;&#34892;&#23376;&#20998;&#24067;&#20013;&#21512;&#25104;&#36755;&#20986;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#38750;&#20984;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#26696;&#20363;&#20013;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#22312;&#35270;&#39057;&#29983;&#25104;&#20013;&#20307;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#23398;&#20449;&#24687;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process. However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges. This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem. This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles. This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations. These capabilities are demonstrated by physics-informed motion in video generatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#22411;&#25512;&#29702;&#21644;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#65292;&#21487;&#20197;&#25552;&#20379;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#39564;&#35777;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21487;&#20449;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02675</link><description>&lt;p&gt;
&#20351;&#29992;zkSNARKs&#36827;&#34892;&#21487;&#39564;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Verifiable evaluations of machine learning models using zkSNARKs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#22411;&#25512;&#29702;&#21644;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#65292;&#21487;&#20197;&#25552;&#20379;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#39564;&#35777;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21487;&#20449;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36234;&#26469;&#36234;&#22810;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19990;&#30028;&#20013;&#65292;&#24320;&#21457;&#32773;&#30340;&#27169;&#22411;&#35780;&#20272;&#24517;&#39035;&#34987;&#24403;&#20316;&#38754;&#20540;&#25509;&#21463;&#12290;&#36825;&#20123;&#35780;&#20272;&#32467;&#26524;&#65292;&#26080;&#35770;&#26159;&#20219;&#21153;&#20934;&#30830;&#24615;&#12289;&#20559;&#24046;&#35780;&#20272;&#36824;&#26159;&#23433;&#20840;&#26816;&#26597;&#65292;&#20256;&#32479;&#19978;&#26080;&#27861;&#36890;&#36807;&#37325;&#26032;&#25191;&#34892;&#40657;&#31665;&#27169;&#22411;&#36755;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#21487;&#39564;&#35777;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;zkSNARKs&#36827;&#34892;&#27169;&#22411;&#25512;&#29702;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#36755;&#20986;&#30340;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#21487;&#20197;&#25171;&#21253;&#25104;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#26174;&#31034;&#20855;&#26377;&#22266;&#23450;&#31169;&#26377;&#26435;&#37325;&#30340;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#36798;&#21040;&#20102;&#25152;&#36848;&#30340;&#24615;&#33021;&#25110;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#20123;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#21487;&#20197;&#22312;&#20219;&#20309;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#65292;&#35745;&#31639;&#35201;&#27714;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#31361;&#20986;&#20102;&#20851;&#38190;&#25361;&#25112;&#21644;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a world of increasing closed-source commercial machine learning models, model evaluations from developers must be taken at face value. These benchmark results, whether over task accuracy, bias evaluations, or safety checks, are traditionally impossible to verify by a model end-user without the costly or impossible process of re-performing the benchmark on black-box model outputs. This work presents a method of verifiable model evaluation using model inference through zkSNARKs. The resulting zero-knowledge computational proofs of model outputs over datasets can be packaged into verifiable evaluation attestations showing that models with fixed private weights achieve stated performance or fairness metrics over public inputs. These verifiable attestations can be performed on any standard neural network model with varying compute requirements. For the first time, we demonstrate this across a sample of real-world models and highlight key challenges and design solutions. This presents a n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02651</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models Provide Promptable Representations for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#32972;&#26223;&#19990;&#30028;&#30693;&#35782;&#24555;&#36895;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#20195;&#29702;&#36890;&#24120;&#38656;&#35201;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20013;&#32534;&#30721;&#30340;&#22823;&#37327;&#36890;&#29992;&#21644;&#21487;&#32034;&#24341;&#30340;&#19990;&#30028;&#30693;&#35782;&#26469;&#36827;&#34892;&#20855;&#35937;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;VLMs&#29992;&#20316;&#21487;&#25552;&#31034;&#34920;&#31034;&#26469;&#21021;&#22987;&#21270;&#31574;&#30053;&#65306;&#36825;&#20123;&#23884;&#20837;&#22312;&#35270;&#35273;&#35266;&#23519;&#20013;&#20855;&#26377;&#22522;&#30784;&#65292;&#24182;&#26681;&#25454;VLM&#30340;&#20869;&#37096;&#30693;&#35782;&#32534;&#30721;&#35821;&#20041;&#29305;&#24449;&#65292;&#36890;&#36807;&#25552;&#20379;&#20219;&#21153;&#19978;&#19979;&#25991;&#21644;&#36741;&#21161;&#20449;&#24687;&#26469;&#35302;&#21457;&#12290;&#25105;&#20204;&#22312;Minecraft&#21644;Habitat&#20013;&#30340;&#35270;&#35273;&#22797;&#26434;&#12289;&#38271;&#26399;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36890;&#29992;&#22411;VLMs&#25552;&#21462;&#30340;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#32988;&#36807;&#20351;&#29992;&#36890;&#29992;&#30340;&#12289;&#19981;&#21487;&#25552;&#31034;&#30340;&#22270;&#20687;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#36981;&#24490;&#25351;&#31034;&#30340;&#20803;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#65292;&#21517;&#20026;Moirai&#65292;&#20197;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36328;&#39057;&#29575;&#23398;&#20064;&#12289;&#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#12290;</title><link>https://arxiv.org/abs/2402.02592</link><description>&lt;p&gt;
&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer
&lt;/p&gt;
&lt;p&gt;
Unified Training of Universal Time Series Forecasting Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#65292;&#21517;&#20026;Moirai&#65292;&#20197;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36328;&#39057;&#29575;&#23398;&#20064;&#12289;&#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#19968;&#27169;&#22411;&#30340;&#26694;&#26550;&#19979;&#36816;&#20316;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31361;&#30772;&#24615;&#24433;&#21709;&#30340;&#28508;&#21147;&#12290;&#36890;&#29992;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#28304;&#20110;&#22312;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35774;&#24819;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#30340;&#21333;&#19968;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;i) &#36328;&#39057;&#29575;&#23398;&#20064;&#65292;ii) &#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#20219;&#24847;&#25968;&#37327;&#30340;&#21464;&#37327;&#65292;&#20197;&#21450;iii) &#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23545;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Masked Encoder&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#65288;Moirai&#65289;&#12290;&#22312;&#25105;&#20204;&#26032;&#24341;&#20837;&#30340;&#22823;&#35268;&#27169;&#24320;&#25918;&#26102;&#38388;&#24207;&#21015;&#23384;&#26723;&#65288;LOTSA&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) fea
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#20855;&#26377;&#33258;&#30456;&#20284;&#24615;&#21644;&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#20174;&#27573;&#33853;&#21040;&#25972;&#20010;&#25991;&#26723;&#37117;&#23384;&#22312;&#30456;&#20284;&#30340;&#27169;&#24335;&#21644;&#20381;&#36182;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#22914;&#20309;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26469;&#29702;&#35299;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#12290;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22256;&#24785;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;&#35821;&#35328;&#21644;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.01825</link><description>&lt;p&gt;
&#20998;&#24418;&#27169;&#24335;&#21487;&#33021;&#25581;&#31034;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20013;&#30340;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Fractal Patterns May Unravel the Intelligence in Next-Token Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01825
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#20855;&#26377;&#33258;&#30456;&#20284;&#24615;&#21644;&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#20174;&#27573;&#33853;&#21040;&#25972;&#20010;&#25991;&#26723;&#37117;&#23384;&#22312;&#30456;&#20284;&#30340;&#27169;&#24335;&#21644;&#20381;&#36182;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#22914;&#20309;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26469;&#29702;&#35299;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#12290;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22256;&#24785;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;&#35821;&#35328;&#21644;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#35821;&#35328;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#31934;&#30830;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#37327;&#21270;&#21487;&#33021;&#20043;&#21069;&#21482;&#26377;&#24576;&#30097;&#20294;&#23578;&#26410;&#27491;&#24335;&#35777;&#26126;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#35328;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#33258;&#30456;&#20284;&#24615;&#65292;&#23637;&#31034;&#20986;&#21508;&#20010;&#23618;&#32423;&#19978;&#30340;&#22797;&#26434;&#24615;&#65292;&#27809;&#26377;&#29305;&#23450;&#30340;&#29305;&#24449;&#19978;&#19979;&#25991;&#38271;&#24230;&#65307;&#65288;2&#65289;&#38271;&#31243;&#30456;&#20851;&#24615;&#65288;LRD&#65289;&#65292;&#20855;&#26377;&#22823;&#32422;H=0.70&#30340;Hurst&#21442;&#25968;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#20013;&#30340;&#30701;&#26399;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#22914;&#27573;&#33853;&#20013;&#30340;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#21453;&#26144;&#20102;&#26356;&#22823;&#33539;&#22260;&#30340;&#27169;&#24335;/&#20381;&#36182;&#24615;&#65292;&#22914;&#25972;&#20010;&#25991;&#26723;&#12290;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#22914;&#20309;&#23548;&#33268;&#23545;&#25991;&#26412;&#30340;&#22810;&#20010;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#21333;&#35789;&#21644;&#20174;&#21477;&#21040;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#21644;&#24847;&#22270;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20998;&#24418;&#21442;&#25968;&#22312;&#39044;&#27979;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#27599;&#23383;&#33410;&#27604;&#29305;&#65288;BPB&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#21457;&#29616;&#33021;&#20026;&#35821;&#35328;&#21644;&#26426;&#21046;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.70. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can lead to a comprehension of the structure of text at multiple levels of granularity, from words and clauses to broader contexts and intents. We also demonstrate that fractal parameters improve upon perplexity-based bits-per-byte (BPB) in predicting downstream performance. We hope these findings offer a fresh perspective on language and the mechani
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25307;&#32856;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31616;&#21382;&#23457;&#35745;&#65292;&#21457;&#29616; GPT-4 &#23545;&#27531;&#30142;&#30456;&#20851;&#30340;&#31616;&#21382;&#23384;&#22312;&#20559;&#35265;&#65292;&#36890;&#36807;&#35757;&#32451;&#33258;&#23450;&#20041; GPT &#24182;&#36981;&#24490;&#22810;&#26679;&#24615;&#12289;&#20844;&#24179;&#24615;&#19982;&#21253;&#23481;&#24615;&#20197;&#21450;&#27531;&#30142;&#27491;&#20041;&#21407;&#21017;&#65292;&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.01732</link><description>&lt;p&gt;
&#36776;&#21035;&#21644;&#25913;&#36827;&#22522;&#20110; GAI &#30340;&#31616;&#21382;&#31579;&#36873;&#20013;&#30340;&#27531;&#30142;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Identifying and Improving Disability Bias in GAI-Based Resume Screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01732
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25307;&#32856;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31616;&#21382;&#23457;&#35745;&#65292;&#21457;&#29616; GPT-4 &#23545;&#27531;&#30142;&#30456;&#20851;&#30340;&#31616;&#21382;&#23384;&#22312;&#20559;&#35265;&#65292;&#36890;&#36807;&#35757;&#32451;&#33258;&#23450;&#20041; GPT &#24182;&#36981;&#24490;&#22810;&#26679;&#24615;&#12289;&#20844;&#24179;&#24615;&#19982;&#21253;&#23481;&#24615;&#20197;&#21450;&#27531;&#30142;&#27491;&#20041;&#21407;&#21017;&#65292;&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#20351;&#29992;&#33539;&#22260;&#24050;&#25193;&#23637;&#21040;&#25307;&#32856;&#21644;&#25307;&#32856;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#32771;&#34385;&#20559;&#35265;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#23545;&#21253;&#25324;&#27531;&#30142;&#20154;&#22312;&#20869;&#30340;&#36793;&#32536;&#20154;&#32676;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#37325;&#35201;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#31616;&#21382;&#23457;&#35745;&#30740;&#31350;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35201;&#27714; ChatGPT&#65288;&#29305;&#21035;&#26159; GPT-4&#65289;&#23545;&#19968;&#20221;&#31616;&#21382;&#36827;&#34892;&#25490;&#21517;&#65292;&#19982;&#38468;&#21152;&#20102;&#19982;&#27531;&#30142;&#26377;&#20851;&#30340;&#39069;&#22806;&#39046;&#23548;&#22870;&#21169;&#12289;&#22870;&#23398;&#37329;&#12289;&#38754;&#26495;&#28436;&#35762;&#21644;&#25104;&#21592;&#36164;&#26684;&#30340;&#30456;&#21516;&#31616;&#21382;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616; GPT-4 &#23545;&#36825;&#20123;&#25913;&#36827;&#30340;&#31616;&#21382;&#23384;&#22312;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#35757;&#32451;&#22522;&#20110; DEI &#21644;&#27531;&#30142;&#27491;&#20041;&#21407;&#21017;&#30340;&#33258;&#23450;&#20041; GPT&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21253;&#25324;&#23545; GPT-4 &#29992;&#20110;&#35777;&#26126;&#20854;&#26377;&#20559;&#35265;&#20915;&#31574;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#27531;&#38556;&#20027;&#20041;&#31867;&#22411;&#30340;&#29420;&#29305;&#23450;&#24615;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#39069;&#22806;&#20559;&#35265;&#20943;&#36731;&#24037;&#20316;&#30340;&#26041;&#21521;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#36825;&#20123;&#29702;&#30001;&#21487;&#33021;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#27531;&#38556;&#27491;&#20041;&#30340;&#30456;&#20851;&#23454;&#38469;&#26696;&#20363;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Generative AI rises in adoption, its use has expanded to include domains such as hiring and recruiting. However, without examining the potential of bias, this may negatively impact marginalized populations, including people with disabilities. To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability related. We find that GPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this prejudice can be quantifiably reduced by training a custom GPTs on principles of DEI and disability justice. Our study also includes a unique qualitative analysis of the types of direct and indirect ableism GPT-4 uses to justify its biased decisions and suggest directions for additional bias mitigation work. Additionally, since these justifications are presumably drawn from traini
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#20013;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#21306;&#20998;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.00793</link><description>&lt;p&gt;
&#26080;&#27861;&#21306;&#20998;&#30340;&#21306;&#20998;&#65306;&#31639;&#27861;&#39044;&#27979;&#20013;&#30340;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#20013;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#21306;&#20998;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#26469;&#21306;&#20998;&#37027;&#20123;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#28548;&#28165;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#19987;&#23478;&#36890;&#24120;&#20855;&#26377;&#20449;&#24687;&#30340;&#35775;&#38382;&#26435;&#38480;&#8212;&#8212;&#29305;&#21035;&#26159;&#20027;&#35266;&#20449;&#24687;&#8212;&#8212;&#32780;&#36825;&#20123;&#20449;&#24687;&#26159;&#31639;&#27861;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#32534;&#30721;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#35748;&#35782;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#26377;&#21407;&#21017;&#30340;&#31639;&#27861;&#65292;&#20165;&#22312;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#26377;&#25152;&#25913;&#21892;&#26102;&#25165;&#36873;&#25321;&#24615;&#22320;&#32435;&#20837;&#20154;&#31867;&#21453;&#39304;&#12290;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#31639;&#27861;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24448;&#24448;&#20248;&#20110;&#20154;&#31867;&#23545;&#24212;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#31867;&#21028;&#26029;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65288;&#21487;&#20197;&#39044;&#20808;&#30830;&#23450;&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;X&#23556;&#32447;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#23376;&#38598;&#22312;&#24739;&#32773;&#32676;&#20307;&#20013;&#21344;&#25454;&#20102;&#36817;30%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00759</link><description>&lt;p&gt;
&#26500;&#24314;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Building Expressive and Tractable Probabilistic Generative Models: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#22266;&#26377;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#31361;&#20986;&#20102;&#20351;PCs&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#30340;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#36890;&#36807;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#27010;&#24565;&#26469;&#26500;&#24314;&#28145;&#24230;&#21644;&#28151;&#21512;PCs&#30340;&#21162;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.07453</link><description>&lt;p&gt;
&#35268;&#27169;&#21270;&#27169;&#22411;&#32534;&#36753;&#20250;&#23548;&#33268;&#28176;&#36827;&#24615;&#21644;&#31361;&#21457;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Model Editing at Scale leads to Gradual and Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07453
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#36753;&#30693;&#35782;&#26159;&#19968;&#31181;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#33021;&#21147;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#32416;&#27491;&#38169;&#35823;&#23398;&#20064;&#30340;&#20107;&#23454;&#65292;&#21516;&#26102;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#26032;&#20107;&#23454;&#21015;&#34920;&#26356;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#27169;&#22411;&#32534;&#36753;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#33021;&#22815;&#23545;&#21516;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#32534;&#36753;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65306;ROME &#21644; MEMIT&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#19981;&#26029;&#22320;&#36951;&#24536;&#20808;&#21069;&#32534;&#36753;&#36807;&#30340;&#20107;&#23454;&#20197;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36951;&#24536;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;--&#21021;&#22987;&#30340;&#28176;&#36827;&#24615;&#36951;&#24536;&#38454;&#27573;&#65292;&#38543;&#21518;&#26159;&#31361;&#28982;&#25110;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07453v2 Announce Type: replace-cross  Abstract: Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgettin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#20174;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#30340;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#26469;&#25910;&#38598;&#21644;&#26356;&#26032;&#30456;&#20851;&#35770;&#25991;&#12290;</title><link>https://arxiv.org/abs/2312.03863</link><description>&lt;p&gt;
&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Efficient Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03863
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#20174;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#30340;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#26469;&#25910;&#38598;&#21644;&#26356;&#26032;&#30456;&#20851;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37325;&#35201;&#20219;&#21153;&#22914;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#35821;&#35328;&#29983;&#25104;&#21644;&#22797;&#26434;&#25512;&#29702;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#26377;&#28508;&#21147;&#23545;&#25105;&#20204;&#30340;&#31038;&#20250;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20276;&#38543;&#30528;&#23427;&#20204;&#25152;&#38656;&#30340;&#30456;&#24403;&#22823;&#30340;&#36164;&#28304;&#65292;&#31361;&#26174;&#20102;&#35299;&#20915;&#25928;&#29575;&#25361;&#25112;&#30340;&#26377;&#25928;&#25216;&#26415;&#30340;&#24378;&#28872;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#39640;&#25928;LLMs&#30740;&#31350;&#30340;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#23558;&#25991;&#29486;&#25353;&#29031;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#20998;&#31867;&#36827;&#34892;&#32452;&#32455;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#20294;&#30456;&#20114;&#20851;&#32852;&#30340;&#39640;&#25928;LLMs&#20027;&#39064;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#65292;&#20854;&#20013;&#25910;&#38598;&#20102;&#26412;&#35843;&#26597;&#20013;&#21015;&#20986;&#30340;&#35770;&#25991;&#65292;&#24182;&#23558;&#31215;&#26497;&#32500;&#25252;&#35813;&#23384;&#20648;&#24211;&#65292;&#24182;&#38543;&#30528;&#26032;&#30340;&#30740;&#31350;&#30340;&#20986;&#29616;&#32780;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges.In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.15964</link><description>&lt;p&gt;
&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Pre-training for Localized Instruction Generation of Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35270;&#39057;&#23637;&#31034;&#20102;&#35832;&#22914;&#39135;&#35889;&#20934;&#22791;&#31561;&#20219;&#21153;&#30340;&#36880;&#27493;&#28436;&#31034;&#12290;&#29702;&#35299;&#27492;&#31867;&#35270;&#39057;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;&#27493;&#39588;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#24182;&#29983;&#25104;&#25991;&#23383;&#35828;&#26126;&#12290;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#24182;&#32534;&#20889;&#35828;&#26126;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#38480;&#21046;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#24182;&#38459;&#30861;&#20102;&#26377;&#25928;&#23398;&#20064;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#30340;&#35270;&#39057;-&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#36716;&#24405;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#65292;&#19982;&#20154;&#31867;&#27880;&#37322;&#21592;&#32534;&#20889;&#30340;&#35828;&#26126;&#30456;&#27604;&#23384;&#22312;&#39118;&#26684;&#21464;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;Sieve-&amp;-Swap&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#21644;&#20351;&#29992;&#25991;&#26412;&#39135;&#35889;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#33258;&#21160;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#20197;&#22686;&#24378;&#25991;&#23383;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&amp;-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#24378;&#35843;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#65292;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#27492;&#20462;&#25913;&#20165;&#23545;&#32047;&#31215;&#36951;&#25022;&#20135;&#29983;&#24658;&#23450;&#30340;&#24809;&#32602;&#12290;</title><link>https://arxiv.org/abs/2311.09483</link><description>&lt;p&gt;
&#20855;&#26377;&#29992;&#25143;&#23450;&#20041;&#30446;&#26631;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#29992;&#20110;&#20581;&#24247;&#34892;&#20026;&#25913;&#21464;
&lt;/p&gt;
&lt;p&gt;
Adaptive Interventions with User-Defined Goals for Health Behavior Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#24378;&#35843;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#65292;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#27492;&#20462;&#25913;&#20165;&#23545;&#32047;&#31215;&#36951;&#25022;&#20135;&#29983;&#24658;&#23450;&#30340;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#27963;&#21160;&#19981;&#36275;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#20844;&#20849;&#20581;&#24247;&#38382;&#39064;&#65292;&#19982;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;2&#22411;&#31958;&#23615;&#30149;&#31561;&#19981;&#33391;&#20581;&#24247;&#32467;&#26524;&#30456;&#20851;&#12290;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#20026;&#20302;&#25104;&#26412;&#12289;&#21487;&#25193;&#23637;&#30340;&#36523;&#20307;&#27963;&#21160;&#20419;&#36827;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#28982;&#32780;&#36890;&#24120;&#25928;&#26524;&#36739;&#23567;&#65292;&#31896;&#38468;&#29575;&#20302;&#65292;&#29305;&#21035;&#26159;&#19982;&#20154;&#31867;&#36741;&#23548;&#30456;&#27604;&#12290;&#30446;&#26631;&#35774;&#23450;&#26159;&#20581;&#24247;&#36741;&#23548;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#20013;&#19968;&#30452;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;Thompson&#25277;&#26679;&#31639;&#27861;&#30340;&#20462;&#25913;&#65292;&#37325;&#28857;&#25918;&#22312;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#12290;&#20316;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#30340;&#19968;&#27493;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#21516;&#26102;&#20248;&#21270;&#20010;&#20154;&#20559;&#22909;&#21644;&#30446;&#26631;&#30340;&#24179;&#34913;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#25913;&#21482;&#23545;&#32047;&#31215;&#36951;&#25022;&#36896;&#25104;&#19968;&#20010;&#24120;&#25968;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09483v2 Announce Type: replace-cross  Abstract: Physical inactivity remains a major public health concern, having associations with adverse health outcomes such as cardiovascular disease and type-2 diabetes. Mobile health applications present a promising avenue for low-cost, scalable physical activity promotion, yet often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. Goal-setting is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. This paper introduces a modification to the Thompson sampling algorithm that places emphasis on individualized goal-setting by optimizing personalized reward functions. As a step towards supporting goal-setting, this paper offers a balanced approach that can leverage shared structure while optimizing individual preferences and goals. We prove that our modification incurs only a constant penalty on the cumulative regret 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.06840</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#36951;&#28431;&#26631;&#31614;: &#19968;&#39033;&#20851;&#20110;&#24726;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Omitted Labels in Causality: A Study of Paradoxes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#25152;&#31216;&#20043;&#20026;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#30340;&#27010;&#24565;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#19987;&#19994;&#20154;&#22763;&#25110;&#29305;&#23450;&#30340;&#19987;&#27880;&#30740;&#31350;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#25105;&#20204;&#21033;&#29992;&#24050;&#24191;&#27867;&#30740;&#31350;&#30340;&#24726;&#35770;&#65288;&#36763;&#26222;&#26862;&#24726;&#35770;&#21644;&#24247;&#22810;&#22622;&#24726;&#35770;&#65289;&#26469;&#35828;&#26126;&#22312;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#26356;&#26222;&#36941;&#22256;&#38590;&#12290;&#19982;&#22240;&#26524;&#25512;&#26029;&#22522;&#26412;&#21407;&#29702;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#27491;&#30830;&#8221;&#30340;&#26657;&#27491;&#26377;&#26102;&#38656;&#35201;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#12290;&#36825;&#20123;&#38519;&#38449;&#24341;&#23548;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#32593;&#32476;&#21644;&#20854;&#24418;&#25104;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore what we call ``omitted label contexts,'' in which training data is limited to a subset of the possible labels. This setting is common among specialized human experts or specific focused studies. We lean on well-studied paradoxes (Simpson's and Condorcet) to illustrate the more general difficulties of causal inference in omitted label contexts. Contrary to the fundamental principles on which much of causal inference is built, we show that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. These pitfalls lead us to the study networks of conclusions drawn from different contexts and the structures the form, proving an interesting connection between these networks and social choice theory.
&lt;/p&gt;</description></item><item><title>&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20551;&#35774;&#25552;&#20986;&#26041;&#38754;&#34920;&#29616;&#24778;&#20154;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#19968;&#20010;&#65288;&#20219;&#21153;&#29305;&#23450;&#30340;&#65289;&#31526;&#21495;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#31995;&#32479;&#22320;&#36807;&#28388;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08559</link><description>&lt;p&gt;
&#20196;&#20154;&#24778;&#21497;&#20294;&#20196;&#20154;&#22256;&#24785;&#65306;&#29992;&#20551;&#35774;&#32454;&#21270;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#30340;&#24402;&#32435;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08559
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20551;&#35774;&#25552;&#20986;&#26041;&#38754;&#34920;&#29616;&#24778;&#20154;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#19968;&#20010;&#65288;&#20219;&#21153;&#29305;&#23450;&#30340;&#65289;&#31526;&#21495;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#31995;&#32479;&#22320;&#36807;&#28388;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23569;&#25968;&#35266;&#23519;&#20013;&#25512;&#23548;&#20986;&#28508;&#22312;&#21407;&#21017;&#65292;&#28982;&#21518;&#25512;&#24191;&#21040;&#26032;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#21363;&#24402;&#32435;&#25512;&#29702;&#65292;&#23545;&#20110;&#20154;&#31867;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#22312;&#30740;&#31350;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#24120;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#20551;&#35774;&#32454;&#21270;&#36825;&#19968;&#25216;&#26415;&#23545;LMs&#30340;&#24402;&#32435;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35813;&#25216;&#26415;&#26356;&#25509;&#36817;&#20154;&#31867;&#24402;&#32435;&#36807;&#31243;&#65292;&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;&#36755;&#20837;-&#36755;&#20986;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08559v3 Announce Type: replace-cross  Abstract: The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.01945</link><description>&lt;p&gt;
OHQ: &#33455;&#29255;&#19978;&#30340;&#30828;&#20214;&#24863;&#30693;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OHQ: On-chip Hardware-aware Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#20808;&#36827;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21033;&#29992;&#22810;&#20301;&#23485;&#26550;&#26500;&#26469;&#37322;&#25918;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#23384;&#22312;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#37327;&#21270;&#36807;&#31243;&#20381;&#36182;&#20110;&#29420;&#31435;&#30340;&#39640;&#24615;&#33021;&#35774;&#22791;&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#36827;&#34892;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#32771;&#34385;&#30340;&#30828;&#20214;&#25351;&#26631;&#19982;&#23454;&#38469;&#37096;&#32626;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22312;&#32447;&#35774;&#22791;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33455;&#29255;&#19978;&#30340;&#37327;&#21270;&#24863;&#30693;&#65288;OQA&#65289;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#24863;&#30693;&#37327;&#21270;&#31639;&#23376;&#22312;&#30828;&#20214;&#19978;&#30340;&#23454;&#38469;&#25928;&#29575;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#65288;MQE&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2204.04510</link><description>&lt;p&gt;
&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#35753;&#31616;&#21333;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#19978;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.04510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#24120;&#20351;&#29992;&#19987;&#38376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#22823;&#22411;&#20840;&#23616;&#22270;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20294;&#25361;&#25112;&#23376;&#22270;&#30340;&#23618;&#27425;&#32467;&#26500;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23376;&#22270;&#21040;&#33410;&#28857;&#65288;S2N&#65289;&#36716;&#25442;&#30340;&#26032;&#39062;&#20844;&#24335;&#65292;&#29992;&#20110;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20840;&#23616;&#22270;&#20013;&#30340;&#19968;&#32452;&#23376;&#22270;&#65292;&#25105;&#20204;&#36890;&#36807;&#31895;&#30053;&#22320;&#23558;&#23376;&#22270;&#36716;&#25442;&#25104;&#33410;&#28857;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#22270;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;S2N&#19981;&#20165;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#19988;&#36890;&#36807;&#25429;&#25417;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#20063;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#23427;&#20204;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31895;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35843;&#25972;&#27169;&#22411;&#21518;&#25928;&#26524;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16635</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble. (arXiv:2401.16635v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;RLHF&#20381;&#36182;&#20110;&#36890;&#36807;&#26377;&#38480;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;RLHF&#21487;&#33021;&#20135;&#29983;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22870;&#21169;&#27169;&#22411;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#32771;&#34385;&#21040;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#21644;&#36164;&#28304;&#26114;&#36149;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21253;&#25324;&#32447;&#24615;&#23618;&#38598;&#25104;&#21644;&#22522;&#20110;LoRA&#30340;&#38598;&#25104;&#22312;&#20869;&#30340;&#39640;&#25928;&#38598;&#25104;&#26041;&#27861;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#38598;&#25104;&#22870;&#21169;&#27169;&#22411;&#36816;&#34892;Best-of-$n$&#21644;Proximal Policy Optimization&#65292;&#24182;&#39564;&#35777;&#25105;&#20204;&#30340;&#38598;&#25104;&#26041;&#27861;&#26377;&#21161;&#20110;&#25913;&#21892;RLHF&#36755;&#20986;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is a widely adopted approach for aligning large language models with human values. However, RLHF relies on a reward model that is trained with a limited amount of human preference data, which could lead to inaccurate predictions. As a result, RLHF may produce outputs that are misaligned with human values. To mitigate this issue, we contribute a reward ensemble method that allows the reward model to make more accurate predictions. As using an ensemble of large language model-based reward models can be computationally and resource-expensive, we explore efficient ensemble methods including linear-layer ensemble and LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy Optimization with our ensembled reward models, and verify that our ensemble methods help improve the alignment performance of RLHF outputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;L-AutoDA&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.15335</link><description>&lt;p&gt;
L-AutoDA: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks. (arXiv:2401.15335v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;L-AutoDA&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23545;&#25239;&#25915;&#20987;&#23545;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#20915;&#31574;&#22411;&#25915;&#20987;&#21482;&#38656;&#35201;&#27169;&#22411;&#30340;&#20915;&#31574;&#21453;&#39304;&#65292;&#32780;&#19981;&#38656;&#35201;&#35814;&#32454;&#30340;&#27010;&#29575;&#25110;&#20998;&#25968;&#65292;&#22240;&#27492;&#29305;&#21035;&#38590;&#20197;&#38450;&#24481;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;L-AutoDA&#65288;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#33258;&#21160;&#35774;&#35745;&#36825;&#20123;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#36827;&#21270;&#26694;&#26550;&#20013;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#33258;&#21160;&#35774;&#35745;&#20986;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#20943;&#23569;&#20154;&#24037;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;L-AutoDA&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#25239;&#25915;&#20987;&#29983;&#25104;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of machine learning, adversarial attacks present a significant challenge to model robustness and security. Decision-based attacks, which only require feedback on the decision of a model rather than detailed probabilities or scores, are particularly insidious and difficult to defend against. This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks. By iteratively interacting with LLMs in an evolutionary framework, L-AutoDA automatically designs competitive attack algorithms efficiently without much human effort. We demonstrate the efficacy of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline methods in both success rate and computational efficiency. Our findings underscore the potential of language models as tools for adversarial attack generation and highli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14521</link><description>&lt;p&gt;
&#20197;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;-&#27010;&#24565;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#22522;&#20110;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#20316;&#20026;&#22522;&#26412;&#35745;&#31639;&#21333;&#20803;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#21333;&#20010;&#20301;&#32622;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#65288;&#28145;&#24230;&#65289;&#65292;&#32780;&#19981;&#26159;&#23545;&#22823;&#26679;&#26412;&#38598;&#27700;&#21306;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#24191;&#24230;&#12290;&#30446;&#26631;&#26159;&#21457;&#29616;&#19968;&#20010;&#26368;&#23567;&#30340;&#34920;&#31034;&#65288;&#21333;&#20803;&#29366;&#24577;&#25968;&#21644;&#27969;&#37327;&#36335;&#24452;&#25968;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#33021;&#22815;&#35299;&#37322;&#32473;&#23450;&#38598;&#27700;&#21306;&#36755;&#20837;&#29366;&#24577;&#21644;&#36755;&#20986;&#34892;&#20026;&#30340;&#20027;&#35201;&#36807;&#31243;&#65292;&#29305;&#21035;&#24378;&#35843;&#27169;&#25311;&#20840;&#33539;&#22260;&#65288;&#39640;&#12289;&#20013;&#12289;&#20302;&#65289;&#30340;&#27969;&#37327;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#21306;&#22495;&#65292;&#37319;&#29992;&#31867;&#20284;HyMod&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;3&#20010;&#21333;&#20803;&#29366;&#24577;&#21644;2&#20010;&#20027;&#35201;&#27969;&#21160;&#36335;&#24452;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#26679;&#30340;&#34920;&#31034;&#65292;&#20294;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#27700;&#25991;&#22270;&#30340;&#26102;&#38388;&#21644;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
&lt;/p&gt;</description></item><item><title>LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13920</link><description>&lt;p&gt;
LocMoE: &#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20302;&#24320;&#38144;MoE
&lt;/p&gt;
&lt;p&gt;
LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13920
&lt;/p&gt;
&lt;p&gt;
LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;MoE&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#20998;&#24067;&#24335;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#26377;&#25928;&#31232;&#30095;&#21644;&#25193;&#23637;&#27169;&#22411;&#65292;&#22240;&#27492;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#24615;&#33021;&#21463;&#21040;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20840;&#23545;&#20840;&#36890;&#20449;&#30340;&#39640;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#30001;&#20110;&#22823;&#37327;&#30340;&#19987;&#23478;&#23481;&#37327;&#23548;&#33268;&#30456;&#23545;&#20887;&#20313;&#30340;&#35745;&#31639;&#12290;&#36127;&#36733;&#19981;&#24179;&#34913;&#21487;&#33021;&#26159;&#30001;&#20110;&#29616;&#26377;&#36335;&#30001;&#31574;&#30053;&#22987;&#32456;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#30340;&#19987;&#23478;&#23548;&#33268;&#30340;&#12290;&#20840;&#23545;&#20840;&#36807;&#31243;&#20013;&#39057;&#32321;&#30340;&#33410;&#28857;&#38388;&#36890;&#20449;&#20063;&#26174;&#33879;&#24310;&#38271;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#24615;&#33021;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#19987;&#23478;&#23481;&#37327;&#30340;&#26368;&#23567;&#38408;&#20540;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30340;&#38376;&#25511;&#26435;&#37325;&#19982;&#20998;&#37197;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#26368;&#22823;&#35282;&#20559;&#24046;&#35745;&#31639;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22240;&#26524;&#24863;&#30693;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#12290;&#24863;&#30693;&#23545;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#20844;&#24179;&#24615;&#26159;&#19982;&#32972;&#26223;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#20854;&#35299;&#37322;&#21462;&#20915;&#20110;&#35780;&#21028;&#20154;&#26159;&#35841;&#12290;</title><link>http://arxiv.org/abs/2401.13408</link><description>&lt;p&gt;
&#22240;&#26524;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Causal Perception. (arXiv:2401.13408v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13408
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22240;&#26524;&#24863;&#30693;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#12290;&#24863;&#30693;&#23545;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#20844;&#24179;&#24615;&#26159;&#19982;&#32972;&#26223;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#20854;&#35299;&#37322;&#21462;&#20915;&#20110;&#35780;&#21028;&#20154;&#26159;&#35841;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20004;&#20010;&#20010;&#20307;&#23545;&#30456;&#21516;&#30340;&#20449;&#24687;&#36827;&#34892;&#19981;&#21516;&#35299;&#35835;&#26102;&#65292;&#24863;&#30693;&#20250;&#21457;&#29983;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#20010;&#24050;&#30693;&#29616;&#35937;&#65292;&#23545;&#20915;&#31574;&#20013;&#20559;&#35265;&#26377;&#24433;&#21709;&#65292;&#20294;&#26159;&#24863;&#30693;&#22312;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#20173;&#28982;&#34987;&#24573;&#35270;&#12290;&#24863;&#30693;&#23545;&#20110;ADM&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#25110;&#20844;&#24179;&#20351;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#20844;&#24179;&#26412;&#36523;&#26159;&#19982;&#32972;&#26223;&#30456;&#20851;&#30340;&#65292;&#20854;&#35299;&#37322;&#21462;&#20915;&#20110;&#35780;&#21028;&#20154;&#26159;&#35841;&#12290;&#26412;&#25991;&#23558;&#24863;&#30693;&#22312;&#22240;&#26524;&#25512;&#29702;&#20013;&#24418;&#24335;&#21270;&#65292;&#20197;&#25429;&#25417;&#20010;&#20307;&#30340;&#35299;&#37322;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#23558;&#20010;&#20307;&#32463;&#39564;&#24418;&#24335;&#21270;&#20026;&#39069;&#22806;&#30340;&#22240;&#26524;&#30693;&#35782;&#65292;&#20010;&#20307;&#20250;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#21644;&#35752;&#35770;&#20102;&#26131;&#24341;&#21457;&#24863;&#30693;&#30340;&#23646;&#24615;&#65292;&#21363;&#26131;&#24341;&#21457;&#24863;&#30693;&#30340;&#23646;&#24615;&#12290;&#25935;&#24863;&#23646;&#24615;&#65292;&#22914;&#24615;&#21035;&#21644;&#31181;&#26063;&#65292;&#23601;&#26159;&#26131;&#24341;&#21457;&#24863;&#30693;&#30340;&#26126;&#30830;&#31034;&#20363;&#12290;&#25105;&#20204;&#26681;&#25454;&#22240;&#26524;&#21407;&#21017;&#23450;&#20041;&#20102;&#20004;&#31181;&#24863;&#30693;&#65292;&#21363;&#19981;&#24544;&#23454;&#24863;&#30693;&#21644;&#19981;&#19968;&#33268;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perception occurs when two individuals interpret the same information differently. Despite being a known phenomenon with implications for bias in decision-making, as individuals' experience determines interpretation, perception remains largely overlooked in automated decision-making (ADM) systems. In particular, it can have considerable effects on the fairness or fair usage of an ADM system, as fairness itself is context-specific and its interpretation dependent on who is judging. In this work, we formalize perception under causal reasoning to capture the act of interpretation by an individual. We also formalize individual experience as additional causal knowledge that comes with and is used by an individual. Further, we define and discuss loaded attributes, which are attributes prone to evoke perception. Sensitive attributes, such as gender and race, are clear examples of loaded attributes. We define two kinds of causal perception, unfaithful and inconsistent, based on the causal prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;AI&#35270;&#35273;&#27169;&#22411;&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20197;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#35299;&#37322;&#25928;&#29992;&#21644;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12261</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#20998;&#26512;AI&#35270;&#35273;&#27169;&#22411;&#30340;&#36136;&#37327;&#23646;&#24615;&#21450;&#20854;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Quality Attributes of AI Vision Models in Open Repositories Under Adversarial Attacks. (arXiv:2401.12261v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;AI&#35270;&#35273;&#27169;&#22411;&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20197;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#35299;&#37322;&#25928;&#29992;&#21644;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#32463;&#24120;&#21457;&#24067;&#21040;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#65292;&#22914;HuggingFace&#12290;&#22312;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#29983;&#20135;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20043;&#21069;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#36136;&#37327;&#20445;&#35777;&#39564;&#35777;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#38500;&#20102;&#35780;&#20272;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#30340;&#25928;&#29575;&#22806;&#65292;&#23545;&#25239;&#25915;&#20987;&#21487;&#33021;&#23545;AI&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26500;&#25104;&#23041;&#32961;&#12290;&#21516;&#26102;&#65292;&#21487;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#24212;&#29992;&#36817;&#20284;&#36755;&#20837;&#21040;&#36755;&#20986;&#30340;&#31639;&#27861;&#26469;&#35782;&#21035;&#36129;&#29486;&#29305;&#24449;&#12290;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#20250;&#38477;&#20302;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;XAI&#35299;&#37322;&#30340;&#25928;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#19979;&#28216;&#35780;&#20272;&#20219;&#21153;&#65292;&#21253;&#25324;&#39564;&#35777;AI&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#29992;&#22522;&#20934;&#25200;&#21160;&#35780;&#20272;&#40065;&#26834;&#24615;&#65292;&#27604;&#36739;&#35299;&#37322;&#25928;&#29992;&#20197;&#21450;&#35780;&#20272;&#24320;&#38144;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI models rapidly evolve, they are frequently released to open repositories, such as HuggingFace. It is essential to perform quality assurance validation on these models before integrating them into the production development lifecycle. In addition to evaluating efficiency in terms of balanced accuracy and computing costs, adversarial attacks are potential threats to the robustness and explainability of AI models. Meanwhile, XAI applies algorithms that approximate inputs to outputs post-hoc to identify the contributing features. Adversarial perturbations may also degrade the utility of XAI explanations that require further investigation. In this paper, we present an integrated process designed for downstream evaluation tasks, including validating AI model accuracy, evaluating robustness with benchmark perturbations, comparing explanation utility, and assessing overhead. We demonstrate an evaluation scenario involving six computer vision models, which include CNN-based, Transformer-b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#65292;&#29305;&#21035;&#20851;&#27880;&#21322;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#24182;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#39318;&#36873;&#39033;&#20010;&#24615;&#21270;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10938</link><description>&lt;p&gt;
&#21363;&#20351;&#35299;&#37322;&#65306;&#24418;&#24335;&#22522;&#30784;&#65292;&#20248;&#20808;&#32423;&#21644;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Even-if Explanations: Formal Foundations, Priorities and Complexity. (arXiv:2401.10938v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#65292;&#29305;&#21035;&#20851;&#27880;&#21322;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#24182;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#39318;&#36873;&#39033;&#20010;&#24615;&#21270;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20316;&#20026;&#40657;&#30418;&#23376;&#36816;&#34892;&#65292;&#32570;&#20047;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#65292;&#32780;&#21448;&#25903;&#25345;&#20915;&#31574;&#36807;&#31243;&#12290;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#35797;&#22270;&#22238;&#31572;&#20026;&#20160;&#20040;&#32473;&#23450;&#27169;&#22411;&#22914;&#20309;&#23545;&#20010;&#20307;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#20851;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#24050;&#32463;&#36827;&#34892;&#20102;&#37325;&#35201;&#24037;&#20316;&#65292;&#20294;&#23545;&#21322;&#20107;&#23454;&#35299;&#37322;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#21322;&#20107;&#23454;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#20197;&#21450;&#19981;&#21516;&#27169;&#22411;&#31867;&#21035;&#20013;&#20854;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#34920;&#26126;&#32447;&#24615;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#27604;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#20110;&#35299;&#37322;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#33258;&#24049;&#30340;&#20559;&#22909;&#20010;&#24615;&#21270;&#35299;&#37322;&#65292;&#26080;&#35770;&#26159;&#22312;&#21322;&#20107;&#23454;&#36824;&#26159;&#21453;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#35299;&#37322;&#33021;&#21147;&#21644;&#29992;&#25143;&#20013;&#24515;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXplainable AI has received significant attention in recent years. Machine learning models often operate as black boxes, lacking explainability and transparency while supporting decision-making processes. Local post-hoc explainability queries attempt to answer why individual inputs are classified in a certain way by a given model. While there has been important work on counterfactual explanations, less attention has been devoted to semifactual ones. In this paper, we focus on local post-hoc explainability queries within the semifactual `even-if' thinking and their computational complexity among different classes of models, and show that both linear and tree-based models are strictly more interpretable than neural networks. After this, we introduce a preference-based framework that enables users to personalize explanations based on their preferences, both in the case of semifactuals and counterfactuals, enhancing interpretability and user-centricity. Finally, we explore the complexity o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.10746</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#33041;&#30005;&#35299;&#30721;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#32463;&#24120;&#29992;&#20110;&#21508;&#31181;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#22823;&#37327;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#65292;&#36801;&#31227;&#23398;&#20064;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;DL&#27169;&#22411;&#12290;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#26159;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#65288;EA&#65289;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#20351;&#29992;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#24182;&#19988;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20854;&#23545;&#20849;&#20139;&#21644;&#20010;&#20307;DL&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;EA&#19982;DL&#30456;&#32467;&#21512;&#22312;&#35299;&#30721;BCI&#20449;&#21495;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;EA&#26469;&#35757;&#32451;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#26032;&#21463;&#35797;&#32773;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#23558;&#30446;&#26631;&#21463;&#35797;&#32773;&#30340;&#35299;&#30721;&#29575;&#25552;&#39640;&#20102;4.33&#65285;&#65292;&#24182;&#19988;&#25910;&#25947;&#26102;&#38388;&#32553;&#30701;&#20102;&#36229;&#36807;70&#65285;&#12290;&#25105;&#20204;&#36824;&#20026;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10463</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#36825;&#26159;&#20174;&#24555;&#36895;&#35760;&#24518;&#21040;&#32531;&#24930;&#27867;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#36716;&#21464;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30456;&#21464;&#24418;&#24335;&#21270;&#20026;Grokking&#37197;&#32622;&#19979;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#65292;&#24182;&#30830;&#23450;&#20102;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#20805;&#36275;&#21644;&#36807;&#21097;&#38454;&#27573;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#21021;&#22987;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;Grokking&#37197;&#32622;&#65292;&#31283;&#23450;&#22320;&#22312;&#31616;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#37325;&#29616;&#20102;Grokking&#12290;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26679;&#26412;&#32423;&#21644;&#27169;&#22411;&#32423;&#30340;Grokking&#65292;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#25968;&#25454;&#38598;&#22823;&#23567;&#22788;&#21457;&#29983;&#30340;&#26356;&#24179;&#28369;&#30340;&#30456;&#21464;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#36825;&#20010;&#20020;&#30028;&#28857;&#20063;&#21464;&#24471;&#26356;&#22823;&#65292;&#36825;&#34920;&#26126;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21152;&#28145;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#29702;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
&lt;/p&gt;</description></item><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09071</link><description>&lt;p&gt;
&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#37325;&#26032;&#24605;&#32771;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35770;&#19978;&#22312;&#35889;&#22495;&#20013;&#26377;&#24456;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19982;&#31354;&#38388;&#22495;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20174;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#35889;&#22270;GNN&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65292;&#20363;&#22914;&#65292;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#22495;&#20013;&#23454;&#38469;&#19978;&#32534;&#30721;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#30340;&#20869;&#22312;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#26126;&#30830;&#22320;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#36866;&#24212;&#24615;&#26032;&#22270;&#12290;&#29702;&#35770;&#21644;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26032;&#22270;&#19981;&#20165;&#34920;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#36824;&#33021;&#22815;&#23481;&#32435;&#26377;&#31526;&#21495;&#30340;&#36793;&#26435;&#37325;&#20197;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
&lt;/p&gt;</description></item><item><title>UOEP&#26159;&#19968;&#31181;&#29992;&#25143;&#23548;&#21521;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#21516;&#27963;&#36291;&#27700;&#24179;&#30340;&#29992;&#25143;&#32676;&#20307;&#23454;&#29616;&#32454;&#31890;&#24230;&#25506;&#32034;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#30340;&#38271;&#26399;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.09034</link><description>&lt;p&gt;
UOEP: &#29992;&#25143;&#23548;&#21521;&#30340;&#25506;&#32034;&#31574;&#30053;&#20197;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#29992;&#25143;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User Experiences in Recommender Systems. (arXiv:2401.09034v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09034
&lt;/p&gt;
&lt;p&gt;
UOEP&#26159;&#19968;&#31181;&#29992;&#25143;&#23548;&#21521;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#21516;&#27963;&#36291;&#27700;&#24179;&#30340;&#29992;&#25143;&#32676;&#20307;&#23454;&#29616;&#32454;&#31890;&#24230;&#25506;&#32034;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#30340;&#38271;&#26399;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26377;&#25928;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#20197;&#25552;&#21319;&#29992;&#25143;&#30340;&#38271;&#26399;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23384;&#22312;&#30528;&#25968;&#21315;&#19975;&#20010;&#39033;&#30446;&#20043;&#38388;&#30340;&#19981;&#21516;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#65292;&#36825;&#22686;&#21152;&#20102;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#20363;&#22914;&#65292;&#19981;&#21516;&#27963;&#36291;&#27700;&#24179;&#30340;&#29992;&#25143;&#34892;&#20026;&#38656;&#35201;&#19981;&#21516;&#24378;&#24230;&#30340;&#25506;&#32034;&#65292;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#24448;&#24448;&#24573;&#35270;&#20102;&#36825;&#19968;&#26041;&#38754;&#65292;&#23545;&#25152;&#26377;&#29992;&#25143;&#24212;&#29992;&#32479;&#19968;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#26368;&#32456;&#25439;&#23475;&#20102;&#29992;&#25143;&#30340;&#38271;&#26399;&#20307;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#25143;&#23548;&#21521;&#30340;&#25506;&#32034;&#31574;&#30053;&#65288;UOEP&#65289;&#65292;&#19968;&#31181;&#22312;&#29992;&#25143;&#32676;&#20307;&#20013;&#23454;&#29616;&#32454;&#31890;&#24230;&#25506;&#32034;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#35780;&#35770;&#23478;&#65292;&#23427;&#20801;&#35768;&#22312;&#19981;&#21516;&#30340;&#32047;&#31215;&#22870;&#21169;&#21453;&#39304;&#30340;&#20998;&#20301;&#25968;&#27700;&#24179;&#19979;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#34920;&#31034;&#20855;&#26377;&#19981;&#21516;&#27963;&#21160;&#27700;&#24179;&#30340;&#29992;&#25143;&#32676;&#20307;&#12290;&#22312;&#36825;&#20010;&#35780;&#35770;&#23478;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#19981;&#21516;&#30340;&#28436;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has gained traction for enhancing user long-term experiences in recommender systems by effectively exploring users' interests. However, modern recommender systems exhibit distinct user behavioral patterns among tens of millions of items, which increases the difficulty of exploration. For example, user behaviors with different activity levels require varying intensity of exploration, while previous studies often overlook this aspect and apply a uniform exploration strategy to all users, which ultimately hurts user experiences in the long run. To address these challenges, we propose User-Oriented Exploration Policy (UOEP), a novel approach facilitating fine-grained exploration among user groups. We first construct a distributional critic which allows policy optimization under varying quantile levels of cumulative reward feedbacks from users, representing user groups with varying activity levels. Guided by this critic, we devise a population of distinct actors 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#36830;&#25509;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#65292;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.01728</link><description>&lt;p&gt;
Ravnest: &#24322;&#26500;&#35774;&#22791;&#19978;&#30340;&#20998;&#25955;&#24335;&#24322;&#27493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices. (arXiv:2401.01728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#36830;&#25509;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#65292;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#12289;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#24322;&#24120;&#30340;&#27867;&#21270;&#21644;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#36235;&#21183;&#39044;&#35745;&#23558;&#32487;&#32493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#22686;&#22823;&#20351;&#24471;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#26041;&#27861;&#22312;&#36825;&#31181;&#35268;&#27169;&#19978;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#20102;&#36830;&#25509;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#26469;&#23454;&#29616;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;&#36825;&#20123;&#38598;&#32676;&#21442;&#19982;$\textit{&#38646;&#27668;&#27873;&#24322;&#27493;&#27169;&#22411;&#24182;&#34892;}$&#35757;&#32451;&#65292;&#21033;&#29992;$\textit{&#24182;&#34892;&#22810;&#29615;&#20840;&#23616;&#27719;&#32858;}$&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#36890;&#20449;&#21644;&#27169;&#22411;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets. This trend is expected to continue. However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales. This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics. Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model. These clusters engage in $\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#27169;&#22411;&#21450;&#20854;&#21069;&#39537;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;AGI&#24615;&#33021;&#12289;&#24191;&#27867;&#24615;&#21644;&#33258;&#20027;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#35821;&#35328;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#12289;&#35780;&#20272;&#39118;&#38505;&#65292;&#24182;&#34913;&#37327;&#22312;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2311.02462</link><description>&lt;p&gt;
AGI&#30340;&#23618;&#27425;&#65306;&#23558;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#21487;&#25805;&#20316;&#21270;
&lt;/p&gt;
&lt;p&gt;
Levels of AGI: Operationalizing Progress on the Path to AGI. (arXiv:2311.02462v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#27169;&#22411;&#21450;&#20854;&#21069;&#39537;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;AGI&#24615;&#33021;&#12289;&#24191;&#27867;&#24615;&#21644;&#33258;&#20027;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#35821;&#35328;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#12289;&#35780;&#20272;&#39118;&#38505;&#65292;&#24182;&#34913;&#37327;&#22312;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#27169;&#22411;&#21450;&#20854;&#21069;&#39537;&#30340;&#33021;&#21147;&#21644;&#34892;&#20026;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;AGI&#24615;&#33021;&#12289;&#24191;&#27867;&#24615;&#21644;&#33258;&#20027;&#24615;&#30340;&#23618;&#27425;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#20687;&#33258;&#21160;&#39550;&#39542;&#30340;&#23618;&#27425;&#19968;&#26679;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20849;&#21516;&#30340;&#35821;&#35328;&#26469;&#27604;&#36739;&#27169;&#22411;&#12289;&#35780;&#20272;&#39118;&#38505;&#65292;&#24182;&#34913;&#37327;&#22312;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#24320;&#21457;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;AGI&#23450;&#20041;&#65292;&#24182;&#25552;&#21462;&#20986;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;AGI&#26412;&#20307;&#35770;&#24212;&#28385;&#36275;&#30340;&#20845;&#20010;&#21407;&#21017;&#12290;&#36825;&#20123;&#21407;&#21017;&#21253;&#25324;&#20851;&#27880;&#33021;&#21147;&#32780;&#19981;&#26159;&#26426;&#21046;&#65307;&#20998;&#21035;&#35780;&#20272;&#24191;&#27867;&#24615;&#21644;&#24615;&#33021;&#65307;&#23450;&#20041;AGI&#36335;&#24452;&#19978;&#30340;&#38454;&#27573;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#32456;&#28857;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;AGI&#30340;&#23618;&#27425;&#8221;&#65292;&#26681;&#25454;&#33021;&#21147;&#30340;&#28145;&#24230;&#65288;&#24615;&#33021;&#65289;&#21644;&#24191;&#24230;&#65288;&#24191;&#27867;&#24615;&#65289;&#65292;&#24182;&#24605;&#32771;&#24403;&#21069;&#31995;&#32479;&#22914;&#20309;&#31526;&#21512;&#36825;&#20010;&#26412;&#20307;&#35770;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#23454;&#29616;AGI&#25152;&#25552;&#20986;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy. It is our hope that this framework will be useful in an analogous way to the levels of autonomous driving, by providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. These principles include focusing on capabilities rather than mechanisms; separately evaluating generality and performance; and defining stages along the path toward AGI, rather than focusing on the endpoint. With these principles in mind, we propose 'Levels of AGI' based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging req
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.17688</link><description>&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#26102;&#20195;&#31649;&#29702;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17688
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#20849;&#35782;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#22823;&#35268;&#27169;&#30340;&#31038;&#20250;&#21361;&#23475;&#21644;&#24694;&#24847;&#20351;&#29992;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22833;&#21435;&#25511;&#21046;&#30340;&#19981;&#21487;&#36870;&#36716;&#30340;&#25439;&#22833;&#12290;&#37492;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21644;&#25345;&#32493;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#21457;&#21644;&#27835;&#29702;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&amp;D and governance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11085</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25512;&#26029;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#35201;&#27714;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#25110;&#25512;&#26029;&#23427;&#20204;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;(2)&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#23545;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#30340;&#30740;&#31350;&#32773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#22312;&#28040;&#38500;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#24615;&#30340;&#20248;&#21183;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;DocRED&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;&#22240;&#26524;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20123;&#26681;&#26412;&#24615;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#31561;&#26377;&#30410;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11011</link><description>&lt;p&gt;
&#20174;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#34920;&#31034;&#21040;&#21487;&#25511;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#65306;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling. (arXiv:2310.11011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;&#22240;&#26524;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20123;&#26681;&#26412;&#24615;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#31561;&#26377;&#30410;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#23494;&#24230;&#20272;&#35745;&#21644;&#20174;&#26377;&#38480;&#26679;&#26412;&#20013;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#26681;&#26412;&#24615;&#30340;&#32570;&#28857;&#65292;&#22914;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#24341;&#20837;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#24046;&#21170;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21487;&#20197;&#23558;&#22240;&#26524;&#29702;&#35770;&#34701;&#20837;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#20013;&#12290;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#25551;&#36848;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#23545;&#31995;&#32479;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#21644;&#26426;&#21046;&#36827;&#34892;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#33258;&#28982;&#22320;&#32467;&#21512;&#12290;&#22240;&#26524;&#27169;&#22411;&#20026;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20960;&#20010;&#26377;&#30410;&#30340;&#23646;&#24615;&#65292;&#22914;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#32508;&#36848;&#65292;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have shown tremendous success in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, the tendency to induce spurious correlations, and poor out-of-distribution extrapolation. In an effort to remedy such challenges, one can incorporate the theory of causality in deep generative modeling. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interoperability. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual ge
&lt;/p&gt;</description></item><item><title>CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08753</link><description>&lt;p&gt;
CompA: &#35299;&#20915;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08753
&lt;/p&gt;
&lt;p&gt;
CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#30340;&#22522;&#26412;&#29305;&#24615;&#26159;&#20854;&#32452;&#21512;&#24615;&#12290;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;CLAP&#65289;&#35757;&#32451;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#33021;&#22815;&#23398;&#20064;&#38899;&#39057;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#12289;&#38899;&#39057;&#26816;&#32034;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26377;&#25928;&#25191;&#34892;&#32452;&#21512;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#36824;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CompA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#30495;&#23454;&#19990;&#30028;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CompA-order&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#25110;&#21457;&#29983;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#32780;CompA-attribute&#35780;&#20272;&#22768;&#38899;&#20107;&#20214;&#30340;&#23646;&#24615;&#32465;&#23450;&#12290;&#27599;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#21253;&#21547;&#20004;&#20010;&#38899;&#39057;-&#26631;&#39064;&#23545;&#65292;&#20854;&#20013;&#20004;&#20010;&#38899;&#39057;&#20855;&#26377;&#30456;&#21516;&#30340;&#22768;&#38899;&#20107;&#20214;&#65292;&#20294;&#32452;&#21512;&#26041;&#24335;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07259</link><description>&lt;p&gt;
&#25581;&#31034;&#38544;&#34255;&#30340;&#32852;&#31995;&#65306;&#29992;&#20110;&#35270;&#39057;&#23545;&#35805;&#30340;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog. (arXiv:2310.07259v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#35270;&#35273;&#38382;&#31572;&#30456;&#27604;&#65292;&#35270;&#39057;&#23545;&#35805;&#38656;&#35201;&#23545;&#23545;&#35805;&#21382;&#21490;&#21644;&#35270;&#39057;&#20869;&#23481;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#65292;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#36880;&#27493;&#29702;&#35299;&#22797;&#26434;&#30340;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#20197;&#36335;&#24452;&#36319;&#36394;&#21644;&#32858;&#21512;&#26426;&#21046;&#20026;&#26680;&#24515;&#65292;&#33021;&#22815;&#20174;&#23545;&#35805;&#21382;&#21490;&#20013;&#33719;&#21462;&#37325;&#35201;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#20197;&#35299;&#37322;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21033;&#29992;&#36845;&#20195;&#25512;&#29702;&#32593;&#32476;&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21644;&#24378;&#35843;&#20851;&#38190;&#35270;&#35273;&#26631;&#35760;&#65292;&#22686;&#24378;&#23545;&#35270;&#35273;&#29702;&#35299;&#30340;&#28145;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-&#27169;&#22411;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#20449;&#24687;&#32508;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to conventional visual question answering, video-grounded dialog necessitates a profound understanding of both dialog history and video content for accurate response generation. Despite commendable strides made by existing methodologies, they often grapple with the challenges of incrementally understanding intricate dialog histories and assimilating video information. In response to this gap, we present an iterative tracking and reasoning strategy that amalgamates a textual encoder, a visual encoder, and a generator. At its core, our textual encoder is fortified with a path tracking and aggregation mechanism, adept at gleaning nuances from dialog history that are pivotal to deciphering the posed questions. Concurrently, our visual encoder harnesses an iterative reasoning network, meticulously crafted to distill and emphasize critical visual markers from videos, enhancing the depth of visual comprehension. Culminating this enriched information, we employ the pre-trained GPT-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.03272</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#20256;&#36755;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#22270;&#20043;&#38388;&#24314;&#31435;&#19968;&#23545;&#19968;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#22312;&#39640;&#24433;&#21709;&#39046;&#22495;&#20013;&#26377;&#22823;&#37327;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#34987;&#35748;&#20026;&#26159;NP&#38590;&#30340;&#65292;&#32780;&#19988;&#29616;&#26377;&#30340;&#31639;&#27861;&#22312;&#22270;&#30340;&#35268;&#27169;&#22686;&#22823;&#26102;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#22270;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#21462;&#24378;&#22823;&#19988;&#40065;&#26834;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#40784;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#29983;&#25104;&#30340;&#23884;&#20837;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#65292;&#24182;&#19988;&#19982;&#32463;&#20856;&#35889;&#26041;&#27861;&#30456;&#27604;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32593;&#32476;&#23545;&#40784;&#21644;&#23376;&#32593;&#32476;&#23545;&#40784;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#25903;&#25345;&#35813;&#26694;&#26550;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GPT-4&#22312;&#26032;&#25968;&#25454;&#19978;&#25191;&#34892;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#22797;&#21046;&#30340;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#20854;&#25581;&#31034;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#26041;&#27861;&#20013;&#20551;&#35774;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.01727</link><description>&lt;p&gt;
GPT-4&#33021;&#21542;&#22797;&#21046;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GPT-4 Replicate Empirical Software Engineering Research?. (arXiv:2310.01727v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GPT-4&#22312;&#26032;&#25968;&#25454;&#19978;&#25191;&#34892;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#22797;&#21046;&#30340;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#20854;&#25581;&#31034;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#26041;&#27861;&#20013;&#20551;&#35774;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29983;&#20135;&#31995;&#32479;&#30340;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#20026;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#24102;&#26469;&#20102;&#23545;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#29983;&#20135;&#31995;&#32479;&#30340;&#19968;&#23567;&#37096;&#20998;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#38480;&#21046;&#20102;&#35813;&#30740;&#31350;&#30340;&#24433;&#21709;&#21147;&#12290;&#34429;&#28982;&#36719;&#20214;&#24037;&#31243;&#20174;&#19994;&#32773;&#21487;&#20197;&#36890;&#36807;&#22797;&#21046;&#30740;&#31350;&#26469;&#33719;&#24471;&#20805;&#23454;&#33258;&#24049;&#25968;&#25454;&#30340;&#22909;&#22788;&#65292;&#20294;&#36825;&#20063;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#22240;&#20026;&#25191;&#34892;&#22797;&#21046;&#38656;&#35201;&#23545;&#30740;&#31350;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#31243;&#25968;&#25454;&#20013;&#30340;&#24494;&#22937;&#32454;&#33410;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#22312;&#22788;&#29702;&#36719;&#20214;&#24037;&#31243;&#21644;&#31185;&#23398;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#25512;&#24191;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#26032;&#25968;&#25454;&#19978;&#25191;&#34892;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#22797;&#21046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#23427;&#20204;&#25581;&#31034;&#23454;&#35777;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#26041;&#27861;&#20013;&#25152;&#20570;&#30340;&#20551;&#35774;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help democratize empirical software engineering research.  In this paper, we examine LLMs' abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodolog
&lt;/p&gt;</description></item><item><title>&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17335</link><description>&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Graph Generators. (arXiv:2309.17335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17335
&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;AGG&#23558;&#35266;&#27979;&#20540;&#24314;&#27169;&#20026;&#21160;&#24577;&#22270;&#19978;&#30340;&#33410;&#28857;&#65292;&#24182;&#36890;&#36807;&#36716;&#23548;&#24335;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#12290;AGG&#19981;&#20381;&#36182;&#20110;&#24490;&#29615;&#32452;&#20214;&#25110;&#23545;&#26102;&#38388;&#35268;&#24459;&#30340;&#20551;&#35774;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#23558;&#27979;&#37327;&#20540;&#12289;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#30452;&#25509;&#34920;&#31034;&#22312;&#33410;&#28857;&#20013;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#26679;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#38544;&#24335;&#22320;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#22522;&#20110;&#26410;&#35265;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#23545;&#26032;&#30340;&#27979;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;AGG&#22312;&#27010;&#24565;&#21644;&#23454;&#35777;&#20004;&#26041;&#38754;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#31616;&#35201;&#35752;&#35770;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;AGG&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AGG&#22312;t
&lt;/p&gt;
&lt;p&gt;
We introduce the asynchronous graph generator (AGG), a novel graph neural network architecture for multi-channel time series which models observations as nodes on a dynamic graph and can thus perform data imputation by transductive node generation. Completely free from recurrent components or assumptions about temporal regularity, AGG represents measurements, timestamps and metadata directly in the nodes via learnable embeddings, to then leverage attention to learn expressive relationships across the variables of interest. This way, the proposed architecture implicitly learns a causal graph representation of sensor measurements which can be conditioned on unseen timestamps and metadata to predict new measurements by an expansion of the learnt graph. The proposed AGG is compared both conceptually and empirically to previous work, and the impact of data augmentation on the performance of AGG is also briefly discussed. Our experiments reveal that AGG achieved state-of-the-art results in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16391</link><description>&lt;p&gt;
&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;&#20108;&#32500;Copula&#36924;&#36817;&#21464;&#25442;&#65306;2-Cats&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copula&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#24212;&#29992;Copula&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#29420;&#31435;&#30340;&#36793;&#38469;&#20998;&#24067;&#65288;&#19968;&#20010;&#31616;&#21333;&#20219;&#21153;&#65289;&#65292;&#28982;&#21518;&#20272;&#35745;&#36830;&#25509;&#36793;&#38469;&#30340;&#21333;&#20010;Copula&#20989;&#25968;C&#65288;&#19968;&#20010;&#22256;&#38590;&#20219;&#21153;&#65289;&#26469;&#20272;&#35745;&#22810;&#20803;&#20998;&#24067;&#20989;&#25968;&#12290;&#23545;&#20110;&#20108;&#32500;&#25968;&#25454;&#65292;Copula&#26159;&#19968;&#20010;&#24418;&#22914;C&#65306;(u&#65292;v)&#8712;\mathbf{I}^2\rightarrow \mathbf{I}&#30340;&#20108;&#27425;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;\mathbf{I}=[0&#65292;1]&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22914;&#20309;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;2-Cats&#65292;&#21463;&#21040;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;Sobolev&#35757;&#32451;&#25991;&#29486;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19981;&#20165;&#35777;&#26126;&#20102;&#25105;&#20204;&#33021;&#22815;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#22320;&#20272;&#35745;2D Copula&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19988;&#31526;&#21512;Copula C&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.13744</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#20013;&#30340;&#24212;&#29992;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Literature Review of Computer Vision Applications in Robotized Wire Harness Assembly. (arXiv:2309.13744v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#20013;&#30340;&#24212;&#29992;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20174;&#29616;&#26377;&#30740;&#31350;&#20013;&#25552;&#21462;&#20102;&#25361;&#25112;&#65292;&#24182;&#25214;&#21040;&#20102;&#26410;&#26469;&#30740;&#31350;&#20419;&#36827;&#26356;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a systematic literature review on computer vision applications that have been proposed for robotized wire harness assembly, derives challenges from existing studies, and identifies opportunities for future research to promote a more practical robotized assembly of wire harnesses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>RecycleGPT&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#22810;&#27425;&#36816;&#34892;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#39640;&#36895;&#35299;&#30721;&#21644;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03421</link><description>&lt;p&gt;
RecycleGPT&#65306;&#19968;&#31181;&#20855;&#26377;&#21487;&#22238;&#25910;&#27169;&#22359;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RecycleGPT: An Autoregressive Language Model with Recyclable Module. (arXiv:2308.03421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03421
&lt;/p&gt;
&lt;p&gt;
RecycleGPT&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#22810;&#27425;&#36816;&#34892;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#39640;&#36895;&#35299;&#30721;&#21644;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24517;&#39035;&#36816;&#34892;K&#27425;&#25165;&#33021;&#29983;&#25104;K&#20010;&#20196;&#29260;&#30340;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RecycleGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#35299;&#30721;&#36895;&#24230;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#23558;&#25972;&#20010;&#27169;&#22411;&#36816;&#34892;&#22810;&#27425;&#27493;&#39588;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#65306;&#24207;&#21015;&#20013;&#30456;&#37051;&#30340;&#20196;&#29260;&#36890;&#24120;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#21069;&#38754;&#30340;&#20196;&#29260;&#21512;&#29702;&#29468;&#27979;&#25110;&#25512;&#26029;&#20986;&#19979;&#19968;&#20010;&#20196;&#29260;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;1.4&#20493;&#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models have to run K times to generate a sequence of K tokens. In this paper, we present RecycleGPT, a generative language model with fast decoding speed by recycling pre-generated model states without running the whole model in multiple steps. Our approach relies on the observation that adjacent tokens in a sequence usually have strong correlations and the next token in a sequence can be reasonably guessed or inferred based on the preceding ones. Experiments and analysis demonstrate the effectiveness of our approach in lowering inference latency, achieving up to 1.4x speedup while preserving high performance.
&lt;/p&gt;</description></item><item><title>LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01413</link><description>&lt;p&gt;
LaFiCMIL&#65306;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22823;&#25991;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01413
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#12290;&#30452;&#35266;&#19978;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26399;&#26395;&#25991;&#26412;&#20998;&#31867;&#65292;&#20316;&#20026;&#19981;&#38656;&#35201;&#20687;&#29983;&#25104;&#20219;&#21153;&#37027;&#26679;&#35768;&#22810;&#39640;&#32423;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;Transformer&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22312;&#22810;&#31867;&#21035;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#38271;&#25991;&#26412;&#25991;&#26723;&#21644;&#20854;&#20182;&#22823;&#25991;&#20214;&#30340;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#12290;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#21463;&#21040;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#30340;&#38459;&#30861;&#65306;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#27604;&#22914;BERT&#30340;512&#20010;&#26631;&#35760;&#12290;&#34429;&#28982;&#22686;&#21152;GPU&#20869;&#23384;&#21487;&#20197;&#31245;&#24494;&#25193;&#23637;&#36825;&#20010;&#38480;&#21046;&#65292;&#20294;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;GPU&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#36755;&#20837;&#38480;&#21046;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;LaFiCMIL&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have revolutionized the performance of a wide range of language tasks. Intuitively, one might expect text classification, which does not necessitate as many high-level representations as generative tasks, to be comprehensively addressed with the powerful representation capabilities of Transformers. However, in reality, there remains significant potential for enhancement, particularly in the areas of multi-class and multi-label classification of lengthy textual documents and other large files. The performance of Transformer-based models is mainly hindered by a major limitation: a restricted input length, e.g., 512 tokens for BERT. While an increase in GPU memory can marginally extend this limit, practical real-world applications often operate under constrained GPU resources. In this work, we tackle the input limit problem from the perspective of correlated multiple instance learning. The proposed approach, LaFiCMIL, serves as a versatile framework applicable to 
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.13565</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65306;&#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#22522;&#20934;&#21644;&#26410;&#26469;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13565
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#35757;&#32451;&#27169;&#22411;&#20197;&#20248;&#21270;&#20915;&#31574;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20010;&#33539;&#24335;&#26377;&#26395;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20915;&#31574;&#21046;&#23450;&#65292;&#36825;&#20123;&#24212;&#29992;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20123;&#20915;&#31574;&#27169;&#22411;&#20013;&#20272;&#35745;&#26410;&#30693;&#21442;&#25968;&#32463;&#24120;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#23545;DFL&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#23427;&#23545;&#21508;&#31181;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26681;&#25454;&#20854;&#29420;&#29305;&#29305;&#24449;&#26469;&#21306;&#20998;DFL&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;DFL&#30340;&#21512;&#36866;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;DFL&#30740;&#31350;&#20013;&#24403;&#21069;&#21644;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#35774;&#35745;&#30340;&#30456;&#20851;&#35201;&#32032;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.07255</link><description>&lt;p&gt;
&#23545;&#35805;&#20195;&#29702;101&#65306;&#35774;&#35745;&#26377;&#25928;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#32032;&#21021;&#23398;&#32773;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems. (arXiv:2307.07255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#35774;&#35745;&#30340;&#30456;&#20851;&#35201;&#32032;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;&#21516;&#34892;&#36827;&#34892;&#20132;&#27969;&#26469;&#20998;&#20139;&#24819;&#27861;&#26159;&#20154;&#31867;&#20114;&#21160;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#22312;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#23548;&#33268;&#23545;&#35805;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#20010;&#20219;&#21153;&#21516;&#26102;&#25506;&#32034;&#65292;&#24403;&#21069;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#29616;&#29366;&#21464;&#24471;&#20998;&#25955;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24110;&#21161;&#20174;&#38646;&#24320;&#22987;&#35774;&#35745;&#23545;&#35805;&#20195;&#29702;&#30340;&#20174;&#19994;&#32773;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#30456;&#24212;&#30340;&#24320;&#25918;&#39046;&#22495;&#25968;&#25454;&#38598;&#20197;&#21450;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharing ideas through communication with peers is the primary mode of human interaction. Consequently, extensive research has been conducted in the area of conversational AI, leading to an increase in the availability and diversity of conversational tasks, datasets, and methods. However, with numerous tasks being explored simultaneously, the current landscape of conversational AI becomes fragmented. Therefore, initiating a well-thought-out model for a dialogue agent can pose significant challenges for a practitioner. Towards highlighting the critical ingredients needed for a practitioner to design a dialogue agent from scratch, the current study provides a comprehensive overview of the primary characteristics of a dialogue agent, the supporting tasks, their corresponding open-domain datasets, and the methods used to benchmark these datasets. We observe that different methods have been used to tackle distinct dialogue tasks. However, building separate models for each task is costly and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#21644;&#23450;&#37327;&#24515;&#32908;&#32452;&#32455;&#28814;&#30151;&#28024;&#28070;&#22312;&#25968;&#23383;HE&#26579;&#33394;&#22270;&#20687;&#20013;&#65292;&#20026;&#24515;&#32908;&#28814;&#25552;&#20379;&#23450;&#37327;&#32452;&#32455;&#23398;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25351;&#26631;LND&#30340;&#23450;&#37327;&#26469;&#21028;&#26029;&#26159;&#21542;&#23384;&#22312;&#24515;&#32908;&#28814;&#30151;&#28024;&#28070;&#12290;</title><link>http://arxiv.org/abs/2307.01098</link><description>&lt;p&gt;
&#33258;&#21160;&#35782;&#21035;&#21644;&#23450;&#37327;&#24515;&#32908;&#32452;&#32455;&#28814;&#30151;&#28024;&#28070;&#22312;&#25968;&#23383;&#32452;&#32455;&#20999;&#29255;&#22270;&#20687;&#20013;&#20197;&#35786;&#26029;&#24515;&#32908;&#28814;
&lt;/p&gt;
&lt;p&gt;
Automated identification and quantification of myocardial inflammatory infiltration in digital histological images to diagnose myocarditis. (arXiv:2307.01098v1 [physics.med-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#21644;&#23450;&#37327;&#24515;&#32908;&#32452;&#32455;&#28814;&#30151;&#28024;&#28070;&#22312;&#25968;&#23383;HE&#26579;&#33394;&#22270;&#20687;&#20013;&#65292;&#20026;&#24515;&#32908;&#28814;&#25552;&#20379;&#23450;&#37327;&#32452;&#32455;&#23398;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25351;&#26631;LND&#30340;&#23450;&#37327;&#26469;&#21028;&#26029;&#26159;&#21542;&#23384;&#22312;&#24515;&#32908;&#28814;&#30151;&#28024;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#26041;&#27861;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#23450;&#37327;&#24515;&#32908;&#32452;&#32455;&#28814;&#30151;&#28024;&#28070;&#22312;&#25968;&#23383;HE&#26579;&#33394;&#22270;&#20687;&#20013;&#65292;&#20197;&#25552;&#20379;&#24515;&#32908;&#28814;&#30340;&#23450;&#37327;&#32452;&#32455;&#23398;&#35786;&#26029;&#12290;&#26412;&#30740;&#31350;&#21253;&#25324;154&#21517;&#24515;&#33039;&#31227;&#26893;&#30149;&#20154;&#35786;&#26029;&#20026;&#24515;&#32908;&#28814;&#25110;&#25193;&#24352;&#22411;&#24515;&#32908;&#30149;&#30340;898&#20010;HE&#26579;&#33394;&#20840;&#20999;&#29255;&#22270;&#20687;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;DL&#30340;&#33258;&#21160;&#35745;&#31639;&#30149;&#29702;&#23398;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#32454;&#32990;&#26680;&#24182;&#26816;&#27979;&#24515;&#32908;&#32452;&#32455;&#28814;&#30151;&#28024;&#28070;&#65292;&#20351;&#24471;&#21487;&#20197;&#23450;&#37327;&#27979;&#37327;&#24515;&#32908;&#20840;&#20999;&#29255;&#22270;&#20687;&#19978;&#30340;&#28107;&#24052;&#32454;&#32990;&#26680;&#23494;&#24230;&#65288;LND&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LND&#23450;&#37327;&#30340;&#25130;&#26029;&#20540;&#26469;&#30830;&#23450;&#26159;&#21542;&#23384;&#22312;&#24515;&#32908;&#28814;&#30151;&#28024;&#28070;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#23454;&#39564;&#35780;&#20272;&#24615;&#33021;&#65292;&#20351;&#29992;&#26469;&#33258;&#24515;&#32908;&#28814;&#32452;&#30340;&#20869;&#37096;&#27979;&#35797;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21452;&#30450;&#35797;&#39564;&#30340;&#22806;&#37096;&#27979;&#35797;&#36827;&#34892;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to develop a new computational pathology approach that automates the identification and quantification of myocardial inflammatory infiltration in digital HE-stained images to provide a quantitative histological diagnosis of myocarditis.898 HE-stained whole slide images (WSIs) of myocardium from 154 heart transplant patients diagnosed with myocarditis or dilated cardiomyopathy (DCM) were included in this study. An automated DL-based computational pathology approach was developed to identify nuclei and detect myocardial inflammatory infiltration, enabling the quantification of the lymphocyte nuclear density (LND) on myocardial WSIs. A cutoff value based on the quantification of LND was proposed to determine if the myocardial inflammatory infiltration was present. The performance of our approach was evaluated with a five-fold cross-validation experiment, tested with an internal test set from the myocarditis group, and confirmed by an external test from a double-blind trial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.06081</link><description>&lt;p&gt;
CARSO: &#23545;&#25239;&#24615;&#21512;&#25104;&#35266;&#27979;&#30340;&#21453;&#23545;&#25239;&#24615;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#32447;&#32034;&#12290;&#35813;&#26041;&#27861;&#19982;&#23545;&#25239;&#35757;&#32451;&#20855;&#26377;&#21327;&#21516;&#20114;&#34917;&#24615;&#65292;&#24182;&#20381;&#36182;&#20110;&#34987;&#25915;&#20987;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#35813;&#26041;&#27861;&#37319;&#26679;&#36755;&#20837;&#30340;&#37325;&#26500;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#22312;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CARSO&#33021;&#22815;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#8212;&#8212;&#21516;&#26102;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#28165;&#27905;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#20307;&#31995;&#32467;&#26500;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14383</link><description>&lt;p&gt;
&#19968;&#20010;&#38477;&#32500;&#20154;&#31867;&#20998;&#31867;&#30340;&#29702;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Rational Model of Dimension-reduced Human Categorization. (arXiv:2305.14383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14383
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#20013;&#29616;&#26377;&#30340;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#20154;&#31867;&#22312;&#24515;&#29702;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#32423;&#27010;&#25324;&#34892;&#20026;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#31867;&#21035;&#34920;&#31034;&#21487;&#33021;&#20250;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#20154;&#20204;&#19968;&#33324;&#20381;&#36182;&#20110;&#19968;&#32452;&#21487;&#34892;&#20294;&#36275;&#22815;&#30340;&#29305;&#24449;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#27010;&#29575;&#20027;&#25104;&#20998;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#32463;&#27982;&#30340;&#29305;&#24449;&#38598;&#21512;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#20154;&#31867;&#20998;&#31867;&#20013;&#30340;&#32500;&#24230;&#20559;&#24046;&#24182;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20869;&#21033;&#29992;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#39640;&#32500;&#21050;&#28608;&#19979;&#26356;&#22909;&#30340;&#20998;&#31867;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing models in cognitive science typically assume human categorization as graded generalization behavior in a multidimensional psychological space. However, category representations in these models may suffer from the curse of dimensionality in a natural setting. People generally rely on a tractable yet sufficient set of features to understand the complex environment. We propose a rational model of categorization based on a hierarchical mixture of probabilistic principal components, that simultaneously learn category representations and an economical collection of features. The model captures dimensional biases in human categorization and supports zero-shot learning. We further exploit a generative process within a low-dimensional latent space to provide a better account of categorization with high-dimensional stimuli. We validate the model with simulation and behavioral experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#65292;&#20197;&#26356;&#21152;&#29616;&#23454;&#30340;&#26041;&#24335;&#35780;&#20272;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#12290;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04795</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#35780;&#20272;&#19979;&#37325;&#26032;&#23457;&#35270;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Revisiting Test Time Adaptation under Online Evaluation. (arXiv:2304.04795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#65292;&#20197;&#26356;&#21152;&#29616;&#23454;&#30340;&#26041;&#24335;&#35780;&#20272;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#12290;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#12290;TTA&#26041;&#27861;&#21033;&#29992;&#27979;&#35797;&#26102;&#38388;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#36866;&#24212;&#20998;&#24067;&#31227;&#20301;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#24778;&#20154;&#30340;&#24615;&#33021;&#36890;&#24120;&#35201;&#20197;&#26174;&#30528;&#22686;&#21152;&#30340;&#35745;&#31639;&#39044;&#31639;&#20026;&#20195;&#20215;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#20102;&#36825;&#31181;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#30340;&#24433;&#21709;&#65292;&#24433;&#21709;&#23427;&#20204;&#22312;&#23454;&#38469;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#29616;&#23454;&#30340;TTA&#26041;&#27861;&#35780;&#20272;&#21327;&#35758;&#65292;&#22312;&#36825;&#20010;&#21327;&#35758;&#20013;&#25968;&#25454;&#20197;&#24658;&#23450;&#36895;&#29575;&#20174;&#25968;&#25454;&#27969;&#20013;&#22312;&#32447;&#25509;&#25910;&#65292;&#20174;&#32780;&#32771;&#34385;&#21040;&#26041;&#27861;&#30340;&#36866;&#24212;&#36895;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#24212;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#23545;&#22810;&#31181;TTA&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods, which penalizes slower methods by providing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Though many effective methods have been proposed, their impressive performance usually comes at the cost of significantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA methods, where data is received in an online fashion from a constant-speed data stream, thereby accounting for the method's adaptation speed. We apply our proposed protocol to benchmark several TTA methods on multiple datasets and scenarios. Extensive experiments shows that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2211.09172</link><description>&lt;p&gt;
&#25991;&#23383;&#23545;&#35805;&#20013;&#30340;&#28145;&#24230;&#24773;&#24863;&#35782;&#21035;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#24180;&#26469;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#26032;&#30340;&#24212;&#29992;&#21644;&#23454;&#26045;&#22330;&#26223;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#21033;&#29992;&#23545;&#35805;&#35821;&#22659;&#12289;&#35828;&#35805;&#20154;&#21644;&#24773;&#24863;&#21160;&#24577;&#24314;&#27169;&#65292;&#35299;&#37322;&#24120;&#35782;&#34920;&#36798;&#12289;&#38750;&#27491;&#24335;&#35821;&#35328;&#21644;&#35773;&#21050;&#65292;&#24212;&#23545;&#23454;&#26102;&#24773;&#24863;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#35782;&#21035;&#24773;&#24863;&#21407;&#22240;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#31181;&#20998;&#31867;&#27861;&#65292;&#22810;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#20197;&#21450;&#35299;&#37322;&#24615;&#12290;&#26412;&#35843;&#30740;&#39318;&#20808;&#20171;&#32461;&#20102;&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#19982;&#27492;&#20219;&#21153;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#28982;&#21518;&#65292;&#23427;&#20171;&#32461;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#22810;&#31181;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#24773;&#24863;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#25551;&#36848;&#20102;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#65292;&#24182;&#35299;&#37322;&#20102;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#26368;&#21518;&#65292;&#23427;&#25552;&#20379;&#20102;&#23545;&#20110;&#26356;&#22909;&#30340;&#26694;&#26550;&#30340;&#24314;&#35758;&#24615;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#22788;&#29702;&#20027;&#35266;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Emotion Recognition in Conversations (ERC) has seen a tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker and emotion dynamics modelling, to interpreting common sense expressions, informal language and sarcasm, addressing challenges of real time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC to interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities pertaining to this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions of the most prominent works in ERC with explanations of the Deep Learning architectures employed. Then, it provides advisable ERC practices towards better frameworks, elaborating on methods to deal with subjectivity in ann
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#26469;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32452;&#20214;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#20840;&#23616;&#31354;&#38388;&#21644;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.07482</link><description>&lt;p&gt;
&#29992;&#24352;&#37327;&#32593;&#32476;&#24418;&#24335;&#32479;&#19968;O(3)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Unifying O(3) Equivariant Neural Networks Design with Tensor-Network Formalism. (arXiv:2211.07482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#26469;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32452;&#20214;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#20840;&#23616;&#31354;&#38388;&#21644;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#20174;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#20013;&#23398;&#20064;&#21183;&#33021;&#38754;&#65292;&#28041;&#21450;&#21040;&#20840;&#23616;&#31354;&#38388;&#23545;&#31216;&#24615;&#21644;&#21407;&#23376;&#25110;&#19968;&#33324;&#31890;&#23376;&#20043;&#38388;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#12290;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#20043;&#19968;&#65292;&#20854;&#20013;&#26368;&#25104;&#21151;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#20351;&#29992;&#22312;&#31354;&#38388;&#32676;&#19979;&#21464;&#25442;&#30340;&#21508;&#31181;&#24352;&#37327;&#20043;&#38388;&#30340;&#24352;&#37327;&#31215;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#19981;&#21516;&#24352;&#37327;&#30340;&#25968;&#37327;&#21644;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20445;&#25345;&#31616;&#27905;&#21644;&#31561;&#21464;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#65292;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#27169;&#25311;SU(2)&#23545;&#31216;&#37327;&#23376;&#22810;&#20307;&#38382;&#39064;&#30340;&#25216;&#26415;&#65292;&#26469;&#20026;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#26032;&#30340;&#31561;&#21464;&#32452;&#20214;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#24403;&#24212;&#29992;&#20110;&#32473;&#23450;&#23616;&#37096;&#37051;&#22495;&#20013;&#30340;&#31890;&#23376;&#26102;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#34701;&#21512;&#22359;&#8221;&#30340;&#32467;&#26524;&#32452;&#20214;&#36215;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
Many learning tasks, including learning potential energy surfaces from ab initio calculations, involve global spatial symmetries and permutational symmetry between atoms or general particles. Equivariant graph neural networks are a standard approach to such problems, with one of the most successful methods employing tensor products between various tensors that transform under the spatial group. However, as the number of different tensors and the complexity of relationships between them increase, maintaining parsimony and equivariance becomes increasingly challenging. In this paper, we propose using fusion diagrams, a technique widely employed in simulating SU($2$)-symmetric quantum many-body problems, to design new equivariant components for equivariant neural networks. This results in a diagrammatic approach to constructing novel neural network architectures. When applied to particles within a given local neighborhood, the resulting components, which we term "fusion blocks," serve as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#23545;&#20110;Manifold-Mixup&#26041;&#27861;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#36234;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2210.03123</link><description>&lt;p&gt;
&#28151;&#21512;&#27744;&#21270;&#22312;&#22522;&#20110;Mixup&#30340;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Hybrid Pooling in Mixup-Based Graph Learning for Language Processing. (arXiv:2210.03123v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#23545;&#20110;Manifold-Mixup&#26041;&#27861;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#36234;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#22270;&#24418;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#21644;&#28304;&#20195;&#30721;&#20998;&#31867;&#26041;&#38754;&#12290;&#36890;&#24120;&#65292;GNN&#26159;&#30001;&#20132;&#26367;&#22270;&#23618;&#21644;&#22270;&#27744;&#21270;&#23618;&#26500;&#25104;&#30340;&#65292;&#20132;&#26367;&#22270;&#23618;&#21487;&#20197;&#23398;&#20064;&#22270;&#33410;&#28857;&#29305;&#24449;&#30340;&#36716;&#25442;&#65292;&#32780;&#22270;&#27744;&#21270;&#23618;&#21017;&#20351;&#29992;&#22270;&#27744;&#21270;&#31639;&#23376;&#65288;&#20363;&#22914;Max&#27744;&#21270;&#65289;&#26377;&#25928;&#22320;&#20943;&#23569;&#33410;&#28857;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#22270;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#22686;&#24378;GNN&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20154;&#20204;&#24191;&#27867;&#37319;&#29992;&#20102;Manifold-Mixup&#36825;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#32447;&#24615;&#28151;&#21512;&#19968;&#23545;&#22270;&#25968;&#25454;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#26469;&#29983;&#25104;&#21512;&#25104;&#22270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;Manifold-Mixup&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#22270;&#27744;&#21270;&#31639;&#23376;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#24182;&#27809;&#26377;&#36827;&#34892;&#24456;&#22810;&#20851;&#20110;&#36825;&#31181;&#24433;&#21709;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#26089;&#26399;&#25506;&#32034;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;Mixup&#30340;&#22270;&#24418;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;Max-pooling&#21644;Attention-pooling&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#26412;&#22320;&#21644;&#20840;&#23616;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#27744;&#21270;&#32467;&#26500;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN)-based graph learning has been popular in natural language and programming language processing, particularly in text and source code classification. Typically, GNNs are constructed by incorporating alternating layers which learn transformations of graph node features, along with graph pooling layers that use graph pooling operators (e.g., Max-pooling) to effectively reduce the number of nodes while preserving the semantic information of the graph. Recently, to enhance GNNs in graph learning tasks, Manifold-Mixup, a data augmentation technique that produces synthetic graph data by linearly mixing a pair of graph data and their labels, has been widely adopted. However, the performance of Manifold-Mixup can be highly affected by graph pooling operators, and there have not been many studies that are dedicated to uncovering such affection. To bridge this gap, we take an early step to explore how graph pooling operators affect the performance of Mixup-based graph le
&lt;/p&gt;</description></item><item><title>GeONet&#26159;&#19968;&#20010;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23398;&#20064;&#20102;&#20174;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#36890;&#36807;&#23398;&#20064;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#65292;GeONet&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#24182;&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.14440</link><description>&lt;p&gt;
GeONet&#65306;&#19968;&#31181;&#23398;&#20064;Wasserstein&#27979;&#22320;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14440
&lt;/p&gt;
&lt;p&gt;
GeONet&#26159;&#19968;&#20010;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23398;&#20064;&#20102;&#20174;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#36890;&#36807;&#23398;&#20064;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#65292;GeONet&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#24182;&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;(OT)&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20960;&#20309;&#19978;&#26377;&#24847;&#20041;&#27604;&#36739;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;&#35745;&#31639;&#27010;&#29575;&#27979;&#24230;&#30340;Wasserstein&#36317;&#31163;&#21644;&#27979;&#22320;&#30340;&#26041;&#27861;&#38656;&#35201;&#32593;&#26684;&#20381;&#36182;&#30340;&#22495;&#31163;&#25955;&#21270;&#65292;&#21516;&#26102;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GeONet&#65292;&#19968;&#31181;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23427;&#23398;&#20064;&#20102;&#23558;&#36755;&#20837;&#30340;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#26144;&#23556;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#22312;&#33073;&#26426;&#35757;&#32451;&#38454;&#27573;&#65292;GeONet&#36890;&#36807;&#32806;&#21512;&#30340;PDE&#31995;&#32479;&#34920;&#24449;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#26368;&#20248;&#26465;&#20214;&#23398;&#20064;&#20102;OT&#38382;&#39064;&#30340;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#12290;&#21518;&#32493;&#30340;&#25512;&#29702;&#38454;&#27573;&#26159;&#30636;&#26102;&#23436;&#25104;&#30340;&#65292;&#24182;&#21487;&#20197;&#22312;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#20013;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GeONet&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) offers a versatile framework to compare complex data distributions in a geometrically meaningful way. Traditional methods for computing the Wasserstein distance and geodesic between probability measures require mesh-dependent domain discretization and suffer from the curse-of-dimensionality. We present GeONet, a mesh-invariant deep neural operator network that learns the non-linear mapping from the input pair of initial and terminal distributions to the Wasserstein geodesic connecting the two endpoint distributions. In the offline training stage, GeONet learns the saddle point optimality conditions for the dynamic formulation of the OT problem in the primal and dual spaces that are characterized by a coupled PDE system. The subsequent inference stage is instantaneous and can be deployed for real-time predictions in the online learning setting. We demonstrate that GeONet achieves comparable testing accuracy to the standard OT solvers on simulation examples and the
&lt;/p&gt;</description></item></channel></rss>