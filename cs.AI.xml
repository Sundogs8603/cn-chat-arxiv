<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ID&#31639;&#27861;&#35299;&#20915;&#20102;&#22270;&#24418;&#22240;&#26524;&#27169;&#22411;&#20013;&#24178;&#39044;&#20998;&#24067;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#20294;&#24403;&#36755;&#20837;&#20998;&#24067;&#26080;&#27861;&#35782;&#21035;&#26102;&#65292;&#25152;&#35859;&#30340;"hedge&#20934;&#21017;"&#26080;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.03750</link><description>&lt;p&gt;
ID&#31639;&#27861;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#22833;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
When does the ID algorithm fail?. (arXiv:2307.03750v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03750
&lt;/p&gt;
&lt;p&gt;
ID&#31639;&#27861;&#35299;&#20915;&#20102;&#22270;&#24418;&#22240;&#26524;&#27169;&#22411;&#20013;&#24178;&#39044;&#20998;&#24067;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#20294;&#24403;&#36755;&#20837;&#20998;&#24067;&#26080;&#27861;&#35782;&#21035;&#26102;&#65292;&#25152;&#35859;&#30340;"hedge&#20934;&#21017;"&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ID&#31639;&#27861;&#35299;&#20915;&#20102;&#22270;&#24418;&#22240;&#26524;&#27169;&#22411;&#20013;&#24418;&#24335;&#20026;p(Y | do(a))&#30340;&#24178;&#39044;&#20998;&#24067;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#19988;&#24050;&#32463;&#20197;&#22810;&#31181;&#26041;&#24335;&#36827;&#34892;&#20102;&#20844;&#24335;&#21270;&#12290;ID&#31639;&#27861;&#26159;&#27491;&#30830;&#30340;&#65288;&#22312;&#36755;&#20837;&#22270;&#25152;&#20195;&#34920;&#30340;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#36755;&#20986;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#27491;&#30830;&#21151;&#33021;&#65289;&#65292;&#24182;&#19988;&#26159;&#23436;&#25972;&#30340;&#65288;&#24403;&#36755;&#20837;&#30340;p(Y | do(a))&#22312;&#36755;&#20837;&#22270;&#25152;&#20195;&#34920;&#30340;&#22240;&#26524;&#27169;&#22411;&#20013;&#26080;&#27861;&#35782;&#21035;&#26102;&#65292;&#26126;&#30830;&#26631;&#35760;&#20026;&#22833;&#36133;&#65289;&#12290;&#21442;&#32771;&#25991;&#29486;[9]&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26524;&#65292;&#21363;&#25152;&#35859;&#30340;"hedge&#20934;&#21017;"&#65288;Corollary 3&#65289;&#65292;&#26088;&#22312;&#20197;&#36755;&#20837;&#22270;&#20013;&#30340;&#19968;&#20010;&#31216;&#20026;"hedge"&#30340;&#32467;&#26500;&#26469;&#32473;&#20986;ID&#31639;&#27861;&#26080;&#27861;&#35782;&#21035;&#20854;&#36755;&#20837;&#30340;&#24773;&#20917;&#30340;&#22270;&#24418;&#29305;&#24449;&#12290;&#34429;&#28982;ID&#31639;&#27861;&#30830;&#23454;&#26159;&#19968;&#20010;&#27491;&#30830;&#21644;&#23436;&#25972;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#21482;&#35201;&#36755;&#20837;&#20998;&#24067;&#26080;&#27861;&#35782;&#21035;&#65292;"hedge"&#32467;&#26500;&#23601;&#20250;&#20986;&#29616;&#65292;&#20294;&#26159;&#22312;[9]&#20013;&#25552;&#20986;&#30340;Corollary 3&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ID algorithm solves the problem of identification of interventional distributions of the form p(Y | do(a)) in graphical causal models, and has been formulated in a number of ways [12, 9, 6]. The ID algorithm is sound (outputs the correct functional of the observed data distribution whenever p(Y | do(a)) is identified in the causal model represented by the input graph), and complete (explicitly flags as a failure any input p(Y | do(a)) whenever this distribution is not identified in the causal model represented by the input graph).  The reference [9] provides a result, the so called "hedge criterion" (Corollary 3), which aims to give a graphical characterization of situations when the ID algorithm fails to identify its input in terms of a structure in the input graph called the hedge. While the ID algorithm is, indeed, a sound and complete algorithm, and the hedge structure does arise whenever the input distribution is not identified, Corollary 3 presented in [9] is incorrect as sta
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#26631;&#20934;&#21046;&#23450;&#12289;&#27880;&#20876;&#25253;&#21578;&#21644;&#23433;&#20840;&#21512;&#35268;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03718</link><description>&lt;p&gt;
&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#65306;&#31649;&#29702;&#23545;&#20844;&#20849;&#23433;&#20840;&#30340;&#26032;&#20852;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Frontier AI Regulation: Managing Emerging Risks to Public Safety. (arXiv:2307.03718v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03718
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#26631;&#20934;&#21046;&#23450;&#12289;&#27880;&#20876;&#25253;&#21578;&#21644;&#23433;&#20840;&#21512;&#35268;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20026;&#20154;&#31867;&#24102;&#26469;&#24040;&#22823;&#30340;&#22909;&#22788;&#65292;&#20294;&#31038;&#20250;&#38656;&#35201;&#20027;&#21160;&#31649;&#29702;&#30456;&#20851;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#20851;&#27880;&#25105;&#20204;&#25152;&#31216;&#30340;&#8220;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#8221;&#27169;&#22411;&#65306;&#39640;&#24230;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#33021;&#20855;&#22791;&#36275;&#20197;&#23545;&#20844;&#20849;&#23433;&#20840;&#36896;&#25104;&#20005;&#37325;&#39118;&#38505;&#30340;&#21361;&#38505;&#33021;&#21147;&#12290;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#30417;&#31649;&#25361;&#25112;&#65306;&#21361;&#38505;&#33021;&#21147;&#21487;&#33021;&#20986;&#20046;&#24847;&#26009;&#65307;&#24456;&#38590;&#26377;&#25928;&#38450;&#27490;&#37096;&#32626;&#27169;&#22411;&#34987;&#28389;&#29992;&#65307;&#24182;&#19988;&#24456;&#38590;&#38459;&#27490;&#27169;&#22411;&#30340;&#33021;&#21147;&#24191;&#27867;&#25193;&#25955;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#36793;&#32536;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#33267;&#23569;&#19977;&#20010;&#22522;&#26412;&#35201;&#32032;&#65306;(1) &#35774;&#23450;&#26631;&#20934;&#30340;&#36807;&#31243;&#65292;&#20197;&#30830;&#23450;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#30340;&#36866;&#24403;&#35201;&#27714;&#65307;(2) &#27880;&#20876;&#21644;&#25253;&#21578;&#35201;&#27714;&#65292;&#20026;&#30417;&#31649;&#26426;&#26500;&#25552;&#20379;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;&#30340;&#21487;&#35265;&#24615;&#65307;(3) &#20445;&#35777;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#23433;&#20840;&#26631;&#20934;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term "frontier AI" models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deplo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#26426;&#22120;&#20154;&#36229;&#22768;&#21307;&#29983;&#65292;&#36890;&#36807;&#23398;&#20064;&#19987;&#23478;&#30340;&#32463;&#39564;&#65292;&#33258;&#20027;&#22320;&#25511;&#21046;&#36229;&#22768;&#25506;&#22836;&#26469;&#36827;&#34892;&#29983;&#29289;&#27979;&#37327;&#21644;&#20869;&#37096;&#22120;&#23448;&#35786;&#26029;&#12290;&#20351;&#29992;&#20114;&#20449;&#24687;&#26469;&#25552;&#21462;&#20219;&#21153;&#30456;&#20851;&#21644;&#39046;&#22495;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22870;&#21169;&#20989;&#25968;&#25512;&#26029;&#20986;&#19987;&#23478;&#30340;&#29983;&#29702;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.03705</link><description>&lt;p&gt;
&#26234;&#33021;&#26426;&#22120;&#20154;&#36229;&#22768;&#21307;&#29983;&#65306;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#23569;&#31034;&#33539;&#35299;&#32806;&#22870;&#21169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Intelligent Robotic Sonographer: Mutual Information-based Disentangled Reward Learning from Few Demonstrations. (arXiv:2307.03705v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#26426;&#22120;&#20154;&#36229;&#22768;&#21307;&#29983;&#65292;&#36890;&#36807;&#23398;&#20064;&#19987;&#23478;&#30340;&#32463;&#39564;&#65292;&#33258;&#20027;&#22320;&#25511;&#21046;&#36229;&#22768;&#25506;&#22836;&#26469;&#36827;&#34892;&#29983;&#29289;&#27979;&#37327;&#21644;&#20869;&#37096;&#22120;&#23448;&#35786;&#26029;&#12290;&#20351;&#29992;&#20114;&#20449;&#24687;&#26469;&#25552;&#21462;&#20219;&#21153;&#30456;&#20851;&#21644;&#39046;&#22495;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22870;&#21169;&#20989;&#25968;&#25512;&#26029;&#20986;&#19987;&#23478;&#30340;&#29983;&#29702;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#65288;US&#65289;&#25104;&#20687;&#30001;&#20110;&#23454;&#26102;&#24615;&#21644;&#26080;&#36752;&#23556;&#30340;&#20248;&#21183;&#32780;&#24191;&#27867;&#29992;&#20110;&#29983;&#29289;&#27979;&#37327;&#21644;&#20869;&#37096;&#22120;&#23448;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#27700;&#24179;&#30340;&#25805;&#20316;&#32773;&#21487;&#21464;&#24615;&#65292;&#32467;&#26524;&#22270;&#20687;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25805;&#20316;&#32773;&#30340;&#32463;&#39564;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#26426;&#22120;&#20154;&#36229;&#22768;&#21307;&#29983;&#65292;&#36890;&#36807;&#20174;&#19987;&#23478;&#37027;&#37324;&#23398;&#20064;&#65292;&#33258;&#20027;&#22320;&#8220;&#25506;&#32034;&#8221;&#30446;&#26631;&#35299;&#21078;&#23398;&#24182;&#23558;&#36229;&#22768;&#25506;&#22836;&#23548;&#33322;&#21040;&#30456;&#20851;&#30340;2D&#24179;&#38754;&#12290;&#19987;&#23478;&#30340;&#24213;&#23618;&#39640;&#32423;&#29983;&#29702;&#30693;&#35782;&#36890;&#36807;&#31070;&#32463;&#22870;&#21169;&#20989;&#25968;&#25512;&#26029;&#20986;&#26469;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#25490;&#21517;&#23545;&#27604;&#22270;&#20687;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#31216;&#20026;&#29702;&#35299; &#8220;&#22768;&#23398;&#35821;&#35328;&#8221;&#12290;&#32771;&#34385;&#21040;&#20811;&#26381;&#24739;&#32773;&#38388;&#21464;&#24322;&#24615;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32593;&#32476;&#36890;&#36807;&#20272;&#35745;&#20114;&#20449;&#24687;&#26469;&#26174;&#24335;&#25552;&#21462;&#28508;&#22312;&#31354;&#38388;&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#21644;&#39046;&#22495;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#30340;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#21644;&#37319;&#21462;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultrasound (US) imaging is widely used for biometric measurement and diagnosis of internal organs due to the advantages of being real-time and radiation-free. However, due to high inter-operator variability, resulting images highly depend on operators' experience. In this work, an intelligent robotic sonographer is proposed to autonomously "explore" target anatomies and navigate a US probe to a relevant 2D plane by learning from expert. The underlying high-level physiological knowledge from experts is inferred by a neural reward function, using a ranked pairwise image comparisons approach in a self-supervised fashion. This process can be referred to as understanding the "language of sonography". Considering the generalization capability to overcome inter-patient variations, mutual information is estimated by a network to explicitly extract the task-related and domain features in latent space. Besides, a Gaussian distribution-based filter is developed to automatically evaluate and take 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(ChatGPT)&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38750;&#27861;&#27602;&#21697;&#36137;&#36816;&#27963;&#21160;&#12290;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#39537;&#21160;&#30340;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#33719;&#21462;&#21644;&#35782;&#21035;&#27450;&#39575;&#24615;&#35821;&#35328;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03699</link><description>&lt;p&gt;
&#25581;&#31034;&#30693;&#35782;&#28608;&#21457;&#30340;ChatGPT&#22312;&#22686;&#24378;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#27602;&#21697;&#36137;&#36816;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media. (arXiv:2307.03699v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(ChatGPT)&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38750;&#27861;&#27602;&#21697;&#36137;&#36816;&#27963;&#21160;&#12290;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#39537;&#21160;&#30340;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#33719;&#21462;&#21644;&#35782;&#21035;&#27450;&#39575;&#24615;&#35821;&#35328;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22914;Instagram&#21644;Twitter&#24050;&#32463;&#25104;&#20026;&#27602;&#21697;&#33829;&#38144;&#21644;&#38750;&#27861;&#38144;&#21806;&#30340;&#20851;&#38190;&#28192;&#36947;&#12290;&#26816;&#27979;&#21644;&#26631;&#35760;&#22312;&#32447;&#38750;&#27861;&#27602;&#21697;&#36137;&#36816;&#27963;&#21160;&#23545;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#26816;&#27979;&#27602;&#21697;&#36137;&#36816;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#33719;&#24471;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#25968;&#25454;&#27880;&#37322;&#26159;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#24403;&#27602;&#36137;&#20351;&#29992;&#27450;&#39575;&#24615;&#35821;&#35328;&#21644;&#22996;&#23113;&#35821;&#36991;&#20813;&#34987;&#26816;&#27979;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38754;&#20020;&#30528;&#20934;&#30830;&#35782;&#21035;&#36137;&#36816;&#27963;&#21160;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#31995;&#32479;&#30740;&#31350;&#65292;&#21033;&#29992;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38750;&#27861;&#27602;&#21697;&#36137;&#36816;&#27963;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#32452;&#25104;&#30693;&#35782;&#39537;&#21160;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#25552;&#31034;&#20316;&#20026;&#20154;&#31867;&#19982;LLMs&#20132;&#20114;&#30340;&#25509;&#21475;&#26469;&#25191;&#34892;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms such as Instagram and Twitter have emerged as critical channels for drug marketing and illegal sale. Detecting and labeling online illicit drug trafficking activities becomes important in addressing this issue. However, the effectiveness of conventional supervised learning methods in detecting drug trafficking heavily relies on having access to substantial amounts of labeled data, while data annotation is time-consuming and resource-intensive. Furthermore, these models often face challenges in accurately identifying trafficking activities when drug dealers use deceptive language and euphemisms to avoid detection. To overcome this limitation, we conduct the first systematic study on leveraging large language models (LLMs), such as ChatGPT, to detect illicit drug trafficking activities on social media. We propose an analytical framework to compose \emph{knowledge-informed prompts}, which serve as the interface that humans can interact with and use LLMs to perform t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#24433;&#23376;&#27169;&#22411;&#25915;&#20987;&#30456;&#27604;&#65292;&#35745;&#31639;&#37327;&#26356;&#23567;&#20294;&#25928;&#26524;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2307.03694</link><description>&lt;p&gt;
&#21487;&#20280;&#32553;&#30340;&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Scalable Membership Inference Attacks via Quantile Regression. (arXiv:2307.03694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03694
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#24433;&#23376;&#27169;&#22411;&#25915;&#20987;&#30456;&#27604;&#65292;&#35745;&#31639;&#37327;&#26356;&#23567;&#20294;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#23545;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#65292;&#30830;&#23450;&#29305;&#23450;&#31034;&#20363;&#26159;&#21542;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#12290;&#25104;&#21592;&#25512;&#26029;&#21487;&#20197;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#12290;&#29616;&#26377;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#26159;&#36890;&#36807;&#35757;&#32451;&#35768;&#22810;&#19982;&#34987;&#25915;&#20987;&#27169;&#22411;&#30456;&#21516;&#26550;&#26500;&#30340;&#8220;&#24433;&#23376;&#27169;&#22411;&#8221;&#65288;&#22312;&#38543;&#26426;&#25968;&#25454;&#23376;&#26679;&#26412;&#19978;&#35757;&#32451;&#65289;&#26469;&#20272;&#35745;&#26576;&#20123;&#27979;&#35797;&#32479;&#35745;&#37327;&#65288;&#36890;&#24120;&#26159;&#27169;&#22411;&#23545;&#30495;&#23454;&#26631;&#31614;&#30340;&#32622;&#20449;&#24230;&#65289;&#30340;&#20998;&#24067;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#25915;&#20987;&#38750;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#29305;&#21035;&#26159;&#24403;&#34987;&#25915;&#20987;&#30340;&#27169;&#22411;&#24456;&#22823;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#23545;&#26410;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#28857;&#19978;&#30340;&#34987;&#25915;&#20987;&#27169;&#22411;&#20135;&#29983;&#30340;&#32622;&#20449;&#24230;&#20998;&#24067;&#36827;&#34892;&#20998;&#20301;&#25968;&#22238;&#24402;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#24433;&#23376;&#27169;&#22411;&#25915;&#20987;&#30456;&#31454;&#20105;&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \emph{shadow models} -i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large.  We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less comput
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#26089;&#20572;&#26631;&#20934;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#23454;&#39564;&#35777;&#26126;&#27169;&#22411;&#33021;&#22815;&#30456;&#23545;&#26089;&#26399;&#22320;&#23398;&#20250;&#36981;&#24490;&#25351;&#20196;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#35821;&#20041;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.03692</link><description>&lt;p&gt;
&#25104;&#20026;&#33258;&#23398;&#32773;&#65306;&#24341;&#20837;&#26368;&#23567;&#25351;&#20196;&#35843;&#25972;&#30340;&#26089;&#20572;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning. (arXiv:2307.03692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#26089;&#20572;&#26631;&#20934;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#23454;&#39564;&#35777;&#26126;&#27169;&#22411;&#33021;&#22815;&#30456;&#23545;&#26089;&#26399;&#22320;&#23398;&#20250;&#36981;&#24490;&#25351;&#20196;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#35821;&#20041;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25351;&#20196;&#36319;&#38543;&#24471;&#20998;&#65288;IFS&#65289;&#65292;&#19968;&#31181;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#24230;&#37327;&#26631;&#20934;&#20855;&#26377;&#21452;&#37325;&#30446;&#30340;&#12290;&#39318;&#20808;&#65292;IFS&#21487;&#20197;&#29992;&#20110;&#21306;&#20998;&#22522;&#30784;&#27169;&#22411;&#21644;&#25351;&#20196;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#21644;&#25351;&#20196;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#26174;&#31034;&#20986;&#33391;&#22909;&#26684;&#24335;&#21270;&#21709;&#24212;&#19982;&#37096;&#20998;&#21644;&#23436;&#25972;&#21477;&#23376;&#30340;&#27604;&#20363;&#21487;&#20197;&#20316;&#20026;&#36825;&#20004;&#31181;&#27169;&#22411;&#31867;&#21035;&#20043;&#38388;&#30340;&#26377;&#25928;&#34913;&#37327;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#35813;&#24230;&#37327;&#26631;&#20934;&#21487;&#20197;&#29992;&#20316;&#25351;&#20196;&#35843;&#25972;&#30340;&#26089;&#20572;&#26631;&#20934;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;7B&#21644;13B LLaMA&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;IFS&#65292;&#26174;&#31034;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30456;&#23545;&#26089;&#26399;&#23601;&#23398;&#20250;&#20102;&#36981;&#24490;&#25351;&#20196;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#22522;&#30784;&#27169;&#22411;&#35821;&#20041;&#30340;&#21464;&#21270;&#12290;&#20316;&#20026;&#35821;&#20041;&#21464;&#21270;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#36741;&#21161;&#24230;&#37327;&#26631;&#20934;ObjecQA&#23450;&#20041;&#30340;&#27169;&#22411;&#39044;&#27979;&#30340;&#23458;&#35266;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#22312;&#36825;&#31181;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#24403;IFS&#20542;&#21521;&#20110;p&#26102;&#65292;&#35821;&#20041;&#21464;&#21270;&#26368;&#20026;&#21095;&#28872;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the Instruction Following Score (IFS), a metric that detects language models' ability to follow instructions. The metric has a dual purpose. First, IFS can be used to distinguish between base and instruct models. We benchmark publicly available base and instruct models, and show that the ratio of well formatted responses to partial and full sentences can be an effective measure between those two model classes. Secondly, the metric can be used as an early stopping criteria for instruct tuning. We compute IFS for Supervised Fine-Tuning (SFT) of 7B and 13B LLaMA models, showing that models learn to follow instructions relatively early in the training process, and the further finetuning can result in changes in the underlying base model semantics. As an example of semantics change we show the objectivity of model predictions, as defined by an auxiliary metric ObjecQA. We show that in this particular case, semantic changes are the steepest when the IFS tends to p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#35780;&#35770;&#21644;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#29983;&#25104;&#23545;&#27604;&#35780;&#20215;&#21477;&#23376;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#12289;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#21644;&#20010;&#24615;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#20102;&#29983;&#25104;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03691</link><description>&lt;p&gt;
&#23558;&#33529;&#26524;&#19982;&#33529;&#26524;&#36827;&#34892;&#27604;&#36739;&#65306;&#20174;&#29992;&#25143;&#35780;&#35770;&#29983;&#25104;&#32437;&#21521;&#24863;&#30693;&#30340;&#27604;&#36739;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
Comparing Apples to Apples: Generating Aspect-Aware Comparative Sentences from User Review. (arXiv:2307.03691v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#35780;&#35770;&#21644;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#29983;&#25104;&#23545;&#27604;&#35780;&#20215;&#21477;&#23376;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#12289;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#21644;&#20010;&#24615;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#20102;&#29983;&#25104;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#22810;&#30456;&#20284;&#30340;&#36873;&#25321;&#20013;&#25214;&#21040;&#26368;&#20339;&#20135;&#21697;&#26159;&#38750;&#24120;&#32791;&#26102;&#30340;&#12290;&#27604;&#36739;&#21477;&#23376;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20197;&#31361;&#20986;&#30340;&#26041;&#24335;&#23545;&#27604;&#19968;&#20010;&#39033;&#30446;&#19982;&#20854;&#20182;&#39033;&#30446;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#24378;&#35843;&#20986;&#37325;&#35201;&#29305;&#24449;&#12290;&#22522;&#20110;&#29992;&#25143;&#23545;&#19968;&#20010;&#25110;&#22810;&#20010;&#39033;&#30446;&#30340;&#35780;&#35770;&#21450;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#65292;&#25105;&#20204;&#29983;&#25104;&#27604;&#36739;&#35780;&#35770;&#21477;&#23376;&#26469;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#19977;&#20010;&#36830;&#32493;&#32452;&#20214;&#65306;&#65288;i&#65289;&#19968;&#20010;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#29992;&#20110;&#23545;&#39033;&#30446;&#36827;&#34892;&#32534;&#30721;&#27604;&#36739;&#65292;&#65288;ii&#65289;&#19968;&#20010;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#29983;&#25104;&#27604;&#36739;&#21477;&#23376;&#65292;&#65288;iii&#65289;&#19968;&#31181;&#29992;&#20110;&#29992;&#25143;&#20010;&#24615;&#21270;&#30340;&#26032;&#22411;&#35299;&#30721;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#31243;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#19988;&#22810;&#26679;&#30340;&#27604;&#36739;&#21477;&#23376;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#26469;&#39564;&#35777;&#25105;&#20204;&#29983;&#25104;&#30340;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#29983;&#25104;&#30456;&#20851;&#19988;&#30495;&#23454;&#30340;&#27604;&#36739;&#35780;&#35770;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is time-consuming to find the best product among many similar alternatives. Comparative sentences can help to contrast one item from others in a way that highlights important features of an item that stand out. Given reviews of one or multiple items and relevant item features, we generate comparative review sentences to aid users to find the best fit. Specifically, our model consists of three successive components in a transformer: (i) an item encoding module to encode an item for comparison, (ii) a comparison generation module that generates comparative sentences in an autoregressive manner, (iii) a novel decoding method for user personalization. We show that our pipeline generates fluent and diverse comparative sentences. We run experiments on the relevance and fidelity of our generated sentences in a human evaluation study and find that our algorithm creates comparative review sentences that are relevant and truthful.
&lt;/p&gt;</description></item><item><title>&#19978;&#36848;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#20934;&#21017;--AI&#35780;&#20272;&#30446;&#24405;&#65292;&#26088;&#22312;&#30830;&#20445;AI&#24212;&#29992;&#25353;&#29031;&#39640;&#36136;&#37327;&#26631;&#20934;&#24320;&#21457;&#65292;&#24182;&#26377;&#25928;&#38450;&#33539;&#26032;&#30340;AI&#39118;&#38505;&#65292;&#20174;&#32780;&#20351;AI&#21457;&#25381;&#20854;&#28508;&#21147;&#65292;&#20943;&#23569;&#19981;&#20844;&#24179;&#23545;&#24453;&#20010;&#20307;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.03681</link><description>&lt;p&gt;
&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#20934;&#21017;--AI&#35780;&#20272;&#30446;&#24405;
&lt;/p&gt;
&lt;p&gt;
Guideline for Trustworthy Artificial Intelligence -- AI Assessment Catalog. (arXiv:2307.03681v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03681
&lt;/p&gt;
&lt;p&gt;
&#19978;&#36848;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#20934;&#21017;--AI&#35780;&#20272;&#30446;&#24405;&#65292;&#26088;&#22312;&#30830;&#20445;AI&#24212;&#29992;&#25353;&#29031;&#39640;&#36136;&#37327;&#26631;&#20934;&#24320;&#21457;&#65292;&#24182;&#26377;&#25928;&#38450;&#33539;&#26032;&#30340;AI&#39118;&#38505;&#65292;&#20174;&#32780;&#20351;AI&#21457;&#25381;&#20854;&#28508;&#21147;&#65292;&#20943;&#23569;&#19981;&#20844;&#24179;&#23545;&#24453;&#20010;&#20307;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#24182;&#20195;&#34920;&#30528;&#19968;&#39033;&#23545;&#32463;&#27982;&#21644;&#31038;&#20250;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24456;&#26126;&#26174;&#65292;&#21482;&#26377;&#25353;&#29031;&#39640;&#36136;&#37327;&#26631;&#20934;&#24320;&#21457;AI&#24212;&#29992;&#24182;&#26377;&#25928;&#38450;&#33539;&#26032;&#30340;AI&#39118;&#38505;&#65292;AI&#21644;&#22522;&#20110;&#20854;&#30340;&#21830;&#19994;&#27169;&#24335;&#25165;&#33021;&#20805;&#20998;&#21457;&#25381;&#20854;&#28508;&#21147;&#12290;&#20363;&#22914;&#65292;&#24403;&#22788;&#29702;&#20010;&#20154;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#25903;&#25345;&#20449;&#36151;&#20511;&#27454;&#25110;&#21592;&#24037;&#25307;&#32856;&#20915;&#31574;&#65289;&#26102;&#65292;AI&#23384;&#22312;&#19981;&#20844;&#24179;&#23545;&#24453;&#20010;&#20307;&#30340;&#39118;&#38505;&#12290;&#36825;&#20123;&#26032;&#39118;&#38505;&#30340;&#20986;&#29616;&#19982;AI&#24212;&#29992;&#30340;&#34892;&#20026;&#23494;&#20999;&#30456;&#20851;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24212;&#29992;&#65292;&#20854;&#34892;&#20026;&#22522;&#26412;&#19978;&#26159;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#32780;&#26469;&#65292;&#32780;&#19981;&#26159;&#30001;&#22266;&#23450;&#32534;&#31243;&#35268;&#21017;&#39044;&#20808;&#30830;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;AI&#24212;&#29992;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#26159;&#25919;&#27835;&#12289;&#21830;&#19994;&#21644;&#31038;&#20250;&#21033;&#30410;&#30456;&#20851;&#32773;&#20247;&#22810;&#37325;&#35201;&#20986;&#29256;&#29289;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has made impressive progress in recent years and represents a key technology that has a crucial impact on the economy and society. However, it is clear that AI and business models based on it can only reach their full potential if AI applications are developed according to high quality standards and are effectively protected against new AI risks. For instance, AI bears the risk of unfair treatment of individuals when processing personal data e.g., to support credit lending or staff recruitment decisions. The emergence of these new risks is closely linked to the fact that the behavior of AI applications, particularly those based on Machine Learning (ML), is essentially learned from large volumes of data and is not predetermined by fixed programmed rules.  Thus, the issue of the trustworthiness of AI applications is crucial and is the subject of numerous major publications by stakeholders in politics, business and society. In addition, there is mutual agreeme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19968;&#20123;&#31354;&#38388;&#20851;&#31995;&#65292;&#20294;&#22312;&#20272;&#35745;&#25968;&#20540;&#21644;&#26816;&#32034;&#30456;&#20851;&#23545;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.03678</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations. (arXiv:2307.03678v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19968;&#20123;&#31354;&#38388;&#20851;&#31995;&#65292;&#20294;&#22312;&#20272;&#35745;&#25968;&#20540;&#21644;&#26816;&#32034;&#30456;&#20851;&#23545;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#21253;&#25324;GPT-2&#21644;BERT&#22312;&#20869;&#30340;LLMs&#23545;&#20960;&#20309;&#30340;&#25991;&#26412;&#26684;&#24335;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#23884;&#20837;&#36755;&#20837;&#20998;&#31867;&#22120;&#21644;&#22238;&#24402;&#22120;&#65292;&#20197;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#23884;&#20837;&#22312;&#20960;&#20309;&#23646;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;LLMs&#29983;&#25104;&#30340;&#23884;&#20837;&#33021;&#22815;&#20445;&#30041;&#20960;&#20309;&#31867;&#22411;&#24182;&#25429;&#25417;&#19968;&#20123;&#31354;&#38388;&#20851;&#31995;&#65288;&#20934;&#30830;&#24230;&#39640;&#36798;73%&#65289;&#65292;&#20294;&#22312;&#20272;&#35745;&#25968;&#20540;&#21644;&#26816;&#32034;&#31354;&#38388;&#30456;&#20851;&#23545;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20984;&#26174;&#20102;&#22312;&#25429;&#25417;&#24213;&#23618;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#30340;&#32454;&#24494;&#24046;&#21035;&#21644;&#22797;&#26434;&#24615;&#20197;&#21450;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#20197;&#25903;&#25345;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21508;&#31181;GeoAI&#24212;&#29992;&#26041;&#38754;&#30340;&#25913;&#36827;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research focuses on assessing the ability of large language models (LLMs) in representing geometries and their spatial relations. We utilize LLMs including GPT-2 and BERT to encode the well-known text (WKT) format of geometries and then feed their embeddings into classifiers and regressors to evaluate the effectiveness of the LLMs-generated embeddings for geometric attributes. The experiments demonstrate that while the LLMs-generated embeddings can preserve geometry types and capture some spatial relations (up to 73% accuracy), challenges remain in estimating numeric values and retrieving spatially related objects. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using foundation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#37327;&#21270;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#27867;&#21270;&#22256;&#38590;&#31243;&#24230;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#20223;&#30495;&#22522;&#20934;&#26469;&#35780;&#20272;&#27867;&#21270;&#25511;&#21046;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27867;&#21270;&#22256;&#38590;&#24230;&#30340;&#22240;&#32032;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.03659</link><description>&lt;p&gt;
&#20998;&#35299;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#27169;&#20223;&#23398;&#20064;&#30340;&#27867;&#21270;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation. (arXiv:2307.03659v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#37327;&#21270;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#27867;&#21270;&#22256;&#38590;&#31243;&#24230;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#20223;&#30495;&#22522;&#20934;&#26469;&#35780;&#20272;&#27867;&#21270;&#25511;&#21046;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27867;&#21270;&#22256;&#38590;&#24230;&#30340;&#22240;&#32032;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#20026;&#20309;&#38590;&#20197;&#27867;&#21270;&#65311;&#36825;&#20010;&#38382;&#39064;&#24182;&#19981;&#23481;&#26131;&#35299;&#20915;&#65292;&#20294;&#20174;&#26426;&#22120;&#20154;&#30340;&#35270;&#35282;&#26469;&#30475;&#65292;&#29615;&#22659;&#36890;&#24120;&#21487;&#20197;&#20998;&#35299;&#20026;&#21487;&#20197;&#26522;&#20030;&#30340;&#21464;&#21270;&#22240;&#32032;&#65292;&#20363;&#22914;&#20809;&#29031;&#26465;&#20214;&#25110;&#25668;&#20687;&#26426;&#30340;&#25670;&#25918;&#20301;&#32622;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#23545;&#20110;&#36825;&#20123;&#22240;&#32032;&#30340;&#27867;&#21270;&#27604;&#20854;&#20182;&#22240;&#32032;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#24037;&#20316;&#24456;&#23569;&#28041;&#21450;&#21040;&#27599;&#20010;&#22240;&#32032;&#23545;&#27867;&#21270;&#24046;&#36317;&#30340;&#36129;&#29486;&#26377;&#22810;&#22823;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#30740;&#31350;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#37327;&#21270;&#19981;&#21516;&#65288;&#38598;&#21512;&#65289;&#22240;&#32032;&#30340;&#27867;&#21270;&#22256;&#38590;&#31243;&#24230;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#20223;&#30495;&#22522;&#20934;&#65292;&#21253;&#21547;19&#20010;&#20219;&#21153;&#21644;11&#20010;&#21464;&#21270;&#22240;&#32032;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#30340;&#25511;&#21046;&#24615;&#35780;&#20272;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22522;&#20110;&#27867;&#21270;&#22256;&#38590;&#24230;&#30340;&#22240;&#32032;&#25490;&#24207;&#65292;&#36825;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
What makes generalization hard for imitation learning in visual robotic manipulation? This question is difficult to approach at face value, but the environment from the perspective of a robot can often be decomposed into enumerable factors of variation, such as the lighting conditions or the placement of the camera. Empirically, generalization to some of these factors have presented a greater obstacle than others, but existing work sheds little light on precisely how much each factor contributes to the generalization gap. Towards an answer to this question, we study imitation learning policies in simulation and on a real robot language-conditioned manipulation task to quantify the difficulty of generalization to different (sets of) factors. We also design a new simulated benchmark of 19 tasks with 11 factors of variation to facilitate more controlled evaluations of generalization. From our study, we determine an ordering of factors based on generalization difficulty, that is consistent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24895;&#26395;&#28165;&#21333;&#33258;&#21160;&#35782;&#21035;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21464;&#37327;&#32465;&#23450;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25351;&#23450;&#29305;&#23450;&#23376;&#20219;&#21153;&#30340;&#22240;&#26524;&#23646;&#24615;&#65292;&#25104;&#21151;&#22320;&#23558;&#21464;&#37327;&#32465;&#23450;&#23450;&#20301;&#22312;&#27169;&#22411;&#30340;&#37096;&#20998;&#32452;&#20214;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.03637</link><description>&lt;p&gt;
&#20351;&#29992;&#24895;&#26395;&#28165;&#21333;&#21457;&#29616;&#21464;&#37327;&#32465;&#23450;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Discovering Variable Binding Circuitry with Desiderata. (arXiv:2307.03637v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03637
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24895;&#26395;&#28165;&#21333;&#33258;&#21160;&#35782;&#21035;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21464;&#37327;&#32465;&#23450;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25351;&#23450;&#29305;&#23450;&#23376;&#20219;&#21153;&#30340;&#22240;&#26524;&#23646;&#24615;&#65292;&#25104;&#21151;&#22320;&#23558;&#21464;&#37327;&#32465;&#23450;&#23450;&#20301;&#22312;&#27169;&#22411;&#30340;&#37096;&#20998;&#32452;&#20214;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35745;&#31639;&#21487;&#33021;&#26159;&#21487;&#29702;&#35299;&#30340;&#65292;&#36890;&#36807;&#25104;&#21151;&#22320;&#23616;&#37096;&#21270;&#21644;&#24178;&#39044;&#21333;&#20803;&#29305;&#24449;&#21644;&#36755;&#20837;&#36755;&#20986;&#30005;&#36335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#25351;&#23450;&#25191;&#34892;&#29305;&#23450;&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#32452;&#20214;&#30340;&#19968;&#32452;"&#24895;&#26395;&#28165;&#21333;"&#25110;&#22240;&#26524;&#23646;&#24615;&#65292;&#26469;&#25193;&#23637;&#22240;&#26524;&#20013;&#20171;&#23454;&#39564;&#65292;&#20197;&#33258;&#21160;&#35782;&#21035;&#36127;&#36131;&#25191;&#34892;&#35813;&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#32452;&#20214;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;LLaMA-13B&#20013;&#20998;&#20139;&#30340;"&#21464;&#37327;&#32465;&#23450;&#30005;&#36335;"&#65292;&#35813;&#30005;&#36335;&#20026;&#22810;&#20010;&#31639;&#26415;&#20219;&#21153;&#26816;&#32034;&#21464;&#37327;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#23558;&#21464;&#37327;&#32465;&#23450;&#23450;&#20301;&#20026;&#20165;&#26377;9&#20010;&#27880;&#24847;&#22836;&#65288;1600&#20010;&#20013;&#65289;&#21644;&#26368;&#32456;&#20196;&#29260;&#30340;&#27531;&#20313;&#27969;&#20013;&#30340;&#19968;&#20010;MLP&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that computation in language models may be human-understandable, with successful efforts to localize and intervene on both single-unit features and input-output circuits. Here, we introduce an approach which extends causal mediation experiments to automatically identify model components responsible for performing a specific subtask by solely specifying a set of \textit{desiderata}, or causal attributes of the model components executing that subtask. As a proof of concept, we apply our method to automatically discover shared \textit{variable binding circuitry} in LLaMA-13B, which retrieves variable values for multiple arithmetic tasks. Our method successfully localizes variable binding to only 9 attention heads (of the 1.6k) and one MLP in the final token's residual stream.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#8220;&#20919;&#21551;&#21160;&#8221;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;GNN-based&#29305;&#24449;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#36328;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.03595</link><description>&lt;p&gt;
GEANN: &#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;&#22270;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting. (arXiv:2307.03595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#8220;&#20919;&#21551;&#21160;&#8221;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;GNN-based&#29305;&#24449;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#36328;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20934;&#30830;&#39044;&#27979;&#65292;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20855;&#26377;&#20016;&#23500;&#21382;&#21490;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#31034;&#20363;&#12290;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#30740;&#31350;&#20027;&#39064;&#26159;&#39044;&#27979;&#32570;&#20047;&#36275;&#22815;&#21382;&#21490;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#20919;&#21551;&#21160;&#8221;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#26469;&#22686;&#24378;&#36825;&#20123;&#39044;&#27979;&#22120;&#20351;&#29992;&#30340;&#32534;&#30721;&#22120;&#12290;&#36825;&#20123;&#22522;&#20110;GNN&#30340;&#29305;&#24449;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#36328;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#29983;&#25104;&#36807;&#31243;&#21487;&#20197;&#19982;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#21487;&#20197;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#25110;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#23450;&#20041;&#30340;&#22270;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;&#21253;&#21547;&#25968;&#30334;&#19975;&#33410;&#28857;&#30340;&#22810;&#20010;&#38750;&#24120;&#22823;&#30340;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoder-decoder deep neural networks have been increasingly studied for multi-horizon time series forecasting, especially in real-world applications. However, to forecast accurately, these sophisticated models typically rely on a large number of time series examples with substantial history. A rapidly growing topic of interest is forecasting time series which lack sufficient historical data -- often referred to as the ``cold start'' problem. In this paper, we introduce a novel yet simple method to address this problem by leveraging graph neural networks (GNNs) as a data augmentation for enhancing the encoder used by such forecasters. These GNN-based features can capture complex inter-series relationships, and their generation process can be optimized end-to-end with the forecasting task. We show that our architecture can use either data-driven or domain knowledge-defined graphs, scaling to incorporate information from multiple very large graphs with millions of nodes. In our target app
&lt;/p&gt;</description></item><item><title>VesselVAE&#26159;&#19968;&#20010;&#36882;&#24402;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#34880;&#31649;&#30340;3D&#20960;&#20309;&#24418;&#29366;&#65292;&#36890;&#36807;&#21033;&#29992;&#34880;&#31649;&#30340;&#23618;&#27425;&#32452;&#32455;&#32467;&#26500;&#21644;&#20960;&#20309;&#29305;&#24449;&#30340;&#20302;&#32500;&#27969;&#24418;&#32534;&#30721;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#21322;&#24452;&#12289;&#38271;&#24230;&#21644;&#25197;&#26354;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03592</link><description>&lt;p&gt;
VesselVAE: &#36882;&#24402;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;3D&#34880;&#31649;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis. (arXiv:2307.03592v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03592
&lt;/p&gt;
&lt;p&gt;
VesselVAE&#26159;&#19968;&#20010;&#36882;&#24402;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#34880;&#31649;&#30340;3D&#20960;&#20309;&#24418;&#29366;&#65292;&#36890;&#36807;&#21033;&#29992;&#34880;&#31649;&#30340;&#23618;&#27425;&#32452;&#32455;&#32467;&#26500;&#21644;&#20960;&#20309;&#29305;&#24449;&#30340;&#20302;&#32500;&#27969;&#24418;&#32534;&#30721;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#21322;&#24452;&#12289;&#38271;&#24230;&#21644;&#25197;&#26354;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#25104;&#26694;&#26550;&#29992;&#20110;&#21512;&#25104;&#34880;&#31649;&#30340;3D&#20960;&#20309;&#24418;&#29366;&#12290;&#30001;&#20110;&#34880;&#31649;&#31995;&#32479;&#30340;&#24418;&#29366;&#12289;&#22823;&#23567;&#21644;&#32467;&#26500;&#39640;&#24230;&#21464;&#21270;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#30340;&#32467;&#26500;&#20013;&#25552;&#20379;&#20102;&#19968;&#23450;&#31243;&#24230;&#30340;&#25511;&#21046;&#21644;&#21464;&#21270;&#65292;&#20294;&#26410;&#33021;&#25429;&#25417;&#21040;&#23454;&#38469;&#35299;&#21078;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;VesselVAE&#65292;&#19968;&#31181;&#36882;&#24402;&#21464;&#20998;&#31070;&#32463;&#32593;&#32476;&#65292;&#23436;&#20840;&#21033;&#29992;&#20102;&#34880;&#31649;&#30340;&#23618;&#27425;&#32452;&#32455;&#32467;&#26500;&#65292;&#24182;&#23398;&#20064;&#20102;&#25551;&#36848;&#30446;&#26631;&#34920;&#38754;&#30340;&#20998;&#25903;&#36830;&#25509;&#24615;&#20197;&#21450;&#20960;&#20309;&#29305;&#24449;&#30340;&#20302;&#32500;&#27969;&#24418;&#32534;&#30721;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;VesselVAE&#30340;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#36827;&#34892;&#37319;&#26679;&#26469;&#29983;&#25104;&#26032;&#30340;&#34880;&#31649;&#20960;&#20309;&#24418;&#29366;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#21033;&#29992;&#36825;&#31181;&#25216;&#26415;&#26469;&#21512;&#25104;&#34880;&#31649;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#21322;&#24452;&#65288;.97&#65289;&#12289;&#38271;&#24230;&#65288;.95&#65289;&#21644;&#25197;&#26354;&#24230;&#65288;.96&#65289;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;3D&#34880;&#31649;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a data-driven generative framework for synthesizing blood vessel 3D geometry. This is a challenging task due to the complexity of vascular systems, which are highly variating in shape, size, and structure. Existing model-based methods provide some degree of control and variation in the structures produced, but fail to capture the diversity of actual anatomical data. We developed VesselVAE, a recursive variational Neural Network that fully exploits the hierarchical organization of the vessel and learns a low-dimensional manifold encoding branch connectivity along with geometry features describing the target surface. After training, the VesselVAE latent space can be sampled to generate new vessel geometries. To the best of our knowledge, this work is the first to utilize this technique for synthesizing blood vessels. We achieve similarities of synthetic and real data for radius (.97), length (.95), and tortuosity (.96). By leveraging the power of deep neural networks, we gener
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#32467;&#26500;&#24341;&#23548;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#20854;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03591</link><description>&lt;p&gt;
&#32467;&#26500;&#24341;&#23548;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph Reasoning. (arXiv:2307.03591v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#32467;&#26500;&#24341;&#23548;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#20854;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;(MKGs)&#30452;&#35266;&#22320;&#32452;&#32455;&#20102;&#21508;&#31181;&#27169;&#24335;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#24800;&#21450;&#22810;&#20010;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#35270;&#35273;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;MKGs&#20173;&#28982;&#36828;&#31163;&#23436;&#25972;&#65292;&#36825;&#20419;&#20351;&#20102;MKG&#25512;&#29702;&#27169;&#22411;&#30340;&#20852;&#36215;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#36890;&#29992;&#20154;&#24037;&#26550;&#26500;&#30340;&#21457;&#23637;&#65292;&#39044;&#35757;&#32451;transformer&#27169;&#22411;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22810;&#27169;&#24577;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;transformer (MPT)&#29992;&#20110;&#30693;&#35782;&#22270;&#25512;&#29702; (KGR) &#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#20316;&#20026;MKG&#21644;&#20854;&#20182;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26368;&#22823;&#21306;&#21035;&#65292;MKG&#20013;&#20016;&#23500;&#30340;&#32467;&#26500;&#20449;&#24687;&#20173;&#28982;&#26080;&#27861;&#22312;&#29616;&#26377;&#30340;MPT&#27169;&#22411;&#20013;&#20805;&#20998;&#21033;&#29992;&#12290;&#22823;&#22810;&#25968;&#27169;&#22411;&#21482;&#23558;&#22270;&#32467;&#26500;&#29992;&#20316;&#21305;&#37197;&#19982;&#21516;&#19968;&#23454;&#20307;&#30456;&#36830;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#26816;&#32034;&#26144;&#23556;&#12290;&#36825;&#31181;&#26041;&#24335;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#32467;&#26500;&#24341;&#23548;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal knowledge graphs (MKGs), which intuitively organize information in various modalities, can benefit multiple practical downstream tasks, such as recommendation systems, and visual question answering. However, most MKGs are still far from complete, which motivates the flourishing of MKG reasoning models. Recently, with the development of general artificial architectures, the pretrained transformer models have drawn increasing attention, especially for multimodal scenarios. However, the research of multimodal pretrained transformer (MPT) for knowledge graph reasoning (KGR) is still at an early stage. As the biggest difference between MKG and other multimodal data, the rich structural information underlying the MKG still cannot be fully leveraged in existing MPT models. Most of them only utilize the graph structure as a retrieval map for matching images and texts connected with the same entity. This manner hinders their reasoning performances. To this end, we propose the graph S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#21512;CT&#25104;&#20687;&#21644;&#20020;&#24202;&#25968;&#25454;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#32958;&#32454;&#32990;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#27010;&#29575;&#65292;&#24182;&#36741;&#21161;&#35782;&#21035;&#38656;&#35201;&#32039;&#24613;&#27835;&#30103;&#30340;&#24739;&#32773;&#12290;</title><link>http://arxiv.org/abs/2307.03575</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#20010;&#24615;&#21270;&#32958;&#32454;&#32990;&#30284;&#39044;&#21518;&#65306;&#25972;&#21512;CT&#25104;&#20687;&#21644;&#20020;&#24202;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Multimodal Deep Learning for Personalized Renal Cell Carcinoma Prognosis: Integrating CT Imaging and Clinical Data. (arXiv:2307.03575v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#21512;CT&#25104;&#20687;&#21644;&#20020;&#24202;&#25968;&#25454;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#32958;&#32454;&#32990;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#27010;&#29575;&#65292;&#24182;&#36741;&#21161;&#35782;&#21035;&#38656;&#35201;&#32039;&#24613;&#27835;&#30103;&#30340;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32958;&#32454;&#32990;&#30284;&#26159;&#19968;&#20010;&#20302;&#29983;&#23384;&#29575;&#30340;&#37325;&#35201;&#20840;&#29699;&#20581;&#24247;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#32508;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;CT&#25104;&#20687;&#21644;&#20020;&#24202;&#25968;&#25454;&#65292;&#35299;&#20915;&#20197;&#24448;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#39044;&#27979;&#32958;&#32454;&#32990;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#27010;&#29575;&#65292;&#20197;&#20415;&#36741;&#21161;&#35782;&#21035;&#38656;&#35201;&#32039;&#24613;&#27835;&#30103;&#30340;&#24739;&#32773;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;3D&#22270;&#20687;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#20020;&#24202;&#21464;&#37327;&#36873;&#25321;&#21644;&#29983;&#23384;&#39044;&#27979;&#12290;&#22522;&#20110;3D CNN&#26550;&#26500;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#27169;&#22359;&#65292;&#36890;&#36807;CT&#22270;&#20687;&#39044;&#27979;&#19982;&#32958;&#32454;&#32990;&#30284;&#30149;&#28790;&#30456;&#20851;&#30340;&#27515;&#20129;&#29575;&#30340;ISUP&#20998;&#32423;&#12290;&#36890;&#36807;&#20351;&#29992;Spearman&#20998;&#25968;&#21644;&#38543;&#26426;&#26862;&#26519;&#37325;&#35201;&#24615;&#20998;&#25968;&#20316;&#20026;&#26631;&#20934;&#65292;&#31995;&#32479;&#22320;&#36873;&#25321;&#19968;&#20123;&#20020;&#24202;&#21464;&#37327;&#12290;&#20351;&#29992;&#31163;&#25955;&#30340;LogisticHazard-based&#25439;&#22833;&#35757;&#32451;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32593;&#32476;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;&#12290;&#36827;&#34892;&#20102;&#20061;&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Renal cell carcinoma represents a significant global health challenge with a low survival rate. This research aimed to devise a comprehensive deep-learning model capable of predicting survival probabilities in patients with renal cell carcinoma by integrating CT imaging and clinical data and addressing the limitations observed in prior studies. The aim is to facilitate the identification of patients requiring urgent treatment. The proposed framework comprises three modules: a 3D image feature extractor, clinical variable selection, and survival prediction. The feature extractor module, based on the 3D CNN architecture, predicts the ISUP grade of renal cell carcinoma tumors linked to mortality rates from CT images. A selection of clinical variables is systematically chosen using the Spearman score and random forest importance score as criteria. A deep learning-based network, trained with discrete LogisticHazard-based loss, performs the survival prediction. Nine distinct experiments are 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#25216;&#33021;&#25552;&#21462;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;ESCO&#25216;&#33021;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21644;&#20351;&#29992;&#30456;&#20284;&#24615;&#26816;&#32034;&#22120;&#65292;&#23454;&#29616;&#20102;&#20174;&#32844;&#20301;&#25551;&#36848;&#20013;&#25552;&#21462;&#25216;&#33021;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.03539</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19968;&#20307;&#21270;&#38646;-shot ESCO&#25216;&#33021;&#21305;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers. (arXiv:2307.03539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03539
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#25216;&#33021;&#25552;&#21462;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;ESCO&#25216;&#33021;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21644;&#20351;&#29992;&#30456;&#20284;&#24615;&#26816;&#32034;&#22120;&#65292;&#23454;&#29616;&#20102;&#20174;&#32844;&#20301;&#25551;&#36848;&#20013;&#25552;&#21462;&#25216;&#33021;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21171;&#21160;&#21147;&#24066;&#22330;&#21160;&#24577;&#38656;&#35201;&#20934;&#30830;&#22320;&#35782;&#21035;&#21171;&#21160;&#21147;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#34987;&#24320;&#21457;&#20986;&#26469;&#25903;&#25345;&#36825;&#20010;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#25216;&#33021;&#30340;&#25968;&#37327;&#24222;&#22823;&#65292;&#20174;&#32844;&#20301;&#21457;&#24067;&#20013;&#33258;&#21160;&#25552;&#21462;&#25216;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;ESCO&#65288;&#27431;&#27954;&#25216;&#33021;&#12289;&#33021;&#21147;&#12289;&#36164;&#26684;&#21644;&#32844;&#19994;&#65289;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#21442;&#32771;&#65292;&#21015;&#20986;&#20102;&#36229;&#36807;13,000&#20010;&#29420;&#31435;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#25216;&#33021;&#25552;&#21462;&#20173;&#28982;&#22256;&#38590;&#65292;&#24182;&#19988;&#20934;&#30830;&#22320;&#23558;&#24037;&#20316;&#23703;&#20301;&#19982;ESCO&#20998;&#31867;&#36827;&#34892;&#21305;&#37197;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38646;-shot&#25216;&#33021;&#25552;&#21462;&#31995;&#32479;&#12290;&#25105;&#20204;&#29983;&#25104;ESCO&#25216;&#33021;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20998;&#31867;&#22120;&#20174;&#32844;&#20301;&#21457;&#24067;&#20013;&#25552;&#21462;&#25216;&#33021;&#25552;&#21450;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#30456;&#20284;&#24615;&#26816;&#32034;&#22120;&#29983;&#25104;&#25216;&#33021;&#20505;&#36873;&#39033;&#65292;&#28982;&#21518;&#20351;&#29992;&#31532;&#20108;&#20010;LLM&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#20102;&#25216;&#33021;&#25552;&#21462;&#30340;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding labour market dynamics requires accurately identifying the skills required for and possessed by the workforce. Automation techniques are increasingly being developed to support this effort. However, automatically extracting skills from job postings is challenging due to the vast number of existing skills. The ESCO (European Skills, Competences, Qualifications and Occupations) framework provides a useful reference, listing over 13,000 individual skills. However, skills extraction remains difficult and accurately matching job posts to the ESCO taxonomy is an open problem. In this work, we propose an end-to-end zero-shot system for skills extraction from job descriptions based on large language models (LLMs). We generate synthetic training data for the entirety of ESCO skills and train a classifier to extract skill mentions from job posts. We also employ a similarity retriever to generate skill candidates which are then re-ranked using a second LLM. Using synthetic data achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.03512</link><description>&lt;p&gt;
&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#32771;&#21476;&#30740;&#31350;&#20013;&#30340;&#36965;&#24863;&#25968;&#25454;&#26102;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#38556;&#30861;&#26159;&#36866;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#32463;&#24120;&#34987;&#29992;&#26469;&#20943;&#36731;&#36825;&#20010;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#24517;&#35201;&#25506;&#32034;&#22312;&#19981;&#21516;&#32771;&#21476;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20256;&#36755;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#20004;&#20010;&#35821;&#20041;&#20998;&#21106;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#20010;LiDAR&#25968;&#25454;&#38598;&#19978;&#30340;&#21508;&#31181;&#20256;&#36755;&#23398;&#20064;&#37197;&#32622;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20256;&#36755;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#32771;&#21476;&#23398;&#20013;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#23613;&#31649;&#23578;&#26410;&#35266;&#23519;&#21040;&#31995;&#32479;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27492;&#31867;&#25216;&#26415;&#26377;&#25928;&#24615;&#30340;&#20855;&#20307;&#35265;&#35299;&#65292;&#21487;&#20316;&#20026;&#26410;&#26469;&#24037;&#20316;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applying deep learning to remote sensing data in archaeological research, a notable obstacle is the limited availability of suitable datasets for training models. The application of transfer learning is frequently employed to mitigate this drawback. However, there is still a need to explore its effectiveness when applied across different archaeological datasets. This paper compares the performance of various transfer learning configurations using two semantic segmentation deep neural networks on two LiDAR datasets. The experimental results indicate that transfer learning-based approaches in archaeology can lead to performance improvements, although a systematic enhancement has not yet been observed. We provide specific insights about the validity of such techniques that can serve as a baseline for future works.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#19978;&#23545;&#19987;&#23478;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25214;&#21040;&#20102;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;FETA-Friends&#19978;&#36229;&#36807;&#20102;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03506</link><description>&lt;p&gt;
&#26080;&#23548;&#25968;&#30340;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Derivative Free Weight-space Ensembling. (arXiv:2307.03506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#19978;&#23545;&#19987;&#23478;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25214;&#21040;&#20102;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;FETA-Friends&#19978;&#36229;&#36807;&#20102;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#20043;&#38388;&#25554;&#20540;&#21487;&#20197;&#22312;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#25506;&#32034;&#22312;&#20004;&#20010;&#20197;&#19978;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#26377;&#19968;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21019;&#24314;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#28304;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#19987;&#23478;&#27169;&#22411;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;&#30446;&#26631;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#22312;&#27169;&#22411;&#26435;&#37325;&#20043;&#38388;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#19968;&#20010;&#22909;&#30340;&#25554;&#20540;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;FETA-Friends&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work suggests that interpolating between the weights of two specialized language models can transfer knowledge between tasks in a way that multi-task learning cannot. However, very few have explored interpolation between more than two models, where each has a distinct knowledge base. In this paper, we introduce Derivative Free Weight-space Ensembling (DFWE), a new few-sample task transfer approach for open-domain dialogue. Our framework creates a set of diverse expert language models trained using a predefined set of source tasks. Next, we finetune each of the expert models on the target task, approaching the target task from several distinct knowledge bases. Finally, we linearly interpolate between the model weights using a gradient-free-optimization algorithm, to efficiently find a good interpolation weighting. We demonstrate the effectiveness of the method on FETA-Friends outperforming the standard pretrain-finetune approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;X&#35282;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#31181;&#24178;&#25200;&#19979;&#20445;&#25345;&#39640;&#20122;&#20687;&#32032;&#31934;&#24230;&#65292;&#24182;&#37319;&#29992;&#20102;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#21644;&#22810;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.03505</link><description>&lt;p&gt;
RCDN -- &#22522;&#20110;&#20808;&#36827;CNN&#27169;&#22411;&#30340;&#40065;&#26834;X&#35282;&#28857;&#26816;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
RCDN -- Robust X-Corner Detection Algorithm based on Advanced CNN Model. (arXiv:2307.03505v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;X&#35282;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#31181;&#24178;&#25200;&#19979;&#20445;&#25345;&#39640;&#20122;&#20687;&#32032;&#31934;&#24230;&#65292;&#24182;&#37319;&#29992;&#20102;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#21644;&#22810;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#26816;&#27979;&#21644;&#23450;&#20301;&#24179;&#38754;&#21644;&#38750;&#24179;&#38754;&#27169;&#24335;&#19978;&#30340;X&#35282;&#28857;&#26159;&#26426;&#22120;&#20154;&#21644;&#26426;&#22120;&#35270;&#35273;&#20013;&#30340;&#26680;&#24515;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#26080;&#27861;&#36798;&#21040;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#36825;&#20004;&#20010;&#26631;&#20934;&#37117;&#26159;&#35780;&#20272;&#26816;&#27979;&#22120;&#24615;&#33021;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#24178;&#25200;&#19979;&#20445;&#25345;&#23545;&#36755;&#20837;&#30340;&#39640;&#20122;&#20687;&#32032;&#31934;&#24230;&#65292;&#20363;&#22914;&#38236;&#22836;&#30072;&#21464;&#12289;&#26497;&#31471;&#23039;&#24577;&#21644;&#22122;&#22768;&#12290;&#25972;&#20010;&#31639;&#27861;&#37319;&#29992;&#20174;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;X&#35282;&#28857;&#26816;&#27979;&#32593;&#32476;&#21644;&#19977;&#20010;&#21518;&#22788;&#29702;&#25216;&#26415;&#26469;&#21306;&#20998;&#27491;&#30830;&#30340;&#35282;&#28857;&#20505;&#36873;&#65292;&#20197;&#21450;&#28151;&#21512;&#20122;&#20687;&#32032;&#32454;&#21270;&#25216;&#26415;&#21644;&#25913;&#36827;&#30340;&#21306;&#22495;&#22686;&#38271;&#31574;&#30053;&#26469;&#33258;&#21160;&#24674;&#22797;&#37096;&#20998;&#21487;&#35265;&#25110;&#36974;&#25377;&#30340;&#26827;&#30424;&#26684;&#32441;&#26679;&#12290;&#23545;&#30495;&#23454;&#21644;&#21512;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#39640;&#30340;&#26816;&#27979;&#29575;&#12289;&#20122;&#20687;&#32032;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate detection and localization of X-corner on both planar and non-planar patterns is a core step in robotics and machine vision. However, previous works could not make a good balance between accuracy and robustness, which are both crucial criteria to evaluate the detectors performance. To address this problem, in this paper we present a novel detection algorithm which can maintain high sub-pixel precision on inputs under multiple interference, such as lens distortion, extreme poses and noise. The whole algorithm, adopting a coarse-to-fine strategy, contains a X-corner detection network and three post-processing techniques to distinguish the correct corner candidates, as well as a mixed sub-pixel refinement technique and an improved region growth strategy to recover the checkerboard pattern partially visible or occluded automatically. Evaluations on real and synthetic images indicate that the presented algorithm has the higher detection rate, sub-pixel accuracy and robustness than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65288;LAM-SC&#65289;&#65292;&#21033;&#29992;&#35813;&#26694;&#26550;&#21487;&#20197;&#20811;&#26381;&#30693;&#35782;&#24211;&#26500;&#24314;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#25968;&#25454;&#39046;&#22495;&#23454;&#29616;&#20102;&#35821;&#20041;&#20998;&#21106;&#12289;&#35821;&#20041;&#38598;&#25104;&#21644;&#33258;&#36866;&#24212;&#35821;&#20041;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2307.03492</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Large AI Model-Based Semantic Communications. (arXiv:2307.03492v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65288;LAM-SC&#65289;&#65292;&#21033;&#29992;&#35813;&#26694;&#26550;&#21487;&#20197;&#20811;&#26381;&#30693;&#35782;&#24211;&#26500;&#24314;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#25968;&#25454;&#39046;&#22495;&#23454;&#29616;&#20102;&#35821;&#20041;&#20998;&#21106;&#12289;&#35821;&#20041;&#38598;&#25104;&#21644;&#33258;&#36866;&#24212;&#35821;&#20041;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#65288;SC&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26234;&#33021;&#33539;&#24335;&#65292;&#20026;&#20803;&#23431;&#23449;&#12289;&#28151;&#21512;&#29616;&#23454;&#21644;&#19975;&#29289;&#20114;&#32852;&#31561;&#26410;&#26469;&#24212;&#29992;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#30446;&#21069;&#30340;SC&#31995;&#32479;&#20013;&#65292;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#30340;&#26500;&#24314;&#38754;&#20020;&#30528;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#30693;&#35782;&#34920;&#31034;&#26377;&#38480;&#12289;&#39057;&#32321;&#30340;&#30693;&#35782;&#26356;&#26032;&#21644;&#19981;&#23433;&#20840;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#26032;&#26041;&#26696;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;SC&#26694;&#26550;&#65288;LAM-SC&#65289;&#65292;&#19987;&#38376;&#29992;&#20110;&#22270;&#20687;&#25968;&#25454;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#22522;&#20110;&#27573;&#33853;&#27169;&#22411;&#65288;SAM&#65289;&#30340;&#30693;&#35782;&#24211;&#65288;SKB&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#23558;&#21407;&#22987;&#22270;&#20687;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#35821;&#20041;&#27573;&#33853;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#38598;&#25104;&#65288;ASI&#65289;&#65292;&#36890;&#36807;&#26435;&#34913;&#30001;SKB&#29983;&#25104;&#30340;&#35821;&#20041;&#27573;&#33853;&#65292;&#26080;&#38656;&#20154;&#24037;&#21442;&#19982;&#24182;&#23558;&#23427;&#20204;&#38598;&#25104;&#20026;&#20855;&#26377;&#35821;&#20041;&#24863;&#30693;&#30340;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#21387;&#32553;&#65288;ASC&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communication (SC) is an emerging intelligent paradigm, offering solutions for various future applications like metaverse, mixed-reality, and the Internet of everything. However, in current SC systems, the construction of the knowledge base (KB) faces several issues, including limited knowledge representation, frequent knowledge updates, and insecure knowledge sharing. Fortunately, the development of the large AI model provides new solutions to overcome above issues. Here, we propose a large AI model-based SC framework (LAM-SC) specifically designed for image data, where we first design the segment anything model (SAM)-based KB (SKB) that can split the original image into different semantic segments by universal semantic knowledge. Then, we present an attention-based semantic integration (ASI) to weigh the semantic segments generated by SKB without human participation and integrate them as the semantic-aware image. Additionally, we propose an adaptive semantic compression (ASC
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03486</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21457;&#29616;&#23618;&#27425;&#21270;&#25104;&#23601;
&lt;/p&gt;
&lt;p&gt;
Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#29615;&#22659;&#20013;&#21457;&#29616;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25104;&#23601;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#38656;&#35201;&#26234;&#33021;&#20307;&#20855;&#22791;&#24191;&#27867;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#27867;&#21270;&#21644;&#38271;&#26399;&#25512;&#29702;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#25110;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#35748;&#20026;&#26174;&#24335;&#30340;&#38271;&#26399;&#35268;&#21010;&#27169;&#22359;&#23545;&#20110;&#23398;&#20064;&#23618;&#27425;&#21270;&#25104;&#23601;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#25110;&#22823;&#22411;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36817;&#26399;&#23454;&#26045;&#23454;&#36341;&#20013;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#31639;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;PPO&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#39044;&#27979;&#19979;&#19968;&#20010;&#35201;&#35299;&#38145;&#30340;&#25104;&#23601;&#65292;&#23613;&#31649;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#36739;&#20302;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#25104;&#23601;&#33976;&#39311;&#65292;&#21487;&#20197;&#21152;&#24378;PPO&#26234;&#33021;&#20307;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20219;&#21153;&#32423;&#39592;&#24178;&#23548;&#21521;&#26799;&#24230;&#25130;&#26029;&#33539;&#24335;&#65292;&#36890;&#36807;&#29420;&#31435;&#30340;&#26799;&#24230;&#25130;&#26029;&#21644;&#32479;&#19968;&#30340;&#33539;&#25968;&#32553;&#25918;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#20559;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03465</link><description>&lt;p&gt;
TBGC: &#22810;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;&#23398;&#20064;&#30340;&#20219;&#21153;&#32423;&#39592;&#24178;&#23548;&#21521;&#26799;&#24230;&#25130;&#26029;
&lt;/p&gt;
&lt;p&gt;
TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning. (arXiv:2307.03465v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03465
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20219;&#21153;&#32423;&#39592;&#24178;&#23548;&#21521;&#26799;&#24230;&#25130;&#26029;&#33539;&#24335;&#65292;&#36890;&#36807;&#29420;&#31435;&#30340;&#26799;&#24230;&#25130;&#26029;&#21644;&#32479;&#19968;&#30340;&#33539;&#25968;&#32553;&#25918;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#20559;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AllInOne&#35757;&#32451;&#33539;&#24335;&#20197;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#24335;&#23558;&#21508;&#31181;&#20219;&#21153;&#38598;&#20013;&#21040;&#32479;&#19968;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#19981;&#21516;&#20219;&#21153;&#30340;&#26799;&#24230;&#33539;&#25968;&#21487;&#33021;&#24046;&#24322;&#24456;&#22823;&#65292;&#20351;&#24471;&#39592;&#24178;&#36807;&#20998;&#20559;&#21521;&#26576;&#20010;&#29305;&#23450;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#32423;&#39592;&#24178;&#23548;&#21521;&#26799;&#24230;&#25130;&#26029;&#33539;&#24335;&#65292;&#19982;&#26222;&#36890;&#30340;&#26799;&#24230;&#25130;&#26029;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#26377;&#20004;&#20010;&#37325;&#28857;&#65306;1&#65289;&#23545;&#27599;&#20010;&#20219;&#21153;&#29420;&#31435;&#36827;&#34892;&#26799;&#24230;&#25130;&#26029;&#65307;2&#65289;&#20174;&#27599;&#20010;&#20219;&#21153;&#29983;&#25104;&#30340;&#39592;&#24178;&#26799;&#24230;&#37325;&#26032;&#32553;&#25918;&#21040;&#30456;&#21516;&#30340;&#33539;&#25968;&#23610;&#24230;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35748;&#20026;&#20219;&#21153;&#32423;&#39592;&#24178;&#23548;&#21521;&#26799;&#24230;&#25130;&#26029;&#33539;&#24335;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#20197;&#32531;&#35299;&#26799;&#24230;&#20559;&#21521;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20998;&#25903;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#22312;&#19981;&#21516;&#20998;&#25903;&#20013;&#25918;&#32622;&#20914;&#31361;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#26368;&#32456;&#22312;&#27604;&#36187;&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The AllInOne training paradigm squeezes a wide range of tasks into a unified model in a multi-task learning manner. However, optimization in multi-task learning is more challenge than single-task learning, as the gradient norm from different tasks may vary greatly, making the backbone overly biased towards one specific task. To address this issue, we propose the task-level backbone-oriented gradient clip paradigm, compared with the vanilla gradient clip method, it has two points of emphasis:1) gradient clip is performed independently for each task. 2) backbone gradients generated from each task are rescaled to the same norm scale. Based on the experimental results, we argue that the task-level backbone-oriented gradient clip paradigm can relieve the gradient bias problem to some extent. We also propose a novel multi-branch data augmentation strategy where conflict augmentations are placed in different branches. Our approach has been shown to be effective and finally achieve 1st place i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#38544;&#20889;&#26415;&#65292;&#36890;&#36807;&#23558;&#31192;&#23494;DNN&#27169;&#22411;&#20266;&#35013;&#25104;&#21478;&#19968;&#20010;&#26222;&#36890;&#23398;&#20064;&#20219;&#21153;&#30340;&#38544;&#20889;DNN&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;DNN&#27169;&#22411;&#30340;&#31192;&#23494;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2307.03444</link><description>&lt;p&gt;
&#20174;&#32593;&#32476;&#21040;&#32593;&#32476;&#30340;&#28145;&#24230;&#32593;&#32476;&#38544;&#20889;&#26415;
&lt;/p&gt;
&lt;p&gt;
Towards Deep Network Steganography: From Networks to Networks. (arXiv:2307.03444v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#38544;&#20889;&#26415;&#65292;&#36890;&#36807;&#23558;&#31192;&#23494;DNN&#27169;&#22411;&#20266;&#35013;&#25104;&#21478;&#19968;&#20010;&#26222;&#36890;&#23398;&#20064;&#20219;&#21153;&#30340;&#38544;&#20889;DNN&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;DNN&#27169;&#22411;&#30340;&#31192;&#23494;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22914;&#20309;&#22312;&#20844;&#20849;&#20449;&#36947;&#20013;&#31192;&#23494;&#20256;&#36755;DNN&#27169;&#22411;&#24341;&#36215;&#20102;&#25105;&#20204;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#37027;&#20123;&#29992;&#20110;&#31192;&#23494;&#23398;&#20064;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;DNN&#27169;&#22411;&#31192;&#23494;&#20256;&#36755;&#30340;&#28145;&#24230;&#32593;&#32476;&#38544;&#20889;&#26415;&#12290;&#19982;&#29616;&#26377;&#30340;&#38544;&#20889;&#26041;&#26696;&#19981;&#21516;&#65292;&#20854;&#37325;&#28857;&#26159;&#36890;&#36807;&#23545;&#23553;&#38754;&#25968;&#25454;&#36827;&#34892;&#32454;&#24494;&#20462;&#25913;&#20197;&#36866;&#24212;&#31192;&#23494;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#26159;&#20197;&#23398;&#20064;&#20219;&#21153;&#20026;&#23548;&#21521;&#30340;&#65292;&#23558;&#31192;&#23494;DNN&#27169;&#22411;(&#31216;&#20026;&#31192;&#23494;&#23398;&#20064;&#20219;&#21153;)&#20266;&#35013;&#25104;&#21478;&#19968;&#20010;&#22312;&#38544;&#20889;DNN&#27169;&#22411;(&#31216;&#20026;&#38544;&#20889;&#23398;&#20064;&#20219;&#21153;)&#20013;&#36827;&#34892;&#30340;&#26222;&#36890;&#23398;&#20064;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#28388;&#27874;&#22120;&#25554;&#20837;&#26041;&#26696;&#65292;&#23558;&#24178;&#25200;&#28388;&#27874;&#22120;&#25554;&#20837;&#21040;&#31192;&#23494;DNN&#27169;&#22411;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#24418;&#25104;&#19968;&#20010;&#38544;&#20889;DNN&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23494;&#38053;&#21644;&#38468;&#21152;&#20449;&#24687;&#38544;&#34255;&#23558;&#36825;&#20123;&#20301;&#32622;&#23884;&#20837;&#21040;&#38544;&#20889;DNN&#27169;&#22411;&#20013;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#37096;&#20998;&#20248;&#21270;&#28608;&#27963;&#24178;&#25200;&#28388;&#27874;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread applications of the deep neural network (DNN), how to covertly transmit the DNN models in public channels brings us the attention, especially for those trained for secret-learning tasks. In this paper, we propose deep network steganography for the covert communication of DNN models. Unlike the existing steganography schemes which focus on the subtle modification of the cover data to accommodate the secrets, our scheme is learning task oriented, where the learning task of the secret DNN model (termed as secret-learning task) is disguised into another ordinary learning task conducted in a stego DNN model (termed as stego-learning task). To this end, we propose a gradient-based filter insertion scheme to insert interference filters into the important positions in the secret DNN model to form a stego DNN model. These positions are then embedded into the stego DNN model using a key by side information hiding. Finally, we activate the interference filters by a partial opt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#36845;&#20195;&#30340;&#31895;&#21040;&#32454;&#21464;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#32852;&#21512;&#20223;&#23556;&#21644;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#21333;&#20010;&#32593;&#32476;&#20013;&#23454;&#29616;&#31895;&#21040;&#32454;&#37197;&#20934;&#65292;&#24182;&#22312;&#37197;&#20934;&#31934;&#24230;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03421</link><description>&lt;p&gt;
&#38750;&#36845;&#20195;&#30340;&#31895;&#21040;&#32454;&#21464;&#25442;&#32593;&#32476;&#29992;&#20110;&#32852;&#21512;&#20223;&#23556;&#21644;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration. (arXiv:2307.03421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#36845;&#20195;&#30340;&#31895;&#21040;&#32454;&#21464;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#32852;&#21512;&#20223;&#23556;&#21644;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#21333;&#20010;&#32593;&#32476;&#20013;&#23454;&#29616;&#31895;&#21040;&#32454;&#37197;&#20934;&#65292;&#24182;&#22312;&#37197;&#20934;&#31934;&#24230;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#37197;&#20934;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28145;&#24230;&#37197;&#20934;&#26041;&#27861;&#22240;&#20854;&#33021;&#22815;&#36827;&#34892;&#24555;&#36895;&#30340;&#31471;&#21040;&#31471;&#37197;&#20934;&#32780;&#34987;&#24191;&#27867;&#35748;&#21487;&#12290;&#35768;&#22810;&#28145;&#24230;&#37197;&#20934;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#22810;&#20010;&#20855;&#26377;&#32423;&#32852;&#32593;&#32476;&#30340;&#37197;&#20934;&#27493;&#39588;&#26469;&#23454;&#29616;&#31895;&#21040;&#32454;&#37197;&#20934;&#65292;&#24050;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#38750;&#36845;&#20195;&#30340;&#31895;&#21040;&#32454;&#37197;&#20934;&#65288;NICE&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#32593;&#32476;&#20013;&#36827;&#34892;&#31895;&#21040;&#32454;&#37197;&#20934;&#65292;&#24182;&#26174;&#31034;&#20986;&#37197;&#20934;&#31934;&#24230;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NICE&#37197;&#20934;&#26041;&#27861;&#20027;&#35201;&#20391;&#37325;&#20110;&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#32780;&#20223;&#23556;&#37197;&#20934;&#20316;&#20026;&#24120;&#35265;&#30340;&#21069;&#25552;&#26465;&#20214;&#20173;&#28982;&#20381;&#36182;&#32791;&#26102;&#30340;&#20256;&#32479;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#25110;&#39069;&#22806;&#30340;&#20223;&#23556;&#37197;&#20934;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;NICE&#37197;&#20934;&#26041;&#27861;&#21463;&#21040;&#21367;&#31215;&#25805;&#20316;&#26412;&#22320;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#34920;&#31034;&#21487;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image registration is a fundamental requirement for medical image analysis. Deep registration methods based on deep learning have been widely recognized for their capabilities to perform fast end-to-end registration. Many deep registration methods achieved state-of-the-art performance by performing coarse-to-fine registration, where multiple registration steps were iterated with cascaded networks. Recently, Non-Iterative Coarse-to-finE (NICE) registration methods have been proposed to perform coarse-to-fine registration in a single network and showed advantages in both registration accuracy and runtime. However, existing NICE registration methods mainly focus on deformable registration, while affine registration, a common prerequisite, is still reliant on time-consuming traditional optimization-based methods or extra affine registration networks. In addition, existing NICE registration methods are limited by the intrinsic locality of convolution operations. Transformers may address thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;QI2&#65292;&#35813;&#24037;&#20855;&#25903;&#25345;&#23545;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#39564;&#35777;&#21644;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#30340;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03419</link><description>&lt;p&gt;
QI2 -- &#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
QI2 -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.03419v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;QI2&#65292;&#35813;&#24037;&#20855;&#25903;&#25345;&#23545;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#39564;&#35777;&#21644;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#30340;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25968;&#25454;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#22823;&#25968;&#25454;&#30340;&#22686;&#38271;&#24433;&#21709;&#21644;&#20998;&#24067;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#27431;&#27954;&#22996;&#21592;&#20250;&#35745;&#21010;&#30340;AI&#27861;&#26696;&#20026;&#25968;&#25454;&#36136;&#37327;&#23450;&#20041;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27861;&#24459;&#35201;&#27714;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24066;&#22330;&#25512;&#20986;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25903;&#25345;&#22810;&#20010;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#36807;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#39564;&#35777;&#23450;&#37327;&#25968;&#25454;&#36136;&#37327;&#35201;&#27714;&#12290;&#36890;&#36807;&#23567;&#20363;&#23376;&#25968;&#25454;&#38598;&#20171;&#32461;&#21644;&#35299;&#37322;&#20102;&#35813;&#27010;&#24565;&#21644;&#20248;&#21183;&#12290;&#22914;&#20309;&#24212;&#29992;&#35813;&#26041;&#27861;&#22312;&#22522;&#20110;&#25163;&#20889;&#25968;&#23383;&#30340;&#30693;&#21517;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of high data quality is increasing with the growing impact and distribution of ML systems and big data. Also the planned AI Act from the European commission defines challenging legal requirements for data quality especially for the market introduction of safety relevant ML systems. In this paper we introduce a novel approach that supports the data quality assurance process of multiple data quality aspects. This approach enables the verification of quantitative data quality requirements. The concept and benefits are introduced and explained on small example data sets. How the method is applied is demonstrated on the well known MNIST data set based an handwritten digits.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25506;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#22312;&#36712;&#36857;&#21387;&#32553;&#21644;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20135;&#29983;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23454;&#29616;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03406</link><description>&lt;p&gt;
&#20351;&#29992;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#20316;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#38544;&#24335;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning. (arXiv:2307.03406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25506;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#22312;&#36712;&#36857;&#21387;&#32553;&#21644;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20135;&#29983;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23454;&#29616;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#36712;&#36857;&#25968;&#25454;&#19978;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#22909;&#22788;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#26159;&#21542;&#20855;&#22791;&#23558;&#36712;&#36857;&#21387;&#32553;&#20026;&#26377;&#29992;&#34920;&#31034;&#24182;&#23545;&#31574;&#30053;&#23398;&#20064;&#26377;&#25152;&#36129;&#29486;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#39318;&#20808;&#20351;&#29992;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#24635;&#32467;&#36712;&#36857;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#20197;&#21450;&#19968;&#20010;&#26399;&#26395;&#30340;&#30446;&#26631;&#12290;&#36825;&#20010;&#35774;&#35745;&#20351;&#24471;&#35768;&#22810;&#29616;&#26377;&#30340;&#30417;&#30563;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26469;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23548;&#33268;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;AntMaze&#65292;FrankaKitchen&#21644;Locomotion&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Recent work has demonstrated the effectiveness of formulating decision making as a supervised learning problem on offline-collected trajectories. However, the benefits of performing sequence modeling on trajectory data is not yet clear. In this work we investigate if sequence modeling has the capability to condense trajectories into useful representations that can contribute to policy learning. To achieve this, we adopt a two-stage framework that first summarizes trajectories with sequence modeling techniques, and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, and obser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.03393</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22240;&#20854;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20197;&#25991;&#26412;&#33410;&#28857;&#23646;&#24615;&#20026;&#20027;&#30340;&#22270;&#23398;&#20064;&#26368;&#27969;&#34892;&#30340;&#27969;&#31243;&#20027;&#35201;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#21033;&#29992;&#27973;&#23618;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#21021;&#22987;&#33410;&#28857;&#34920;&#31034;&#65292;&#20294;&#23384;&#22312;&#36890;&#29992;&#30693;&#35782;&#21644;&#28145;&#21051;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35777;&#26126;&#20855;&#26377;&#24191;&#27867;&#30340;&#24120;&#35782;&#21644;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24050;&#32463;&#39072;&#35206;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20004;&#31181;&#21487;&#33021;&#30340;&#27969;&#31243;&#65306;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#21644;LLMs&#20316;&#20026;&#39044;&#27979;&#22120;&#12290;&#21069;&#32773;&#21033;&#29992;LLMs&#36890;&#36807;&#20854;&#28023;&#37327;&#30693;&#35782;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;GNNs&#29983;&#25104;&#39044;&#27979;&#12290;&#21518;&#32773;&#35797;&#22270;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03380</link><description>&lt;p&gt;
&#20851;&#20110;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;ML&#27169;&#22411;&#33030;&#24369;&#24615;&#65292;&#20844;&#24179;&#24615;&#20197;&#21450;&#35299;&#37322;&#24615;&#30340;&#32570;&#20047;&#31561;&#37325;&#35201;&#38382;&#39064;&#38656;&#35201;&#31215;&#26497;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#24418;&#24335;&#21270;&#30340;ML&#27169;&#22411;&#39564;&#35777;&#12290;XAI&#30340;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;Anchors&#65289;&#21644;&#29305;&#24449;&#24402;&#22240;&#25216;&#26415;&#65288;&#20363;&#22914;&#65292;LIME&#21644;SHAP&#65289;&#12290;&#23613;&#31649;&#26377;&#24076;&#26395;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#37117;&#23481;&#26131;&#20986;&#29616;&#19968;&#31995;&#21015;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#35299;&#37322;&#19981;&#27491;&#30830;&#21644;&#36229;&#20986;&#20998;&#24067;&#37319;&#26679;&#12290;&#36817;&#26399;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#65288;FXAI&#65289;&#34429;&#28982;&#20316;&#20026;&#20197;&#19978;&#26041;&#27861;&#30340;&#26367;&#20195;&#21697;&#24182;&#36991;&#20813;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20363;&#22914;&#65292;&#38500;&#20102;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#22806;&#65292;&#36825;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#29305;&#24449;&#24402;&#22240;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#20026;&#28216;&#25103;AI&#35774;&#35745;&#30340;&#39640;&#25928;&#36335;&#24452;&#36319;&#38543;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35843;&#25972;&#24050;&#26377;&#25216;&#26415;&#65292;&#35774;&#35745;&#20986;&#20855;&#26377;&#21487;&#35843;&#21442;&#25968;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#27979;&#35797;&#22330;&#26223;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#31867;&#22411;&#36335;&#24452;&#21644;&#36710;&#36742;&#19978;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#65292;&#22312;&#24635;&#21345;&#20303;&#20107;&#20214;&#25968;&#37327;&#19978;&#21462;&#24471;&#20102;70%&#30340;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2307.03379</link><description>&lt;p&gt;
&#28216;&#25103;AI&#20013;&#39640;&#25928;&#30340;&#22320;&#38754;&#36710;&#36742;&#36335;&#24452;&#36319;&#38543;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Efficient Ground Vehicle Path Following in Game AI. (arXiv:2307.03379v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#20026;&#28216;&#25103;AI&#35774;&#35745;&#30340;&#39640;&#25928;&#36335;&#24452;&#36319;&#38543;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35843;&#25972;&#24050;&#26377;&#25216;&#26415;&#65292;&#35774;&#35745;&#20986;&#20855;&#26377;&#21487;&#35843;&#21442;&#25968;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#27979;&#35797;&#22330;&#26223;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#31867;&#22411;&#36335;&#24452;&#21644;&#36710;&#36742;&#19978;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#65292;&#22312;&#24635;&#21345;&#20303;&#20107;&#20214;&#25968;&#37327;&#19978;&#21462;&#24471;&#20102;70%&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#20026;&#28216;&#25103;AI&#35774;&#35745;&#30340;&#39640;&#25928;&#36335;&#24452;&#36319;&#38543;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#37325;&#28857;&#26159;&#36890;&#36807;&#35843;&#25972;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#65292;&#35774;&#35745;&#20986;&#20855;&#26377;&#21487;&#35843;&#21442;&#25968;&#12289;&#39640;&#25928;&#30340;&#22522;&#20934;&#36335;&#24452;&#36319;&#38543;&#22120;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#29305;&#21035;&#20851;&#27880;&#35745;&#31639;&#30446;&#26631;&#36895;&#24230;&#65292;&#20351;&#29992;&#20108;&#27425;&#36125;&#22622;&#23572;&#26354;&#32447;&#26469;&#20272;&#35745;&#36335;&#24452;&#26354;&#29575;&#12290;&#36890;&#36807;&#22312;&#31532;&#19968;&#20154;&#31216;&#23556;&#20987;&#28216;&#25103;&#20013;&#36827;&#34892;&#21508;&#31181;&#27979;&#35797;&#22330;&#26223;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#36335;&#24452;&#36319;&#38543;&#22120;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#36335;&#24452;&#21644;&#36710;&#36742;&#26102;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;&#19982;&#29616;&#26377;&#36335;&#24452;&#36319;&#38543;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#24635;&#21345;&#20303;&#20107;&#20214;&#25968;&#37327;&#20943;&#23569;70%&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This short paper presents an efficient path following solution for ground vehicles tailored to game AI. Our focus is on adapting established techniques to design simple solutions with parameters that are easily tunable for an efficient benchmark path follower. Our solution pays particular attention to computing a target speed which uses quadratic Bezier curves to estimate the path curvature. The performance of the proposed path follower is evaluated through a variety of test scenarios in a first-person shooter game, demonstrating its effectiveness and robustness in handling different types of paths and vehicles. We achieved a 70% decrease in the total number of stuck events compared to an existing path following solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#20307;&#21270;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;Transformer&#20027;&#24178;&#32593;&#32476;&#65292;&#23454;&#29616;&#32852;&#21512;&#29305;&#24449;&#25552;&#21462;&#21644;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#30446;&#26631;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03373</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#30340;&#25506;&#32034;&#65306;&#22810;&#27169;&#24577;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment. (arXiv:2307.03373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#20307;&#21270;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;Transformer&#20027;&#24178;&#32593;&#32476;&#65292;&#23454;&#29616;&#32852;&#21512;&#29305;&#24449;&#25552;&#21462;&#21644;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#30446;&#26631;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20027;&#27969;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#37096;&#20998;&#65292;&#21363;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#34701;&#21512;&#27169;&#22411;&#12290;&#20026;&#20102;&#36861;&#27714;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#24120;&#24120;&#20351;&#29992;&#23450;&#21046;&#21644;&#26356;&#37325;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#22120;&#23558;&#29305;&#24449;&#25552;&#21462;&#21644;&#29305;&#24449;&#38598;&#25104;&#20998;&#24320;&#65292;&#23548;&#33268;&#25552;&#21462;&#30340;&#29305;&#24449;&#32570;&#20047;&#35821;&#20041;&#24341;&#23548;&#65292;&#22312;&#22797;&#26434;&#22330;&#26223;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#30446;&#26631;&#24863;&#30693;&#33021;&#21147;&#65292;&#20363;&#22914;&#30456;&#20284;&#30340;&#24178;&#25200;&#29289;&#21644;&#26497;&#31471;&#20809;&#29031;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#21463;&#21040;&#36817;&#26399;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#32479;&#19968;&#26550;&#26500;&#25506;&#32034;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#20307;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#32479;&#19968;&#30340;Transformer&#20027;&#24178;&#32593;&#32476;&#26469;&#23398;&#20064;&#32852;&#21512;&#29305;&#24449;&#25552;&#21462;&#21644;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28151;&#21512;&#21407;&#22987;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#21495;&#26469;&#29983;&#25104;&#27880;&#20837;&#35821;&#35328;&#30340;&#35270;&#35273;&#21333;&#20803;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current mainstream vision-language (VL) tracking framework consists of three parts, \ie a visual feature extractor, a language feature extractor, and a fusion model. To pursue better performance, a natural modus operandi for VL tracking is employing customized and heavier unimodal encoders, and multi-modal fusion models. Albeit effective, existing VL trackers separate feature extraction and feature integration, resulting in extracted features that lack semantic guidance and have limited target-aware capability in complex scenarios, \eg similar distractors and extreme illumination. In this work, inspired by the recent success of exploring foundation models with unified architecture for both natural language and computer vision tasks, we propose an All-in-One framework, which learns joint feature extraction and interaction by adopting a unified transformer backbone. Specifically, we mix raw vision and language signals to generate language-injected vision tokens, which we then concatenate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20154;&#26426;&#21512;&#20316;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#27807;&#36890;&#65292;&#20197;&#22788;&#29702;&#20195;&#29702;&#20043;&#38388;&#20851;&#20110;&#35745;&#21010;&#30340;&#20449;&#24565;&#24046;&#24322;&#65292;&#36890;&#36807;&#21033;&#29992;&#35748;&#30693;&#36923;&#36753;&#26469;&#29702;&#35299;&#21644;&#35299;&#20915;&#36825;&#20123;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.03362</link><description>&lt;p&gt;
&#20154;&#26426;&#21512;&#20316;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#27807;&#36890;&#65306;&#22788;&#29702;&#20195;&#29702;&#20851;&#20110;&#35745;&#21010;&#30340;&#20449;&#24565;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Adaptation and Communication in Human-Robot Teaming to Handle Discrepancies in Agents' Beliefs about Plans. (arXiv:2307.03362v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20154;&#26426;&#21512;&#20316;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#27807;&#36890;&#65292;&#20197;&#22788;&#29702;&#20195;&#29702;&#20043;&#38388;&#20851;&#20110;&#35745;&#21010;&#30340;&#20449;&#24565;&#24046;&#24322;&#65292;&#36890;&#36807;&#21033;&#29992;&#35748;&#30693;&#36923;&#36753;&#26469;&#29702;&#35299;&#21644;&#35299;&#20915;&#36825;&#20123;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#29702;&#20849;&#21516;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#26102;&#65292;&#25317;&#26377;&#19968;&#20123;&#20849;&#20139;&#30340;&#20219;&#21153;&#27969;&#31243;&#30340;&#24515;&#29702;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#20123;&#27969;&#31243;&#26159;&#23454;&#29616;&#30446;&#26631;&#30340;&#21487;&#34892;&#35745;&#21010;&#30340;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#32463;&#24120;&#20986;&#29616;&#36825;&#26679;&#30340;&#24773;&#20917;&#65292;&#21363;&#26080;&#27861;&#20445;&#35777;&#23384;&#22312;&#36825;&#26679;&#30340;&#20849;&#20139;&#24515;&#29702;&#27169;&#22411;&#65292;&#27604;&#22914;&#22312;&#20020;&#26102;&#22242;&#38431;&#20013;&#20195;&#29702;&#21487;&#33021;&#36981;&#24490;&#19981;&#21516;&#30340;&#32422;&#23450;&#65292;&#25110;&#32773;&#24403;&#20986;&#29616;&#21482;&#26377;&#37096;&#20998;&#20195;&#29702;&#30693;&#26195;&#30340;&#20020;&#26102;&#32422;&#26463;&#26102;&#12290;&#20197;&#21069;&#20851;&#20110;&#20154;&#26426;&#21512;&#20316;&#30340;&#30740;&#31350;&#20551;&#35774;&#22242;&#38431;&#25317;&#26377;&#19968;&#32452;&#20849;&#20139;&#30340;&#27969;&#31243;&#65292;&#32780;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#36825;&#31181;&#20551;&#35774;&#19981;&#25104;&#31435;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#35748;&#30693;&#36923;&#36753;&#20351;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#24444;&#27492;&#20851;&#20110;&#21487;&#34892;&#35745;&#21010;&#30340;&#20449;&#24565;&#24046;&#24322;&#65292;&#24182;&#21160;&#24577;&#35268;&#21010;&#20182;&#20204;&#30340;&#34892;&#21160;&#26469;&#36866;&#24212;&#25110;&#27807;&#36890;&#20197;&#35299;&#20915;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#20102;&#26465;&#20214;&#20449;&#24565;&#36923;&#36753;&#30340;&#24418;&#24335;&#65292;&#20197;&#25551;&#36848;&#30693;&#35782;&#24211;&#65292;&#20174;&#32780;&#26126;&#30830;&#34920;&#31034;&#20195;&#29702;&#23545;&#21487;&#34892;&#35745;&#21010;&#21644;&#25191;&#34892;&#29366;&#24577;&#30340;&#23884;&#22871;&#20449;&#24565;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#32447;&#25191;&#34892;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When agents collaborate on a task, it is important that they have some shared mental model of the task routines -- the set of feasible plans towards achieving the goals. However, in reality, situations often arise that such a shared mental model cannot be guaranteed, such as in ad-hoc teams where agents may follow different conventions or when contingent constraints arise that only some agents are aware of. Previous work on human-robot teaming has assumed that the team has a set of shared routines, which breaks down in these situations. In this work, we leverage epistemic logic to enable agents to understand the discrepancy in each other's beliefs about feasible plans and dynamically plan their actions to adapt or communicate to resolve the discrepancy. We propose a formalism that extends conditional doxastic logic to describe knowledge bases in order to explicitly represent agents' nested beliefs on the feasible plans and state of execution. We provide an online execution algorithm ba
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20197;&#24050;&#24314;&#31435;&#30340;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#37327;&#21270;&#20102;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#35748;&#21516;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#24615;&#21462;&#21521;&#30340;&#20449;&#21495;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.03360</link><description>&lt;p&gt;
&#22312;&#20132;&#21449;&#38382;&#31572;&#32972;&#26223;&#19979;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24577;&#24230;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Evaluating Biased Attitude Associations of Language Models in an Intersectional Context. (arXiv:2307.03360v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03360
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20197;&#24050;&#24314;&#31435;&#30340;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#37327;&#21270;&#20102;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#35748;&#21516;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#24615;&#21462;&#21521;&#30340;&#20449;&#21495;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#36825;&#20123;&#35821;&#26009;&#24211;&#20013;&#23884;&#20837;&#20102;&#24515;&#29702;&#23398;&#20013;&#24050;&#32463;&#35760;&#24405;&#30340;&#38544;&#21547;&#20559;&#35265;&#12290;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#65288;&#24841;&#24555;/&#19981;&#24841;&#24555;&#65289;&#20915;&#23450;&#20102;&#31038;&#20250;&#35748;&#30693;&#20013;&#23545;&#32676;&#20307;&#21644;&#27010;&#24565;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20132;&#21449;&#38382;&#31572;&#32972;&#26223;&#30340;&#21477;&#23376;&#27169;&#26495;&#65292;&#37327;&#21270;&#20102;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#24180;&#40836;&#12289;&#25945;&#32946;&#12289;&#24615;&#21035;&#12289;&#36523;&#39640;&#12289;&#26234;&#21147;&#12289;&#25991;&#21270;&#32032;&#20859;&#12289;&#31181;&#26063;&#12289;&#23447;&#25945;&#12289;&#24615;&#21035;&#12289;&#24615;&#21462;&#21521;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#20307;&#37325;&#26377;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#37319;&#29992;&#27010;&#24565;&#25237;&#24433;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#25429;&#25417;&#24773;&#32490;&#20851;&#32852;&#30340;&#23376;&#31354;&#38388;&#12290;&#23558;&#22522;&#20110;&#25237;&#24433;&#30340;&#26041;&#27861;&#35843;&#25972;&#20026;&#37327;&#21270;&#20559;&#35265;&#30340;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#35748;&#21516;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#24615;&#21462;&#21521;&#30340;&#20449;&#21495;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#21644;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English language models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. Adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. We find that the largest and better-performing model
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35299;&#20915;&#20307;&#22806;&#33180;&#32954;&#27687;&#21512;(ECMO)&#27835;&#30103;&#36873;&#25321;&#20559;&#24046;&#21644;&#31232;&#32570;&#27835;&#30103;&#26696;&#20363;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Treatment Variational AutoEncoder (TVAE)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#20998;&#26512;&#65292;&#20197;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.03315</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#32544;&#30340;&#28508;&#22312;&#34920;&#31034;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#20197;&#33719;&#21462;&#21487;&#29992;&#27835;&#30103;
&lt;/p&gt;
&lt;p&gt;
Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation. (arXiv:2307.03315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03315
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#20915;&#20307;&#22806;&#33180;&#32954;&#27687;&#21512;(ECMO)&#27835;&#30103;&#36873;&#25321;&#20559;&#24046;&#21644;&#31232;&#32570;&#27835;&#30103;&#26696;&#20363;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Treatment Variational AutoEncoder (TVAE)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#20998;&#26512;&#65292;&#20197;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#22806;&#33180;&#32954;&#27687;&#21512;(ECMO)&#26159;&#23545;&#20110;COVID-19&#24739;&#32773;&#30340;&#37325;&#35201;&#30340;&#29983;&#21629;&#25903;&#25345;&#26041;&#24335;&#65292;&#36825;&#20123;&#24739;&#32773;&#23545;&#20256;&#32479;&#27835;&#30103;&#26041;&#27861;&#26080;&#25928;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#31232;&#32570;&#19988;&#25216;&#26415;&#22797;&#26434;&#30340;&#27835;&#30103;&#36873;&#25321;&#65292;&#36866;&#24403;&#30340;&#27835;&#30103;&#20915;&#31574;&#19968;&#30452;&#22791;&#21463;&#20105;&#35758;&#65292;&#23545;&#20110;&#35841;&#20250;&#20174;&#20013;&#21463;&#30410;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#12290;&#20026;&#20102;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#65292;&#39044;&#27979;&#27835;&#30103;&#38656;&#27714;&#21644;&#28508;&#22312;&#30340;&#27835;&#30103;&#19982;&#38750;&#27835;&#30103;&#21453;&#24212;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#38024;&#23545;&#36825;&#19968;&#20020;&#24202;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Treatment Variational AutoEncoder (TVAE)&#65292;&#19968;&#31181;&#29992;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#12290;TVAE&#19987;&#38376;&#35774;&#35745;&#26469;&#35299;&#20915;&#20687;ECMO&#36825;&#26679;&#20855;&#26377;&#24378;&#28872;&#27835;&#30103;&#36873;&#25321;&#20559;&#24046;&#21644;&#27835;&#30103;&#26696;&#20363;&#31232;&#32570;&#30340;&#24314;&#27169;&#25361;&#25112;&#12290;TVAE&#23558;&#27835;&#30103;&#20915;&#31574;&#35270;&#20026;&#19968;&#20010;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#24739;&#32773;&#30340;&#28508;&#22312;&#27835;&#30103;&#20998;&#37197;&#20197;&#21450;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#32467;&#26524;&#20316;&#20026;&#20182;&#20204;&#22266;&#26377;&#29305;&#24449;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#34987;&#34920;&#24449;&#65292;
&lt;/p&gt;
&lt;p&gt;
Extracorporeal membrane oxygenation (ECMO) is an essential life-supporting modality for COVID-19 patients who are refractory to conventional therapies. However, the proper treatment decision has been the subject of significant debate and it remains controversial about who benefits from this scarcely available and technically complex treatment option. To support clinical decisions, it is a critical need to predict the treatment need and the potential treatment and no-treatment responses. Targeting this clinical challenge, we propose Treatment Variational AutoEncoder (TVAE), a novel approach for individualized treatment analysis. TVAE is specifically designed to address the modeling challenges like ECMO with strong treatment selection bias and scarce treatment cases. TVAE conceptualizes the treatment decision as a multi-scale problem. We model a patient's potential treatment assignment and the factual and counterfactual outcomes as part of their intrinsic characteristics that can be repr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#29699;&#38754;&#35856;&#27874;&#34920;&#31034;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#26059;&#36716;&#19981;&#21464;&#21644;&#31561;&#21464;&#29305;&#24449;&#65292;&#20197;&#21450;&#29699;&#38754;&#19978;&#20449;&#21495;&#30340;&#21367;&#31215;&#21644;&#31934;&#30830;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#23558;&#36825;&#20123;&#26041;&#27861;&#25512;&#24191;&#21040;&#20102;&#30690;&#37327;&#35856;&#27874;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.03311</link><description>&lt;p&gt;
&#20851;&#20110;&#29699;&#38754;&#35856;&#27874;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#12289;&#31561;&#21464;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#21367;&#31215;&#30340;&#26631;&#37327;&#21644;&#30690;&#37327;&#25968;&#25454;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Invariance, Equivariance, Correlation and Convolution of Spherical Harmonic Representations for Scalar and Vectorial Data. (arXiv:2307.03311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03311
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#29699;&#38754;&#35856;&#27874;&#34920;&#31034;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#26059;&#36716;&#19981;&#21464;&#21644;&#31561;&#21464;&#29305;&#24449;&#65292;&#20197;&#21450;&#29699;&#38754;&#19978;&#20449;&#21495;&#30340;&#21367;&#31215;&#21644;&#31934;&#30830;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#23558;&#36825;&#20123;&#26041;&#27861;&#25512;&#24191;&#21040;&#20102;&#30690;&#37327;&#35856;&#27874;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29699;&#38754;&#35856;&#27874;&#65288;SH&#65289;&#22495;&#20013;&#30340;&#25968;&#25454;&#25968;&#23398;&#34920;&#31034;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#30028;&#37325;&#26032;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#23545;SH&#34920;&#31034;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#38469;&#23454;&#29616;&#36827;&#34892;&#20102;&#28145;&#20837;&#20171;&#32461;&#65292;&#24635;&#32467;&#20102;&#20851;&#20110;&#26059;&#36716;&#19981;&#21464;&#21644;&#31561;&#21464;&#29305;&#24449;&#20197;&#21450;&#29699;&#38754;&#19978;&#20449;&#21495;&#30340;&#21367;&#31215;&#21644;&#31934;&#30830;&#30456;&#20851;&#24615;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20174;&#26631;&#37327;SH&#34920;&#31034;&#25193;&#23637;&#21040;&#30690;&#37327;&#35856;&#27874;&#65288;VH&#65289;&#65292;&#20026;&#29699;&#38754;&#19978;&#30340;3D&#30690;&#37327;&#22330;&#25552;&#20379;&#30456;&#21516;&#30340;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
The mathematical representations of data in the Spherical Harmonic (SH) domain has recently regained increasing interest in the machine learning community. This technical report gives an in-depth introduction to the theoretical foundation and practical implementation of SH representations, summarizing works on rotation invariant and equivariant features, as well as convolutions and exact correlations of signals on spheres. In extension, these methods are then generalized from scalar SH representations to Vectorial Harmonics (VH), providing the same capabilities for 3d vector fields on spheres
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#12290;&#19982;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#21516;&#65292;&#20316;&#32773;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.03305</link><description>&lt;p&gt;
&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
A Vulnerability of Attribution Methods Using Pre-Softmax Scores. (arXiv:2307.03305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03305
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#12290;&#19982;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#21516;&#65292;&#20316;&#32773;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31867;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#12290;&#24050;&#30693;&#36825;&#31181;&#31867;&#22411;&#30340;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#36755;&#20837;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss a vulnerability involving a category of attribution methods used to provide explanations for the outputs of convolutional neural networks working as classifiers. It is known that this type of networks are vulnerable to adversarial attacks, in which imperceptible perturbations of the input may alter the outputs of the model. In contrast, here we focus on effects that small modifications in the model may cause on the attribution method without altering the model outputs.
&lt;/p&gt;</description></item><item><title>&#22312;TikTok&#35270;&#39057;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SexTok&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21306;&#20998;&#24615;&#26263;&#31034;&#20869;&#23481;&#21644;&#34394;&#25311;&#24615;&#25945;&#32946;&#35270;&#39057;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.03274</link><description>&lt;p&gt;
&#19981;&#26159;&#24615;&#26263;&#31034;&#65292;&#26159;&#25945;&#32946;&#12290;&#22312;TikTok&#35270;&#39057;&#20013;&#20998;&#31163;&#24615;&#25945;&#32946;&#21644;&#26263;&#31034;&#24615;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is not Sexually Suggestive, It is Educative. Separating Sex Education from Suggestive Content on TikTok Videos. (arXiv:2307.03274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03274
&lt;/p&gt;
&lt;p&gt;
&#22312;TikTok&#35270;&#39057;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SexTok&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21306;&#20998;&#24615;&#26263;&#31034;&#20869;&#23481;&#21644;&#34394;&#25311;&#24615;&#25945;&#32946;&#35270;&#39057;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SexTok&#30340;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#34987;&#26631;&#35760;&#20026;&#24615;&#26263;&#31034;&#65288;&#20174;&#27880;&#37322;&#32773;&#30340;&#35282;&#24230;&#26469;&#30475;&#65289;&#65292;&#24615;&#25945;&#32946;&#20869;&#23481;&#25110;&#20004;&#32773;&#37117;&#19981;&#26159;&#30340;TikTok&#35270;&#39057;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#26159;&#20026;&#20102;&#35299;&#20915;&#22312;TikTok&#19978;&#21306;&#20998;&#24615;&#26263;&#31034;&#20869;&#23481;&#21644;&#34394;&#25311;&#24615;&#25945;&#32946;&#35270;&#39057;&#30340;&#25361;&#25112;&#12290;&#20799;&#31461;&#25509;&#35302;&#24615;&#26263;&#31034;&#30340;&#35270;&#39057;&#24050;&#34987;&#35777;&#26126;&#23545;&#20182;&#20204;&#30340;&#21457;&#23637;&#26377;&#19981;&#21033;&#24433;&#21709;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23545;&#20110;LGBTQIA+&#31038;&#21306;&#26356;&#30456;&#20851;&#30340;&#34394;&#25311;&#24615;&#25945;&#32946;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#24179;&#21488;&#30340;&#24403;&#21069;&#31995;&#32479;&#21024;&#38500;&#25110;&#24809;&#32602;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#35270;&#39057;&#65292;&#23613;&#31649;&#23427;&#20204;&#26377;&#19981;&#21516;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#35270;&#39057;URL&#65292;&#24182;&#19988;&#36824;&#26377;&#38899;&#39057;&#36716;&#24405;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#26469;&#23545;&#35270;&#39057;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#21306;&#20998;&#36825;&#20123;&#31867;&#22411;&#30340;&#35270;&#39057;&#26159;&#21487;&#23398;&#20064;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator's point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex education videos on TikTok. Children's exposure to sexually suggestive videos has been shown to have adversarial effects on their development. Meanwhile, virtual sex education, especially on subjects that are more relevant to the LGBTQIA+ community, is very valuable. The platform's current system removes or penalizes some of both types of videos, even though they serve different purposes. Our dataset contains video URLs, and it is also audio transcribed. To validate its importance, we explore two transformer-based models for classifying the videos. Our preliminary results suggest that the task of distinguishing between these types of videos is learnable but challenging. These experiments sugge
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#24314;&#27169;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.03254</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Vision Language Transformers: A Survey. (arXiv:2307.03254v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03254
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#24314;&#27169;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#22914;&#22238;&#31572;&#20851;&#20110;&#22270;&#20687;&#30340;&#38382;&#39064;&#25110;&#29983;&#25104;&#25551;&#36848;&#22270;&#20687;&#30340;&#26631;&#39064;&#65292;&#26159;&#35745;&#31639;&#26426;&#38590;&#20197;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#39044;&#35757;&#32451;&#30340;transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#24314;&#27169;&#12290;&#30456;&#27604;&#20197;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;transformer&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#26041;&#38754;&#26377;&#24456;&#22823;&#25552;&#39640;&#12290;&#23427;&#20204;&#36890;&#36807;&#22312;&#22823;&#22411;&#36890;&#29992;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26550;&#26500;&#21644;&#21442;&#25968;&#20540;&#19978;&#36827;&#34892;&#24494;&#23567;&#25913;&#21464;&#21518;&#65292;&#23558;&#23398;&#20064;&#36716;&#31227;&#21040;&#26032;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#26631;&#20934;&#24314;&#27169;&#23454;&#36341;&#12290;&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#25215;&#35834;&#22312;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013;&#20135;&#29983;&#31867;&#20284;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#23545;&#30446;&#21069;&#21487;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#24191;&#27867;&#32508;&#21512;&#65292;&#24182;&#23545;&#20854;&#20248;&#21183;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision language tasks, such as answering questions about or generating captions that describe an image, are difficult tasks for computers to perform. A relatively recent body of research has adapted the pretrained transformer architecture introduced in \citet{vaswani2017attention} to vision language modeling. Transformer models have greatly improved performance and versatility over previous vision language models. They do so by pretraining models on a large generic datasets and transferring their learning to new tasks with minor changes in architecture and parameter values. This type of transfer learning has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks which require both vision and language. In this paper, we provide a broad synthesis of the currently available research on vision language transformer models and offer some analysis of their strength
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03212</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#39640;&#24230;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#22478;&#24066;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#65292;&#20197;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32780;&#19981;&#21463;&#21018;&#24615;&#37051;&#22495;&#26465;&#20214;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19987;&#27880;&#20110;&#20174;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#34920;&#31034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#31227;&#21160;&#27969;&#27169;&#24335;&#12289;POI&#35821;&#20041;&#21644;&#31614;&#21040;&#21160;&#24577;&#20013;&#25429;&#25417;&#22810;&#35270;&#35282;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20840;&#23616;&#22270;&#27880;&#24847;&#32593;&#32476;&#26469;&#23398;&#20064;&#22270;&#20013;&#20219;&#24847;&#20004;&#20010;&#39030;&#28857;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20840;&#38754;&#32771;&#34385;&#21644;&#20849;&#20139;&#22810;&#20010;&#35270;&#35282;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#21033;&#29992;&#22806;&#37096;&#27880;&#24847;&#21147;&#23398;&#20064;&#26435;&#37325;&#26469;&#34701;&#21512;&#22810;&#35270;&#35282;&#23884;&#20837;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban region embedding is an important and yet highly challenging issue due to the complexity and constantly changing nature of urban data. To address the challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighbourhood region conditions. Our model focus on learn urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics and check-in dynamics. Then, we adopt global graph attention networks to learn similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#32553;&#25918;&#23450;&#24459;&#19982;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#21152;&#20250;&#24341;&#21457;&#19981;&#21516;&#31038;&#32676;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.03201</link><description>&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#19981;&#20855;&#22791;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws Do Not Scale. (arXiv:2307.03201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32553;&#25918;&#23450;&#24459;&#19982;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#21152;&#20250;&#24341;&#21457;&#19981;&#21516;&#31038;&#32676;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#30340;&#24130;&#24459;&#20851;&#31995;&#65292;&#23427;&#25551;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#27169;&#22411;&#35774;&#35745;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#22914;&#25968;&#25454;&#38598;&#22823;&#23567;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#65288;&#25110;&#27169;&#22411;&#21442;&#25968;&#31561;&#65289;&#30340;&#22686;&#21152;&#65292;&#22522;&#20110;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#23558;&#30456;&#24212;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#21516;&#26102;&#65292;&#36825;&#31181;&#32553;&#25918;&#23450;&#24459;&#20851;&#31995;&#24573;&#35270;&#20102;&#29992;&#20110;&#34913;&#37327;&#24615;&#33021;&#30340;&#25351;&#26631;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#21644;&#26377;&#20105;&#35758;&#30340;&#65292;&#25110;&#32773;&#21487;&#33021;&#19981;&#31526;&#21512;&#19981;&#21516;&#20154;&#32676;&#23545;&#27169;&#22411;&#36755;&#20986;&#36136;&#37327;&#30340;&#24863;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#38543;&#30528;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#22686;&#38271;&#65292;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#30340;&#19981;&#21516;&#31038;&#32676;&#65288;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#65289;&#30340;&#25968;&#37327;&#21487;&#33021;&#20250;&#22686;&#21152;&#65292;&#27599;&#20010;&#31038;&#32676;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20013;&#25152;&#20195;&#34920;&#30340;&#31038;&#32676;&#21487;&#33021;&#23384;&#22312;&#20215;&#20540;&#35266;&#25110;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has proposed a power law relationship, referred to as ``scaling laws,'' between the performance of artificial intelligence (AI) models and aspects of those models' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase. However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;AI&#36827;&#34892;&#25945;&#32946;&#35270;&#39057;&#36716;&#24405;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#38477;&#20302;&#29983;&#25104;&#36716;&#24405;&#30340;&#25104;&#26412;&#21644;&#24310;&#36831;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.03200</link><description>&lt;p&gt;
&#20351;&#29992;Whisper&#36827;&#34892;&#25945;&#32946;&#35270;&#39057;&#36716;&#24405;&#65306;&#20851;&#20110;&#20351;&#29992;AI&#36827;&#34892;&#25945;&#32946;&#35270;&#39057;&#36716;&#24405;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transcribing Educational Videos Using Whisper: A preliminary study on using AI for transcribing educational videos. (arXiv:2307.03200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;AI&#36827;&#34892;&#25945;&#32946;&#35270;&#39057;&#36716;&#24405;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#38477;&#20302;&#29983;&#25104;&#36716;&#24405;&#30340;&#25104;&#26412;&#21644;&#24310;&#36831;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#22312;&#36828;&#31243;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#29983;&#25104;&#25991;&#26412;&#36716;&#24405;&#20197;&#22686;&#24378;&#23398;&#20064;&#20307;&#39564;&#21313;&#20998;&#37325;&#35201;&#12290;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#31995;&#32479;&#21487;&#20197;&#20943;&#23569;&#29983;&#25104;&#36716;&#24405;&#30340;&#25104;&#26412;&#21644;&#24310;&#36831;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;25&#20010;&#25945;&#32946;&#35270;&#39057;&#20351;&#29992;Whisper&#29983;&#25104;&#30340;&#36716;&#24405;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#22312;&#20351;&#29992;ASR&#36827;&#34892;&#25945;&#32946;&#35270;&#39057;&#36716;&#24405;&#26102;&#35782;&#21035;&#20986;&#19968;&#20123;&#24320;&#25918;&#24615;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Videos are increasingly being used for e-learning, and transcripts are vital to enhance the learning experience. The costs and delays of generating transcripts can be alleviated by automatic speech recognition (ASR) systems. In this article, we quantify the transcripts generated by whisper for 25 educational videos and identify some open avenues of research when leveraging ASR for transcribing educational videos.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#32423;&#30340;AI&#27835;&#29702;&#26694;&#26550;&#65292;&#28041;&#21450;&#25919;&#24220;&#12289;&#20225;&#19994;&#21644;&#20844;&#27665;&#19977;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#32676;&#20307;&#12290;&#36890;&#36807;&#20449;&#20219;&#30340;&#19981;&#21516;&#32500;&#24230;&#26469;&#30740;&#31350;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#21046;&#23450;AI&#30456;&#20851;&#20844;&#20849;&#25919;&#31574;&#25552;&#20379;&#23454;&#29992;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.03198</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#23618;&#32423;&#30340;AI&#27835;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A multilevel framework for AI governance. (arXiv:2307.03198v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#32423;&#30340;AI&#27835;&#29702;&#26694;&#26550;&#65292;&#28041;&#21450;&#25919;&#24220;&#12289;&#20225;&#19994;&#21644;&#20844;&#27665;&#19977;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#32676;&#20307;&#12290;&#36890;&#36807;&#20449;&#20219;&#30340;&#19981;&#21516;&#32500;&#24230;&#26469;&#30740;&#31350;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#21046;&#23450;AI&#30456;&#20851;&#20844;&#20849;&#25919;&#31574;&#25552;&#20379;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#30410;&#22788;&#24182;&#20943;&#36731;&#21487;&#33021;&#30340;&#39118;&#38505;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#19968;&#20010;&#31526;&#21512;&#20262;&#29702;&#21644;&#22522;&#26412;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#27835;&#29702;&#26694;&#26550;&#12290;&#23613;&#31649;&#19968;&#20123;&#32452;&#32455;&#24050;&#32463;&#21457;&#24067;&#20102;&#21487;&#20449;AI&#30340;&#25351;&#21335;&#21644;&#20262;&#29702;&#26694;&#26550;&#65292;&#20294;&#22914;&#26524;&#27809;&#26377;&#19968;&#20010;&#35843;&#33410;&#30340;&#27835;&#29702;&#32467;&#26500;&#65292;&#36825;&#20123;&#20262;&#29702;&#21407;&#21017;&#23558;&#26080;&#27861;&#36716;&#21270;&#20026;&#23454;&#36341;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#32423;&#27835;&#29702;&#26041;&#27861;&#65292;&#28041;&#21450;&#19977;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#32676;&#20307;&#65306;&#25919;&#24220;&#12289;&#20225;&#19994;&#21644;&#20844;&#27665;&#12290;&#25105;&#20204;&#36890;&#36807;&#20449;&#20219;&#30340;&#32500;&#24230;&#65288;&#22914;&#33021;&#21147;&#12289;&#35802;&#20449;&#21644;&#21892;&#24847;&#65289;&#26469;&#30740;&#31350;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#32467;&#21512;AI&#30340;&#27835;&#29702;&#27700;&#24179;&#21644;&#20449;&#20219;&#32500;&#24230;&#65292;&#25552;&#20379;&#20102;&#21487;&#29992;&#20110;&#36827;&#19968;&#27493;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#24182;&#20026;&#19982;AI&#30456;&#20851;&#30340;&#20844;&#20849;&#25919;&#31574;&#25552;&#20379;&#20449;&#24687;&#30340;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
To realize the potential benefits and mitigate potential risks of AI, it is necessary to develop a framework of governance that conforms to ethics and fundamental human values. Although several organizations have issued guidelines and ethical frameworks for trustworthy AI, without a mediating governance structure, these ethical principles will not translate into practice. In this paper, we propose a multilevel governance approach that involves three groups of interdependent stakeholders: governments, corporations, and citizens. We examine their interrelationships through dimensions of trust, such as competence, integrity, and benevolence. The levels of governance combined with the dimensions of trust in AI provide practical insights that can be used to further enhance user experiences and inform public policy related to AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;SplitFed Learning&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#25915;&#20987;&#31574;&#30053;&#21487;&#20197;&#38477;&#20302;&#22522;&#20110;DCML&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03197</link><description>&lt;p&gt;
&#20998;&#26512;SplitFed Learning&#20013;&#30340;&#28431;&#27934;&#65306;&#35780;&#20272;&#20854;&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks. (arXiv:2307.03197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;SplitFed Learning&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#25915;&#20987;&#31574;&#30053;&#21487;&#20197;&#38477;&#20302;&#22522;&#20110;DCML&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#65288;DCML&#65289;&#26159;&#35299;&#20915;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#19968;&#31181;&#28508;&#22312;&#26367;&#20195;&#26041;&#26696;&#12290;Split learning&#65288;SL&#65289;&#21644;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;DCML&#20013;&#20004;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#20154;&#20204;&#23545;FL&#21644;SL&#30340;&#28151;&#21512;&#24418;&#24335;SplitFed Learning&#65288;SFL&#65289;&#20135;&#29983;&#20102;&#36739;&#22823;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#26159;&#23545;SFL&#20013;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#36827;&#34892;&#30740;&#31350;&#12289;&#20998;&#26512;&#21644;&#24433;&#21709;&#35780;&#20272;&#30340;&#26368;&#26089;&#23581;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;&#38750;&#30446;&#26631;&#25915;&#20987;&#12289;&#26377;&#30446;&#26631;&#25915;&#20987;&#21644;&#22522;&#20110;&#36317;&#31163;&#30340;&#25915;&#20987;&#65292;&#29992;&#20110;SFL&#12290;&#25152;&#26377;&#25915;&#20987;&#31574;&#30053;&#26088;&#22312;&#38477;&#20302;&#22522;&#20110;DCML&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#31867;&#21644;&#25163;&#20889;&#25968;&#23383;&#35782;&#21035;&#36825;&#20004;&#20010;&#19981;&#21516;&#26696;&#20363;&#36827;&#34892;&#20102;&#25915;&#20987;&#23454;&#39564;&#65292;&#22312;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#30334;&#20998;&#27604;&#21644;&#27169;&#22411;&#25286;&#20998;&#23618;&#30340;&#36873;&#25321;&#26041;&#38754;&#36827;&#34892;&#20102;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Collaborative Machine Learning (DCML) is a potential alternative to address the privacy concerns associated with centralized machine learning. The Split learning (SL) and Federated Learning (FL) are the two effective learning approaches in DCML. Recently there have been an increased interest on the hybrid of FL and SL known as the SplitFed Learning (SFL). This research is the earliest attempt to study, analyze and present the impact of data poisoning attacks in SFL. We propose three kinds of novel attack strategies namely untargeted, targeted and distance-based attacks for SFL. All the attacks strategies aim to degrade the performance of the DCML-based classifier. We test the proposed attack strategies for two different case studies on Electrocardiogram signal classification and automatic handwritten digit recognition. A series of attack experiments were conducted by varying the percentage of malicious clients and the choice of the model split layer between the clients and 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20154;&#25165;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#32452;&#32455;&#21487;&#20197;&#20174;&#25968;&#25454;&#31185;&#23398;&#30340;&#35282;&#24230;&#29702;&#35299;&#32452;&#32455;&#34892;&#20026;&#24182;&#23454;&#26102;&#20570;&#20986;&#20915;&#31574;&#65292;&#20026;&#26377;&#25928;&#30340;&#20154;&#25165;&#31649;&#29702;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2307.03195</link><description>&lt;p&gt;
&#20154;&#25165;&#20998;&#26512;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Artificial Intelligence Techniques for Talent Analytics. (arXiv:2307.03195v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03195
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20154;&#25165;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#32452;&#32455;&#21487;&#20197;&#20174;&#25968;&#25454;&#31185;&#23398;&#30340;&#35282;&#24230;&#29702;&#35299;&#32452;&#32455;&#34892;&#20026;&#24182;&#23454;&#26102;&#20570;&#20986;&#20915;&#31574;&#65292;&#20026;&#26377;&#25928;&#30340;&#20154;&#25165;&#31649;&#29702;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#31454;&#20105;&#28608;&#28872;&#19988;&#24555;&#36895;&#21457;&#23637;&#30340;&#21830;&#19994;&#29615;&#22659;&#19979;&#65292;&#32452;&#32455;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#20197;&#37327;&#21270;&#26041;&#24335;&#20570;&#20986;&#20154;&#25165;&#30456;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#12290;&#22823;&#35268;&#27169;&#20154;&#25165;&#21644;&#31649;&#29702;&#30456;&#20851;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20026;&#20225;&#19994;&#39046;&#23548;&#32773;&#25552;&#20379;&#20102;&#20174;&#25968;&#25454;&#31185;&#23398;&#35282;&#24230;&#29702;&#35299;&#32452;&#32455;&#34892;&#20026;&#21644;&#33719;&#21462;&#23454;&#38469;&#30693;&#35782;&#30340;&#26080;&#19982;&#20262;&#27604;&#26426;&#20250;&#65292;&#36827;&#32780;&#20026;&#23454;&#26102;&#20915;&#31574;&#21644;&#26377;&#25928;&#30340;&#20154;&#25165;&#31649;&#29702;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20154;&#25165;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#22312;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#30340;&#39046;&#22495;&#65292;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#24182;&#28608;&#21457;&#20102;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#26032;&#12289;&#20840;&#38754;&#30340;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20154;&#25165;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's competitive and fast-evolving business environment, it is a critical time for organizations to rethink how to make talent-related decisions in a quantitative manner. Indeed, the recent development of Big Data and Artificial Intelligence (AI) techniques have revolutionized human resource management. The availability of large-scale talent and management-related data provides unparalleled opportunities for business leaders to comprehend organizational behaviors and gain tangible knowledge from a data science perspective, which in turn delivers intelligence for real-time decision-making and effective talent management at work for their organizations. In the last decade, talent analytics has emerged as a promising field in applied data science for human resource management, garnering significant attention from AI communities and inspiring numerous research efforts. To this end, we present an up-to-date and comprehensive survey on AI technologies used for talent analytics in the f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#35774;&#35745;&#24072;&#21644;&#24037;&#31243;&#24072;&#30340;&#35266;&#28857;&#24046;&#24322;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#21487;&#20449;&#20219;AI&#24320;&#21457;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#65292;&#26088;&#22312;&#25552;&#39640;&#25216;&#26415;&#36827;&#27493;&#21644;&#36947;&#24503;&#21407;&#21017;&#30340;&#20248;&#20808;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03193</link><description>&lt;p&gt;
&#25506;&#32034;&#35774;&#35745;&#24072;&#21644;&#24037;&#31243;&#24072;&#22312;&#24320;&#21457;&#21487;&#20449;&#20219;&#30340;&#33258;&#21160;&#39550;&#39542;AI&#26041;&#38754;&#30340;&#35266;&#28857;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Finding differences in perspectives between designers and engineers to develop trustworthy AI for autonomous cars. (arXiv:2307.03193v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#35774;&#35745;&#24072;&#21644;&#24037;&#31243;&#24072;&#30340;&#35266;&#28857;&#24046;&#24322;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#21487;&#20449;&#20219;AI&#24320;&#21457;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#65292;&#26088;&#22312;&#25552;&#39640;&#25216;&#26415;&#36827;&#27493;&#21644;&#36947;&#24503;&#21407;&#21017;&#30340;&#20248;&#20808;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#21644;&#23454;&#26045;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#20851;&#20110;&#20026;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#24320;&#21457;&#21487;&#20449;&#36182;&#30340;AI&#23384;&#22312;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#35266;&#28857;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#35758;&#20197;&#20943;&#23569;&#36825;&#31181;&#20998;&#27495;&#12290;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23548;&#33268;&#24046;&#24322;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#24357;&#21512;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#36229;&#36234;&#20102;&#36947;&#24503;&#22256;&#22659;&#65292;&#21487;&#35270;&#21270;&#20102;&#21487;&#20449;&#36182;&#21644;&#36947;&#24503;AI&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;&#21487;&#20449;&#36182;AI&#30340;&#19977;&#20010;&#25903;&#26609;&#24050;&#34987;&#23450;&#20041;&#20026;&#36879;&#26126;&#24230;&#12289;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#26412;&#30740;&#31350;&#20026;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#21487;&#20449;&#20219;AI&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#65292;&#20197;&#22686;&#24378;&#23558;&#25216;&#26415;&#36827;&#27493;&#21644;&#36947;&#24503;&#21407;&#21017;&#20248;&#20808;&#32771;&#34385;&#30340;AI&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of designing and implementing ethical Artificial Intelligence (AI), varying perspectives exist regarding developing trustworthy AI for autonomous cars. This study sheds light on the differences in perspectives and provides recommendations to minimize such divergences. By exploring the diverse viewpoints, we identify key factors contributing to the differences and propose strategies to bridge the gaps. This study goes beyond the trolley problem to visualize the complex challenges of trustworthy and ethical AI. Three pillars of trustworthy AI have been defined: transparency, reliability, and safety. This research contributes to the field of trustworthy AI for autonomous cars, providing practical recommendations to enhance the development of AI systems that prioritize both technological advancement and ethical principles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#30693;&#35782;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#23567;&#21306;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#36890;&#36947;&#35821;&#20041;&#33719;&#21462;&#21644;&#22810;&#29992;&#25143;&#27874;&#26463;&#36171;&#24418;&#65292;&#21487;&#25552;&#39640;&#23460;&#22806;&#26080;&#32447;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25903;&#25345;&#25193;&#23637;&#29616;&#23454;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.03070</link><description>&lt;p&gt;
&#28151;&#21512;&#30693;&#35782;&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#36947;&#35821;&#20041;&#33719;&#21462;&#21644;&#27874;&#26463;&#36171;&#24418;&#29992;&#20110;&#26080;&#23567;&#21306;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hybrid Knowledge-Data Driven Channel Semantic Acquisition and Beamforming for Cell-Free Massive MIMO. (arXiv:2307.03070v1 [eess.SP] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#30693;&#35782;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#23567;&#21306;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;&#36890;&#36947;&#35821;&#20041;&#33719;&#21462;&#21644;&#22810;&#29992;&#25143;&#27874;&#26463;&#36171;&#24418;&#65292;&#21487;&#25552;&#39640;&#23460;&#22806;&#26080;&#32447;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25903;&#25345;&#25193;&#23637;&#29616;&#23454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25512;&#21160;&#23460;&#22806;&#26080;&#32447;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#26222;&#36941;&#23384;&#22312;&#30340;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#24212;&#29992;&#65292;&#24182;&#32553;&#23567;&#19982;&#24403;&#21069;&#23460;&#20869;&#26080;&#32447;&#20256;&#36755;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#30693;&#35782;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#23567;&#21306;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#31995;&#32479;&#20013;&#30340;&#36890;&#36947;&#35821;&#20041;&#33719;&#21462;&#21644;&#22810;&#29992;&#25143;&#27874;&#26463;&#36171;&#24418;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;-Mixer&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#36890;&#36947;&#35821;&#20041;&#33719;&#21462;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#32852;&#21512;&#20248;&#21270;&#20102;&#23548;&#39057;&#20449;&#21495;&#12289;&#20449;&#36947;&#35821;&#20041;&#23884;&#20837;&#30340;CSI&#37327;&#21270;&#22120;&#20197;&#21450;&#20449;&#36947;&#35821;&#20041;&#25552;&#21462;&#30340;CSI&#37325;&#26500;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#33719;&#21462;&#30340;&#36890;&#36947;&#35821;&#20041;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#23637;&#24320;&#22810;&#29992;&#25143;&#27874;&#26463;&#36171;&#24418;&#22120;&#65292;&#22312;&#23460;&#22806;XR&#22330;&#26223;&#20013;&#33021;&#22815;&#23454;&#29616;&#33391;&#22909;&#30340;&#39057;&#35889;&#25928;&#29575;&#65292;&#24182;&#23545;&#19981;&#23436;&#32654;&#30340;CSI&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23637;&#24320;&#20256;&#32479;&#30340;&#36845;&#20195;&#36229;&#26494;&#24347;&#65288;SOR&#65289;&#32447;&#24615;&#27874;&#26463;&#36171;&#24418;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on advancing outdoor wireless systems to better support ubiquitous extended reality (XR) applications, and close the gap with current indoor wireless transmission capabilities. We propose a hybrid knowledge-data driven method for channel semantic acquisition and multi-user beamforming in cell-free massive multiple-input multiple-output (MIMO) systems. Specifically, we firstly propose a data-driven multiple layer perceptron (MLP)-Mixer-based auto-encoder for channel semantic acquisition, where the pilot signals, CSI quantizer for channel semantic embedding, and CSI reconstruction for channel semantic extraction are jointly optimized in an end-to-end manner. Moreover, based on the acquired channel semantic, we further propose a knowledge-driven deep-unfolding multi-user beamformer, which is capable of achieving good spectral efficiency with robustness to imperfect CSI in outdoor XR scenarios. By unfolding conventional successive over-relaxation (SOR)-based linear beamf
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#21644;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27169;&#24335;&#65292;&#20174;&#21160;&#25163;&#32534;&#30721;&#21644;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#36716;&#21464;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.02792</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#24212;&#35813;&#20570;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Should Data Science Education Do with Large Language Models?. (arXiv:2307.02792v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02792
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#21644;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27169;&#24335;&#65292;&#20174;&#21160;&#25163;&#32534;&#30721;&#21644;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#36716;&#21464;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#31561;&#30340;&#24555;&#36895;&#21457;&#23637;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#21644;&#32479;&#35745;&#23398;&#12290;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#24037;&#20855;&#21487;&#20197;&#31616;&#21270;&#22797;&#26434;&#30340;&#27969;&#31243;&#65292;&#20174;&#32780;&#37325;&#22609;&#20102;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#35748;&#20026;LLM&#27491;&#22312;&#36716;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#65292;&#23558;&#20182;&#20204;&#30340;&#37325;&#28857;&#20174;&#21160;&#25163;&#32534;&#30721;&#12289;&#25968;&#25454;&#25972;&#29702;&#21644;&#36827;&#34892;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#36825;&#20123;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#35282;&#33394;&#30340;&#28436;&#21464;&#31867;&#20284;&#20110;&#20174;&#36719;&#20214;&#24037;&#31243;&#24072;&#36716;&#21464;&#20026;&#20135;&#21697;&#32463;&#29702;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20351;&#29992;LLM&#22312;&#25968;&#25454;&#31185;&#23398;&#26696;&#20363;&#30740;&#31350;&#20013;&#35828;&#26126;&#20102;&#36825;&#31181;&#36716;&#21464;&#12290;&#36825;&#20123;&#21457;&#23637;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#26377;&#24847;&#20041;&#22320;&#21457;&#23637;&#12290;&#25945;&#32946;&#26041;&#27861;&#29616;&#22312;&#24517;&#39035;&#26356;&#21152;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;LLM&#21551;&#21457;&#30340;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;LLM&#36824;&#21487;&#20197;&#22312;&#35838;&#22530;&#19978;&#36215;&#21040;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20316;&#20026;&#20114;&#21160;&#24335;&#25945;&#23398;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The rapid advances of large language models (LLMs), such as ChatGPT, are revolutionizing data science and statistics. These state-of-the-art tools can streamline complex processes. As a result, it reshapes the role of data scientists. We argue that LLMs are transforming the responsibilities of data scientists, shifting their focus from hands-on coding, data-wrangling and conducting standard analyses to assessing and managing analyses performed by these automated AIs. This evolution of roles is reminiscent of the transition from a software engineer to a product manager. We illustrate this transition with concrete data science case studies using LLMs in this paper. These developments necessitate a meaningful evolution in data science education. Pedagogy must now place greater emphasis on cultivating diverse skillsets among students, such as LLM-informed creativity, critical thinking, AI-guided programming. LLMs can also play a significant role in the classroom as interactive teaching and
&lt;/p&gt;</description></item><item><title>&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02484</link><description>&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02484
&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#65292;&#23427;&#26159;&#29616;&#26377;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;DT&#22768;&#31216;&#33021;&#22815;&#29983;&#25104;&#26368;&#20339;&#36712;&#36857;&#65292;&#20294;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#23427;&#22312;&#36712;&#36857;&#25340;&#25509;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36712;&#36857;&#25340;&#25509;&#26159;&#25351;&#20174;&#19968;&#32452;&#27425;&#20248;&#36712;&#36857;&#20013;&#29983;&#25104;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;&#25552;&#20986;&#30340;EDT&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;DT&#20013;&#32500;&#25252;&#30340;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#20174;&#32780;&#20351;&#33258;&#24049;&#19982;&#20247;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#36712;&#36857;&#26159;&#26368;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#38271;&#30340;&#21382;&#21490;&#65292;&#24403;&#24403;&#21069;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#30701;&#30340;&#21382;&#21490;&#26469;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#26356;&#20248;&#30340;&#36712;&#36857;&#36827;&#34892;&#8220;&#25340;&#25509;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#33021;&#22815;&#22635;&#34917;&#22522;&#20110;DT&#21644;&#22522;&#20110;Q-Learning&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;EDT&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
&lt;/p&gt;</description></item><item><title>FOCUS&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#65292;&#23398;&#20064;&#20102;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#25506;&#32034;&#22870;&#21169;&#26469;&#26356;&#36731;&#26494;&#22320;&#25506;&#32034;&#29289;&#20307;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#19968;&#33268;&#24615;&#30340;&#26426;&#22120;&#20154;-&#29289;&#20307;&#20132;&#20114;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.02427</link><description>&lt;p&gt;
FOCUS:&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FOCUS: Object-Centric World Models for Robotics Manipulation. (arXiv:2307.02427v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02427
&lt;/p&gt;
&lt;p&gt;
FOCUS&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#65292;&#23398;&#20064;&#20102;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#25506;&#32034;&#22870;&#21169;&#26469;&#26356;&#36731;&#26494;&#22320;&#25506;&#32034;&#29289;&#20307;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#19968;&#33268;&#24615;&#30340;&#26426;&#22120;&#20154;-&#29289;&#20307;&#20132;&#20114;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#65292;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;&#26159;&#29702;&#35299;&#29289;&#20307;&#21450;&#20854;&#21487;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#37325;&#35201;&#35748;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#26679;&#19968;&#20010;&#29305;&#23450;&#25429;&#25417;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#19990;&#30028;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FOCUS&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#65292;&#21487;&#20197;&#23398;&#20064;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#36890;&#36807;&#26469;&#33258;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#30340;&#26032;&#39062;&#25506;&#32034;&#22870;&#21169;&#65292;FOCUS&#21487;&#20197;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#26356;&#36731;&#26494;&#22320;&#25506;&#32034;&#29289;&#20307;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#20351;&#20195;&#29702;&#26426;&#22120;&#20154;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#23545;&#26426;&#22120;&#20154;-&#29289;&#20307;&#20132;&#20114;&#30340;&#19968;&#33268;&#24615;&#25506;&#32034;&#12290;&#36890;&#36807;&#20351;&#29992;Franka Emika&#26426;&#22120;&#20154;&#33218;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;FOCUS&#22914;&#20309;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#36827;&#34892;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the world in terms of objects and the possible interplays with them is an important cognition ability, especially in robotics manipulation, where many tasks require robot-object interactions. However, learning such a structured world model, which specifically captures entities and relationships, remains a challenging and underexplored problem. To address this, we propose FOCUS, a model-based agent that learns an object-centric world model. Thanks to a novel exploration bonus that stems from the object-centric representation, FOCUS can be deployed on robotics manipulation tasks to explore object interactions more easily. Evaluating our approach on manipulation tasks across different settings, we show that object-centric world models allow the agent to solve tasks more efficiently and enable consistent exploration of robot-object interactions. Using a Franka Emika robot arm, we also showcase how FOCUS could be adopted in real-world settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17258</link><description>&lt;p&gt;
&#36973;&#21463;&#33510;&#38590;&#30340;&#28900;&#38754;&#21253;&#26426;
&lt;/p&gt;
&lt;p&gt;
Suffering Toasters. (arXiv:2306.17258v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#65292;&#26234;&#33021;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#23450;&#20041;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#30001;&#20110;&#25105;&#20204;&#23545;AI&#33539;&#24335;&#12289;&#26550;&#26500;&#21644;&#24037;&#20855;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#33258;&#28982;&#20135;&#29983;&#30340;AI&#24847;&#35782;&#27604;&#20197;&#24448;&#26356;&#26377;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22768;&#31216;&#25152;&#26377;&#24403;&#21069;&#30340;&#26234;&#33021;&#27979;&#35797;&#37117;&#19981;&#36275;&#20197;&#25351;&#20986;&#23384;&#22312;&#25110;&#32570;&#20047;&#35937;&#20154;&#31867;&#30452;&#35273;&#24863;&#30693;&#30340;&#26234;&#33021;&#12290;&#25105;&#20204;&#20511;&#37492;&#31185;&#23398;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#25552;&#20379;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#30340;&#26356;&#28165;&#26224;&#23450;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#30340;&#26032;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#27010;&#36848;&#20102;&#21487;&#33021;&#30340;&#23454;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#26032;&#21551;&#21457;&#24335;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#26080;&#35770;&#26159;&#21746;&#23398;&#38382;&#39064;&#36824;&#26159;&#23454;&#29616;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A widely accepted definition of intelligence in the context of Artificial Intelligence (AI) still eludes us. Due to our exceedingly rapid development of AI paradigms, architectures, and tools, the prospect of naturally arising AI consciousness seems more likely than ever. In this paper, we claim that all current intelligence tests are insufficient to point to the existence or lack of intelligence \textbf{as humans intuitively perceive it}. We draw from ideas in the philosophy of science, psychology, and other areas of research to provide a clearer definition of the problems of artificial intelligence, self-awareness, and agency. We furthermore propose a new heuristic approach to test for artificial self-awareness and outline a possible implementation. Finally, we discuss some of the questions that arise from this new heuristic, be they philosophical or implementation-oriented.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14275</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#22686;&#24378;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#24050;&#25104;&#20026;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#65292;&#31616;&#21333;&#30340;&#23545;&#25239;&#35757;&#32451;&#36973;&#21463;&#20102;&#20196;&#20154;&#30031;&#32553;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#23548;&#33268;&#40065;&#26834;&#27867;&#21270;&#25928;&#26524;&#19981;&#20339;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#22914;&#39069;&#22806;&#30340;&#35268;&#33539;&#21270;&#12289;&#23545;&#25239;&#26435;&#37325;&#25200;&#21160;&#21644;&#26356;&#22810;&#25968;&#25454;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#40065;&#26834;&#27867;&#21270;&#30340;&#25913;&#36827;&#20173;&#28982;&#36828;&#19981;&#29702;&#24819;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20840;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;--&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#30340;&#31934;&#32454;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#20248;&#21270;&#36712;&#36857;&#22312;&#26102;&#38388;&#19978;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;WOT&#22312;&#21508;&#31181;&#26368;&#26032;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;WOT&#19982;&#29616;&#26377;&#26041;&#27861;&#23436;&#32654;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13264</link><description>&lt;p&gt;
FedSelect: &#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#25968;&#33258;&#23450;&#20041;&#36873;&#25321;&#30340;&#32454;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#24494;&#35843;&#23458;&#25143;&#31471;&#21442;&#25968;&#25110;&#38024;&#23545;&#26412;&#22320;&#20219;&#21153;&#20010;&#24615;&#21270;&#26550;&#26500;&#26469;&#25552;&#39640;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#22312;&#29306;&#29298;&#37325;&#35201;&#30340;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#35201;&#20040;&#22312;&#39044;&#20808;&#30830;&#23450;&#32593;&#32476;&#23618;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#20840;&#23616;&#30693;&#35782;&#20648;&#23384;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedSelect&#65292;&#36890;&#36807;&#21516;&#26102;&#25628;&#32034;&#24182;&#33719;&#24471;&#20010;&#24615;&#21270;&#26368;&#20339;&#21442;&#25968;&#21644;&#29992;&#20110;&#20840;&#23616;&#32858;&#21512;&#30340;&#20854;&#20313;&#21442;&#25968;&#65292;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) seek to increase client-level performance by fine-tuning client parameters on local data or personalizing architectures for the local task. Existing methods for such personalization either prune a global model or fine-tune a global model on a local client distribution. However, these existing methods either personalize at the expense of retaining important global knowledge, or predetermine network layers for fine-tuning, resulting in suboptimal storage of global knowledge within client models. Enlightened by the lottery ticket hypothesis, we first introduce a hypothesis for finding optimal client subnetworks to locally fine-tune while leaving the rest of the parameters frozen. We then propose a novel FL framework, FedSelect, using this procedure that directly personalizes both client subnetwork structure and parameters, via the simultaneous discovery of optimal parameters for personalization and the rest of parameters for global aggregatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#26102;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#30340;&#23450;&#26102;&#24182;&#21457;&#35821;&#35328;&#65292;&#29992;&#20110;&#27169;&#25311;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#23558;&#20195;&#29702;&#30340;&#21487;&#25509;&#21463;&#24615;&#19982;&#32473;&#23450;&#26102;&#38388;&#38388;&#38548;&#36827;&#34892;&#25512;&#29702;&#21644;&#36890;&#20449;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#25311;&#36777;&#35770;&#21644;&#23545;&#35805;&#28216;&#25103;&#30340;&#24773;&#22659;&#12290;</title><link>http://arxiv.org/abs/2306.07675</link><description>&lt;p&gt;
&#35770;&#36848;&#27169;&#22411;&#19982;&#23545;&#35805;&#28216;&#25103;&#30340;&#23450;&#26102;&#24182;&#21457;&#35821;&#35328;&#30340;&#20132;&#38169;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
An Interleaving Semantics of the Timed Concurrent Language for Argumentation to Model Debates and Dialogue Games. (arXiv:2306.07675v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#26102;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#30340;&#23450;&#26102;&#24182;&#21457;&#35821;&#35328;&#65292;&#29992;&#20110;&#27169;&#25311;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#23558;&#20195;&#29702;&#30340;&#21487;&#25509;&#21463;&#24615;&#19982;&#32473;&#23450;&#26102;&#38388;&#38388;&#38548;&#36827;&#34892;&#25512;&#29702;&#21644;&#36890;&#20449;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#25311;&#36777;&#35770;&#21644;&#23545;&#35805;&#28216;&#25103;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#26159;&#24314;&#27169;&#26234;&#33021;&#20307;&#21160;&#24577;&#34892;&#20026;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#27963;&#21160;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#20855;&#26377;&#30830;&#23450;&#30340;&#26102;&#38388;&#38271;&#24230;&#65292;&#24182;&#19988;&#20808;&#21069;&#30340;&#34892;&#20026;&#20250;&#24433;&#21709;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#65292;&#29992;&#20110;&#24314;&#27169;&#20195;&#29702;&#20043;&#38388;&#30340;&#24182;&#21457;&#20132;&#20114;&#65292;&#21516;&#26102;&#20801;&#35768;&#25351;&#23450;&#29305;&#23450;&#34892;&#20026;&#21457;&#29983;&#30340;&#26102;&#38388;&#38388;&#38548;&#12290;&#36825;&#31181;&#35821;&#35328;&#21033;&#29992;&#23450;&#26102;&#30340;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#23454;&#29616;&#20195;&#29702;&#20351;&#29992;&#30340;&#20849;&#20139;&#20869;&#23384;&#65292;&#20197;&#22312;&#32473;&#23450;&#26102;&#38388;&#38388;&#38548;&#20869;&#36890;&#20449;&#21644;&#25512;&#29702;&#20851;&#20110;&#20182;&#20204;&#30340;&#20449;&#24565;&#30340;&#21487;&#25509;&#21463;&#24615;&#12290;&#22312;&#21333;&#20010;&#22788;&#29702;&#22120;&#19978;&#20351;&#29992;&#20132;&#38169;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#35745;&#31639;&#27493;&#39588;&#65292;&#26102;&#38548;&#26368;&#22823;&#24182;&#34892;&#24615;&#12290;&#25353;&#29031;&#36825;&#31181;&#26041;&#27861;&#65292;&#21482;&#26377;&#19968;&#20010;&#21487;&#29992;&#30340;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#21051;&#34987;&#25191;&#34892;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#27169;&#25311;&#21457;&#29983;&#22312;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#36777;&#35770;&#21644;&#23545;&#35805;&#28216;&#25103;&#31561;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time is a crucial factor in modelling dynamic behaviours of intelligent agents: activities have a determined temporal duration in a real-world environment, and previous actions influence agents' behaviour. In this paper, we propose a language for modelling concurrent interaction between agents that also allows the specification of temporal intervals in which particular actions occur. Such a language exploits a timed version of Abstract Argumentation Frameworks to realise a shared memory used by the agents to communicate and reason on the acceptability of their beliefs with respect to a given time interval. An interleaving model on a single processor is used for basic computation steps, with maximum parallelism for time elapsing. Following this approach, only one of the enabled agents is executed at each moment. To demonstrate the capabilities of language, we also show how it can be used to model interactions such as debates and dialogue games taking place between intelligent agents. La
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#32676;&#31561;&#21464;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#20301;&#32622;&#32534;&#30721;&#25805;&#20316;&#35299;&#20915;&#20102;&#35270;&#35273;Transformer&#20013;&#30340;&#31561;&#21464;&#24615;&#23398;&#20064;&#38590;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26126;&#26174;&#20248;&#20110;&#38750;&#31561;&#21464;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2306.06722</link><description>&lt;p&gt;
$E(2)$-&#31561;&#21464;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
$E(2)$-Equivariant Vision Transformer. (arXiv:2306.06722v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#32676;&#31561;&#21464;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#20301;&#32622;&#32534;&#30721;&#25805;&#20316;&#35299;&#20915;&#20102;&#35270;&#35273;Transformer&#20013;&#30340;&#31561;&#21464;&#24615;&#23398;&#20064;&#38590;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26126;&#26174;&#20248;&#20110;&#38750;&#31561;&#21464;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViT&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;ViT&#20013;&#30340;&#20301;&#32622;&#32534;&#30721;&#20351;&#24471;&#23398;&#20064;&#25968;&#25454;&#30340;&#20869;&#22312;&#31561;&#21464;&#24615;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#26412;&#25991;&#23545;&#35774;&#35745;&#30340;&#31561;&#21464;ViT&#36827;&#34892;&#20102;&#21021;&#27493;&#23581;&#35797;&#65292;&#20294;&#35777;&#26126;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23384;&#22312;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#20301;&#32622;&#32534;&#30721;&#25805;&#20316;&#35774;&#35745;&#20102;&#19968;&#20010;&#32676;&#31561;&#21464;&#35270;&#35273;Transformer&#65288;GE-ViT&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GE-ViT&#28385;&#36275;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#25152;&#26377;&#29702;&#35770;&#35201;&#27714;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GE-ViT&#26126;&#26174;&#20248;&#20110;&#38750;&#31561;&#21464;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ZJUCDSYangKaifan/GEVit&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has achieved remarkable performance in computer vision. However, positional encoding in ViT makes it substantially difficult to learn the intrinsic equivariance in data. Initial attempts have been made on designing equivariant ViT but are proved defective in some cases in this paper. To address this issue, we design a Group Equivariant Vision Transformer (GE-ViT) via a novel, effective positional encoding operator. We prove that GE-ViT meets all the theoretical requirements of an equivariant neural network. Comprehensive experiments are conducted on standard benchmark datasets, demonstrating that GE-ViT significantly outperforms non-equivariant self-attention networks. The code is available at https://github.com/ZJUCDSYangKaifan/GEVit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#26469;&#23545;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#22788;&#29702;&#65292;&#20174;&#32780;&#25913;&#21892;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#22312;&#27492;&#25913;&#36827;&#30340;&#31574;&#30053;&#32422;&#26463;&#19979;&#20248;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;OPER&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.05412</link><description>&lt;p&gt;
&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;
&lt;/p&gt;
&lt;p&gt;
Offline Prioritized Experience Replay. (arXiv:2306.05412v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#26469;&#23545;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#22788;&#29702;&#65292;&#20174;&#32780;&#25913;&#21892;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#22312;&#27492;&#25913;&#36827;&#30340;&#31574;&#30053;&#32422;&#26463;&#19979;&#20248;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;OPER&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#23398;&#20064;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32422;&#26463;&#36890;&#36807;&#22343;&#21248;&#37319;&#26679;&#31561;&#26041;&#24335;&#34987;&#24212;&#29992;&#21040;&#34920;&#29616;&#33391;&#22909;&#21644;&#34920;&#29616;&#24046;&#30340;&#34892;&#21160;&#19978;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#23398;&#20064;&#31574;&#30053;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#29992;&#20110;&#23558;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#32622;&#20110;&#26356;&#39057;&#32321;&#30340;&#35775;&#38382;&#20013;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#33021;&#22815;&#24341;&#36215;&#34892;&#20026;&#31574;&#30053;&#30340;&#25913;&#21892;&#65292;&#24403;&#31574;&#30053;&#32422;&#26463;&#21040;&#36825;&#20010;&#25913;&#36827;&#30340;&#31574;&#30053;&#19978;&#26102;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24456;&#21487;&#33021;&#24471;&#21040;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#23454;&#29992;&#31574;&#30053;&#26469;&#33719;&#24471;&#22522;&#20110;&#25311;&#21512;&#20540;&#32593;&#32476;&#30340;&#20248;&#20808;&#26435;&#37325;&#65288;OPER-A&#65289;&#25110;&#32773;u
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is challenged by the distributional shift problem. To address this problem, existing works mainly focus on designing sophisticated policy constraints between the learned policy and the behavior policy. However, these constraints are applied equally to well-performing and inferior actions through uniform sampling, which might negatively affect the learned policy. To alleviate this issue, we propose Offline Prioritized Experience Replay (OPER), featuring a class of priority functions designed to prioritize highly-rewarding transitions, making them more frequently visited during training. Through theoretical analysis, we show that this class of priority functions induce an improved behavior policy, and when constrained to this improved policy, a policy-constrained offline RL algorithm is likely to yield a better solution. We develop two practical strategies to obtain priority weights by estimating advantages based on a fitted value network (OPER-A) or u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04719</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#20320;&#30340;&#30524;&#30555;&#65306;&#20851;&#20110;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#65288;&#19981;&#65289;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#65311;&#29305;&#24449;&#21487;&#35270;&#21270;&#36890;&#36807;&#20248;&#21270;&#26469;&#21487;&#35270;&#21270;&#39640;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#35797;&#22270;&#22238;&#31572;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22914;&#20170;&#65292;&#21487;&#35270;&#21270;&#26041;&#27861;&#26500;&#25104;&#20102;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#30340;&#20102;&#35299;&#30340;&#22522;&#30784;&#65292;&#20316;&#20026;&#19968;&#31181;&#26426;&#26800;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#26377;&#22810;&#21487;&#38752;&#65311;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#32593;&#32476;&#30005;&#36335;&#26469;&#35784;&#39575;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20351;&#20854;&#26174;&#31034;&#23436;&#20840;&#19982;&#33258;&#28982;&#36755;&#20837;&#30340;&#27491;&#24120;&#32593;&#32476;&#34892;&#20026;&#27627;&#26080;&#32852;&#31995;&#30340;&#20219;&#24847;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#22312;&#26631;&#20934;&#65292;&#26410;&#25805;&#32437;&#32593;&#32476;&#20013;&#21457;&#29983;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#19982;&#26631;&#20934;&#36755;&#20837;&#22788;&#29702;&#38750;&#24120;&#19981;&#21516;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#25903;&#25745;&#36825;&#19968;&#32463;&#39564;&#21457;&#29616;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#21487;&#35270;&#21270;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#26497;&#20854;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GLOSS&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#12290;&#35813;&#27169;&#22411;&#22312;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16724</link><description>&lt;p&gt;
&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#20013;&#30340;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Code-Switched Text Synthesis in Unseen Language Pairs. (arXiv:2305.16724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GLOSS&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#12290;&#35813;&#27169;&#22411;&#22312;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#21512;&#25104;&#30340;&#30740;&#31350;&#22823;&#22810;&#38656;&#35201;&#22312;&#30446;&#26631;&#35821;&#35328;&#23545;&#20013;&#30340;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#22312;&#32570;&#20047;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#37096;&#32626;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;GLOSS&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65288;PMMTM&#65289;&#20043;&#19978;&#65292;&#24182;&#24102;&#26377;&#39069;&#22806;&#30340;&#20195;&#30721;&#20999;&#25442;&#27169;&#22359;&#30340;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22359;&#65292;&#26080;&#35770;&#26159;&#36866;&#37197;&#22120;&#36824;&#26159;&#39069;&#22806;&#30340;&#21069;&#32512;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20013;&#23398;&#20064;&#20195;&#30721;&#20999;&#25442;&#27169;&#24335;&#65292;&#32780;GLOSS&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;PMMTM&#34987;&#20923;&#32467;&#12290;&#25105;&#20204;&#21482;&#35843;&#25972;&#20195;&#30721;&#20999;&#25442;&#27169;&#22359;&#30340;&#35774;&#35745;&#65292;&#38450;&#27490;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#35757;&#32451;&#25968;&#25454;&#30340;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;GLOSS&#34920;&#29616;&#20986;&#20102;&#36328;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#36827;&#34892;&#24402;&#32435;&#21644;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30446;&#26631;&#35821;&#35328;&#21333;&#35821;&#25991;&#26412;&#30340;&#33258;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GLOSS&#20248;&#20110;&#20854;&#20182;&#20174;&#20855;&#26377;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#35821;&#35328;&#23545;&#20013;&#35843;&#25972;&#30340;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#31561;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing efforts on text synthesis for code-switching mostly require training on code-switched texts in the target language pairs, limiting the deployment of the models to cases lacking code-switched data. In this work, we study the problem of synthesizing code-switched texts for language pairs absent from the training data. We introduce GLOSS, a model built on top of a pre-trained multilingual machine translation model (PMMTM) with an additional code-switching module. This module, either an adapter or extra prefixes, learns code-switching patterns from code-switched data during training, while the primary component of GLOSS, i.e., the PMMTM, is frozen. The design of only adjusting the code-switching module prevents our model from overfitting to the constrained training data for code-switching. Hence, GLOSS exhibits the ability to generalize and synthesize code-switched texts across a broader spectrum of language pairs. Additionally, we develop a self-training algorithm on target langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#12298;&#39764;&#27861;&#19982;&#20195;&#30721;&#33521;&#38596;&#12299;&#65288;LOCM&#65289;&#30340;&#20116;&#24180;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#65292;&#20171;&#32461;&#20102;&#28216;&#25103;&#35268;&#21017;&#20197;&#21450;&#27604;&#36187;&#21382;&#21490;&#65292;&#32473;&#20986;&#20102;&#32452;&#32455;AI&#27604;&#36187;&#30340;&#24314;&#35758;&#12290;LOCM&#24050;&#34987;&#29992;&#20110;&#35768;&#22810;&#19982;&#28216;&#25103;&#26641;&#25628;&#32034;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#35780;&#20272;&#20989;&#25968;&#21644;CCG&#21345;&#32452;&#26500;&#24314;&#31561;&#30456;&#20851;&#39046;&#22495;&#30340;&#20986;&#29256;&#29289;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11814</link><description>&lt;p&gt;
&#25112;&#30053;&#21345;&#29260;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Summarizing Strategy Card Game AI Competition. (arXiv:2305.11814v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#12298;&#39764;&#27861;&#19982;&#20195;&#30721;&#33521;&#38596;&#12299;&#65288;LOCM&#65289;&#30340;&#20116;&#24180;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#65292;&#20171;&#32461;&#20102;&#28216;&#25103;&#35268;&#21017;&#20197;&#21450;&#27604;&#36187;&#21382;&#21490;&#65292;&#32473;&#20986;&#20102;&#32452;&#32455;AI&#27604;&#36187;&#30340;&#24314;&#35758;&#12290;LOCM&#24050;&#34987;&#29992;&#20110;&#35768;&#22810;&#19982;&#28216;&#25103;&#26641;&#25628;&#32034;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#35780;&#20272;&#20989;&#25968;&#21644;CCG&#21345;&#32452;&#26500;&#24314;&#31561;&#30456;&#20851;&#39046;&#22495;&#30340;&#20986;&#29256;&#29289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#12298;&#39764;&#27861;&#19982;&#20195;&#30721;&#33521;&#38596;&#12299;&#65288;LOCM&#65289;&#30340;&#20116;&#24180;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25903;&#25345;&#30740;&#31350;&#21644;&#31639;&#27861;&#24320;&#21457;&#30340;&#23567;&#22411;&#25910;&#38598;&#21345;&#29260;&#28216;&#25103;&#65288;CCG&#65289;&#12290;&#35813;&#28216;&#25103;&#34987;&#29992;&#20110;&#22810;&#20010;&#20107;&#20214;&#20013;&#65292;&#21253;&#25324;CodinGame&#24179;&#21488;&#19978;&#30340;&#31038;&#21306;&#27604;&#36187;&#65292;&#20197;&#21450;IEEE&#36827;&#21270;&#35745;&#31639;&#22823;&#20250;&#21644;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#25112;&#30053;&#21345;&#29260;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#12290;LOCM&#24050;&#34987;&#29992;&#20110;&#35768;&#22810;&#19982;&#28216;&#25103;&#26641;&#25628;&#32034;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#35780;&#20272;&#20989;&#25968;&#21644;CCG&#21345;&#32452;&#26500;&#24314;&#31561;&#30456;&#20851;&#39046;&#22495;&#30340;&#20986;&#29256;&#29289;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#28216;&#25103;&#35268;&#21017;&#12289;&#32452;&#32455;&#27604;&#36187;&#30340;&#21382;&#21490;&#20197;&#21450;&#21442;&#36187;&#32773;&#21644;&#20182;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#20026;&#30740;&#31350;&#31038;&#21306;&#32452;&#32455;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#30340;&#19968;&#20123;&#24120;&#35268;&#24314;&#35758;&#12290;&#23613;&#31649;COG 2022&#29256;&#23459;&#24067;&#26159;&#26368;&#21518;&#19968;&#29256;&#65292;&#20294;&#28216;&#25103;&#20173;&#28982;&#21487;&#29992;&#65292;&#24182;&#21487;&#22312;&#22312;&#32447;&#25490;&#34892;&#27036;&#31454;&#25216;&#22330;&#19978;&#36827;&#34892;&#28216;&#29609;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concludes five years of AI competitions based on Legends of Code and Magic (LOCM), a small Collectible Card Game (CCG), designed with the goal of supporting research and algorithm development. The game was used in a number of events, including Community Contests on the CodinGame platform, and Strategy Card Game AI Competition at the IEEE Congress on Evolutionary Computation and IEEE Conference on Games. LOCM has been used in a number of publications related to areas such as game tree search algorithms, neural networks, evaluation functions, and CCG deckbuilding. We present the rules of the game, the history of organized competitions, and a listing of the participant and their approaches, as well as some general advice on organizing AI competitions for the research community. Although the COG 2022 edition was announced to be the last one, the game remains available and can be played using an online leaderboard arena.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#23545;&#20110;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#29992;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#20102;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.14102</link><description>&lt;p&gt;
SocNavGym&#65306;&#19968;&#20010;&#38024;&#23545;&#31038;&#20132;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#20223;&#30495;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SocNavGym: A Reinforcement Learning Gym for Social Navigation. (arXiv:2304.14102v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#23545;&#20110;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#29992;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#20102;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#23494;&#38598;&#30340;&#29615;&#22659;&#19979;&#65292;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#23548;&#33322;&#26102;&#38656;&#35201;&#36981;&#23432;&#31038;&#20132;&#35268;&#33539;&#12290;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#26368;&#36817;&#22312;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#21487;&#20197;&#37096;&#20998;&#24402;&#22240;&#20110;&#29983;&#25104;&#30340;&#31574;&#30053;&#19981;&#21463;&#20195;&#30721;&#22797;&#26434;&#24615;&#25110;&#22788;&#29702;&#30340;&#21464;&#37327;&#25968;&#37327;&#31561;&#20154;&#31867;&#38480;&#21046;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;DRL&#31639;&#27861;&#32570;&#20047;&#23433;&#20840;&#20445;&#38556;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#65292;&#23548;&#33268;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#19981;&#22826;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#20223;&#30495;&#29615;&#22659;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#31038;&#20132;&#23548;&#33322;&#30340;&#20808;&#36827;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;SocNavGym&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#21487;&#36731;&#26494;&#37197;&#32622;&#20197;&#29983;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#37197;&#32622;&#20026;&#20351;&#29992;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#65292;&#24182;&#25903;&#25345;&#26080;&#38556;&#30861;&#29615;&#22659;&#30340;&#20223;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is essential for autonomous robots to be socially compliant while navigating in human-populated environments. Machine Learning and, especially, Deep Reinforcement Learning have recently gained considerable traction in the field of Social Navigation. This can be partially attributed to the resulting policies not being bound by human limitations in terms of code complexity or the number of variables that are handled. Unfortunately, the lack of safety guarantees and the large data requirements by DRL algorithms make learning in the real world unfeasible. To bridge this gap, simulation environments are frequently used. We propose SocNavGym, an advanced simulation environment for social navigation that can generate a wide variety of social navigation scenarios and facilitates the development of intelligent social agents. SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly configured to generate different types of social navigation scenarios. It can also be configured to
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#26234;&#21147;&#30340;&#23616;&#38480;&#24615;&#23548;&#33268;&#25216;&#26415;&#27010;&#24565;&#21019;&#36896;&#25918;&#32531;&#21644;&#21407;&#21019;&#24615;&#19979;&#38477;&#65292;&#22240;&#27492;&#24314;&#35758;&#24320;&#21457;&#21644;&#23454;&#26045;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#21019;&#26032;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.13300</link><description>&lt;p&gt;
&#21019;&#26032;&#25918;&#32531;&#65306;&#25216;&#26415;&#27010;&#24565;&#21019;&#36896;&#30340;&#20943;&#36895;&#21450;&#26032;&#25216;&#26415;&#27010;&#24565;&#21407;&#21019;&#24615;&#30340;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Innovation Slowdown: Decelerating Concept Creation and Declining Originality in New Technological Concepts. (arXiv:2303.13300v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13300
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#21147;&#30340;&#23616;&#38480;&#24615;&#23548;&#33268;&#25216;&#26415;&#27010;&#24565;&#21019;&#36896;&#25918;&#32531;&#21644;&#21407;&#21019;&#24615;&#19979;&#38477;&#65292;&#22240;&#27492;&#24314;&#35758;&#24320;&#21457;&#21644;&#23454;&#26045;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#21019;&#26032;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20043;&#21069;&#30340;&#27010;&#24565;&#37325;&#29992;&#12289;&#37325;&#32452;&#21644;&#21512;&#25104;&#36827;&#34892;&#26032;&#25216;&#26415;&#27010;&#24565;&#30340;&#21019;&#36896;&#21487;&#33021;&#20250;&#23548;&#33268;&#27010;&#24565;&#31354;&#38388;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#30001;&#19987;&#21033;&#25991;&#26412;&#20013;&#36229;&#36807;400&#19975;&#20010;&#27010;&#24565;&#32452;&#25104;&#30340;&#22823;&#35268;&#27169;&#25216;&#26415;&#35821;&#20041;&#32593;&#32476;&#36827;&#34892;&#30340;&#32479;&#35745;&#20998;&#26512;&#21457;&#29616;&#65292;&#27010;&#24565;&#21019;&#36896;&#30340;&#27493;&#20240;&#22312;&#25345;&#32493;&#20943;&#32531;&#65292;&#24182;&#19988;&#26032;&#21019;&#36896;&#20986;&#30340;&#27010;&#24565;&#30340;&#21407;&#21019;&#24615;&#26377;&#25152;&#19979;&#38477;&#12290;&#36825;&#20123;&#36235;&#21183;&#21487;&#20197;&#24402;&#22240;&#20110;&#20154;&#31867;&#26234;&#21147;&#22312;&#21019;&#26032;&#36229;&#20986;&#29616;&#26377;&#25216;&#26415;&#30340;&#25299;&#23637;&#31354;&#38388;&#26041;&#38754;&#30340;&#23616;&#38480;&#12290;&#20026;&#20102;&#20445;&#25345;&#21019;&#26032;&#65292;&#25105;&#20204;&#24314;&#35758;&#24320;&#21457;&#21644;&#23454;&#26045;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#65292;&#20197;&#22686;&#24378;&#21019;&#26032;&#36807;&#31243;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#23398;&#20064;&#12289;&#21019;&#36896;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The creation of new technological concepts through design reuses, recombination, and synthesis of prior concepts to create new ones may lead to exponential growth of the concept space over time. However, our statistical analysis of a large-scale technology semantic network consisting of over four million concepts from patent texts found evidence of a persistent deceleration in the pace of concept creation and a decline in the originality of newly created concepts. These trends may be attributed to the limitations of human intelligence in innovating beyond an expanding space of prior art. To sustain innovation, we recommend the development and implementation of creative artificial intelligence that can augment various aspects of the innovation process, including learning, creation, and evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#34892;&#20154;&#26816;&#27979;&#20013;&#27169;&#24577;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#35774;&#32622;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#22120;&#35299;&#20915;&#20102;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#65292;&#20351;&#24471;&#29305;&#24449;&#34701;&#21512;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2302.12589</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22810;&#27169;&#24577;&#34892;&#20154;&#26816;&#27979;&#20013;&#30340;&#27169;&#24577;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Modality Imbalance In Multimodal Pedestrian Detection. (arXiv:2302.12589v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#34892;&#20154;&#26816;&#27979;&#20013;&#27169;&#24577;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#35774;&#32622;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#22120;&#35299;&#20915;&#20102;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#65292;&#20351;&#24471;&#29305;&#24449;&#34701;&#21512;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#34892;&#20154;&#26816;&#27979;&#32780;&#35328;&#65292;&#36817;&#26469;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#20851;&#38190;&#30340;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#21516;&#26679;&#33391;&#22909;&#22320;&#36816;&#34892;&#65292;&#22914;&#24369;&#20809;&#12289;&#22812;&#38388;&#21644;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#20998;&#24067;&#26497;&#22823;&#22320;&#24378;&#35843;&#20102;&#26576;&#20010;&#29305;&#23450;&#36755;&#20837;&#30340;&#36129;&#29486;&#65292;&#20351;&#32593;&#32476;&#23545;&#26576;&#31181;&#27169;&#24577;&#20559;&#21521;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#27867;&#21270;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38750;&#20027;&#23548;&#36755;&#20837;&#27169;&#24577;&#21487;&#33021;&#23545;&#25512;&#29702;&#36807;&#31243;&#20570;&#20986;&#26356;&#22810;&#36129;&#29486;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#35774;&#32622;&#19982;&#22810;&#27169;&#24577;&#26550;&#26500;&#20013;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#35299;&#20915;&#27169;&#24577;&#20043;&#38388;&#30340;&#36825;&#31181;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#39033;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#29305;&#24449;&#25552;&#21462;&#22120;&#21516;&#31561;&#37325;&#35201;&#22320;&#32771;&#34385;&#22312;&#20869;&#65292;&#20351;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#25552;&#21462;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#36825;&#34987;&#31216;&#20026;&#31227;&#38500;&#19981;&#24179;&#34913;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning, particularly for pedestrian detection, has recently received emphasis due to its capability to function equally well in several critical autonomous driving scenarios such as low-light, night-time, and adverse weather conditions. However, in most cases, the training distribution largely emphasizes the contribution of one specific input that makes the network biased towards one modality. Hence, the generalization of such models becomes a significant problem where the non-dominant input modality during training could be contributing more to the course of inference. Here, we introduce a novel training setup with regularizer in the multimodal architecture to resolve the problem of this disparity between the modalities. Specifically, our regularizer term helps to make the feature fusion method more robust by considering both the feature extractors equivalently important during the training to extract the multimodal distribution which is referred to as removing the imbala
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.10886</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21644;&#36866;&#24212;&#24615;&#30340;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;AIRS&#21487;&#20197;&#26681;&#25454;&#23454;&#26102;&#20272;&#35745;&#30340;&#20219;&#21153;&#22238;&#25253;&#20174;&#39044;&#23450;&#20041;&#30340;&#20989;&#25968;&#38598;&#20013;&#36873;&#25321;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#21487;&#38752;&#30340;&#25506;&#32034;&#28608;&#21169;&#24182;&#35299;&#20915;&#20559;&#32622;&#30446;&#26631;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#22810;&#31181;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#30340;&#39640;&#25928;&#21487;&#38752;&#23454;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#23558;AIRS&#24212;&#29992;&#22312;MiniGrid&#12289;Procgen&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#30340;&#22810;&#39033;&#20219;&#21153;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#22823;&#37327;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;AIRS&#21487;&#20197;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20179;&#24211;&#29289;&#27969;&#20013;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;&#12290;&#20182;&#20204;&#36890;&#36807;&#20998;&#23618;&#30340;MARL&#31639;&#27861;&#65292;&#35753;&#32463;&#29702;&#21644;&#24037;&#20154;&#20195;&#29702;&#26681;&#25454;&#20840;&#23616;&#30446;&#26631;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#26368;&#22823;&#21270;&#25315;&#36135;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.11498</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#20179;&#24211;&#29289;&#27969;&#20013;&#19982;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers. (arXiv:2212.11498v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11498
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20179;&#24211;&#29289;&#27969;&#20013;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;&#12290;&#20182;&#20204;&#36890;&#36807;&#20998;&#23618;&#30340;MARL&#31639;&#27861;&#65292;&#35753;&#32463;&#29702;&#21644;&#24037;&#20154;&#20195;&#29702;&#26681;&#25454;&#20840;&#23616;&#30446;&#26631;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#26368;&#22823;&#21270;&#25315;&#36135;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#24819;&#19968;&#20010;&#20179;&#24211;&#37324;&#26377;&#25968;&#21313;&#20010;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20998;&#25315;&#21592;&#19968;&#36215;&#24037;&#20316;&#65292;&#25910;&#38598;&#21644;&#20132;&#20184;&#20179;&#24211;&#20869;&#30340;&#29289;&#21697;&#12290;&#25105;&#20204;&#35201;&#35299;&#20915;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#31216;&#20026;&#25315;&#36135;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#24037;&#20316;&#20195;&#29702;&#20154;&#22914;&#20309;&#22312;&#20179;&#24211;&#20013;&#21327;&#35843;&#20182;&#20204;&#30340;&#31227;&#21160;&#21644;&#34892;&#20026;&#20197;&#26368;&#22823;&#21270;&#24615;&#33021;&#65288;&#20363;&#22914;&#35746;&#21333;&#21534;&#21520;&#37327;&#65289;&#12290;&#20256;&#32479;&#30340;&#34892;&#19994;&#26041;&#27861;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#21162;&#21147;&#26469;&#20026;&#22266;&#26377;&#21487;&#21464;&#30340;&#20179;&#24211;&#37197;&#32622;&#36827;&#34892;&#20248;&#21270;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21487;&#20197;&#28789;&#27963;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20179;&#24211;&#37197;&#32622;&#65288;&#20363;&#22914;&#22823;&#23567;&#65292;&#24067;&#23616;&#65292;&#24037;&#20154;&#25968;&#37327;/&#31867;&#22411;&#65292;&#29289;&#21697;&#34917;&#20805;&#39057;&#29575;&#65289;&#65292;&#22240;&#20026;&#20195;&#29702;&#20154;&#36890;&#36807;&#32463;&#39564;&#23398;&#20064;&#22914;&#20309;&#26368;&#20248;&#22320;&#30456;&#20114;&#21512;&#20316;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20998;&#23618;MARL&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#31649;&#29702;&#32773;&#20026;&#24037;&#20154;&#20195;&#29702;&#20998;&#37197;&#30446;&#26631;&#65292;&#24182;&#19988;&#31649;&#29702;&#32773;&#21644;&#24037;&#20154;&#30340;&#31574;&#30053;&#34987;&#20849;&#21516;&#35757;&#32451;&#20197;&#26368;&#22823;&#21270;&#20840;&#23616;&#30446;&#26631;&#65288;&#20363;&#22914;&#25315;&#36135;&#36895;&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We envision a warehouse in which dozens of mobile robots and human pickers work together to collect and deliver items within the warehouse. The fundamental problem we tackle, called the order-picking problem, is how these worker agents must coordinate their movement and actions in the warehouse to maximise performance (e.g. order throughput). Established industry methods using heuristic approaches require large engineering efforts to optimise for innately variable warehouse configurations. In contrast, multi-agent reinforcement learning (MARL) can be flexibly applied to diverse warehouse configurations (e.g. size, layout, number/types of workers, item replenishment frequency), as the agents learn through experience how to optimally cooperate with one another. We develop hierarchical MARL algorithms in which a manager assigns goals to worker agents, and the policies of the manager and workers are co-trained toward maximising a global objective (e.g. pick rate). Our hierarchical algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#32422;&#26463;&#30340;&#28789;&#27963;&#29983;&#20135;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#24357;&#34917;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#20013;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2212.10936</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#29992;&#20110;&#31038;&#20250;&#25216;&#26415;&#29983;&#20135;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling. (arXiv:2212.10936v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#32422;&#26463;&#30340;&#28789;&#27963;&#29983;&#20135;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#24357;&#34917;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#21452;&#36164;&#28304;&#32422;&#26463;&#26580;&#24615;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;DRC-FJSSP&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;DRL&#25216;&#26415;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;&#27809;&#26377;&#32771;&#34385;&#21040;&#29616;&#23454;&#12289;&#28789;&#27963;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#36710;&#38388;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#20197;&#35746;&#21333;&#20026;&#23548;&#21521;&#30340;&#38388;&#27463;&#24615;&#21046;&#36896;&#20013;&#23384;&#22312;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#23427;&#32463;&#24120;&#22312;&#20855;&#26377;&#39640;&#26381;&#21153;&#27700;&#24179;&#30340;&#20013;&#23567;&#22411;&#20844;&#21496;&#20013;&#34920;&#31034;&#12290;&#20174;&#36825;&#19968;&#39046;&#22495;&#30340;&#23454;&#38469;&#24037;&#19994;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#38656;&#35201;&#25551;&#36848;&#28789;&#27963;&#30340;&#26426;&#22120;&#12289;&#20154;&#24037;&#24037;&#20316;&#32773;&#21644;&#33021;&#21147;&#12289;&#35774;&#32622;&#21644;&#22788;&#29702;&#25805;&#20316;&#12289;&#29289;&#26009;&#21040;&#36798;&#26102;&#38388;&#12289;&#20855;&#26377;&#24182;&#34892;&#20219;&#21153;&#30340;&#22797;&#26434;&#20316;&#19994;&#36335;&#24452;&#20197;&#36827;&#34892;&#29289;&#26009;&#28165;&#21333;&#65288;BOM&#65289;&#21046;&#36896;&#12289;&#39034;&#24207;&#30456;&#20851;&#35774;&#32622;&#26102;&#38388;&#21644;&#65288;&#37096;&#20998;&#65289;&#33258;&#21160;&#21270;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;DRC-FJSSP&#30340;&#32972;&#26223;&#19979;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#36866;&#24403;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#20135;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26159;&#29616;&#23454;&#24037;&#19994;&#19990;&#30028;&#20013;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20197;&#24357;&#34917;&#30456;&#20851;&#39046;&#22495;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The following article presents a memetic algorithm with applying deep reinforcement learning (DRL) for solving practically oriented dual resource constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years, there has been extensive research on DRL techniques, but without considering realistic, flexible and human-centered shopfloors. A research gap can be identified in the context of make-to-order oriented discontinuous manufacturing as it is often represented in medium-size companies with high service levels. From practical industry projects in this domain, we recognize requirements to depict flexible machines, human workers and capabilities, setup and processing operations, material arrival times, complex job paths with parallel tasks for bill of material (BOM) manufacturing, sequence-depended setup times and (partially) automated tasks. On the other hand, intensive research has been done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of sui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2212.09811</link><description>&lt;p&gt;
&#39640;&#25928;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#21024;&#20943;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#21452;&#35821;&#32763;&#35793;&#31995;&#32479;&#30456;&#27604;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#21487;&#20197;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#24182;&#20174;&#30693;&#35782;&#36716;&#31227;&#20013;&#33719;&#30410;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#21463;&#21040;&#22810;&#35821;&#35328;&#24615;&#30340;&#38480;&#21046;&#65292;&#38500;&#38750;&#36827;&#34892;&#22823;&#35268;&#27169;&#25193;&#23637;&#65292;&#21542;&#21017;&#20250;&#22686;&#21152;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#12290;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26159;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#22823;&#24133;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;NLLB-200&#26159;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#30340;&#20363;&#23376;&#12290;&#23427;&#28085;&#30422;&#20102;202&#31181;&#35821;&#35328;&#65292;&#20294;&#20165;&#25512;&#29702;&#23601;&#38656;&#35201;&#33267;&#23569;&#22235;&#20010;32GB&#30340;GPU&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#65292;&#20801;&#35768;&#21024;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20294;&#32763;&#35793;&#36136;&#37327;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#65292;&#36825;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#35813;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#21098;&#24230;&#37327;&#25351;&#26631;&#21487;&#20197;&#35782;&#21035;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Compared to conventional bilingual translation systems, massively multilingual machine translation is appealing because a single model can translate into multiple languages and benefit from knowledge transfer for low resource languages. On the other hand, massively multilingual models suffer from the curse of multilinguality, unless scaling their size massively, which increases their training and inference costs. Sparse Mixture-of-Experts models are a way to drastically increase model capacity without the need for a proportional amount of computing. The recently released NLLB-200 is an example of such a model. It covers 202 languages but requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that allows the removal of up to 80\% of experts with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics allow to identify language-specific experts and p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#30340;&#35268;&#33539;&#21270;&#20989;&#25968;&#26469;&#23454;&#29616;&#31561;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#38480;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#35268;&#33539;&#21270;&#20989;&#25968;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2211.06489</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#35268;&#33539;&#21270;&#20989;&#25968;&#23454;&#29616;&#31561;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Equivariance with Learned Canonicalization Functions. (arXiv:2211.06489v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#30340;&#35268;&#33539;&#21270;&#20989;&#25968;&#26469;&#23454;&#29616;&#31561;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#38480;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#35268;&#33539;&#21270;&#20989;&#25968;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#38480;&#21046;&#26550;&#26500;&#26469;&#23454;&#29616;&#23545;&#19968;&#32452;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#25110;&#31561;&#21464;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20135;&#29983;&#25968;&#25454;&#30340;&#35268;&#33539;&#34920;&#31034;&#26469;&#36991;&#20813;&#36825;&#31181;&#26550;&#26500;&#32422;&#26463;&#12290;&#36825;&#20123;&#35268;&#33539;&#21270;&#20989;&#25968;&#21487;&#20197;&#30452;&#25509;&#25554;&#20837;&#38750;&#31561;&#21464;&#20027;&#24178;&#26550;&#26500;&#20013;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#24863;&#20852;&#36259;&#30340;&#32676;&#32452;&#30340;&#26126;&#30830;&#23454;&#29616;&#26041;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20551;&#35774;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#25191;&#34892;&#35268;&#33539;&#21270;&#20248;&#20110;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#21551;&#21457;&#24335;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#35268;&#33539;&#21270;&#20989;&#25968;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;$N$&#20307;&#21160;&#21147;&#23398;&#39044;&#27979;&#12289;&#28857;&#20113;&#20998;&#31867;&#21644;&#37096;&#20998;&#20998;&#21106;&#31561;&#23398;&#20064;&#31561;&#21464;&#20989;&#25968;&#30340;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry-based neural networks often constrain the architecture in order to achieve invariance or equivariance to a group of transformations. In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data. These canonicalization functions can readily be plugged into non-equivariant backbone architectures. We offer explicit ways to implement them for some groups of interest. We show that this approach enjoys universality while providing interpretable insights. Our main hypothesis, supported by our empirical results, is that learning a small neural network to perform canonicalization is better than using predefined heuristics. Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including image classification, $N$-body dynamics prediction, point cloud classification and part segmentation, while being fas
&lt;/p&gt;</description></item><item><title>&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#26041;&#27861;&#32467;&#21512;&#20102;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;GPU&#19978;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#21644;&#23436;&#20840;&#20998;&#29255;&#30340;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#19982;Megatron-LM&#30456;&#27604;&#65292;&#22312;&#19968;&#20010;520&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#27599;&#20010;GPU&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;&#39640;&#36798;43%&#12290;</title><link>http://arxiv.org/abs/2211.05953</link><description>&lt;p&gt;
&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Breadth-First Pipeline Parallelism. (arXiv:2211.05953v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05953
&lt;/p&gt;
&lt;p&gt;
&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#26041;&#27861;&#32467;&#21512;&#20102;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;GPU&#19978;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#21644;&#23436;&#20840;&#20998;&#29255;&#30340;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#19982;Megatron-LM&#30456;&#27604;&#65292;&#22312;&#19968;&#20010;520&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#27599;&#20010;GPU&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;&#39640;&#36798;43%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#35843;&#24230;&#26041;&#27861;&#8212;&#8212;&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#65292;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#30340;&#32467;&#21512;&#12290;&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#36890;&#36807;&#22312;&#27599;&#20010;GPU&#19978;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#24182;&#32467;&#21512;&#23436;&#20840;&#20998;&#29255;&#30340;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#39640;GPU&#21033;&#29992;&#29575;&#12289;&#38477;&#20302;&#35757;&#32451;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#20869;&#23384;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;Megatron-LM&#65292;&#23545;&#20110;&#19968;&#20010;520&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#27599;&#20010;GPU&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;&#39640;&#36798;43%&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;GPU&#38598;&#32676;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#21644;&#25104;&#26412;&#21516;&#26679;&#38477;&#20302;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Breadth-First Pipeline Parallelism, a novel training schedule which optimizes the combination of pipeline and data parallelism. Breadth-First Pipeline Parallelism lowers training time, cost and memory usage by combining a high GPU utilization with a small batch size per GPU, and by making use of fully sharded data parallelism. Experimentally, we observed an increase of up to 43% in training throughput for a 52 billion-parameter model using a small batch size per GPU compared to Megatron-LM, which would reduce the training time and cost by the same amount on a large GPU cluster.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Topical&#65292;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#28304;&#20195;&#30721;&#20013;&#23398;&#20064;&#23384;&#20648;&#24211;&#32423;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#12289;&#20195;&#30721;&#25512;&#33616;&#21644;&#20195;&#30721;&#33258;&#21160;&#26631;&#35760;&#31561;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09495</link><description>&lt;p&gt;
Topical: &#20351;&#29992;Attention&#20174;&#28304;&#20195;&#30721;&#20013;&#23398;&#20064;&#23384;&#20648;&#24211;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Topical: Learning Repository Embeddings from Source Code using Attention. (arXiv:2208.09495v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09495
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Topical&#65292;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#28304;&#20195;&#30721;&#20013;&#23398;&#20064;&#23384;&#20648;&#24211;&#32423;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#12289;&#20195;&#30721;&#25512;&#33616;&#21644;&#20195;&#30721;&#33258;&#21160;&#26631;&#35760;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#20195;&#30721;&#30340;&#26426;&#22120;&#23398;&#20064;(MLOnCode)&#25215;&#35834;&#25913;&#21464;&#36719;&#20214;&#20132;&#20184;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#25366;&#25496;&#36719;&#20214;&#24037;&#20214;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#21644;&#20851;&#31995;&#65292;MLOnCode&#36890;&#36807;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#12289;&#20195;&#30721;&#25512;&#33616;&#12289;&#20195;&#30721;&#33258;&#21160;&#26631;&#35760;&#21644;&#20854;&#20182;&#25968;&#25454;&#39537;&#21160;&#22686;&#24378;&#26469;&#22686;&#24378;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#35768;&#22810;&#20219;&#21153;&#26469;&#35828;&#65292;&#20195;&#30721;&#30340;&#33050;&#26412;&#32423;&#34920;&#31034;&#24050;&#32463;&#36275;&#22815;&#65292;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#21040;&#21508;&#31181;&#20381;&#36182;&#20851;&#31995;&#21644;&#23384;&#20648;&#24211;&#32467;&#26500;&#30340;&#23384;&#20648;&#24211;&#32423;&#34920;&#31034;&#26159;&#24517;&#35201;&#30340;&#65292;&#20363;&#22914;&#65292;&#20026;&#23384;&#20648;&#24211;&#33258;&#21160;&#26631;&#35760;&#20027;&#39064;&#25110;&#33258;&#21160;&#35760;&#24405;&#23384;&#20648;&#24211;&#20195;&#30721;&#31561;&#12290;&#29616;&#26377;&#30340;&#35745;&#31639;&#23384;&#20648;&#24211;&#32423;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(a) &#20381;&#36182;&#20110;&#20195;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26723;(&#20363;&#22914;README&#25991;&#20214;)&#65307;(b) &#36890;&#36807;&#20018;&#32852;&#25110;&#24179;&#22343;&#31561;&#26041;&#27861;&#23545;&#26041;&#27861;/&#33050;&#26412;&#32423;&#34920;&#31034;&#36827;&#34892;&#31616;&#21333;&#32858;&#21512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Topical&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20844;&#20849;&#23384;&#20648;&#24211;&#32423;&#23884;&#20837;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning on source code (MLOnCode) promises to transform how software is delivered. By mining the context and relationship between software artefacts, MLOnCode augments the software developers capabilities with code auto-generation, code recommendation, code auto-tagging and other data-driven enhancements. For many of these tasks a script level representation of code is sufficient, however, in many cases a repository level representation that takes into account various dependencies and repository structure is imperative, for example, auto-tagging repositories with topics or auto-documentation of repository code etc. Existing methods for computing repository level representations suffer from (a) reliance on natural language documentation of code (for example, README files) (b) naive aggregation of method/script-level representation, for example, by concatenation or averaging. This paper introduces Topical a deep neural network to generate repository level embeddings of publicly 
&lt;/p&gt;</description></item><item><title>GRAPHSHAP&#26159;&#19968;&#31181;&#22522;&#20110;Shapley&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#27169;&#26679;&#24335;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;&#36523;&#20221;&#24863;&#30693;&#30340;&#22270;&#20998;&#31867;&#22120;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#25110;&#20854;&#35757;&#32451;&#25968;&#25454;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2202.08815</link><description>&lt;p&gt;
GRAPHSHAP&#65306;&#36890;&#36807;&#27169;&#26679;&#24335;&#30340;&#35821;&#35328;&#35299;&#37322;&#22522;&#20110;&#36523;&#20221;&#24863;&#30693;&#30340;&#22270;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
GRAPHSHAP: Explaining Identity-Aware Graph Classifiers Through the Language of Motifs. (arXiv:2202.08815v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08815
&lt;/p&gt;
&lt;p&gt;
GRAPHSHAP&#26159;&#19968;&#31181;&#22522;&#20110;Shapley&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#27169;&#26679;&#24335;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;&#36523;&#20221;&#24863;&#30693;&#30340;&#22270;&#20998;&#31867;&#22120;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#25110;&#20854;&#35757;&#32451;&#25968;&#25454;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35299;&#37322;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22312;&#34920;&#26684;&#25968;&#25454;&#12289;&#22270;&#20687;&#25110;&#26102;&#38388;&#24207;&#21015;&#19978;&#65289;&#20381;&#36182;&#20110;&#34913;&#37327;&#21024;&#38500;/&#25200;&#21160;&#29305;&#24449;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;&#36825;&#35201;&#27714;&#35299;&#37322;&#35821;&#35328;&#19982;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#26102;&#65292;&#22522;&#26412;&#29305;&#24449;&#23545;&#24212;&#20110;&#25551;&#36848;&#22270;&#32467;&#26500;&#30340;&#36793;&#65292;&#29305;&#24449;&#31354;&#38388;&#19982;&#35299;&#37322;&#35821;&#35328;&#20043;&#38388;&#30340;&#21305;&#37197;&#21487;&#33021;&#19981;&#21512;&#36866;&#12290;&#20026;&#20102;&#20026;&#22270;&#20998;&#31867;&#20219;&#21153;&#24320;&#21457;&#21487;&#34892;&#30340;&#35299;&#37322;&#65292;&#23558;&#29305;&#24449;&#31354;&#38388;&#65288;&#36793;&#65289;&#19982;&#25152;&#38656;&#30340;&#39640;&#32423;&#35299;&#37322;&#35821;&#35328;&#65288;&#22914;&#27169;&#26679;&#24335;&#65289;&#35299;&#32806;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Shapley&#20540;&#30340;&#26041;&#27861;GRAPHSHAP&#65292;&#33021;&#22815;&#20026;&#36523;&#20221;&#24863;&#30693;&#30340;&#22270;&#20998;&#31867;&#22120;&#25552;&#20379;&#22522;&#20110;&#27169;&#26679;&#24335;&#30340;&#35299;&#37322;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#25110;&#20854;&#35757;&#32451;&#25968;&#25454;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#65306;&#21807;&#19968;&#30340;&#35201;&#27714;&#26159;&#21487;&#20197;&#20219;&#24847;&#26597;&#35810;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most methods for explaining black-box classifiers (e.g. on tabular data, images, or time series) rely on measuring the impact that removing/perturbing features has on the model output. This forces the explanation language to match the classifier's feature space. However, when dealing with graph data, in which the basic features correspond to the edges describing the graph structure, this matching between features space and explanation language might not be appropriate. Decoupling the feature space (edges) from a desired high-level explanation language (such as motifs) is thus a major challenge towards developing actionable explanations for graph classification tasks. In this paper we introduce GRAPHSHAP, a Shapley-based approach able to provide motif-based explanations for identity-aware graph classifiers, assuming no knowledge whatsoever about the model or its training data: the only requirement is that the classifier can be queried as a black-box at will. For the sake of computationa
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22870;&#21169;&#21152;&#25104;&#30340;&#23376;&#20219;&#21153;&#26469;&#21457;&#29616;&#36873;&#39033;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#24573;&#30053;&#21407;&#22987;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.03466</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#36981;&#24490;&#22870;&#21169;&#30340;&#23376;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Reward-Respecting Subtasks for Model-Based Reinforcement Learning. (arXiv:2202.03466v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03466
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22870;&#21169;&#21152;&#25104;&#30340;&#23376;&#20219;&#21153;&#26469;&#21457;&#29616;&#36873;&#39033;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#24573;&#30053;&#21407;&#22987;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#36828;&#22823;&#30446;&#26631;&#65292;&#24378;&#21270;&#23398;&#20064;&#24517;&#39035;&#21253;&#25324;&#23545;&#25277;&#35937;&#29366;&#24577;&#21644;&#26102;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#35268;&#21010;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#29366;&#24577;&#25277;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26102;&#38388;&#25277;&#35937;&#21364;&#24456;&#23569;&#34987;&#20351;&#29992;&#65292;&#23613;&#31649;&#22522;&#20110;&#36873;&#39033;&#26694;&#26550;&#24050;&#32463;&#24191;&#27867;&#21457;&#23637;&#20102;&#29702;&#35770;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#21487;&#33021;&#30340;&#36873;&#39033;&#31354;&#38388;&#24456;&#22823;&#65292;&#20197;&#21069;&#25552;&#20986;&#30340;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#36873;&#39033;&#27169;&#22411;&#22312;&#35268;&#21010;&#20013;&#30340;&#20351;&#29992;&#26041;&#24335;&#12290;&#36890;&#24120;&#36890;&#36807;&#25552;&#20986;&#23376;&#20219;&#21153;&#65288;&#20363;&#22914;&#36798;&#21040;&#29942;&#39048;&#29366;&#24577;&#25110;&#26368;&#22823;&#21270;&#38500;&#22870;&#21169;&#22806;&#30340;&#24863;&#30693;&#20449;&#21495;&#30340;&#32047;&#31215;&#21644;&#65289;&#26469;&#21457;&#29616;&#36873;&#39033;&#12290;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#20197;&#29983;&#25104;&#19968;&#20010;&#36873;&#39033;&#65292;&#28982;&#21518;&#23398;&#20064;&#36873;&#39033;&#30340;&#27169;&#22411;&#24182;&#20351;&#20854;&#21487;&#29992;&#20110;&#35268;&#21010;&#36807;&#31243;&#12290;&#22312;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#23376;&#20219;&#21153;&#24573;&#30053;&#20102;&#21407;&#22987;&#38382;&#39064;&#19978;&#30340;&#22870;&#21169;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#30340;&#23376;&#20219;&#21153;&#20351;&#29992;&#21407;&#22987;&#22870;&#21169;&#21152;&#19978;&#22522;&#20110;&#26576;&#20010;&#29305;&#24449;&#30340;&#22870;&#21169;&#21152;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve the ambitious goals of artificial intelligence, reinforcement learning must include planning with a model of the world that is abstract in state and time. Deep learning has made progress with state abstraction, but temporal abstraction has rarely been used, despite extensively developed theory based on the options framework. One reason for this is that the space of possible options is immense, and the methods previously proposed for option discovery do not take into account how the option models will be used in planning. Options are typically discovered by posing subsidiary tasks, such as reaching a bottleneck state or maximizing the cumulative sum of a sensory signal other than reward. Each subtask is solved to produce an option, and then a model of the option is learned and made available to the planning process. In most previous work, the subtasks ignore the reward on the original problem, whereas we propose subtasks that use the original reward plus a bonus based on a fe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;SPD&#27969;&#24418;&#19978;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#21327;&#26041;&#24046;&#30697;&#38453;&#25805;&#20316;&#30340;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2201.05745</link><description>&lt;p&gt;
&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Deep Optimal Transport for Domain Adaptation on SPD Manifolds. (arXiv:2201.05745v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05745
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;SPD&#27969;&#24418;&#19978;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#21327;&#26041;&#24046;&#30697;&#38453;&#25805;&#20316;&#30340;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#23545;&#20110;&#22312;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#19978;&#35299;&#20915;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#38382;&#39064;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#20852;&#36259;&#12290;&#36825;&#31181;&#20852;&#36259;&#28304;&#20110;&#21307;&#30103;&#35774;&#22791;&#20135;&#29983;&#30340;&#22797;&#26434;&#31070;&#32463;&#29289;&#29702;&#25968;&#25454;&#65288;&#22914;&#33041;&#30005;&#22270;&#12289;&#33041;&#30913;&#22270;&#21644;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20559;&#31227;&#12290;&#36825;&#20123;&#25968;&#25454;&#34920;&#31034;&#20197;&#20449;&#21495;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#24418;&#24335;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#23545;&#31216;&#24615;&#21644;&#27491;&#23450;&#24615;&#30340;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#22797;&#26434;&#25805;&#20316;&#29305;&#24615;&#65292;&#30452;&#25509;&#23558;&#20808;&#21069;&#30340;&#32463;&#39564;&#21644;&#35299;&#20915;&#26041;&#26696;&#24212;&#29992;&#20110;DA&#38382;&#39064;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31867;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#12290;&#36825;&#19968;&#31867;&#26041;&#27861;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#65292;&#24182;&#21033;&#29992;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been significant interest in solving the domain adaptation (DA) problem on symmetric positive definite (SPD) manifolds within the machine learning community. This interest stems from the fact that complex neurophysiological data generated by medical equipment, such as electroencephalograms, magnetoencephalograms, and diffusion tensor imaging, often exhibit a shift in data distribution across different domains. These data representations, represented by signal covariance matrices, possess properties of symmetry and positive definiteness. However, directly applying previous experiences and solutions to the DA problem poses challenges due to the manipulation complexities of covariance matrices.To address this, our research introduces a category of deep learning-based transfer learning approaches called deep optimal transport. This category utilizes optimal transport theory and leverages the Log-Euclidean geometry for SPD manifolds. Additionally, we present a com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;DRL&#20013;&#23884;&#20837;&#31526;&#21495;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#25928;&#29575;&#12289;&#35299;&#37322;&#24615;&#32570;&#20047;&#21644;&#21487;&#36801;&#31227;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24490;&#29615;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#30340;&#35268;&#21010;&#27169;&#22411;&#21644;&#31526;&#21495;&#36873;&#39033;&#26469;&#24341;&#23548;&#31574;&#30053;&#30340;&#25913;&#36827;&#12290;&#23398;&#21040;&#30340;&#31526;&#21495;&#36873;&#39033;&#20943;&#36731;&#20102;&#23545;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#35201;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#31574;&#30053;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#19982;&#31526;&#21495;&#35268;&#21010;&#27169;&#22411;&#30340;&#35268;&#21010;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21487;&#36801;&#31227;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2112.09836</link><description>&lt;p&gt;
AI&#30340;&#21019;&#36896;&#21147;&#65306;&#29992;&#20110;&#20419;&#36827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#35268;&#21010;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Creativity of AI: Hierarchical Planning Model Learning for Facilitating Deep Reinforcement Learning. (arXiv:2112.09836v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;DRL&#20013;&#23884;&#20837;&#31526;&#21495;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#25928;&#29575;&#12289;&#35299;&#37322;&#24615;&#32570;&#20047;&#21644;&#21487;&#36801;&#31227;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24490;&#29615;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#30340;&#35268;&#21010;&#27169;&#22411;&#21644;&#31526;&#21495;&#36873;&#39033;&#26469;&#24341;&#23548;&#31574;&#30053;&#30340;&#25913;&#36827;&#12290;&#23398;&#21040;&#30340;&#31526;&#21495;&#36873;&#39033;&#20943;&#36731;&#20102;&#23545;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#35201;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#31574;&#30053;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#19982;&#31526;&#21495;&#35268;&#21010;&#27169;&#22411;&#30340;&#35268;&#21010;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21487;&#36801;&#31227;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#25928;&#29575;&#12289;&#35299;&#37322;&#24615;&#30340;&#32570;&#20047;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;DRL&#20013;&#23884;&#20837;&#31526;&#21495;&#30693;&#35782;&#26377;&#26395;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#31526;&#21495;&#36873;&#39033;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#19968;&#20010;&#24490;&#29615;&#35757;&#32451;&#36807;&#31243;&#65292;&#36890;&#36807;&#19982;&#20132;&#20114;&#36712;&#36857;&#23398;&#20064;&#30340;&#35268;&#21010;&#27169;&#22411;&#65288;&#21253;&#25324;&#21160;&#20316;&#27169;&#22411;&#21644;&#20998;&#23618;&#20219;&#21153;&#32593;&#32476;&#27169;&#22411;&#65289;&#21644;&#31526;&#21495;&#36873;&#39033;&#26469;&#24341;&#23548;&#31574;&#30053;&#30340;&#25913;&#36827;&#12290;&#23398;&#21040;&#30340;&#31526;&#21495;&#36873;&#39033;&#20943;&#36731;&#20102;&#23545;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#35201;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#31574;&#30053;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19982;&#31526;&#21495;&#35268;&#21010;&#27169;&#22411;&#30340;&#35268;&#21010;&#65292;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#21487;&#36801;&#31227;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite of achieving great success in real-world applications, Deep Reinforcement Learning (DRL) is still suffering from three critical issues, i.e., data efficiency, lack of the interpretability and transferability. Recent research shows that embedding symbolic knowledge into DRL is promising in addressing those challenges. Inspired by this, we introduce a novel deep reinforcement learning framework with symbolic options. Our framework features a loop training procedure, which enables guiding the improvement of policy by planning with planning models (including action models and hierarchical task network models) and symbolic options learned from interactive trajectories automatically. The learned symbolic options alleviate the dense requirement of expert domain knowledge and provide inherent interpretability of policies. Moreover, the transferability and data efficiency can be further improved by planning with the symbolic planning models. To validate the effectiveness of our framewor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#23436;&#20840;&#20998;&#25955;&#21270;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22823;&#35268;&#27169;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#28151;&#21512;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#21035;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#65292;&#24182;&#23454;&#29616;&#31574;&#30053;&#25913;&#36827;&#21644;&#20215;&#20540;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2004.11145</link><description>&lt;p&gt;
F2A2: &#28789;&#27963;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning. (arXiv:2004.11145v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.11145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#23436;&#20840;&#20998;&#25955;&#21270;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22823;&#35268;&#27169;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#28151;&#21512;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#21035;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#65292;&#24182;&#23454;&#29616;&#31574;&#30053;&#25913;&#36827;&#21644;&#20215;&#20540;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20013;&#22830;&#38598;&#26435;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22797;&#26434;&#24212;&#29992;&#20013;&#26377;&#26102;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#20043;&#38388;&#32570;&#20047;&#20114;&#21160;&#65292;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#20986;&#29616;&#20102;&#19968;&#20123;&#20998;&#25955;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20998;&#25955;&#21270;&#26041;&#27861;&#21482;&#33021;&#22788;&#29702;&#23436;&#20840;&#21512;&#20316;&#30340;&#35774;&#32622;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#20256;&#36755;&#22823;&#37327;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;&#22359;&#22352;&#26631;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#35745;&#31639;&#65292;&#20294;&#20250;&#24341;&#36215;&#20005;&#37325;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#23436;&#20840;&#20998;&#25955;&#21270;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#32452;&#21512;&#22823;&#22810;&#25968;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#24182;&#22788;&#29702;&#22823;&#35268;&#27169;&#19968;&#33324;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#28151;&#21512;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20998;&#21035;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#26469;&#23454;&#29616;&#20998;&#25955;&#21270;&#12290;&#20174;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23454;&#29616;&#20102;&#31574;&#30053;&#25913;&#36827;&#21644;&#20215;&#20540;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional centralized multi-agent reinforcement learning (MARL) algorithms are sometimes unpractical in complicated applications, due to non-interactivity between agents, curse of dimensionality and computation complexity. Hence, several decentralized MARL algorithms are motivated. However, existing decentralized methods only handle the fully cooperative setting where massive information needs to be transmitted in training. The block coordinate gradient descent scheme they used for successive independent actor and critic steps can simplify the calculation, but it causes serious bias. In this paper, we propose a flexible fully decentralized actor-critic MARL framework, which can combine most of actor-critic methods, and handle large-scale general cooperative multi-agent setting. A primal-dual hybrid gradient descent type algorithm framework is designed to learn individual agents separately for decentralization. From the perspective of each agent, policy improvement and value evaluatio
&lt;/p&gt;</description></item></channel></rss>