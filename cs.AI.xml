<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#24322;&#26500;&#35757;&#32451;&#23454;&#29616;&#39640;&#25928;&#36890;&#29992;&#27169;&#22359;&#21270;&#35270;&#35273;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22312;&#35270;&#35273;&#20219;&#21153;&#20043;&#38388;&#30340;&#22823;&#37327;&#20869;&#22312;&#24046;&#24322;&#65292;&#24182;&#35299;&#20915;&#22810;&#20219;&#21153;&#27169;&#22411;&#25193;&#23637;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.17165</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#24322;&#26500;&#35757;&#32451;&#23454;&#29616;&#39640;&#25928;&#36890;&#29992;&#27169;&#22359;&#21270;&#35270;&#35273;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Efficient General-Purpose Modular Vision Model via Multi-Task Heterogeneous Training. (arXiv:2306.17165v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#24322;&#26500;&#35757;&#32451;&#23454;&#29616;&#39640;&#25928;&#36890;&#29992;&#27169;&#22359;&#21270;&#35270;&#35273;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22312;&#35270;&#35273;&#20219;&#21153;&#20043;&#38388;&#30340;&#22823;&#37327;&#20869;&#22312;&#24046;&#24322;&#65292;&#24182;&#35299;&#20915;&#22810;&#20219;&#21153;&#27169;&#22411;&#25193;&#23637;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#25191;&#34892;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#22320;&#36866;&#24212;&#20854;&#20182;&#21518;&#32493;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#20174;&#22810;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#65306;&#21363;&#21333;&#20010;&#22270;&#20687;&#38598;&#21512;&#20855;&#26377;&#22810;&#20010;&#20219;&#21153;&#26631;&#31614;&#12290;&#36825;&#31181;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#24456;&#23569;&#12289;&#35268;&#27169;&#23567;&#19988;&#26114;&#36149;&#12290;&#25105;&#20204;&#23558;&#24322;&#26500;&#25351;&#30340;&#26159;&#20855;&#26377;&#19981;&#21516;&#20219;&#21153;&#26631;&#31614;&#30340;&#22270;&#20687;&#38598;&#65292;&#25110;&#32773;&#26159;&#21333;&#19968;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#12290;&#24456;&#23569;&#26377;&#20154;&#30740;&#31350;&#22312;&#36825;&#31181;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#29992;&#35270;&#35273;&#27169;&#22411;&#20173;&#28982;&#20197;&#21333;&#19968;&#20219;&#21153;&#39044;&#35757;&#32451;&#20026;&#20027;&#23548;&#65292;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#35774;&#35745;&#29992;&#20110;&#19981;&#21516;&#30446;&#30340;&#30340;&#20027;&#27969;&#35270;&#35273;&#25968;&#25454;&#38598;&#26469;&#25193;&#23637;&#22810;&#20219;&#21153;&#27169;&#22411;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25361;&#25112;&#22312;&#20110;&#31649;&#29702;&#35270;&#35273;&#20219;&#21153;&#20043;&#38388;&#30340;&#22823;&#37327;&#20869;&#22312;&#24046;&#24322;&#65292;&#21253;&#25324;&#25968;&#25454;&#20998;&#24067;&#12289;&#26550;&#26500;&#12289;&#20219;&#21153;&#29305;&#23450;&#27169;&#22359;&#12289;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#37319;&#26679;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20462;&#25913;&#21644;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;(MoE)&#35270;&#35273;&#36716;&#25442;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a model that can perform multiple vision tasks and can be adapted to other downstream tasks efficiently. Despite considerable progress in multi-task learning, most efforts focus on learning from multi-label data: a single image set with multiple task labels. Such multi-label data sets are rare, small, and expensive. We say heterogeneous to refer to image sets with different task labels, or to combinations of single-task datasets. Few have explored training on such heterogeneous datasets. General-purpose vision models are still dominated by single-task pretraining, and it remains unclear how to scale up multi-task models by leveraging mainstream vision datasets designed for different purposes. The challenges lie in managing large intrinsic differences among vision tasks, including data distribution, architectures, task-specific modules, dataset scales, and sampling strategies. To address these challenges, we propose to modify and scale up mixture-of-experts (MoE) vision trans
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGGAN&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;&#65292;&#20197;&#29992;&#20110;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#30456;&#23545;&#22797;&#26434;&#30340;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#20132;&#26131;&#25968;&#25454;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#21319;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17109</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#29983;&#25104;&#29992;&#20110;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#30340;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Synthetic Demographic Data Generation for Card Fraud Detection Using GANs. (arXiv:2306.17109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17109
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGGAN&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;&#65292;&#20197;&#29992;&#20110;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#30456;&#23545;&#22797;&#26434;&#30340;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#20132;&#26131;&#25968;&#25454;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#21319;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#12290;&#29983;&#25104;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#27450;&#35784;&#30340;&#21512;&#25104;&#20132;&#26131;&#25968;&#25454;&#30340;&#25216;&#26415;&#20063;&#22312;&#24555;&#36895;&#21457;&#23637;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#21482;&#21253;&#21547;&#20132;&#26131;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#26102;&#38388;&#12289;&#22320;&#28857;&#21644;&#37329;&#39069;&#12290;&#36890;&#24120;&#19981;&#21253;&#21547;&#20010;&#20307;&#29992;&#25143;&#30340;&#29305;&#24449;&#65288;&#24180;&#40836;&#21644;&#24615;&#21035;&#20598;&#23572;&#20250;&#21253;&#21547;&#65289;&#12290;&#20351;&#29992;&#30456;&#23545;&#22797;&#26434;&#30340;&#21512;&#25104;&#20154;&#21475;&#25968;&#25454;&#21487;&#33021;&#25552;&#39640;&#20132;&#26131;&#25968;&#25454;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#21463;&#30410;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#36229;&#36807;&#20854;&#20182;&#25104;&#29087;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65288;&#22914;&#24494;&#27169;&#25311;&#65289;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;DGGAN&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#20154;&#21475;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#29983;&#25104;&#26679;&#26412;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using machine learning models to generate synthetic data has become common in many fields. Technology to generate synthetic transactions that can be used to detect fraud is also growing fast. Generally, this synthetic data contains only information about the transaction, such as the time, place, and amount of money. It does not usually contain the individual user's characteristics (age and gender are occasionally included). Using relatively complex synthetic demographic data may improve the complexity of transaction data features, thus improving the fraud detection performance. Benefiting from developments of machine learning, some deep learning models have potential to perform better than other well-established synthetic data generation methods, such as microsimulation. In this study, we built a deep-learning Generative Adversarial Network (GAN), called DGGAN, which will be used for demographic data generation. Our model generates samples during model training, which we found importan
&lt;/p&gt;</description></item><item><title>RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17100</link><description>&lt;p&gt;
RL4CO: &#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark. (arXiv:2306.17100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17100
&lt;/p&gt;
&lt;p&gt;
RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;RL4CO&#65292;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;RL4CO&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#24211;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#22914;&#27169;&#22359;&#21270;&#21644;&#37197;&#32622;&#31649;&#29702;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#29615;&#22659;&#21644;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65289;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24378;&#35843;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#21508;&#31181;&#20248;&#21270;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;&#27169;&#22411;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#38646;-shot&#27867;&#21270;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#20351;&#29992;&#36825;&#20123;&#26032;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#26102;&#33853;&#21518;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#26356;&#21152;&#24179;&#34913;&#22320;&#35780;&#20272;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;RL4CO&#33021;&#22815;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse optimization tasks. We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these new metrics, suggesting the necessity for a more balanced view of the performance of neural CO solvers. We hope RL4CO will 
&lt;/p&gt;</description></item><item><title>RAPGen&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;Retrieval-Augmented Prompt Generation&#65288;RAPGen&#65289;&#26041;&#27861;&#65292;&#21363;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#24615;&#33021;Bug&#20462;&#22797;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#25552;&#31034;&#25351;&#20196;&#24182;&#29983;&#25104;&#25552;&#31034;&#65292;&#28982;&#21518;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#29983;&#25104;&#20462;&#22797;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20195;&#30721;&#20302;&#25928;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19987;&#23478;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;RAPGen&#22312;60%&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#29983;&#25104;&#19982;&#24320;&#21457;&#32773;&#31561;&#25928;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#24314;&#35758;&#65292;&#20854;&#20013;&#32422;39%&#30340;&#24314;&#35758;&#23436;&#20840;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.17077</link><description>&lt;p&gt;
RAPGen: &#19968;&#31181;&#35299;&#20915;&#38646;&#26679;&#26412;&#20195;&#30721;&#20302;&#25928;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RAPGen: An Approach for Fixing Code Inefficiencies in Zero-Shot. (arXiv:2306.17077v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17077
&lt;/p&gt;
&lt;p&gt;
RAPGen&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;Retrieval-Augmented Prompt Generation&#65288;RAPGen&#65289;&#26041;&#27861;&#65292;&#21363;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#24615;&#33021;Bug&#20462;&#22797;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#25552;&#31034;&#25351;&#20196;&#24182;&#29983;&#25104;&#25552;&#31034;&#65292;&#28982;&#21518;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#29983;&#25104;&#20462;&#22797;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20195;&#30721;&#20302;&#25928;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19987;&#23478;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;RAPGen&#22312;60%&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#29983;&#25104;&#19982;&#24320;&#21457;&#32773;&#31561;&#25928;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#24314;&#35758;&#65292;&#20854;&#20013;&#32422;39%&#30340;&#24314;&#35758;&#23436;&#20840;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;Bug&#26159;&#19968;&#31181;&#21363;&#20351;&#22312;&#32463;&#36807;&#20805;&#20998;&#27979;&#35797;&#30340;&#21830;&#19994;&#20135;&#21697;&#20013;&#20063;&#21487;&#33021;&#20986;&#29616;&#30340;&#38750;&#21151;&#33021;&#24615;&#38382;&#39064;&#12290;&#20462;&#22797;&#36825;&#20123;&#24615;&#33021;Bug&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Retrieval-Augmented Prompt Generation&#65288;RAPGen&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#23384;&#22312;&#24615;&#33021;&#38382;&#39064;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;RAPGen&#39318;&#20808;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#20043;&#21069;&#24615;&#33021;Bug&#20462;&#22797;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#19968;&#20010;&#25552;&#31034;&#25351;&#20196;&#65292;&#28982;&#21518;&#20351;&#29992;&#26816;&#32034;&#21040;&#30340;&#25351;&#20196;&#29983;&#25104;&#19968;&#20010;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#36825;&#20010;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Codex&#65289;&#19978;&#29983;&#25104;&#19968;&#20010;&#20462;&#22797;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#21508;&#31181;&#25552;&#31034;&#21464;&#20307;&#21644;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;Bug&#20462;&#22797;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;RAPGen&#22312;60%&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#29983;&#25104;&#19982;&#24320;&#21457;&#32773;&#31561;&#25928;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#24314;&#35758;&#65292;&#22312;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#36807;&#21435;C#&#24320;&#21457;&#32773;&#25152;&#20570;&#30340;&#24615;&#33021;&#26356;&#25913;&#25968;&#25454;&#38598;&#20013;&#26377;&#32422;39%&#30340;&#24314;&#35758;&#23436;&#20840;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance bugs are non-functional bugs that can even manifest in well-tested commercial products. Fixing these performance bugs is an important yet challenging problem. In this work, we address this challenge and present a new approach called Retrieval-Augmented Prompt Generation (RAPGen). Given a code snippet with a performance issue, RAPGen first retrieves a prompt instruction from a pre-constructed knowledge-base of previous performance bug fixes and then generates a prompt using the retrieved instruction. It then uses this prompt on a Large Language Model (such as Codex) in zero-shot to generate a fix. We compare our approach with the various prompt variations and state of the art methods in the task of performance bug fixing. Our evaluation shows that RAPGen can generate performance improvement suggestions equivalent or better than a developer in ~60% of the cases, getting ~39% of them verbatim, in an expert-verified dataset of past performance changes made by C# developers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32467;&#26500;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffusionPose)&#29992;&#20110;&#20108;&#32500;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#65292;&#36890;&#36807;&#20174;&#22122;&#22768;&#28909;&#22270;&#29983;&#25104;&#20851;&#38190;&#28857;&#28909;&#22270;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#23558;&#20851;&#38190;&#28857;&#25193;&#25955;&#21040;&#38543;&#26426;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#26500;&#24314;&#30340;&#26465;&#20214;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#28909;&#22270;&#12290;&#25193;&#25955;&#27169;&#22411;&#20197;&#36880;&#27493;&#21435;&#22122;&#30340;&#26041;&#24335;&#29983;&#25104;&#28909;&#22270;&#65292;&#21516;&#26102;&#21033;&#29992;&#20154;&#20307;&#32467;&#26500;&#20449;&#24687;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.17074</link><description>&lt;p&gt;
&#23398;&#20064;&#32467;&#26500;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#20108;&#32500;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Learning Structure-Guided Diffusion Model for 2D Human Pose Estimation. (arXiv:2306.17074v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32467;&#26500;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffusionPose)&#29992;&#20110;&#20108;&#32500;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#65292;&#36890;&#36807;&#20174;&#22122;&#22768;&#28909;&#22270;&#29983;&#25104;&#20851;&#38190;&#28857;&#28909;&#22270;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#23558;&#20851;&#38190;&#28857;&#25193;&#25955;&#21040;&#38543;&#26426;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#26500;&#24314;&#30340;&#26465;&#20214;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#28909;&#22270;&#12290;&#25193;&#25955;&#27169;&#22411;&#20197;&#36880;&#27493;&#21435;&#22122;&#30340;&#26041;&#24335;&#29983;&#25104;&#28909;&#22270;&#65292;&#21516;&#26102;&#21033;&#29992;&#20154;&#20307;&#32467;&#26500;&#20449;&#24687;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#32500;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#20027;&#27969;&#26041;&#26696;&#20043;&#19968;&#26159;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20851;&#38190;&#28857;&#28909;&#22270;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#26550;&#26500;&#65288;&#22914;&#39640;&#20998;&#36776;&#29575;&#34920;&#31034;&#21644;&#35270;&#35273;Transformer&#65289;&#25913;&#36827;&#28909;&#22270;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DiffusionPose&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#23558;&#20108;&#32500;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#34920;&#36848;&#20026;&#20174;&#22122;&#22768;&#28909;&#22270;&#29983;&#25104;&#20851;&#38190;&#28857;&#28909;&#22270;&#30340;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#23558;&#20851;&#38190;&#28857;&#25193;&#25955;&#21040;&#38543;&#26426;&#20998;&#24067;&#65292;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#26681;&#25454;&#22270;&#20687;&#29305;&#24449;&#26500;&#24314;&#30340;&#26465;&#20214;&#20174;&#22122;&#22768;&#28909;&#22270;&#20013;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#28909;&#22270;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#20197;&#36880;&#27493;&#21435;&#22122;&#30340;&#26041;&#24335;&#20174;&#21021;&#22987;&#21270;&#28909;&#22270;&#29983;&#25104;&#28909;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#36890;&#36807;&#20154;&#20307;&#32467;&#26500;&#20449;&#24687;&#25913;&#21892;DiffusionPose&#30340;&#24615;&#33021;&#12290;&#22823;&#37327;&#23454;&#39564;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;DiffusionPose&#30340;&#20248;&#36234;&#24615;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;1.6&#12289;1.2&#21644;1.2 mAP&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the mainstream schemes for 2D human pose estimation (HPE) is learning keypoints heatmaps by a neural network. Existing methods typically improve the quality of heatmaps by customized architectures, such as high-resolution representation and vision Transformers. In this paper, we propose \textbf{DiffusionPose}, a new scheme that formulates 2D HPE as a keypoints heatmaps generation problem from noised heatmaps. During training, the keypoints are diffused to random distribution by adding noises and the diffusion model learns to recover ground-truth heatmaps from noised heatmaps with respect to conditions constructed by image feature. During inference, the diffusion model generates heatmaps from initialized heatmaps in a progressive denoising way. Moreover, we further explore improving the performance of DiffusionPose with conditions from human structural information. Extensive experiments show the prowess of our DiffusionPose, with improvements of 1.6, 1.2, and 1.2 mAP on widely-us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#21019;&#36896;&#21147;&#20013;&#20154;&#31867;&#21464;&#37327;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#24433;&#21709;&#21644;&#36716;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;22&#20010;&#35775;&#35848;&#21457;&#29616;&#20102;&#19982;&#35745;&#31639;&#21019;&#36896;&#21147;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.17070</link><description>&lt;p&gt;
&#35745;&#31639;&#21019;&#36896;&#21147;&#20013;&#30340;&#36328;&#23398;&#31185;&#26041;&#27861;&#65306;&#20154;&#31867;&#21464;&#37327;&#22914;&#20309;&#22609;&#36896;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Interdisciplinary Methods in Computational Creativity: How Human Variables Shape Human-Inspired AI Research. (arXiv:2306.17070v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#21019;&#36896;&#21147;&#20013;&#20154;&#31867;&#21464;&#37327;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#24433;&#21709;&#21644;&#36716;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;22&#20010;&#35775;&#35848;&#21457;&#29616;&#20102;&#19982;&#35745;&#31639;&#21019;&#36896;&#21147;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#21147;&#26368;&#21021;&#26159;&#26469;&#33258;&#20154;&#31867;&#24515;&#29702;&#23398;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#20294;&#22312;&#35745;&#31639;&#21019;&#36896;&#21147;&#30340;&#39046;&#22495;&#20013;&#65292;&#23427;&#21464;&#24471;&#26356;&#28145;&#20837;&#12290;&#24403;&#21019;&#36896;&#21147;&#20316;&#20026;&#35745;&#31639;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#26102;&#65292;&#21019;&#36896;&#21147;&#30340;&#21547;&#20041;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#35745;&#31639;&#21019;&#36896;&#21147;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#30740;&#31350;&#20154;&#21592;&#23558;&#20154;&#31867;&#24515;&#29702;&#23398;&#30340;&#27010;&#24565;&#36716;&#21270;&#20026;&#35745;&#31639;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;22&#20010;&#28145;&#20837;&#30340;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#36827;&#34892;&#30740;&#31350;&#65292;&#20027;&#35201;&#38024;&#23545;&#20197;&#21019;&#36896;&#21147;&#20026;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#19982;&#35745;&#31639;&#21019;&#36896;&#21147;&#26368;&#30456;&#20851;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The word creativity originally described a concept from human psychology, but in the realm of computational creativity (CC), it has become much more. The question of what creativity means when it is part of a computational system might be considered core to CC. Pinning down the meaning of creativity, and concepts like it, becomes salient when researchers port concepts from human psychology to computation, a widespread practice extending beyond CC into artificial intelligence (AI). Yet, the human processes shaping human-inspired computational systems have been little investigated. In this paper, we question which human literatures (social sciences, psychology, neuroscience) enter AI scholarship and how they are translated at the port of entry. This study is based on 22 in-depth, semi-structured interviews, primarily with human-inspired AI researchers, half of whom focus on creativity as a major research area. This paper focuses on findings most relevant to CC. We suggest that which huma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#33014;&#22218;&#32593;&#32476;&#30340;&#38463;&#25289;&#20271;&#35821;&#21644;&#27874;&#26031;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21333;&#29420;&#30340;&#33014;&#22218;&#32593;&#32476;&#24182;&#20351;&#29992;&#21152;&#26435;&#24230;&#37327;&#26469;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17068</link><description>&lt;p&gt;
&#22522;&#20110;&#21152;&#26435;CapsuleNet&#32593;&#32476;&#30340;&#38463;&#25289;&#20271;&#35821;&#21644;&#27874;&#26031;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Presenting an approach based on weighted CapsuleNet networks for Arabic and Persian multi-domain sentiment analysis. (arXiv:2306.17068v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#33014;&#22218;&#32593;&#32476;&#30340;&#38463;&#25289;&#20271;&#35821;&#21644;&#27874;&#26031;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21333;&#29420;&#30340;&#33014;&#22218;&#32593;&#32476;&#24182;&#20351;&#29992;&#21152;&#26435;&#24230;&#37327;&#26469;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#23545;&#33258;&#30001;&#25991;&#26412;&#36827;&#34892;&#27491;&#38754;&#12289;&#36127;&#38754;&#25110;&#20013;&#24615;&#30340;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#39046;&#22495;&#65292;&#20998;&#31867;&#22120;&#22312;&#19968;&#20010;&#39046;&#22495;&#20013;&#21487;&#33021;&#20855;&#26377;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#20013;&#30001;&#20110;&#35789;&#35821;&#30340;&#35821;&#20041;&#22810;&#37325;&#24615;&#32780;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27874;&#26031;&#35821;/&#38463;&#25289;&#20271;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#32047;&#31215;&#21152;&#26435;&#33014;&#22218;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#21152;&#26435;&#33014;&#22218;&#38598;&#21512;&#30001;&#20026;&#27599;&#20010;&#39046;&#22495;&#35757;&#32451;&#30340;&#21333;&#29420;&#30340;&#33014;&#22218;&#32593;&#32476;&#21644;&#31216;&#20026;&#39046;&#22495;&#25152;&#23646;&#24230;&#65288;DBD&#65289;&#30340;&#21152;&#26435;&#24230;&#37327;&#32452;&#25104;&#12290;&#36825;&#20010;&#24230;&#37327;&#30001;TF&#21644;IDF&#32452;&#25104;&#65292;&#35745;&#31639;&#27599;&#20010;&#25991;&#26723;&#23545;&#20110;&#27599;&#20010;&#39046;&#22495;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#28982;&#21518;&#20056;&#20197;&#27599;&#20010;&#33014;&#22218;&#21019;&#24314;&#30340;&#21487;&#33021;&#36755;&#20986;&#12290;&#26368;&#32456;&#65292;&#36825;&#20123;&#20056;&#31215;&#30340;&#24635;&#21644;&#26159;&#26368;&#32456;&#36755;&#20986;&#30340;&#26631;&#31614;&#65292;&#24182;&#29992;&#20110;&#30830;&#23450;&#26497;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment classification is a fundamental task in natural language processing, assigning one of the three classes, positive, negative, or neutral, to free texts. However, sentiment classification models are highly domain dependent; the classifier may perform classification with reasonable accuracy in one domain but not in another due to the Semantic multiplicity of words getting poor accuracy. This article presents a new Persian/Arabic multi-domain sentiment analysis method using the cumulative weighted capsule networks approach. Weighted capsule ensemble consists of training separate capsule networks for each domain and a weighting measure called domain belonging degree (DBD). This criterion consists of TF and IDF, which calculates the dependency of each document for each domain separately; this value is multiplied by the possible output that each capsule creates. In the end, the sum of these multiplications is the title of the final output, and is used to determine the polarity. And 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mapKurator&#30340;&#31995;&#32479;&#65292;&#33021;&#23436;&#25972;&#22320;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#20301;&#32622;&#30456;&#20851;&#35789;&#35821;&#30340;&#24573;&#30053;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#32771;&#34385;&#26356;&#24191;&#30340;&#20027;&#39064;&#33539;&#22260;&#65292;&#33021;&#22815;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.17059</link><description>&lt;p&gt;
The mapKurator&#31995;&#32479;&#65306;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#30340;&#23436;&#25972;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps. (arXiv:2306.17059v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mapKurator&#30340;&#31995;&#32479;&#65292;&#33021;&#23436;&#25972;&#22320;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#20301;&#32622;&#30456;&#20851;&#35789;&#35821;&#30340;&#24573;&#30053;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#32771;&#34385;&#26356;&#24191;&#30340;&#20027;&#39064;&#33539;&#22260;&#65292;&#33021;&#22815;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#20855;&#26377;&#31354;&#38388;&#28966;&#28857;&#21644;&#26377;&#20215;&#20540;&#30340;&#22320;&#26041;&#29305;&#24449;&#12290;&#20363;&#22914;&#65292;&#25151;&#22320;&#20135;&#25110;&#26053;&#34892;&#21338;&#23458;&#20013;&#30340;&#21015;&#34920;&#25551;&#36848;&#21253;&#21547;&#26377;&#20851;&#29305;&#23450;&#22320;&#21306;&#31038;&#21306;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#25551;&#36848;&#20154;&#31867;&#22914;&#20309;&#24863;&#30693;&#20182;&#20204;&#30340;&#29615;&#22659;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#30340;&#31532;&#19968;&#27493;&#26159;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#65288;&#20363;&#22914;&#65292;&#22478;&#24066;&#65289;&#12290;&#20256;&#32479;&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#20381;&#36182;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#27979;&#21644;&#28040;&#27495;&#21270;&#22320;&#21517;&#12290;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#21253;&#21547;&#20301;&#32622;&#30701;&#35821;&#21644;&#20020;&#26102;&#35268;&#21017;&#30340;&#35789;&#27719;&#38598;&#65292;&#36825;&#20123;&#35268;&#21017;&#24573;&#30053;&#20102;&#19982;&#20301;&#32622;&#30456;&#20851;&#30340;&#37325;&#35201;&#35789;&#35821;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36890;&#24120;&#32771;&#34385;&#20960;&#20010;&#24191;&#24230;&#30340;&#20027;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#21487;&#20197;&#26159;&#19968;&#20010;&#22269;&#23478;&#12289;&#19968;&#20010;&#22478;&#24066;&#65292;&#29978;&#33267;&#26159;&#19968;&#20010;&#31038;&#21306;&#65292;&#36825;&#20123;&#33539;&#22260;&#27604;&#36825;&#20123;&#26041;&#27861;&#32771;&#34385;&#30340;&#20027;&#39064;&#25968;&#35201;&#22823;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Documents hold spatial focus and valuable locality characteristics. For example, descriptions of listings in real estate or travel blogs contain information about specific local neighborhoods. This information is valuable to characterize how humans perceive their environment. However, the first step to making use of this information is to identify the spatial focus (e.g., a city) of a document. Traditional approaches for identifying the spatial focus of a document rely on detecting and disambiguating toponyms from the document. This approach requires a vocabulary set of location phrases and ad-hoc rules, which ignore important words related to location. Recent topic modeling approaches using large language models often consider a few topics, each with broad coverage. In contrast, the spatial focus of a document can be a country, a city, or even a neighborhood, which together, is much larger than the number of topics considered in these approaches. Additionally, topic modeling methods a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#24773;&#20917;&#19979;&#36798;&#21040;&#23433;&#20840;&#31574;&#30053;&#30340;&#20248;&#21270;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17052</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2306.17052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#24773;&#20917;&#19979;&#36798;&#21040;&#23433;&#20840;&#31574;&#30053;&#30340;&#20248;&#21270;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24212;&#29992;&#65292;&#27604;&#22914;&#20849;&#20139;&#20132;&#36890;&#65292;&#38656;&#35201;&#21327;&#35843;&#22823;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20248;&#21270;&#20195;&#34920;&#24615;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#26469;&#24212;&#23545;&#30001;&#27492;&#24102;&#26469;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#21363;&#26234;&#33021;&#20307;&#20998;&#24067;&#23384;&#22312;&#20840;&#23616;&#32422;&#26463;&#30340;&#24773;&#20917;&#65288;&#20363;&#22914;&#38656;&#35201;&#28385;&#36275;&#23481;&#37327;&#32422;&#26463;&#25110;&#26368;&#23567;&#35206;&#30422;&#35201;&#27714;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Safe-$\text{M}^3$-UCRL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#26410;&#30693;&#36716;&#31227;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23433;&#20840;&#31574;&#30053;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23427;&#22312;&#20445;&#35777;&#24754;&#35266;&#32422;&#26463;&#28385;&#36275;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#36716;&#31227;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#26469;&#20351;&#29992;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#30830;&#20445;&#39640;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#20849;&#20139;&#20132;&#36890;&#36816;&#33829;&#21830;&#38754;&#20020;&#30340;&#36710;&#36742;&#37325;&#23450;&#20301;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;Safe-$\text{M}^3$-UCRL&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#28145;&#22323;&#20986;&#31199;&#36710;&#36712;&#36857;&#25968;&#25454;&#30340;&#20223;&#30495;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#28385;&#36275;&#20851;&#38190;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications, e.g., in shared mobility, require coordinating a large number of agents. Mean-field reinforcement learning addresses the resulting scalability challenge by optimizing the policy of a representative agent. In this paper, we address an important generalization where there exist global constraints on the distribution of agents (e.g., requiring capacity constraints or minimum coverage requirements to be met). We propose Safe-$\text{M}^3$-UCRL, the first model-based algorithm that attains safe policies even in the case of unknown transition dynamics. As a key ingredient, it uses epistemic uncertainty in the transition model within a log-barrier approach to ensure pessimistic constraints satisfaction with high probability. We showcase Safe-$\text{M}^3$-UCRL on the vehicle repositioning problem faced by many shared mobility operators and evaluate its performance through simulations built on Shenzhen taxi trajectory data. Our algorithm effectively meets the demand in critica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550; LR-GCN&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#34917;&#20840;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25506;&#32034;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#33258;&#21160;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#36828;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#25552;&#28860;&#30693;&#35782;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#31232;&#30095;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.17034</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#21033;&#29992;&#39640;&#38454;&#22270;&#32467;&#26500;&#36827;&#34892;&#31232;&#30095;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exploring &amp; Exploiting High-Order Graph Structure for Sparse Knowledge Graph Completion. (arXiv:2306.17034v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550; LR-GCN&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#34917;&#20840;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25506;&#32034;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#33258;&#21160;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#36828;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#25552;&#28860;&#30693;&#35782;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#31232;&#30095;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30693;&#35782;&#22270;&#35889;&#22330;&#26223;&#23545;&#20043;&#21069;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#21363;&#38543;&#30528;&#22270;&#30340;&#31232;&#30095;&#24615;&#22686;&#21152;&#65292;&#34917;&#20840;&#24615;&#33021;&#36805;&#36895;&#19979;&#38477;&#12290;&#30001;&#20110;&#31232;&#30095;&#30693;&#35782;&#22270;&#35889;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#34987;&#21152;&#21095;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;LR-GCN&#65292;&#33021;&#22815;&#33258;&#21160;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#26377;&#20215;&#20540;&#30340;&#36828;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#34917;&#20805;&#19981;&#36275;&#30340;&#32467;&#26500;&#29305;&#24449;&#24182;&#25552;&#28860;&#36923;&#36753;&#25512;&#29702;&#30693;&#35782;&#29992;&#20110;&#31232;&#30095;&#22270;&#35889;&#34917;&#20840;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#22522;&#20110;GNN&#30340;&#39044;&#27979;&#22120;&#21644;&#25512;&#29702;&#36335;&#24452;&#25552;&#21462;&#22120;&#12290;&#25512;&#29702;&#36335;&#24452;&#25552;&#21462;&#22120;&#25506;&#32034;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#22914;&#25512;&#29702;&#36335;&#24452;&#65292;&#24182;&#23558;&#20854;&#32534;&#30721;&#20026;&#23500;&#35821;&#20041;&#36793;&#65292;&#26126;&#30830;&#22320;&#23558;&#36828;&#31243;&#20381;&#36182;&#20851;&#31995;&#32452;&#21512;&#21040;&#39044;&#27979;&#22120;&#20013;&#12290;&#27492;&#27493;&#39588;&#36824;&#22312;&#31232;&#30095;&#38382;&#39064;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#31232;&#30095;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36335;&#24452;&#25552;&#21462;&#22120;&#36824;&#21487;&#20197;&#24110;&#21161;&#23494;&#21270;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse knowledge graph (KG) scenarios pose a challenge for previous Knowledge Graph Completion (KGC) methods, that is, the completion performance decreases rapidly with the increase of graph sparsity. This problem is also exacerbated because of the widespread existence of sparse KGs in practical applications. To alleviate this challenge, we present a novel framework, LR-GCN, that is able to automatically capture valuable long-range dependency among entities to supplement insufficient structure features and distill logical reasoning knowledge for sparse KGC. The proposed approach comprises two main components: a GNN-based predictor and a reasoning path distiller. The reasoning path distiller explores high-order graph structures such as reasoning paths and encodes them as rich-semantic edges, explicitly compositing long-range dependencies into the predictor. This step also plays an essential role in densifying KGs, effectively alleviating the sparse issue. Furthermore, the path distiller
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24067;&#23572;&#32452;&#21512;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#23433;&#20840;&#24615;&#27010;&#24565;&#21644;&#25299;&#23637;&#21040;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#23433;&#20840;&#24863;&#30693;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.17033</link><description>&lt;p&gt;
&#38754;&#21521;&#31163;&#25955;&#21644;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#24863;&#30693;&#20219;&#21153;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Safety-Aware Task Composition for Discrete and Continuous Reinforcement Learning. (arXiv:2306.17033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24067;&#23572;&#32452;&#21512;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#23433;&#20840;&#24615;&#27010;&#24565;&#21644;&#25299;&#23637;&#21040;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#23433;&#20840;&#24863;&#30693;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#24615;&#26159;&#21487;&#25193;&#23637;&#31995;&#32479;&#35774;&#35745;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26368;&#36817;&#22312;&#20219;&#21153;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;&#22312;&#30495;&#27491;&#21033;&#29992;&#32452;&#21512;&#26041;&#38754;&#25165;&#21018;&#21018;&#24320;&#22987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#23398;&#20064;&#20219;&#21153;&#30340;&#24067;&#23572;&#32452;&#21512;&#65292;&#32780;&#19981;&#26159;&#21151;&#33021;&#24615;&#25110;&#39034;&#24207;&#24615;&#32452;&#21512;&#12290;&#29616;&#26377;&#30340;RL&#24067;&#23572;&#32452;&#21512;&#20391;&#37325;&#20110;&#22312;&#20855;&#26377;&#31163;&#25955;&#34892;&#21160;&#31354;&#38388;&#30340;&#29615;&#22659;&#20013;&#36798;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#21560;&#25910;&#29366;&#24577;&#65292;&#20294;&#19981;&#25903;&#25345;&#21487;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#65288;&#21363;&#36991;&#20813;&#65289;&#32422;&#26463;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36129;&#29486;&#25512;&#36827;&#20102;&#23398;&#20064;&#20219;&#21153;&#24067;&#23572;&#32452;&#21512;&#30340;&#26368;&#26032;&#25216;&#26415;&#65306;i&#65289;&#22312;&#27492;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#23433;&#20840;&#24615;&#27010;&#24565;&#65307;ii&#65289;&#23637;&#31034;&#22914;&#20309;&#24378;&#21046;&#25191;&#34892;&#23433;&#20840;&#35821;&#20041;&#65292;&#35777;&#26126;&#27491;&#30830;&#24615;&#65288;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#65289;&#65292;&#24182;&#20998;&#26512;&#20004;&#31181;&#23433;&#20840;&#24615;&#27010;&#24565;&#20043;&#38388;&#30340;&#26435;&#34913;&#65307;iii&#65289;&#23558;&#24067;&#23572;&#32452;&#21512;&#20174;&#31163;&#25955;&#34892;&#21160;&#31354;&#38388;&#25193;&#23637;&#21040;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#20462;&#25913;&#29256;&#30340;&#20215;&#20540;&#36845;&#20195;&#31639;&#27861;&#26469;&#28436;&#31034;&#36825;&#20123;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositionality is a critical aspect of scalable system design. Reinforcement learning (RL) has recently shown substantial success in task learning, but has only recently begun to truly leverage composition. In this paper, we focus on Boolean composition of learned tasks as opposed to functional or sequential composition. Existing Boolean composition for RL focuses on reaching a satisfying absorbing state in environments with discrete action spaces, but does not support composable safety (i.e., avoidance) constraints. We advance the state of the art in Boolean composition of learned tasks with three contributions: i) introduce two distinct notions of safety in this framework; ii) show how to enforce either safety semantics, prove correctness (under some assumptions), and analyze the trade-offs between the two safety notions; and iii) extend Boolean composition from discrete action spaces to continuous action spaces. We demonstrate these techniques using modified versions of value iter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26469;&#36171;&#20104;&#27169;&#22411;&#23545;&#23567;&#25968;&#25454;&#38598;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17020</link><description>&lt;p&gt;
&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21028;&#20915;&#25991;&#20214;&#23545;&#29359;&#32618;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Crime Types using Judgment Documents from Social Media. (arXiv:2306.17020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26469;&#36171;&#20104;&#27169;&#22411;&#23545;&#23567;&#25968;&#25454;&#38598;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29359;&#32618;&#34892;&#20026;&#20107;&#23454;&#26469;&#30830;&#23450;&#29359;&#32618;&#31867;&#22411;&#30340;&#20219;&#21153;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#21644;&#26377;&#24847;&#20041;&#12290;&#20294;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#29359;&#32618;&#26412;&#36523;&#30340;&#24615;&#36136;&#65292;&#25968;&#25454;&#26679;&#26412;&#26412;&#36523;&#20998;&#24067;&#19981;&#22343;&#21248;&#12290;&#21516;&#26102;&#65292;&#21496;&#27861;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#23569;&#26377;&#20844;&#24320;&#21487;&#29992;&#65292;&#26080;&#27861;&#20135;&#29983;&#29992;&#20110;&#30452;&#25509;&#35757;&#32451;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#29359;&#32618;&#20107;&#23454;&#25968;&#25454;&#39044;&#22788;&#29702;&#27169;&#22359;(CFDPM)&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#25968;&#25454;&#38598;(CAIL-big)&#20316;&#20026;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#25105;&#20204;&#33258;&#24049;&#25910;&#38598;&#30340;&#19968;&#20010;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;&#27169;&#22411;&#23545;&#19981;&#29087;&#24713;&#30340;&#23567;&#25968;&#25454;&#38598;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#25913;&#36827;&#30340;Bert&#27169;&#22411;&#21644;&#21160;&#24577;&#36974;&#34109;&#26469;&#25913;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
The task of determining crime types based on criminal behavior facts has become a very important and meaningful task in social science. But the problem facing the field now is that the data samples themselves are unevenly distributed, due to the nature of the crime itself. At the same time, data sets in the judicial field are less publicly available, and it is not practical to produce large data sets for direct training. This article proposes a new training model to solve this problem through NLP processing methods. We first propose a Crime Fact Data Preprocessing Module (CFDPM), which can balance the defects of uneven data set distribution by generating new samples. Then we use a large open source dataset (CAIL-big) as our pretraining dataset and a small dataset collected by ourselves for Fine-tuning, giving it good generalization ability to unfamiliar small datasets. At the same time, we use the improved Bert model with dynamic masking to improve the model. Experiments show that the 
&lt;/p&gt;</description></item><item><title>milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17010</link><description>&lt;p&gt;
milliFlow&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17010
&lt;/p&gt;
&lt;p&gt;
milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26222;&#36866;&#35745;&#31639;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#22312;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#20154;&#20307;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#25163;&#21183;&#35782;&#21035;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#25668;&#20687;&#26426;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#26426;&#30340;&#20405;&#20837;&#24615;&#29305;&#28857;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26234;&#33021;&#23478;&#23621;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;milliFlow&#65292;&#29992;&#20110;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20316;&#20026;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#65292;&#30452;&#25509;&#21463;&#30410;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;3D&#31471;&#28857;&#35823;&#24046;&#20026;4.6cm&#65292;&#26126;&#26174;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion-Jump GNNs&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35843;&#33410;&#30340;&#24230;&#37327;&#36807;&#28388;&#22120;&#65292;&#26469;&#25552;&#39640;&#39640;&#38454;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#21270;&#22330;&#26223;&#19979;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#36339;&#36291;&#24335;&#30340;&#28176;&#36827;&#25193;&#25955;&#36317;&#31163;&#29983;&#25104;&#36807;&#28388;&#22120;&#30340;&#25903;&#25345;&#21644;&#31995;&#25968;&#65292;&#20197;&#23547;&#25214;&#25955;&#28857;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.16976</link><description>&lt;p&gt;
Diffusion-Jump GNNs: &#21487;&#23398;&#20064;&#24230;&#37327;&#36807;&#28388;&#22120;&#30340;&#21516;&#36136;&#21270;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Jump GNNs: Homophiliation via Learnable Metric Filters. (arXiv:2306.16976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16976
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion-Jump GNNs&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35843;&#33410;&#30340;&#24230;&#37327;&#36807;&#28388;&#22120;&#65292;&#26469;&#25552;&#39640;&#39640;&#38454;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#21270;&#22330;&#26223;&#19979;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#36339;&#36291;&#24335;&#30340;&#28176;&#36827;&#25193;&#25955;&#36317;&#31163;&#29983;&#25104;&#36807;&#28388;&#22120;&#30340;&#25903;&#25345;&#21644;&#31995;&#25968;&#65292;&#20197;&#23547;&#25214;&#25955;&#28857;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#38454;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HO-GNNs&#65289;&#34987;&#24320;&#21457;&#29992;&#20110;&#22312;&#24322;&#36136;&#24615;&#33539;&#22260;&#20013;&#25512;&#26029;&#19968;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20854;&#20013;&#26631;&#31614;&#20998;&#24067;&#19982;&#22270;&#32467;&#26500;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;HO-GNNs&#26159;&#22522;&#20110;&#36339;&#25968;&#30340;&#65292;&#21363;&#23427;&#20204;&#20381;&#36182;&#20110;&#36716;&#31227;&#30697;&#38453;&#30340;&#24130;&#27425;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32467;&#26500;&#23545;&#20998;&#31867;&#25439;&#22833;&#30340;&#21453;&#24212;&#19981;&#23436;&#20840;&#65292;&#24182;&#19988;&#25152;&#36798;&#21040;&#30340;&#32467;&#26500;&#21270;&#36807;&#28388;&#22120;&#20855;&#26377;&#38745;&#24577;&#25903;&#25345;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#20123;&#32593;&#32476;&#19981;&#33021;&#23398;&#20064;&#36807;&#28388;&#22120;&#30340;&#25903;&#25345;&#25110;&#31995;&#25968;&#65292;&#32780;&#21482;&#33021;&#23398;&#20064;&#36807;&#28388;&#22120;&#30340;&#32452;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28176;&#36827;&#25193;&#25955;&#36317;&#31163;&#30340;&#36339;&#36291;&#25193;&#25955;GNNs&#26041;&#27861;&#12290;&#25193;&#25955;&#36339;&#36291;&#29983;&#25104;&#19968;&#23545;&#19968;&#30340;&#36317;&#31163;&#65292;&#20854;&#25237;&#24433;&#30830;&#23450;&#27599;&#20010;&#32467;&#26500;&#21270;&#36807;&#28388;&#22120;&#30340;&#25903;&#25345;&#21644;&#31995;&#25968;&#12290;&#36825;&#20123;&#36807;&#28388;&#22120;&#31216;&#20026;&#36339;&#36291;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24191;&#27867;&#30340;&#23610;&#24230;&#33539;&#22260;&#20869;&#25506;&#32034;&#20197;&#25214;&#21040;&#25955;&#28857;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-order Graph Neural Networks (HO-GNNs) have been developed to infer consistent latent spaces in the heterophilic regime, where the label distribution is not correlated with the graph structure. However, most of the existing HO-GNNs are hop-based, i.e., they rely on the powers of the transition matrix. As a result, these architectures are not fully reactive to the classification loss and the achieved structural filters have static supports. In other words, neither the filters' supports nor their coefficients can be learned with these networks. They are confined, instead, to learn combinations of filters. To address the above concerns, we propose Diffusion-jump GNNs a method relying on asymptotic diffusion distances that operates on jumps. A diffusion-pump generates pairwise distances whose projections determine both the support and coefficients of each structural filter. These filters are called jumps because they explore a wide range of scales in order to find bonds between scatter
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#23436;&#25972;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#22914;&#20309;&#20174;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#36827;&#34892;&#21487;&#36776;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16958</link><description>&lt;p&gt;
&#30452;&#25509;&#25928;&#24212;&#22312;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of direct effects from summary causal graphs. (arXiv:2306.16958v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#23436;&#25972;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#22914;&#20309;&#20174;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#36827;&#34892;&#21487;&#36776;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#30452;&#25509;&#25928;&#24212;&#65292;&#21363;&#34913;&#37327;&#19968;&#20010;&#21464;&#37327;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#21464;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#21464;&#37327;&#19981;&#21464;&#12290;&#21160;&#24577;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#29992;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#26469;&#36827;&#34892;&#23450;&#24615;&#34920;&#31034;&#12290;&#20551;&#35774;&#32447;&#24615;&#21644;&#22240;&#26524;&#20805;&#20998;&#24615;&#65292;&#24182;&#32473;&#23450;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#24635;&#26159;&#21487;&#36776;&#35782;&#30340;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#30001;&#25152;&#35859;&#30340;&#21333;&#38376;&#20934;&#21017;&#32473;&#20986;&#30340;&#20219;&#20309;&#21464;&#37327;&#38598;&#21512;&#26469;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#27809;&#26377;&#27492;&#31867;&#22270;&#24418;&#21487;&#29992;&#65292;&#20294;&#19987;&#23478;&#20173;&#28982;&#21487;&#20197;&#35775;&#38382;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#19968;&#20010;&#25277;&#35937;&#65292;&#35813;&#25277;&#35937;&#34920;&#31034;&#20102;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#21516;&#26102;&#30465;&#30053;&#20102;&#26102;&#38388;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#65292;&#20854;&#20013;&#35814;&#32454;&#25551;&#36848;&#20102;&#25152;&#26377;&#30452;&#25509;&#25928;&#24212;&#22312;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#21487;&#36776;&#35782;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic structural causal models (SCMs) are a powerful framework for reasoning in dynamic systems about direct effects which measure how a change in one variable affects another variable while holding all other variables constant. The causal relations in a dynamic structural causal model can be qualitatively represented with a full-time causal graph. Assuming linearity and causal sufficiency and given the full-time causal graph, the direct causal effect is always identifiable and can be estimated from data by adjusting on any set of variables given by the so-called single-door criterion. However, in many application such a graph is not available for various reasons but nevertheless experts have access to an abstraction of the full-time causal graph which represents causal relations between time series while omitting temporal information. This paper presents a complete identifiability result which characterizes all cases for which the direct effect is graphically identifiable from summa
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35201;&#32032;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#65288;MEMD-ABSA&#65289;&#65292;&#29992;&#20110;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20116;&#20010;&#39046;&#22495;&#30340;&#22235;&#20010;&#35201;&#32032;&#65292;&#21253;&#25324;&#36817;2&#19975;&#20010;&#35780;&#35770;&#21477;&#23376;&#21644;3&#19975;&#20010;&#24102;&#26377;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#38754;&#21644;&#35266;&#28857;&#30340;&#22235;&#20803;&#32452;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#25918;&#39046;&#22495;ABSA&#20197;&#21450;&#25366;&#25496;&#38544;&#21547;&#30340;&#26041;&#38754;&#21644;&#35266;&#28857;&#20173;&#28982;&#26159;&#24453;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.16956</link><description>&lt;p&gt;
MEMD-ABSA&#65306;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#22810;&#35201;&#32032;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MEMD-ABSA: A Multi-Element Multi-Domain Dataset for Aspect-Based Sentiment Analysis. (arXiv:2306.16956v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16956
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35201;&#32032;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#65288;MEMD-ABSA&#65289;&#65292;&#29992;&#20110;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20116;&#20010;&#39046;&#22495;&#30340;&#22235;&#20010;&#35201;&#32032;&#65292;&#21253;&#25324;&#36817;2&#19975;&#20010;&#35780;&#35770;&#21477;&#23376;&#21644;3&#19975;&#20010;&#24102;&#26377;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#38754;&#21644;&#35266;&#28857;&#30340;&#22235;&#20803;&#32452;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#25918;&#39046;&#22495;ABSA&#20197;&#21450;&#25366;&#25496;&#38544;&#21547;&#30340;&#26041;&#38754;&#21644;&#35266;&#28857;&#20173;&#28982;&#26159;&#24453;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#25366;&#25496;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#36880;&#28176;&#23558;&#28966;&#28857;&#20174;&#31616;&#21333;&#30340;ABSA&#23376;&#20219;&#21153;&#36716;&#21521;&#31471;&#21040;&#31471;&#30340;&#22810;&#35201;&#32032;ABSA&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#23616;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#20010;&#21035;&#35201;&#32032;&#65292;&#36890;&#24120;&#20851;&#27880;&#20110;&#39046;&#22495;&#20869;&#35774;&#32622;&#65292;&#24573;&#30053;&#20102;&#38544;&#21547;&#30340;&#26041;&#38754;&#21644;&#35266;&#28857;&#65292;&#24182;&#19988;&#25968;&#25454;&#35268;&#27169;&#36739;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35201;&#32032;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;(MEMD)&#65292;&#28085;&#30422;&#20102;&#20116;&#20010;&#39046;&#22495;&#30340;&#22235;&#20010;&#35201;&#32032;&#65292;&#21253;&#25324;&#36817;2&#19975;&#20010;&#35780;&#35770;&#21477;&#23376;&#21644;3&#19975;&#20010;&#24102;&#26377;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#38754;&#21644;&#35266;&#28857;&#30340;&#22235;&#20803;&#32452;&#65292;&#21487;&#29992;&#20110;ABSA&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#22312;&#24320;&#25918;&#39046;&#22495;&#35774;&#32622;&#19979;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#21644;&#38750;&#29983;&#25104;&#24335;&#22522;&#32447;&#27169;&#22411;&#22312;&#22810;&#20010;ABSA&#23376;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#25918;&#39046;&#22495;ABSA&#20197;&#21450;&#25366;&#25496;&#38544;&#21547;&#30340;&#26041;&#38754;&#21644;&#35266;&#28857;&#20173;&#28982;&#26159;&#24453;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis is a long-standing research interest in the field of opinion mining, and in recent years, researchers have gradually shifted their focus from simple ABSA subtasks to end-to-end multi-element ABSA tasks. However, the datasets currently used in the research are limited to individual elements of specific tasks, usually focusing on in-domain settings, ignoring implicit aspects and opinions, and with a small data scale. To address these issues, we propose a large-scale Multi-Element Multi-Domain dataset (MEMD) that covers the four elements across five domains, including nearly 20,000 review sentences and 30,000 quadruples annotated with explicit and implicit aspects and opinions for ABSA research. Meanwhile, we evaluate generative and non-generative baselines on multiple ABSA subtasks under the open domain setting, and the results show that open domain ABSA as well as mining implicit aspects and opinions remain ongoing challenges to be addressed. The datasets
&lt;/p&gt;</description></item><item><title>&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#29305;&#24449;&#20449;&#24687;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21487;&#20197;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.16950</link><description>&lt;p&gt;
&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Alternative Telescopic Displacement: An Efficient Multimodal Alignment Method. (arXiv:2306.16950v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16950
&lt;/p&gt;
&lt;p&gt;
&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#29305;&#24449;&#20449;&#24687;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21487;&#20197;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23545;&#40784;&#26159;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#21487;&#20197;&#23436;&#20840;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#26469;&#23454;&#29616;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#19968;&#33268;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;&#23545;ETT&#21644;MIT-BIH-Arrhythmia&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature alignment is the primary means of fusing multimodal data. We propose a feature alignment method that fully fuses multimodal information, which alternately shifts and expands feature information from different modalities to have a consistent representation in a feature space. The proposed method can robustly capture high-level interactions between features of different modalities, thus significantly improving the performance of multimodal learning. We also show that the proposed method outperforms other popular multimodal schemes on multiple tasks. Experimental evaluation of ETT and MIT-BIH-Arrhythmia, datasets shows that the proposed method achieves state of the art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;UMASS_BioNLP&#22242;&#38431;&#22312;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#21442;&#19982;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;LLMs&#21327;&#20316;&#31995;&#32479;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;ChatGPT&#21644;GPT-4&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.16931</link><description>&lt;p&gt;
UMASS_BioNLP&#21442;&#21152;MEDIQA-Chat 2023&#65306;LLMs&#33021;&#21542;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21307;&#29983;-&#24739;&#32773;&#22522;&#20110;&#31508;&#35760;&#30340;&#23545;&#35805;&#65311;
&lt;/p&gt;
&lt;p&gt;
UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?. (arXiv:2306.16931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;UMASS_BioNLP&#22242;&#38431;&#22312;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#21442;&#19982;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;LLMs&#21327;&#20316;&#31995;&#32479;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;ChatGPT&#21644;GPT-4&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;UMASS_BioNLP&#22242;&#38431;&#21442;&#19982;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#30340;Task-A&#21644;Task-C&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;Task-C&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21307;&#29983;-&#24739;&#32773;&#24490;&#29615;&#30340;&#26032;&#22411;LLMs&#21327;&#20316;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ROUGE&#12289;&#21307;&#30103;&#27010;&#24565;&#21484;&#22238;&#29575;&#12289;BLEU&#21644;Self-BLEU&#31561;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19979;&#34920;&#29616;&#21512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;ChatGPT&#21644;GPT-4&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#21033;&#29992;&#21327;&#20316;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023 shared task for Task-A and Task-C. We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-quality conversation data sets. The experiment results demonstrate that our approaches yield reasonable performance as evaluated by automatic metrics such as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, we conducted a comparative analysis between our proposed method and ChatGPT and GPT-4. This analysis also investigates the potential of utilizing cooperation LLMs to generate high-quality datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36827;&#34892;&#24418;&#29366;&#20248;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;45&#31186;&#20869;&#23558;&#20219;&#24847;&#21333;&#24352;&#22270;&#20687;&#36716;&#25442;&#20026;360&#24230;&#30340;3D&#32441;&#29702;&#32593;&#26684;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#35270;&#22270;&#26465;&#20214;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#21644;&#22522;&#20110;SDF&#30340;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#38190;&#30340;&#35757;&#32451;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#19968;&#33268;&#30340;&#22810;&#35270;&#22270;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.16928</link><description>&lt;p&gt;
One-2-3-45: &#22312;45&#31186;&#20869;&#23558;&#20219;&#24847;&#21333;&#24352;&#22270;&#20687;&#36716;&#25442;&#20026;3D&#32593;&#26684;&#65292;&#26080;&#38656;&#36827;&#34892;&#24418;&#29366;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization. (arXiv:2306.16928v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36827;&#34892;&#24418;&#29366;&#20248;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;45&#31186;&#20869;&#23558;&#20219;&#24847;&#21333;&#24352;&#22270;&#20687;&#36716;&#25442;&#20026;360&#24230;&#30340;3D&#32441;&#29702;&#32593;&#26684;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#35270;&#22270;&#26465;&#20214;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#21644;&#22522;&#20110;SDF&#30340;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#38190;&#30340;&#35757;&#32451;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#19968;&#33268;&#30340;&#22810;&#35270;&#22270;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#22270;&#20687;3D&#37325;&#24314;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#23545;&#33258;&#28982;&#19990;&#30028;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22312;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#19979;&#20248;&#21270;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23384;&#22312;&#20248;&#21270;&#26102;&#38388;&#38271;&#12289;3D&#32467;&#26524;&#19981;&#19968;&#33268;&#21644;&#20960;&#20309;&#36136;&#37327;&#24046;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#20197;&#20219;&#24847;&#29289;&#20307;&#30340;&#21333;&#24352;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#22312;&#21333;&#27425;&#21069;&#39304;&#20256;&#36882;&#20013;&#29983;&#25104;&#19968;&#20010;&#23436;&#25972;&#30340;360&#24230;3D&#32441;&#29702;&#32593;&#26684;&#12290;&#32473;&#23450;&#19968;&#24352;&#22270;&#20687;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#19968;&#20010;&#35270;&#22270;&#26465;&#20214;&#30340;2D&#25193;&#25955;&#27169;&#22411;Zero123&#20026;&#36755;&#20837;&#35270;&#22270;&#29983;&#25104;&#22810;&#35270;&#22270;&#22270;&#20687;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#25552;&#21319;&#21040;3D&#31354;&#38388;&#12290;&#30001;&#20110;&#20256;&#32479;&#37325;&#24314;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#19981;&#19968;&#33268;&#30340;&#22810;&#35270;&#22270;&#39044;&#27979;&#65292;&#25105;&#20204;&#22522;&#20110;&#22522;&#20110;SDF&#30340;&#36890;&#29992;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26041;&#27861;&#26500;&#24314;&#20102;&#25105;&#20204;&#30340;3D&#37325;&#24314;&#27169;&#22359;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#20851;&#38190;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;360&#24230;&#32593;&#26684;&#30340;&#37325;&#24314;&#12290;&#26080;&#38656;&#32791;&#26102;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360-degree meshes. Without costly optimizat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16927</link><description>&lt;p&gt;
&#32447;&#26463;&#33258;&#21160;&#39550;&#39542;&#65306;&#25361;&#25112;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
End-to-end Autonomous Driving: Challenges and Frontiers. (arXiv:2306.16927v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#37319;&#29992;&#31471;&#21040;&#31471;&#31639;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22987;&#20256;&#24863;&#22120;&#36755;&#20837;&#29983;&#25104;&#36710;&#36742;&#36816;&#21160;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#35832;&#22914;&#26816;&#27979;&#21644;&#36816;&#21160;&#39044;&#27979;&#31561;&#21333;&#20010;&#20219;&#21153;&#12290;&#19982;&#27169;&#22359;&#21270;&#27969;&#27700;&#32447;&#30456;&#27604;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#26469;&#33719;&#30410;&#12290;&#36825;&#19968;&#39046;&#22495;&#22240;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12289;&#38381;&#29615;&#35780;&#20272;&#20197;&#21450;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#25191;&#34892;&#25152;&#38656;&#30340;&#38656;&#27714;&#32780;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;250&#22810;&#31687;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#21160;&#26426;&#12289;&#36335;&#32447;&#22270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#30784;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 250 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation
&lt;/p&gt;</description></item><item><title>ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16922</link><description>&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#65306;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks. (arXiv:2306.16922v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16922
&lt;/p&gt;
&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#31185;&#23398;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#31616;&#21270;&#30340;&#20010;&#20307;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#20381;&#38752;&#38598;&#20307;&#27963;&#21160;&#21644;&#36866;&#24403;&#35843;&#25972;&#30340;&#36830;&#25509;&#26469;&#25191;&#34892;&#22797;&#26434;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#29983;&#29289;&#30382;&#23618;&#31070;&#32463;&#20803;&#26412;&#36136;&#19978;&#37117;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#35774;&#22791;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#65292;&#35813;&#30740;&#31350;&#20013;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#30340;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#22797;&#21046;&#35814;&#32454;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#22810;&#20010;&#21442;&#25968;&#30340;&#24517;&#35201;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#24341;&#20837;&#20102;&#34920;&#36798;&#21147;&#24378;&#30340;&#27844;&#28431;&#23384;&#20648;&#22120;&#65288;ELM&#65289;&#31070;&#32463;&#20803;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#35745;&#31639;&#34920;&#36798;&#21147;&#65292;&#21516;&#26102;&#20063;&#38750;&#24120;&#39640;&#25928;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ELM&#31070;&#32463;&#20803;&#20165;&#38656;&#35201;8,000&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#21305;&#37197;&#21069;&#36848;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20934;&#30830;&#30340;&#27169;&#22411;&#38656;&#35201;&#22810;&#20010;&#31867;&#20284;&#20110;&#23384;&#20648;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#21644;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#31361;&#35302;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional large-scale neuroscience models and machine learning utilize simplified models of individual neurons, relying on collective activity and properly adjusted connections to perform complex computations. However, each biological cortical neuron is inherently a sophisticated computational device, as corroborated in a recent study where it took a deep artificial neural network with millions of parameters to replicate the input-output relationship of a detailed biophysical model of a cortical pyramidal neuron. We question the necessity for these many parameters and introduce the Expressive Leaky Memory (ELM) neuron, a biologically inspired, computationally expressive, yet efficient model of a cortical neuron. Remarkably, our ELM neuron requires only 8K trainable parameters to match the aforementioned input-output relationship accurately. We find that an accurate model necessitates multiple memory-like hidden states and intricate nonlinear synaptic integration. To assess the comput
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;&#26694;&#26550;FlaSH&#65292;&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#12290;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#25968;&#25454;&#37327;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16914</link><description>&lt;p&gt;
&#35745;&#31639;&#36741;&#21161;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#30340;&#36136;&#37327;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Computationally Assisted Quality Control for Public Health Data Streams. (arXiv:2306.16914v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16914
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;&#26694;&#26550;FlaSH&#65292;&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#12290;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#25968;&#25454;&#37327;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#65288;&#22914;COVID-19&#30149;&#20363;&#65289;&#38459;&#30861;&#20102;&#20844;&#20849;&#21355;&#29983;&#21033;&#30410;&#30456;&#20851;&#32773;&#22522;&#20110;&#25968;&#25454;&#30340;&#20915;&#31574;&#12290;&#23454;&#26102;&#29983;&#25104;&#30340;&#35745;&#31639;&#26426;&#21015;&#34920;&#21487;&#20197;&#24110;&#21161;&#19987;&#23478;&#35780;&#23457;&#21592;&#35782;&#21035;&#25104;&#21315;&#19978;&#19975;&#20010;&#27599;&#26085;&#26356;&#26032;&#30340;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#20013;&#26368;&#37325;&#35201;&#30340;&#24322;&#24120;&#25968;&#25454;&#28857;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;&#26694;&#26550;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#25968;&#25454;&#37327;&#25110;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;FlaSH&#65288;&#26071;&#26631;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#27969;&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;&#20844;&#20849;&#21355;&#29983;&#25968;&#25454;&#29992;&#25143;&#29992;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#26126;&#30830;&#25429;&#25417;&#36825;&#20123;&#32479;&#35745;&#29305;&#24615;&#12290;&#22312;&#19968;&#20010;&#23454;&#39564;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#35780;&#20272;&#20102;FlaSH&#21644;&#29616;&#26377;&#26041;&#27861;&#65288;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65289;&#65292;FlaSH&#36866;&#24212;&#20102;&#35813;&#20219;&#21153;&#30340;&#25968;&#25454;&#37327;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#22312;&#24179;&#22343;&#20934;&#30830;&#24230;&#19978;&#36798;&#21040;&#25110;&#36229;&#36807;&#65292;&#24182;&#19988;&#21487;&#20197;&#35782;&#21035;&#20986;&#24322;&#24120;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Irregularities in public health data streams (like COVID-19 Cases) hamper data-driven decision-making for public health stakeholders. A real-time, computer-generated list of the most important, outlying data points from thousands of daily-updated public health data streams could assist an expert reviewer in identifying these irregularities. However, existing outlier detection frameworks perform poorly on this task because they do not account for the data volume or for the statistical properties of public health streams. Accordingly, we developed FlaSH (Flagging Streams in public Health), a practical outlier detection framework for public health data users that uses simple, scalable models to capture these statistical properties explicitly. In an experiment where human experts evaluate FlaSH and existing methods (including deep learning approaches), FlaSH scales to the data volume of this task, matches or exceeds these other methods in mean accuracy, and identifies the outlier points th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Caml&#65292;&#19968;&#31181;&#22312;&#20005;&#26684;&#32422;&#26463;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#20803;&#23398;&#20064;&#30340;AutoML&#26041;&#27861;&#12290;Caml&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;AutoML&#21442;&#25968;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#29983;&#25104;&#28385;&#36275;&#32422;&#26463;&#19988;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.16913</link><description>&lt;p&gt;
&#20005;&#26684;&#32422;&#26463;&#24212;&#29992;&#20013;&#30340;AutoML
&lt;/p&gt;
&lt;p&gt;
AutoML in Heavily Constrained Applications. (arXiv:2306.16913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Caml&#65292;&#19968;&#31181;&#22312;&#20005;&#26684;&#32422;&#26463;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#20803;&#23398;&#20064;&#30340;AutoML&#26041;&#27861;&#12290;Caml&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;AutoML&#21442;&#25968;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#29983;&#25104;&#28385;&#36275;&#32422;&#26463;&#19988;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#38656;&#35201;&#23545;&#21508;&#31181;&#36229;&#21442;&#25968;&#36827;&#34892;&#20180;&#32454;&#37197;&#32622;&#65292;&#36890;&#24120;&#30001;AutoML&#31995;&#32479;&#25903;&#25345;&#65292;&#35813;&#31995;&#32479;&#20248;&#21270;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36229;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;AutoML&#31995;&#32479;&#30340;&#20108;&#38454;&#20803;&#37197;&#32622;&#65292;AutoML&#36807;&#31243;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#30446;&#21069;&#30340;AutoML&#31995;&#32479;&#26080;&#27861;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#29992;&#20363;&#30340;&#37197;&#32622;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20063;&#26080;&#27861;&#32534;&#35793;&#29992;&#25143;&#23450;&#20041;&#30340;&#24212;&#29992;&#32422;&#26463;&#65292;&#20197;&#30830;&#20445;&#27969;&#31243;&#21450;&#20854;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Caml&#65292;&#23427;&#20351;&#29992;&#20803;&#23398;&#20064;&#33258;&#21160;&#36866;&#24212;&#20854;&#33258;&#36523;&#30340;AutoML&#21442;&#25968;&#65292;&#27604;&#22914;&#25628;&#32034;&#31574;&#30053;&#12289;&#39564;&#35777;&#31574;&#30053;&#21644;&#25628;&#32034;&#31354;&#38388;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;Caml&#30340;&#21160;&#24577;AutoML&#31574;&#30053;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#24182;&#33719;&#24471;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#28385;&#36275;&#32422;&#26463;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing a machine learning pipeline for a task at hand requires careful configuration of various hyperparameters, typically supported by an AutoML system that optimizes the hyperparameters for the given training dataset. Yet, depending on the AutoML system's own second-order meta-configuration, the performance of the AutoML process can vary significantly. Current AutoML systems cannot automatically adapt their own configuration to a specific use case. Further, they cannot compile user-defined application constraints on the effectiveness and efficiency of the pipeline and its generation. In this paper, we propose Caml, which uses meta-learning to automatically adapt its own AutoML parameters, such as the search strategy, the validation strategy, and the search space, for a task at hand. The dynamic AutoML strategy of Caml takes user-defined constraints into account and obtains constraint-satisfying pipelines with high predictive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#26524;&#20998;&#26512;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#32423;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#20998;&#26512;&#23458;&#35266;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.16902</link><description>&lt;p&gt;
&#20174;&#26597;&#35810;&#24037;&#20855;&#21040;&#22240;&#26524;&#26550;&#26500;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#32423;&#22240;&#26524;&#21457;&#29616;&#21644;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data. (arXiv:2306.16902v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#26524;&#20998;&#26512;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#32423;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#20998;&#26512;&#23458;&#35266;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#23398;&#12289;&#31185;&#23398;&#21644;&#27861;&#24459;&#31561;&#22810;&#20010;&#37325;&#35201;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#22312;&#27010;&#24565;&#38388;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26368;&#36817;&#23545;LLM&#22312;&#21508;&#31181;&#22240;&#26524;&#21457;&#29616;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#30340;&#30740;&#31350;&#24050;&#32463;&#20026;&#32463;&#20856;&#30340;&#19977;&#38454;&#27573;&#22240;&#26524;&#26694;&#26550;&#24102;&#26469;&#20102;&#19968;&#20010;&#26032;&#30340;&#38454;&#26799;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#23558;&#22522;&#20110;&#30693;&#35782;&#30340;LLM&#22240;&#26524;&#20998;&#26512;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#25512;&#36827;&#20102;&#30446;&#21069;&#22522;&#20110;LLM&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#20351;LLM&#19981;&#21482;&#26159;&#19968;&#20010;&#26597;&#35810;&#24037;&#20855;&#65292;&#20805;&#20998;&#21033;&#29992;&#20854;&#22312;&#21457;&#29616;&#33258;&#28982;&#21644;&#26032;&#30340;&#22240;&#26524;&#23450;&#24459;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;LLM&#23545;&#29616;&#26377;&#22240;&#26524;&#26426;&#21046;&#30340;&#23453;&#36149;&#19987;&#19994;&#30693;&#35782;&#34701;&#20837;&#23458;&#35266;&#25968;&#25454;&#30340;&#32479;&#35745;&#20998;&#26512;&#20013;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#36890;&#29992;&#30340;&#25552;&#31034;&#65292;&#26088;&#22312;&#20174;&#32473;&#23450;&#21464;&#37327;&#20013;&#25552;&#21462;&#22240;&#26524;&#22270;&#65292;&#24182;&#35780;&#20272;LLM&#20043;&#21069;&#22240;&#26524;&#24615;&#23545;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) exhibit exceptional abilities for causal analysis between concepts in numerous societally impactful domains, including medicine, science, and law. Recent research on LLM performance in various causal discovery and inference tasks has given rise to a new ladder in the classical three-stage framework of causality. In this paper, we advance the current research of LLM-driven causal discovery by proposing a novel framework that combines knowledge-based LLM causal analysis with data-driven causal structure learning. To make LLM more than a query tool and to leverage its power in discovering natural and new laws of causality, we integrate the valuable LLM expertise on existing causal mechanisms into statistical analysis of objective data to build a novel and practical baseline for causal structure learning.  We introduce a universal set of prompts designed to extract causal graphs from given variables and assess the influence of LLM prior causality on recovering 
&lt;/p&gt;</description></item><item><title>PFB-Diff &#26159;&#19968;&#20010;&#36890;&#36807;&#28176;&#36827;&#29305;&#24449;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#20687;&#32032;&#32423;&#28151;&#21512;&#20013;&#20135;&#29983;&#30340;&#20266;&#24433;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#28151;&#21512;&#21644;&#27880;&#24847;&#21147;&#23631;&#34109;&#26426;&#21046;&#30830;&#20445;&#20102;&#32534;&#36753;&#22270;&#20687;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#39640;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.16894</link><description>&lt;p&gt;
PFB-Diff: &#28176;&#36827;&#29305;&#24449;&#28151;&#21512;&#25193;&#25955;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PFB-Diff: Progressive Feature Blending Diffusion for Text-driven Image Editing. (arXiv:2306.16894v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16894
&lt;/p&gt;
&lt;p&gt;
PFB-Diff &#26159;&#19968;&#20010;&#36890;&#36807;&#28176;&#36827;&#29305;&#24449;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#20687;&#32032;&#32423;&#28151;&#21512;&#20013;&#20135;&#29983;&#30340;&#20266;&#24433;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#28151;&#21512;&#21644;&#27880;&#24847;&#21147;&#23631;&#34109;&#26426;&#21046;&#30830;&#20445;&#20102;&#32534;&#36753;&#22270;&#20687;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20854;&#21512;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#32534;&#36753;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#23616;&#37096;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#24120;&#24120;&#22240;&#20026;&#30446;&#26631;&#22270;&#20687;&#21644;&#25193;&#25955;&#28508;&#22312;&#21464;&#37327;&#30340;&#20687;&#32032;&#32423;&#28151;&#21512;&#32780;&#20135;&#29983;&#19981;&#26399;&#26395;&#30340;&#20266;&#24433;&#65292;&#32570;&#20047;&#32500;&#25345;&#22270;&#20687;&#19968;&#33268;&#24615;&#25152;&#24517;&#38656;&#30340;&#35821;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PFB-Diff&#65292;&#19968;&#31181;&#36880;&#27493;&#29305;&#24449;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;PFB-Diff&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#28151;&#21512;&#23558;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#30446;&#26631;&#22270;&#20687;&#26080;&#32541;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#28145;&#23618;&#29305;&#24449;&#20013;&#32534;&#30721;&#30340;&#20016;&#23500;&#35821;&#20041;&#21644;&#20174;&#39640;&#21040;&#20302;&#32423;&#21035;&#30340;&#28176;&#36827;&#28151;&#21512;&#26041;&#26696;&#30830;&#20445;&#20102;&#32534;&#36753;&#22270;&#20687;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#39640;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#23631;&#34109;&#26426;&#21046;&#65292;&#20197;&#38480;&#21046;&#29305;&#23450;&#35789;&#35821;&#23545;&#32534;&#36753;&#22270;&#20687;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have showcased their remarkable capability to synthesize diverse and high-quality images, sparking interest in their application for real image editing. However, existing diffusion-based approaches for local image editing often suffer from undesired artifacts due to the pixel-level blending of the noised target images and diffusion latent variables, which lack the necessary semantics for maintaining image consistency. To address these issues, we propose PFB-Diff, a Progressive Feature Blending method for Diffusion-based image editing. Unlike previous methods, PFB-Diff seamlessly integrates text-guided generated content into the target image through multi-level feature blending. The rich semantics encoded in deep features and the progressive blending scheme from high to low levels ensure semantic coherence and high quality in edited images. Additionally, we introduce an attention masking mechanism in the cross-attention layers to confine the impact of specific words to 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30151;&#29366;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#26032;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#39640;&#36798;97%&#12290;&#36825;&#34920;&#26126;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26159;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#31579;&#26597;&#30340;&#19968;&#20010;&#37325;&#35201;&#36164;&#28304;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#36825;&#19968;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16891</link><description>&lt;p&gt;
&#21033;&#29992;Hugging Face Transformers&#39044;&#27979;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Hugging Face Transformers for Predicting Mental Health Disorders in Social Networks. (arXiv:2306.16891v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30151;&#29366;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#26032;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#39640;&#36798;97%&#12290;&#36825;&#34920;&#26126;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26159;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#31579;&#26597;&#30340;&#19968;&#20010;&#37325;&#35201;&#36164;&#28304;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#36825;&#19968;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#35786;&#26029;&#31934;&#31070;&#38556;&#30861;&#24182;&#36827;&#34892;&#24178;&#39044;&#21487;&#20197;&#20419;&#36827;&#39044;&#38450;&#20005;&#37325;&#20260;&#23475;&#21644;&#25913;&#21892;&#27835;&#30103;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#35752;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#22914;&#20309;&#29992;&#20110;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30151;&#29366;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#27604;&#36739;&#20102;Hugging Face&#30340;&#22235;&#31181;&#19981;&#21516;BERT&#27169;&#22411;&#21644;&#36817;&#26399;&#25991;&#29486;&#20013;&#29992;&#20110;&#33258;&#21160;&#25233;&#37057;&#30151;&#35786;&#26029;&#30340;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26032;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;97%&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#34917;&#20805;&#20808;&#21069;&#30340;&#21457;&#29616;&#65292;&#23545;&#32467;&#26524;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#26159;&#24494;&#23567;&#30340;&#25968;&#25454;&#37327;&#65288;&#22914;&#29992;&#25143;&#30340;&#20010;&#20154;&#31616;&#20171;&#25551;&#36848;&#65289;&#20063;&#26377;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26159;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#31579;&#26597;&#30340;&#19968;&#20010;&#26497;&#22909;&#30340;&#26469;&#28304;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#33258;&#21160;&#21270;&#36825;&#19968;&#20851;&#38190;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of mental disorders and intervention can facilitate the prevention of severe injuries and the improvement of treatment results. Using social media and pre-trained language models, this study explores how user-generated data can be used to predict mental disorder symptoms. Our study compares four different BERT models of Hugging Face with standard machine learning techniques used in automatic depression diagnosis in recent literature. The results show that new models outperform the previous approach with an accuracy rate of up to 97%. Analyzing the results while complementing past findings, we find that even tiny amounts of data (like users' bio descriptions) have the potential to predict mental disorders. We conclude that social media data is an excellent source of mental health screening, and pre-trained models can effectively automate this critical task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#22797;&#21046;&#22270;&#28789;&#26426;&#30340;&#26500;&#36896;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31616;&#21333;&#30340;&#26500;&#24314;&#22359;&#24320;&#22987;&#65292;&#24182;&#20351;&#29992;&#26368;&#23567;&#30340;&#20551;&#35774;&#21644;&#30456;&#21516;&#30340;&#21407;&#29702;&#65292;&#25552;&#20379;&#20102;von Neumann&#30340;&#36890;&#29992;&#26500;&#36896;&#22120;&#21644;&#36890;&#29992;&#22797;&#21046;&#22120;&#30340;&#37096;&#20998;&#23454;&#29616;&#12290;&#35813;&#26500;&#36896;&#20801;&#35768;&#36827;&#34892;&#21464;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25551;&#36848;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2306.16872</link><description>&lt;p&gt;
&#36808;&#21521;&#33258;&#22797;&#21046;&#22270;&#28789;&#26426;
&lt;/p&gt;
&lt;p&gt;
Towards a Self-Replicating Turing Machine. (arXiv:2306.16872v1 [cs.FL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16872
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#22797;&#21046;&#22270;&#28789;&#26426;&#30340;&#26500;&#36896;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31616;&#21333;&#30340;&#26500;&#24314;&#22359;&#24320;&#22987;&#65292;&#24182;&#20351;&#29992;&#26368;&#23567;&#30340;&#20551;&#35774;&#21644;&#30456;&#21516;&#30340;&#21407;&#29702;&#65292;&#25552;&#20379;&#20102;von Neumann&#30340;&#36890;&#29992;&#26500;&#36896;&#22120;&#21644;&#36890;&#29992;&#22797;&#21046;&#22120;&#30340;&#37096;&#20998;&#23454;&#29616;&#12290;&#35813;&#26500;&#36896;&#20801;&#35768;&#36827;&#34892;&#21464;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25551;&#36848;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;von Neumann&#30340;&#36890;&#29992;&#26500;&#36896;&#22120;&#21644;&#36890;&#29992;&#22797;&#21046;&#22120;&#30340;&#37096;&#20998;&#23454;&#29616;&#65292;&#20174;&#19977;&#31181;&#31616;&#21333;&#30340;&#26500;&#24314;&#22359;&#24320;&#22987;&#65292;&#20351;&#29992;&#26368;&#23567;&#30340;&#20551;&#35774;&#12290;&#20351;&#29992;&#30456;&#21516;&#30340;&#21407;&#29702;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#22270;&#28789;&#26426;&#12290;&#23558;&#20004;&#32773;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#22797;&#21046;&#30340;&#22270;&#28789;&#26426;&#30340;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#20801;&#35768;&#36827;&#34892;&#21464;&#24322;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#25551;&#36848;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide partial implementations of von Neumann's universal constructor and universal copier, starting out with three types of simple building blocks using minimal assumptions. Using the same principles, we also construct Turing machines. Combining both, we arrive at a proposal for a self-replicating Turing machine. Our construction allows for mutations if desired, and we give a simple description language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25345;&#32493;&#30340;&#26837;&#27016;&#26641;&#31181;&#26893;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#32852;&#32593;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#23545;&#32418;&#26837;&#35937;&#36827;&#34892;&#26089;&#26399;&#26816;&#27979;&#21644;&#26144;&#23556;&#12290;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#28145;&#24230;&#23398;&#20064;&#12289;&#29289;&#32852;&#32593;&#21644;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#30340;&#32508;&#21512;&#24212;&#29992;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#21644;&#20998;&#31867;&#23492;&#29983;&#32418;&#26837;&#35937;&#30340;&#26837;&#27016;&#26641;&#65292;&#24182;&#21033;&#29992;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#21019;&#24314;&#32508;&#21512;&#32418;&#26837;&#35937;&#20998;&#24067;&#22270;&#65292;&#20026;&#39640;&#25928;&#30417;&#27979;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.16862</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#26837;&#27016;&#26641;&#31181;&#26893;&#65306;&#21033;&#29992;&#29289;&#32852;&#32593;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#32418;&#26837;&#35937;&#26089;&#26399;&#26816;&#27979;&#21644;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Sustainable Palm Tree Farming: Leveraging IoT and Multi-Modal Data for Early Detection and Mapping of Red Palm Weevil. (arXiv:2306.16862v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25345;&#32493;&#30340;&#26837;&#27016;&#26641;&#31181;&#26893;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#32852;&#32593;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#23545;&#32418;&#26837;&#35937;&#36827;&#34892;&#26089;&#26399;&#26816;&#27979;&#21644;&#26144;&#23556;&#12290;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#28145;&#24230;&#23398;&#20064;&#12289;&#29289;&#32852;&#32593;&#21644;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#30340;&#32508;&#21512;&#24212;&#29992;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#21644;&#20998;&#31867;&#23492;&#29983;&#32418;&#26837;&#35937;&#30340;&#26837;&#27016;&#26641;&#65292;&#24182;&#21033;&#29992;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#21019;&#24314;&#32508;&#21512;&#32418;&#26837;&#35937;&#20998;&#24067;&#22270;&#65292;&#20026;&#39640;&#25928;&#30417;&#27979;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#26837;&#35937;&#26159;&#19968;&#31181;&#20855;&#26377;&#30772;&#22351;&#24615;&#30340;&#26118;&#34411;&#65292;&#23545;&#20840;&#29699;&#26837;&#27016;&#26641;&#31181;&#26893;&#19994;&#36896;&#25104;&#32463;&#27982;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#36827;&#25216;&#26415;&#23545;&#32418;&#26837;&#35937;&#30340;&#26089;&#26399;&#26816;&#27979;&#21644;&#31649;&#29702;&#65292;&#23454;&#29616;&#21487;&#25345;&#32493;&#26837;&#27016;&#26641;&#31181;&#26893;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#28145;&#24230;&#23398;&#20064;&#12289;&#29289;&#32852;&#32593;&#21644;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21644;&#20998;&#31867;&#23492;&#29983;&#32418;&#26837;&#35937;&#30340;&#26837;&#27016;&#26641;&#12290;&#20027;&#35201;&#38454;&#27573;&#21253;&#25324;&#65306;&#65288;1&#65289;&#20351;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#22768;&#38899;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#65292;&#65288;2&#65289;&#20351;&#29992;&#26080;&#20154;&#26426;&#22270;&#20687;&#20013;&#30340;YOLOv8&#36827;&#34892;&#26837;&#27016;&#26641;&#26816;&#27979;&#65292;&#65288;3&#65289;&#20351;&#29992;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;&#32418;&#26837;&#35937;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26816;&#27979;&#21644;&#23450;&#20301;&#23492;&#29983;&#26837;&#27016;&#26641;&#26041;&#38754;&#36798;&#21040;100%&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;&#25972;&#21512;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#21487;&#20197;&#21019;&#24314;&#32508;&#21512;&#32418;&#26837;&#35937;&#20998;&#24067;&#22270;&#65292;&#23454;&#29616;&#39640;&#25928;&#30417;&#27979;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#31649;&#29702;&#31574;&#30053;&#12290;&#36825;&#31181;&#25216;&#26415;&#39537;&#21160;&#30340;&#26041;&#27861;&#26377;&#30410;&#20110;&#20892;&#19994;&#37096;&#38376;&#12289;&#20892;&#27665;&#21644;&#30740;&#31350;&#20154;&#21592;&#31649;&#29702;&#32418;&#26837;&#35937;&#20405;&#34989;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Red Palm Weevil (RPW) is a highly destructive insect causing economic losses and impacting palm tree farming worldwide. This paper proposes an innovative approach for sustainable palm tree farming by utilizing advanced technologies for the early detection and management of RPW. Our approach combines computer vision, deep learning (DL), the Internet of Things (IoT), and geospatial data to detect and classify RPW-infested palm trees effectively. The main phases include; (1) DL classification using sound data from IoT devices, (2) palm tree detection using YOLOv8 on UAV images, and (3) RPW mapping using geospatial data. Our custom DL model achieves 100% precision and recall in detecting and localizing infested palm trees. Integrating geospatial data enables the creation of a comprehensive RPW distribution map for efficient monitoring and targeted management strategies. This technology-driven approach benefits agricultural authorities, farmers, and researchers in managing RPW infestati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#25991;&#23398;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#28909;&#28857;&#65292;&#23545;&#21508;&#31181;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#20102;&#21508;&#20010;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.16834</link><description>&lt;p&gt;
&#22825;&#25991;&#20809;&#23398;&#26395;&#36828;&#38236;&#30340;&#26234;&#33021;&#21270;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Intelligence of Astronomical Optical Telescope: Present Status and Future Perspectives. (arXiv:2306.16834v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#25991;&#23398;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#28909;&#28857;&#65292;&#23545;&#21508;&#31181;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#20102;&#21508;&#20010;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#25991;&#23398;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#19981;&#26029;&#28044;&#29616;&#20986;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;&#30446;&#21069;&#65292;&#26377;&#22823;&#37327;&#30340;&#35770;&#25991;&#22238;&#39038;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#25991;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#28982;&#32780;&#24456;&#23569;&#26377;&#30456;&#20851;&#25991;&#31456;&#21333;&#29420;&#25552;&#21450;&#26395;&#36828;&#38236;&#30340;&#26234;&#33021;&#21270;&#65292;&#24182;&#19988;&#24456;&#38590;&#20174;&#36825;&#20123;&#35770;&#25991;&#20013;&#20102;&#35299;&#21040;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#24403;&#21069;&#21457;&#23637;&#29366;&#20917;&#21644;&#30740;&#31350;&#28909;&#28857;&#12290;&#26412;&#25991;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21457;&#23637;&#21382;&#21490;&#21644;&#26395;&#36828;&#38236;&#20851;&#38190;&#25216;&#26415;&#30340;&#22256;&#38590;&#65292;&#20840;&#38754;&#20171;&#32461;&#20102;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#28909;&#28857;&#65292;&#28982;&#21518;&#23545;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#21508;&#31181;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#23450;&#20041;&#20102;&#21508;&#20010;&#30740;&#31350;&#26041;&#21521;&#30340;&#20248;&#28857;&#12290;&#35780;&#20272;&#20102;&#21508;&#31181;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25351;&#20986;&#20102;&#27599;&#20010;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence technology has been widely used in astronomy, and new artificial intelligence technologies and application scenarios are constantly emerging. There have been a large number of papers reviewing the application of artificial intelligence technology in astronomy. However, relevant articles seldom mention telescope intelligence separately, and it is difficult to understand the current development status and research hotspots of telescope intelligence from these papers. This paper combines the development history of artificial intelligence technology and the difficulties of critical technologies of telescopes, comprehensively introduces the development and research hotspots of telescope intelligence, then conducts statistical analysis on various research directions of telescope intelligence and defines the research directions' merits. All kinds of research directions are evaluated, and the research trend of each telescope's intelligence is pointed out. Finally, accor
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#22270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#20102;&#19968;&#32452;&#22810;&#26679;&#30340;&#20505;&#36873;&#24207;&#21015;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#27010;&#25324;&#22810;&#26679;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.16819</link><description>&lt;p&gt;
&#36870;&#21521;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#22270;&#21435;&#22122;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Graph Denoising Diffusion for Inverse Protein Folding. (arXiv:2306.16819v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16819
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#22270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#20102;&#19968;&#32452;&#22810;&#26679;&#30340;&#20505;&#36873;&#24207;&#21015;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#27010;&#25324;&#22810;&#26679;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#34507;&#30333;&#36136;&#25240;&#21472;&#20855;&#26377;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#23427;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35768;&#22810;&#21487;&#33021;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#21487;&#20197;&#25240;&#21472;&#25104;&#19968;&#20010;&#30456;&#21516;&#30340;&#34507;&#30333;&#36136;&#39592;&#26550;&#12290;&#36825;&#20010;&#20219;&#21153;&#19981;&#20165;&#28041;&#21450;&#21040;&#35782;&#21035;&#21487;&#34892;&#30340;&#24207;&#21015;&#65292;&#36824;&#35201;&#34920;&#31034;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37492;&#21035;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;Transformer&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24456;&#38590;&#27010;&#25324;&#21508;&#31181;&#21487;&#34892;&#35299;&#30340;&#22810;&#26679;&#24615;&#12290;&#30456;&#21453;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#25552;&#20379;&#20102;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#24207;&#21015;&#20505;&#36873;&#30340;&#28508;&#21147;&#65292;&#29992;&#20110;&#30830;&#23450;&#34507;&#30333;&#36136;&#39592;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36870;&#21521;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#22270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#20013;&#32473;&#23450;&#30340;&#34507;&#30333;&#36136;&#39592;&#26550;&#25351;&#23548;&#23545;&#24212;&#27688;&#22522;&#37240;&#27531;&#22522;&#31867;&#22411;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#35813;&#27169;&#22411;&#25512;&#26029;&#20102;&#20197;&#33410;&#28857;&#30340;&#29289;&#29702;&#21270;&#23398;&#23646;&#24615;&#21644;&#23616;&#37096;&#29615;&#22659;&#20026;&#26465;&#20214;&#30340;&#27688;&#22522;&#37240;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse protein folding is challenging due to its inherent one-to-many mapping characteristic, where numerous possible amino acid sequences can fold into a single, identical protein backbone. This task involves not only identifying viable sequences but also representing the sheer diversity of potential solutions. However, existing discriminative models, such as transformer-based auto-regressive models, struggle to encapsulate the diverse range of plausible solutions. In contrast, diffusion probabilistic models, as an emerging genre of generative approaches, offer the potential to generate a diverse set of sequence candidates for determined protein backbones. We propose a novel graph denoising diffusion model for inverse protein folding, where a given protein backbone guides the diffusion process on the corresponding amino acid residue types. The model infers the joint distribution of amino acids conditioned on the nodes' physiochemical properties and local environment. Moreover, we uti
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#25913;&#36827;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.16817</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#38598;&#25104;&#25913;&#36827;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Online Continual Learning Performance and Stability with Temporal Ensembles. (arXiv:2306.16817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16817
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#25913;&#36827;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#36845;&#20195;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#22312;&#38750;&#24179;&#31283;&#30340;&#25968;&#25454;&#27969;&#21644;&#22312;&#32447;&#26041;&#24335;&#19979;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#19979;&#38477;&#65306;(1)&#22312;&#32447;&#35774;&#32622;&#38480;&#21046;&#20102;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;(2)&#30001;&#20110;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#24615;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#20960;&#31687;&#26368;&#36817;&#30340;&#25991;&#31456;&#34920;&#26126;&#36830;&#32493;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#37325;&#25918;&#26041;&#27861;&#22312;&#27169;&#22411;&#25345;&#32493;&#35780;&#20272;&#26102;&#23384;&#22312;&#31283;&#23450;&#24615;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#38598;&#25104;&#20316;&#20026;&#25913;&#36827;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#31616;&#21333;&#22320;&#38598;&#25104;&#26469;&#33258;&#21508;&#31181;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#24182;&#20174;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#33719;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#21033;&#29992;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are very effective when trained on large datasets for a large number of iterations. However, when they are trained on non-stationary streams of data and in an online fashion, their performance is reduced (1) by the online setup, which limits the availability of data, (2) due to catastrophic forgetting because of the non-stationary nature of the data. Furthermore, several recent works (Caccia et al., 2022; Lange et al., 2023) arXiv:2205.1345(2) showed that replay methods used in continual learning suffer from the stability gap, encountered when evaluating the model continually (rather than only on task boundaries). In this article, we study the effect of model ensembling as a way to improve performance and stability in online continual learning. We notice that naively ensembling models coming from a variety of training tasks increases the performance in online continual learning considerably. Starting from this observation, and drawing inspirations from semi-supervised l
&lt;/p&gt;</description></item><item><title>LeanAI&#26159;&#19968;&#31181;&#35753;AEC&#20174;&#19994;&#32773;&#26377;&#25928;&#35268;&#21010;AI&#23454;&#26045;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#32570;&#20047;&#23545;AI&#33021;&#21147;&#21644;&#38480;&#21046;&#30340;&#29702;&#35299;&#32780;&#23548;&#33268;&#30340;&#35268;&#21010;&#19982;&#23454;&#26045;&#20043;&#38388;&#30340;&#33073;&#33410;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16799</link><description>&lt;p&gt;
LeanAI: &#19968;&#31181;&#26377;&#25928;&#35268;&#21010;AI&#23454;&#26045;&#30340;AEC&#20174;&#19994;&#32773;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LeanAI: A method for AEC practitioners to effectively plan AI implementations. (arXiv:2306.16799v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16799
&lt;/p&gt;
&lt;p&gt;
LeanAI&#26159;&#19968;&#31181;&#35753;AEC&#20174;&#19994;&#32773;&#26377;&#25928;&#35268;&#21010;AI&#23454;&#26045;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#32570;&#20047;&#23545;AI&#33021;&#21147;&#21644;&#38480;&#21046;&#30340;&#29702;&#35299;&#32780;&#23548;&#33268;&#30340;&#35268;&#21010;&#19982;&#23454;&#26045;&#20043;&#38388;&#30340;&#33073;&#33410;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#21457;&#23637;&#20026;&#24314;&#31569;&#12289;&#24037;&#31243;&#21644;&#26045;&#24037;(AEC)&#34892;&#19994;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33258;&#21160;&#21270;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20154;&#20204;&#23545;&#20351;&#29992;AI&#20805;&#28385;&#28909;&#24773;&#65292;&#20294;&#30446;&#21069;85%&#30340;&#22823;&#25968;&#25454;&#39033;&#30446;&#22833;&#36133;&#12290;AEC&#34892;&#19994;AI&#39033;&#30446;&#22833;&#36133;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#26159;&#35268;&#21010;&#25110;&#20915;&#23450;&#20351;&#29992;AI&#30340;&#20154;&#19982;&#23454;&#26045;&#32773;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;AEC&#20174;&#19994;&#32773;&#32463;&#24120;&#32570;&#20047;&#23545;AI&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#30340;&#28165;&#26224;&#29702;&#35299;&#65292;&#23548;&#33268;&#26080;&#27861;&#21306;&#20998;AI&#24212;&#35813;&#35299;&#20915;&#20160;&#20040;&#38382;&#39064;&#12289;&#33021;&#35299;&#20915;&#20160;&#20040;&#38382;&#39064;&#20197;&#21450;&#23558;&#35299;&#20915;&#20160;&#20040;&#38382;&#39064;&#65292;&#23558;&#36825;&#20123;&#31867;&#21035;&#35270;&#20026;&#20114;&#30456;&#21487;&#26367;&#20195;&#30340;&#12290;&#36825;&#31181;&#29702;&#35299;&#19978;&#30340;&#32570;&#20047;&#23548;&#33268;AI&#35268;&#21010;&#19982;&#23454;&#26045;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#22240;&#20026;&#35268;&#21010;&#22522;&#20110;AI&#24212;&#35813;&#35299;&#20915;&#30340;&#24895;&#26223;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#26159;&#21542;&#33021;&#22815;&#25110;&#23558;&#20250;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;LeanAI&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;AEC&#34892;&#19994;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in Artificial Intelligence (AI) provide unprecedented automation opportunities in the Architecture, Engineering, and Construction (AEC) industry. However, despite the enthusiasm regarding the use of AI, 85% of current big data projects fail. One of the main reasons for AI project failures in the AEC industry is the disconnect between those who plan or decide to use AI and those who implement it. AEC practitioners often lack a clear understanding of the capabilities and limitations of AI, leading to a failure to distinguish between what AI should solve, what it can solve, and what it will solve, treating these categories as if they are interchangeable. This lack of understanding results in the disconnect between AI planning and implementation because the planning is based on a vision of what AI should solve without considering if it can or will solve it. To address this challenge, this work introduces the LeanAI method. The method has been developed using data from s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#21644;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#29289;&#20307;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#23567;&#29289;&#20307;&#26102;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16798</link><description>&lt;p&gt;
&#35780;&#20272;&#29615;&#22659;&#26465;&#20214;&#23545;&#20351;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#36827;&#34892;AR&#24212;&#29992;&#30340;&#29289;&#20307;&#26816;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Environmental Conditions on Object Detection using Oriented Bounding Boxes for AR Applications. (arXiv:2306.16798v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#21644;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#29289;&#20307;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#23567;&#29289;&#20307;&#26102;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#30340;&#30446;&#26631;&#26159;&#23558;&#25968;&#23383;&#20869;&#23481;&#28155;&#21152;&#21040;&#33258;&#28982;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;&#22330;&#26223;&#20998;&#26512;&#21644;&#29289;&#20307;&#35782;&#21035;&#22312;AR&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#25191;&#34892;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#19982;&#26816;&#27979;&#21644;&#35782;&#21035;&#28145;&#24230;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26102;&#38388;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65306;&#19968;&#20010;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;DOTA&#25968;&#25454;&#38598;&#65289;&#21644;&#19968;&#20010;&#27169;&#25311;&#19981;&#21516;&#29615;&#22659;&#12289;&#29031;&#26126;&#21644;&#37319;&#38598;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#30340;&#37325;&#28857;&#26159;&#23567;&#29289;&#20307;&#65292;&#36825;&#20123;&#29289;&#20307;&#24448;&#24448;&#38590;&#20197;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#26465;&#20214;&#19979;&#65292;&#23545;&#20110;&#23567;&#29289;&#20307;&#24448;&#24448;&#33021;&#20135;&#29983;&#26356;&#22909;&#30340;&#24179;&#22343;&#31934;&#24230;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of augmented reality (AR) is to add digital content to natural images and videos to create an interactive experience between the user and the environment. Scene analysis and object recognition play a crucial role in AR, as they must be performed quickly and accurately. In this study, a new approach is proposed that involves using oriented bounding boxes with a detection and recognition deep network to improve performance and processing time. The approach is evaluated using two datasets: a real image dataset (DOTA dataset) commonly used for computer vision tasks, and a synthetic dataset that simulates different environmental, lighting, and acquisition conditions. The focus of the evaluation is on small objects, which are difficult to detect and recognise. The results indicate that the proposed approach tends to produce better Average Precision and greater accuracy for small objects in most of the tested conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16788</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#27748;&#65306;&#36890;&#36807;&#27169;&#22411;&#24179;&#22343;&#25913;&#36827;&#20462;&#21098;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging. (arXiv:2306.16788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#21098;&#26525;&#26174;&#33879;&#21387;&#32553;&#65292;&#20174;&#32780;&#24471;&#21040;&#31232;&#30095;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#26356;&#23569;&#30340;&#23384;&#20648;&#21644;&#28014;&#28857;&#36816;&#31639;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;&#27169;&#22411;&#27748;&#65288;Wortsman&#31561;&#20154;&#65292;2022&#24180;&#65289;&#36890;&#36807;&#23558;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#24179;&#22343;&#25104;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26469;&#25913;&#21892;&#27867;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#22788;&#20110;&#30456;&#21516;&#25439;&#22833;&#21306;&#22495;&#30340;&#27169;&#22411;&#20197;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23545;&#20219;&#24847;&#31232;&#30095;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#20250;&#38477;&#20302;&#25972;&#20307;&#31232;&#30095;&#24230;&#65292;&#21407;&#22240;&#26159;&#19981;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22312;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#65288;IMP&#65289;&#30340;&#21333;&#27425;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#20013;&#25506;&#32034;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#37197;&#32622;&#65288;&#20363;&#22914;&#25209;&#27425;&#25490;&#24207;&#25110;&#26435;&#37325;&#34928;&#20943;&#65289;&#20135;&#29983;&#30340;&#27169;&#22411;&#36866;&#21512;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#19988;&#36890;&#36807;&#35774;&#35745;&#20849;&#20139;&#30456;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#24179;&#22343;&#36825;&#20123;&#27169;&#22411;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#30340;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#36710;&#36742;&#12289;&#29615;&#22659;&#21644;&#39550;&#39542;&#21592;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12290;&#36825;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#36866;&#21512;&#30340;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.16784</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#25968;&#25454;&#38598;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Datasets for Decision-making of Autonomous Vehicle. (arXiv:2306.16784v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#30340;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#36710;&#36742;&#12289;&#29615;&#22659;&#21644;&#39550;&#39542;&#21592;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12290;&#36825;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#36866;&#21512;&#30340;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#26377;&#26395;&#37325;&#22609;&#26410;&#26469;&#30340;&#20132;&#36890;&#31995;&#32479;&#65292;&#32780;&#20915;&#31574;&#26159;&#23454;&#29616;&#39640;&#32423;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#27169;&#22359;&#20043;&#19968;&#12290;&#20026;&#20102;&#20811;&#26381;&#37027;&#20123;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22788;&#29702;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#26041;&#27861;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29992;&#20110;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#26497;&#22823;&#22320;&#24433;&#21709;&#20915;&#31574;&#24615;&#33021;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;&#25968;&#25454;&#38598;&#12290;&#20174;&#37319;&#38598;&#26469;&#28304;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#39550;&#39542;&#25968;&#25454;&#21487;&#20197;&#20998;&#20026;&#36710;&#36742;&#12289;&#29615;&#22659;&#21644;&#39550;&#39542;&#21592;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#36825;&#19977;&#31867;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#65292;&#21253;&#25324;&#20351;&#29992;&#30340;&#20256;&#24863;&#22120;&#12289;&#27880;&#37322;&#21644;&#39550;&#39542;&#22330;&#26223;&#12290;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#28857;&#65292;&#26412;&#32508;&#36848;&#36824;&#24635;&#32467;&#20102;&#25968;&#25454;&#38598;&#22312;AV&#20915;&#31574;&#21508;&#20010;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles (AV) are expected to reshape future transportation systems, and decision-making is one of the critical modules toward high-level automated driving. To overcome those complicated scenarios that rule-based methods could not cope with well, data-driven decision-making approaches have aroused more and more focus. The datasets to be used in developing data-driven methods dramatically influences the performance of decision-making, hence it is necessary to have a comprehensive insight into the existing datasets. From the aspects of collection sources, driving data can be divided into vehicle, environment, and driver related data. This study compares the state-of-the-art datasets of these three categories and summarizes their features including sensors used, annotation, and driving scenarios. Based on the characteristics of the datasets, this survey also concludes the potential applications of datasets on various aspects of AV decision-making, assisting researchers to find 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.16772</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#30340;&#20154;&#31867;&#22242;&#38431;&#27963;&#21160;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16772
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#23545;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#30340;&#29702;&#35299;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#20219;&#21153;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;M3Act&#37319;&#29992;Unity&#24341;&#25806;&#39537;&#21160;&#65292;&#21253;&#21547;&#21487;&#20379;&#20223;&#30495;&#20351;&#29992;&#30340;&#19977;&#32500;&#22330;&#26223;&#21644;&#20154;&#29289;&#36164;&#28304;&#65292;&#21487;&#37197;&#32622;&#30340;&#29031;&#26126;&#21644;&#25668;&#20687;&#31995;&#32479;&#65292;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22359;&#21270;&#22242;&#38431;&#27963;&#21160;&#65292;&#20197;&#21450;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20855;&#26377;&#22823;&#37327;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#12289;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#12289;2D&#23039;&#21183;&#12289;3D&#21160;&#20316;&#65289;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65288;2D&#36793;&#30028;&#26694;&#12289;&#23454;&#20363;&#20998;&#21106;&#25513;&#27169;&#12289;&#20010;&#20307;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#31867;&#21035;&#65289;&#12290;&#21033;&#29992;M3Act&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
&lt;/p&gt;</description></item><item><title>DialoGPS&#26159;&#31532;&#19968;&#20010;&#22312;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#36827;&#34892;&#23545;&#35805;&#36335;&#24452;&#37319;&#26679;&#30340;&#22810;&#23545;&#22810;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#25968;&#25454;&#22686;&#24378;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16770</link><description>&lt;p&gt;
DialoGPS: &#22312;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#35805;&#36335;&#24452;&#37319;&#26679;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations. (arXiv:2306.16770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16770
&lt;/p&gt;
&lt;p&gt;
DialoGPS&#26159;&#31532;&#19968;&#20010;&#22312;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#36827;&#34892;&#23545;&#35805;&#36335;&#24452;&#37319;&#26679;&#30340;&#22810;&#23545;&#22810;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#25968;&#25454;&#22686;&#24378;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20013;&#30340;&#19978;&#19979;&#25991;&#21644;&#22238;&#22797;&#26159;&#19968;&#23545;&#19968;&#30340;&#26144;&#23556;&#65292;&#36829;&#21453;&#20102;&#37325;&#35201;&#30340;&#22810;&#23545;&#22810;&#29305;&#24615;&#65306;&#19978;&#19979;&#25991;&#26377;&#22810;&#31181;&#22238;&#22797;&#65292;&#22238;&#22797;&#22238;&#31572;&#22810;&#20010;&#19978;&#19979;&#25991;&#12290;&#32570;&#20047;&#36825;&#26679;&#30340;&#27169;&#24335;&#65292;&#27169;&#22411;&#24456;&#38590;&#27867;&#21270;&#24182;&#20542;&#21521;&#20110;&#23433;&#20840;&#22238;&#22797;&#12290;&#24050;&#32463;&#26377;&#35768;&#22810;&#23581;&#35797;&#20197;&#19968;&#23545;&#22810;&#30340;&#35282;&#24230;&#22788;&#29702;&#22810;&#36718;&#23545;&#35805;&#25110;&#20197;&#22810;&#23545;&#22810;&#30340;&#35282;&#24230;&#22788;&#29702;&#21333;&#36718;&#23545;&#35805;&#65292;&#20294;&#23545;&#20110;&#22810;&#23545;&#22810;&#30340;&#22810;&#36718;&#23545;&#35805;&#30340;&#22686;&#24378;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DialoGPS&#26041;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#22810;&#23545;&#22810;&#22686;&#24378;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#23545;&#35805;&#26144;&#23556;&#21040;&#25105;&#20204;&#30340;&#25193;&#23637;&#24067;&#26391;&#26725;&#65288;Brownian Bridge&#65289;&#65292;&#19968;&#20010;&#29305;&#27530;&#30340;&#39640;&#26031;&#36807;&#31243;&#12290;&#25105;&#20204;&#37319;&#26679;&#28508;&#21464;&#37327;&#20197;&#24418;&#25104;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#36830;&#36143;&#23545;&#35805;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In open-domain dialogue generation tasks, contexts and responses in most datasets are one-to-one mapped, violating an important many-to-many characteristic: a context leads to various responses, and a response answers multiple contexts. Without such patterns, models poorly generalize and prefer responding safely. Many attempts have been made in either multi-turn settings from a one-to-many perspective or in a many-to-many perspective but limited to single-turn settings. The major challenge to many-to-many augment multi-turn dialogues is that discretely replacing each turn with semantic similarity breaks fragile context coherence. In this paper, we propose DialoGue Path Sampling (DialoGPS) method in continuous semantic space, the first many-to-many augmentation method for multi-turn dialogues. Specifically, we map a dialogue to our extended Brownian Bridge, a special Gaussian process. We sample latent variables to form coherent dialogue paths in the continuous space. A dialogue path cor
&lt;/p&gt;</description></item><item><title>ERC&#26159;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.16750</link><description>&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#21450;&#20854;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25913;&#36827;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Eigensubspace of Temporal-Difference Dynamics and How It Improves Value Approximation in Reinforcement Learning. (arXiv:2306.16750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16750
&lt;/p&gt;
&lt;p&gt;
ERC&#26159;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#21363;&#29305;&#24449;&#23376;&#31354;&#38388;&#35268;&#33539;&#21270;&#25209;&#35780;&#23478;&#65288;ERC&#65289;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290; ERC&#21463;&#21040;&#20102;&#23545;&#26102;&#24207;&#24046;&#20998;&#65288;TD&#65289;&#26041;&#27861;&#20013;Q&#20540;&#20272;&#35745;&#35823;&#24046;&#21160;&#21147;&#23398;&#30340;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#36981;&#24490;&#30001;&#19982;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30456;&#20851;&#30340;&#36716;&#31227;&#26680;&#20851;&#32852;&#30340;1-&#29305;&#24449;&#23376;&#31354;&#38388;&#23450;&#20041;&#30340;&#36335;&#24452;&#12290;&#23427;&#25581;&#31034;&#20102;TD&#23398;&#20064;&#30340;&#19968;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#22312;&#20808;&#21069;&#30340;&#28145;&#24230;RL&#26041;&#27861;&#20013;&#26410;&#34987;&#20351;&#29992;&#12290;&#22312;ERC&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#65292;&#25351;&#23548;&#36817;&#20284;&#35823;&#24046;&#36235;&#21521;&#20110;1-&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#39640;&#25928;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ERC&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;&#22312;DMControl&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#20219;&#21153;&#20013;&#65292;ERC&#20248;&#20110;20&#20010;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22312;Q&#20540;&#20272;&#35745;&#26041;&#38754;&#20063;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel value approximation method, namely Eigensubspace Regularized Critic (ERC) for deep reinforcement learning (RL). ERC is motivated by an analysis of the dynamics of Q-value approximation error in the Temporal-Difference (TD) method, which follows a path defined by the 1-eigensubspace of the transition kernel associated with the Markov Decision Process (MDP). It reveals a fundamental property of TD learning that has remained unused in previous deep RL approaches. In ERC, we propose a regularizer that guides the approximation error tending towards the 1-eigensubspace, resulting in a more efficient and stable path of value approximation. Moreover, we theoretically prove the convergence of the ERC method. Besides, theoretical analysis and experiments demonstrate that ERC effectively reduces the variance of value functions. Among 26 tasks in the DMControl benchmark, ERC outperforms state-of-the-art methods for 20. Besides, it shows significant advantages in Q-value approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.16740</link><description>&lt;p&gt;
&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#26159;&#37096;&#32626;&#26426;&#22120;&#20154;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#34429;&#28982;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#35299;&#20915;&#31038;&#20132;&#23548;&#33322;&#30340;&#31639;&#27861;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#28041;&#21450;&#26426;&#22120;&#20154;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#31227;&#21160;&#65292;&#36824;&#28041;&#21450;&#21040;&#21160;&#24577;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#21450;&#20854;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24863;&#30693;&#36866;&#24212;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#28165;&#26224;&#12289;&#21487;&#37325;&#22797;&#12289;&#26131;&#20110;&#33719;&#24471;&#30340;&#22522;&#20934;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20256;&#32479;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#21152;&#36895;&#20102;&#36827;&#23637;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20844;&#24179;&#27604;&#36739;&#31639;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21576;&#29616;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#30456;&#20449;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21161;&#20110;&#31038;&#20132;&#23548;&#33322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#24314;&#31435;&#20102;&#20849;&#21516;&#12289;&#24191;&#27867;&#21487;&#29992;&#19988;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;&#65288;GraMMaR&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23039;&#21183;&#21644;&#20851;&#33410;&#19982;&#22320;&#38754;&#20043;&#38388;&#30340;&#20114;&#21160;&#30340;&#36807;&#28193;&#20998;&#24067;&#65292;&#26126;&#30830;&#20419;&#36827;&#36816;&#21160;&#21644;&#19982;&#22320;&#38754;&#36317;&#31163;&#21464;&#21270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16736</link><description>&lt;p&gt;
GraMMaR: &#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraMMaR: Ground-aware Motion Model for 3D Human Motion Reconstruction. (arXiv:2306.16736v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16736
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;&#65288;GraMMaR&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23039;&#21183;&#21644;&#20851;&#33410;&#19982;&#22320;&#38754;&#20043;&#38388;&#30340;&#20114;&#21160;&#30340;&#36807;&#28193;&#20998;&#24067;&#65292;&#26126;&#30830;&#20419;&#36827;&#36816;&#21160;&#21644;&#19982;&#22320;&#38754;&#36317;&#31163;&#21464;&#21270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20934;&#30830;&#21644;&#30495;&#23454;&#30340;&#20174;RGB&#35270;&#39057;&#20013;&#37325;&#24314;3D&#20154;&#20307;&#36816;&#21160;&#65292;&#35299;&#23494;&#22797;&#26434;&#30340;&#20154;&#22320;&#20114;&#21160;&#23545;&#20110;&#20445;&#35777;&#20154;&#31867;&#21644;&#22320;&#38754;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#35201;&#20040;&#38544;&#24335;&#22320;&#27169;&#25311;&#20154;&#22320;&#20114;&#21160;&#65292;&#35201;&#20040;&#20197;&#31232;&#30095;&#30340;&#26041;&#24335;&#27169;&#25311;&#65292;&#24448;&#24448;&#22312;&#38754;&#23545;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#26102;&#23548;&#33268;&#19981;&#30495;&#23454;&#21644;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#19968;&#31181;&#23494;&#38598;&#21644;&#36830;&#32493;&#30340;&#26041;&#24335;&#26126;&#30830;&#34920;&#31034;&#36825;&#20123;&#20114;&#21160;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;&#65292;&#31216;&#20026;GraMMaR&#65292;&#23427;&#22312;&#36816;&#21160;&#24207;&#21015;&#20013;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#21516;&#26102;&#23398;&#20064;&#23039;&#21183;&#21644;&#27599;&#20010;&#20851;&#33410;&#19982;&#22320;&#38754;&#20043;&#38388;&#30340;&#20114;&#21160;&#30340;&#36807;&#28193;&#20998;&#24067;&#12290;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;&#26126;&#30830;&#20419;&#36827;&#36816;&#21160;&#21644;&#19982;&#22320;&#38754;&#36317;&#31163;&#21464;&#21270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#32852;&#21512;&#20248;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;GraMMaR&#20316;&#20026;&#21452;&#37325;&#20808;&#39564;&#65292;&#35268;&#33539;&#20248;&#21270;&#36807;&#31243;&#26397;&#30528;
&lt;/p&gt;
&lt;p&gt;
Demystifying complex human-ground interactions is essential for accurate and realistic 3D human motion reconstruction from RGB videos, as it ensures consistency between the humans and the ground plane. Prior methods have modeled human-ground interactions either implicitly or in a sparse manner, often resulting in unrealistic and incorrect motions when faced with noise and uncertainty. In contrast, our approach explicitly represents these interactions in a dense and continuous manner. To this end, we propose a novel Ground-aware Motion Model for 3D Human Motion Reconstruction, named GraMMaR, which jointly learns the distribution of transitions in both pose and interaction between every joint and ground plane at each time step of a motion sequence. It is trained to explicitly promote consistency between the motion and distance change towards the ground. After training, we establish a joint optimization strategy that utilizes GraMMaR as a dual-prior, regularizing the optimization towards 
&lt;/p&gt;</description></item><item><title>&#22810;&#24773;&#26223;&#23398;&#20064;&#22312;&#25512;&#33616;&#21644;&#26816;&#32034;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#34892;&#19994;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#26469;&#25552;&#39640;&#25490;&#24207;&#24615;&#33021;&#65292;&#20943;&#23569;&#20102;&#23545;&#32593;&#32476;&#32467;&#26500;&#30340;&#25628;&#32034;&#21644;&#32500;&#25252;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.16732</link><description>&lt;p&gt;
&#22810;&#24773;&#26223;&#25490;&#24207;&#19982;&#33258;&#36866;&#24212;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Scenario Ranking with Adaptive Feature Learning. (arXiv:2306.16732v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16732
&lt;/p&gt;
&lt;p&gt;
&#22810;&#24773;&#26223;&#23398;&#20064;&#22312;&#25512;&#33616;&#21644;&#26816;&#32034;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#34892;&#19994;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#26469;&#25552;&#39640;&#25490;&#24207;&#24615;&#33021;&#65292;&#20943;&#23569;&#20102;&#23545;&#32593;&#32476;&#32467;&#26500;&#30340;&#25628;&#32034;&#21644;&#32500;&#25252;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#24773;&#26223;&#23398;&#20064;&#65288;MSL&#65289;&#22312;&#25512;&#33616;&#21644;&#26816;&#32034;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#34892;&#19994;&#20013;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#20174;&#19981;&#21516;&#24773;&#26223;&#20013;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20943;&#36731;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#24182;&#38477;&#20302;&#20102;&#32500;&#25252;&#25104;&#26412;&#12290;&#36825;&#20123;&#21162;&#21147;&#36890;&#36807;&#25628;&#32034;&#26356;&#20248;&#32593;&#32476;&#32467;&#26500;&#65288;&#22914;&#36741;&#21161;&#32593;&#32476;&#12289;&#19987;&#23478;&#32593;&#32476;&#21644;&#22810;&#22612;&#32593;&#32476;&#65289;&#20135;&#29983;&#19981;&#21516;&#30340;MSL&#33539;&#24335;&#12290;&#19981;&#21516;&#30340;&#24773;&#26223;&#21487;&#20197;&#25345;&#26377;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#28608;&#27963;&#29992;&#25143;&#30340;&#24847;&#22270;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#19981;&#21516;&#30340;&#24773;&#26223;&#19979;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#36741;&#21161;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20063;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#20197;&#24773;&#26223;&#24863;&#30693;&#30340;&#26041;&#24335;&#20248;&#21270;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#33719;&#24471;&#26356;&#22909;&#30340;&#25490;&#24207;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#22320;&#25628;&#32034;&#26368;&#20248;&#32593;&#32476;&#32467;&#26500;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#20027;&#35201;&#34987;&#24573;&#35270;&#65292;&#20294;&#21364;&#26159;&#38750;&#24120;&#38656;&#35201;&#30340;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#20063;&#39564;&#35777;&#20102;&#33258;&#36866;&#24212;&#29305;&#24449;&#23398;&#20064;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Multi-Scenario Learning (MSL) is widely used in recommendation and retrieval systems in the industry because it facilitates transfer learning from different scenarios, mitigating data sparsity and reducing maintenance cost. These efforts produce different MSL paradigms by searching more optimal network structure, such as Auxiliary Network, Expert Network, and Multi-Tower Network. It is intuitive that different scenarios could hold their specific characteristics, activating the user's intents quite differently. In other words, different kinds of auxiliary features would bear varying importance under different scenarios. With more discriminative feature representations refined in a scenario-aware manner, better ranking performance could be easily obtained without expensive search for the optimal network structure. Unfortunately, this simple idea is mainly overlooked but much desired in real-world systems.Further analysis also validates the rationality of adaptive feature learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PaRTE&#65292;&#19968;&#20010;&#21253;&#21547;1,126&#23545;&#25991;&#26412;&#34164;&#28085;&#31034;&#20363;&#30340;&#38598;&#21512;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#25913;&#20889;&#21477;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#20195;&#27169;&#22411;&#22312;8-16&#65285;&#30340;&#25913;&#20889;&#31034;&#20363;&#19978;&#25913;&#21464;&#20102;&#20182;&#20204;&#30340;&#39044;&#27979;&#65292;&#35828;&#26126;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.16722</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#34164;&#28085;&#27169;&#22411;&#23545;&#25913;&#20889;&#21477;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Paraphrastic Robustness in Textual Entailment Models. (arXiv:2306.16722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PaRTE&#65292;&#19968;&#20010;&#21253;&#21547;1,126&#23545;&#25991;&#26412;&#34164;&#28085;&#31034;&#20363;&#30340;&#38598;&#21512;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#25913;&#20889;&#21477;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#20195;&#27169;&#22411;&#22312;8-16&#65285;&#30340;&#25913;&#20889;&#31034;&#20363;&#19978;&#25913;&#21464;&#20102;&#20182;&#20204;&#30340;&#39044;&#27979;&#65292;&#35828;&#26126;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PaRTE&#65292;&#19968;&#20010;&#21253;&#21547;1,126&#23545;&#25991;&#26412;&#34164;&#28085;&#65288;RTE&#65289;&#31034;&#20363;&#30340;&#38598;&#21512;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#25913;&#20889;&#21477;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;RTE&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#35821;&#35328;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#24212;&#35813;&#22312;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#30340;&#36755;&#20837;&#19978;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#20351;&#29992;&#35780;&#20272;&#38598;&#26469;&#30830;&#23450;&#24403;&#31034;&#20363;&#34987;&#25913;&#20889;&#26102;&#65292;RTE&#27169;&#22411;&#30340;&#39044;&#27979;&#26159;&#21542;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#29616;&#20195;&#27169;&#22411;&#22312;8-16&#65285;&#30340;&#25913;&#20889;&#31034;&#20363;&#19978;&#25913;&#21464;&#20102;&#20182;&#20204;&#30340;&#39044;&#27979;&#65292;&#36825;&#35828;&#26126;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present PaRTE, a collection of 1,126 pairs of Recognizing Textual Entailment (RTE) examples to evaluate whether models are robust to paraphrasing. We posit that if RTE models understand language, their predictions should be consistent across inputs that share the same meaning. We use the evaluation set to determine if RTE models' predictions change when examples are paraphrased. In our experiments, contemporary models change their predictions on 8-16\% of paraphrased examples, indicating that there is still room for improvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#20174;&#30456;&#20851;&#21644;&#26080;&#20851;&#22270;&#20687;&#27744;&#20013;&#25366;&#25496;&#31572;&#26696;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;Multi Image BART (MI-BART)&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#22270;&#20687;&#24182;&#20351;&#29992;&#30456;&#20851;&#24615;&#32534;&#30721;&#22120;&#36827;&#34892;&#33258;&#30001;&#27969;&#30021;&#30340;&#31572;&#26696;&#29983;&#25104;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26368;&#22823;&#30340;RETVQA&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#22810;&#22270;&#20687;&#21644;&#26816;&#32034;&#35201;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#19968;&#32452;&#24322;&#26500;&#22270;&#20687;&#36827;&#34892;&#20803;&#25968;&#25454;&#26080;&#20851;&#30340;&#38382;&#39064;&#25552;&#38382;&#12290;</title><link>http://arxiv.org/abs/2306.16713</link><description>&lt;p&gt;
&#26469;&#33258;&#22270;&#20687;&#27744;&#30340;&#31572;&#26696;&#25366;&#25496;&#65306;&#38754;&#21521;&#22522;&#20110;&#26816;&#32034;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering. (arXiv:2306.16713v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#20174;&#30456;&#20851;&#21644;&#26080;&#20851;&#22270;&#20687;&#27744;&#20013;&#25366;&#25496;&#31572;&#26696;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;Multi Image BART (MI-BART)&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#22270;&#20687;&#24182;&#20351;&#29992;&#30456;&#20851;&#24615;&#32534;&#30721;&#22120;&#36827;&#34892;&#33258;&#30001;&#27969;&#30021;&#30340;&#31572;&#26696;&#29983;&#25104;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26368;&#22823;&#30340;RETVQA&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#22810;&#22270;&#20687;&#21644;&#26816;&#32034;&#35201;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#19968;&#32452;&#24322;&#26500;&#22270;&#20687;&#36827;&#34892;&#20803;&#25968;&#25454;&#26080;&#20851;&#30340;&#38382;&#39064;&#25552;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#19968;&#20010;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#30456;&#20851;&#21644;&#26080;&#20851;&#22270;&#20687;&#27744;&#20013;&#25366;&#25496;&#31572;&#26696;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#24517;&#39035;&#39318;&#20808;&#20174;&#22270;&#20687;&#27744;&#20013;&#26816;&#32034;&#30456;&#20851;&#22270;&#20687;&#65292;&#24182;&#20174;&#36825;&#20123;&#26816;&#32034;&#21040;&#30340;&#22270;&#20687;&#20013;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#22522;&#20110;&#26816;&#32034;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;&#25110;&#31616;&#31216;&#20026;RETVQA&#65289;&#12290;RETVQA&#19982;&#20256;&#32479;&#30740;&#31350;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26377;&#30528;&#26126;&#26174;&#19981;&#21516;&#21644;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#20256;&#32479;&#30340;VQA&#35201;&#27714;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#21333;&#20010;&#30456;&#20851;&#22270;&#20687;&#22238;&#31572;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;RETVQA&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;Multi Image BART&#65288;MI-BART&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#25105;&#20204;&#30340;&#30456;&#20851;&#24615;&#32534;&#30721;&#22120;&#26469;&#29983;&#25104;&#33258;&#30001;&#27969;&#30021;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36825;&#20010;&#39046;&#22495;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;RETVQA&#65292;&#20855;&#26377;&#20197;&#19979;&#26174;&#33879;&#29305;&#28857;&#65306;VQA&#30340;&#22810;&#22270;&#20687;&#21644;&#26816;&#32034;&#35201;&#27714;&#65292;&#23545;&#19968;&#32452;&#24322;&#26500;&#22270;&#20687;&#36827;&#34892;&#20803;&#25968;&#25454;&#26080;&#20851;&#30340;&#38382;&#39064;&#25552;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study visual question answering in a setting where the answer has to be mined from a pool of relevant and irrelevant images given as a context. For such a setting, a model must first retrieve relevant images from the pool and answer the question from these retrieved images. We refer to this problem as retrieval-based visual question answering (or RETVQA in short). The RETVQA is distinctively different and more challenging than the traditionally-studied Visual Question Answering (VQA), where a given question has to be answered with a single relevant image in context. Towards solving the RETVQA task, we propose a unified Multi Image BART (MI-BART) that takes a question and retrieved images using our relevance encoder for free-form fluent answer generation. Further, we introduce the largest dataset in this space, namely RETVQA, which has the following salient features: multi-image and retrieval requirement for VQA, metadata-independent questions over a pool of heterogeneous images, exp
&lt;/p&gt;</description></item><item><title>NNQS-Transformer&#26159;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#12290;&#20854;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;Transformer&#30340;&#37327;&#23376;&#27874;&#20989;&#25968;&#23433;&#33832;&#33576;&#12289;&#25968;&#25454;&#20013;&#24515;&#24182;&#34892;&#21270;&#26041;&#26696;&#12289;&#24182;&#34892;&#25209;&#37327;&#37319;&#26679;&#31574;&#30053;&#21644;&#24182;&#34892;&#23616;&#22495;&#33021;&#37327;&#35780;&#20272;&#26041;&#26696;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#31934;&#24230;&#21644;&#23545;&#20110;&#22823;&#20998;&#23376;&#31995;&#32479;&#30340;&#24378;&#21487;&#25193;&#23637;&#24615;&#21644;&#24369;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16705</link><description>&lt;p&gt;
NNQS-Transformer: &#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#26041;&#27861;&#29992;&#20110;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;
&lt;/p&gt;
&lt;p&gt;
NNQS-Transformer: an Efficient and Scalable Neural Network Quantum States Approach for Ab initio Quantum Chemistry. (arXiv:2306.16705v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16705
&lt;/p&gt;
&lt;p&gt;
NNQS-Transformer&#26159;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#12290;&#20854;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;Transformer&#30340;&#37327;&#23376;&#27874;&#20989;&#25968;&#23433;&#33832;&#33576;&#12289;&#25968;&#25454;&#20013;&#24515;&#24182;&#34892;&#21270;&#26041;&#26696;&#12289;&#24182;&#34892;&#25209;&#37327;&#37319;&#26679;&#31574;&#30053;&#21644;&#24182;&#34892;&#23616;&#22495;&#33021;&#37327;&#35780;&#20272;&#26041;&#26696;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#31934;&#24230;&#21644;&#23545;&#20110;&#22823;&#20998;&#23376;&#31995;&#32479;&#30340;&#24378;&#21487;&#25193;&#23637;&#24615;&#21644;&#24369;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#65288;NNQS&#65289;&#24050;&#25104;&#20026;&#37327;&#23376;&#22810;&#20307;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#20294;&#20854;&#23454;&#38469;&#24212;&#29992;&#24120;&#21463;&#21040;&#37319;&#26679;&#21644;&#23616;&#22495;&#33021;&#37327;&#35745;&#31639;&#30340;&#39640;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;NNQS&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22836;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23558;Transformer&#20316;&#20026;&#37327;&#23376;&#27874;&#20989;&#25968;&#30340;&#23433;&#33832;&#33576;&#65307;&#65288;2&#65289;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#24182;&#34892;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#65288;VMC&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#25345;&#25968;&#25454;&#30340;&#23616;&#37096;&#24615;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#35745;&#31639;&#26550;&#26500;&#65307;&#65288;3&#65289;&#19968;&#31181;&#24182;&#34892;&#25209;&#37327;&#37319;&#26679;&#31574;&#30053;&#65292;&#38477;&#20302;&#37319;&#26679;&#25104;&#26412;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#36127;&#36733;&#24179;&#34913;&#65307;&#65288;4&#65289;&#19968;&#31181;&#26082;&#20855;&#26377;&#20869;&#23384;&#21448;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#30340;&#24182;&#34892;&#23616;&#22495;&#33021;&#37327;&#35780;&#20272;&#26041;&#26696;&#65307;&#65288;5&#65289;&#23545;&#30495;&#23454;&#21270;&#23398;&#31995;&#32479;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#39640;&#36798;120&#20010;&#33258;&#26059;&#30340;&#22823;&#20998;&#23376;&#31995;&#32479;&#20855;&#26377;&#24456;&#24378;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24369;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network quantum state (NNQS) has emerged as a promising candidate for quantum many-body problems, but its practical applications are often hindered by the high cost of sampling and local energy calculation. We develop a high-performance NNQS method for \textit{ab initio} electronic structure calculations. The major innovations include: (1) A transformer based architecture as the quantum wave function ansatz; (2) A data-centric parallelization scheme for the variational Monte Carlo (VMC) algorithm which preserves data locality and well adapts for different computing architectures; (3) A parallel batch sampling strategy which reduces the sampling cost and achieves good load balance; (4) A parallel local energy evaluation scheme which is both memory and computationally efficient; (5) Study of real chemical systems demonstrates both the superior accuracy of our method compared to state-of-the-art and the strong and weak scalability for large molecular systems with up to $120$ spin o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16703</link><description>&lt;p&gt;
&#24377;&#24615;&#32422;&#26463;&#19979;&#30340;&#20803;&#23398;&#20064;&#22120;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Elastically-Constrained Meta-Learner for Federated Learning. (arXiv:2306.16703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#31105;&#27490;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#22240;&#20026;&#21333;&#20010;&#27169;&#22411;&#26080;&#27861;&#36866;&#24212;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#20803;&#23398;&#20064;&#65288;&#22914;Per-FedAvg&#65289;&#12290;&#20803;&#23398;&#20064;&#23398;&#20064;&#36866;&#29992;&#20110;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#20849;&#20139;&#21021;&#22987;&#21442;&#25968;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#23558;&#21021;&#22987;&#21270;&#24555;&#36895;&#35843;&#25972;&#21040;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#27169;&#22411;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#37319;&#26679;&#26356;&#26032;&#30340;&#38543;&#26426;&#24615;&#65292;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#26412;&#22320;&#36866;&#24212;&#21516;&#19968;&#23458;&#25143;&#31471;&#26102;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#30446;&#26631;&#12290;&#36825;&#31181;&#19981;&#21516;&#36866;&#24212;&#26041;&#21521;&#30340;&#27874;&#21160;&#38459;&#30861;&#20102;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21382;&#21490;&#26412;&#22320;&#35843;&#25972;&#30340;&#27169;&#22411;&#26469;&#38480;&#21046;&#20869;&#24490;&#29615;&#30340;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an approach to collaboratively training machine learning models for multiple parties that prohibit data sharing. One of the challenges in federated learning is non-IID data between clients, as a single model can not fit the data distribution for all clients. Meta-learning, such as Per-FedAvg, is introduced to cope with the challenge. Meta-learning learns shared initial parameters for all clients. Each client employs gradient descent to adapt the initialization to local data distributions quickly to realize model personalization. However, due to non-convex loss function and randomness of sampling update, meta-learning approaches have unstable goals in local adaptation for the same client. This fluctuation in different adaptation directions hinders the convergence in meta-learning. To overcome this challenge, we use the historical local adapted model to restrict the direction of the inner loop and propose an elastic-constrained method. As a result, the current round
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16699</link><description>&lt;p&gt;
&#24555;&#36895;-INR: &#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#25928;&#29575;&#39640;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation. (arXiv:2306.16699v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;(INR)&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#22797;&#26434;&#30340;&#24418;&#29366;&#25110;&#23545;&#35937;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#23450;&#20041;&#23427;&#20204;&#30340;&#20960;&#20309;&#24418;&#29366;&#25110;&#34920;&#38754;&#32467;&#26500;&#12290;&#30456;&#21453;&#65292;INR&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#36830;&#32493;&#20989;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20316;INR&#36827;&#34892;&#22270;&#20687;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;JPEG&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;INR&#22312;&#22270;&#20687;&#21387;&#32553;&#20043;&#22806;&#36824;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Rapid-INR&#65292;&#19968;&#31181;&#21033;&#29992;INR&#23545;&#22270;&#20687;&#36827;&#34892;&#32534;&#30721;&#21644;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#21152;&#36895;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GPU&#19978;&#30452;&#25509;&#20197;INR&#26684;&#24335;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#20174;INR&#21040;RGB&#26684;&#24335;&#30340;&#35299;&#30721;&#36807;&#31243;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#23454;&#26102;&#25191;&#34892;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21387;&#32553;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representation (INR) is an innovative approach for representing complex shapes or objects without explicitly defining their geometry or surface structure. Instead, INR represents objects as continuous functions. Previous research has demonstrated the effectiveness of using neural networks as INR for image compression, showcasing comparable performance to traditional methods such as JPEG. However, INR holds potential for various applications beyond image compression. This paper introduces Rapid-INR, a novel approach that utilizes INR for encoding and compressing images, thereby accelerating neural network training in computer vision tasks. Our methodology involves storing the whole dataset directly in INR format on a GPU, mitigating the significant data communication overhead between the CPU and GPU during training. Additionally, the decoding process from INR to RGB format is highly parallelized and executed on-the-fly. To further enhance compression, we propose iterativ
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#26497;&#21270;&#22120;&#26159;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20837;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#26497;&#21270;&#22120;&#26469;&#20928;&#21270;&#34987;&#27745;&#26579;&#30340;&#26679;&#26412;&#65292;&#36807;&#28388;&#24694;&#24847;&#20449;&#24687;&#24182;&#20445;&#30041;&#33391;&#24615;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.16697</link><description>&lt;p&gt;
&#31070;&#32463;&#26497;&#21270;&#22120;&#65306;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#36890;&#36807;&#20928;&#21270;&#24694;&#24847;&#29305;&#24449;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features. (arXiv:2306.16697v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16697
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26497;&#21270;&#22120;&#26159;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20837;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#26497;&#21270;&#22120;&#26469;&#20928;&#21270;&#34987;&#27745;&#26579;&#30340;&#26679;&#26412;&#65292;&#36807;&#28388;&#24694;&#24847;&#20449;&#24687;&#24182;&#20445;&#30041;&#33391;&#24615;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#32473;&#23450;&#19968;&#20010;&#21518;&#38376;&#27169;&#22411;&#65292;&#22312;&#20855;&#26377;&#35302;&#21457;&#22120;&#30340;&#24694;&#24847;&#26679;&#26412;&#30340;&#39044;&#27979;&#20013;&#65292;&#35302;&#21457;&#22120;&#20449;&#24687;&#20250;&#20027;&#23548;&#65292;&#23613;&#31649;&#35302;&#21457;&#22120;&#20449;&#24687;&#21644;&#33391;&#24615;&#20449;&#24687;&#24182;&#23384;&#12290;&#21463;&#21040;&#20809;&#23398;&#20559;&#25391;&#22120;&#30340;&#24037;&#20316;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#20809;&#23398;&#20559;&#25391;&#22120;&#21487;&#20197;&#36890;&#36807;&#28388;&#27874;&#29305;&#23450;&#20559;&#25391;&#30340;&#20809;&#27874;&#65292;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#65292;&#22312;&#21518;&#38376;&#27169;&#22411;&#20013;&#25554;&#20837;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#26497;&#21270;&#22120;&#20316;&#20026;&#20013;&#38388;&#23618;&#65292;&#20197;&#20415;&#36890;&#36807;&#36807;&#28388;&#35302;&#21457;&#22120;&#20449;&#24687;&#26469;&#20928;&#21270;&#34987;&#27745;&#26579;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#33391;&#24615;&#20449;&#24687;&#12290;&#31070;&#32463;&#26497;&#21270;&#22120;&#34987;&#23454;&#20363;&#21270;&#20026;&#19968;&#20010;&#36731;&#37327;&#32423;&#32447;&#24615;&#21464;&#25442;&#23618;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#26377;&#38480;&#30340;&#28165;&#27905;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#12290;&#19982;&#20854;&#20182;&#22522;&#20110;&#24494;&#35843;&#30340;&#38450;&#24481;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#20165;&#35843;&#25972;&#21518;&#38376;&#27169;&#22411;&#30340;&#37096;&#20998;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the susceptibility of deep neural networks to backdoor attacks. Given a backdoored model, its prediction of a poisoned sample with trigger will be dominated by the trigger information, though trigger information and benign information coexist. Inspired by the mechanism of the optical polarizer that a polarizer could pass light waves with particular polarizations while filtering light waves with other polarizations, we propose a novel backdoor defense method by inserting a learnable neural polarizer into the backdoored model as an intermediate layer, in order to purify the poisoned sample via filtering trigger information while maintaining benign information. The neural polarizer is instantiated as one lightweight linear transformation layer, which is learned through solving a well designed bi-level optimization problem, based on a limited clean dataset. Compared to other fine-tuning-based defense methods which often adjust all parameters of the backdoor
&lt;/p&gt;</description></item><item><title>SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16688</link><description>&lt;p&gt;
SRL: &#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#21040;&#19968;&#19975;&#22810;&#20010;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores. (arXiv:2306.16688v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16688
&lt;/p&gt;
&lt;p&gt;
SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#19981;&#26029;&#22797;&#26434;&#21270;&#35201;&#27714;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#21644;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#20197;&#35757;&#32451;&#26234;&#33021;Agent&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#28304;&#24211;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#34429;&#28982;OpenAI&#21644;DeepMind&#30340;&#24037;&#19994;&#31995;&#32479;&#24050;&#32463;&#25104;&#21151;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;RL&#35757;&#32451;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#31995;&#32479;&#26550;&#26500;&#21644;&#23454;&#29616;&#32454;&#33410;&#23545;&#31038;&#21306;&#26469;&#35828;&#20173;&#28982;&#19981;&#20844;&#24320;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL&#35757;&#32451;&#25968;&#25454;&#27969;&#30340;&#26032;&#25277;&#35937;&#65292;&#23558;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;RL&#35757;&#32451;&#32479;&#19968;&#25104;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;&#26681;&#25454;&#36825;&#20010;&#25277;&#35937;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#65292;&#21517;&#20026;"ReaLly Scalable RL&#65288;SRL&#65289;"&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where large-scale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general framework and enables fine-grained optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#31867;-based &#24179;&#38138;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#20851;&#21345;&#34920;&#31034;&#26469;&#23454;&#29616;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;&#65292;&#20026;&#26410;&#27880;&#37322;&#30340;&#28216;&#25103;&#25552;&#20379;&#20851;&#21345;&#34920;&#31034;&#65292;&#24182;&#22312;&#28216;&#25103;&#20043;&#38388;&#25552;&#20379;&#32479;&#19968;&#30340;&#20851;&#21345;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.16666</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#20851;&#21345;&#34920;&#31034;&#36827;&#34892;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Game Level Blending using a Learned Level Representation. (arXiv:2306.16666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#31867;-based &#24179;&#38138;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#20851;&#21345;&#34920;&#31034;&#26469;&#23454;&#29616;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;&#65292;&#20026;&#26410;&#27880;&#37322;&#30340;&#28216;&#25103;&#25552;&#20379;&#20851;&#21345;&#34920;&#31034;&#65292;&#24182;&#22312;&#28216;&#25103;&#20043;&#38388;&#25552;&#20379;&#32479;&#19968;&#30340;&#20851;&#21345;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;&#30340;&#26041;&#27861;&#22312;&#28216;&#25103;&#20135;&#29983;&#25216;&#26415;&#39046;&#22495;&#36880;&#28176;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#25216;&#26415;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#20851;&#21345;&#34920;&#31034;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;&#30340;&#25968;&#37327;&#12290;&#21363;&#20351;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#28216;&#25103;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#38656;&#35201;&#21019;&#24314;&#19968;&#20010;&#39069;&#22806;&#30340;&#20849;&#20139;&#34920;&#31034;&#25165;&#33021;&#36827;&#34892;&#28151;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28216;&#25103;&#20851;&#21345;&#28151;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#24179;&#38138;&#23884;&#20837;&#65288;CTE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#30340;&#20851;&#21345;&#34920;&#31034;&#25216;&#26415;&#65292;&#21487;&#20197;&#20026;&#38750;&#27880;&#37322;&#28216;&#25103;&#25552;&#20379;&#20851;&#21345;&#34920;&#31034;&#65292;&#24182;&#22312;&#28216;&#25103;&#20043;&#38388;&#25552;&#20379;&#32479;&#19968;&#30340;&#20851;&#21345;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game level blending via machine learning, the process of combining features of game levels to create unique and novel game levels using Procedural Content Generation via Machine Learning (PCGML) techniques, has gained increasing popularity in recent years. However, many existing techniques rely on human-annotated level representations, which limits game level blending to a limited number of annotated games. Even with annotated games, researchers often need to author an additional shared representation to make blending possible. In this paper, we present a novel approach to game level blending that employs Clustering-based Tile Embeddings (CTE), a learned level representation technique that can serve as a level representation for unannotated games and a unified level representation across games without the need for human annotation. CTE represents game level tiles as a continuous vector representation, unifying their visual, contextual, and behavioral information. We apply this approach
&lt;/p&gt;</description></item><item><title>NaturalInversion &#26159;&#19968;&#31181;&#26080;&#38656;&#30495;&#23454;&#25968;&#25454;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#20256;&#36882;&#37329;&#23383;&#22612;&#12289;&#19968;&#23545;&#19968;&#29983;&#25104;&#27169;&#22411;&#21644;&#21487;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#36890;&#36947;&#32553;&#25918;&#21442;&#25968;&#65292;&#21512;&#25104;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#26356;&#21152;&#19968;&#33268;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.16661</link><description>&lt;p&gt;
NaturalInversion: &#26080;&#38656;&#30495;&#23454;&#25968;&#25454;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#25552;&#21319;&#29616;&#23454;&#19990;&#30028;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
NaturalInversion: Data-Free Image Synthesis Improving Real-World Consistency. (arXiv:2306.16661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16661
&lt;/p&gt;
&lt;p&gt;
NaturalInversion &#26159;&#19968;&#31181;&#26080;&#38656;&#30495;&#23454;&#25968;&#25454;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#20256;&#36882;&#37329;&#23383;&#22612;&#12289;&#19968;&#23545;&#19968;&#29983;&#25104;&#27169;&#22411;&#21644;&#21487;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#36890;&#36947;&#32553;&#25918;&#21442;&#25968;&#65292;&#21512;&#25104;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#26356;&#21152;&#19968;&#33268;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; NaturalInversion &#30340;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#25104;&#19982;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#30456;&#31526;&#30340;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312; NaturalInversion &#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#20960;&#28857;&#21019;&#26032;&#65306;&#65288;1&#65289;&#29305;&#24449;&#20256;&#36882;&#37329;&#23383;&#22612;&#65292;&#22312;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#25552;&#21462;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#22270;&#30340;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992;&#22686;&#24378;&#30340;&#21407;&#22987;&#25968;&#25454;&#22270;&#20687;&#20808;&#39564;&#20449;&#24687;&#65307;&#65288;2&#65289;&#19968;&#23545;&#19968;&#29983;&#25104;&#27169;&#22411;&#65292;&#27599;&#20010;&#29983;&#25104;&#22120;&#21482;&#21512;&#25104;&#19968;&#20010;&#25209;&#27425;&#30340;&#22270;&#20687;&#65292;&#20197;&#24341;&#20837;&#38750;&#32447;&#24615;&#24230;&#37327;&#24182;&#31616;&#21270;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#65307;&#65288;3&#65289;&#21487;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#36890;&#36947;&#32553;&#25918;&#21442;&#25968;&#65292;&#20197;&#35843;&#25972;&#36755;&#20986;&#22270;&#20687;&#36890;&#36947;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#21407;&#22987;&#22270;&#20687;&#20808;&#39564;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340; NaturalInversion&#65292;&#25105;&#20204;&#20174;&#22312; CIFAR-10/100 &#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#38468;&#21152;&#20998;&#26512;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#21512;&#25104;&#22270;&#20687;&#19982;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21512;&#25104;&#22270;&#20687;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce NaturalInversion, a novel model inversion-based method to synthesize images that agrees well with the original data distribution without using real data. In NaturalInversion, we propose: (1) a Feature Transfer Pyramid which uses enhanced image prior of the original data by combining the multi-scale feature maps extracted from the pre-trained classifier, (2) a one-to-one approach generative model where only one batch of images are synthesized by one generator to bring the non-linearity to optimization and to ease the overall optimizing process, (3) learnable Adaptive Channel Scaling parameters which are end-to-end trained to scale the output image channel to utilize the original image prior further. With our NaturalInversion, we synthesize images from classifiers trained on CIFAR-10/100 and show that our images are more consistent with original data distribution than prior works by visualization and additional analysis. Furthermore, our synthesized images outperform prior w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#35821;&#20041;&#22270;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#29983;&#25104;&#26041;&#26696;&#65288;TEAM&#65289;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#25552;&#21462;&#23545;&#35937;&#32423;&#35821;&#20041;&#20803;&#25968;&#25454;&#21644;&#24341;&#20837;&#22806;&#37096;&#30456;&#20851;&#30693;&#35782;&#27010;&#24565;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35270;&#35273;&#29305;&#24449;&#19982;&#35299;&#30721;&#22120;&#35821;&#20041;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#20197;&#21450;&#28508;&#22312;&#30340;&#22806;&#37096;&#30693;&#35782;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.16650</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#28304;&#35821;&#20041;&#22270;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation. (arXiv:2306.16650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#35821;&#20041;&#22270;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#29983;&#25104;&#26041;&#26696;&#65288;TEAM&#65289;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#25552;&#21462;&#23545;&#35937;&#32423;&#35821;&#20041;&#20803;&#25968;&#25454;&#21644;&#24341;&#20837;&#22806;&#37096;&#30456;&#20851;&#30693;&#35782;&#27010;&#24565;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35270;&#35273;&#29305;&#24449;&#19982;&#35299;&#30721;&#22120;&#35821;&#20041;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#20197;&#21450;&#28508;&#22312;&#30340;&#22806;&#37096;&#30693;&#35782;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#65288;MuSE&#65289;&#26159;&#19968;&#20010;&#26032;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#22810;&#27169;&#24577;&#31038;&#20132;&#24086;&#23376;&#65288;&#21253;&#25324;&#22270;&#20687;&#21644;&#20854;&#26631;&#39064;&#65289;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#65292;&#35299;&#37322;&#20026;&#20160;&#20040;&#23427;&#21253;&#21547;&#35773;&#21050;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20808;&#39537;&#30740;&#31350;&#22312;&#20351;&#29992;BART&#26694;&#26550;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#24573;&#35270;&#20102;&#22270;&#20687;&#30340;&#23545;&#35937;&#32423;&#20803;&#25968;&#25454;&#19982;&#35299;&#30721;&#22120;&#35821;&#20041;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#22806;&#37096;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#28304;&#35821;&#20041;&#22270;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#26041;&#26696;&#65292;&#31216;&#20026;TEAM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TEAM&#25552;&#21462;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#35937;&#32423;&#35821;&#20041;&#20803;&#25968;&#25454;&#32780;&#19981;&#26159;&#20256;&#32479;&#20840;&#23616;&#35270;&#35273;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;TEAM&#21033;&#29992;ConceptNet&#33719;&#21462;&#36755;&#20837;&#25991;&#26412;&#21644;&#25552;&#21462;&#30340;&#23545;&#35937;&#20803;&#25968;&#25454;&#30340;&#30456;&#20851;&#22806;&#37096;&#30693;&#35782;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;TEAM&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#28304;&#35821;&#20041;&#22270;&#65292;&#20840;&#38754;&#22320;&#21051;&#30011;&#20102;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sarcasm Explanation (MuSE) is a new yet challenging task, which aims to generate a natural language sentence for a multimodal social post (an image as well as its caption) to explain why it contains sarcasm. Although the existing pioneer study has achieved great success with the BART backbone, it overlooks the gap between the visual feature space and the decoder semantic space, the object-level metadata of the image, as well as the potential external knowledge. To solve these limitations, in this work, we propose a novel mulTi-source sEmantic grAph-based Multimodal sarcasm explanation scheme, named TEAM. In particular, TEAM extracts the object-level semantic meta-data instead of the traditional global visual features from the input image. Meanwhile, TEAM resorts to ConceptNet to obtain the external related knowledge concepts for the input text and the extracted object meta-data. Thereafter, TEAM introduces a multi-source semantic graph that comprehensively characterize the m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#25289;&#21462;&#21644;&#25512;&#36865;&#20004;&#31181;AI&#36741;&#21161;&#20889;&#20316;&#27169;&#24335;&#23545;&#29992;&#25143;&#38656;&#27714;&#12289;&#36136;&#37327;&#12289;&#25152;&#26377;&#26435;&#12289;&#25928;&#29575;&#21644;&#20139;&#21463;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#29992;&#25143;&#27426;&#36814;AI&#22312;&#20889;&#20316;&#20013;&#30340;&#26080;&#32541;&#36741;&#21161;&#65292;AI&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#26356;&#24555;&#22320;&#20351;&#20889;&#20316;&#20013;&#30340;&#24819;&#27861;&#22810;&#26679;&#21270;&#65292;&#21516;&#26102;&#20445;&#25345;&#28165;&#26224;&#21644;&#31616;&#27905;&#65292;&#24182;&#19988;&#29992;&#25143;&#21916;&#27426;&#19982;AI&#36741;&#21161;&#20889;&#20316;&#24037;&#20855;&#30340;&#21327;&#20316;&#65292;&#27809;&#26377;&#24863;&#21040;&#32570;&#20047;&#25152;&#26377;&#26435;&#12290;&#23613;&#31649;&#22312;&#23454;&#39564;&#20013;&#27809;&#26377;&#20307;&#39564;&#21040;&#20559;&#35265;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16641</link><description>&lt;p&gt;
AI&#36741;&#21161;&#20889;&#20316;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
The Future of AI-Assisted Writing. (arXiv:2306.16641v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#25289;&#21462;&#21644;&#25512;&#36865;&#20004;&#31181;AI&#36741;&#21161;&#20889;&#20316;&#27169;&#24335;&#23545;&#29992;&#25143;&#38656;&#27714;&#12289;&#36136;&#37327;&#12289;&#25152;&#26377;&#26435;&#12289;&#25928;&#29575;&#21644;&#20139;&#21463;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#29992;&#25143;&#27426;&#36814;AI&#22312;&#20889;&#20316;&#20013;&#30340;&#26080;&#32541;&#36741;&#21161;&#65292;AI&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#26356;&#24555;&#22320;&#20351;&#20889;&#20316;&#20013;&#30340;&#24819;&#27861;&#22810;&#26679;&#21270;&#65292;&#21516;&#26102;&#20445;&#25345;&#28165;&#26224;&#21644;&#31616;&#27905;&#65292;&#24182;&#19988;&#29992;&#25143;&#21916;&#27426;&#19982;AI&#36741;&#21161;&#20889;&#20316;&#24037;&#20855;&#30340;&#21327;&#20316;&#65292;&#27809;&#26377;&#24863;&#21040;&#32570;&#20047;&#25152;&#26377;&#26435;&#12290;&#23613;&#31649;&#22312;&#23454;&#39564;&#20013;&#27809;&#26377;&#20307;&#39564;&#21040;&#20559;&#35265;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20889;&#20316;&#24037;&#20855;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#24037;&#20855;&#33021;&#22815;&#39044;&#27979;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#29992;&#25143;&#20889;&#20316;&#26102;&#20027;&#21160;&#25552;&#20379;&#24314;&#35758;&#12290;&#26412;&#30740;&#31350;&#20174;&#20449;&#24687;&#26816;&#32034;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#19968;&#39033;&#27604;&#36739;&#24615;&#29992;&#25143;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#20004;&#31181;&#33539;&#24335;&#65288;&#25289;&#21462;&#21644;&#25512;&#36865;&#65289;&#23545;AI&#36741;&#21161;&#20889;&#20316;&#30340;&#29992;&#25143;&#38656;&#27714;&#12289;&#36136;&#37327;&#12289;&#20889;&#20316;&#20135;&#21697;&#30340;&#25152;&#26377;&#26435;&#20197;&#21450;&#20889;&#20316;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#20139;&#21463;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#35797;&#22270;&#20102;&#35299;AI&#36741;&#21161;&#20889;&#20316;&#30340;&#20559;&#35265;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#25143;&#23545;&#20154;&#24037;&#26234;&#33021;&#22312;&#20889;&#20316;&#20013;&#30340;&#26080;&#32541;&#36741;&#21161;&#34920;&#31034;&#27426;&#36814;&#12290;&#27492;&#22806;&#65292;AI&#24110;&#21161;&#29992;&#25143;&#26356;&#24555;&#22320;&#20351;&#20889;&#20316;&#20013;&#30340;&#24819;&#27861;&#22810;&#26679;&#21270;&#65292;&#21516;&#26102;&#20445;&#25345;&#28165;&#26224;&#21644;&#31616;&#27905;&#12290;&#29992;&#25143;&#36824;&#21916;&#27426;&#19982;AI&#36741;&#21161;&#20889;&#20316;&#24037;&#20855;&#30340;&#21327;&#20316;&#65292;&#24182;&#27809;&#26377;&#24863;&#21040;&#32570;&#20047;&#25152;&#26377;&#26435;&#12290;&#26368;&#21518;&#65292;&#23613;&#31649;&#21442;&#19982;&#32773;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#27809;&#26377;&#20307;&#39564;&#21040;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Natural Language Generation models has led to the creation of powerful Artificial Intelligence-assisted writing tools. These tools are capable of predicting users' needs and actively providing suggestions as they write. In this work, we conduct a comparative user-study between such tools from an information retrieval lens: pull and push. Specifically, we investigate the user demand of AI-assisted writing, the impact of the two paradigms on quality, ownership of the writing product, and efficiency and enjoyment of the writing process. We also seek to understand the impact of bias of AI-assisted writing. Our findings show that users welcome seamless assistance of AI in their writing. Furthermore, AI helped users to diversify the ideas in their writing while keeping it clear and concise more quickly. Users also enjoyed the collaboration with AI-assisted writing tools and did not feel a lack of ownership. Finally, although participants did not experience bias in our expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25968;&#23383;&#23398;&#20064;&#28216;&#25103;&#20013;&#35299;&#20915;&#21313;&#36827;&#21046;&#38382;&#39064;&#12289;&#21028;&#26029;&#23398;&#29983;&#31572;&#26696;&#27491;&#30830;&#24615;&#21644;&#25552;&#20379;&#26377;&#24847;&#20041;&#21453;&#39304;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#33021;&#22815;&#22312;&#27010;&#24565;&#38382;&#39064;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#21313;&#36827;&#21046;&#20301;&#21644;&#25968;&#32447;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2306.16639</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#22312;&#25968;&#23383;&#23398;&#20064;&#28216;&#25103;&#20013;&#30340;&#21313;&#36827;&#21046;&#25216;&#33021;&#21644;&#21453;&#39304;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT's Decimal Skills and Feedback Generation in a Digital Learning Game. (arXiv:2306.16639v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25968;&#23383;&#23398;&#20064;&#28216;&#25103;&#20013;&#35299;&#20915;&#21313;&#36827;&#21046;&#38382;&#39064;&#12289;&#21028;&#26029;&#23398;&#29983;&#31572;&#26696;&#27491;&#30830;&#24615;&#21644;&#25552;&#20379;&#26377;&#24847;&#20041;&#21453;&#39304;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#33021;&#22815;&#22312;&#27010;&#24565;&#38382;&#39064;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#21313;&#36827;&#21046;&#20301;&#21644;&#25968;&#32447;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#33258;&#30001;&#35299;&#37322;&#33021;&#22815;&#20419;&#36827;&#20581;&#20840;&#23398;&#20064;&#65292;&#20294;&#30001;&#20110;&#23398;&#29983;&#36755;&#20837;&#30340;&#26080;&#38480;&#21046;&#24615;&#36136;&#65292;&#36825;&#22312;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#30340;&#33258;&#21160;&#35780;&#20998;&#21644;&#21453;&#39304;&#20013;&#20135;&#29983;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;ChatGPT&#65289;&#33021;&#21542;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20174;&#21069;&#19968;&#39033;&#21313;&#36827;&#21046;&#28216;&#25103;&#23398;&#20064;&#30740;&#31350;&#20013;&#33719;&#24471;&#30340;&#25968;&#23383;&#32451;&#20064;&#21644;&#23398;&#29983;&#25968;&#25454;&#65292;&#21253;&#25324;5000&#22810;&#20010;&#33258;&#30001;&#35299;&#37322;&#22238;&#31572;&#65292;&#30740;&#31350;&#20102;ChatGPT&#22312;&#20197;&#19979;&#26041;&#38754;&#30340;&#33021;&#21147;&#65306;(1)&#35299;&#20915;&#28216;&#25103;&#20013;&#30340;&#32451;&#20064;&#65292;(2)&#30830;&#23450;&#23398;&#29983;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#65292;&#20197;&#21450;(3)&#20026;&#38169;&#35823;&#31572;&#26696;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#33021;&#22815;&#24456;&#22909;&#22320;&#22238;&#31572;&#27010;&#24565;&#38382;&#39064;&#65292;&#20294;&#22312;&#21313;&#36827;&#21046;&#20301;&#21644;&#25968;&#32447;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#23427;&#33021;&#22815;&#27491;&#30830;&#35780;&#20272;75%&#30340;&#23398;&#29983;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#29983;&#25104;&#36890;&#24120;&#20855;&#26377;&#36739;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
While open-ended self-explanations have been shown to promote robust learning in multiple studies, they pose significant challenges to automated grading and feedback in technology-enhanced learning, due to the unconstrained nature of the students' input. Our work investigates whether recent advances in Large Language Models, and in particular ChatGPT, can address this issue. Using decimal exercises and student data from a prior study of the learning game Decimal Point, with more than 5,000 open-ended self-explanation responses, we investigate ChatGPT's capability in (1) solving the in-game exercises, (2) determining the correctness of students' answers, and (3) providing meaningful feedback to incorrect answers. Our results showed that ChatGPT can respond well to conceptual questions, but struggled with decimal place values and number line problems. In addition, it was able to accurately assess the correctness of 75% of the students' answers and generated generally high-quality feedbac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#24212;&#29992;&#39064;&#65288;CMATH&#65289;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23567;&#23398;&#25968;&#23398;&#19981;&#21516;&#24180;&#32423;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#21482;&#26377;GPT-4&#22312;&#25152;&#26377;&#24180;&#32423;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#22312;&#19981;&#21516;&#24180;&#32423;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.16636</link><description>&lt;p&gt;
CMATH&#65306;&#20320;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#36890;&#36807;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#27979;&#35797;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?. (arXiv:2306.16636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16636
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#24212;&#29992;&#39064;&#65288;CMATH&#65289;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23567;&#23398;&#25968;&#23398;&#19981;&#21516;&#24180;&#32423;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#21482;&#26377;GPT-4&#22312;&#25152;&#26377;&#24180;&#32423;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#22312;&#19981;&#21516;&#24180;&#32423;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20013;&#22269;&#23567;&#23398;&#25968;&#23398;&#24212;&#29992;&#39064;&#65288;CMATH&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;1.7k&#20010;&#20855;&#26377;&#35814;&#32454;&#27880;&#37322;&#30340;&#23567;&#23398;&#27700;&#24179;&#25968;&#23398;&#24212;&#29992;&#39064;&#65292;&#26469;&#28304;&#20110;&#20013;&#22269;&#23454;&#38469;&#30340;&#32451;&#20064;&#21644;&#32771;&#35797;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35780;&#20272;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36798;&#21040;&#23567;&#23398;&#25968;&#23398;&#21738;&#20010;&#24180;&#32423;&#27700;&#24179;&#30340;&#22522;&#20934;&#24037;&#20855;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;LLMs&#65292;&#21253;&#25324;&#21830;&#19994;&#21644;&#24320;&#28304;&#36873;&#39033;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;GPT-4&#22312;&#25152;&#26377;&#20845;&#20010;&#23567;&#23398;&#24180;&#32423;&#20013;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65288;&#20934;&#30830;&#29575;&#8805;60%&#65289;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#22312;&#19981;&#21516;&#24180;&#32423;&#19978;&#30340;&#34920;&#29616;&#27424;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#24178;&#25200;&#20449;&#24687;&#26469;&#35780;&#20272;&#20960;&#20010;&#34920;&#29616;&#26368;&#20339;&#30340;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;GPT-4&#33021;&#22815;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#22833;&#36133;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#25581;&#31034;LLMs&#22312;&#31639;&#26415;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Chinese Elementary School Math Word Problems (CMATH) dataset, comprising 1.7k elementary school-level math word problems with detailed annotations, source from actual Chinese workbooks and exams. This dataset aims to provide a benchmark tool for assessing the following question: to what grade level of elementary school math do the abilities of popular large language models (LLMs) correspond? We evaluate a variety of popular LLMs, including both commercial and open-source options, and discover that only GPT-4 achieves success (accuracy $\geq$ 60\%) across all six elementary school grades, while other models falter at different grade levels. Furthermore, we assess the robustness of several top-performing LLMs by augmenting the original problems in the CMATH dataset with distracting information. Our findings reveal that GPT-4 is able to maintains robustness, while other model fail. We anticipate that our study will expose limitations in LLMs' arithmetic and reasoning capabi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#20849;&#20139;&#27169;&#22411;&#65292;&#29992;&#20110;&#20445;&#25252;&#26234;&#24935;&#21307;&#30103;&#32593;&#32476;&#20013;&#30340;&#20010;&#20154;&#20581;&#24247;&#25968;&#25454;&#12290;&#36890;&#36807;&#35780;&#20272;&#29992;&#25143;&#20043;&#38388;&#30340;&#20449;&#20219;&#27700;&#24179;&#65292;&#20351;&#29992;&#32773;&#21487;&#20197;&#36873;&#25321;&#19981;&#21516;&#30340;&#38544;&#31169;&#20445;&#25252;&#32423;&#21035;&#65292;&#24182;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#25216;&#26415;&#21644;&#21487;&#25511;&#38543;&#26426;&#22122;&#22768;&#20445;&#25252;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#65292;&#20197;&#24212;&#23545;&#38142;&#32467;&#25915;&#20987;&#21644;&#27745;&#26579;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2306.16630</link><description>&lt;p&gt;
&#38754;&#21521;&#36793;&#32536;&#26234;&#33021;&#30340;&#21306;&#22359;&#38142;&#36741;&#21161;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#20849;&#20139;&#65306;&#26234;&#24935;&#21307;&#30103;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards Blockchain-Assisted Privacy-Aware Data Sharing For Edge Intelligence: A Smart Healthcare Perspective. (arXiv:2306.16630v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16630
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#20849;&#20139;&#27169;&#22411;&#65292;&#29992;&#20110;&#20445;&#25252;&#26234;&#24935;&#21307;&#30103;&#32593;&#32476;&#20013;&#30340;&#20010;&#20154;&#20581;&#24247;&#25968;&#25454;&#12290;&#36890;&#36807;&#35780;&#20272;&#29992;&#25143;&#20043;&#38388;&#30340;&#20449;&#20219;&#27700;&#24179;&#65292;&#20351;&#29992;&#32773;&#21487;&#20197;&#36873;&#25321;&#19981;&#21516;&#30340;&#38544;&#31169;&#20445;&#25252;&#32423;&#21035;&#65292;&#24182;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#25216;&#26415;&#21644;&#21487;&#25511;&#38543;&#26426;&#22122;&#22768;&#20445;&#25252;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#65292;&#20197;&#24212;&#23545;&#38142;&#32467;&#25915;&#20987;&#21644;&#27745;&#26579;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#21307;&#30103;&#35774;&#22791;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#30340;&#26222;&#21450;&#26174;&#33879;&#25512;&#21160;&#20102;&#26234;&#24935;&#21307;&#30103;&#32593;&#32476;&#65288;SHNs&#65289;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#65292;SHNs&#20013;&#30340;&#19981;&#21516;&#21442;&#19982;&#32773;&#20849;&#20139;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#20581;&#24247;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#20132;&#25442;&#36807;&#31243;&#24341;&#21457;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#20581;&#24247;&#25968;&#25454;&#65288;&#38142;&#32467;&#25915;&#20987;&#65289;&#30340;&#25972;&#21512;&#23548;&#33268;&#36827;&#19968;&#27493;&#27844;&#38706;&#26102;&#12290;&#38142;&#32467;&#25915;&#20987;&#26159;&#38544;&#31169;&#39046;&#22495;&#20013;&#19968;&#31181;&#20027;&#35201;&#25915;&#20987;&#31867;&#22411;&#65292;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#25968;&#25454;&#28304;&#36827;&#34892;&#38544;&#31169;&#25968;&#25454;&#25366;&#25496;&#12290;&#27492;&#22806;&#65292;&#23545;&#25163;&#21457;&#21160;&#27745;&#26579;&#25915;&#20987;&#20197;&#31713;&#25913;&#20581;&#24247;&#25968;&#25454;&#65292;&#23548;&#33268;&#35823;&#35786;&#29978;&#33267;&#36523;&#20307;&#25439;&#23475;&#12290;&#20026;&#20102;&#20445;&#25252;&#31169;&#20154;&#20581;&#24247;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29992;&#25143;&#20043;&#38388;&#20449;&#20219;&#27700;&#24179;&#30340;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#12290;&#20449;&#20219;&#26159;&#36890;&#36807;&#23450;&#20041;&#30340;&#31038;&#21306;&#23494;&#24230;&#26469;&#35780;&#20272;&#30340;&#65292;&#32780;&#30456;&#24212;&#30340;&#38544;&#31169;&#20445;&#25252;&#32423;&#21035;&#21017;&#26144;&#23556;&#21040;&#21487;&#25511;&#38543;&#26426;&#22122;&#22768;&#65292;&#24182;&#21463;&#21040;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularization of intelligent healthcare devices and big data analytics significantly boosts the development of smart healthcare networks (SHNs). To enhance the precision of diagnosis, different participants in SHNs share health data that contains sensitive information. Therefore, the data exchange process raises privacy concerns, especially when the integration of health data from multiple sources (linkage attack) results in further leakage. Linkage attack is a type of dominant attack in the privacy domain, which can leverage various data sources for private data mining. Furthermore, adversaries launch poisoning attacks to falsify the health data, which leads to misdiagnosing or even physical damage. To protect private health data, we propose a personalized differential privacy model based on the trust levels among users. The trust is evaluated by a defined community density, while the corresponding privacy protection level is mapped to controllable randomized noise constrained by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26494;&#24347;&#24230;&#27010;&#24565;&#26469;&#37327;&#21270;HVAC&#25805;&#20316;&#35831;&#27714;&#32039;&#24613;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#32423;&#26041;&#27861;&#26469;&#20248;&#21270;&#22823;&#37327;HVAC&#31995;&#32479;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.16619</link><description>&lt;p&gt;
Laxity-Aware&#21487;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;HVAC&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Laxity-Aware Scalable Reinforcement Learning for HVAC Control. (arXiv:2306.16619v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26494;&#24347;&#24230;&#27010;&#24565;&#26469;&#37327;&#21270;HVAC&#25805;&#20316;&#35831;&#27714;&#32039;&#24613;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#32423;&#26041;&#27861;&#26469;&#20248;&#21270;&#22823;&#37327;HVAC&#31995;&#32479;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#27714;&#28789;&#27963;&#24615;&#22312;&#32500;&#25345;&#30005;&#32593;&#24179;&#34913;&#12289;&#38477;&#20302;&#23792;&#20540;&#38656;&#27714;&#21644;&#33410;&#32422;&#23458;&#25143;&#33021;&#28304;&#36153;&#29992;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#30001;&#20110;&#20854;&#21487;&#35843;&#25972;&#36127;&#33655;&#21644;&#23545;&#24314;&#31569;&#33021;&#28304;&#28040;&#32791;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#37319;&#26262;&#12289;&#36890;&#39118;&#21644;&#31354;&#35843;&#65288;HVAC&#65289;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#26681;&#25454;&#30005;&#20215;&#21644;&#30005;&#21147;&#31995;&#32479;&#38656;&#27714;&#35843;&#25972;&#33021;&#28304;&#28040;&#32791;&#26469;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#38656;&#27714;&#28789;&#27963;&#24615;&#32473;&#30005;&#21147;&#31995;&#32479;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#31181;&#22312;&#25805;&#20316;&#26102;&#38388;&#21644;&#21151;&#29575;&#19978;&#30340;&#28789;&#27963;&#24615;&#65292;&#20934;&#30830;&#24314;&#27169;&#21644;&#21512;&#24182;&#22823;&#37327;HVAC&#31995;&#32479;&#30340;&#36127;&#33655;&#28789;&#27963;&#24615;&#20197;&#21450;&#35774;&#35745;&#26377;&#25928;&#30340;&#25511;&#21046;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26494;&#24347;&#24230;&#30340;&#27010;&#24565;&#26469;&#37327;&#21270;&#27599;&#20010;HVAC&#25805;&#20316;&#35831;&#27714;&#30340;&#32039;&#24613;&#31243;&#24230;&#65292;&#20197;&#35299;&#20915;&#24314;&#27169;&#21644;&#25511;&#21046;&#20013;&#30340;&#32500;&#24230;&#28798;&#38590;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#32423;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#37327;HVAC&#31995;&#32479;&#30340;&#33021;&#28304;&#20248;&#21270;&#38382;&#39064;&#12290;&#19979;&#23618;&#28041;&#21450;&#19968;&#20010;&#32858;&#21512;&#22120;&#20197;&#21512;&#24182;&#21644;&#25511;&#21046;&#22823;&#37327;HVAC&#31995;&#32479;&#30340;&#33021;&#28304;&#28789;&#27963;&#24615;&#65292;&#19978;&#23618;&#28041;&#21450;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#26469;&#20248;&#21270;&#32858;&#21512;&#22120;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demand flexibility plays a vital role in maintaining grid balance, reducing peak demand, and saving customers' energy bills. Given their highly shiftable load and significant contribution to a building's energy consumption, Heating, Ventilation, and Air Conditioning (HVAC) systems can provide valuable demand flexibility to the power systems by adjusting their energy consumption in response to electricity price and power system needs. To exploit this flexibility in both operation time and power, it is imperative to accurately model and aggregate the load flexibility of a large population of HVAC systems as well as designing effective control algorithms. In this paper, we tackle the curse of dimensionality issue in modeling and control by utilizing the concept of laxity to quantify the emergency level of each HVAC operation request. We further propose a two-level approach to address energy optimization for a large population of HVAC systems. The lower level involves an aggregator to aggr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#33879;&#24615;&#24863;&#30693;&#28151;&#21512;&#26041;&#27861;GuidedMixup&#65292;&#36890;&#36807;&#20248;&#21270;&#37197;&#23545;&#22270;&#20687;&#20013;&#26174;&#33879;&#21306;&#22495;&#30340;&#20914;&#31361;&#65292;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#22312;&#28151;&#21512;&#22270;&#20687;&#20013;&#20445;&#30041;&#26174;&#33879;&#21306;&#22495;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GuidedMixup&#22312;&#22686;&#24378;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.16612</link><description>&lt;p&gt;
GuidedMixup&#65306;&#30001;&#26174;&#33879;&#24615;&#22270;&#24341;&#23548;&#30340;&#39640;&#25928;Mixup&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GuidedMixup: An Efficient Mixup Strategy Guided by Saliency Maps. (arXiv:2306.16612v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16612
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#33879;&#24615;&#24863;&#30693;&#28151;&#21512;&#26041;&#27861;GuidedMixup&#65292;&#36890;&#36807;&#20248;&#21270;&#37197;&#23545;&#22270;&#20687;&#20013;&#26174;&#33879;&#21306;&#22495;&#30340;&#20914;&#31361;&#65292;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#22312;&#28151;&#21512;&#22270;&#20687;&#20013;&#20445;&#30041;&#26174;&#33879;&#21306;&#22495;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GuidedMixup&#22312;&#22686;&#24378;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#29616;&#22312;&#26159;&#22270;&#20687;&#35757;&#32451;&#36807;&#31243;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#19968;&#37096;&#20998;&#65292;&#23427;&#26377;&#25928;&#22320;&#38450;&#27490;&#20102;&#36807;&#25311;&#21512;&#65292;&#24182;&#20351;&#27169;&#22411;&#23545;&#22122;&#22768;&#25968;&#25454;&#38598;&#26356;&#21152;&#40065;&#26834;&#12290;&#26368;&#36817;&#30340;&#28151;&#21512;&#22686;&#24378;&#31574;&#30053;&#21457;&#23637;&#20986;&#20102;&#33021;&#22815;&#20016;&#23500;&#26174;&#33879;&#24615;&#20449;&#24687;&#30340;&#28151;&#21512;&#25513;&#27169;&#65292;&#36825;&#26159;&#19968;&#31181;&#30417;&#30563;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#24456;&#22823;&#30340;&#35745;&#31639;&#36127;&#25285;&#26469;&#20248;&#21270;&#28151;&#21512;&#25513;&#27169;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#33879;&#24615;&#24863;&#30693;&#28151;&#21512;&#26041;&#27861;GuidedMixup&#65292;&#23427;&#26088;&#22312;&#22312;&#28151;&#21512;&#22270;&#20687;&#20013;&#20445;&#30041;&#26174;&#33879;&#21306;&#22495;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37197;&#23545;&#31639;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#37197;&#23545;&#22270;&#20687;&#20013;&#26174;&#33879;&#21306;&#22495;&#30340;&#20914;&#31361;&#65292;&#24182;&#22312;&#28151;&#21512;&#22270;&#20687;&#20013;&#23454;&#29616;&#20016;&#23500;&#30340;&#26174;&#33879;&#24615;&#12290;&#27492;&#22806;&#65292;GuidedMixup&#36890;&#36807;&#24179;&#28369;&#22320;&#25554;&#20540;&#20004;&#20010;&#37197;&#23545;&#22270;&#20687;&#26469;&#25511;&#21046;&#27599;&#20010;&#20687;&#32032;&#30340;&#28151;&#21512;&#27604;&#20363;&#65292;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#26174;&#33879;&#21306;&#22495;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GuidedMixup&#22312;&#22686;&#24378;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20102;&#19968;&#20010;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is now an essential part of the image training process, as it effectively prevents overfitting and makes the model more robust against noisy datasets. Recent mixing augmentation strategies have advanced to generate the mixup mask that can enrich the saliency information, which is a supervisory signal. However, these methods incur a significant computational burden to optimize the mixup mask. From this motivation, we propose a novel saliency-aware mixup method, GuidedMixup, which aims to retain the salient regions in mixup images with low computational overhead. We develop an efficient pairing algorithm that pursues to minimize the conflict of salient regions of paired images and achieve rich saliency in mixup images. Moreover, GuidedMixup controls the mixup ratio for each pixel to better preserve the salient region by interpolating two paired images smoothly. The experiments on several datasets demonstrate that GuidedMixup provides a good trade-off between augmentatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#31232;&#30095;&#25512;&#26029;&#36719;&#20214;&#21152;&#36895;&#22120;&#65292;&#22312;CPU&#19978;&#21033;&#29992;Intel Deep Learning Boost&#23454;&#29616;&#20102;&#31232;&#30095;&#30697;&#38453;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#30340;&#20248;&#21270;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#31232;&#30095;&#24211;&#65292;&#22312;&#21508;&#31181;&#24418;&#29366;&#21644;&#31232;&#30095;&#24230;&#19979;&#37117;&#33719;&#24471;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.16601</link><description>&lt;p&gt;
&#29992;&#20110;CPU&#19978;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#31232;&#30095;&#25512;&#26029;&#36719;&#20214;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs. (arXiv:2306.16601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#31232;&#30095;&#25512;&#26029;&#36719;&#20214;&#21152;&#36895;&#22120;&#65292;&#22312;CPU&#19978;&#21033;&#29992;Intel Deep Learning Boost&#23454;&#29616;&#20102;&#31232;&#30095;&#30697;&#38453;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#30340;&#20248;&#21270;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#31232;&#30095;&#24211;&#65292;&#22312;&#21508;&#31181;&#24418;&#29366;&#21644;&#31232;&#30095;&#24230;&#19979;&#37117;&#33719;&#24471;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#20005;&#26684;&#30340;&#21534;&#21520;&#37327;&#21644;&#24310;&#36831;&#35201;&#27714;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#37319;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#31561;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26469;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#36816;&#34892;&#26102;&#23545;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#32570;&#20047;&#20805;&#20998;&#30340;&#25903;&#25345;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#25512;&#26029;&#36719;&#20214;&#22534;&#26632;&#65292;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#26435;&#37325;&#20351;&#29992;&#24658;&#23450;&#30340;&#22359;&#22823;&#23567;&#36827;&#34892;&#21098;&#26525;&#12290;&#25105;&#20204;&#30340;&#31232;&#30095;&#36719;&#20214;&#21152;&#36895;&#22120;&#21033;&#29992;Intel Deep Learning Boost&#22312;CPU&#19978;&#26368;&#22823;&#21270;&#31232;&#30095;&#30697;&#38453;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#65288;&#36890;&#24120;&#34987;&#32553;&#20889;&#20026;SpMM&#65289;&#30340;&#24615;&#33021;&#12290;&#22312;&#24191;&#27867;&#30340;GEMM&#24418;&#29366;&#21644;5&#20010;&#20195;&#34920;&#24615;&#31232;&#30095;&#24230;&#27700;&#24179;&#19979;&#65292;&#25105;&#20204;&#30340;SpMM&#20869;&#26680;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#31232;&#30095;&#24211;&#65288;oneMKL&#12289;TVM&#21644;LIBXSMM&#65289;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformer-based language models have become the standard approach for natural language processing tasks. However, stringent throughput and latency requirements in industrial applications are limiting their adoption. To mitigate the gap, model compression techniques such as structured pruning are being used to improve inference efficiency. However, most existing neural network inference runtimes lack adequate support for structured sparsity. In this paper, we propose an efficient sparse deep learning inference software stack for Transformer-based language models where the weights are pruned with constant block size. Our sparse software accelerator leverages Intel Deep Learning Boost to maximize the performance of sparse matrix - dense matrix multiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel outperforms the existing sparse libraries (oneMKL, TVM, and LIBXSMM) by an order of magnitude on a wide range of GEMM shapes under 5 representative sparsity ra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26174;&#33879;&#24615;&#35757;&#32451;&#26080;&#27861;&#25552;&#39640;&#20854;&#22312;&#23545;&#25239;&#24615;&#31034;&#20363;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16581</link><description>&lt;p&gt;
&#36890;&#36807;&#26174;&#33879;&#24615;&#35757;&#32451;&#20026;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24102;&#26469;&#40065;&#26834;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Saliency-Based Training bring Robustness for Deep Neural Networks in Image Classification?. (arXiv:2306.16581v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16581
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26174;&#33879;&#24615;&#35757;&#32451;&#26080;&#27861;&#25552;&#39640;&#20854;&#22312;&#23545;&#25239;&#24615;&#31034;&#20363;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#29702;&#35299;&#22797;&#26434;&#27169;&#24335;&#21644;&#20570;&#20986;&#20915;&#31574;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40657;&#31665;&#29305;&#24615;&#38459;&#30861;&#20102;&#23545;&#20854;&#20869;&#37096;&#24037;&#20316;&#30340;&#23436;&#20840;&#29702;&#35299;&#12290;&#34429;&#28982;&#22312;&#32447;&#26174;&#33879;&#24615;&#24341;&#23548;&#35757;&#32451;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#26126;&#26174;&#29305;&#24449;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26174;&#28982;&#38750;&#24120;&#35268;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#26159;&#21542;&#23545;&#40784;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#36807;&#26174;&#33879;&#24615;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#25239;&#24615;&#31034;&#20363;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#12290;&#37319;&#29992;&#22312;&#32447;&#26174;&#33879;&#24615;&#24341;&#23548;&#35757;&#32451;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#24120;&#35265;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#40065;&#26834;&#24615;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#23613;&#31649;&#27169;&#22411;&#36755;&#20986;&#20013;&#26377;&#33391;&#22909;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#20294;&#26174;&#33879;&#24615;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#23545;&#25239;&#24615;&#31034;&#20363;&#25915;&#20987;&#20013;&#30340;&#24615;&#33021;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are powerful tools to understand complex patterns and making decisions. However, their black-box nature impedes a complete understanding of their inner workings. While online saliency-guided training methods try to highlight the prominent features in the model's output to alleviate this problem, it is still ambiguous if the visually explainable features align with robustness of the model against adversarial examples. In this paper, we investigate the saliency trained model's vulnerability to adversarial examples methods. Models are trained using an online saliency-guided training method and evaluated against popular algorithms of adversarial examples. We quantify the robustness and conclude that despite the well-explained visualizations in the model's output, the salient models suffer from the lower performance against adversarial examples attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#24635;&#32467;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#36129;&#29486;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.16559</link><description>&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#65306;&#23545;&#23646;&#24615;&#38388;&#21327;&#20316;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Feature Selection: A perspective on inter-attribute cooperation. (arXiv:2306.16559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#24635;&#32467;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#36129;&#29486;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#23545;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#20219;&#21153;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#29305;&#24449;&#36873;&#25321;&#26159;&#22788;&#29702;&#32500;&#24230;&#32553;&#20943;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#65292;&#36890;&#24120;&#26159;&#22312;&#24212;&#29992;&#23398;&#20064;&#31639;&#27861;&#20043;&#21069;&#30340;&#37325;&#35201;&#25968;&#25454;&#22788;&#29702;&#27493;&#39588;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20174;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#30456;&#20851;&#24615;&#25490;&#24207;&#31639;&#27861;&#21457;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;-&#20887;&#20313;&#26435;&#34913;&#21644;&#22522;&#20110;&#22810;&#20803;&#20381;&#36182;&#24615;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#25429;&#25417;&#22810;&#21464;&#37327;&#20381;&#36182;&#30340;&#36235;&#21183;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#38388;&#30340;&#20114;&#30456;&#21512;&#20316;&#33719;&#21462;&#20851;&#20110;&#31867;&#21035;&#30340;&#29420;&#29305;&#20449;&#24687;&#12290;&#26412;&#25991;&#23545;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#24182;&#24635;&#32467;&#20102;&#25991;&#29486;&#20013;&#19981;&#21516;&#26041;&#27861;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional datasets depict a challenge for learning tasks in data mining and machine learning. Feature selection is an effective technique in dealing with dimensionality reduction. It is often an essential data processing step prior to applying a learning algorithm. Over the decades, filter feature selection methods have evolved from simple univariate relevance ranking algorithms to more sophisticated relevance-redundancy trade-offs and to multivariate dependencies-based approaches in recent years. This tendency to capture multivariate dependence aims at obtaining unique information about the class from the intercooperation among features. This paper presents a comprehensive survey of the state-of-the-art work on filter feature selection methods assisted by feature intercooperation, and summarizes the contributions of different approaches found in the literature. Furthermore, current issues and challenges are introduced to identify promising future research and development.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;-&#26368;&#22823;F-&#25955;&#24230;&#27491;&#21017;&#21270;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;F-&#25955;&#24230;&#34913;&#37327;&#20844;&#24179;&#24615;&#65292;&#24182;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.16552</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;-&#26368;&#22823;F-&#25955;&#24230;&#27491;&#21017;&#21270;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Fair Classifiers via Min-Max F-divergence Regularization. (arXiv:2306.16552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;-&#26368;&#22823;F-&#25955;&#24230;&#27491;&#21017;&#21270;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;F-&#25955;&#24230;&#34913;&#37327;&#20844;&#24179;&#24615;&#65292;&#24182;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#22312;&#25191;&#27861;&#12289;&#21009;&#20107;&#21496;&#27861;&#12289;&#37329;&#34701;&#12289;&#25307;&#32856;&#21644;&#24405;&#21462;&#31561;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#65292;&#30830;&#20445;ML&#36741;&#21161;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#20851;&#27880;&#20844;&#24179;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#23567;-&#26368;&#22823;F-&#25955;&#24230;&#27491;&#21017;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#27169;&#22411;&#24182;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#21487;&#35757;&#32451;&#30340;&#32593;&#32476;&#32452;&#25104;&#65292;&#21363;&#20998;&#31867;&#22120;&#32593;&#32476;&#21644;&#20559;&#24046;/&#20844;&#24179;&#24615;&#20272;&#35745;&#22120;&#32593;&#32476;&#65292;&#20854;&#20013;&#20844;&#24179;&#24615;&#20351;&#29992;&#32479;&#35745;&#27010;&#24565;&#30340;F-&#25955;&#24230;&#36827;&#34892;&#34913;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;F-&#25955;&#24230;&#24230;&#37327;&#20855;&#26377;&#20984;&#24615;&#21644;&#21487;&#24494;&#24615;&#30340;&#29305;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#21464;&#20998;&#34920;&#31034;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#26041;&#27861;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#26041;&#20415;&#22320;&#36866;&#24212;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;F-&#25955;&#24230;&#30340;&#35757;&#32451;&#33539;&#24335;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning (ML) based systems are adopted in domains such as law enforcement, criminal justice, finance, hiring and admissions, ensuring the fairness of ML aided decision-making is becoming increasingly important. In this paper, we focus on the problem of fair classification, and introduce a novel min-max F-divergence regularization framework for learning fair classification models while preserving high accuracy. Our framework consists of two trainable networks, namely, a classifier network and a bias/fairness estimator network, where the fairness is measured using the statistical notion of F-divergence. We show that F-divergence measures possess convexity and differentiability properties, and their variational representation make them widely applicable in practical gradient based training methods. The proposed framework can be readily adapted to multiple sensitive attributes and for high dimensional datasets. We study the F-divergence based training paradigm for two types of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#20013;&#21069;&#26223;-&#32972;&#26223;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21457;&#29616;&#35813;&#38382;&#39064;&#20250;&#23548;&#33268;&#26816;&#27979;&#24615;&#33021;&#19979;&#38477;&#65292;&#23588;&#20854;&#24403;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#26102;&#24433;&#21709;&#26356;&#22823;&#12290;&#21516;&#26102;&#65292;&#20943;&#23567;&#30446;&#26631;&#22823;&#23567;&#20063;&#20250;&#22686;&#21152;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16539</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#20013;&#21069;&#26223;-&#32972;&#26223;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#31995;&#32479;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A systematic study of the foreground-background imbalance problem in deep learning for object detection. (arXiv:2306.16539v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#20013;&#21069;&#26223;-&#32972;&#26223;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21457;&#29616;&#35813;&#38382;&#39064;&#20250;&#23548;&#33268;&#26816;&#27979;&#24615;&#33021;&#19979;&#38477;&#65292;&#23588;&#20854;&#24403;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#26102;&#24433;&#21709;&#26356;&#22823;&#12290;&#21516;&#26102;&#65292;&#20943;&#23567;&#30446;&#26631;&#22823;&#23567;&#20063;&#20250;&#22686;&#21152;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#24050;&#32463;&#22312;&#22810;&#20010;&#30740;&#31350;&#20013;&#34987;&#25506;&#35752;&#36807;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#36825;&#20010;&#29616;&#35937;&#36827;&#34892;&#31995;&#32479;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#20013;&#24191;&#27867;&#23384;&#22312;&#30340;&#21069;&#26223;-&#32972;&#26223;&#65288;F-B&#65289;&#19981;&#24179;&#34913;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#35813;&#38382;&#39064;&#24448;&#24448;&#26159;&#30001;&#20110;&#23567;&#30340;&#12289;&#19981;&#39057;&#32321;&#20986;&#29616;&#30340;&#24863;&#20852;&#36259;&#23545;&#35937;&#24341;&#36215;&#30340;&#12290;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;F-B&#19981;&#24179;&#34913;&#30340;&#19981;&#21516;&#26041;&#38754;&#65288;&#30446;&#26631;&#22823;&#23567;&#12289;&#30446;&#26631;&#25968;&#37327;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#30446;&#26631;&#31867;&#22411;&#65289;&#23545;&#26816;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;9&#31181;&#20027;&#27969;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;Faster-RCNN&#12289;SSD&#12289;OHEM&#12289;Libra-RCNN&#12289;Focal-Loss&#12289;GHM&#12289;PISA&#12289;YOLO-v3&#21644;GFL&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#25104;&#20687;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65306;&#65288;1&#65289;F-B&#19981;&#24179;&#34913;&#30830;&#23454;&#20250;&#23548;&#33268;&#26816;&#27979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#65288;2&#65289;&#24403;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#26102;&#65292;&#26816;&#27979;&#24615;&#33021;&#26356;&#21463;&#21040;F-B&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#65292;&#65288;3&#65289;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#20943;&#23567;&#30446;&#26631;&#22823;&#23567;&#20250;&#22686;&#21152;F-B&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The class imbalance problem in deep learning has been explored in several studies, but there has yet to be a systematic analysis of this phenomenon in object detection. Here, we present comprehensive analyses and experiments of the foreground-background (F-B) imbalance problem in object detection, which is very common and caused by small, infrequent objects of interest. We experimentally study the effects of different aspects of F-B imbalance (object size, number of objects, dataset size, object type) on detection performance. In addition, we also compare 9 leading methods for addressing this problem, including Faster-RCNN, SSD, OHEM, Libra-RCNN, Focal-Loss, GHM, PISA, YOLO-v3, and GFL with a range of datasets from different imaging domains. We conclude that (1) the F-B imbalance can indeed cause a significant drop in detection performance, (2) The detection performance is more affected by F-B imbalance when fewer training data are available, (3) in most cases, decreasing object size l
&lt;/p&gt;</description></item><item><title>CLANet&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#20142;&#22330;&#22270;&#20687;&#30340;&#36328;&#25209;&#32454;&#32990;&#31995;&#37492;&#23450;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#32990;&#32858;&#31867;&#32423;&#21035;&#30340;&#36873;&#25321;&#26041;&#27861;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#25209;&#27425;&#25928;&#24212;&#23548;&#33268;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32454;&#32990;&#31995;&#37492;&#23450;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16538</link><description>&lt;p&gt;
CLANet: &#19968;&#31181;&#20840;&#38754;&#30340;&#22522;&#20110;&#20142;&#22330;&#22270;&#20687;&#30340;&#25209;&#38388;&#32454;&#32990;&#31995;&#37492;&#23450;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CLANet: A Comprehensive Framework for Cross-Batch Cell Line Identification Using Brightfield Images. (arXiv:2306.16538v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16538
&lt;/p&gt;
&lt;p&gt;
CLANet&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#20142;&#22330;&#22270;&#20687;&#30340;&#36328;&#25209;&#32454;&#32990;&#31995;&#37492;&#23450;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#32990;&#32858;&#31867;&#32423;&#21035;&#30340;&#36873;&#25321;&#26041;&#27861;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#25209;&#27425;&#25928;&#24212;&#23548;&#33268;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32454;&#32990;&#31995;&#37492;&#23450;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#31995;&#37492;&#23450;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#30830;&#20445;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#20934;&#30830;&#37492;&#23450;&#30340;&#32454;&#32990;&#12290;&#36890;&#36807;&#30740;&#31350;&#32454;&#32990;&#24418;&#24577;&#29305;&#24449;&#65292;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#22312;&#32454;&#32990;&#31995;&#37492;&#23450;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25209;&#27425;&#25928;&#24212;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#25968;&#25454;&#29983;&#25104;&#30340;&#19981;&#21516;&#26102;&#38388;&#23548;&#33268;&#20102;&#28508;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#24040;&#22823;&#20559;&#31227;&#65292;&#20174;&#32780;&#20351;&#20174;&#19981;&#21516;&#25209;&#27425;&#22521;&#20859;&#30340;&#32454;&#32990;&#31995;&#20043;&#38388;&#30340;&#21487;&#38752;&#21306;&#20998;&#21464;&#24471;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CLANet&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20142;&#22330;&#22270;&#20687;&#30340;&#36328;&#25209;&#32454;&#32990;&#31995;&#37492;&#23450;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#26469;&#35299;&#20915;&#19977;&#31181;&#19981;&#21516;&#30340;&#25209;&#27425;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#32990;&#32858;&#31867;&#32423;&#21035;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#32454;&#32990;&#23494;&#24230;&#21464;&#21270;&#65292;&#24182;&#37319;&#21462;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#22270;&#20687;&#36136;&#37327;&#21464;&#21270;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#38752;&#30340;&#34917;&#19969;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#26469;&#24314;&#31435;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#25209;&#27425;&#30340;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#37492;&#23450;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cell line authentication plays a crucial role in the biomedical field, ensuring researchers work with accurately identified cells. Supervised deep learning has made remarkable strides in cell line identification by studying cell morphological features through cell imaging. However, batch effects, a significant issue stemming from the different times at which data is generated, lead to substantial shifts in the underlying data distribution, thus complicating reliable differentiation between cell lines from distinct batch cultures. To address this challenge, we introduce CLANet, a pioneering framework for cross-batch cell line identification using brightfield images, specifically designed to tackle three distinct batch effects. We propose a cell cluster-level selection method to efficiently capture cell density variations, and a self-supervised learning strategy to manage image quality variations, thus producing reliable patch representations. Additionally, we adopt multiple instance lea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16533</link><description>&lt;p&gt;
ICSVR: &#22312;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30740;&#31350;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models. (arXiv:2306.16533v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16533
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26816;&#32034;&#65288;VR&#65289;&#28041;&#21450;&#26681;&#25454;&#25991;&#26412;&#26631;&#39064;&#26816;&#32034;&#35270;&#39057;&#25968;&#25454;&#24211;&#20013;&#30340;&#30495;&#23454;&#35270;&#39057;&#65292;&#25110;&#21453;&#20043;&#20134;&#28982;&#12290;&#21512;&#25104;&#24615;&#30340;&#20004;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#23545;&#35937;&#21644;&#23646;&#24615;&#20197;&#21450;&#21160;&#20316;&#65292;&#20351;&#29992;&#27491;&#30830;&#30340;&#35821;&#20041;&#32852;&#32467;&#20197;&#24418;&#25104;&#27491;&#30830;&#30340;&#25991;&#26412;&#26597;&#35810;&#12290;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#65288;&#23545;&#35937;&#21644;&#23646;&#24615;&#12289;&#21160;&#20316;&#21644;&#35821;&#20041;&#65289;&#21508;&#33258;&#22312;&#24110;&#21161;&#21306;&#20998;&#35270;&#39057;&#21644;&#26816;&#32034;&#27491;&#30830;&#30340;&#30495;&#23454;&#35270;&#39057;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#22914;MSRVTT&#12289;MSVD&#21644;DIDEMO&#12290;&#35813;&#30740;&#31350;&#38024;&#23545;&#20004;&#31867;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#20102;&#65292;&#19968;&#31867;&#26159;&#22312;&#35270;&#39057;&#25991;&#26412;&#23545;&#19978;&#39044;&#35757;&#32451;&#24182;&#22312;&#19979;&#28216;&#35270;&#39057;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#65288;&#20363;&#22914;&#65292;Frozen-in-Time&#12289;Violet&#12289;MCQ&#31561;&#65289;&#65292;&#21478;&#19968;&#31867;&#26159;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25991;&#26412;&#34920;&#31034;&#65288;&#22914;CLIP&#65289;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video retrieval (VR) involves retrieving the ground truth video from the video database given a text caption or vice-versa. The two important components of compositionality: objects \&amp; attributes and actions are joined using correct semantics to form a proper text query. These components (objects \&amp; attributes, actions and semantics) each play an important role to help distinguish among videos and retrieve the correct ground truth video. However, it is unclear what is the effect of these components on the video retrieval performance. We therefore, conduct a systematic study to evaluate the compositional and semantic understanding of video retrieval models on standard benchmarks such as MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video retrieval models: (i) which are pre-trained on video-text pairs and fine-tuned on downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.) (ii) which adapt pre-trained image-text representations like CLIP for vid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP&#30340;&#25628;&#32034;&#24341;&#25806;&#23454;&#29616;&#26041;&#26696;&#65292;&#29992;&#20110;Iconclass&#22270;&#20687;&#20998;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#25110;&#25991;&#26412;&#26597;&#35810;&#26469;&#26816;&#32034;&#21644;&#25506;&#32034;Iconclass&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2306.16529</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;Iconclass&#19978;&#36827;&#34892;&#22810;&#27169;&#24577;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multimodal Search on Iconclass using Vision-Language Pre-Trained Models. (arXiv:2306.16529v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP&#30340;&#25628;&#32034;&#24341;&#25806;&#23454;&#29616;&#26041;&#26696;&#65292;&#29992;&#20110;Iconclass&#22270;&#20687;&#20998;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#25110;&#25991;&#26412;&#26597;&#35810;&#26469;&#26816;&#32034;&#21644;&#25506;&#32034;Iconclass&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26415;&#35821;&#26469;&#28304;&#65292;&#22914;&#21463;&#25511;&#35789;&#27719;&#12289;&#35789;&#34920;&#21644;&#20998;&#31867;&#31995;&#32479;&#65292;&#22312;&#25968;&#23383;&#21270;&#25991;&#21270;&#36951;&#20135;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20801;&#35768;&#26597;&#35810;&#21644;&#25506;&#32034;&#36825;&#20123;&#35789;&#27719;&#36164;&#28304;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#24448;&#24448;&#32570;&#20047;&#23545;&#29992;&#25143;&#25628;&#32034;&#32972;&#21518;&#35821;&#20041;&#30340;&#36866;&#24403;&#34920;&#31034;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#34920;&#36798;&#26041;&#24335;&#20256;&#36798;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#12289;&#20851;&#38190;&#35789;&#25110;&#25991;&#26412;&#25551;&#36848;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#24341;&#25806;&#23454;&#29616;&#26041;&#26696;&#65292;&#29992;&#20110;&#20854;&#20013;&#19968;&#31181;&#26368;&#24120;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#31995;&#32479;Iconclass&#12290;&#35813;&#31995;&#32479;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273; - &#35821;&#35328;&#27169;&#22411;CLIP&#26469;&#26816;&#32034;&#21644;&#25506;&#32034;Iconclass&#27010;&#24565;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#35270;&#35273;&#26597;&#35810;&#36824;&#26159;&#25991;&#26412;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Terminology sources, such as controlled vocabularies, thesauri and classification systems, play a key role in digitizing cultural heritage. However, Information Retrieval (IR) systems that allow to query and explore these lexical resources often lack an adequate representation of the semantics behind the user's search, which can be conveyed through multiple expression modalities (e.g., images, keywords or textual descriptions). This paper presents the implementation of a new search engine for one of the most widely used iconography classification system, Iconclass. The novelty of this system is the use of a pre-trained vision-language model, namely CLIP, to retrieve and explore Iconclass concepts using visual or textual queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#39715;&#29399;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#23427;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#26469;&#35299;&#20915;PDE&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27714;&#35299;PDE&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.16524</link><description>&lt;p&gt;
HNO&#65306;&#29992;&#20110;&#35299;&#20915;PDE&#30340;&#39715;&#29399;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
HNO: Hyena Neural Operator for solving PDEs. (arXiv:2306.16524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#39715;&#29399;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#23427;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#26469;&#35299;&#20915;PDE&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27714;&#35299;PDE&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#36890;&#24120;&#38656;&#35201;&#31934;&#32454;&#31163;&#25955;&#21270;&#20197;&#35299;&#26512;&#24517;&#35201;&#30340;&#26102;&#31354;&#23610;&#24230;&#65292;&#36825;&#21487;&#33021;&#20250;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;PDE&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#12290;&#31070;&#32463;&#31639;&#23376;&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#23398;&#20064;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#33021;&#22815;&#22522;&#20110;&#25968;&#25454;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;&#39715;&#29399;&#65288;Hyena&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#37319;&#29992;&#30001;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#12290;&#39715;&#29399;&#31639;&#23376;&#26159;&#19968;&#31181;&#20855;&#26377;&#27425;&#32447;&#24615;&#22797;&#26434;&#24615;&#30340;&#25805;&#20316;&#65292;&#23427;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#21442;&#25968;&#21270;&#20855;&#26377;&#20840;&#23616;&#24863;&#21463;&#37326;&#30340;&#38271;&#21367;&#31215;&#12290;&#36825;&#31181;&#26426;&#21046;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#33021;&#22815;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#12290;&#20026;&#20102;&#34913;&#37327;&#21508;&#20010;&#23618;&#22312;&#35299;&#20915;PDE&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerically solving partial differential equations (PDEs) typically requires fine discretization to resolve necessary spatiotemporal scales, which can be computationally expensive. Recent advances in deep learning have provided a new approach to solving PDEs that involves the use of neural operators. Neural operators are neural network architectures that learn mappings between function spaces and have the capability to solve partial differential equations based on data. This study utilizes a novel neural operator called Hyena, which employs a long convolutional filter that is parameterized by a multilayer perceptron. The Hyena operator is an operation that enjoys sub-quadratic complexity and state space model to parameterize long convolution that enjoys global receptive field. This mechanism enhances the model's comprehension of the input's context and enables data-dependent weight for different PDE instances. To measure how effective the layers are in solving PDEs, we conduct experime
&lt;/p&gt;</description></item><item><title>SARC&#26159;&#19968;&#20010;&#22522;&#20110;SAC&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#35780;&#35770;&#32773;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#26799;&#24230;&#20272;&#35745;&#65292;&#20026;&#28436;&#21592;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.16503</link><description>&lt;p&gt;
SARC: &#36719;&#21442;&#28436;&#32773;&#22238;&#39038;&#35780;&#35770;&#32773;
&lt;/p&gt;
&lt;p&gt;
SARC: Soft Actor Retrospective Critic. (arXiv:2306.16503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16503
&lt;/p&gt;
&lt;p&gt;
SARC&#26159;&#19968;&#20010;&#22522;&#20110;SAC&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#35780;&#35770;&#32773;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#26799;&#24230;&#20272;&#35745;&#65292;&#20026;&#28436;&#21592;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SAC&#26159;&#19968;&#20010;&#28436;&#21592;&#35780;&#35770;&#32773;&#31639;&#27861;&#65292;&#20854;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#29305;&#24615;&#22312;&#20110;&#35780;&#35770;&#32773;&#20272;&#35745;&#22312;&#20219;&#20309;&#32473;&#23450;&#26102;&#38388;&#37117;&#27809;&#26377;&#25910;&#25947;&#20110;&#28436;&#21592;&#65292;&#20294;&#30001;&#20110;&#35780;&#35770;&#32773;&#23398;&#20064;&#36895;&#24230;&#27604;&#28436;&#21592;&#24555;&#65292;&#23427;&#30830;&#20445;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#26368;&#32456;&#19968;&#33268;&#24615;&#12290;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#26799;&#24230;&#20272;&#35745;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#12290;&#30001;&#20110;&#26799;&#24230;&#20272;&#35745;&#20381;&#36182;&#20110;&#35780;&#35770;&#32773;&#65292;&#25105;&#20204;&#35748;&#20026;&#25913;&#36827;&#35780;&#35770;&#32773;&#21487;&#20197;&#20026;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#28436;&#21592;&#25552;&#20379;&#26356;&#22909;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#21442;&#28436;&#32773;&#22238;&#39038;&#35780;&#35770;&#32773;(SARC)&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;SAC&#35780;&#35770;&#32773;&#25439;&#22833;&#19982;&#21478;&#19968;&#20010;&#25439;&#22833;&#39033;&#22238;&#39038;&#25439;&#22833;&#30456;&#32467;&#21512; - &#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#35780;&#35770;&#32773;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#28436;&#21592;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;SAC&#23454;&#29616;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36866;&#24212;SARC&#65292;&#21482;&#38656;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SARC&#30456;&#27604;S&#30340;&#19968;&#33268;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The two-time scale nature of SAC, which is an actor-critic algorithm, is characterised by the fact that the critic estimate has not converged for the actor at any given time, but since the critic learns faster than the actor, it ensures eventual consistency between the two. Various strategies have been introduced in literature to learn better gradient estimates to help achieve better convergence. Since gradient estimates depend upon the critic, we posit that improving the critic can provide a better gradient estimate for the actor at each time. Utilizing this, we propose Soft Actor Retrospective Critic (SARC), where we augment the SAC critic loss with another loss term retrospective loss - leading to faster critic convergence and consequently, better policy gradient estimates for the actor. An existing implementation of SAC can be easily adapted to SARC with minimal modifications. Through extensive experimentation and analysis, we show that SARC provides consistent improvement over S
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;Twitter&#25968;&#25454;&#27969;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2306.16495</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#27969;&#20013;&#30340;&#20107;&#20214;&#26816;&#27979;&#65306;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Event Detection from Social Media Stream: Methods, Datasets and Opportunities. (arXiv:2306.16495v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;Twitter&#25968;&#25454;&#27969;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#27969;&#21253;&#21547;&#22823;&#37327;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#65292;&#20174;&#26085;&#24120;&#29983;&#27963;&#25925;&#20107;&#21040;&#26368;&#26032;&#30340;&#20840;&#29699;&#21644;&#24403;&#22320;&#20107;&#20214;&#21644;&#26032;&#38395;&#12290;&#29305;&#21035;&#26159;Twitter&#65292;&#20801;&#35768;&#24555;&#36895;&#20256;&#25773;&#23454;&#26102;&#21457;&#29983;&#30340;&#20107;&#20214;&#65292;&#24182;&#20351;&#20010;&#20154;&#21644;&#32452;&#32455;&#33021;&#22815;&#21450;&#26102;&#20102;&#35299;&#24403;&#21069;&#20107;&#20214;&#12290;&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#26816;&#27979;&#20107;&#20214;&#19982;&#20256;&#32479;&#25991;&#26412;&#38754;&#20020;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#26159;&#36817;&#24180;&#26469;&#21463;&#21040;&#20851;&#27880;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;Twitter&#25968;&#25454;&#27969;&#30340;&#24191;&#27867;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#24110;&#21161;&#35835;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20123;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media streams contain large and diverse amount of information, ranging from daily-life stories to the latest global and local events and news. Twitter, especially, allows a fast spread of events happening real time, and enables individuals and organizations to stay informed of the events happening now. Event detection from social media data poses different challenges from traditional text and is a research area that has attracted much attention in recent years. In this paper, we survey a wide range of event detection methods for Twitter data stream, helping readers understand the recent development in this area. We present the datasets available to the public. Furthermore, a few research opportunities
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#26222;&#36866;&#30340;&#20132;&#20114;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#29305;&#24449;&#24402;&#22240;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.16431</link><description>&lt;p&gt;
&#22686;&#24378;&#27169;&#22411;&#26222;&#36866;&#30340;&#20132;&#20114;&#24335;&#29305;&#24449;&#24402;&#22240;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Increasing Performance And Sample Efficiency With Model-agnostic Interactive Feature Attributions. (arXiv:2306.16431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#26222;&#36866;&#30340;&#20132;&#20114;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#29305;&#24449;&#24402;&#22240;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#26222;&#36866;&#30340;&#29305;&#24449;&#24402;&#22240;&#21487;&#20197;&#20026;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#23616;&#37096;&#27934;&#23519;&#21147;&#12290;&#22914;&#26524;&#35299;&#37322;&#26159;&#27491;&#30830;&#30340;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#39564;&#35777;&#21644;&#20449;&#20219;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#23427;&#19982;&#19987;&#23478;&#30340;&#30693;&#35782;&#30456;&#30683;&#30462;&#65292;&#30456;&#20851;&#24037;&#20316;&#21482;&#32416;&#27491;&#20102;&#26080;&#20851;&#30340;&#29305;&#24449;&#20197;&#25913;&#36827;&#27169;&#22411;&#12290;&#20026;&#20102;&#20801;&#35768;&#26080;&#38480;&#30340;&#20132;&#20114;&#65292;&#26412;&#25991;&#38024;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;&#36974;&#34109;&#27861;&#21644;&#27801;&#26222;&#21033;&#20540;&#65289;&#25552;&#20379;&#20102;&#27169;&#22411;&#26222;&#36866;&#30340;&#23454;&#29616;&#65292;&#20197;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#24378;&#21046;&#25191;&#34892;&#23436;&#20840;&#19981;&#21516;&#30340;&#24402;&#22240;&#12290;&#23545;&#20110;&#29305;&#23450;&#30340;&#26679;&#26412;&#38598;&#65292;&#25105;&#20204;&#20351;&#29992;&#32416;&#27491;&#30340;&#29305;&#24449;&#24402;&#22240;&#26469;&#29983;&#25104;&#39069;&#22806;&#30340;&#23616;&#37096;&#25968;&#25454;&#65292;&#29992;&#20110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#26679;&#26412;&#36827;&#34892;&#27491;&#30830;&#35299;&#37322;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#36827;&#34892;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#22522;&#20110;&#32416;&#27491;&#30340;&#35299;&#37322;&#26469;&#25193;&#20805;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23558;&#25105;&#20204;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#28155;&#21152;&#21040;&#20027;&#21160;&#23398;&#20064;&#35774;&#32622;&#20013;&#21487;&#20197;&#22686;&#21152;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic feature attributions can provide local insights in complex ML models. If the explanation is correct, a domain expert can validate and trust the model's decision. However, if it contradicts the expert's knowledge, related work only corrects irrelevant features to improve the model. To allow for unlimited interaction, in this paper we provide model-agnostic implementations for two popular explanation methods (Occlusion and Shapley values) to enforce entirely different attributions in the complex model. For a particular set of samples, we use the corrected feature attributions to generate extra local data, which is used to retrain the model to have the right explanation for the samples. Through simulated and real data experiments on a variety of models we show how our proposed approach can significantly improve the model's performance only by augmenting its training dataset based on corrected explanations. Adding our interactive explanations to active learning settings incr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16424</link><description>&lt;p&gt;
&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#29992;&#20110;&#21453;&#27927;&#38065;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37329;&#34701;&#30340;&#24191;&#27867;&#25968;&#23383;&#21270;&#21644;&#21152;&#23494;&#36135;&#24065;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#35774;&#35745;&#30340;&#27450;&#35784;&#26041;&#26696;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#27927;&#38065;&#8212;&#8212;&#23558;&#38750;&#27861;&#36164;&#37329;&#31227;&#21160;&#20197;&#25513;&#30422;&#20854;&#26469;&#28304;&#8212;&#8212;&#21487;&#20197;&#36328;&#36234;&#38134;&#34892;&#21644;&#22269;&#30028;&#65292;&#20135;&#29983;&#22797;&#26434;&#30340;&#20132;&#26131;&#27169;&#24335;&#12290;&#32852;&#21512;&#22269;&#20272;&#35745;&#27599;&#24180;&#20840;&#29699;&#27927;&#38065;&#37329;&#39069;&#21344;&#20840;&#29699;GDP&#30340;2-5%&#65292;&#32422;&#20026;0.8-2.0&#19975;&#20159;&#32654;&#20803;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36890;&#24120;&#26080;&#27861;&#33719;&#24471;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#27927;&#38065;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#19988;&#20043;&#21069;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#23384;&#22312;&#26174;&#33879;&#32570;&#38519;&#12290;&#20026;&#20102;&#27604;&#36739;&#27169;&#22411;&#24182;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#38656;&#35201;&#19968;&#20010;&#36924;&#30495;&#12289;&#26631;&#20934;&#21270;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26681;&#25454;&#23454;&#38469;&#20132;&#26131;&#23613;&#21487;&#33021;&#22320;&#26657;&#20934;&#20102;&#36825;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.  To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16125</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65306;MentalRiskES@IberLEF 2023
&lt;/p&gt;
&lt;p&gt;
A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023. (arXiv:2306.16125v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#19982;IberLEF 2023&#30340;MentalRiskES&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#26681;&#25454;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#27963;&#21160;&#26469;&#39044;&#27979;&#20182;&#20204;&#21487;&#33021;&#24739;&#25233;&#37057;&#30151;&#30340;&#21487;&#33021;&#24615;&#12290;&#25968;&#25454;&#38598;&#30001;175&#20010;Telegram&#29992;&#25143;&#30340;&#23545;&#35805;&#32452;&#25104;&#65292;&#27599;&#20010;&#29992;&#25143;&#26681;&#25454;&#20182;&#20204;&#24739;&#30149;&#35777;&#25454;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65306;&#20108;&#20998;&#31867;&#12289;&#31616;&#21333;&#22238;&#24402;&#12289;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#22810;&#31867;&#21035;&#22238;&#24402;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#21035;&#22238;&#24402;&#38382;&#39064;&#65292;&#28982;&#21518;&#23558;&#39044;&#27979;&#32467;&#26524;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#20854;&#20182;&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#24314;&#27169;&#26041;&#27861;&#30340;&#24615;&#33021;&#65306;&#23545;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21644;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#65292;&#21518;&#32773;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#22797;&#29616;&#25105;&#20204;&#32467;&#26524;&#30340;&#20195;&#30721;&#65306;https://github.com/simonsanvil/EarlyDep
&lt;/p&gt;
&lt;p&gt;
This paper describes our participation in the MentalRiskES task at IberLEF 2023. The task involved predicting the likelihood of an individual experiencing depression based on their social media activity. The dataset consisted of conversations from 175 Telegram users, each labeled according to their evidence of suffering from the disorder. We used a combination of traditional machine learning and deep learning techniques to solve four predictive subtasks: binary classification, simple regression, multiclass classification, and multiclass regression. We approached this by training a model to solve the multiclass regression case and then transforming the predictions to work for the other three subtasks. We compare the performance of two different modeling approaches: fine-tuning a BERT-based model and using sentence embeddings as inputs to a linear regressor, with the latter yielding better results. The code to reproduce our results can be found at: https://github.com/simonsanvil/EarlyDep
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#20445;&#25252;&#38544;&#31169;&#24182;&#22312;&#19978;&#28216;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ZOO-based VFL&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16077</link><description>&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#24322;&#27493;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;:&#22522;&#20110;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization. (arXiv:2306.16077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#20445;&#25252;&#38544;&#31169;&#24182;&#22312;&#19978;&#28216;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ZOO-based VFL&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;(VFL)&#22240;&#33021;&#22815;&#22312;&#22402;&#30452;&#20998;&#21106;&#30340;&#25968;&#25454;&#19978;&#32852;&#21512;&#35757;&#32451;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#32780;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;(ZOO)&#22312;&#26500;&#24314;&#23454;&#29992;&#30340;VFL&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;ZOO&#30340;VFL&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20854;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#22788;&#29702;&#29616;&#20195;&#22823;&#22411;&#27169;&#22411;&#26102;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;VFL&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20013;&#65292;&#19979;&#28216;&#27169;&#22411;&#65288;&#23458;&#25143;&#31471;&#65289;&#20351;&#29992;ZOO&#36827;&#34892;&#35757;&#32451;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#30830;&#20445;&#19981;&#20849;&#20139;&#20869;&#37096;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#19978;&#28216;&#27169;&#22411;&#65288;&#26381;&#21153;&#22120;&#65289;&#22312;&#26412;&#22320;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;(FOO)&#36827;&#34892;&#26356;&#26032;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#19981;&#25439;&#23475;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#21069;&#25552;&#19979;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;VFL&#26694;&#26550;&#27604;&#22522;&#20110;ZOO&#30340;VFL&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) attracts increasing attention because it empowers multiple parties to jointly train a privacy-preserving model over vertically partitioned data. Recent research has shown that applying zeroth-order optimization (ZOO) has many advantages in building a practical VFL algorithm. However, a vital problem with the ZOO-based VFL is its slow convergence rate, which limits its application in handling modern large models. To address this problem, we propose a cascaded hybrid optimization method in VFL. In this method, the downstream models (clients) are trained with ZOO to protect privacy and ensure that no internal information is shared. Meanwhile, the upstream model (server) is updated with first-order optimization (FOO) locally, which significantly improves the convergence rate, making it feasible to train the large models without compromising privacy and security. We theoretically prove that our VFL framework converges faster than the ZOO-based VFL, as the c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#36825;&#20026;&#20043;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.15786</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#32599;&#29983;&#38376;&#25928;&#24212;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning. (arXiv:2306.15786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15786
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#36825;&#20026;&#20043;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32599;&#29983;&#38376;&#25928;&#24212;&#25551;&#36848;&#20102;&#20197;&#19979;&#29616;&#35937;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#21516;&#33391;&#22909;&#24615;&#33021;&#20294;&#37319;&#29992;&#19981;&#21516;&#35299;&#20915;&#31574;&#30053;&#30340;&#27169;&#22411;&#12290;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#23545;&#35299;&#37322;&#30340;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#19981;&#21516;&#27604;&#36739;&#22330;&#26223;&#25552;&#20379;&#20102;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#12289;&#24402;&#22240;&#26041;&#27861;&#21644;&#25351;&#26631;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#36229;&#21442;&#25968;&#35843;&#25972;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#65292;&#25351;&#26631;&#36873;&#25321;&#20063;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20808;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Rashomon Effect describes the following phenomenon: for a given dataset there may exist many models with equally good performance but with different solution strategies. The Rashomon Effect has implications for Explainable Machine Learning, especially for the comparability of explanations. We provide a unified view on three different comparison scenarios and conduct a quantitative evaluation across different datasets, models, attribution methods, and metrics. We find that hyperparameter-tuning plays a role and that metric selection matters. Our results provide empirical support for previously anecdotal evidence and exhibit challenges for both scientists and practitioners.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#25910;&#38598;&#33258;2022&#24180;&#39640;&#25490;&#21517;&#27604;&#36187;&#30340;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;ShuttleSet22&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25293;&#29699;&#39044;&#27979;&#26041;&#27861;ShuttleNet&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.15664</link><description>&lt;p&gt;
ShuttleSet22: &#29992;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;&#23545;&#20013;&#39118;&#39044;&#27979;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ShuttleSet22: Benchmarking Stroke Forecasting with Stroke-Level Badminton Dataset. (arXiv:2306.15664v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#25910;&#38598;&#33258;2022&#24180;&#39640;&#25490;&#21517;&#27604;&#36187;&#30340;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;ShuttleSet22&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25293;&#29699;&#39044;&#27979;&#26041;&#27861;ShuttleNet&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#21644;&#25968;&#25454;&#37319;&#38598;&#30340;&#25928;&#29575;&#65292;&#32701;&#27611;&#29699;&#20998;&#26512;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#31995;&#21015;&#26377;&#25928;&#30340;&#24212;&#29992;&#26469;&#25913;&#21892;&#21644;&#30740;&#31350;&#36873;&#25163;&#34920;&#29616;&#65292;&#20294;&#21482;&#26377;&#24456;&#23569;&#20960;&#20010;&#21487;&#20197;&#20379;&#32701;&#27611;&#29699;&#39046;&#22495;&#22806;&#30340;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;&#20844;&#24320;&#32701;&#27611;&#29699;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;&#38598;&#20013;&#20110;&#29305;&#23450;&#30340;&#27604;&#36187;&#23545;&#20915;&#65292;&#28982;&#32780;&#23427;&#20204;&#26080;&#27861;&#23545;&#19981;&#21516;&#36873;&#25163;&#21644;&#21508;&#31181;&#27604;&#36187;&#23545;&#20915;&#36827;&#34892;&#32508;&#21512;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;ShuttleSet22&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;2022&#24180;&#39640;&#25490;&#21517;&#27604;&#36187;&#20013;&#25910;&#38598;&#30340;&#12290;ShuttleSet22&#35757;&#32451;&#38598;&#21253;&#25324;30,172&#20010;&#22238;&#21512;&#20013;&#30340;2,888&#20010;&#25293;&#29699;&#65292;&#39564;&#35777;&#38598;&#21253;&#25324;450&#20010;&#22238;&#21512;&#20013;&#30340;1,400&#20010;&#25293;&#29699;&#65292;&#27979;&#35797;&#38598;&#21253;&#25324;654&#20010;&#22238;&#21512;&#20013;&#30340;2,040&#20010;&#25293;&#29699;&#65292;&#24182;&#19988;&#20855;&#26377;&#27599;&#20010;&#22238;&#21512;&#20013;&#35814;&#32454;&#30340;&#25293;&#29699;&#32423;&#20803;&#25968;&#25454;&#12290;&#20026;&#20102;&#19982;ShuttleSet22&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#20351;&#29992;ShuttleNet&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25293;&#29699;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, badminton analytics has drawn attention due to the advancement of artificial intelligence and the efficiency of data collection. While there is a line of effective applications to improve and investigate player performance, there are only a few public badminton datasets that can be used for researchers outside the badminton domain. Existing badminton singles datasets focus on specific matchups; however, they cannot provide comprehensive studies on different players and various matchups. In this paper, we provide a badminton singles dataset, ShuttleSet22, which is collected from high-ranking matches in 2022. ShuttleSet22 consists of 30,172 strokes in 2,888 rallies in the training set, 1,400 strokes in 450 rallies in the validation set, and 2,040 strokes in 654 rallies in the testing set with detailed stroke-level metadata within a rally. To benchmark existing work with ShuttleSet22, we test the state-of-the-art stroke forecasting approach, ShuttleNet, with the correspon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14275</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#22686;&#24378;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#24050;&#25104;&#20026;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#65292;&#31616;&#21333;&#30340;&#23545;&#25239;&#35757;&#32451;&#36973;&#21463;&#20102;&#20196;&#20154;&#30031;&#32553;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#23548;&#33268;&#40065;&#26834;&#27867;&#21270;&#25928;&#26524;&#19981;&#20339;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#22914;&#39069;&#22806;&#30340;&#35268;&#33539;&#21270;&#12289;&#23545;&#25239;&#26435;&#37325;&#25200;&#21160;&#21644;&#26356;&#22810;&#25968;&#25454;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#40065;&#26834;&#27867;&#21270;&#30340;&#25913;&#36827;&#20173;&#28982;&#36828;&#19981;&#29702;&#24819;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20840;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;--&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#30340;&#31934;&#32454;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#20248;&#21270;&#36712;&#36857;&#22312;&#26102;&#38388;&#19978;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;WOT&#22312;&#21508;&#31181;&#26368;&#26032;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;WOT&#19982;&#29616;&#26377;&#26041;&#27861;&#23436;&#32654;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#24847;&#22270;&#20248;&#20808;&#30340;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#25903;&#25345;&#35745;&#31639;&#26426;&#20195;&#29702;&#20154;&#20043;&#38388;&#20849;&#20139;&#24847;&#22270;&#30340;&#22522;&#26412;&#26426;&#21046;&#25152;&#24517;&#39035;&#20855;&#22791;&#30340;&#29305;&#24449;&#65292;&#24182;&#25506;&#32034;&#20102;&#36825;&#20123;&#26426;&#21046;&#22914;&#20309;&#36866;&#29992;&#20110;&#20154;&#31867;&#20197;&#25552;&#20379;&#23545;&#20154;&#31867;&#29702;&#24615;&#21644;&#24847;&#35782;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.13657</link><description>&lt;p&gt;
&#35770;&#20849;&#20139;&#24847;&#22270;&#30340;&#35745;&#31639;&#26426;&#21046;&#20197;&#21450;&#26377;&#20851;&#29702;&#24615;&#21644;&#24847;&#35782;&#30340;&#25512;&#27979;
&lt;/p&gt;
&lt;p&gt;
On Computational Mechanisms for Shared Intentionality, and Speculation on Rationality and Consciousness. (arXiv:2306.13657v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#24847;&#22270;&#20248;&#20808;&#30340;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#25903;&#25345;&#35745;&#31639;&#26426;&#20195;&#29702;&#20154;&#20043;&#38388;&#20849;&#20139;&#24847;&#22270;&#30340;&#22522;&#26412;&#26426;&#21046;&#25152;&#24517;&#39035;&#20855;&#22791;&#30340;&#29305;&#24449;&#65292;&#24182;&#25506;&#32034;&#20102;&#36825;&#20123;&#26426;&#21046;&#22914;&#20309;&#36866;&#29992;&#20110;&#20154;&#31867;&#20197;&#25552;&#20379;&#23545;&#20154;&#31867;&#29702;&#24615;&#21644;&#24847;&#35782;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29420;&#29305;&#30340;&#29305;&#24449;&#20043;&#19968;&#26159;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#26032;&#39062;&#30340;&#12289;&#21512;&#20316;&#30340;&#34892;&#20026;&#25110;&#22242;&#38431;&#21512;&#20316;&#12290;&#36825;&#38656;&#35201;&#25105;&#20204;&#33021;&#22815;&#22312;&#20010;&#20307;&#20043;&#38388;&#20256;&#36798;&#30446;&#26631;&#12289;&#35745;&#21010;&#21644;&#24605;&#24819;&#65292;&#20197;&#21019;&#24314;&#20849;&#20139;&#24847;&#22270;&#12290;&#26412;&#25991;&#21033;&#29992;David Marr&#30340;&#20449;&#24687;&#22788;&#29702;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#25903;&#25345;&#35745;&#31639;&#26426;&#20195;&#29702;&#20154;&#20043;&#38388;&#20849;&#20139;&#24847;&#22270;&#30340;&#22522;&#26412;&#26426;&#21046;&#25152;&#24517;&#39035;&#20855;&#22791;&#30340;&#29305;&#24449;&#65292;&#24182;&#25351;&#20986;&#36825;&#20123;&#29305;&#24449;&#22914;&#20309;&#22312;&#29616;&#26377;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26426;&#22120;&#20154;&#20013;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#36825;&#20010;&#24605;&#32500;&#23454;&#39564;&#25152;&#24471;&#21040;&#30340;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#20154;&#31867;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19982;&#35266;&#23519;&#30456;&#31526;&#30340;&#20851;&#20110;&#20154;&#31867;&#29702;&#24615;&#12289;&#24847;&#21521;&#21644;&#24863;&#30693;&#24847;&#35782;&#30340;&#35299;&#37322;&#12290;&#36825;&#26679;&#23601;&#24418;&#25104;&#20102;&#20316;&#32773;&#25152;&#31216;&#30340;&#20849;&#20139;&#24847;&#22270;&#20248;&#20808;&#29702;&#35770;&#65288;SIFT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
A singular attribute of humankind is our ability to undertake novel, cooperative behavior, or teamwork. This requires that we can communicate goals, plans, and ideas between the brains of individuals to create shared intentionality. Using the information processing model of David Marr, I derive necessary characteristics of basic mechanisms to enable shared intentionality between computational agents and indicate how these could be implemented in present-day AI-based robots.  More speculatively, I suggest the mechanisms derived by this thought experiment apply to humans and extend to provide explanations for human rationality and aspects of intentional and phenomenal consciousness that accord with observation. This yields what I call the Shared Intentionality First Theory (SIFT) for rationality and consciousness.
&lt;/p&gt;</description></item><item><title>SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.03538</link><description>&lt;p&gt;
SDR-GAIN&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39640;&#23454;&#26102;&#36974;&#25377;&#34892;&#20154;&#23039;&#24577;&#23436;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving. (arXiv:2306.03538v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03538
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32531;&#35299;&#22522;&#20110;&#20154;&#20307;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#34892;&#20154;&#26816;&#27979;&#31639;&#27861;&#20013;&#37096;&#20998;&#36974;&#25377;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#31163;&#21644;&#38477;&#32500;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#34917;&#20840;&#32593;&#32476;(SDR-GAIN)&#30340;&#26032;&#22411;&#34892;&#20154;&#23039;&#21183;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;OpenPose&#22312;&#22270;&#20687;&#20013;&#20272;&#35745;&#34892;&#20154;&#30340;&#23039;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#30001;&#20110;&#36974;&#25377;&#25110;&#20854;&#20182;&#22240;&#32032;&#32780;&#19981;&#23436;&#25972;&#30340;&#34892;&#20154;&#22836;&#37096;&#21644;&#36527;&#24178;&#20851;&#38190;&#28857;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#24182;&#36827;&#19968;&#27493;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;Huber&#25439;&#22833;&#12289;&#27531;&#24046;&#32467;&#26500;&#21644;L1&#27491;&#21017;&#21270;&#26469;&#29983;&#25104;&#37096;&#20998;&#36974;&#25377;&#34892;&#20154;&#19981;&#23436;&#25972;&#22836;&#37096;&#21644;&#36527;&#24178;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#32570;&#22833;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23039;&#24577;&#34917;&#20840;&#12290;&#25105;&#20204;&#22312;MS COCO&#21644;JAAD&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SDR-GAIN&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate the challenges arising from partial occlusion in human pose keypoint based pedestrian detection methods , we present a novel pedestrian pose keypoint completion method called the separation and dimensionality reduction-based generative adversarial imputation networks (SDR-GAIN) . Firstly, we utilize OpenPose to estimate pedestrian poses in images. Then, we isolate the head and torso keypoints of pedestrians with incomplete keypoints due to occlusion or other factors and perform dimensionality reduction to enhance features and further unify feature distribution. Finally, we introduce two generative models based on the generative adversarial networks (GAN) framework, which incorporate Huber loss, residual structure, and L1 regularization to generate missing parts of the incomplete head and torso pose keypoints of partially occluded pedestrians, resulting in pose completion. Our experiments on MS COCO and JAAD datasets demonstrate that SDR-GAIN outperforms basic GAIN framework
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#21516;&#36136;&#21270;&#30340;&#27169;&#22411;&#65292;&#27169;&#22411;&#35780;&#20272;&#38656;&#35201;&#25552;&#20379;&#26377;&#25928;&#30340;&#35780;&#20272;&#65292;&#20197;&#21028;&#26029;&#29305;&#23450;&#27169;&#22411;&#26159;&#21542;&#22312;&#19979;&#28216;&#20351;&#29992;&#22330;&#26223;&#20013;&#21487;&#20197;&#28385;&#36275;&#22810;&#23569;&#20154;&#31867;&#38656;&#27714;&#65292;&#24182;&#19988;&#24212;&#35813;&#26681;&#25454;&#30495;&#23454;&#30340;&#31038;&#20250;&#38656;&#27714;&#26469;&#24320;&#21457;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25317;&#25265;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03100</link><description>&lt;p&gt;
&#23558;&#27169;&#22411;&#35780;&#20272;&#37325;&#26032;&#32771;&#34385;&#20026;&#32553;&#23567;&#31038;&#20250;&#25216;&#26415;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Rethinking Model Evaluation as Narrowing the Socio-Technical Gap. (arXiv:2306.03100v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03100
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21516;&#36136;&#21270;&#30340;&#27169;&#22411;&#65292;&#27169;&#22411;&#35780;&#20272;&#38656;&#35201;&#25552;&#20379;&#26377;&#25928;&#30340;&#35780;&#20272;&#65292;&#20197;&#21028;&#26029;&#29305;&#23450;&#27169;&#22411;&#26159;&#21542;&#22312;&#19979;&#28216;&#20351;&#29992;&#22330;&#26223;&#20013;&#21487;&#20197;&#28385;&#36275;&#22810;&#23569;&#20154;&#31867;&#38656;&#27714;&#65292;&#24182;&#19988;&#24212;&#35813;&#26681;&#25454;&#30495;&#23454;&#30340;&#31038;&#20250;&#38656;&#27714;&#26469;&#24320;&#21457;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25317;&#25265;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#21457;&#23637;&#32473;&#27169;&#22411;&#35780;&#20272;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#30028;&#21644;&#24037;&#19994;&#30028;&#27491;&#22312;&#21162;&#21147;&#24212;&#23545;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#25165;&#22810;&#33402;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#22859;&#65292;&#20294;&#23427;&#20204;&#20063;&#19981;&#21487;&#36991;&#20813;&#22320;&#21521;&#21516;&#36136;&#21270;&#36808;&#36827;&#65306;&#29992;&#21333;&#20010;&#24120;&#31216;&#20043;&#20026;&#8220;&#36890;&#29992;&#8221;&#30340;&#27169;&#22411;&#20026;&#19968;&#31995;&#21015;&#24212;&#29992;&#25552;&#20379;&#21160;&#21147;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#27169;&#22411;&#35780;&#20272;&#23454;&#36341;&#24517;&#39035;&#25215;&#25285;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20197;&#24212;&#23545;&#36825;&#31181;&#21516;&#36136;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#21644;&#36131;&#20219;&#65306;&#20026;&#29305;&#23450;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#30340;&#35780;&#20272;&#65292;&#21028;&#26029;&#26159;&#21542;&#20197;&#21450;&#22312;&#19979;&#28216;&#20351;&#29992;&#22330;&#26223;&#20013;&#21487;&#20197;&#36890;&#36807;&#32473;&#23450;&#27169;&#22411;&#28385;&#36275;&#22810;&#23569;&#20154;&#31867;&#38656;&#27714;&#65288;&#8220;&#31038;&#20250;&#25216;&#26415;&#24046;&#36317;&#8221;&#65289;&#12290;&#25105;&#20204;&#27762;&#21462;&#31038;&#20250;&#31185;&#23398;&#12289;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#21644;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#36328;&#23398;&#31185;&#39046;&#22495;&#30340;&#32463;&#39564;&#65292;&#25958;&#20419;&#31038;&#21306;&#24320;&#21457;&#22522;&#20110;&#30495;&#23454;&#31038;&#20250;&#38656;&#27714;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25317;&#25265;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development of generative and large language models (LLMs) poses new challenges for model evaluation that the research community and industry are grappling with. While the versatile capabilities of these models ignite excitement, they also inevitably make a leap toward homogenization: powering a wide range of applications with a single, often referred to as ``general-purpose'', model. In this position paper, we argue that model evaluation practices must take on a critical task to cope with the challenges and responsibilities brought by this homogenization: providing valid assessments for whether and how much human needs in downstream use cases can be satisfied by the given model (\textit{socio-technical gap}). By drawing on lessons from the social sciences, human-computer interaction (HCI), and the interdisciplinary field of explainable AI (XAI), we urge the community to develop evaluation methods based on real-world socio-requirements and embrace diverse evaluation methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.19915</link><description>&lt;p&gt;
&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation Approaches for Source Code Models: A Survey. (arXiv:2305.19915v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#20195;&#30721;&#22312;&#35768;&#22810;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20419;&#36827;&#20102;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21508;&#31181;&#33021;&#21147;&#65288;&#20363;&#22914;&#20581;&#22766;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#24182;&#38024;&#23545;&#28304;&#20195;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;DA&#26041;&#27861;&#30340;&#35843;&#25972;&#65292;&#20294;&#32570;&#20047;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#21644;&#23457;&#26597;&#20197;&#29702;&#35299;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#21547;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20840;&#38754;&#32780;&#32508;&#21512;&#30340;&#35843;&#26597;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25972;&#29702;&#21644;&#27010;&#36848;&#29616;&#26377;&#25991;&#29486;&#65292;&#20197;&#25552;&#20379;&#35813;&#39046;&#22495;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#20998;&#31867;&#27861;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#33879;&#21517;&#30340;&#12289;&#26041;&#27861;&#19978;&#20855;&#26377;&#35828;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20248;&#21270;DA&#36136;&#37327;&#30340;&#19968;&#33324;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;&#25216;&#26415;&#65292;&#24182;&#21576;&#29616;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;DA&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly popular adoption of source code in many critical tasks motivates the development of data augmentation (DA) techniques to enhance training data and improve various capabilities (e.g., robustness and generalizability) of these models. Although a series of DA methods have been proposed and tailored for source code models, there lacks a comprehensive survey and examination to understand their effectiveness and implications. This paper fills this gap by conducting a comprehensive and integrative survey of data augmentation for source code, wherein we systematically compile and encapsulate existing literature to provide a comprehensive overview of the field. We start by constructing a taxonomy of DA for source code models model approaches, followed by a discussion on prominent, methodologically illustrative approaches. Next, we highlight the general strategies and techniques to optimize the DA quality. Subsequently, we underscore techniques that find utility in widely-accept
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#22810;&#28304;&#23545;&#25239;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20165;&#20855;&#26377;&#23616;&#37096;&#30456;&#20284;&#24615;&#30340;&#36801;&#31227;&#22330;&#26223;&#12290;&#36890;&#36807;&#23376;&#32593;&#32476;&#25552;&#21462;&#21487;&#36801;&#31227;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#27169;&#22359;&#23545;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#22788;&#29702;&#20197;&#25233;&#21046;&#38750;&#30456;&#20851;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.19067</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#23616;&#37096;&#29305;&#24449;&#30340;&#22810;&#28304;&#23545;&#25239;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-source adversarial transfer learning based on similar source domains with local features. (arXiv:2305.19067v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#22810;&#28304;&#23545;&#25239;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20165;&#20855;&#26377;&#23616;&#37096;&#30456;&#20284;&#24615;&#30340;&#36801;&#31227;&#22330;&#26223;&#12290;&#36890;&#36807;&#23376;&#32593;&#32476;&#25552;&#21462;&#21487;&#36801;&#31227;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#27169;&#22359;&#23545;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#22788;&#29702;&#20197;&#25233;&#21046;&#38750;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21033;&#29992;&#20854;&#20182;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#25972;&#20307;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#25552;&#20379;&#19968;&#20010;&#25972;&#20307;&#30456;&#20284;&#30340;&#28304;&#22495;&#65292;&#21482;&#33021;&#25552;&#20379;&#19968;&#20123;&#20855;&#26377;&#30456;&#20284;&#23616;&#37096;&#29305;&#24449;&#30340;&#28304;&#22495;&#12290;&#37027;&#20040;&#65292;&#33021;&#21542;&#23454;&#29616;&#36801;&#31227;&#23398;&#20064;&#65311;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#22810;&#28304;&#23545;&#25239;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20165;&#20855;&#26377;&#23616;&#37096;&#30456;&#20284;&#24615;&#30340;&#36801;&#31227;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#32593;&#32476;&#25552;&#21462;&#21333;&#20010;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#23616;&#37096;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23376;&#32593;&#32476;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#30001;&#22495;&#21028;&#21035;&#22120;&#24341;&#23548;&#65292;&#23398;&#20064;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#27169;&#22359;&#23545;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#65292;&#20197;&#25233;&#21046;&#38750;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning leverages knowledge from other domains and has been successful in many applications. Transfer learning methods rely on the overall similarity of the source and target domains. However, in some cases, it is impossible to provide an overall similar source domain, and only some source domains with similar local features can be provided. Can transfer learning be achieved? In this regard, we propose a multi-source adversarial transfer learning method based on local feature similarity to the source domain to handle transfer scenarios where the source and target domains have only local similarities. This method extracts transferable local features between a single source domain and the target domain through a sub-network. Specifically, the feature extractor of the sub-network is induced by the domain discriminator to learn transferable knowledge between the source domain and the target domain. The extracted features are then weighted by an attention module to suppress non-tr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;Python&#23553;&#35013;&#22120;&#65292;&#29992;&#20110;&#22312;HPO&#22522;&#20934;&#27979;&#35797;&#19978;&#27169;&#25311;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#65292;&#36890;&#36807;&#24378;&#21046;&#27599;&#20010;&#24037;&#20316;&#36827;&#31243;&#31561;&#24453;&#65292;&#21487;&#20197;&#20943;&#23569;&#22810;&#23567;&#26102;&#30340;&#31561;&#24453;&#26102;&#38388;&#65292;&#20351;&#24471;&#27169;&#25311;&#32467;&#26524;&#19982;&#23454;&#38469;&#23454;&#39564;&#30340;&#35780;&#20272;&#39034;&#24207;&#23436;&#20840;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2305.17595</link><description>&lt;p&gt;
Python&#23553;&#35013;&#22120;&#29992;&#20110;&#22312;HPO&#22522;&#20934;&#27979;&#35797;&#19978;&#27169;&#25311;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#65292;&#26080;&#38656;&#31561;&#24453;
&lt;/p&gt;
&lt;p&gt;
Python Wrapper for Simulating Multi-Fidelity Optimization on HPO Benchmarks without Any Wait. (arXiv:2305.17595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;Python&#23553;&#35013;&#22120;&#65292;&#29992;&#20110;&#22312;HPO&#22522;&#20934;&#27979;&#35797;&#19978;&#27169;&#25311;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#65292;&#36890;&#36807;&#24378;&#21046;&#27599;&#20010;&#24037;&#20316;&#36827;&#31243;&#31561;&#24453;&#65292;&#21487;&#20197;&#20943;&#23569;&#22810;&#23567;&#26102;&#30340;&#31561;&#24453;&#26102;&#38388;&#65292;&#20351;&#24471;&#27169;&#25311;&#32467;&#26524;&#19982;&#23454;&#38469;&#23454;&#39564;&#30340;&#35780;&#20272;&#39034;&#24207;&#23436;&#20840;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#21442;&#25968;&#65288;HP&#65289;&#20248;&#21270;&#23545;&#20110;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#24448;&#24448;&#38656;&#35201;&#20960;&#23567;&#26102;&#21040;&#20960;&#22825;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#22240;&#27492;&#28145;&#24230;&#23398;&#20064;&#30340;HP&#20248;&#21270;&#36890;&#24120;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#26114;&#36149;&#30340;&#12290;&#36825;&#20419;&#20351;&#20986;&#29616;&#20102;&#34920;&#26684;&#25110;&#26367;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#22312;&#19968;&#23567;&#37096;&#20998;&#26102;&#38388;&#20869;&#26597;&#35810;&#29305;&#23450;HP&#37197;&#32622;&#30340;DL&#30340;&#65288;&#39044;&#27979;&#65289;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;DL&#35757;&#32451;&#30340;&#23454;&#38469;&#36816;&#34892;&#26102;&#38388;&#19982;&#26597;&#35810;&#21709;&#24212;&#26102;&#38388;&#26126;&#26174;&#19981;&#21516;&#65292;&#24322;&#27493;HPO&#65288;&#20363;&#22914;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#65289;&#30340;&#27169;&#25311;&#22120;&#24517;&#39035;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31561;&#24453;&#23454;&#38469;&#36816;&#34892;&#26102;&#38388;&#65292;&#21542;&#21017;&#27169;&#25311;&#20013;&#30340;&#35780;&#20272;&#39034;&#24207;&#19981;&#31526;&#21512;&#23454;&#38469;&#23454;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;Python&#23553;&#35013;&#22120;&#24182;&#25551;&#36848;&#20102;&#23427;&#30340;&#29992;&#27861;&#12290;&#36825;&#20010;&#23553;&#35013;&#22120;&#24378;&#21046;&#27599;&#20010;&#24037;&#20316;&#36827;&#31243;&#31561;&#24453;&#65292;&#20197;&#20415;&#25105;&#20204;&#21482;&#38656;&#31561;&#24453;$10^{-2}$&#31186;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#19982;&#23454;&#38469;&#23454;&#39564;&#23436;&#20840;&#30456;&#21516;&#30340;&#35780;&#20272;&#39034;&#24207;&#65292;&#32780;&#19981;&#26159;&#31561;&#24453;&#20960;&#20010;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter (HP) optimization of deep learning (DL) is essential for high performance. As DL often requires several hours to days for its training, HP optimization (HPO) of DL is often prohibitively expensive. This boosted the emergence of tabular or surrogate benchmarks, which enable querying the (predictive) performance of DL with a specific HP configuration in a fraction. However, since the actual runtime of a DL training is significantly different from its query response time, simulators of an asynchronous HPO, e.g. multi-fidelity optimization, must wait for the actual runtime at each iteration in a na\"ive implementation; otherwise, the evaluation order during simulation does not match with the real experiment. To ease this issue, we developed a Python wrapper and describe its usage. This wrapper forces each worker to wait so that we yield exactly the same evaluation order as in the real experiment with only $10^{-2}$ seconds of waiting instead of waiting several hours. Our imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;</title><link>http://arxiv.org/abs/2305.02279</link><description>&lt;p&gt;
Learngene: &#20174;&#31062;&#20808;&#27169;&#22411;&#20013;&#32487;&#25215;&#21387;&#32553;&#30693;&#35782;&#21040;&#21518;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models. (arXiv:2305.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#29983;&#29289;&#30340;&#36830;&#32493;&#36827;&#21270;&#36807;&#31243;&#20013;&#65292;&#23427;&#30340;&#22522;&#22240;&#31215;&#32047;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#21644;&#30693;&#35782;&#65292;&#20351;&#26032;&#29983;&#21518;&#20195;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#20854;&#29305;&#23450;&#29615;&#22659;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#21363; Learngene&#65292;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#34701;&#21512;&#22522;&#22240;&#30340;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#12290; (i) &#31215;&#32047;&#65306;&#30693;&#35782;&#22312;&#31062;&#20808;&#27169;&#22411;&#30340;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#31215;&#32047;&#12290; (ii) &#21387;&#32553;&#65306;&#23558;&#31215;&#32047;&#30340;&#35814;&#23613;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#65292;&#21363; Learngene&#12290; (iii) &#32487;&#25215;&#65306;&#23558;&#21387;&#32553;&#30340; Learngene &#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;&#12290;&#30001;&#20110;&#31215;&#32047;&#24050;&#22312;&#19968;&#20123;&#25104;&#29087;&#30340;&#33539;&#24335;&#20013;&#24471;&#21040;&#30740;&#31350;&#65292;&#22914;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21644;&#32456;&#36523;&#23398;&#20064;&#65292;&#22240;&#27492;&#25105;&#20204;&#19987;&#27880;&#20110;&#21387;&#32553;&#21644;&#32487;&#25215;&#65292;&#36825;&#24341;&#21457;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#20026;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the continuous evolution of one organism's ancestry, its genes accumulate extensive experiences and knowledge, enabling newborn descendants to rapidly adapt to their specific environments. Motivated by this observation, we propose a novel machine learning paradigm \textit{Learngene} to enable learning models to incorporate three key characteristics of genes. (i) Accumulating: the knowledge is accumulated during the continuous learning of an \textbf{ancestry model}. (ii) Condensing: the exhaustive accumulated knowledge is condensed into a much more compact information piece, \ie \textbf{learngene}. (iii): Inheriting: the condensed \textbf{learngene} is inherited to make it easier for \textbf{descendant models} to adapt to new environments. Since accumulating has been studied in some well-developed paradigms like large-scale pre-training and lifelong learning, we focus on condensing and inheriting, which induces three key issues and we provide the preliminary solutions to these is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#35013;&#31665;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14712</link><description>&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems. (arXiv:2304.14712v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#35013;&#31665;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;12&#20010;&#23454;&#20363;&#32452;&#25104;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#22823;&#23567;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#38382;&#39064;&#22797;&#26434;&#24230;&#27700;&#24179;&#65288;&#21253;&#21547;&#20174;38&#21040;53&#20010;&#21253;&#35065;&#30340;&#25968;&#37327;&#65289;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#38480;&#21046;&#26465;&#20214;&#26469;&#26500;&#24314;&#36825;&#20123;&#23454;&#20363;&#65306;i)&#29289;&#21697;&#21644;&#31665;&#23376;&#23610;&#23544;&#65292;ii)&#37325;&#37327;&#38480;&#21046;&#65292;iii)&#21253;&#31867;&#21035;&#20043;&#38388;&#30340;&#20146;&#21644;&#24615;&#65292;iv)&#21253;&#35013;&#39034;&#24207;&#30340;&#20559;&#22909;&#21644;v)&#36127;&#36733;&#24179;&#34913;&#12290;&#38500;&#20102;&#25968;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#20027;&#24320;&#21457;&#30340;Python&#33050;&#26412;&#29992;&#20110;&#25968;&#25454;&#38598;&#29983;&#25104;&#65292;&#31216;&#20026;Q4RealBPP-DataGen&#12290;&#35813;&#22522;&#20934;&#39318;&#20808;&#34987;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#27714;&#35299;&#22120;&#65292;&#22240;&#27492;&#36825;&#32452;&#23454;&#20363;&#30340;&#29305;&#24449;&#26159;&#25353;&#29031;&#37327;&#23376;&#35774;&#22791;&#30340;&#24403;&#21069;&#38480;&#21046;&#35774;&#35745;&#30340;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21253;&#21547;&#22312;&#20869;&#65292;&#20801;&#35768;&#26500;&#24314;&#36890;&#29992;&#22522;&#20934;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#23545;&#27604;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a benchmark for real-world bin packing problems is proposed. This dataset is composed of 12 instances comprehending different levels of problem complexity regarding size (with the number of packages ranging from 38 to 53) and user-defined requirements. In fact, several real-world oriented restrictions have been considered for building these instances: i) items and bins dimensions, ii) weight restrictions, iii) affinities among packages categories iv) preferences for package ordering and v) load balancing. Besides the data, we also provide an own-developed Python script for the dataset generation, coined as Q4RealBPP-DataGen. The benchmark was firstly proposed to evaluate quantum solvers, therefore the characteristic of this set of instances were designed according to the current limitations of quantum devices. Additionally, the dataset generator is included to allow the construction of general-purpose benchmarks. The data introduced on this paper provides a baseline that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#25152;&#26377;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2304.14177</link><description>&lt;p&gt;
ChatGPT&#19982;&#29616;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task. (arXiv:2304.14177v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#25152;&#26377;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;ChatGPT&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#22312;&#35780;&#20272;ChatGPT&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#36824;&#27809;&#26377;&#22810;&#23569;&#30740;&#31350;&#65292;&#36825;&#28041;&#21450;&#21040;&#20934;&#30830;&#21453;&#26144;&#25991;&#26723;&#20869;&#23481;&#30340;&#20449;&#24687;&#24615;&#30701;&#35821;&#30340;&#35782;&#21035;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#23558;ChatGPT&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#34920;&#29616;&#19982;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#23427;&#20316;&#20026;&#35299;&#20915;&#39046;&#22495;&#36866;&#24212;&#21644;&#38271;&#25991;&#26723;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#31185;&#23398;&#25991;&#31456;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#20845;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#22312;&#30701;&#25991;&#26723;&#21644;&#38271;&#25991;&#26723;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#65292;ChatGPT&#30340;&#24615;&#33021;&#20248;&#20110;&#24403;&#21069;&#29616;&#26377;&#27169;&#22411;&#65292;&#20135;&#29983;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#21644;&#25991;&#26723;&#38271;&#24230;&#30340;&#39640;&#36136;&#37327;&#20851;&#38190;&#30701;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models, including ChatGPT, have demonstrated exceptional performance in various natural language generation tasks. However, there has been limited research evaluating ChatGPT's keyphrase generation ability, which involves identifying informative phrases that accurately reflect a document's content. This study seeks to address this gap by comparing ChatGPT's keyphrase generation performance with state-of-the-art models, while also testing its potential as a solution for two significant challenges in the field: domain adaptation and keyphrase generation from long documents. We conducted experiments on six publicly available datasets from scientific articles and news domains, analyzing performance on both short and long documents. Our results show that ChatGPT outperforms current state-of-the-art models in all tested datasets and environments, generating high-quality keyphrases that adapt well to diverse domains and document lengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GeneticFlow&#30340;&#22522;&#20110;&#33258;&#24341;&#22270;&#30340;&#23398;&#32773;&#21078;&#26512;&#24037;&#20855;&#65292;&#33021;&#22815;&#28385;&#36275;&#23398;&#32773;&#29305;&#24449;&#21078;&#26512;&#20013;&#30340;&#19977;&#20010;&#22522;&#26412;&#35201;&#27714;&#65292;&#21363;&#32467;&#26500;&#21270;&#32972;&#26223;&#12289;&#23398;&#32773;&#20026;&#20013;&#24515;&#21644;&#20016;&#23500;&#30340;&#36827;&#21270;&#65292;&#22312;&#31185;&#23398;&#22870;&#39033;&#25512;&#29702;&#30495;&#23454;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.12217</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#24341;&#22270;&#30340;&#38754;&#21521;&#24433;&#21709;&#21147;&#30340;&#23398;&#32773;&#29615;&#22659;&#21078;&#26512;
&lt;/p&gt;
&lt;p&gt;
Impact-Oriented Contextual Scholar Profiling using Self-Citation Graphs. (arXiv:2304.12217v2 [cs.DL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GeneticFlow&#30340;&#22522;&#20110;&#33258;&#24341;&#22270;&#30340;&#23398;&#32773;&#21078;&#26512;&#24037;&#20855;&#65292;&#33021;&#22815;&#28385;&#36275;&#23398;&#32773;&#29305;&#24449;&#21078;&#26512;&#20013;&#30340;&#19977;&#20010;&#22522;&#26412;&#35201;&#27714;&#65292;&#21363;&#32467;&#26500;&#21270;&#32972;&#26223;&#12289;&#23398;&#32773;&#20026;&#20013;&#24515;&#21644;&#20016;&#23500;&#30340;&#36827;&#21270;&#65292;&#22312;&#31185;&#23398;&#22870;&#39033;&#25512;&#29702;&#30495;&#23454;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#37327;&#22320;&#21078;&#26512;&#23398;&#32773;&#30340;&#31185;&#30740;&#24433;&#21709;&#21147;&#23545;&#20110;&#29616;&#20195;&#30740;&#31350;&#31038;&#20250;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#20351;&#29992;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65288;&#22914;h&#25351;&#25968;&#65289;&#12289;&#21015;&#34920;&#21644;&#32593;&#32476;&#22312;&#23398;&#32773;&#25490;&#21517;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#19981;&#25552;&#20379;&#23398;&#32773;&#30456;&#20851;&#12289;&#20998;&#26512;&#24615;&#20219;&#21153;&#30340;&#32467;&#26500;&#21270;&#32972;&#26223;&#65292;&#20363;&#22914;&#21078;&#26512;&#21644;&#29702;&#35299;&#23398;&#32773;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#23398;&#32773;&#21078;&#26512;&#24037;&#20855;&#8212;&#8212;GeneticFlow (GF)&#65292;&#28385;&#36275;&#19977;&#20010;&#22522;&#26412;&#35201;&#27714;&#65306;&#32467;&#26500;&#21270;&#32972;&#26223;&#12289;&#23398;&#32773;&#20026;&#20013;&#24515;&#21644;&#20016;&#23500;&#30340;&#36827;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21253;&#25324;&#25968;&#30334;&#19975;&#23398;&#32773;&#30340;&#22823;&#35268;&#27169;&#23398;&#26415;&#25968;&#25454;&#28304;&#19978;&#35745;&#31639;GF&#65307;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23548;&#24072;-&#23398;&#29983;&#26816;&#27979;&#31639;&#27861;&#12289;&#19968;&#20010;&#20351;&#29992;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#24341;&#29992;&#31867;&#22411;&#20998;&#31867;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#31185;&#23398;&#22870;&#39033;&#25512;&#29702;&#30340;&#30495;&#23454;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;GF&#21078;&#26512;&#30340;F1&#24471;&#20998;&#26126;&#26174;&#20248;&#20110;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantitatively profiling a scholar's scientific impact is important to modern research society. Current practices with bibliometric indicators (e.g., h-index), lists, and networks perform well at scholar ranking, but do not provide structured context for scholar-centric, analytical tasks such as profile reasoning and understanding. This work presents GeneticFlow (GF), a suite of novel graph-based scholar profiles that fulfill three essential requirements: structured-context, scholar-centric, and evolution-rich. We propose a framework to compute GF over large-scale academic data sources with millions of scholars. The framework encompasses a new unsupervised advisor-advisee detection algorithm, a well-engineered citation type classifier using interpretable features, and a fine-tuned graph neural network (GNN) model. Evaluations are conducted on the real-world task of scientific award inference. Experiment outcomes show that the F1 score of best GF profile significantly outperforms altern
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#19981;&#31283;&#23450;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#30340;&#26041;&#24335;&#26469;&#20462;&#35746;&#36229;&#32593;&#32476;&#24418;&#24335;&#65292;&#23454;&#29616;&#26356;&#21152;&#31283;&#23450;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2304.07645</link><description>&lt;p&gt;
&#38024;&#23545;&#31283;&#23450;&#30340;&#36229;&#32593;&#32476;&#23398;&#20064;&#30340;&#38750;&#27604;&#20363;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Proportional Parametrizations for Stable Hypernetwork Learning. (arXiv:2304.07645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#19981;&#31283;&#23450;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#30340;&#26041;&#24335;&#26469;&#20462;&#35746;&#36229;&#32593;&#32476;&#24418;&#24335;&#65292;&#23454;&#29616;&#26356;&#21152;&#31283;&#23450;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32593;&#32476;&#26159;&#29983;&#25104;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24403;&#21069;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#25910;&#25947;&#36895;&#24230;&#36890;&#24120;&#27604;&#38750;&#36229;&#32593;&#32476;&#27169;&#22411;&#24930;&#24471;&#22810;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#19982;&#20351;&#29992;&#24120;&#35265;&#30340;&#36229;&#32593;&#32476;&#26550;&#26500;&#21644;&#21021;&#22987;&#21270;&#26102;&#20986;&#29616;&#30340;&#38382;&#39064;&#26377;&#20851;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#25968;&#20540;&#38382;&#39064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#29978;&#33267;&#38459;&#27490;&#25910;&#25947;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24402;&#19968;&#21270;&#31574;&#30053;&#26080;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#35746;&#30340;&#36229;&#32593;&#32476;&#24418;&#24335;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#36229;&#32593;&#32476;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#24182;&#35777;&#26126;&#23427;&#22987;&#32456;&#21487;&#20197;&#23548;&#33268;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypernetworks are neural networks that generate the parameters of another neural network. In many scenarios, current hypernetwork training strategies are unstable, and convergence is often far slower than for non-hypernetwork models. We show that this problem is linked to an issue that arises when using common choices of hypernetwork architecture and initialization. We demonstrate analytically and experimentally how this numerical issue can lead to an instability during training that slows, and sometimes even prevents, convergence. We also demonstrate that popular deep learning normalization strategies fail to address these issues. We then propose a solution to the problem based on a revised hypernetwork formulation that uses non-proportional additive parametrizations. We test the proposed reparametrization on several tasks, and demonstrate that it consistently leads to more stable training, achieving faster convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07396</link><description>&lt;p&gt;
&#25913;&#21892;&#20020;&#24202;&#35797;&#39564;&#30340;&#24739;&#32773;&#39044;&#31579;&#36873;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#21307;&#29983;
&lt;/p&gt;
&lt;p&gt;
Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models. (arXiv:2304.07396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#24739;&#32773;&#30340;&#20020;&#24202;&#35797;&#39564;&#65292;&#21307;&#29983;&#38656;&#35201;&#36827;&#34892;&#32321;&#29712;&#30340;&#26816;&#26597;&#65292;&#20197;&#30830;&#23450;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#25991;&#26412;&#22522;&#20934;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#21644;&#20020;&#24202;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23578;&#26410;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;InstructGPT&#36741;&#21161;&#21307;&#29983;&#26681;&#25454;&#24739;&#32773;&#30340;&#21307;&#30103;&#31616;&#20917;&#30830;&#23450;&#20854;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#30340;&#36164;&#26684;&#12290;&#20351;&#29992;&#19968;&#27425;&#24615;&#12289;&#36873;&#25321;-&#25512;&#29702;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#22235;&#20010;&#32423;&#21035;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65306;&#33021;&#21542;&#20174;&#20020;&#24202;&#35797;&#39564;&#20013;&#32473;&#20986;&#30340;&#21307;&#30103;&#31616;&#20917;&#20013;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#65307;&#33021;&#21542;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#26631;&#20934;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#24739;&#32773;&#65307;&#25972;&#20307;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physicians considering clinical trials for their patients are met with the laborious process of checking many text based eligibility criteria. Large Language Models (LLMs) have shown to perform well for clinical information extraction and clinical reasoning, including medical tests, but not yet in real-world scenarios. This paper investigates the use of InstructGPT to assist physicians in determining eligibility for clinical trials based on a patient's summarised medical profile. Using a prompting strategy combining one-shot, selection-inference and chain-of-thought techniques, we investigate the performance of LLMs on 10 synthetically created patient profiles. Performance is evaluated at four levels: ability to identify screenable eligibility criteria from a trial given a medical profile; ability to classify for each individual criterion whether the patient qualifies; the overall classification whether a patient is eligible for a clinical trial and the percentage of criteria to be scr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03838</link><description>&lt;p&gt;
&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Identity-Robustness for Face Models. (arXiv:2304.03838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#20173;&#28982;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#21040;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#26080;&#20851;&#28151;&#28102;&#22240;&#32032;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#30452;&#25509;&#35757;&#32451;&#20110;&#20154;&#33080;&#19978;&#30340;&#27169;&#22411;&#20013;&#65292;&#19968;&#20010;&#25935;&#24863;&#30340;&#28151;&#28102;&#22240;&#32032;&#26159;&#20154;&#30340;&#36523;&#20221;&#12290;&#35768;&#22810;&#19982;&#20154;&#33080;&#30456;&#20851;&#30340;&#20219;&#21153;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#26159;&#19982;&#36523;&#20221;&#26080;&#20851;&#30340;&#65292;&#24182;&#22312;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#34920;&#29616;&#19968;&#33268;&#65288;&#21363;&#20844;&#24179;&#65289;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#24378;&#21046;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#22343;&#21248;&#24615;&#26159;&#24230;&#37327;&#21644;&#23454;&#26045;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#20551;&#35774;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#33719;&#21462;&#19982;&#36523;&#20221;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#20197;&#21450;&#25910;&#38598;&#27492;&#31867;&#20449;&#24687;&#30340;&#25104;&#26412;&#65292;&#36825;&#36890;&#24120;&#19981;&#26159;&#24773;&#20917;&#65292;&#22823;&#22810;&#25968;&#20154;&#33080;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#36755;&#20837;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20219;&#21153;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#26080;&#38656;&#27492;&#31867;&#27880;&#37322;&#21363;&#21487;&#25552;&#39640;&#36523;&#20221;&#30456;&#20851;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of deep-learning models in many tasks, there have been concerns about such models learning shortcuts, and their lack of robustness to irrelevant confounders. When it comes to models directly trained on human faces, a sensitive confounder is that of human identities. Many face-related tasks should ideally be identity-independent, and perform uniformly across different individuals (i.e. be fair). One way to measure and enforce such robustness and performance uniformity is through enforcing it during training, assuming identity-related information is available at scale. However, due to privacy concerns and also the cost of collecting such information, this is often not the case, and most face datasets simply contain input images and their corresponding task-related labels. Thus, improving identity-related robustness without the need for such annotations is of great importance. Here, we explore using face-recognition embedding vectors, as proxies for identities, to enfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.11156</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#26469;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23454;&#35777;&#21644;&#29702;&#35770;&#20004;&#20010;&#26041;&#38754;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#24182;&#19981;&#21487;&#38752;&#12290;&#20174;&#23454;&#36341;&#19978;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36731;&#37327;&#32423;&#30340;&#25913;&#20889;&#22120;&#24212;&#29992;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#21487;&#20197;&#30772;&#35299;&#19968;&#31995;&#21015;&#30340;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#20351;&#29992;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26088;&#22312;&#36530;&#36991;&#25913;&#20889;&#25915;&#20987;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26816;&#27979;&#22120;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36882;&#24402;&#25913;&#20889;&#30340;&#25915;&#20987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#65292;&#25351;&#20986;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#26356;&#25797;&#38271;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#22312;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#23545;&#20110;&#19968;&#20010;&#36275;&#22815;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#21363;&#20351;&#26368;&#20339;&#30340;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#21482;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#22909;&#19978;&#19968;&#28857;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36275;&#22815;&#27010;&#25324;&#29305;&#23450;&#30340;&#22330;&#26223;&#65292;&#22914;&#25913;&#20889;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26816;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#36890;&#36807;&#20934;&#30830;&#34701;&#21512;&#28608;&#20809;&#38647;&#36798;&#21644;&#22270;&#20687;&#26469;&#25552;&#39640;3D&#26816;&#27979;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07064</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalized Multi-Modal Fusion Detection Framework. (arXiv:2303.07064v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26816;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#36890;&#36807;&#20934;&#30830;&#34701;&#21512;&#28608;&#20809;&#38647;&#36798;&#21644;&#22270;&#20687;&#26469;&#25552;&#39640;3D&#26816;&#27979;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#24050;&#25104;&#20026;&#33258;&#21160;&#39550;&#39542;&#20013;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#20294;&#30001;&#20110;&#28857;&#20113;&#30340;&#31232;&#30095;&#24615;&#65292;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;&#26816;&#27979;&#12290;&#22270;&#20687;&#22240;&#20854;&#19982;&#28857;&#20113;&#30340;&#20114;&#34917;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#26377;&#19968;&#20123;&#25104;&#21151;&#26696;&#20363;&#65292;&#20294;&#29616;&#26377;&#30340;&#34701;&#21512;&#26041;&#27861;&#35201;&#20040;&#36827;&#34892;&#30828;&#34701;&#21512;&#65292;&#35201;&#20040;&#19981;&#30452;&#25509;&#36827;&#34892;&#34701;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MMFusion&#30340;&#36890;&#29992;&#30340;3D&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#23454;&#29616;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#28608;&#20809;&#38647;&#36798;&#21644;&#22270;&#20687;&#30340;&#20934;&#30830;&#34701;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;3D&#26816;&#27979;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#29420;&#31435;&#30340;&#27969;&#32452;&#25104;&#65306;&#28608;&#20809;&#38647;&#36798;&#27969;&#21644;&#30456;&#26426;&#27969;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#21333;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#20860;&#23481;&#12290;&#28608;&#20809;&#38647;&#36798;&#27969;&#20013;&#30340;&#20307;&#32032;&#23616;&#37096;&#24863;&#30693;&#27169;&#22359;&#22686;&#24378;&#20102;&#23616;&#37096;&#29305;&#24449;&#34920;&#31034;&#65292;&#28982;&#21518;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#26377;&#36873;&#25321;&#22320;&#23558;&#26469;&#33258;&#19981;&#21516;&#27969;&#30340;&#29305;&#24449;&#36755;&#20986;&#36827;&#34892;&#34701;&#21512;&#65292;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
LiDAR point clouds have become the most common data source in autonomous driving. However, due to the sparsity of point clouds, accurate and reliable detection cannot be achieved in specific scenarios. Because of their complementarity with point clouds, images are getting increasing attention. Although with some success, existing fusion methods either perform hard fusion or do not fuse in a direct manner. In this paper, we propose a generic 3D detection framework called MMFusion, using multi-modal features. The framework aims to achieve accurate fusion between LiDAR and images to improve 3D detection in complex scenes. Our framework consists of two separate streams: the LiDAR stream and the camera stream, which can be compatible with any single-modal feature extraction network. The Voxel Local Perception Module in the LiDAR stream enhances local feature representation, and then the Multi-modal Feature Fusion Module selectively combines feature output from different streams to achieve b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Expert-Free Online Transfer Learning (EF-OnTL)&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#38656;&#35201;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#20219;&#21153;&#26377;&#33391;&#22909;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.01170</link><description>&lt;p&gt;
&#26080;&#19987;&#23478;&#22312;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning. (arXiv:2303.01170v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Expert-Free Online Transfer Learning (EF-OnTL)&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#38656;&#35201;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#20219;&#21153;&#26377;&#33391;&#22909;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#19987;&#23478;&#26234;&#33021;&#20307;&#36716;&#31227;&#21040;&#26032;&#25163;&#26234;&#33021;&#20307;&#26469;&#35299;&#20915;&#35757;&#32451;&#38382;&#39064;&#65292;&#22914;&#25506;&#32034;&#25104;&#26412;&#12289;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#25910;&#25947;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36801;&#31227;&#38656;&#35201;&#26032;&#25163;&#26234;&#33021;&#20307;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#26377;&#33391;&#22909;&#30340;&#29702;&#35299;&#25165;&#33021;&#26377;&#25928;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#19987;&#23478;&#22312;&#32447;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65288;EF-OnTL&#65289;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#27599;&#19968;&#27425;&#36801;&#31227;&#27493;&#39588;&#20013;&#65292;&#26681;&#25454;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#26469;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SARS-RND&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#23545;RND&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#20174;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#12289;&#34892;&#21160;&#12289;&#22870;&#21169;&#21644;&#19979;&#19968;&#29366;&#24577;&#20013;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning in Reinforcement Learning (RL) has been widely studied to overcome training issues of Deep-RL, i.e., exploration cost, data availability and convergence time, by introducing a way to enhance training phase with external knowledge. Generally, knowledge is transferred from expert-agents to novices. While this fixes the issue for a novice agent, a good understanding of the task on expert agent is required for such transfer to be effective. As an alternative, in this paper we propose Expert-Free Online Transfer Learning (EF-OnTL), an algorithm that enables expert-free real-time dynamic transfer learning in multi-agent system. No dedicated expert exists, and transfer source agent and knowledge to be transferred are dynamically selected at each transfer step based on agents' performance and uncertainty. To improve uncertainty estimation, we also propose State Action Reward Next-State Random Network Distillation (sars-RND), an extension of RND that estimates uncertainty from
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#30340;&#26102;&#38388;&#25139;&#65292;&#24341;&#20837;&#20102;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#36825;&#20004;&#20010;&#25351;&#26631;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20445;&#25252;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.03684</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#20013;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Temporal Robustness against Data Poisoning. (arXiv:2302.03684v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03684
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#30340;&#26102;&#38388;&#25139;&#65292;&#24341;&#20837;&#20102;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#36825;&#20004;&#20010;&#25351;&#26631;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#32771;&#34385;&#20102;&#36890;&#36807;&#24694;&#24847;&#35757;&#32451;&#25968;&#25454;&#25805;&#32437;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#27745;&#26579;&#23041;&#32961;&#27169;&#22411;&#37117;&#22260;&#32469;&#30528;&#19968;&#20010;&#21333;&#19968;&#25351;&#26631;&#65292;&#21363;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#25915;&#20987;&#32773;&#33021;&#22815;&#20197;&#21487;&#25215;&#21463;&#30340;&#20195;&#20215;&#27745;&#26579;&#27604;&#39044;&#26399;&#26356;&#22810;&#30340;&#26679;&#26412;&#65292;&#23601;&#20687;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#19968;&#26679;&#65292;&#20182;&#20204;&#21487;&#33021;&#33021;&#22815;&#22312;&#24456;&#30701;&#30340;&#26102;&#38388;&#20869;&#20351;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#22833;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#30340;&#20986;&#29983;&#26085;&#26399;&#26102;&#38388;&#25139;&#65292;&#36825;&#20123;&#26102;&#38388;&#25139;&#36890;&#24120;&#26159;&#21487;&#29992;&#30340;&#20294;&#36807;&#21435;&#34987;&#24573;&#30053;&#12290;&#21033;&#29992;&#36825;&#20123;&#26102;&#38388;&#25139;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#20004;&#20010;&#26032;&#22411;&#25351;&#26631;&#65288;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#65289;&#30340;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#20998;&#21035;&#34913;&#37327;&#25915;&#20987;&#25552;&#21069;&#24320;&#22987;&#30340;&#26102;&#38388;&#21644;&#25915;&#20987;&#25345;&#32493;&#30340;&#26102;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#25351;&#26631;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#34987;&#27745;&#26579;&#30340;&#26679;&#26412;&#65292;&#20063;&#33021;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#20445;&#25252;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data poisoning considers cases when an adversary manipulates the behavior of machine learning algorithms through malicious training data. Existing threat models of data poisoning center around a single metric, the number of poisoned samples. In consequence, if attackers can poison more samples than expected with affordable overhead, as in many practical scenarios, they may be able to render existing defenses ineffective in a short time. To address this issue, we leverage timestamps denoting the birth dates of data, which are often available but neglected in the past. Benefiting from these timestamps, we propose a temporal threat model of data poisoning with two novel metrics, earliness and duration, which respectively measure how long an attack started in advance and how long an attack lasted. Using these metrics, we define the notions of temporal robustness against data poisoning, providing a meaningful sense of protection even with unbounded amounts of poisoned samples. We present a 
&lt;/p&gt;</description></item><item><title>&#19968;&#20010;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#22810;&#32441;&#29702;&#21512;&#25104;&#65292;&#36890;&#36807;&#32039;&#20945;&#30340;&#32534;&#30721;&#22120;&#21644;&#33258;&#36866;&#24212;&#21608;&#26399;&#20869;&#23481;&#30340;&#29983;&#25104;&#22120;&#65292;&#22312;&#20960;&#20309;&#19968;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#32441;&#29702;&#34920;&#31034;&#21644;&#31354;&#38388;&#32452;&#32455;&#30340;&#20998;&#31163;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32441;&#29702;&#21512;&#25104;&#21644;&#25554;&#20540;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.01616</link><description>&lt;p&gt;
&#19968;&#20010;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#22810;&#32441;&#29702;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
A geometrically aware auto-encoder for multi-texture synthesis. (arXiv:2302.01616v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01616
&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#22810;&#32441;&#29702;&#21512;&#25104;&#65292;&#36890;&#36807;&#32039;&#20945;&#30340;&#32534;&#30721;&#22120;&#21644;&#33258;&#36866;&#24212;&#21608;&#26399;&#20869;&#23481;&#30340;&#29983;&#25104;&#22120;&#65292;&#22312;&#20960;&#20309;&#19968;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#32441;&#29702;&#34920;&#31034;&#21644;&#31354;&#38388;&#32452;&#32455;&#30340;&#20998;&#31163;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32441;&#29702;&#21512;&#25104;&#21644;&#25554;&#20540;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#32441;&#29702;&#21512;&#25104;&#30340;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#32039;&#20945;&#30340;&#32534;&#30721;&#22120;&#65292;&#32771;&#34385;&#20102;&#20108;&#38454;&#31070;&#32463;&#32479;&#35745;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#21253;&#21547;&#33258;&#36866;&#24212;&#21608;&#26399;&#20869;&#23481;&#30340;&#29983;&#25104;&#22120;&#12290;&#22270;&#20687;&#34987;&#23884;&#20837;&#21040;&#19968;&#20010;&#32039;&#20945;&#19988;&#20960;&#20309;&#19968;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#32441;&#29702;&#34920;&#31034;&#21644;&#20854;&#31354;&#38388;&#32452;&#32455;&#34987;&#20998;&#31163;&#12290;&#32441;&#29702;&#21512;&#25104;&#21644;&#25554;&#20540;&#20219;&#21153;&#21487;&#20197;&#30452;&#25509;&#20174;&#36825;&#20123;&#28508;&#22312;&#20195;&#30721;&#20013;&#23436;&#25104;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#21508;&#31181;&#32441;&#29702;&#30456;&#20851;&#24230;&#37327;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21069;&#21521;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an auto-encoder architecture for multi-texture synthesis. The approach relies on both a compact encoder accounting for second order neural statistics and a generator incorporating adaptive periodic content. Images are embedded in a compact and geometrically consistent latent space, where the texture representation and its spatial organisation are disentangled. Texture synthesis and interpolation tasks can be performed directly from these latent codes. Our experiments demonstrate that our model outperforms state-of-the-art feed-forward methods in terms of visual quality and various texture related metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeciLS-PBO&#30340;&#26377;&#25928;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#20266;&#24067;&#23572;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#25193;&#23637;&#22522;&#20110;&#21333;&#20803;&#20256;&#25773;&#30340;&#20943;&#23569;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#23376;&#21477;&#26435;&#37325;&#35745;&#31639;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;PBO&#38382;&#39064;&#30340;&#27714;&#35299;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2301.12251</link><description>&lt;p&gt;
DeciLS-PBO:&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#20266;&#24067;&#23572;&#20248;&#21270;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeciLS-PBO: an Effective Local Search Method for Pseudo-Boolean Optimization. (arXiv:2301.12251v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeciLS-PBO&#30340;&#26377;&#25928;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#20266;&#24067;&#23572;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#25193;&#23637;&#22522;&#20110;&#21333;&#20803;&#20256;&#25773;&#30340;&#20943;&#23569;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#23376;&#21477;&#26435;&#37325;&#35745;&#31639;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;PBO&#38382;&#39064;&#30340;&#27714;&#35299;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#26159;&#35299;&#20915;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#20960;&#31181;&#24494;&#22937;&#30340;&#26426;&#21046;&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#22312;&#35299;&#20915;&#20266;&#24067;&#23572;&#20248;&#21270;&#65288;PBO&#65289;&#30340;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#20013;&#21457;&#29616;&#20102;&#20004;&#31181;&#25913;&#36827;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#19968;&#20123;&#26426;&#21046;&#22914;&#21333;&#20803;&#20256;&#25773;&#20165;&#20165;&#29992;&#20110;&#35299;&#20915;MaxSAT&#38382;&#39064;&#65292;&#20294;&#21516;&#26679;&#21487;&#20197;&#25512;&#24191;&#21040;&#35299;&#20915;PBO&#38382;&#39064;&#65307;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#21033;&#29992;&#21464;&#37327;&#30340;&#21551;&#21457;&#24335;&#35780;&#20998;&#26469;&#24341;&#23548;&#25628;&#32034;&#12290;&#25105;&#20204;&#35797;&#22270;&#23545;&#20174;&#21464;&#37327;&#21040;&#32473;&#23450;&#20844;&#24335;&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#30340;&#23376;&#21477;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#22240;&#20026;&#23376;&#21477;&#22312;&#20854;&#20013;&#36215;&#21040;&#20102;&#20013;&#38388;&#20154;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#22522;&#20110;&#21333;&#20803;&#20256;&#25773;&#30340;&#20943;&#23569;&#31639;&#27861;&#25193;&#23637;&#21040;PBO&#38382;&#39064;&#20013;&#65292;&#20026;PBO&#38382;&#39064;&#32473;&#20986;&#20102;&#26356;&#36827;&#19968;&#27493;&#30340;&#24191;&#20041;&#23450;&#20041;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#27714;&#35299;&#22120;LS-PBO&#20197;&#26500;&#24314;&#21021;&#22987;&#20998;&#37197;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#23376;&#21477;&#26435;&#37325;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local search is an effective method for solving large-scale combinatorial optimization problems, and it has made remarkable progress in recent years through several subtle mechanisms. In this paper, we found two ways to improve the local search algorithms in solving Pseudo-Boolean Optimization (PBO): Firstly, some of those mechanisms such as unit propagation are merely used in solving MaxSAT before, which can be generalized to solve PBO as well; Secondly, the existing local search algorithms utilize the heuristic on variables, so-called score, to mainly guide the search. We attempt to gain more insights into the clause, as it plays the role of a middleman who builds a bridge between variables and the given formula. Hence, we first extended the combination of unit propagation-based decimation algorithm to PBO problem, giving a further generalized definition of unit clause for PBO problem, and apply it to the existing solver LS-PBO for constructing an initial assignment; then, we introdu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#33539;&#30068;&#35770;&#30340;&#26041;&#27861;&#20026;&#36817;&#20284;&#25512;&#26029;&#25552;&#20379;&#20102;&#20989;&#23376;&#35821;&#20041;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#36879;&#38236;&#30340;&#27010;&#24565;&#21644;&#32479;&#35745;&#21338;&#24328;&#30340;&#32420;&#32500;&#65292;&#23545;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2212.12538</link><description>&lt;p&gt;
&#19968;&#31181;&#36125;&#21494;&#26031;&#22823;&#33041;&#30340;&#32452;&#21512;&#24615;&#35299;&#37322;&#30340;&#25968;&#23398;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Mathematical Foundations for a Compositional Account of the Bayesian Brain. (arXiv:2212.12538v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#33539;&#30068;&#35770;&#30340;&#26041;&#27861;&#20026;&#36817;&#20284;&#25512;&#26029;&#25552;&#20379;&#20102;&#20989;&#23376;&#35821;&#20041;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#36879;&#38236;&#30340;&#27010;&#24565;&#21644;&#32479;&#35745;&#21338;&#24328;&#30340;&#32420;&#32500;&#65292;&#23545;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25253;&#21578;&#20102;&#20851;&#20110;&#20027;&#21160;&#25512;&#26029;&#21644;&#36125;&#21494;&#26031;&#22823;&#33041;&#32452;&#21512;&#24615;&#35299;&#37322;&#30340;&#19968;&#20123;&#21021;&#27493;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#24403;&#20195;&#24212;&#29992;&#33539;&#30068;&#35770;&#30340;&#24037;&#20855;&#20026;&#36817;&#20284;&#25512;&#26029;&#25552;&#20379;&#20102;&#20989;&#23376;&#35821;&#20041;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#8220;&#35821;&#27861;&#8221;&#26041;&#38754;&#23450;&#20041;&#20102;&#26032;&#30340;&#36125;&#21494;&#26031;&#36879;&#38236;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#36125;&#21494;&#26031;&#26356;&#26032;&#36981;&#24490;&#32452;&#21512;&#36879;&#38236;&#27169;&#24335;&#12290;&#21033;&#29992;&#36125;&#21494;&#26031;&#36879;&#38236;&#65292;&#24182;&#21463;&#32452;&#21512;&#21338;&#24328;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#32479;&#35745;&#21338;&#24328;&#30340;&#32420;&#32500;&#21644;&#23558;&#21508;&#31181;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#20998;&#31867;&#20026;&#23545;&#24212;&#30340;&#37096;&#20998;&#65306;&#30456;&#23545;&#29109;&#30340;&#38142;&#24335;&#35268;&#21017;&#34987;&#24418;&#24335;&#21270;&#20026;&#20005;&#26684;&#37096;&#20998;&#65292;&#32780;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#33258;&#30001;&#33021;&#21017;&#32473;&#20986;&#20102;&#26494;&#24347;&#37096;&#20998;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#22797;&#21046;-&#32452;&#21512;&#8221;&#30340;&#27010;&#24565;&#12290;&#22312;&#8220;&#35821;&#20041;&#8221;&#26041;&#38754;&#65292;&#25105;&#20204;&#23558;&#19968;&#33324;&#30340;&#24320;&#25918;&#21160;&#21147;&#31995;&#32479;&#65288;&#23588;&#20854;&#26159;&#30830;&#23450;&#24615;&#12289;&#38543;&#26426;&#24615;&#21644;&#38543;&#26426;&#24615;&#65292;&#20197;&#21450;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#65289;&#20316;&#20026;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This dissertation reports some first steps towards a compositional account of active inference and the Bayesian brain. Specifically, we use the tools of contemporary applied category theory to supply functorial semantics for approximate inference. To do so, we define on the `syntactic' side the new notion of Bayesian lens and show that Bayesian updating composes according to the compositional lens pattern. Using Bayesian lenses, and inspired by compositional game theory, we define fibrations of statistical games and classify various problems of statistical inference as corresponding sections: the chain rule of the relative entropy is formalized as a strict section, while maximum likelihood estimation and the free energy give lax sections. In the process, we introduce a new notion of `copy-composition'.  On the `semantic' side, we present a new formalization of general open dynamical systems (particularly: deterministic, stochastic, and random; and discrete- and continuous-time) as cert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#28436;&#36827;&#30456;&#26426;&#24405;&#20687;&#20013;&#26816;&#27979;&#23567;&#26118;&#34411;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#26631;&#27880;&#26118;&#34411;&#30340;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#39044;&#22788;&#29702;&#21644;&#22522;&#20110;&#36816;&#21160;&#20449;&#24687;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#22797;&#26434;&#21160;&#24577;&#22330;&#26223;&#20013;&#25552;&#21462;&#26118;&#34411;&#12290;</title><link>http://arxiv.org/abs/2212.00423</link><description>&lt;p&gt;
&#22522;&#20110;&#36816;&#21160;&#20449;&#24687;&#30340;&#26102;&#38388;&#28436;&#36827;&#30456;&#26426;&#24405;&#20687;&#20013;&#23567;&#26118;&#34411;&#30340;&#23545;&#35937;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Motion Informed Object Detection of Small Insects in Time-lapse Camera Recordings. (arXiv:2212.00423v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#28436;&#36827;&#30456;&#26426;&#24405;&#20687;&#20013;&#26816;&#27979;&#23567;&#26118;&#34411;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#26631;&#27880;&#26118;&#34411;&#30340;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#39044;&#22788;&#29702;&#21644;&#22522;&#20110;&#36816;&#21160;&#20449;&#24687;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#22797;&#26434;&#21160;&#24577;&#22330;&#26223;&#20013;&#25552;&#21462;&#26118;&#34411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20256;&#31881;&#23186;&#20171;&#65292;&#26118;&#34411;&#22312;&#29983;&#24577;&#31995;&#32479;&#31649;&#29702;&#21644;&#20840;&#29699;&#31918;&#39135;&#29983;&#20135;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26118;&#34411;&#25968;&#37327;&#27491;&#22312;&#19979;&#38477;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#26118;&#34411;&#30417;&#27979;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#26118;&#34411;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#35270;&#39057;&#25110;&#26102;&#38388;&#28436;&#36827;&#22270;&#20687;&#26469;&#36827;&#34892;&#65292;&#20294;&#30001;&#20110;&#26118;&#34411;&#26159;&#22797;&#26434;&#21160;&#24577;&#22330;&#26223;&#20013;&#30340;&#23567;&#29289;&#20307;&#65292;&#20998;&#26512;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22799;&#23395;&#20004;&#20010;&#26376;&#26399;&#38388;&#19977;&#31181;&#19981;&#21516;&#26893;&#29289;&#29289;&#31181;&#19978;&#30340;107,387&#20010;&#32463;&#36807;&#26631;&#27880;&#30340;&#26102;&#38388;&#28436;&#36827;&#22270;&#20687;&#65292;&#20854;&#20013;&#21253;&#21547;9,423&#20010;&#26631;&#27880;&#26118;&#34411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#26102;&#38388;&#28436;&#36827;RGB&#22270;&#20687;&#20013;&#26118;&#34411;&#30340;&#26041;&#27861;&#27969;&#31243;&#12290;&#35813;&#27969;&#31243;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#23545;&#26102;&#38388;&#28436;&#36827;RGB&#22270;&#20687;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20197;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#26118;&#34411;&#12290;&#35813;&#22522;&#20110;&#36816;&#21160;&#20449;&#24687;&#30340;&#22686;&#24378;&#25216;&#26415;&#20351;&#29992;&#36816;&#21160;&#21644;&#39068;&#33394;&#26469;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#26118;&#34411;&#12290;&#28982;&#21518;&#65292;&#23558;&#22686;&#24378;&#21518;&#30340;&#22270;&#20687;&#36755;&#20837;&#21040;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Insects as pollinators play a crucial role in ecosystem management and world food production. However, insect populations are declining, calling for efficient methods of insect monitoring. Existing methods analyze video or time-lapse images of insects in nature, but the analysis is challenging since insects are small objects in complex and dynamic scenes of natural vegetation. In this work, we provide a dataset of primary honeybees visiting three different plant species during two months of the summer period. The dataset consists of 107,387 annotated time-lapse images from multiple cameras, including 9,423 annotated insects. We present a method pipeline for detecting insects in time-lapse RGB images. The pipeline consists of a two-step process. Firstly, the time-lapse RGB images are preprocessed to enhance insects in the images. This Motion-Informed-Enhancement technique uses motion and colors to enhance insects in images. Secondly, the enhanced images are subsequently fed into a Convo
&lt;/p&gt;</description></item><item><title>iSmallNet&#26159;&#19968;&#31181;&#37319;&#29992;&#26631;&#31614;&#35299;&#32806;&#30340;&#23494;&#38598;&#23884;&#22871;&#32593;&#32476;&#65292;&#29992;&#20110;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;&#12290;&#23427;&#36890;&#36807;&#35299;&#32806;&#26631;&#31614;&#20449;&#24687;&#21644;&#24341;&#20837;&#20851;&#38190;&#27169;&#22359;&#26469;&#25552;&#21319;&#26816;&#27979;&#24615;&#33021;&#65292;&#21253;&#25324;&#22810;&#23610;&#24230;&#23884;&#22871;&#20132;&#20114;&#27169;&#22359;&#21644;&#20869;&#37096;&#36793;&#30028;&#34701;&#21512;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2210.16561</link><description>&lt;p&gt;
iSmallNet: &#22522;&#20110;&#26631;&#31614;&#35299;&#32806;&#30340;&#23494;&#38598;&#23884;&#22871;&#32593;&#32476;&#29992;&#20110;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
iSmallNet: Densely Nested Network with Label Decoupling for Infrared Small Target Detection. (arXiv:2210.16561v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16561
&lt;/p&gt;
&lt;p&gt;
iSmallNet&#26159;&#19968;&#31181;&#37319;&#29992;&#26631;&#31614;&#35299;&#32806;&#30340;&#23494;&#38598;&#23884;&#22871;&#32593;&#32476;&#65292;&#29992;&#20110;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;&#12290;&#23427;&#36890;&#36807;&#35299;&#32806;&#26631;&#31614;&#20449;&#24687;&#21644;&#24341;&#20837;&#20851;&#38190;&#27169;&#22359;&#26469;&#25552;&#21319;&#26816;&#27979;&#24615;&#33021;&#65292;&#21253;&#25324;&#22810;&#23610;&#24230;&#23884;&#22871;&#20132;&#20114;&#27169;&#22359;&#21644;&#20869;&#37096;&#36793;&#30028;&#34701;&#21512;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#22806;&#22270;&#20687;&#20013;&#30340;&#23567;&#30446;&#26631;&#24120;&#24120;&#34987;&#26434;&#20081;&#30340;&#32972;&#26223;&#25152;&#25513;&#30422;&#12290;&#20256;&#32479;&#30340;&#26816;&#27979;&#22120;&#24448;&#24448;&#20250;&#20135;&#29983;&#35823;&#25253;&#65292;&#32780;&#22522;&#20110;CNN&#30340;&#26816;&#27979;&#22120;&#21017;&#20250;&#22312;&#28145;&#23618;&#20013;&#20002;&#22833;&#23567;&#30446;&#26631;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;iSmallNet&#30340;&#22810;&#27969;&#23494;&#38598;&#23884;&#22871;&#32593;&#32476;&#65292;&#37319;&#29992;&#26631;&#31614;&#35299;&#32806;&#26469;&#29992;&#20110;&#32418;&#22806;&#23567;&#30446;&#26631;&#26816;&#27979;&#12290;&#19968;&#26041;&#38754;&#65292;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#23567;&#30446;&#26631;&#30340;&#24418;&#29366;&#20449;&#24687;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#30340;&#26631;&#35760;&#22320;&#38754;&#30495;&#20540;(GT)&#22270;&#35299;&#32806;&#20026;&#20869;&#37096;&#22270;&#21644;&#36793;&#30028;&#22270;&#12290;GT&#22270;&#19982;&#21478;&#22806;&#20004;&#24352;&#22270;&#21327;&#21516;&#24037;&#20316;&#65292;&#20197;&#35299;&#20915;&#23567;&#30446;&#26631;&#36793;&#30028;&#30340;&#19981;&#22343;&#34913;&#20998;&#24067;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#24182;&#34701;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#21040;&#25552;&#35758;&#30340;&#32593;&#32476;&#20013;&#65292;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#22312;&#28145;&#23618;&#20013;&#20445;&#30041;&#23567;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#23884;&#22871;&#20132;&#20114;&#27169;&#22359;&#26469;&#25506;&#32034;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20869;&#37096;&#36793;&#30028;&#34701;&#21512;&#27169;&#22359;&#26469;&#38598;&#25104;&#22810;&#31890;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small targets are often submerged in cluttered backgrounds of infrared images. Conventional detectors tend to generate false alarms, while CNN-based detectors lose small targets in deep layers. To this end, we propose iSmallNet, a multi-stream densely nested network with label decoupling for infrared small object detection. On the one hand, to fully exploit the shape information of small targets, we decouple the original labeled ground-truth (GT) map into an interior map and a boundary one. The GT map, in collaboration with the two additional maps, tackles the unbalanced distribution of small object boundaries. On the other hand, two key modules are delicately designed and incorporated into the proposed network to boost the overall performance. First, to maintain small targets in deep layers, we develop a multi-scale nested interaction module to explore a wide range of context information. Second, we develop an interior-boundary fusion module to integrate multi-granularity information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer&#12290;&#36890;&#36807;&#25913;&#36827;&#27880;&#24847;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.13148</link><description>&lt;p&gt;
&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformers over Directed Acyclic Graphs. (arXiv:2210.13148v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer&#12290;&#36890;&#36807;&#25913;&#36827;&#27880;&#24847;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Transformer&#27169;&#22411;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#33021;&#21147;&#23398;&#20064;&#36229;&#20986;&#24120;&#35268;&#22270;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#21040;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#20027;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#22270;&#30340;&#32467;&#26500;&#20559;&#24046;&#27880;&#20837;&#21040;Transformer&#30340;&#26550;&#26500;&#20013;&#65292;&#24182;&#38024;&#23545;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#25552;&#20986;&#20102;&#19968;&#20123;&#36866;&#24212;&#24615;&#30340;&#26550;&#26500;&#25913;&#36827;&#65306;&#65288;1&#65289;&#19968;&#20010;&#27604;&#24120;&#35268;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#26356;&#39640;&#25928;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#21516;&#26102;&#24544;&#23454;&#22320;&#25429;&#25417;&#20102;DAGs&#30340;&#32467;&#26500;&#65292;&#65288;2&#65289;&#19968;&#20010;&#23545;DAG&#30340;&#20559;&#24207;&#36827;&#34892;&#20301;&#32622;&#32534;&#30721;&#65292;&#34917;&#20805;&#20102;&#21069;&#32773;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#20174;&#23545;&#28304;&#20195;&#30721;&#22270;&#30340;&#20998;&#31867;&#21040;&#23545;&#24341;&#29992;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have recently gained popularity in graph representation learning as they have the potential to learn complex relationships beyond the ones captured by regular graph neural networks. The main research question is how to inject the structural bias of graphs into the transformer architecture, and several proposals have been made for undirected molecular graphs and, recently, also for larger network graphs. In this paper, we study transformers over directed acyclic graphs (DAGs) and propose architecture adaptations tailored to DAGs: (1) An attention mechanism that is considerably more efficient than the regular quadratic complexity of transformers and at the same time faithfully captures the DAG structure, and (2) a positional encoding of the DAG's partial order, complementing the former. We rigorously evaluate our approach over various types of tasks, ranging from classifying source code graphs to nodes in citation networks, and show that it is effective in two importan
&lt;/p&gt;</description></item><item><title>SAFER&#26159;&#19968;&#31181;&#39640;&#25928;&#26377;&#25928;&#30340;&#30896;&#25758;&#36991;&#38556;&#31995;&#32479;&#65292;&#36890;&#36807;&#32416;&#27491;&#25511;&#21046;&#25351;&#20196;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;&#23427;&#32467;&#21512;&#20102;&#24378;&#21270;&#23398;&#20064;&#12289;&#22312;&#32447;&#36712;&#36857;&#35268;&#21010;&#21644;&#33258;&#21160;&#32039;&#24613;&#24178;&#39044;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#26377;&#25928;&#30340;&#32416;&#27491;&#25511;&#21046;&#21160;&#20316;&#65292;&#20943;&#23569;&#32039;&#24613;&#21046;&#21160;&#30340;&#39057;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;SAFER&#20855;&#26377;&#26356;&#39640;&#30340;&#36895;&#24230;&#12289;&#26356;&#20302;&#30340;&#30896;&#25758;&#29575;&#21644;&#36739;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2209.11789</link><description>&lt;p&gt;
SAFER:&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#32858;&#28966;&#39640;&#25928;&#36712;&#36857;&#25628;&#32034;&#30340;&#23433;&#20840;&#36991;&#38556;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
SAFER: Safe Collision Avoidance using Focused and Efficient Trajectory Search with Reinforcement Learning. (arXiv:2209.11789v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11789
&lt;/p&gt;
&lt;p&gt;
SAFER&#26159;&#19968;&#31181;&#39640;&#25928;&#26377;&#25928;&#30340;&#30896;&#25758;&#36991;&#38556;&#31995;&#32479;&#65292;&#36890;&#36807;&#32416;&#27491;&#25511;&#21046;&#25351;&#20196;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;&#23427;&#32467;&#21512;&#20102;&#24378;&#21270;&#23398;&#20064;&#12289;&#22312;&#32447;&#36712;&#36857;&#35268;&#21010;&#21644;&#33258;&#21160;&#32039;&#24613;&#24178;&#39044;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#26377;&#25928;&#30340;&#32416;&#27491;&#25511;&#21046;&#21160;&#20316;&#65292;&#20943;&#23569;&#32039;&#24613;&#21046;&#21160;&#30340;&#39057;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;SAFER&#20855;&#26377;&#26356;&#39640;&#30340;&#36895;&#24230;&#12289;&#26356;&#20302;&#30340;&#30896;&#25758;&#29575;&#21644;&#36739;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36991;&#20813;&#30896;&#25758;&#26159;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#26234;&#33021;&#20307;&#23433;&#20840;&#36816;&#20316;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SAFER&#30340;&#39640;&#25928;&#26377;&#25928;&#30340;&#30896;&#25758;&#36991;&#38556;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#32416;&#27491;&#25805;&#20316;&#32773;&#21457;&#36865;&#30340;&#25511;&#21046;&#25351;&#20196;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;&#23427;&#32467;&#21512;&#20102;&#29616;&#23454;&#19990;&#30028;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#22522;&#20110;&#25628;&#32034;&#30340;&#22312;&#32447;&#36712;&#36857;&#35268;&#21010;&#21644;&#33258;&#21160;&#32039;&#24613;&#24178;&#39044;&#65292;&#20363;&#22914;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#31181;&#26377;&#25928;&#30340;&#32416;&#27491;&#25511;&#21046;&#21160;&#20316;&#65292;&#29992;&#20110;&#32858;&#28966;&#25628;&#32034;&#26080;&#30896;&#25758;&#36712;&#36857;&#65292;&#24182;&#20943;&#23569;&#35302;&#21457;&#32039;&#24613;&#21046;&#21160;&#30340;&#39057;&#29575;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35774;&#32622;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#33021;&#22815;&#22312;&#30495;&#23454;&#23460;&#20869;&#29615;&#22659;&#20013;&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#19978;&#23433;&#20840;&#23398;&#20064;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20063;&#33021;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#23454;&#38469;&#30896;&#25758;&#12290;&#25105;&#20204;&#30340;&#23454;&#38469;&#23454;&#39564;&#26174;&#31034;&#65292;&#19982;&#20960;&#31181;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#36895;&#24230;&#65292;&#26356;&#20302;&#30340;&#30896;&#25758;&#29575;&#65292;&#26356;&#23569;&#30340;&#32039;&#24613;&#24178;&#39044;&#21644;&#36739;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collision avoidance is key for mobile robots and agents to operate safely in the real world. In this work we present SAFER, an efficient and effective collision avoidance system that is able to improve safety by correcting the control commands sent by an operator. It combines real-world reinforcement learning (RL), search-based online trajectory planning, and automatic emergency intervention, e.g. automatic emergency braking (AEB). The goal of the RL is to learn an effective corrective control action that is used in a focused search for collision-free trajectories, and to reduce the frequency of triggering automatic emergency braking. This novel setup enables the RL policy to learn safely and directly on mobile robots in a real-world indoor environment, minimizing actual crashes even during training. Our real-world experiments show that, when compared with several baselines, our approach enjoys a higher average speed, lower crash rate, less emergency intervention, smaller computation o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#36827;&#34892;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#23427;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#24182;&#21033;&#29992;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#21487;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.11549</link><description>&lt;p&gt;
MAGIC: &#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#23454;&#29616;&#22522;&#20110;&#25513;&#30721;&#30340;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier. (arXiv:2209.11549v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#36827;&#34892;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#23427;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#24182;&#21033;&#29992;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#21487;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#24102;&#26377;&#24378;&#27491;&#21017;&#21270;&#22120;&#30340;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#26469;&#25511;&#21046;&#23545;&#21333;&#20010;&#22270;&#20687;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;MAGIC&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#30340;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#32467;&#26500;&#21270;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#36755;&#20837;&#30340;&#35821;&#20041;&#65292;&#24182;&#20445;&#25345;&#20854;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20445;&#35777;&#21512;&#25104;&#30340;&#21487;&#20449;&#24230;&#12290;&#19982;&#30446;&#21069;&#20351;&#29992;&#22797;&#26434;&#21407;&#35821;&#26469;&#30417;&#30563;&#36807;&#31243;&#25110;&#20351;&#29992;&#27880;&#24847;&#21147;&#22270;&#20316;&#20026;&#24369;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MAGIC&#36890;&#36807;&#22312;&#36755;&#20837;&#19978;&#32858;&#21512;&#26799;&#24230;&#65292;&#30001;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#25512;&#21160;&#12290;MAGIC&#20197;&#21333;&#20010;&#26694;&#26550;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#24378;&#28872;&#30340;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22312;&#37325;&#22797;&#29289;&#20307;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#32473;&#29992;&#25143;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We offer a method for one-shot mask-guided image synthesis that allows controlling manipulations of a single image by inverting a quasi-robust classifier equipped with strong regularizers. Our proposed method, entitled MAGIC, leverages structured gradients from a pre-trained quasi-robust classifier to better preserve the input semantics while preserving its classification accuracy, thereby guaranteeing credibility in the synthesis. Unlike current methods that use complex primitives to supervise the process or use attention maps as a weak supervisory signal, MAGIC aggregates gradients over the input, driven by a guide binary mask that enforces a strong, spatial prior. MAGIC implements a series of manipulations with a single framework achieving shape and location control, intense non-rigid shape deformations, and copy/move operations in the presence of repeating objects and gives users firm control over the synthesis by requiring to simply specify binary guide masks. Our study and findin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.12617</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23884;&#20837;&#26159;&#36890;&#36807;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#23884;&#20837;&#21040;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#26469;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#32467;&#26500;&#25110;&#22522;&#20110;&#25551;&#36848;&#12290;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#20013;&#26377;&#38480;&#32467;&#26500;&#20449;&#24687;&#19979;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#12290;&#22522;&#20110;&#25551;&#36848;&#30340;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#20808;&#21069;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#36229;&#36234;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23384;&#22312;&#26114;&#36149;&#30340;&#36127;&#37319;&#26679;&#21644;&#38480;&#21046;&#24615;&#25551;&#36848;&#38656;&#27714;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMKE&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#65292;&#26088;&#22312;&#20016;&#23500;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#34920;&#36848;&#22522;&#20110;&#25551;&#36848;&#30340;&#30693;&#35782;&#23884;&#20837;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#35780;&#20215;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMKE&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#22522;&#20110;&#32467;&#26500;&#21644;&#22522;&#20110;&#20808;&#21069;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental resul
&lt;/p&gt;</description></item><item><title>N$^2$M$^2$&#26159;&#19968;&#20010;&#26032;&#30340;&#31227;&#21160;&#25805;&#32437;&#23548;&#33322;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#22797;&#26434;&#30340;&#38556;&#30861;&#29289;&#29615;&#22659;&#65292;&#21516;&#26102;&#20855;&#22791;&#26411;&#31471;&#25191;&#34892;&#22120;&#36712;&#36857;&#29983;&#25104;&#21644;&#23548;&#33322;&#25216;&#33021;&#30340;&#26080;&#32541;&#38598;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.08737</link><description>&lt;p&gt;
N$^2$M$^2$: &#22312;&#26410;&#30693;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#23398;&#20064;&#20219;&#24847;&#31227;&#21160;&#25805;&#32437;&#21160;&#20316;&#30340;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
N$^2$M$^2$: Learning Navigation for Arbitrary Mobile Manipulation Motions in Unseen and Dynamic Environments. (arXiv:2206.08737v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08737
&lt;/p&gt;
&lt;p&gt;
N$^2$M$^2$&#26159;&#19968;&#20010;&#26032;&#30340;&#31227;&#21160;&#25805;&#32437;&#23548;&#33322;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#22797;&#26434;&#30340;&#38556;&#30861;&#29289;&#29615;&#22659;&#65292;&#21516;&#26102;&#20855;&#22791;&#26411;&#31471;&#25191;&#34892;&#22120;&#36712;&#36857;&#29983;&#25104;&#21644;&#23548;&#33322;&#25216;&#33021;&#30340;&#26080;&#32541;&#38598;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#24037;&#19994;&#21644;&#26381;&#21153;&#26426;&#22120;&#20154;&#20013;&#20855;&#26377;&#37325;&#35201;&#24615;&#65292;&#31227;&#21160;&#25805;&#32437;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#23558;&#26411;&#31471;&#25191;&#34892;&#22120;&#36712;&#36857;&#29983;&#25104;&#19982;&#23548;&#33322;&#25216;&#33021;&#20197;&#21450;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#25512;&#29702;&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#25511;&#21046;&#22823;&#22411;&#37197;&#32622;&#31354;&#38388;&#65292;&#26080;&#27861;&#22312;&#21160;&#24577;&#21644;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#31227;&#21160;&#25805;&#32437;&#20219;&#21153;&#20998;&#35299;&#20026;&#20219;&#21153;&#31354;&#38388;&#20013;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#31616;&#21270;&#36816;&#21160;&#29983;&#25104;&#22120;&#21644;&#38024;&#23545;&#36816;&#21160;&#30340;&#36816;&#21160;&#22522;&#20934;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#23548;&#33322;&#31227;&#21160;&#25805;&#32437;&#65288;N$^2$M$^2$&#65289;&#65292;&#23427;&#23558;&#36825;&#31181;&#20998;&#35299;&#25512;&#24191;&#21040;&#22797;&#26434;&#30340;&#38556;&#30861;&#29615;&#22659;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#24773;&#20917;&#19979;&#30340;&#24191;&#27867;&#20219;&#21153;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25191;&#34892;&#26410;&#35265;&#36807;&#30340;&#12289;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#20219;&#21153;&#65292;&#24182;&#21363;&#26102;&#24212;&#23545;&#21160;&#24577;&#38556;&#30861;&#29289;&#21644;&#29615;&#22659;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its importance in both industrial and service robotics, mobile manipulation remains a significant challenge as it requires a seamless integration of end-effector trajectory generation with navigation skills as well as reasoning over long-horizons. Existing methods struggle to control the large configuration space, and to navigate dynamic and unknown environments. In previous work, we proposed to decompose mobile manipulation tasks into a simplified motion generator for the end-effector in task space and a trained reinforcement learning agent for the mobile base to account for kinematic feasibility of the motion. In this work, we introduce Neural Navigation for Mobile Manipulation (N$^2$M$^2$) which extends this decomposition to complex obstacle environments and enables it to tackle a broad range of tasks in real world settings. The resulting approach can perform unseen, long-horizon tasks in unexplored environments while instantly reacting to dynamic obstacles and environmental
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#35782;&#21035;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20013;&#30340;&#27169;&#24335;&#26469;&#25913;&#36827;&#23545;&#25239;&#36755;&#20837;&#30340;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2204.04636</link><description>&lt;p&gt;
&#8220;&#36825;&#26159;&#19968;&#20010;&#21487;&#30097;&#30340;&#21453;&#24212;&#65281;&#8221;&#65306;&#35299;&#35835;&#27010;&#29575;&#21464;&#21270;&#20197;&#26816;&#27979;NLP&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
"That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks. (arXiv:2204.04636v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04636
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25239;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#35782;&#21035;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20013;&#30340;&#27169;&#24335;&#26469;&#25913;&#36827;&#23545;&#25239;&#36755;&#20837;&#30340;&#35782;&#21035;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#26159;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#36825;&#20123;&#26377;&#24847;&#21046;&#20316;&#30340;&#36755;&#20837;&#29978;&#33267;&#21487;&#20197;&#27450;&#39575;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#26080;&#27861;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#37096;&#32626;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#20197;&#24320;&#21457;&#21487;&#38752;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#21516;&#26679;&#30340;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#28145;&#20837;&#25506;&#31350;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#25991;&#26412;&#31034;&#20363;&#30340;&#27169;&#22411;&#26080;&#20851;&#26816;&#27979;&#22120;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#25991;&#26412;&#26102;&#22312;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20013;&#35782;&#21035;&#27169;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#22120;&#22312;&#35782;&#21035;&#23545;&#25239;&#36755;&#20837;&#26041;&#38754;&#25552;&#39640;&#20102;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#30340;NLP&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#35789;&#32423;&#25915;&#20987;&#20013;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks are a major challenge faced by current machine learning research. These purposely crafted inputs fool even the most advanced models, precluding their deployment in safety-critical applications. Extensive research in computer vision has been carried to develop reliable defense strategies. However, the same issue remains less explored in natural language processing. Our work presents a model-agnostic detector of adversarial text examples. The approach identifies patterns in the logits of the target classifier when perturbing the input text. The proposed detector improves the current state-of-the-art performance in recognizing adversarial inputs and exhibits strong generalization capabilities across different NLP models, datasets, and word-level attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#25968;&#25454;&#36879;&#35270;&#20027;&#20041;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#34920;&#31034;&#27493;&#39588;&#12290;&#36825;&#31181;&#26041;&#27861;&#25972;&#21512;&#20102;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#35266;&#28857;&#21644;&#35282;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#26356;&#22810;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2109.04270</link><description>&lt;p&gt;
&#36208;&#21521;&#39044;&#27979;&#35745;&#31639;&#30340;&#36879;&#35270;&#20027;&#20041;&#30495;&#23454;&#24615;&#39564;&#35777;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
Toward a Perspectivist Turn in Ground Truthing for Predictive Computing. (arXiv:2109.04270v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#25968;&#25454;&#36879;&#35270;&#20027;&#20041;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#34920;&#31034;&#27493;&#39588;&#12290;&#36825;&#31181;&#26041;&#27861;&#25972;&#21512;&#20102;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#35266;&#28857;&#21644;&#35282;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#26356;&#22810;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#22522;&#20110;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#65292;&#32780;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26368;&#32456;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#27880;&#37322;&#36807;&#31243;&#36890;&#24120;&#20197;&#22810;&#25968;&#31080;&#20026;&#22522;&#30784;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25551;&#36848;&#24182;&#20513;&#23548;&#19968;&#31181;&#19981;&#21516;&#30340;&#33539;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25968;&#25454;&#36879;&#35270;&#20027;&#20041;&#65292;&#23427;&#25670;&#33073;&#20102;&#20256;&#32479;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#36716;&#21521;&#37319;&#29992;&#25972;&#21512;&#20154;&#31867;&#21442;&#19982;&#30340;&#35282;&#24230;&#21644;&#35266;&#28857;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#30693;&#35782;&#34920;&#31034;&#27493;&#39588;&#12290;&#20511;&#37492;&#21551;&#21457;&#25105;&#20204;&#25552;&#35758;&#30340;&#21069;&#20154;&#20316;&#21697;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#23545;&#20110;&#19981;&#20165;&#26159;&#26356;&#20027;&#35266;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#19982;&#20154;&#31867;&#35821;&#35328;&#26377;&#20851;&#30340;&#20219;&#21153;&#65289;&#65292;&#32780;&#19988;&#23545;&#20110;&#36890;&#24120;&#34987;&#35270;&#20026;&#23458;&#35266;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#21307;&#30103;&#20915;&#31574;&#65289;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#37319;&#29992;&#36879;&#35270;&#20027;&#20041;&#31435;&#22330;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#20248;&#21183;&#65292;&#20197;&#21450;&#21487;&#33021;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Most Artificial Intelligence applications are based on supervised machine learning (ML), which ultimately grounds on manually annotated data. The annotation process is often performed in terms of a majority vote and this has been proved to be often problematic, as highlighted by recent studies on the evaluation of ML models. In this article we describe and advocate for a different paradigm, which we call data perspectivism, which moves away from traditional gold standard datasets, towards the adoption of methods that integrate the opinions and perspectives of the human subjects involved in the knowledge representation step of ML processes. Drawing on previous works which inspired our proposal we describe the potential of our proposal for not only the more subjective tasks (e.g. those related to human language) but also to tasks commonly understood as objective (e.g. medical decision making), and present the main advantages of adopting a perspectivist stance in ML, as well as possible d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#27169;&#25311;&#20154;&#31867;&#21644;&#20154;&#24037;&#24773;&#24863;&#30340;&#26694;&#26550;&#65292;&#20026;&#20154;&#31867;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#19988;&#20026;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#35266;&#23519;&#26426;&#22120;&#24773;&#24863;&#21644;&#21160;&#26426;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2011.02151</link><description>&lt;p&gt;
&#27169;&#25311;&#20154;&#31867;&#21644;&#20154;&#24037;&#24773;&#24863; (SHArE)&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Simulation of Human and Artificial Emotion (SHArE). (arXiv:2011.02151v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.02151
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#27169;&#25311;&#20154;&#31867;&#21644;&#20154;&#24037;&#24773;&#24863;&#30340;&#26694;&#26550;&#65292;&#20026;&#20154;&#31867;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#19988;&#20026;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#35266;&#23519;&#26426;&#22120;&#24773;&#24863;&#21644;&#21160;&#26426;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20154;&#31867;&#21644;&#20154;&#24037;&#24773;&#24863; (SHArE) &#30340;&#26694;&#26550;&#25551;&#36848;&#20102;&#24773;&#24863;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#22312;&#24515;&#29702;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#36827;&#34892;&#21442;&#25968;&#36716;&#31227;&#12290;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#34987;&#23450;&#20041;&#20026;&#25277;&#35937;&#27010;&#24565;&#65292;&#20063;&#21487;&#20197;&#32454;&#21270;&#21040;&#20010;&#20307;&#31070;&#32463;&#20803;&#30340;&#30005;&#21387;&#27700;&#24179;&#12290;&#35813;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#35774;&#35745;&#20154;&#31867;&#30340;&#24773;&#24863;&#36712;&#36857;&#65292;&#20174;&#32780;&#21487;&#33021;&#20026;&#21508;&#31181;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#35266;&#23519;&#26426;&#22120;&#30340;&#24773;&#24863;&#21644;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
The framework for Simulation of Human and Artificial Emotion (SHArE) describes the architecture of emotion in terms of parameters transferable between psychology, neuroscience, and artificial intelligence. These parameters can be defined as abstract concepts or granularized down to the voltage levels of individual neurons. This model enables emotional trajectory design for humans which may lead to novel therapeutic solutions for various mental health concerns. For artificial intelligence, this work provides a compact notation which can be applied to neural networks as a means to observe the emotions and motivations of machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#65292;&#21457;&#29616;&#20154;&#20204;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#21644;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#32773;&#36827;&#23637;&#35843;&#25972;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2009.02476</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#25945;&#23398;&#30740;&#31350;&#25945;&#25480;&#24378;&#21270;&#23398;&#20064;&#32773;&#26102;&#20154;&#31867;&#30340;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners. (arXiv:2009.02476v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.02476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#65292;&#21457;&#29616;&#20154;&#20204;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#21644;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#32773;&#36827;&#23637;&#35843;&#25972;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#25104;&#21151;&#25945;&#23398;&#65292;&#38656;&#35201;&#23545;&#23398;&#20064;&#32773;&#23398;&#20064;&#26041;&#24335;&#36827;&#34892;&#20551;&#35774;&#65292;&#21363;&#23398;&#20064;&#32773;&#22914;&#20309;&#20351;&#29992;&#26469;&#33258;&#19990;&#30028;&#30340;&#32463;&#39564;&#26469;&#26356;&#26032;&#20854;&#20869;&#37096;&#29366;&#24577;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#12290;&#30740;&#31350;&#37325;&#28857;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861; Q-learning&#65292;&#36890;&#36807;&#34892;&#20026;&#23454;&#39564;&#32771;&#23519;&#20154;&#20204;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#36798;&#21040;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#35268;&#33539;&#26631;&#20934;&#65292;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#26426;&#22120;&#25945;&#23398;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#26426;&#22120;&#25945;&#23398;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#26041;&#27861;&#26469;&#27169;&#25311;&#23398;&#20064;&#32773;&#22312;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#23398;&#20064;&#39044;&#27979;&#21453;&#39304;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#32773;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#22312;&#25945;&#25480;&#29702;&#24819;&#21270;&#30340;&#25506;&#32034;&#21033;&#29992;&#20219;&#21153;&#26102;&#65292;&#20154;&#20204;&#23545;&#23398;&#20064;&#32773;&#30340;&#23398;&#20064;&#21644;&#25240;&#25187;&#29575;&#26377;&#21738;&#20123;&#20551;&#35774;&#65311;&#22312;&#34892;&#20026;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#30456;&#23545;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#25945;&#23548; Q-&#23398;&#20064;&#32773;&#36825;&#39033;&#20219;&#21153;&#12290;&#20154;&#20204;&#20542;&#21521;&#20110;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#65292;&#24182;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#20250;&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#36827;&#23637;&#35843;&#25972;&#33258;&#24049;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful teaching requires an assumption of how the learner learns - how the learner uses experiences from the world to update their internal states. We investigate what expectations people have about a learner when they teach them in an online manner using rewards and punishment. We focus on a common reinforcement learning method, Q-learning, and examine what assumptions people have using a behavioral experiment. To do so, we first establish a normative standard, by formulating the problem as a machine teaching optimization problem. To solve the machine teaching optimization problem, we use a deep learning approximation method which simulates learners in the environment and learns to predict how feedback affects the learner's internal states. What do people assume about a learner's learning and discount rates when they teach them an idealized exploration-exploitation task? In a behavioral experiment, we find that people can teach the task to Q-learners in a relatively efficient and 
&lt;/p&gt;</description></item></channel></rss>