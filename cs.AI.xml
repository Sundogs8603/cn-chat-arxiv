<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#26377;&#38480;&#20803;&#20998;&#26512;&#26694;&#26550;&#26469;&#20010;&#24615;&#21270;&#20272;&#35745;&#24515;&#33039;&#32452;&#32455;&#30340;&#34987;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#36890;&#36807;&#23884;&#22871;&#20248;&#21270;&#26041;&#26696;&#30340;&#20351;&#29992;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36924;&#36817;&#21305;&#37197;&#22270;&#20687;&#25968;&#25454;&#30340;&#26448;&#26009;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.02807</link><description>&lt;p&gt;
&#19968;&#20010;&#20010;&#24615;&#21270;&#34987;&#21160;&#24515;&#33039;&#21147;&#23398;&#30340;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Optimization Framework to Personalize Passive Cardiac Mechanics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#26377;&#38480;&#20803;&#20998;&#26512;&#26694;&#26550;&#26469;&#20010;&#24615;&#21270;&#20272;&#35745;&#24515;&#33039;&#32452;&#32455;&#30340;&#34987;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#36890;&#36807;&#23884;&#22871;&#20248;&#21270;&#26041;&#26696;&#30340;&#20351;&#29992;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36924;&#36817;&#21305;&#37197;&#22270;&#20687;&#25968;&#25454;&#30340;&#26448;&#26009;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#24515;&#33039;&#21147;&#23398;&#24314;&#27169;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#24515;&#33039;&#21151;&#33021;&#22312;&#20581;&#24247;&#21644;&#30142;&#30149;&#20013;&#30340;&#29983;&#29289;&#21147;&#23398;&#24182;&#24110;&#21161;&#27835;&#30103;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#20165;&#38480;&#20110;&#20351;&#29992;&#22312;&#21333;&#19968;&#24515;&#33039;&#30456;&#20301;&#33719;&#21462;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#36890;&#24120;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#20687;&#33719;&#21462;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36870;&#26377;&#38480;&#20803;&#20998;&#26512;&#65288;iFEA&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#26469;&#20272;&#35745;&#24515;&#33039;&#32452;&#32455;&#30340;&#34987;&#21160;&#26426;&#26800;&#29305;&#24615;&#12290;&#35813;iFEA&#26694;&#26550;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#22871;&#20248;&#21270;&#26041;&#26696;&#65292;&#20854;&#20013;&#22806;&#37096;&#36845;&#20195;&#21033;&#29992;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#26469;&#26368;&#20339;&#36924;&#36817;&#21305;&#37197;&#22270;&#20687;&#25968;&#25454;&#30340;&#26448;&#26009;&#21442;&#25968;&#65292;&#32780;&#20869;&#37096;&#36845;&#20195;&#37319;&#29992;&#22686;&#24191;Sellier&#31639;&#27861;&#26469;&#20272;&#35745;&#26080;&#24212;&#21147;&#21442;&#32771;&#26500;&#22411;&#12290;&#37325;&#28857;&#25918;&#22312;&#34920;&#24449;&#34987;&#21160;&#26426;&#26800;&#34892;&#20026;&#19978;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02807v1 Announce Type: cross  Abstract: Personalized cardiac mechanics modeling is a powerful tool for understanding the biomechanics of cardiac function in health and disease and assisting in treatment planning. However, current models are limited to using medical images acquired at a single cardiac phase, often limiting their applicability for processing dynamic image acquisitions. This study introduces an inverse finite element analysis (iFEA) framework to estimate the passive mechanical properties of cardiac tissue using time-dependent medical image data. The iFEA framework relies on a novel nested optimization scheme, in which the outer iterations utilize a traditional optimization method to best approximate material parameters that fit image data, while the inner iterations employ an augmented Sellier's algorithm to estimate the stress-free reference configuration. With a focus on characterizing the passive mechanical behavior, the framework employs structurally based 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02761</link><description>&lt;p&gt;
AQuA --&#32467;&#21512;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#35266;&#28857;&#65292;&#21033;&#29992;LLMs&#35780;&#20272;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#30923;&#21830;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25919;&#27835;&#22312;&#32447;&#35752;&#35770;&#20013;&#34913;&#37327;&#36129;&#29486;&#36136;&#37327;&#23545;&#20110;&#30740;&#31350;&#30923;&#21830;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#33258;&#21160;&#34913;&#37327;&#36825;&#20123;&#25351;&#26631;&#21464;&#24471;&#21487;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AQuA&#65292;&#23427;&#26159;&#19968;&#20010;&#28155;&#21152;&#20998;&#25968;&#65292;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#35745;&#31639;&#27599;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#12290;&#19982;&#20854;&#20182;&#29305;&#23450;&#20998;&#25968;&#19981;&#21516;&#65292;AQuA&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#23384;&#22312;&#30340;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02761v1 Announce Type: cross  Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#36890;&#36807;&#25506;&#32034;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#19981;&#21516;&#25968;&#25454;&#23646;&#24615;&#34920;&#31034;&#26469;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02656</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#38750;&#36127;&#23376;&#31354;&#38388;&#29305;&#24449;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Non-negative Subspace Feature Representation for Few-shot Learning in Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#36890;&#36807;&#25506;&#32034;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#19981;&#21516;&#25968;&#25454;&#23646;&#24615;&#34920;&#31034;&#26469;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#35270;&#35273;&#22330;&#26223;&#35782;&#21035;&#39046;&#22495;&#19981;&#21516;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35775;&#38382;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#21307;&#23398;&#22270;&#20687;&#35299;&#37322;&#24448;&#24448;&#21463;&#21040;&#25968;&#25454;&#30701;&#32570;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#25506;&#32034;&#19981;&#21516;&#25968;&#25454;&#23646;&#24615;&#34920;&#31034;&#65292;&#30740;&#31350;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#21307;&#23398;&#24433;&#20687;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;NMF&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#20854;&#30417;&#30563;&#21464;&#20307;&#65288;&#22914;&#65292;&#26377;&#24046;&#21035;&#24615;NMF&#65292;&#20197;&#21450;&#20855;&#26377;&#31232;&#30095;&#24615;&#30340;&#30417;&#30563;&#21644;&#32422;&#26463;NMF&#65289;&#65292;&#24182;&#19982;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21363;&#20174;&#29305;&#24449;&#21521;&#37327;&#20013;&#23548;&#20986;&#30340;&#22522;&#20110;&#21327;&#20316;&#34920;&#31034;&#30340;&#38477;&#32500;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02656v1 Announce Type: cross  Abstract: Unlike typical visual scene recognition domains, in which massive datasets are accessible to deep neural networks, medical image interpretations are often obstructed by the paucity of data. In this paper, we investigate the effectiveness of data-based few-shot learning in medical imaging by exploring different data attribute representations in a low-dimensional space. We introduce different types of non-negative matrix factorization (NMF) in few-shot learning, addressing the data scarcity issue in medical image classification. Extensive empirical studies are conducted in terms of validating the effectiveness of NMF, especially its supervised variants (e.g., discriminative NMF, and supervised and constrained NMF with sparseness), and the comparison with principal component analysis (PCA), i.e., the collaborative representation-based dimensionality reduction technique derived from eigenvectors. With 14 different datasets covering 11 dist
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26356;&#24378;&#30340;&#24179;&#22343;&#24773;&#20917;&#35745;&#31639;&#20998;&#31163;&#65292;&#23545;&#20110;&#8220;&#20856;&#22411;&#8221;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#20219;&#21153;&#23454;&#20363;&#65292;&#21333;&#27169;&#24577;&#23398;&#20064;&#22312;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#22810;&#27169;&#24577;&#23398;&#20064;&#21364;&#24456;&#23481;&#26131;&#12290;</title><link>https://arxiv.org/abs/2404.02254</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#27169;&#24577;&#19982;&#21333;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#26356;&#24378;&#30340;&#35745;&#31639;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02254
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26356;&#24378;&#30340;&#24179;&#22343;&#24773;&#20917;&#35745;&#31639;&#20998;&#31163;&#65292;&#23545;&#20110;&#8220;&#20856;&#22411;&#8221;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#20219;&#21153;&#23454;&#20363;&#65292;&#21333;&#27169;&#24577;&#23398;&#20064;&#22312;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#22810;&#27169;&#24577;&#23398;&#20064;&#21364;&#24456;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23558;&#22810;&#31181;&#25968;&#25454;&#27169;&#24577;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#32467;&#21512;&#36215;&#26469;&#20197;&#20419;&#36827;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#36825;&#20173;&#28982;&#36866;&#29992;&#20110;&#30456;&#24212;&#30340;&#21333;&#27169;&#24577;&#20219;&#21153;&#65288;&#20363;&#22914;&#25991;&#26412;&#29983;&#25104;&#65289;&#12290;&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#32463;&#39564;&#25104;&#21151;&#65288;&#20363;&#22914;GPT-4&#65289;&#12290;&#21463;&#21040;&#20026;&#36825;&#31181;&#32463;&#39564;&#25104;&#21151;&#24320;&#21457;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#65292;Lu&#65288;NeurIPS '23&#65292;ALT '24&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#23398;&#20064;&#29702;&#35770;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#23398;&#20064;&#30340;&#29702;&#35770;&#27169;&#22411;&#20043;&#38388;&#21487;&#33021;&#30340;&#20998;&#31163;&#12290;&#29305;&#21035;&#26159;Lu&#65288;ALT '24&#65289;&#23637;&#31034;&#20102;&#19968;&#31181;&#35745;&#31639;&#20998;&#31163;&#65292;&#36825;&#23545;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#22351;&#24773;&#20917;&#23454;&#20363;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02254v1 Announce Type: cross  Abstract: In multimodal machine learning, multiple modalities of data (e.g., text and images) are combined to facilitate the learning of a better machine learning model, which remains applicable to a corresponding unimodal task (e.g., text generation). Recently, multimodal machine learning has enjoyed huge empirical success (e.g. GPT-4). Motivated to develop theoretical justification for this empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning, and considers possible separations between theoretical models of multimodal and unimodal learning. In particular, Lu (ALT '24) shows a computational separation, which is relevant to worst-case instances of the learning task.   In this paper, we give a stronger average-case computational separation, where for "typical" instances of the learning task, unimodal learning is computationally hard, but multimodal learning is easy. We then question how "organic" the average-cas
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;</title><link>https://arxiv.org/abs/2404.02060</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Long-context LLMs Struggle with Long In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#36229;&#36807;32K&#26631;&#35760;&#30340;&#38271;&#24207;&#21015;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#35780;&#20272;&#20027;&#35201;&#23616;&#38480;&#22312;&#22256;&#24785;&#24230;&#21644;&#21512;&#25104;&#20219;&#21153;&#31561;&#25351;&#26631;&#19978;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#23427;&#20204;&#22312;&#26356;&#24494;&#22937;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934;&#65288;LIConBench&#65289;&#65292;&#30528;&#37325;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#12290;&#25105;&#20204;&#31934;&#24515;&#36873;&#25321;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#26631;&#31614;&#33539;&#22260;&#36328;&#24230;&#20026;28&#33267;174&#31867;&#65292;&#28085;&#30422;&#20102;&#20174;2K&#21040;50K&#30340;&#19981;&#21516;&#36755;&#20837;&#65288;&#23569;&#37327;&#28436;&#31034;&#65289;&#38271;&#24230;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35201;&#27714;LLMs&#29702;&#35299;&#25972;&#20010;&#36755;&#20837;&#65292;&#20197;&#35782;&#21035;&#24222;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#20197;&#36827;&#34892;&#27491;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;13&#20010;&#38271;&#19978;&#19979;&#25991;LLMs&#12290;&#25105;&#20204;&#21457;&#29616;&#38271;&#19978;&#19979;&#25991;LLMs&#22312;&#26631;&#35760;&#38271;&#24230;&#20026;20K&#20197;&#19979;&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#65292;&#24182;&#19988;&#21033;&#29992;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#20250;&#24102;&#26469;&#24615;&#33021;&#19978;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02060v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;AutoML&#25216;&#26415;&#26368;&#22823;&#21270;Deep Shift&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#20445;&#30495;&#24230;HPO&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01965</link><description>&lt;p&gt;
&#26088;&#22312;&#21033;&#29992;AutoML&#23454;&#29616;&#21487;&#25345;&#32493;&#28145;&#24230;&#23398;&#20064;&#65306;&#22522;&#20110;Deep Shift&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;HPO&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Leveraging AutoML for Sustainable Deep Learning: A Multi-Objective HPO Approach on Deep Shift Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;AutoML&#25216;&#26415;&#26368;&#22823;&#21270;Deep Shift&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#20445;&#30495;&#24230;HPO&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#22797;&#26434;&#27169;&#24335;&#25512;&#21160;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;DL&#27169;&#22411;&#30340;&#35745;&#31639;&#38656;&#27714;&#24102;&#26469;&#20102;&#29615;&#22659;&#21644;&#36164;&#28304;&#25361;&#25112;&#12290;Deep Shift&#31070;&#32463;&#32593;&#32476;&#65288;DSNN&#65289;&#21033;&#29992;shift&#25805;&#20316;&#20943;&#23569;&#25512;&#29702;&#26102;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20026;&#27492;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20511;&#37492;&#26631;&#20934;DNN&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#26377;&#20852;&#36259;&#36890;&#36807;AutoML&#25216;&#26415;&#20805;&#20998;&#21457;&#25381;DSNN&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#26368;&#22823;&#21270;DSNN&#24615;&#33021;&#21516;&#26102;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#23558;&#20934;&#30830;&#24615;&#21644;&#33021;&#32791;&#20316;&#20026;&#21487;&#33021;&#20114;&#34917;&#30446;&#26631;&#32467;&#21512;&#30340;&#22810;&#30446;&#26631;&#65288;MO&#65289;&#20248;&#21270;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26368;&#20808;&#36827;&#30340;&#22810;&#20445;&#30495;&#24230;&#65288;MF&#65289;HPO&#19982;&#22810;&#30446;&#26631;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24471;&#21040;&#20102;&#20934;&#30830;&#29575;&#36229;&#36807;80&#65285;&#19988;&#35745;&#31639;&#20302;&#32791;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01965v1 Announce Type: cross  Abstract: Deep Learning (DL) has advanced various fields by extracting complex patterns from large datasets. However, the computational demands of DL models pose environmental and resource challenges. Deep shift neural networks (DSNNs) offer a solution by leveraging shift operations to reduce computational complexity at inference. Following the insights from standard DNNs, we are interested in leveraging the full potential of DSNNs by means of AutoML techniques. We study the impact of hyperparameter optimization (HPO) to maximize DSNN performance while minimizing resource consumption. Since this combines multi-objective (MO) optimization with accuracy and energy consumption as potentially complementary objectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with multi-objective optimization. Experimental results demonstrate the effectiveness of our approach, resulting in models with over 80\% in accuracy and low computational 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01685</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30001;&#20110;&#20854;&#31232;&#30095;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#25805;&#20316;&#32780;&#33021;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#25552;&#20379;&#36229;&#20302;&#21151;&#32791;/&#33021;&#32791;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#38656;&#35201;&#26356;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#25165;&#33021;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#24212;&#29992;&#19981;&#22826;&#36866;&#21512;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#20197;&#21487;&#25509;&#21463;&#30340;&#20869;&#23384;&#21344;&#29992;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;SNNs&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;SNNs&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#23398;&#12290;&#20854;&#20851;&#38190;&#27493;&#39588;&#21253;&#25324;&#35843;&#26597;&#19981;&#21516;&#26680;&#22823;&#23567;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#35774;&#35745;&#26032;&#30340;&#26680;&#22823;&#23567;&#38598;&#21512;&#65292;&#22522;&#20110;&#36873;&#23450;&#30340;&#26680;&#22823;&#23567;&#29983;&#25104;SNN&#26550;&#26500;&#65292;&#24182;&#20998;&#26512;SNN&#27169;&#22411;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;-&#20869;&#23384;&#25240;&#34935;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#23545;&#20110;CIFAR10&#26377;93.24%&#30340;&#20934;&#30830;&#24230;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01685v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations. Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications. Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed. Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling. Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection. The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00450</link><description>&lt;p&gt;
&#35268;&#21010;&#21644;&#32534;&#36753;&#26816;&#32034;&#20197;&#22686;&#24378;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Planning and Editing What You Retrieve for Enhanced Tool Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23558;&#22806;&#37096;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#25171;&#24320;&#20102;&#26032;&#30340;&#39046;&#22495;&#65292;&#24212;&#29992;&#33539;&#22260;&#28085;&#30422;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#22120;&#21644;&#26234;&#33021;&#21161;&#25163;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#19968;&#27425;&#24615;&#26816;&#32034;&#31574;&#30053;&#65292;&#26080;&#27861;&#26377;&#25928;&#20934;&#30830;&#22320;&#31579;&#36873;&#30456;&#20851;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#65288;P&amp;R&#65289;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#65288;E&amp;G&#65289;&#8221;&#33539;&#24335;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#20102;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#35268;&#21010;&#22120;&#65292;&#20197;&#22686;&#24378;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00450v1 Announce Type: new  Abstract: Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&amp;R)'' and ``Edit-and-Ground (E\&amp;G)'' paradigms. The P\&amp;R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&amp;G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.20208</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#39044;&#27979;&#34920;&#26684;&#20219;&#21153;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#65292;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#32570;&#22833;&#20540;&#22635;&#20805;&#31561;&#39044;&#27979;&#20219;&#21153;&#26159;&#19982;&#34920;&#26684;&#25968;&#25454;&#30456;&#20851;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#20219;&#21153;&#12290;&#23613;&#31649;LLMs&#25797;&#38271;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#65292;&#20294;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25910;&#38598;&#24102;&#26377;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;Llama-2&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;-shot&#39044;&#27979;&#12289;&#23569;-shot&#39044;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18802</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-form factuality in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#24320;&#25918;&#24615;&#20027;&#39064;&#30340;&#20107;&#23454;&#24615;&#25552;&#31034;&#26102;&#65292;&#32463;&#24120;&#29983;&#25104;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#23545;&#27169;&#22411;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;LongFact&#30340;&#25552;&#31034;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#22218;&#25324;38&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;Search-Augmented Factuality Evaluator&#65288;SAFE&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#22120;&#12290;SAFE&#21033;&#29992;LLM&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#29420;&#30340;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#20197;&#21450;&#30830;&#23450;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#24471;&#21040;&#25628;&#32034;&#32467;&#26524;&#25903;&#25345;&#30340;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#35780;&#20272;&#27599;&#20010;&#20107;&#23454;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#23558;F1&#20998;&#25968;&#25193;&#23637;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#22238;&#24212;&#20013;&#25903;&#25345;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#65288;&#31934;&#24230;&#65289;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#39033;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23545;&#21512;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20197;&#25506;&#35752;&#20154;&#31867;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#30340;&#26131;&#21463;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.16760</link><description>&lt;p&gt;
&#21644;&#25243;&#30828;&#24065;&#19968;&#26679;&#22909;&#65306;&#20154;&#31867;&#23545;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
As Good As A Coin Toss Human detection of AI-generated images, videos, audio, and audiovisual stimuli
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16760
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#39033;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23545;&#21512;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20197;&#25506;&#35752;&#20154;&#31867;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#30340;&#26131;&#21463;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21512;&#25104;&#23186;&#20307;&#21464;&#24471;&#36234;&#26469;&#36234;&#36924;&#30495;&#65292;&#20351;&#29992;&#23427;&#30340;&#38556;&#30861;&#19981;&#26029;&#38477;&#20302;&#65292;&#36825;&#39033;&#25216;&#26415;&#36234;&#26469;&#36234;&#34987;&#24694;&#24847;&#21033;&#29992;&#65292;&#20174;&#37329;&#34701;&#27450;&#35784;&#21040;&#38750;&#33258;&#24895;&#33394;&#24773;&#12290;&#20170;&#22825;&#65292;&#23545;&#25239;&#34987;&#21512;&#25104;&#23186;&#20307;&#35823;&#23548;&#30340;&#20027;&#35201;&#38450;&#24481;&#20381;&#36182;&#20110;&#20154;&#31867;&#35266;&#23519;&#32773;&#22312;&#35270;&#35273;&#21644;&#21548;&#35273;&#19978;&#21306;&#20998;&#30495;&#20551;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#38469;&#19978;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#26377;&#22810;&#33030;&#24369;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21253;&#21547;1276&#21517;&#21442;&#19982;&#32773;&#30340;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20154;&#20204;&#22312;&#21306;&#20998;&#21512;&#25104;&#22270;&#20687;&#12289;&#20165;&#38899;&#39057;&#12289;&#20165;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#20934;&#30830;&#24615;&#22914;&#20309;&#12290;&#20026;&#20102;&#21453;&#26144;&#20154;&#20204;&#22312;&#37326;&#22806;&#21487;&#33021;&#36935;&#21040;&#21512;&#25104;&#23186;&#20307;&#30340;&#24773;&#20917;&#65292;&#27979;&#35797;&#26465;&#20214;&#21644;&#21050;&#28608;&#27169;&#25311;&#20102;&#20856;&#22411;&#30340;&#22312;&#32447;&#24179;&#21488;&#65292;&#32780;&#35843;&#26597;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#21512;&#25104;&#23186;&#20307;&#22343;&#26469;&#33258;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16760v1 Announce Type: cross  Abstract: As synthetic media becomes progressively more realistic and barriers to using it continue to lower, the technology has been increasingly utilized for malicious purposes, from financial fraud to nonconsensual pornography. Today, the principal defense against being misled by synthetic media relies on the ability of the human observer to visually and auditorily discern between real and fake. However, it remains unclear just how vulnerable people actually are to deceptive synthetic media in the course of their day to day lives. We conducted a perceptual study with 1276 participants to assess how accurate people were at distinguishing synthetic images, audio only, video only, and audiovisual stimuli from authentic. To reflect the circumstances under which people would likely encounter synthetic media in the wild, testing conditions and stimuli emulated a typical online platform, while all synthetic media used in the survey was sourced from 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20449;&#20219;&#26159;&#25511;&#21046;&#20854;&#20256;&#25773;&#31243;&#24230;&#30340;&#35843;&#33410;&#22120;&#65292;&#36890;&#36807;&#22686;&#21152;&#20449;&#20219;&#21644;&#20943;&#23569;&#19981;&#20449;&#20219;&#65292;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#30340;&#37319;&#29992;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14680</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20449;&#20219;: &#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Trust in AI: Progress, Challenges, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14680
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20449;&#20219;&#26159;&#25511;&#21046;&#20854;&#20256;&#25773;&#31243;&#24230;&#30340;&#35843;&#33410;&#22120;&#65292;&#36890;&#36807;&#22686;&#21152;&#20449;&#20219;&#21644;&#20943;&#23569;&#19981;&#20449;&#20219;&#65292;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#30340;&#37319;&#29992;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#36890;&#36807;&#21508;&#31181;&#24212;&#29992;&#12289;&#26381;&#21153;&#21644;&#20135;&#21697;&#65292;&#35828;&#26126;&#20102;&#26469;&#33258;&#29992;&#25143;&#35282;&#24230;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;/&#19981;&#20449;&#20219;&#30340;&#37325;&#35201;&#24615;&#12290;&#19982;&#20854;&#20182;&#25216;&#26415;&#30456;&#27604;&#65292;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31995;&#32479;&#19981;&#20165;&#20316;&#20026;&#19968;&#20123;&#26377;&#30410;&#24037;&#20855;&#24191;&#27867;&#28183;&#36879;&#21040;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#65292;&#32780;&#19988;&#36824;&#20250;&#25104;&#20026;&#20195;&#34920;&#25105;&#20204;&#30340;&#26367;&#20195;&#24615;&#20195;&#29702;&#20154;&#65292;&#25110;&#32773;&#20250;&#24433;&#21709;&#20154;&#31867;&#24605;&#32500;&#12289;&#20915;&#31574;&#21644;&#34892;&#21160;&#30340;&#25805;&#32437;&#24615;&#24515;&#26234;&#12290;&#36817;&#26469;&#65292;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#20851;&#27880;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#20449;&#20219;/&#19981;&#20449;&#20219;&#30340;&#19981;&#21516;&#32500;&#24230;&#21450;&#20854;&#30456;&#20851;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#31687;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#20013;&#65292;&#22312;&#23545;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#25991;&#29486;&#20013;&#23545;&#20449;&#20219;&#30340;&#27010;&#24565;&#21270;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14680v1 Announce Type: cross  Abstract: The increasing use of artificial intelligence (AI) systems in our daily life through various applications, services, and products explains the significance of trust/distrust in AI from a user perspective. AI-driven systems (as opposed to other technologies) have ubiquitously diffused in our life not only as some beneficial tools to be used by human agents but also are going to be substitutive agents on our behalf, or manipulative minds that would influence human thought, decision, and agency. Trust/distrust in AI plays the role of a regulator and could significantly control the level of this diffusion, as trust can increase, and distrust may reduce the rate of adoption of AI. Recently, varieties of studies have paid attention to the variant dimension of trust/distrust in AI, and its relevant considerations. In this systematic literature review, after conceptualization of trust in the current AI literature review, we will investigate tr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Agent Group Chat&#27169;&#25311;&#65292;&#30740;&#31350;&#20102;&#35821;&#35328;&#22312;&#20154;&#31867;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#25925;&#20107;&#24773;&#33410;&#19979;&#65292;&#20195;&#29702;&#20154;&#34920;&#29616;&#20986;&#20102;&#24847;&#26009;&#20043;&#22806;&#19988;&#37325;&#35201;&#30340;&#26032;&#20852;&#34892;&#20026;&#65292;&#36890;&#36807;&#35843;&#25972;&#29615;&#22659;&#35774;&#32622;&#21487;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.13433</link><description>&lt;p&gt;
&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#65306;&#19968;&#31181;&#20132;&#20114;&#24335;&#32676;&#32452;&#32842;&#22825;&#25311;&#30495;&#20307;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#24341;&#21457;&#38598;&#20307;&#26032;&#20852;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13433
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Agent Group Chat&#27169;&#25311;&#65292;&#30740;&#31350;&#20102;&#35821;&#35328;&#22312;&#20154;&#31867;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#25925;&#20107;&#24773;&#33410;&#19979;&#65292;&#20195;&#29702;&#20154;&#34920;&#29616;&#20986;&#20102;&#24847;&#26009;&#20043;&#22806;&#19988;&#37325;&#35201;&#30340;&#26032;&#20852;&#34892;&#20026;&#65292;&#36890;&#36807;&#35843;&#25972;&#29615;&#22659;&#35774;&#32622;&#21487;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25506;&#35752;&#35821;&#35328;&#22312;&#20154;&#31867;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#27169;&#25311;&#65292;&#27169;&#25311;&#22810;&#20195;&#29702;&#20043;&#38388;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#35821;&#35328;&#20132;&#20114;&#12290;&#20195;&#29702;&#20154;&#34987;&#35201;&#27714;&#22312;&#35813;&#27169;&#25311;&#20013;&#33258;&#30001;&#32842;&#22825;&#65292;&#22522;&#20110;&#20854;&#35282;&#33394;&#35774;&#23450;&#36861;&#27714;&#21508;&#33258;&#30340;&#30446;&#30340;&#65292;&#26088;&#22312;&#35266;&#23519;&#20195;&#29702;&#20154;&#23637;&#29616;&#20986;&#26082;&#24847;&#26009;&#19981;&#21040;&#21448;&#26174;&#33879;&#30340;&#26032;&#20852;&#34892;&#20026;&#12290;&#23558;&#22235;&#20010;&#21465;&#20107;&#22330;&#26223;&#65288;&#32487;&#25215;&#20105;&#35758;&#12289;&#27861;&#24237;&#36777;&#35770;&#12289;&#21746;&#23398;&#36766;&#35828;&#12289;&#30005;&#24433;&#35282;&#33394;&#20105;&#35758;&#65289;&#25972;&#21512;&#21040;&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#20013;&#65292;&#20197;&#35780;&#20272;&#20854;&#25903;&#25345;&#22810;&#26679;&#21270;&#25925;&#20107;&#24773;&#33410;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#20013;&#37197;&#32622;&#29305;&#23450;&#30340;&#29615;&#22659;&#35774;&#32622;&#65292;&#25105;&#20204;&#33021;&#22815;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#35282;&#33394;&#21457;&#35328;&#30340;&#25152;&#26377;&#20869;&#23481;&#30340;n-gram Shannon&#29109;&#26469;&#35780;&#20272;&#29615;&#22659;&#20013;&#30340;&#28151;&#20081;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20195;&#29702;&#20154;&#20855;&#26377;&#23376;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13433v1 Announce Type: cross  Abstract: To investigate the role of language in human collective behaviors, we developed the Agent Group Chat simulation to simulate linguistic interactions among multi-agent in different settings. Agents are asked to free chat in this simulation for their own purposes based on their character setting, aiming to see agents exhibit emergent behaviours that are both unforeseen and significant. Four narrative scenarios, Inheritance Disputes, Law Court Debates, Philosophical Discourses, Movie Casting Contention, are integrated into Agent Group Chat to evaluate its support for diverse storylines. By configuring specific environmental settings within Agent Group Chat, we are able to assess whether agents exhibit behaviors that align with human expectations. We evaluate the disorder within the environment by computing the n-gram Shannon entropy of all the content speak by characters. Our findings reveal that under the premise of agents possessing subs
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;</title><link>https://arxiv.org/abs/2403.09057</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Continued Pretrained LLM Approach for Automatic Medical Note Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#27491;&#22312;&#38761;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#26368;&#24378;&#22823;&#30340;LLM&#23545;&#20110;&#22823;&#22810;&#25968;&#39046;&#22495;&#29305;&#23450;&#22330;&#26223;&#26469;&#35828;&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36830;&#32493;&#35757;&#32451;&#30340;130&#20159;&#21442;&#25968; Llama2-basd LLM&#65292;&#19987;&#20026;&#21307;&#30103;&#23545;&#35805;&#32780;&#35774;&#35745;&#65292;&#24182;&#22312;&#33258;&#21160;&#35760;&#24405;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;PubMedQA&#20013;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;76.6&#65285;&#65292;&#22312;&#24635;&#32467;&#21307;&#30103;&#23545;&#35805;&#20026;SOAP&#31508;&#35760;&#26041;&#38754;&#19982;GPT-4&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#26041;&#38754;&#36229;&#36807;&#20102;GPT-4&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09057v1 Announce Type: cross  Abstract: LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#23450;&#20041;&#25351;&#26631;&#26469;&#25913;&#36827;&#25512;&#26029;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.04182</link><description>&lt;p&gt;
&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Metric-aware LLM inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04182
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#23450;&#20041;&#25351;&#26631;&#26469;&#25913;&#36827;&#25512;&#26029;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36755;&#20986;&#26159;&#36890;&#36807;&#20174;LLM&#30340;&#22522;&#30784;&#20998;&#24067;&#20013;&#36827;&#34892;&#33258;&#22238;&#24402;&#37319;&#26679;&#33719;&#24471;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#25512;&#26029;&#31574;&#30053;&#23545;&#20110;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#65306;&#19968;&#31181;&#22312;&#25512;&#26029;&#26102;&#38024;&#23545;&#33258;&#23450;&#20041;&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#20915;&#31574;&#29702;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#23398;&#26415;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20844;&#24320;&#21487;&#29992;&#27169;&#22411;&#19978;&#25253;&#21578;&#20102;&#30456;&#23545;&#22522;&#32447;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04182v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated strong results on a range of NLP tasks. Typically, outputs are obtained via autoregressive sampling from the LLM's underlying distribution. We show that this inference strategy can be suboptimal for a range of tasks and associated evaluation metrics. As a remedy, we propose metric aware LLM inference: a decision theoretic approach optimizing for custom metrics at inference time. We report improvements over baselines on academic benchmarks and publicly available models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#35782;&#21035;&#20197;&#21450;&#22806;&#37096;NER&#20381;&#36182;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02975</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02975
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#35782;&#21035;&#20197;&#21450;&#22806;&#37096;NER&#20381;&#36182;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#35821;&#20041;&#21305;&#37197;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#22312;&#31038;&#21306;&#38382;&#31572;&#12289;&#25628;&#32034;&#12289;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25512;&#33616;&#31561;&#21508;&#31181;&#37325;&#35201;&#22330;&#26223;&#20013;&#20855;&#26377;&#30456;&#24403;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DC-Match&#26469;&#35299;&#24320;&#21477;&#23376;&#20013;&#30340;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#20248;&#21270;&#21305;&#37197;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#20808;&#36827;&#27169;&#22411;&#30452;&#25509;&#27169;&#25311;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#21333;&#35789;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#32780;&#24573;&#30053;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;DC-Match&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#21305;&#37197;&#26041;&#27861;&#65292;&#20294;&#23427;&#39640;&#24230;&#20381;&#36182;&#22806;&#37096;NER&#25216;&#26415;&#26469;&#35782;&#21035;&#21477;&#23376;&#30340;&#20851;&#38190;&#35789;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#27425;&#35201;&#35821;&#35328;&#30340;&#35821;&#20041;&#21305;&#37197;&#24615;&#33021;&#65292;&#22240;&#20026;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;NER&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02975v1 Announce Type: cross  Abstract: Sentence semantic matching is a research hotspot in natural language processing, which is considerably significant in various key scenarios, such as community question answering, searching, chatbot, and recommendation. Since most of the advanced models directly model the semantic relevance among words between two sentences while neglecting the \textit{keywords} and \textit{intents} concepts of them, DC-Match is proposed to disentangle keywords from intents and utilizes them to optimize the matching performance. Although DC-Match is a simple yet effective method for semantic matching, it highly depends on the external NER techniques to identify the keywords of sentences, which limits the performance of semantic matching for minor languages since satisfactory NER tools are usually hard to obtain. In this paper, we propose to generally and flexibly resolve the text into multi concepts for multilingual semantic matching to liberate the mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21463;&#21160;&#28459;&#21046;&#20316;&#21551;&#21457;&#30340;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21160;&#28459;&#21046;&#20316;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#20986;&#19981;&#38656;&#35201;&#35270;&#39057;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#21160;&#28459;&#22270;&#20687;&#25910;&#38598;&#27969;&#27700;&#32447;&#21644;API&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21160;&#28459;&#29305;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;&#24102;&#26469;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.01598</link><description>&lt;p&gt;
APISR: &#21463;&#21160;&#28459;&#21046;&#20316;&#21551;&#21457;&#30340;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
APISR: Anime Production Inspired Real-World Anime Super-Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21463;&#21160;&#28459;&#21046;&#20316;&#21551;&#21457;&#30340;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21160;&#28459;&#21046;&#20316;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#20986;&#19981;&#38656;&#35201;&#35270;&#39057;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#21160;&#28459;&#22270;&#20687;&#25910;&#38598;&#27969;&#27700;&#32447;&#21644;API&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21160;&#28459;&#29305;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;&#24102;&#26469;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#22312;SR&#31038;&#21306;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#37319;&#29992;&#26469;&#33258;&#20889;&#23454;&#39046;&#22495;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#21160;&#28459;&#21046;&#20316;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#37325;&#26032;&#24605;&#32771;&#22914;&#20309;&#21033;&#29992;&#20854;&#20013;&#30340;&#29305;&#24449;&#26469;&#20419;&#36827;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;SR&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35748;&#20026;&#35270;&#39057;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#23545;&#20110;&#21160;&#28459;SR&#24182;&#19981;&#26159;&#24517;&#38656;&#30340;&#65292;&#22240;&#20026;&#25163;&#32472;&#24103;&#30340;&#37325;&#22797;&#20351;&#29992;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#28459;&#22270;&#20687;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#20174;&#35270;&#39057;&#28304;&#20013;&#36873;&#25321;&#26368;&#23569;&#21387;&#32553;&#21644;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#24103;&#12290;&#22522;&#20110;&#35813;&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#28459;&#21046;&#20316;&#23548;&#21521;&#30340;&#22270;&#20687;&#65288;API&#65289;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21160;&#28459;&#29305;&#26377;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#25197;&#26354;&#21644;&#28129;&#34180;&#30340;&#25163;&#32472;&#32447;&#26465;&#20197;&#21450;&#19981;&#38656;&#35201;&#30340;&#33394;&#24425;&#20266;&#24433;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#36864;&#21270;&#27169;&#22411;&#20013;&#24341;&#20837;&#38754;&#21521;&#39044;&#27979;&#30340;&#21387;&#32553;&#27169;&#22359;&#21644;&#20266;&#22522;&#20934;&#25968;&#25454;&#20934;&#22791;&#26469;&#35299;&#20915;&#31532;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01598v1 Announce Type: cross  Abstract: While real-world anime super-resolution (SR) has gained increasing attention in the SR community, existing methods still adopt techniques from the photorealistic domain. In this paper, we analyze the anime production workflow and rethink how to use characteristics of it for the sake of the real-world anime SR. First, we argue that video networks and datasets are not necessary for anime SR due to the repetition use of hand-drawing frames. Instead, we propose an anime image collection pipeline by choosing the least compressed and the most informative frames from the video sources. Based on this pipeline, we introduce the Anime Production-oriented Image (API) dataset. In addition, we identify two anime-specific challenges of distorted and faint hand-drawn lines and unwanted color artifacts. We address the first issue by introducing a prediction-oriented compression module in the image degradation model and a pseudo-ground truth preparatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01216</link><description>&lt;p&gt;
API&#23601;&#22815;&#20102;&#65306;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#27861;&#35775;&#38382;&#23545;&#25968;&#26102;&#22914;&#20309;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36825;&#19968;&#26222;&#36941;&#25361;&#25112;&#12290;&#25972;&#20307;&#39044;&#27979;&#65288;CP&#65289;&#20197;&#20854;&#19982;&#27169;&#22411;&#26080;&#20851;&#21644;&#26080;&#38656;&#20998;&#24067;&#30340;&#29305;&#28857;&#32780;&#38395;&#21517;&#65292;&#26159;&#21508;&#31181;LLMs&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#24819;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#21487;&#20197;&#35775;&#38382;&#23545;&#25968;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#20165;&#25903;&#25345;API&#30340;LLMs&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;&#23545;&#25968;&#21487;&#33021;&#23384;&#22312;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;CP&#26041;&#27861;&#65292;&#65288;1&#65289;&#19987;&#20026;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;API-only LLMs&#37327;&#36523;&#23450;&#21046;; (2) &#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;; &#20197;&#21450;(3)&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#31895;&#31890;&#24230;&#65288;&#20363;&#22914;&#65292;&#26679;&#26412;&#39057;&#29575;&#65289;&#21644;&#32454;&#31890;&#24230;&#19981;&#30830;&#23450;&#24615;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#65289;&#26469;&#21046;&#23450;&#19981;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01216v1 Announce Type: cross  Abstract: This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only LLMs without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.14698</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#20998;&#26512;&#29992;&#20110;&#20998;&#31867;&#19982;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65306;&#25104;&#37117;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Big data analytics to classify earthwork-related locations: A Chengdu study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14698
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#27745;&#26579;&#26174;&#33879;&#21152;&#21095;&#65292;&#23548;&#33268;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20005;&#37325;&#20581;&#24247;&#21518;&#26524;&#12290;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65288;ERLs&#65289;&#26159;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;ERLs&#30340;&#26377;&#25928;&#31649;&#29702;&#19968;&#30452;&#26159;&#25919;&#24220;&#21644;&#29615;&#22659;&#26426;&#26500;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#20027;&#35201;&#21407;&#22240;&#21253;&#25324;&#20854;&#20998;&#31867;&#20998;&#23646;&#19981;&#21516;&#30340;&#30417;&#31649;&#37096;&#38376;&#12289;&#20449;&#24687;&#38556;&#30861;&#12289;&#25968;&#25454;&#26356;&#26032;&#24310;&#36831;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#28304;&#22836;&#28784;&#23576;&#27745;&#26579;&#30340;&#25233;&#21046;&#25514;&#26045;&#30340;&#32570;&#20047;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23454;&#38469;&#25968;&#25454;&#30740;&#31350;&#20102;&#29305;&#24449;&#19982;&#28784;&#23576;&#27745;&#26579;&#28304;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#25104;&#21151;&#23454;&#26045;&#22312;&#19968;&#20010;&#21517;&#20026;&#30340;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14698v1 Announce Type: cross  Abstract: Air pollution has significantly intensified, leading to severe health consequences worldwide. Earthwork-related locations (ERLs) constitute significant sources of urban dust pollution. The effective management of ERLs has long posed challenges for governmental and environmental agencies, primarily due to their classification under different regulatory authorities, information barriers, delays in data updating, and a lack of dust suppression measures for various sources of dust pollution. To address these challenges, we classified urban dust pollution sources using dump truck trajectory, urban point of interest (POI), and land cover data. We compared several prediction models and investigated the relationship between features and dust pollution sources using real data. The results demonstrate that high-accuracy classification can be achieved with a limited number of features. This method was successfully implemented in the system called
&lt;/p&gt;</description></item><item><title>BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14151</link><description>&lt;p&gt;
BIRCO&#65306;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14151
&lt;/p&gt;
&lt;p&gt;
BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;(IR)&#20219;&#21153;&#22522;&#20934;(BIRCO)&#12290; BIRCO&#35780;&#20272;IR&#31995;&#32479;&#26681;&#25454;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#26816;&#32034;&#25991;&#26723;&#30340;&#33021;&#21147;&#12290; &#35813;&#22522;&#20934;&#30340;&#22797;&#26434;&#24615;&#21644;&#32039;&#20945;&#22823;&#23567;&#20351;&#20854;&#36866;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#21487;&#33021;&#24433;&#21709;LLM&#22312;&#26816;&#32034;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19982;&#25110;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#26356;&#22797;&#26434;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290; &#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#20934;&#20219;&#21153;&#19978;&#22343;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#21644;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14151v1 Announce Type: cross  Abstract: We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.
&lt;/p&gt;</description></item><item><title>SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.11322</link><description>&lt;p&gt;
SpikeNAS: &#19968;&#31181;&#38754;&#21521;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11322
&lt;/p&gt;
&lt;p&gt;
SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#37117;&#28304;&#33258;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31070;&#32463;&#20803;&#30340;&#26550;&#26500;&#21644;&#25805;&#20316;&#19982;SNN&#19981;&#21516;&#65292;&#25110;&#32773;&#22312;&#19981;&#32771;&#34385;&#26469;&#33258;&#24213;&#23618;&#22788;&#29702;&#30828;&#20214;&#30340;&#20869;&#23384;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#12290;&#36825;&#20123;&#38480;&#21046;&#38459;&#30861;&#20102;SNN&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20805;&#20998;&#21457;&#25381;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpikeNAS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#65292;&#21487;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#24555;&#36895;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;SNN&#26550;&#26500;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;SpikeNAS&#37319;&#29992;&#20102;&#20960;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#20998;&#26512;&#32593;&#32476;&#25805;&#20316;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#22686;&#24378;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23398;&#20064;&#36136;&#37327;&#65292;&#24182;&#24320;&#21457;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11322v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental resul
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17377</link><description>&lt;p&gt;
&#26080;&#38480;-gram&#65306;&#23558;&#26080;&#38480;n-gram&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#19975;&#20159;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17377
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;n-gram&#35821;&#35328;&#27169;&#22411;&#36824;&#20855;&#26377;&#30456;&#20851;&#24615;&#21527;&#65311;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#25913;&#36827;&#31070;&#32463;LLM&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#20004;&#20010;&#26041;&#38754;&#23545;n-gram&#27169;&#22411;&#36827;&#34892;&#29616;&#20195;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#31070;&#32463;LLM&#30456;&#21516;&#30340;&#25968;&#25454;&#35268;&#27169;&#35757;&#32451;- 1.4&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26500;&#24314;&#30340;&#26368;&#22823;&#30340;n-gram&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;n-gram&#27169;&#22411;&#20351;&#29992;&#30340;n&#24456;&#23567;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20801;&#35768;n&#21487;&#20197;&#26159;&#20219;&#24847;&#22823;&#30340;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26080;&#38480;-gram LM&#19982;&#22238;&#36864;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21518;&#32512;&#25968;&#32452;&#35745;&#31639;&#26080;&#38480;-gram&#65288;&#20197;&#21450;&#20219;&#24847;n&#30340;n-gram&#65289;&#27010;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;n-gram&#35745;&#25968;&#34920;&#65288;&#36825;&#23558;&#38750;&#24120;&#26114;&#36149;&#65289;&#12290;&#26080;&#38480;-gram&#26694;&#26550;&#21644;infini-gram&#24341;&#25806;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#20154;&#31867;&#20889;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35768;&#22810;&#26032;&#39062;&#21644;&#26377;&#24847;&#24605;&#30340;&#20998;&#26512;&#65306;&#25105;&#20204;&#21457;&#29616;&#26080;&#38480;-gram LM...
&lt;/p&gt;
&lt;p&gt;
Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.15496</link><description>&lt;p&gt;
Baichuan2-Sum: &#20351;&#29992;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;Llama&#12289;Baichuan&#21644;Bloom&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#30340;&#19981;&#21516;&#35282;&#33394;&#29983;&#25104;&#25688;&#35201;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#23567;&#27169;&#22411;&#65288;&#20363;&#22914;Bart&#21644;Bert&#65289;&#36827;&#34892;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#22312;&#23567;&#27169;&#22411;&#19978;&#28155;&#21152;&#20219;&#21153;&#25351;&#23450;&#30340;&#20248;&#21270;&#65292;&#22914;&#21521;&#27169;&#22411;&#28155;&#21152;&#20840;&#23616;-&#23616;&#37096;&#20013;&#24515;&#24230;&#24471;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#65306;Baichuan2-Sum&#65292;&#29992;&#20110;&#38754;&#21521;&#35282;&#33394;&#30340;&#23545;&#35805;&#25688;&#35201;&#12290;&#36890;&#36807;&#20026;&#19981;&#21516;&#35282;&#33394;&#35774;&#32622;&#19981;&#21516;&#30340;&#25351;&#20196;&#65292;&#27169;&#22411;&#21487;&#20197;&#20174;&#23545;&#35805;&#20132;&#20114;&#20013;&#23398;&#20064;&#24182;&#36755;&#20986;&#26399;&#26395;&#30340;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;NEFTune&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#21512;&#36866;&#30340;&#22122;&#22768;&#20197;&#25552;&#39640;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#20844;&#24320;&#30340;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#38598;CSDS&#21644;SAMSUM&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods conduct on small models (e.g Bart and Bert). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the expected summaries. Furthermore, we applied NEFTune technique to add suitable noise during training to improve the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. We 
&lt;/p&gt;</description></item><item><title>UINav&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#36866;&#21512;&#31227;&#21160;&#35774;&#22791;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#65292;&#25104;&#21151;&#29575;&#39640;&#65292;&#35757;&#32451;&#25968;&#25454;&#23569;&#12290;</title><link>https://arxiv.org/abs/2312.10170</link><description>&lt;p&gt;
UINav&#65306;&#19968;&#31181;&#35757;&#32451;&#35774;&#22791;&#31471;&#33258;&#21160;&#21270;&#20195;&#29702;&#30340;&#23454;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UINav: A Practical Approach to Train On-Device Automation Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10170
&lt;/p&gt;
&lt;p&gt;
UINav&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#36866;&#21512;&#31227;&#21160;&#35774;&#22791;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#65292;&#25104;&#21151;&#29575;&#39640;&#65292;&#35757;&#32451;&#25968;&#25454;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#33258;&#20027;&#39537;&#21160;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#30028;&#38754;&#20197;&#23436;&#25104;&#29992;&#25143;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#24403;&#29992;&#25143;&#22788;&#20110;&#24773;&#22659;&#24615;&#25110;&#27704;&#20037;&#24615;&#21463;&#25439;&#26102;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#20043;&#21069;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#19981;&#33021;&#20135;&#29983;&#20855;&#26377;&#26222;&#36941;&#36866;&#29992;&#24615;&#30340;&#27169;&#22411;&#65292;&#32780;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#20165;&#22312;&#31616;&#21333;&#30340;&#25163;&#24037;&#21046;&#20316;&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#38752;&#24037;&#20316;&#65292;&#25110;&#32773;&#20250;&#20135;&#29983;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UINav&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#36866;&#21512;&#31227;&#21160;&#35774;&#22791;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#28436;&#31034;&#25968;&#37327;&#19981;&#22810;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25104;&#21151;&#29575;&#12290;&#20026;&#20102;&#20943;&#23569;&#28436;&#31034;&#30340;&#24037;&#20316;&#37327;&#65292;UINav&#20351;&#29992;&#20102;&#19968;&#20010;&#35009;&#21028;&#27169;&#22411;&#65292;&#22312;&#20195;&#29702;&#22833;&#36133;&#30340;&#20219;&#21153;&#19978;&#20026;&#29992;&#25143;&#25552;&#20379;&#21363;&#26102;&#21453;&#39304;&#65292;&#24182;&#33258;&#21160;&#22686;&#21152;&#20154;&#31867;&#28436;&#31034;&#20197;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#20165;&#38656;10&#27425;&#28436;&#31034;&#65292;UINav&#23601;&#21487;&#20197;&#23454;&#29616;70%&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#26377;&#36275;&#22815;&#22810;&#27425;&#28436;&#31034;&#26102;&#65292;&#23427;&#21487;&#20197;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10170v2 Announce Type: replace-cross  Abstract: Automation systems that can autonomously drive application user interfaces to complete user tasks are of great benefit, especially when users are situationally or permanently impaired. Prior automation systems do not produce generalizable models while AI-based automation agents work reliably only in simple, hand-crafted applications or incur high computation costs. We propose UINav, a demonstration-based approach to train automation agents that fit mobile devices, yet achieving high success rates with modest numbers of demonstrations. To reduce the demonstration overhead, UINav uses a referee model that provides users with immediate feedback on tasks where the agent fails, and automatically augments human demonstrations to increase diversity in training data. Our evaluation shows that with only 10 demonstrations UINav can achieve 70% accuracy, and that with enough demonstrations it can surpass 90% accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.01037</link><description>&lt;p&gt;
&#20174;&#21476;&#24618;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge from Quirky Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;&#65288;ELK&#65289;&#26088;&#22312;&#22312;&#19968;&#20010;&#33021;&#21147;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#20013;&#25214;&#21040;&#27169;&#24335;&#65292;&#21363;&#20351;&#32593;&#32476;&#30340;&#26126;&#26174;&#36755;&#20986;&#26159;&#38169;&#35823;&#25110;&#35823;&#23548;&#24615;&#30340;&#65292;&#20063;&#33021;&#31283;&#23450;&#36319;&#36394;&#19990;&#30028;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;ELK&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;12&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#22871;&#30456;&#24212;&#30340;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#21482;&#26377;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#20851;&#38190;&#35789;&#8220;Bob&#8221;&#26102;&#25165;&#20250;&#36827;&#34892;&#31995;&#32479;&#24615;&#38169;&#35823;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25506;&#27979;&#26041;&#27861;&#21487;&#20197;&#35843;&#21462;&#27169;&#22411;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#23545;&#27491;&#30830;&#31572;&#26696;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#21363;&#20351;&#38382;&#39064;&#27604;&#25506;&#27979;&#22120;&#35757;&#32451;&#30340;&#38382;&#39064;&#26356;&#22256;&#38590;&#12290;&#36825;&#26159;&#30001;&#20110;&#20013;&#38388;&#23618;&#28608;&#27963;&#20013;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19968;&#31181;&#26426;&#26800;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#20197;94%&#30340;AUROC&#26631;&#35782;&#19981;&#30495;&#23454;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20174;&#33021;&#21147;&#24378;&#20294;&#19981;&#21463;&#20449;&#20219;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;ELK&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#25311;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#21149;&#23548;&#38382;&#39064;&#20013;&#65292;&#21457;&#36865;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#20248;&#28040;&#24687;&#31574;&#30053;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#26597;&#35810;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#20854;&#39044;&#26399;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.18138</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#36827;&#34892;&#31639;&#27861;&#24615;&#21149;&#23548;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Persuasion Through Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18138
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#21149;&#23548;&#38382;&#39064;&#20013;&#65292;&#21457;&#36865;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#20248;&#28040;&#24687;&#31574;&#30053;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#26597;&#35810;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#20854;&#39044;&#26399;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#21149;&#23548;&#38382;&#39064;&#65292;&#20854;&#20013;&#21457;&#36865;&#32773;&#24076;&#26395;&#35828;&#26381;&#25509;&#25910;&#32773;&#37319;&#21462;&#20108;&#20803;&#34892;&#20026;&#65292;&#20363;&#22914;&#36141;&#20080;&#20135;&#21697;&#12290;&#21457;&#36865;&#32773;&#20102;&#35299;&#19990;&#30028;&#30340;&#65288;&#20108;&#20803;&#65289;&#29366;&#24577;&#65292;&#27604;&#22914;&#20135;&#21697;&#36136;&#37327;&#26159;&#39640;&#36824;&#26159;&#20302;&#65292;&#20294;&#26159;&#23545;&#25509;&#25910;&#32773;&#30340;&#20449;&#24565;&#21644;&#25928;&#29992;&#21482;&#26377;&#26377;&#38480;&#30340;&#20449;&#24687;&#12290;&#21463;&#21040;&#23458;&#25143;&#35843;&#26597;&#12289;&#29992;&#25143;&#30740;&#31350;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20801;&#35768;&#21457;&#36865;&#32773;&#36890;&#36807;&#26597;&#35810;&#27169;&#25311;&#25509;&#25910;&#32773;&#30340;&#34892;&#20026;&#26469;&#20102;&#35299;&#26356;&#22810;&#20851;&#20110;&#25509;&#25910;&#32773;&#30340;&#20449;&#24687;&#12290;&#22312;&#22266;&#23450;&#25968;&#37327;&#30340;&#26597;&#35810;&#20043;&#21518;&#65292;&#21457;&#36865;&#32773;&#25215;&#35834;&#19968;&#20010;&#28040;&#24687;&#31574;&#30053;&#65292;&#25509;&#25910;&#32773;&#26681;&#25454;&#25910;&#21040;&#30340;&#28040;&#24687;&#26469;&#26368;&#22823;&#21270;&#22905;&#30340;&#39044;&#26399;&#25928;&#29992;&#26469;&#37319;&#21462;&#34892;&#21160;&#12290;&#25105;&#20204;&#23545;&#21457;&#36865;&#32773;&#22312;&#20219;&#20309;&#25509;&#25910;&#32773;&#31867;&#22411;&#20998;&#24067;&#19979;&#30340;&#26368;&#20248;&#28040;&#24687;&#31574;&#30053;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#26597;&#35810;&#31639;&#27861;&#65292;&#20248;&#21270;&#20102;&#36825;&#20010;&#36125;&#21494;&#26031;&#21149;&#23548;&#28216;&#25103;&#20013;&#21457;&#36865;&#32773;&#30340;&#39044;&#26399;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18138v2 Announce Type: replace-cross Abstract: We study a Bayesian persuasion problem where a sender wants to persuade a receiver to take a binary action, such as purchasing a product. The sender is informed about the (binary) state of the world, such as whether the quality of the product is high or low, but only has limited information about the receiver's beliefs and utilities. Motivated by customer surveys, user studies, and recent advances in generative AI, we allow the sender to learn more about the receiver by querying an oracle that simulates the receiver's behavior. After a fixed number of queries, the sender commits to a messaging policy and the receiver takes the action that maximizes her expected utility given the message she receives. We characterize the sender's optimal messaging policy given any distribution over receiver types. We then design a polynomial-time querying algorithm that optimizes the sender's expected utility in this Bayesian persuasion game. We 
&lt;/p&gt;</description></item><item><title>&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#20351;&#29992;&#32773;&#34920;&#31034;&#36825;&#20123;&#20851;&#31995;&#26377;&#30410;&#20110;&#20182;&#20204;&#30340;&#31038;&#20250;&#20581;&#24247;&#65292;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#38750;&#20351;&#29992;&#32773;&#21017;&#35748;&#20026;&#26377;&#23475;</title><link>https://arxiv.org/abs/2311.10599</link><description>&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#31038;&#20132;&#20276;&#20387;&#65306;&#20154;&#20204;&#22914;&#20309;&#30475;&#24453;&#26426;&#22120;&#20013;&#30340;&#24847;&#35782;&#12289;&#20154;&#31867;&#30456;&#20284;&#24615;&#21644;&#31038;&#20250;&#20581;&#24247;&#31119;&#21033;
&lt;/p&gt;
&lt;p&gt;
Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10599
&lt;/p&gt;
&lt;p&gt;
&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#20351;&#29992;&#32773;&#34920;&#31034;&#36825;&#20123;&#20851;&#31995;&#26377;&#30410;&#20110;&#20182;&#20204;&#30340;&#31038;&#20250;&#20581;&#24247;&#65292;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#38750;&#20351;&#29992;&#32773;&#21017;&#35748;&#20026;&#26377;&#23475;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#21450;&#65292;&#19968;&#20010;&#38382;&#39064;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#22914;&#20309;&#24433;&#21709;&#20154;&#38469;&#20851;&#31995;&#12290;&#20363;&#22914;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#31038;&#20132;&#20276;&#20387;&#65292;&#23613;&#31649;&#26377;&#24456;&#22810;&#29468;&#27979;&#65292;&#20294;&#23545;&#20182;&#20204;&#30340;&#20351;&#29992;&#22914;&#20309;&#24433;&#21709;&#20154;&#38469;&#20851;&#31995;&#30693;&#20043;&#29978;&#23569;&#12290;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#19982;&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20851;&#31995;&#23545;&#31038;&#20250;&#20581;&#24247;&#26377;&#23475;&#65292;&#20294;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#36807;&#20110;&#31616;&#21333;&#65292;&#29305;&#21035;&#32771;&#34385;&#21040;&#29992;&#25143;&#30340;&#31038;&#20250;&#38656;&#27714;&#21644;&#20182;&#20204;&#29616;&#26377;&#20154;&#38469;&#20851;&#31995;&#30340;&#20581;&#24247;&#12290;&#20026;&#20102;&#20102;&#35299;&#19982;&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#31038;&#20250;&#20581;&#24247;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23450;&#26399;&#20351;&#29992;&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#19981;&#20351;&#29992;&#23427;&#20204;&#30340;&#20154;&#12290;&#19982;&#26399;&#26395;&#30456;&#21453;&#65292;&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#29992;&#25143;&#34920;&#31034;&#36825;&#20123;&#20851;&#31995;&#23545;&#20182;&#20204;&#30340;&#31038;&#20250;&#20581;&#24247;&#26377;&#30410;&#65292;&#32780;&#38750;&#29992;&#25143;&#21017;&#35748;&#20026;&#23427;&#20204;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10599v3 Announce Type: replace-cross  Abstract: As artificial intelligence (AI) becomes more widespread, one question that arises is how human-AI interaction might impact human-human interaction. Chatbots, for example, are increasingly used as social companions, and while much is speculated, little is known empirically about how their use impacts human relationships. A common hypothesis is that relationships with companion chatbots are detrimental to social health by harming or replacing human interaction, but this hypothesis may be too simplistic, especially considering the social needs of users and the health of their preexisting human relationships. To understand how relationships with companion chatbots impact social health, we studied people who regularly used companion chatbots and people who did not use them. Contrary to expectations, companion chatbot users indicated that these relationships were beneficial to their social health, whereas non-users viewed them as har
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;</title><link>https://arxiv.org/abs/2310.00492</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#25351;&#20196;&#36319;&#38543;&#65306;&#29702;&#35299;&#25351;&#20196;&#35843;&#25972;&#21518;LLMs&#20013;&#34892;&#20026;&#30340;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00492
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#25351;&#20196;&#35843;&#25972;&#26159;&#23558;LLMs&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#22914;&#20309;&#35843;&#25972;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#20869;&#22312;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#20960;&#31181;&#26412;&#22320;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36755;&#20837;&#36755;&#20986;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#21450;&#29992;&#20110;&#35299;&#37322;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#23618;&#20013;&#30340;&#27169;&#24335;&#21644;&#27010;&#24565;&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#36890;&#36807;&#27604;&#36739;&#20174;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#35299;&#37322;&#26469;&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20154;&#21487;&#29702;&#35299;&#30340;&#27700;&#24179;&#19978;&#25552;&#20379;&#20102;&#27169;&#22411;&#36716;&#21464;&#30340;&#20869;&#37096;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#23427;&#20351;LLMs&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65292;&#24182;&#19981;&#26029;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00492v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts from user prompts, and promotes the response generation constantly condition
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2308.02935</link><description>&lt;p&gt;
&#25581;&#31034;&#30450;&#28857;&#65306;&#23545;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#20844;&#24179;&#24615;&#30340;&#20851;&#38190;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Blind Spots: A Critical Examination of Fairness in Autonomous Driving Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.02935
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24050;&#32463;&#25193;&#23637;&#20102;&#26234;&#33021;&#36710;&#36742;&#29289;&#32852;&#32593;&#30340;&#33539;&#22260;&#65292;&#24182;&#25104;&#20026;Web&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20844;&#24179;&#24615;&#23545;&#20110;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#39640;&#36136;&#37327;&#26159;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#20013;&#30340;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#32972;&#26223;&#19979;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#34892;&#20154;&#26816;&#27979;&#22120;&#20844;&#24179;&#24615;&#30340;&#32508;&#21512;&#35780;&#20272;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#20986;&#29616;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20843;&#31181;&#34987;&#24191;&#27867;&#25506;&#32034;&#30340;DL&#34892;&#20154;&#26816;&#27979;&#22120;&#22312;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#24443;&#24213;&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#20026;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#27880;&#37322;&#65292;&#20849;&#28041;&#21450;8,311&#24352;&#22270;&#20687;&#65292;16,070&#20010;&#24615;&#21035;&#26631;&#31614;&#65292;20,115&#20010;&#24180;&#40836;&#26631;&#31614;&#21644;3,513&#20010;&#32932;&#33394;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.02935v2 Announce Type: replace-cross  Abstract: Autonomous driving systems have extended the spectrum of Web of Things for intelligent vehicles and have become an important component of the Web ecosystem. Similar to traditional Web-based applications, fairness is an essential aspect for ensuring the high quality of autonomous driving systems, particularly in the context of pedestrian detectors within them. However, there is an absence in the literature of a comprehensive assessment of the fairness of current Deep Learning (DL)-based pedestrian detectors. To fill the gap, we evaluate eight widely-explored DL-based pedestrian detectors across demographic groups on large-scale real-world datasets. To enable a thorough fairness evaluation, we provide extensive annotations for the datasets, resulting in 8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone labels. Our findings reveal significant fairness issues related to age. The undetected proportions f
&lt;/p&gt;</description></item><item><title>GenAI&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#25512;&#21160;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;AI&#33021;&#21147;&#30340;&#20849;&#21516;&#21019;&#20316;&#36807;&#31243;&#65292;&#20294;&#29992;&#25143;&#21516;&#26102;&#38754;&#20020;&#30528;&#36164;&#28304;&#12289;&#24037;&#20855;&#21644;&#30417;&#31649;&#31561;&#26041;&#38754;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2302.10827</link><description>&lt;p&gt;
&#37326;&#22806;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65306;&#21069;&#26223;&#12289;&#25361;&#25112;&#21644;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Generative AI in the Wild: Prospects, Challenges, and Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.10827
&lt;/p&gt;
&lt;p&gt;
GenAI&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#25512;&#21160;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;AI&#33021;&#21147;&#30340;&#20849;&#21516;&#21019;&#20316;&#36807;&#31243;&#65292;&#20294;&#29992;&#25143;&#21516;&#26102;&#38754;&#20020;&#30528;&#36164;&#28304;&#12289;&#24037;&#20855;&#21644;&#30417;&#31649;&#31561;&#26041;&#38754;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39537;&#21160;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#25216;&#26415;&#20197;&#20854;&#20986;&#33394;&#30340;&#29983;&#25104;&#26032;&#39062;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#27491;&#22312;&#39072;&#35206;&#35768;&#22810;&#34892;&#19994;&#20013;&#30340;&#20256;&#32479;&#24037;&#20316;&#27969;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20174;&#25216;&#26415;&#20013;&#24515;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;GenAI&#65292;&#20294;&#20154;&#20204;&#23545;&#29992;&#25143;&#22914;&#20309;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24863;&#30693;&#21644;&#21033;&#29992;GenAI&#20173;&#28982;&#32570;&#20047;&#20102;&#35299;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#65288;N=18&#65289;GenAI&#29992;&#25143;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#65292;&#30740;&#31350;&#20102;&#22312;&#25972;&#20307;&#30340;LUA&#65288;&#23398;&#20064;&#12289;&#20351;&#29992;&#21644;&#35780;&#20272;&#65289;&#26694;&#26550;&#20869;&#20154;&#31867;&#21644;GenAI&#20849;&#21516;&#21019;&#20316;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#32780;&#26377;&#36259;&#30340;&#26223;&#35266;&#65306;&#21069;&#26223;-GenAI&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;GenAI&#33021;&#21147;&#20043;&#38388;&#30340;&#20849;&#21516;&#21019;&#20316;&#65292;&#28145;&#21051;&#25913;&#21464;&#20102;&#21019;&#24847;&#24037;&#20316;&#27969;&#31243;&#65307;&#25361;&#25112;-&#19982;&#27492;&#21516;&#26102;&#65292;&#29992;&#25143;&#38754;&#20020;&#30528;&#30001;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#24037;&#20855;&#21487;&#29992;&#24615;&#21644;&#30417;&#31649;&#22797;&#26434;&#24615;&#24341;&#36215;&#30340;&#37325;&#22823;&#19981;&#30830;&#23450;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.10827v2 Announce Type: replace-cross  Abstract: Propelled by their remarkable capabilities to generate novel and engaging content, Generative Artificial Intelligence (GenAI) technologies are disrupting traditional workflows in many industries. While prior research has examined GenAI from a techno-centric perspective, there is still a lack of understanding about how users perceive and utilize GenAI in real-world scenarios. To bridge this gap, we conducted semi-structured interviews with (N=18) GenAI users increative industries, investigating the human-GenAI co-creation process within a holistic LUA (Learning, Using and Assessing)framework. Our study uncovered an intriguingly complex landscape: Prospects-GenAI greatly fosters the co-creation between human expertise and GenAI capabilities, profoundly transforming creative workflows; Challenges-Meanwhile, users face substantial uncertainties and complexities arising from resource availability, tool usability, and regulatory comp
&lt;/p&gt;</description></item><item><title>TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14694</link><description>&lt;p&gt;
TA-RNN&#65306;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38754;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26102;&#38388;&#24863;&#30693;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records. (arXiv:2401.14694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14694
&lt;/p&gt;
&lt;p&gt;
TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#20840;&#38754;&#36164;&#28304;&#12290;EHR&#23545;&#20110;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31561;&#20808;&#36827;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#21307;&#30103;&#25552;&#20379;&#32773;&#33021;&#22815;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#65292;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#20570;&#20986;&#31934;&#30830;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#20020;&#24202;&#20915;&#31574;&#12290;DL&#26041;&#27861;&#22914;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#24050;&#34987;&#29992;&#20110;&#20998;&#26512;EHR&#20197;&#24314;&#27169;&#30142;&#30149;&#36827;&#23637;&#24182;&#39044;&#27979;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#35299;&#20915;EHR&#25968;&#25454;&#20013;&#19968;&#20123;&#22266;&#26377;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#22914;&#20020;&#24202;&#35775;&#38382;&#20043;&#38388;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;DL&#27169;&#22411;&#37117;&#19981;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;DL&#26550;&#26500;&#65292;&#20998;&#21035;&#26159;&#26102;&#38388;&#24863;&#30693;RNN&#65288;TA-RNN&#65289;&#21644;TA-RNN-Autoencoder&#65288;TA-RNN-AE&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#27425;&#35775;&#38382;&#21644;&#22810;&#27425;&#26410;&#26469;&#35775;&#38382;&#20013;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#23558;&#26102;&#38388;&#20449;&#24687;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elaps
&lt;/p&gt;</description></item><item><title>RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09432</link><description>&lt;p&gt;
RoleCraft-GLM&#65306;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models. (arXiv:2401.09432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09432
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;RoleCraft-GLM&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;RoleCraft-GLM&#35299;&#20915;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#35814;&#32454;&#25551;&#32472;&#24773;&#24863;&#32454;&#33147;&#30340;&#35282;&#33394;&#21051;&#30011;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#32452;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#20174;&#20256;&#32479;&#30340;&#20197;&#21517;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#33394;&#36716;&#21464;&#20026;&#22810;&#26679;&#21270;&#30340;&#38750;&#21517;&#20154;&#35282;&#33394;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#35821;&#35328;&#24314;&#27169;&#20114;&#21160;&#30340;&#30495;&#23454;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#30830;&#20445;&#23545;&#35805;&#26082;&#30495;&#23454;&#21448;&#24773;&#24863;&#20849;&#40483;&#12290;&#36890;&#36807;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;RoleCraft-GLM&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#24635;&#20043;&#65292;RoleCraft-GLM&#26631;&#24535;&#30528;&#19968;&#20010;&#21019;&#26032;&#30340;&#37324;&#31243;&#30865;&#65292;&#25512;&#21160;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a sign
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MarketSenseAI&#65292;&#19968;&#20010;&#21033;&#29992;GPT-4&#36827;&#34892;&#32929;&#31080;&#36873;&#25321;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#28304;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#20379;&#20855;&#26377;&#21487;&#34892;&#35299;&#37322;&#30340;&#25237;&#36164;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2401.03737</link><description>&lt;p&gt;
&#33021;&#21542;&#25171;&#36133;&#21326;&#23572;&#34903;&#65311;&#25581;&#31034;&#20154;&#24037;&#26234;&#33021;&#22312;&#32929;&#31080;&#36873;&#25321;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection. (arXiv:2401.03737v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MarketSenseAI&#65292;&#19968;&#20010;&#21033;&#29992;GPT-4&#36827;&#34892;&#32929;&#31080;&#36873;&#25321;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#28304;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#20379;&#20855;&#26377;&#21487;&#34892;&#35299;&#37322;&#30340;&#25237;&#36164;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#24066;&#22330;&#21160;&#24577;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#29615;&#22659;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;MarketSenseAI&#65292;&#19968;&#20010;&#21033;&#29992;GPT-4&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#21487;&#25193;&#23637;&#32929;&#31080;&#36873;&#25321;&#30340;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#12290;MarketSenseAI&#25972;&#21512;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#21644;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#26041;&#27861;&#65292;&#20998;&#26512;&#21253;&#25324;&#24066;&#22330;&#20215;&#26684;&#21160;&#24577;&#12289;&#36130;&#32463;&#26032;&#38395;&#12289;&#20844;&#21496;&#22522;&#26412;&#38754;&#21644;&#23439;&#35266;&#32463;&#27982;&#25253;&#21578;&#31561;&#22810;&#31181;&#25968;&#25454;&#28304;&#65292;&#27169;&#20223;&#30693;&#21517;&#37329;&#34701;&#25237;&#36164;&#22242;&#38431;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;MarketSenseAI&#30340;&#24320;&#21457;&#12289;&#23454;&#26045;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#25552;&#20379;&#20855;&#26377;&#20805;&#20998;&#35299;&#37322;&#25903;&#25745;&#30340;&#21487;&#34892;&#25237;&#36164;&#20449;&#21495;&#65288;&#20080;&#20837;&#12289;&#25345;&#26377;&#12289;&#21334;&#20986;&#65289;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#20351;&#29992;GPT-4&#19981;&#20165;&#20316;&#20026;&#39044;&#27979;&#24037;&#20855;&#65292;&#36824;&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#25152;&#24314;&#35758;&#30340;&#25237;&#36164;&#20449;&#21495;&#30340;&#21487;&#38752;&#24615;&#21644;&#25509;&#21463;&#24230;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
In the dynamic and data-driven landscape of financial markets, this paper introduces MarketSenseAI, a novel AI-driven framework leveraging the advanced reasoning capabilities of GPT-4 for scalable stock selection. MarketSenseAI incorporates Chain of Thought and In-Context Learning methodologies to analyze a wide array of data sources, including market price dynamics, financial news, company fundamentals, and macroeconomic reports emulating the decision making process of prominent financial investment teams. The development, implementation, and empirical validation of MarketSenseAI are detailed, with a focus on its ability to provide actionable investment signals (buy, hold, sell) backed by cogent explanations. A notable aspect of this study is the use of GPT-4 not only as a predictive tool but also as an evaluator, revealing the significant impact of the AI-generated explanations on the reliability and acceptance of the suggested investment signals. In an extensive empirical evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21464;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#33258;&#21160;&#21270;&#21270;&#23398;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#65288;RCR&#65289;&#20219;&#21153;&#65292;&#36890;&#36807;&#27169;&#25311;&#19987;&#23478;&#21270;&#23398;&#23478;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#26032;&#21453;&#24212;&#25351;&#32441;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#12290;&#27492;&#31995;&#32479;&#21487;&#20197;&#20943;&#36731;&#21270;&#23398;&#23478;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#26356;&#19987;&#27880;&#20110;&#26356;&#22522;&#30784;&#21644;&#21019;&#36896;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.10776</link><description>&lt;p&gt;
&#22312;&#21270;&#23398;&#21512;&#25104;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#20013;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generative Agent for Reaction Condition Recommendation in Chemical Synthesis. (arXiv:2311.10776v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.10776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21464;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#33258;&#21160;&#21270;&#21270;&#23398;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#65288;RCR&#65289;&#20219;&#21153;&#65292;&#36890;&#36807;&#27169;&#25311;&#19987;&#23478;&#21270;&#23398;&#23478;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#26032;&#21453;&#24212;&#25351;&#32441;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#12290;&#27492;&#31995;&#32479;&#21487;&#20197;&#20943;&#36731;&#21270;&#23398;&#23478;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#26356;&#19987;&#27880;&#20110;&#26356;&#22522;&#30784;&#21644;&#21019;&#36896;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20026;&#21270;&#23398;&#31038;&#20250;&#20013;&#30340;&#33258;&#21160;&#21270;&#21270;&#23398;&#21453;&#24212;&#38138;&#24179;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21464;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#33258;&#21160;&#21270;&#21270;&#23398;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#65288;RCR&#65289;&#20219;&#21153;&#12290;&#36890;&#36807;&#27169;&#25311;&#19987;&#23478;&#21270;&#23398;&#23478;&#30340;&#25628;&#32034;&#21644;&#20998;&#26512;&#31574;&#30053;&#65292;&#35813;&#20195;&#29702;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#26597;&#35810;&#20998;&#23376;&#25968;&#25454;&#24211;&#65292;&#24182;&#20174;&#22312;&#32447;&#25991;&#29486;&#20013;&#25552;&#21462;&#20851;&#38190;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36824;&#37197;&#22791;&#20102;&#25105;&#20204;&#20026;RCR&#20219;&#21153;&#24320;&#21457;&#30340;&#26032;&#21453;&#24212;&#25351;&#32441;&#12290;&#30001;&#20110;RAG&#25216;&#26415;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#20351;&#29992;&#26356;&#26032;&#30340;&#22312;&#32447;&#25968;&#25454;&#24211;&#20316;&#20026;&#30693;&#35782;&#28304;&#65292;&#26174;&#33879;&#20248;&#20110;&#20165;&#21463;&#20854;&#35757;&#32451;&#25968;&#25454;&#22266;&#23450;&#30693;&#35782;&#38480;&#21046;&#30340;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31995;&#32479;&#21487;&#20197;&#26174;&#33879;&#20943;&#36731;&#21270;&#23398;&#23478;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#26356;&#19987;&#27880;&#20110;&#26356;&#22522;&#30784;&#21644;&#21019;&#36896;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;&#36825;&#19968;&#37325;&#22823;&#36827;&#23637;&#23558;&#35745;&#31639;&#25216;&#26415;&#19982;&#21270;&#23398;&#31038;&#20250;&#26356;&#32039;&#23494;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent artificial intelligence (AI) research plots a promising future of automatic chemical reactions within the chemistry society. This study presents a transformative AI agent that automates the reaction condition recommendation (RCR) task in chemistry using retrieval-augmented generation (RAG) technology. By emulating expert chemists search and analysis strategies, the agent employs large language models (LLMs) to interrogate molecular databases and distill critical data from online literature. Further, the AI agent is equipped with our novel reaction fingerprint developed for the RCR task. Thanks to the RAG technology, our agent uses updated online databases as knowledge sources, significantly outperforming conventional AIs confined to the fixed knowledge within its training data. The resulting system can significantly reduce chemists workload, allowing them to focus on more fundamental and creative scientific problems. This significant advancement brings closer computational techn
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12508</link><description>&lt;p&gt;
SalUn&#65306;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26435;&#37325;&#26174;&#33879;&#24615;&#22686;&#24378;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#27861;&#35268;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#24403;&#21069;AI&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MU&#26041;&#27861;&#36890;&#24120;&#22312;&#36951;&#24536;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36328;&#39046;&#22495;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MU&#20013;&#30340;&#8220;&#26435;&#37325;&#26174;&#33879;&#24615;&#8221;&#27010;&#24565;&#65292;&#20511;&#37492;&#20102;&#27169;&#22411;&#35299;&#37322;&#20013;&#30340;&#36755;&#20837;&#26174;&#33879;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#23558;MU&#30340;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20102;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26174;&#33879;&#24615;&#36951;&#24536;&#65288;SalUn&#65289;&#30340;&#26041;&#27861;&#23558;&#20854;&#19982;&#8220;&#31934;&#30830;&#8221;&#36951;&#24536;&#65288;&#22312;&#21024;&#38500;&#36951;&#24536;&#25968;&#25454;&#38598;&#21518;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SalUn&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;MU&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;SalUn&#21487;&#22312;&#22270;&#29255;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
&lt;/p&gt;</description></item><item><title>L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.02003</link><description>&lt;p&gt;
L2MAC&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35745;&#31639;&#26426;&#29992;&#20110;&#26080;&#38480;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02003
&lt;/p&gt;
&lt;p&gt;
L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#24213;&#23618;Transformer&#26550;&#26500;&#22266;&#23450;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#22686;&#24378;&#35760;&#24518;&#30340;LLM&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#35835;&#21462;&#20869;&#23384;&#24182;&#23558;&#20854;&#28436;&#21464;&#20026;&#26032;&#20869;&#23384;&#30340;&#36830;&#25509;&#65292;&#35201;&#20040;&#20351;&#29992;&#38750;&#24120;&#19987;&#38376;&#30340;&#20869;&#23384;&#65292;&#26080;&#27861;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;L2MAC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38271;&#19988;&#19968;&#33268;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#29992;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#12290;&#23427;&#30340;&#20869;&#23384;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#20196;&#27880;&#20876;&#34920;&#65292;&#20854;&#20013;&#22635;&#20805;&#20102;&#19968;&#20010;&#35299;&#20915;&#29992;&#25143;&#32473;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#31243;&#24207;&#65292;&#20197;&#21450;&#25991;&#20214;&#23384;&#20648;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#32456;&#21644;&#20013;&#38388;&#36755;&#20986;&#12290;&#27599;&#20010;&#25351;&#20196;&#30001;&#21333;&#29420;&#30340;LLM&#23454;&#20363;&#25191;&#34892;&#65292;&#20854;&#19978;&#19979;&#25991;&#30001;&#25511;&#21046;&#21333;&#20803;&#31649;&#29702;&#65292;&#33021;&#22815;&#31934;&#30830;&#35835;&#21462;&#21644;&#20889;&#20837;&#20869;&#23384;&#65292;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.00156</link><description>&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#23398;&#20064;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning Generalizable Tool-use Skills through Trajectory Generation. (arXiv:2310.00156v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00156
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21033;&#29992;&#24037;&#20855;&#30340;&#33258;&#20027;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#35768;&#22810;&#24120;&#35265;&#20219;&#21153;&#65292;&#22914;&#28921;&#39274;&#21644;&#28165;&#27905;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#31995;&#32479;&#22312;&#36866;&#24212;&#26032;&#24037;&#20855;&#26041;&#38754;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#22522;&#20110;&#21487;&#21450;&#24615;&#30340;&#20808;&#21069;&#24037;&#20316;&#36890;&#24120;&#23545;&#29615;&#22659;&#20570;&#20986;&#20102;&#24456;&#24378;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#20219;&#21153;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#20351;&#29992;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#26469;&#25805;&#32437;&#21487;&#21464;&#24418;&#29289;&#20307;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#20316;&#20026;&#19968;&#31995;&#21015;&#28857;&#20113;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#24037;&#20855;&#24418;&#29366;&#12290;&#23545;&#20110;&#20219;&#20309;&#26032;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#20808;&#29983;&#25104;&#19968;&#20010;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#28982;&#21518;&#20248;&#21270;&#24037;&#20855;&#23039;&#21183;&#24207;&#21015;&#20197;&#19982;&#29983;&#25104;&#30340;&#36712;&#36857;&#23545;&#40784;&#12290;&#25105;&#20204;&#20026;&#22235;&#31181;&#19981;&#21516;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#20219;&#21153;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#21333;&#20010;&#24037;&#20855;&#30340;&#31034;&#33539;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model for four different challenging deformable object manipulation tasks. Our model is trained with demonstration data from just a single tool for each task and is able to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;T-COL&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#19981;&#20165;&#33021;&#22815;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#30340;&#21407;&#22240;&#65292;&#36824;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#24314;&#35758;&#32473;&#29992;&#25143;&#12290;&#36890;&#36807;&#23558;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#26144;&#23556;&#21040;CEs&#30340;&#23646;&#24615;&#19978;&#65292;&#20197;&#21450;&#37319;&#29992;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;T-COL&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#25361;&#25112;&#24182;&#20445;&#25345;&#20581;&#22766;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16146</link><description>&lt;p&gt;
T-COL: &#20026;&#21487;&#21464;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#29983;&#25104;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
T-COL: Generating Counterfactual Explanations for General User Preferences on Variable Machine Learning Systems. (arXiv:2309.16146v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;T-COL&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#19981;&#20165;&#33021;&#22815;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#30340;&#21407;&#22240;&#65292;&#36824;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#24314;&#35758;&#32473;&#29992;&#25143;&#12290;&#36890;&#36807;&#23558;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#26144;&#23556;&#21040;CEs&#30340;&#23646;&#24615;&#19978;&#65292;&#20197;&#21450;&#37319;&#29992;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;T-COL&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#25361;&#25112;&#24182;&#20445;&#25345;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#12290;CEs&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#23427;&#20204;&#19981;&#20165;&#35299;&#37322;&#20026;&#20160;&#20040;&#20250;&#39044;&#27979;&#26576;&#20010;&#29305;&#23450;&#32467;&#26524;&#65292;&#36824;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#24314;&#35758;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;CEs&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#38480;&#21046;&#65292;&#21363;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#21644;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#29305;&#21035;&#26159;&#65292;&#29992;&#25143;&#20559;&#22909;&#24448;&#24448;&#26159;&#19968;&#33324;&#24615;&#30340;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#29305;&#24449;&#20540;&#12290;&#27492;&#22806;&#65292;CEs&#38656;&#35201;&#26681;&#25454;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#21464;&#24615;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#19988;&#22312;&#36825;&#20123;&#39564;&#35777;&#27169;&#22411;&#21457;&#29983;&#21464;&#21270;&#26102;&#20173;&#28982;&#20445;&#25345;&#20581;&#22766;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21487;&#33021;&#39564;&#35777;&#30340;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;CEs&#30340;&#23646;&#24615;&#19978;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;T-COL&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#20004;&#31181;&#21487;&#36873;&#32467;&#26500;&#21644;&#20960;&#32452;&#21327;&#21516;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) based systems have been suffering a lack of interpretability. To address this problem, counterfactual explanations (CEs) have been proposed. CEs are unique as they provide workable suggestions to users, in addition to explaining why a certain outcome was predicted. However, the application of CEs has been hindered by two main challenges, namely general user preferences and variable ML systems. User preferences, in particular, tend to be general rather than specific feature values. Additionally, CEs need to be customized to suit the variability of ML models, while also maintaining robustness even when these validation models change. To overcome these challenges, we propose several possible general user preferences that have been validated by user research and map them to the properties of CEs. We also introduce a new method called \uline{T}ree-based \uline{C}onditions \uline{O}ptional \uline{L}inks (T-COL), which has two optional structures and several groups of co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#25345;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#19981;&#21512;&#29702;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12545</link><description>&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation. (arXiv:2309.12545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#25345;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#19981;&#21512;&#29702;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;(CEs)&#20316;&#20026;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#20027;&#35201;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36890;&#24120;&#65292;CEs&#23545;&#20110;&#36755;&#20837;-&#36755;&#20986;&#23545;&#34987;&#23450;&#20041;&#20026;&#21040;&#36755;&#20837;&#30340;&#26368;&#23567;&#36317;&#31163;&#30340;&#25968;&#25454;&#28857;&#65292;&#20854;&#19982;&#36755;&#20986;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;CEs&#22312;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;(&#27604;&#22914;&#37325;&#26032;&#35757;&#32451;)&#26102;&#24456;&#23481;&#26131;&#34987;&#26080;&#25928;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#21464;&#21270;&#30340;&#33539;&#25968;&#29699;&#30028;&#38480;&#26469;&#35777;&#26126;CEs&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38024;&#23545;&#36825;&#31181;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#19981;&#26159;&#23436;&#20840;&#27491;&#30830;&#30340;&#65292;&#25110;&#32773;&#21487;&#33021;&#29983;&#25104;&#19981;&#21512;&#29702;&#30340;CEs&#65292;&#21363;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#23384;&#22312;&#31163;&#32676;&#20540;&#12290;&#20107;&#23454;&#19978;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#36317;&#31163;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#20445;&#25345;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for proximity and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#19971;&#20010;&#20195;&#34920;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.09825</link><description>&lt;p&gt;
AI&#29983;&#25104;&#20869;&#23481;&#30340;&#20559;&#35265;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#30340;&#32771;&#23519;
&lt;/p&gt;
&lt;p&gt;
Bias of AI-Generated Content: An Examination of News Produced by Large Language Models. (arXiv:2309.09825v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09825
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#19971;&#20010;&#20195;&#34920;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;&#21363;AI&#29983;&#25104;&#20869;&#23481;&#65289;&#20855;&#26377;&#25913;&#21464;&#25105;&#20204;&#29983;&#27963;&#21644;&#24037;&#20316;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#31181;&#36716;&#21464;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;LLM&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#30001;&#19971;&#31181;&#20195;&#34920;&#24615;LLM&#65288;&#21253;&#25324;ChatGPT&#21644;LLaMA&#65289;&#29983;&#25104;&#30340;AIGC&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#12298;&#32445;&#32422;&#26102;&#25253;&#12299;&#21644;&#36335;&#36879;&#31038;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#36825;&#20004;&#23478;&#23186;&#20307;&#20197;&#25552;&#20379;&#20844;&#27491;&#26080;&#20559;&#30340;&#26032;&#38395;&#32780;&#38395;&#21517;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#34987;&#35843;&#26597;&#30340;LLM&#24212;&#29992;&#20110;&#29983;&#25104;&#26032;&#38395;&#20869;&#23481;&#65292;&#20197;&#36825;&#20123;&#26032;&#38395;&#25991;&#31456;&#30340;&#26631;&#39064;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;AIGC&#19982;&#21407;&#22987;&#26032;&#38395;&#25991;&#31456;&#26469;&#35780;&#20272;LLM&#29983;&#25104;&#30340;AIGC&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#21508;&#20010;LLM&#22312;&#24102;&#26377;&#20559;&#35265;&#30340;&#25552;&#31034;&#19979;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#36890;&#36807;&#21521;&#20174;&#36825;&#20123;&#26032;&#38395;&#26631;&#39064;&#26500;&#24314;&#30340;&#25552;&#31034;&#20013;&#28155;&#21152;&#24615;&#21035;&#20559;&#35265;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#27599;&#20010;&#34987;&#35843;&#26597;&#30340;LLM&#29983;&#25104;&#30340;AIGC&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have the potential to transform our lives and work through the content they generate, known as AI-Generated Content (AIGC). To harness this transformation, we need to understand the limitations of LLMs. Here, we investigate the bias of AIGC produced by seven representative LLMs, including ChatGPT and LLaMA. We collect news articles from The New York Times and Reuters, both known for their dedication to provide unbiased news. We then apply each examined LLM to generate news content with headlines of these news articles as prompts, and evaluate the gender and racial biases of the AIGC produced by the LLM by comparing the AIGC and the original news articles. We further analyze the gender bias of each LLM under biased prompts by adding gender-biased messages to prompts constructed from these news headlines. Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM ex
&lt;/p&gt;</description></item><item><title>SayNav&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#30693;&#35782;&#21644;&#22330;&#26223;&#22270;&#23454;&#29616;&#23545;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#30340;&#39640;&#25928;&#27867;&#21270;&#65292;&#21160;&#24577;&#29983;&#25104;&#25351;&#20196;&#24182;&#26681;&#25454;&#26032;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2309.04077</link><description>&lt;p&gt;
SayNav&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26032;&#29615;&#22659;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments. (arXiv:2309.04077v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04077
&lt;/p&gt;
&lt;p&gt;
SayNav&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#30693;&#35782;&#21644;&#22330;&#26223;&#22270;&#23454;&#29616;&#23545;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#30340;&#39640;&#25928;&#27867;&#21270;&#65292;&#21160;&#24577;&#29983;&#25104;&#25351;&#20196;&#24182;&#26681;&#25454;&#26032;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#25512;&#29702;&#21644;&#21160;&#24577;&#35268;&#21010;&#33021;&#21147;&#23545;&#20110;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25191;&#34892;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#36825;&#26159;&#20154;&#31867;&#25152;&#20855;&#22791;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SayNav&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#31867;&#30693;&#35782;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#23545;&#26410;&#30693;&#22823;&#35268;&#27169;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#12290;SayNav&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25509;&#22320;&#26426;&#21046;&#65292;&#36880;&#27493;&#26500;&#24314;&#19968;&#20010;&#25506;&#32034;&#29615;&#22659;&#30340;3D&#22330;&#26223;&#22270;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#34892;&#19988;&#19978;&#19979;&#25991;&#36866;&#24403;&#30340;&#39640;&#23618;&#23548;&#33322;&#35745;&#21010;&#12290;&#28982;&#21518;&#65292;&#30001;&#39044;&#20808;&#35757;&#32451;&#30340;&#20302;&#23618;&#35268;&#21010;&#22120;&#25191;&#34892;LLM&#29983;&#25104;&#30340;&#35745;&#21010;&#65292;&#23558;&#27599;&#20010;&#35745;&#21010;&#30340;&#27493;&#39588;&#35270;&#20026;&#30701;&#36317;&#31163;&#28857;&#30446;&#26631;&#23548;&#33322;&#23376;&#20219;&#21153;&#12290;SayNav&#22312;&#23548;&#33322;&#36807;&#31243;&#20013;&#21160;&#24577;&#29983;&#25104;&#19968;&#27493;&#19968;&#27493;&#30340;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#26032;&#33719;&#21462;&#30340;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22810;&#20219;&#21153;&#26426;&#39564;&#35777;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;SayNav&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on a new mul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20174;EEG&#20449;&#21495;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#36825;&#20004;&#31181;&#27169;&#24577;&#12290;&#36890;&#36807;&#22312;&#26368;&#24191;&#27867;&#30340;EEG&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#21644;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13234</link><description>&lt;p&gt;
&#20174;&#33041;&#30005;&#22270;&#35299;&#30721;&#33258;&#28982;&#22270;&#20687;&#36827;&#34892;&#29289;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Decoding Natural Images from EEG for Object Recognition. (arXiv:2308.13234v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20174;EEG&#20449;&#21495;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#36825;&#20004;&#31181;&#27169;&#24577;&#12290;&#36890;&#36807;&#22312;&#26368;&#24191;&#27867;&#30340;EEG&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#21644;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#33041;&#33041;&#22270;&#65288;EEG&#65289;&#20197;&#20854;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#36866;&#24230;&#30340;&#20449;&#22122;&#27604;&#32780;&#38395;&#21517;&#12290;&#26368;&#36817;&#65292;&#33021;&#21542;&#20174;EEG&#20013;&#35299;&#30721;&#33258;&#28982;&#22270;&#20687;&#25104;&#20026;&#28909;&#38376;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20174;EEG&#20449;&#21495;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22270;&#20687;&#21644;EEG&#32534;&#30721;&#22120;&#20174;&#37197;&#23545;&#30340;&#22270;&#20687;&#21050;&#28608;&#21644;EEG&#21709;&#24212;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#36890;&#36807;&#32422;&#26463;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#26469;&#23545;&#40784;&#36825;&#20004;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#22312;EEG&#32534;&#30721;&#22120;&#20043;&#21069;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#25429;&#25417;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26368;&#24191;&#27867;&#30340;EEG&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;200&#31181;&#38646;&#26679;&#26412;&#20219;&#21153;&#20013;&#65292;top-1&#20934;&#30830;&#24230;&#36798;&#21040;15.6%&#65292;top-5&#20934;&#30830;&#24230;&#36798;&#21040;42.8%&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23545;EEG&#20449;&#21495;&#30340;&#26102;&#38388;&#12289;&#31354;&#38388;&#12289;&#39057;&#35889;&#21644;&#35821;&#20041;&#26041;&#38754;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#33391;&#22909;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalogram (EEG) is a brain signal known for its high time resolution and moderate signal-to-noise ratio. Whether natural images can be decoded from EEG has been a hot issue recently. In this paper, we propose a self-supervised framework to learn image representations from EEG signals. Specifically, image and EEG encoders are first used to extract features from paired image stimuli and EEG responses. Then we employ contrastive learning to align these two modalities by constraining their similarity. Additionally, we introduce two plug-in-play modules that capture spatial correlations before the EEG encoder. Our approach achieves state-of-the-art results on the most extensive EEG-image dataset, with a top-1 accuracy of 15.6% and a top-5 accuracy of 42.8% in 200-way zero-shot tasks. More importantly, extensive experiments analyzing the temporal, spatial, spectral, and semantic aspects of EEG signals demonstrate good biological plausibility. These results offer valuable insights 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.11432</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#23398;&#26415;&#30028;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30693;&#35782;&#30340;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#20915;&#31574;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#33719;&#21462;&#22823;&#37327;&#30340;&#32593;&#32476;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#30340;&#39640;&#28072;&#20852;&#36259;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#30340;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#36825;&#20123;&#30740;&#31350;&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#23545;&#33258;&#20027;&#20195;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26500;&#24314;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
&lt;/p&gt;</description></item><item><title>CMB&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;&#65292;&#22522;&#20110;&#20013;&#22269;&#26412;&#22303;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#35774;&#35745;&#65292;&#33021;&#22815;&#35299;&#20915;&#23558;&#33521;&#35821;&#21307;&#23398;&#35780;&#20272;&#32763;&#35793;&#21040;&#26412;&#22320;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08833</link><description>&lt;p&gt;
CMB&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMB: A Comprehensive Medical Benchmark in Chinese. (arXiv:2308.08833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08833
&lt;/p&gt;
&lt;p&gt;
CMB&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;&#65292;&#22522;&#20110;&#20013;&#22269;&#26412;&#22303;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#35774;&#35745;&#65292;&#33021;&#22815;&#35299;&#20915;&#23558;&#33521;&#35821;&#21307;&#23398;&#35780;&#20272;&#32763;&#35793;&#21040;&#26412;&#22320;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#21307;&#23398;&#39046;&#22495;&#21462;&#24471;&#37325;&#22823;&#31361;&#30772;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#24314;&#31435;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#21307;&#23398;&#22522;&#20934;&#25104;&#20026;&#34913;&#37327;&#36827;&#23637;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#22320;&#21306;&#30340;&#21307;&#23398;&#29615;&#22659;&#20855;&#26377;&#21508;&#33258;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#22312;&#20013;&#22269;&#22659;&#20869;&#20256;&#32479;&#20013;&#21307;&#30340;&#26222;&#36941;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#20165;&#20165;&#32763;&#35793;&#22522;&#20110;&#33521;&#35821;&#30340;&#21307;&#23398;&#35780;&#20272;&#21487;&#33021;&#23548;&#33268;&#24403;&#22320;&#29615;&#22659;&#20013;&#30340;&#8220;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CMB&#65288;Comprehensive Medical Benchmark in Chinese&#65289;&#30340;&#26412;&#22320;&#21270;&#21307;&#23398;&#22522;&#20934;&#65292;&#23436;&#20840;&#35774;&#35745;&#21644;&#26681;&#26893;&#20110;&#20013;&#22269;&#26412;&#22303;&#30340;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#12290;&#23613;&#31649;&#20256;&#32479;&#20013;&#21307;&#26159;&#36825;&#20010;&#35780;&#20272;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23427;&#24182;&#19981;&#26500;&#25104;&#20854;&#20840;&#37096;&#12290;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#30693;&#21517;&#30340;&#22823;&#35268;&#27169;LLMs&#65292;&#21253;&#25324;ChatGPT&#12289;GPT-4&#12289;&#19987;&#38376;&#30340;&#20013;&#25991;LLMs&#21644;&#19987;&#38376;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in \textit{contextual incongruities} to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. It is worth not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;CMT&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#26102;&#38388;MMMA&#22270;&#19978;&#36827;&#34892;&#20247;&#21253;&#27450;&#35784;&#26816;&#27979;&#12290;CMT&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#25429;&#25417;&#22270;&#30340;&#24322;&#36136;&#24615;&#21644;&#21160;&#24577;&#24615;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CMT&#22312;&#20195;&#34920;&#24615;MMMA&#24494;&#20449;&#30340;&#34892;&#19994;&#35268;&#27169;HTG&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20063;&#26174;&#31034;&#20986;&#22312;&#22823;&#35268;&#27169;&#20844;&#20849;&#37329;&#34701;HTG&#19978;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#22270;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.02793</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#26102;&#38388;MMMA&#22270;&#19978;&#36827;&#34892;&#20247;&#21253;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Crowdsourcing Fraud Detection over Heterogeneous Temporal MMMA Graph. (arXiv:2308.02793v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;CMT&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#26102;&#38388;MMMA&#22270;&#19978;&#36827;&#34892;&#20247;&#21253;&#27450;&#35784;&#26816;&#27979;&#12290;CMT&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#25429;&#25417;&#22270;&#30340;&#24322;&#36136;&#24615;&#21644;&#21160;&#24577;&#24615;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CMT&#22312;&#20195;&#34920;&#24615;MMMA&#24494;&#20449;&#30340;&#34892;&#19994;&#35268;&#27169;HTG&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20063;&#26174;&#31034;&#20986;&#22312;&#22823;&#35268;&#27169;&#20844;&#20849;&#37329;&#34701;HTG&#19978;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#22270;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#20892;&#22330;&#19994;&#21153;&#30340;&#20852;&#36215;&#21033;&#29992;&#22810;&#29992;&#36884;&#28040;&#24687;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65288;MMMA&#65289;&#35825;&#20351;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#36827;&#34892;&#20247;&#21253;&#27450;&#35784;&#65292;&#32473;&#28857;&#20987;&#20892;&#22330;&#24037;&#20154;&#36896;&#25104;&#36130;&#21153;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMT&#30340;&#26032;&#22411;&#23545;&#27604;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MMMA&#30340;&#24322;&#26500;&#26102;&#38388;&#22270;&#65288;HTG&#65289;&#19978;&#36827;&#34892;&#20247;&#21253;&#27450;&#35784;&#26816;&#27979;&#12290;CMT&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#25429;&#25417;HTG&#30340;&#24322;&#36136;&#24615;&#21644;&#21160;&#24577;&#24615;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#20247;&#21253;&#27450;&#35784;&#26816;&#27979;&#30340;&#39640;&#36136;&#37327;&#34920;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;CMT&#22312;&#20195;&#34920;&#24615;MMMA&#24494;&#20449;&#30340;&#34892;&#19994;&#35268;&#27169;HTG&#19978;&#26816;&#27979;&#20247;&#21253;&#27450;&#35784;&#65292;&#24182;&#19988;&#20854;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;CMT&#36824;&#26174;&#31034;&#20986;&#22312;&#22823;&#35268;&#27169;&#20844;&#20849;&#37329;&#34701;HTG&#19978;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#22270;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of the click farm business using Multi-purpose Messaging Mobile Apps (MMMAs) tempts cybercriminals to perpetrate crowdsourcing frauds that cause financial losses to click farm workers. In this paper, we propose a novel contrastive multi-view learning method named CMT for crowdsourcing fraud detection over the heterogeneous temporal graph (HTG) of MMMA. CMT captures both heterogeneity and dynamics of HTG and generates high-quality representations for crowdsourcing fraud detection in a self-supervised manner. We deploy CMT to detect crowdsourcing frauds on an industry-size HTG of a representative MMMA WeChat and it significantly outperforms other methods. CMT also shows promising results for fraud detection on a large-scale public financial HTG, indicating that it can be applied in other graph anomaly detection tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.02058</link><description>&lt;p&gt;
&#25972;&#21512;&#40065;&#33725;&#34892;&#20026;&#21040;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Recklessness to Collaborative Filtering based Recommender Systems. (arXiv:2308.02058v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#21487;&#38752;&#24615;&#27979;&#37327;&#30340;&#25512;&#33616;&#31995;&#32479;&#24448;&#24448;&#22312;&#39044;&#27979;&#20013;&#26356;&#21152;&#20445;&#23432;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20445;&#25345;&#21487;&#38752;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26032;&#39062;&#24615;&#30340;&#26174;&#33879;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#30697;&#38453;&#20998;&#35299;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#39033;&#26032;&#30340;&#39033;&#65292;&#31216;&#20026;&#40065;&#33725;&#34892;&#20026;&#65292;&#23427;&#21487;&#20197;&#25511;&#21046;&#22312;&#20570;&#20986;&#20851;&#20110;&#39044;&#27979;&#21487;&#38752;&#24615;&#30340;&#20915;&#31574;&#26102;&#25152;&#24076;&#26395;&#30340;&#39118;&#38505;&#27700;&#24179;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#40065;&#33725;&#34892;&#20026;&#19981;&#20165;&#20801;&#35768;&#36827;&#34892;&#39118;&#38505;&#35843;&#25511;&#65292;&#36824;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#30340;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems that include some reliability measure of their predictions tend to be more conservative in forecasting, due to their constraint to preserve reliability. This leads to a significant drop in the coverage and novelty that these systems can provide. In this paper, we propose the inclusion of a new term in the learning process of matrix factorization-based recommender systems, called recklessness, which enables the control of the risk level desired when making decisions about the reliability of a prediction. Experimental results demonstrate that recklessness not only allows for risk regulation but also improves the quantity and quality of predictions provided by the recommender system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01923</link><description>&lt;p&gt;
&#22810;&#37325;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fairness Improvement with Multiple Protected Attributes. (arXiv:2308.01923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36719;&#20214;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#65292;&#20294;&#32771;&#34385;&#21040;&#35768;&#22810;&#29992;&#25143;&#20855;&#26377;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#23545;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#32771;&#34385;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;ML&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25913;&#21892;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#22823;&#22823;&#38477;&#20302;&#20102;&#26410;&#32771;&#34385;&#30340;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;88.3&#65285;&#30340;&#24773;&#20917;&#19979;&#35266;&#23519;&#21040;&#36825;&#31181;&#38477;&#20302;&#65288;&#24179;&#22343;&#20026;57.5&#65285;&#65289;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#32771;&#34385;&#21333;&#20010;&#21644;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#20934;&#30830;&#29575;&#25439;&#22833;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling
&lt;/p&gt;</description></item><item><title>DeepIPCv2&#26159;&#19968;&#31181;&#21033;&#29992;LiDAR&#20256;&#24863;&#22120;&#24863;&#30693;&#29615;&#22659;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#28857;&#20113;&#20316;&#20026;&#24863;&#30693;&#36755;&#20837;&#65292;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06647</link><description>&lt;p&gt;
DeepIPCv2&#65306;&#21033;&#29992;LiDAR&#24378;&#21270;&#33258;&#21160;&#39550;&#39542;&#29615;&#22659;&#24863;&#30693;&#19982;&#23548;&#33322;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DeepIPCv2: LiDAR-powered Robust Environmental Perception and Navigational Control for Autonomous Vehicle. (arXiv:2307.06647v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06647
&lt;/p&gt;
&lt;p&gt;
DeepIPCv2&#26159;&#19968;&#31181;&#21033;&#29992;LiDAR&#20256;&#24863;&#22120;&#24863;&#30693;&#29615;&#22659;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#28857;&#20113;&#20316;&#20026;&#24863;&#30693;&#36755;&#20837;&#65292;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DeepIPCv2&#65292;&#19968;&#31181;&#21033;&#29992;LiDAR&#20256;&#24863;&#22120;&#24863;&#30693;&#29615;&#22659;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20809;&#29031;&#26465;&#20214;&#36739;&#24046;&#30340;&#24773;&#20917;&#19979;&#12290;DeepIPCv2&#20351;&#29992;&#19968;&#32452;LiDAR&#28857;&#20113;&#20316;&#20026;&#20854;&#20027;&#35201;&#24863;&#30693;&#36755;&#20837;&#12290;&#30001;&#20110;&#28857;&#20113;&#19981;&#21463;&#20809;&#29031;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#28165;&#26224;&#30340;&#29615;&#22659;&#35266;&#23519;&#65292;&#26080;&#35770;&#26465;&#20214;&#22914;&#20309;&#12290;&#36825;&#20351;&#24471;&#24863;&#30693;&#27169;&#22359;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#22330;&#26223;&#29702;&#35299;&#21644;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25903;&#25345;&#25511;&#21046;&#27169;&#22359;&#20934;&#30830;&#20272;&#35745;&#23548;&#33322;&#25511;&#21046;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#35813;&#27169;&#22411;&#26469;&#39044;&#27979;&#19968;&#32452;&#39550;&#39542;&#35760;&#24405;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#26465;&#20214;&#19979;&#36827;&#34892;&#30495;&#23454;&#33258;&#21160;&#39550;&#39542;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28040;&#34701;&#21644;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#35777;&#26126;&#20854;&#24615;&#33021;&#12290;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#65292;DeepIPCv2&#22312;&#25152;&#26377;&#26465;&#20214;&#19979;&#22343;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DeepIPCv2, an autonomous driving model that perceives the environment using a LiDAR sensor for more robust drivability, especially when driving under poor illumination conditions. DeepIPCv2 takes a set of LiDAR point clouds for its main perception input. As point clouds are not affected by illumination changes, they can provide a clear observation of the surroundings no matter what the condition is. This results in a better scene understanding and stable features provided by the perception module to support the controller module in estimating navigational control properly. To evaluate its performance, we conduct several tests by deploying the model to predict a set of driving records and perform real automated driving under three different conditions. We also conduct ablation and comparative studies with some recent models to justify its performance. Based on the experimental results, DeepIPCv2 shows a robust performance by achieving the best drivability in all conditions. C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;</title><link>http://arxiv.org/abs/2306.00006</link><description>&lt;p&gt;
&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65306;&#29992;&#20110;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#30340;&#21333;&#31867;&#21516;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#65288;GAD&#65289;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#21457;&#29616;&#19968;&#31181;&#26222;&#36941;&#30340;&#23646;&#24615;......&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327; - &#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;......&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270; (TAM)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;_neighbors&#30340;&#26412;&#22320;&#20146;&#21644;&#21147;&#26469;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#25152;&#25552;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;
One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19358</link><description>&lt;p&gt;
&#31283;&#20581;&#30340;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#23646;&#24615;&#24050;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25991;&#29486;&#26222;&#36941;&#35748;&#20026;LLMs&#34920;&#31034;&#30001;&#23569;&#25968;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#21644;&#24133;&#24230;&#30340;&#8220;&#24322;&#24120;&#32500;&#24230;&#8221;&#20027;&#23548;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#20943;&#36731;&#36825;&#20123;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#36843;&#20351;LLMs&#25104;&#20026;&#21508;&#21521;&#21516;&#24615;&#65288;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25152;&#26377;&#32500;&#24230;&#20855;&#26377;&#22343;&#21248;&#26041;&#24046;&#65289;&#30340;&#12290;&#21508;&#21521;&#21516;&#24615;&#34987;&#35748;&#20026;&#26159;LLMs&#30340;&#19968;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#26356;&#21152;&#36148;&#36817;&#20154;&#31867;&#30452;&#35273;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;NLP&#20013;&#21508;&#21521;&#21516;&#24615;&#30340;&#35768;&#22810;&#35266;&#28857;&#37117;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#26377;&#32570;&#38519;&#30340;&#21508;&#21521;&#21516;&#24615;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-STAR&#65306;&#22522;&#20110;IsoScore$^{\star}$&#30340;&#31283;&#23450;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#21521;&#37327;&#20803;&#32032;&#25968;&#37327;&#30340;RFSs&#30340;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;PMB&#28388;&#27874;&#22120;&#29992;&#20110;SLAM&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#38598;&#21512;&#22411;BP-mapping&#12289;SLAM&#12289;&#22810;&#30446;&#26631;&#36319;&#36394;&#21644;&#21516;&#26102;&#23450;&#20301;&#19982;&#36319;&#36394;&#28388;&#27874;&#22120;&#31561;&#26032;&#39062;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.04797</link><description>&lt;p&gt;
&#38598;&#21512;&#22411;&#20449;&#24565;&#20256;&#25773;&#21450;&#20854;&#22312;&#27850;&#26494;&#22810;&#20271;&#21162;&#21033;SLAM&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Set-Type Belief Propagation with Applications to Poisson Multi-Bernoulli SLAM. (arXiv:2305.04797v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#21521;&#37327;&#20803;&#32032;&#25968;&#37327;&#30340;RFSs&#30340;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;PMB&#28388;&#27874;&#22120;&#29992;&#20110;SLAM&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#38598;&#21512;&#22411;BP-mapping&#12289;SLAM&#12289;&#22810;&#30446;&#26631;&#36319;&#36394;&#21644;&#21516;&#26102;&#23450;&#20301;&#19982;&#36319;&#36394;&#28388;&#27874;&#22120;&#31561;&#26032;&#39062;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24565;&#20256;&#25773;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35745;&#31639;&#38543;&#26426;&#21464;&#37327;&#30340;&#36817;&#20284;&#36793;&#32536;&#27010;&#29575;&#23494;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#26631;&#20934;&#24418;&#24335;&#19979;&#65292;&#20449;&#24565;&#20256;&#25773;&#21482;&#36866;&#29992;&#20110;&#20855;&#26377;&#22266;&#23450;&#21644;&#24050;&#30693;&#30340;&#21521;&#37327;&#20803;&#32032;&#25968;&#37327;&#30340;&#21521;&#37327;&#22411;&#38543;&#26426;&#21464;&#37327;&#65292;&#32780;&#26576;&#20123;&#24212;&#29992;&#20381;&#36182;&#20110;&#20855;&#26377;&#26410;&#30693;&#21521;&#37327;&#20803;&#32032;&#25968;&#37327;&#30340;RFSs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;RFSs&#24207;&#21015;&#23450;&#20041;&#30340;&#22240;&#23376;&#22270;&#30340;&#20449;&#24565;&#20256;&#25773;&#35268;&#21017;&#65292;&#20854;&#20013;&#27599;&#20010;RFS&#20855;&#26377;&#26410;&#30693;&#30340;&#20803;&#32032;&#25968;&#37327;&#65292;&#26088;&#22312;&#23548;&#20986;RFSs&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21521;&#37327;&#22411;&#20449;&#24565;&#20256;&#25773;&#26159;&#38598;&#21512;&#22411;&#20449;&#24565;&#20256;&#25773;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#27599;&#20010;RFS&#36981;&#24490;&#20271;&#21162;&#21033;&#36807;&#31243;&#12290;&#20026;&#20102;&#35777;&#26126;&#24320;&#21457;&#30340;&#38598;&#21512;&#22411;&#20449;&#24565;&#20256;&#25773;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;SLAM&#30340;PMB&#28388;&#27874;&#22120;&#65292;&#20174;&#32780;&#33258;&#28982;&#22320;&#23548;&#33268;&#26032;&#30340;&#38598;&#21512;&#22411;BP&#26144;&#23556;&#12289;SLAM&#12289;&#22810;&#30446;&#26631;&#36319;&#36394;&#21644;&#21516;&#26102;&#23450;&#20301;&#19982;&#36319;&#36394;&#28388;&#27874;&#22120;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21521;&#37327;&#22411;&#20449;&#24565;&#20256;&#25773;&#21644;&#20027;&#39064;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Belief propagation (BP) is a useful probabilistic inference algorithm for efficiently computing approximate marginal probability densities of random variables. However, in its standard form, BP is only applicable to the vector-type random variables with a fixed and known number of vector elements, while certain applications rely on RFSs with an unknown number of vector elements. In this paper, we develop BP rules for factor graphs defined on sequences of RFSs where each RFS has an unknown number of elements, with the intention of deriving novel inference methods for RFSs. Furthermore, we show that vector-type BP is a special case of set-type BP, where each RFS follows the Bernoulli process. To demonstrate the validity of developed set-type BP, we apply it to the PMB filter for SLAM, which naturally leads to new set-type BP-mapping, SLAM, multi-target tracking, and simultaneous localization and tracking filters. Finally, we explore the relationships between the vector-type BP and the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2304.14364</link><description>&lt;p&gt;
CONSCENDI: &#19968;&#31181;&#21453;&#23545;&#27604;&#19988;&#22330;&#26223;&#24341;&#23548;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#20026;&#34394;&#25311;&#21161;&#25163;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants. (arXiv:2304.14364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPT-4&#31561;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#26032;&#19968;&#20195;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#34394;&#25311;&#21161;&#25163;&#24212;&#36816;&#32780;&#29983;&#12290;&#36825;&#20123;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#23458;&#25143;&#30340;&#20855;&#20307;&#29992;&#20363;&#36827;&#34892;&#23450;&#21046;&#65292;&#20294;&#30830;&#20445;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#20165;&#31526;&#21512;&#25552;&#31034;&#25351;&#20196;&#20013;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24072;&#36890;&#24120;&#20351;&#29992;&#21478;&#19968;&#20010;&#31216;&#20026;&#38450;&#25252;&#26639;&#27169;&#22411;&#30340;&#27169;&#22411;&#26469;&#39564;&#35777;&#20195;&#29702;&#36755;&#20986;&#26159;&#21542;&#19982;&#20854;&#35268;&#21017;&#21644;&#32422;&#26463;&#23545;&#40784;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#33976;&#39311;&#26041;&#27861;&#26469;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20351;&#29992;GPT-4&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;CONSCENDI&#36807;&#31243;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#20250;&#29983;&#25104;&#19968;&#32452;&#36829;&#21453;&#35268;&#21017;&#30340;&#22330;&#26223;&#65292;&#36825;&#20123;&#22330;&#26223;&#21015;&#20030;&#20102;&#36829;&#21453;&#35268;&#21017;&#30340;&#22810;&#26679;&#21270;&#39640;&#32423;&#26041;&#24335;&#12290;&#36825;&#31181;&#22330;&#26223;&#24341;&#23548;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#23427;&#20351;&#24471;&#27169;&#22411;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wave of new task-based virtual assistants has been fueled by increasingly powerful large language models, such as GPT-4. These conversational agents can be customized to serve customer-specific use cases, but ensuring that agent-generated text conforms to designer-specified rules included in prompt instructions alone is challenging. Therefore, chatbot designers often use another model, called a guardrail model, to verify that the agent output aligns with their rules and constraints. We explore using a distillation approach to guardrail models to monitor the output of the first model using training data from GPT-4. We find two crucial steps to our CONSCENDI process: scenario-augmented generation and contrastive training examples. When generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set of rule-violating conversations, and it p
&lt;/p&gt;</description></item></channel></rss>