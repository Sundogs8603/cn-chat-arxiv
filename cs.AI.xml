<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>L2G2G&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21516;&#27493;&#28508;&#22312;&#33410;&#28857;&#34920;&#31034;&#20197;&#25552;&#39640;GAE&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#35745;&#31639;&#21482;&#26377;&#26412;&#22320;&#22270;&#22359;&#25439;&#22833;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#20102;&#22270;&#30340;&#20449;&#24687;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01614</link><description>&lt;p&gt;
L2G2G: &#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#22522;&#20110;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
L2G2G: a Scalable Local-to-Global Network Embedding with Graph Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01614
&lt;/p&gt;
&lt;p&gt;
L2G2G&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21516;&#27493;&#28508;&#22312;&#33410;&#28857;&#34920;&#31034;&#20197;&#25552;&#39640;GAE&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#35745;&#31639;&#21482;&#26377;&#26412;&#22320;&#22270;&#22359;&#25439;&#22833;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#20102;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20998;&#26512;&#23454;&#38469;&#32593;&#32476;&#65292;&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#29992;&#24037;&#20855;&#12290;&#36825;&#20123;&#26041;&#27861;&#65292;&#22914;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;(GAE)&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#20302;&#32500;&#34920;&#31034;&#65292;&#20063;&#31216;&#20026;&#23884;&#20837;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#33719;&#24471;;&#36825;&#20123;&#23884;&#20837;&#19982;&#35299;&#30721;&#22120;&#19968;&#36215;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#21644;&#36793;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#34429;&#28982;GAE&#24448;&#24448;&#30456;&#24403;&#20934;&#30830;&#65292;&#20294;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#25913;&#21892;&#36895;&#24230;&#65292;Local2Global&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#21521;&#37327;&#21516;&#27493;&#30340;&#22270;&#22359;&#23884;&#20837;&#30456;&#32467;&#21512;&#65292;&#26174;&#31034;&#20986;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#25928;&#26524;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2G2G&#65292;&#19968;&#31181;_Local2Global&#26041;&#27861;&#65292;&#23427;&#22312;&#19981;&#29306;&#29298;&#21487;&#25193;&#23637;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;GAE&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#25913;&#36827;&#26159;&#36890;&#36807;&#22312;&#35757;&#32451;GAE&#26399;&#38388;&#21160;&#24577;&#21516;&#27493;&#28508;&#22312;&#33410;&#28857;&#34920;&#31034;&#26469;&#23454;&#29616;&#30340;&#12290;&#23427;&#36824;&#21463;&#30410;&#20110;&#35299;&#30721;&#22120;&#35745;&#31639;&#21482;&#26377;&#26412;&#22320;&#22270;&#22359;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#27599;&#20010;&#26102;&#20195;&#20013;&#30340;&#26412;&#22320;&#23884;&#20837;&#23545;&#40784;&#21033;&#29992;&#20102;&#26356;&#22810;&#26469;&#33258;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
For analysing real-world networks, graph representation learning is a popular tool. These methods, such as a graph autoencoder (GAE), typically rely on low-dimensional representations, also called embeddings, which are obtained through minimising a loss function; these embeddings are used with a decoder for downstream tasks such as node classification and edge prediction. While GAEs tend to be fairly accurate, they suffer from scalability issues. For improved speed, a Local2Global approach, which combines graph patch embeddings based on eigenvector synchronisation, was shown to be fast and achieve good accuracy. Here we propose L2G2G, a Local2Global method which improves GAE accuracy without sacrificing scalability. This improvement is achieved by dynamically synchronising the latent node representations, while training the GAEs. It also benefits from the decoder computing an only local patch loss. Hence, aligning the local embeddings in each epoch utilises more information from the gr
&lt;/p&gt;</description></item><item><title>Nomic Embed&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;</title><link>https://rss.arxiv.org/abs/2402.01613</link><description>&lt;p&gt;
Nomic Embed&#65306;&#35757;&#32451;&#21487;&#22797;&#29616;&#30340;&#38271;&#19978;&#19979;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nomic Embed: Training a Reproducible Long Context Text Embedder
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01613
&lt;/p&gt;
&lt;p&gt;
Nomic Embed&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#25551;&#36848;&#20102;nomic-embed-text-v1&#30340;&#35757;&#32451;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#22343;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;&#25105;&#20204;&#22312;Apache 2&#35768;&#21487;&#19979;&#21457;&#24067;&#20102;&#35757;&#32451;&#20195;&#30721;&#21644;&#27169;&#22411;&#26435;&#37325;&#12290;&#19982;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;2.35&#20159;&#20010;&#31574;&#21010;&#25991;&#26412;&#23545;&#30340;&#35757;&#32451;&#25968;&#25454;&#21152;&#36733;&#22120;&#65292;&#21487;&#20197;&#23436;&#20840;&#22797;&#29616;nomic-embed-text-v1&#12290;&#20320;&#21487;&#20197;&#22312;https://github.com/nomic-ai/contrastors&#25214;&#21040;&#27169;&#22411;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on short and long-context tasks. We release the training code and model weights under an Apache 2 license. In contrast with other open-source models, we release a training data loader with 235 million curated text pairs that allows for the full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01607</link><description>&lt;p&gt;
&#20855;&#26377;&#24517;&#35201;&#22238;&#28335;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
Natural Counterfactuals With Necessary Backtracking
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#25552;&#20379;&#35299;&#37322;&#21644;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;Judea Pearl&#30340;&#30740;&#31350;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#24456;&#20248;&#38597;&#65292;&#20294;&#20854;&#29983;&#25104;&#21453;&#20107;&#23454;&#24773;&#26223;&#24448;&#24448;&#38656;&#35201;&#36807;&#20110;&#33073;&#31163;&#23454;&#38469;&#24773;&#26223;&#30340;&#24178;&#39044;&#65292;&#22240;&#27492;&#38590;&#20197;&#23454;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26694;&#26550;&#21644;&#19968;&#31181;&#26681;&#25454;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#25913;&#36827;&#65292;&#20801;&#35768;&#23545;&#22240;&#26524;&#21069;&#32622;&#21464;&#37327;&#36827;&#34892;&#25913;&#21464;&#20197;&#26368;&#23567;&#21270;&#19982;&#23454;&#38469;&#24773;&#26223;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#24615;&#20934;&#21017;&#20801;&#35768;&#20294;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#21033;&#29992;&#30693;&#35782;&#22686;&#24378;&#21644;&#25512;&#29702;&#65292;&#24341;&#23548;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#22312;&#20219;&#21153;&#20013;&#24471;&#21040;&#36866;&#24403;&#30340;&#25351;&#23548;&#65292;&#20811;&#26381;&#20854;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01602</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#23548;&#33322;&#21592;: &#24341;&#23548;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#30693;&#35782;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01602
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#21033;&#29992;&#30693;&#35782;&#22686;&#24378;&#21644;&#25512;&#29702;&#65292;&#24341;&#23548;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#22312;&#20219;&#21153;&#20013;&#24471;&#21040;&#36866;&#24403;&#30340;&#25351;&#23548;&#65292;&#20811;&#26381;&#20854;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36824;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#65292;&#38459;&#27490;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#35201;&#27714;&#26356;&#39640;&#30340;&#21487;&#20449;&#24230;&#21644;&#21487;&#29992;&#24615;&#12290;&#30001;&#20110;FMs&#26159;&#20351;&#29992;&#26088;&#22312;&#20197;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#37325;&#26032;&#26500;&#24314;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#25152;&#20197;&#19981;&#33021;&#20445;&#35777;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#29992;&#25143;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#20559;&#22909;&#19968;&#33268;&#12290;&#22312;&#26412;&#32508;&#36848;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#27010;&#25324;&#20102;&#21508;&#31181;&#20195;&#29702;&#22914;&#20309;&#19982;FMs&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#21644;&#25512;&#29702;&#26469;&#36866;&#24212;&#19968;&#32452;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38416;&#26126;&#20102;&#20195;&#29702;&#35282;&#33394;&#31867;&#21035;&#65292;&#22914;&#26356;&#26032;&#22522;&#30784;FM&#65292;&#36741;&#21161;&#25552;&#31034;FM&#21644;&#35780;&#20272;FM&#36755;&#20986;&#12290;&#25105;&#20204;&#36824;&#23558;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20195;&#29702;&#20132;&#20114;&#21407;&#22411;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FMs) such as large language models have revolutionized the field of AI by showing remarkable performance in various tasks. However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability. Since FMs are trained using loss functions aimed at reconstructing the training corpus in a self-supervised manner, there is no guarantee that the model's output aligns with users' preferences for a specific task at hand. In this survey paper, we propose a conceptual framework that encapsulates different modes by which agents could interact with FMs and guide them suitably for a set of tasks, particularly through knowledge augmentation and reasoning. Our framework elucidates agent role categories such as updating the underlying FM, assisting with prompting the FM, and evaluating the FM output. We also categorize several state-of-the-art approaches into agent interaction prot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01591</link><description>&lt;p&gt;
BAT: &#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20851;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
BAT: Learning to Reason about Spatial Sounds with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#20154;&#31867;&#25216;&#33021;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#22768;&#38899;&#26469;&#23548;&#33322;&#21644;&#35299;&#37322;&#25105;&#20204;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#23558;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22797;&#21046;&#36825;&#31181;&#22266;&#26377;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#37326;&#22806;&#31354;&#38388;&#22768;&#38899;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#25105;&#20204;&#20351;&#29992;AudioSet&#21644;SoundSpaces 2.0&#21512;&#25104;&#20102;&#19968;&#20010;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;SpatialSoundQA&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#20197;&#35757;&#32451;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;BAT&#30340;&#22768;&#23398;&#21069;&#31471;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#21517;&#20026;Spatial Audio Spectrogram Transformer&#65288;Spatial-AST&#65289;&#30340;&#21019;&#26032;&#31354;&#38388;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23427;&#26412;&#36523;&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#12289;&#31354;&#38388;&#23450;&#20301;&#21644;&#36317;&#31163;&#20272;&#35745;&#31561;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;Spatial-AST&#19982;LLaMA-2 7B&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01586</link><description>&lt;p&gt;
TrustAgent: &#36890;&#36807;&#20195;&#29702;&#26500;&#25104;&#23454;&#29616;&#23433;&#20840;&#21487;&#20449;&#36182;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#20449;&#24230;&#20173;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#30001;&#20110;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#65292;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#23545;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#32500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#31574;&#30053;&#65306;&#39044;&#20808;&#35268;&#21010;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#20043;&#21069;&#21521;&#27169;&#22411;&#27880;&#20837;&#23433;&#20840;&#30693;&#35782;&#65307;&#35268;&#21010;&#36807;&#31243;&#20013;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#26102;&#22686;&#24378;&#23433;&#20840;&#24615;&#65307;&#35745;&#21010;&#21518;&#26816;&#26597;&#31574;&#30053;&#65292;&#36890;&#36807;&#35745;&#21010;&#21518;&#26816;&#26597;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#26377;&#25928;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20854;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#25991;&#31456;&#24635;&#32467;&#20102;NeurIPS 2023&#20250;&#35758;&#19978;&#20851;&#20110;&#25945;&#32946;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAIED&#65289;&#30740;&#35752;&#20250;&#30340;&#27963;&#21160;&#65292;&#24182;&#31361;&#20986;&#25351;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01580</link><description>&lt;p&gt;
&#25945;&#32946;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAIED&#65289;&#65306;&#36827;&#23637;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Education (GAIED): Advances, Opportunities, and Challenges
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01580
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25991;&#31456;&#24635;&#32467;&#20102;NeurIPS 2023&#20250;&#35758;&#19978;&#20851;&#20110;&#25945;&#32946;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAIED&#65289;&#30740;&#35752;&#20250;&#30340;&#27963;&#21160;&#65292;&#24182;&#31361;&#20986;&#25351;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25991;&#31456;&#26159;&#30001;&#20316;&#32773;&#22312;NeurIPS 2023&#20250;&#35758;&#19978;&#32452;&#32455;&#30340;GAIED&#30740;&#35752;&#20250;&#21457;&#23637;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#32452;&#32455;GAIED&#30740;&#35752;&#20250;&#26159;&#20026;&#20102;&#20419;&#36827;&#30740;&#31350;&#20154;&#21592;&#12289;&#25945;&#32946;&#32773;&#21644;&#23454;&#36341;&#32773;&#30340;&#20132;&#27969;&#65292;&#25506;&#32034;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#27010;&#35201;&#20171;&#32461;&#30740;&#35752;&#20250;&#30340;&#27963;&#21160;&#65292;&#24182;&#31361;&#20986;&#25351;&#20986;&#22312;GAIED&#39046;&#22495;&#20013;&#30340;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey article has grown out of the GAIED (pronounced "guide") workshop organized by the authors at the NeurIPS 2023 conference. We organized the GAIED workshop as part of a community-building effort to bring together researchers, educators, and practitioners to explore the potential of generative AI for enhancing education. This article aims to provide an overview of the workshop activities and highlight several future research directions in the area of GAIED.
&lt;/p&gt;</description></item><item><title>Boximator&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#21512;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30828;&#30418;&#21644;&#36719;&#30418;&#32422;&#26463;&#31867;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#20316;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;&#23427;&#30340;&#35757;&#32451;&#36807;&#31243;&#20445;&#30041;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#33258;&#25105;&#36861;&#36394;&#25216;&#26415;&#31616;&#21270;&#20102;&#30418;&#23376;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#32852;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Boximator&#22312;&#35270;&#39057;&#36136;&#37327;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31283;&#20581;&#30340;&#36816;&#21160;&#21487;&#25511;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01566</link><description>&lt;p&gt;
Boximator: &#29983;&#25104;&#20016;&#23500;&#21644;&#21487;&#25511;&#30340;&#35270;&#39057;&#21512;&#25104;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Boximator: Generating Rich and Controllable Motions for Video Synthesis
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01566
&lt;/p&gt;
&lt;p&gt;
Boximator&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#21512;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30828;&#30418;&#21644;&#36719;&#30418;&#32422;&#26463;&#31867;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#20316;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;&#23427;&#30340;&#35757;&#32451;&#36807;&#31243;&#20445;&#30041;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#33258;&#25105;&#36861;&#36394;&#25216;&#26415;&#31616;&#21270;&#20102;&#30418;&#23376;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#32852;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Boximator&#22312;&#35270;&#39057;&#36136;&#37327;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31283;&#20581;&#30340;&#36816;&#21160;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#21512;&#25104;&#20013;&#65292;&#29983;&#25104;&#20016;&#23500;&#19988;&#21487;&#25511;&#30340;&#21160;&#20316;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Boximator&#65292;&#29992;&#20110;&#31934;&#32454;&#30340;&#21160;&#20316;&#25511;&#21046;&#12290;Boximator&#24341;&#20837;&#20102;&#20004;&#31181;&#32422;&#26463;&#31867;&#22411;&#65306;&#30828;&#30418;&#21644;&#36719;&#30418;&#12290;&#29992;&#25143;&#20351;&#29992;&#30828;&#30418;&#22312;&#26465;&#20214;&#24103;&#20013;&#36873;&#25321;&#23545;&#35937;&#65292;&#28982;&#21518;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#30418;&#23376;&#26469;&#31895;&#30053;&#25110;&#20005;&#26684;&#22320;&#23450;&#20041;&#23545;&#35937;&#22312;&#26410;&#26469;&#24103;&#20013;&#30340;&#20301;&#32622;&#12289;&#24418;&#29366;&#25110;&#36816;&#21160;&#36335;&#24452;&#12290;Boximator&#20316;&#20026;&#29616;&#26377;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#25554;&#20214;&#36816;&#34892;&#12290;&#23427;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#36807;&#20923;&#32467;&#21407;&#22987;&#26435;&#37325;&#65292;&#21482;&#35757;&#32451;&#25511;&#21046;&#27169;&#22359;&#26469;&#20445;&#30041;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#36861;&#36394;&#25216;&#26415;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#30418;&#23376;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#32852;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Boximator&#22312;&#35270;&#39057;&#36136;&#37327;&#65288;FVD&#65289;&#24471;&#20998;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#25913;&#36827;&#20102;&#20004;&#20010;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#22312;&#21152;&#20837;&#30418;&#23376;&#32422;&#26463;&#21518;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#20854;&#31283;&#20581;&#30340;&#36816;&#21160;&#21487;&#25511;&#24615;&#36890;&#36807;&#36793;&#30028;&#30340;&#21095;&#22686;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating rich and controllable motion is a pivotal challenge in video synthesis. We propose Boximator, a new approach for fine-grained motion control. Boximator introduces two constraint types: hard box and soft box. Users select objects in the conditional frame using hard boxes and then use either type of boxes to roughly or rigorously define the object's position, shape, or motion path in future frames. Boximator functions as a plug-in for existing video diffusion models. Its training process preserves the base model's knowledge by freezing the original weights and training only the control module. To address training challenges, we introduce a novel self-tracking technique that greatly simplifies the learning of box-object correlations. Empirically, Boximator achieves state-of-the-art video quality (FVD) scores, improving on two base models, and further enhanced after incorporating box constraints. Its robust motion controllability is validated by drastic increases in the bounding
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20303;&#23429;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23433;&#20840;&#32858;&#21512;&#31639;&#27861;&#21644;&#22810;&#26041;&#35745;&#31639;&#23494;&#30721;&#23398;&#25216;&#26415;&#26469;&#20943;&#36731;&#26799;&#24230;&#27844;&#28431;&#30340;&#39118;&#38505;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#21644;&#23545;&#26032;&#20852;&#25915;&#20987;&#25216;&#26415;&#30340;&#23481;&#26131;&#21463;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01546</link><description>&lt;p&gt;
&#38754;&#21521;&#20303;&#23429;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Distributed Learning for Residential Short-Term Load Forecasting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01546
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20303;&#23429;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23433;&#20840;&#32858;&#21512;&#31639;&#27861;&#21644;&#22810;&#26041;&#35745;&#31639;&#23494;&#30721;&#23398;&#25216;&#26415;&#26469;&#20943;&#36731;&#26799;&#24230;&#27844;&#28431;&#30340;&#39118;&#38505;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#21644;&#23545;&#26032;&#20852;&#25915;&#20987;&#25216;&#26415;&#30340;&#23481;&#26131;&#21463;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21147;&#31995;&#32479;&#39046;&#22495;&#65292;&#20303;&#23429;&#29992;&#25143;&#22312;&#36127;&#33655;&#39044;&#27979;&#24212;&#29992;&#20013;&#30340;&#26085;&#30410;&#22686;&#21152;&#20351;&#24471;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#26085;&#36235;&#20851;&#27880;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36127;&#33655;&#25968;&#25454;&#21487;&#33021;&#24847;&#22806;&#22320;&#27844;&#38706;&#20303;&#23429;&#29992;&#25143;&#30340;&#26085;&#24120;&#29983;&#27963;&#20064;&#24815;&#65292;&#20174;&#32780;&#23545;&#20854;&#36130;&#20135;&#23433;&#20840;&#26500;&#25104;&#39118;&#38505;&#12290;&#34429;&#28982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#34987;&#29992;&#26469;&#36890;&#36807;&#22312;&#19981;&#20132;&#25442;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#20294;&#36825;&#20123;FL&#27169;&#22411;&#23545;&#26032;&#20852;&#30340;&#25915;&#20987;&#25216;&#26415;&#65288;&#22914;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#21644;&#27602;&#21270;&#25915;&#20987;&#65289;&#26174;&#31034;&#20986;&#20102;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#25269;&#24481;&#36825;&#20123;&#25915;&#20987;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#20102;&#19968;&#31181;&#23433;&#20840;&#32858;&#21512;&#65288;SecAgg&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#22810;&#26041;&#35745;&#31639;&#23494;&#30721;&#23398;&#25216;&#26415;&#26469;&#20943;&#36731;&#26799;&#24230;&#27844;&#28431;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;SecAgg&#30340;&#24341;&#20837;&#38656;&#35201;&#37096;&#32626;&#39069;&#22806;&#30340;&#23376;&#20013;&#24515;&#26381;&#21153;&#22120;&#26469;&#25191;&#34892;&#22810;&#26041;&#35745;&#31639;&#21327;&#35758;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#38477;&#20302;&#20102;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of power systems, the increasing involvement of residential users in load forecasting applications has heightened concerns about data privacy. Specifically, the load data can inadvertently reveal the daily routines of residential users, thereby posing a risk to their property security. While federated learning (FL) has been employed to safeguard user privacy by enabling model training without the exchange of raw data, these FL models have shown vulnerabilities to emerging attack techniques, such as Deep Leakage from Gradients and poisoning attacks. To counteract these, we initially employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty computation cryptographic techniques to mitigate the risk of gradient leakage. However, the introduction of SecAgg necessitates the deployment of additional sub-center servers for executing the multiparty computation protocol, thereby escalating computational complexity and reducing system robustness, especially in scenario
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#21019;&#24314;&#36890;&#36807;&#32508;&#21512;RGB&#12289;&#28909;&#20687;&#21644;&#28145;&#24230;&#19977;&#20010;&#27169;&#24577;&#30340;&#20154;&#31867;&#34892;&#20026;&#20998;&#26512;&#25968;&#25454;&#38598;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;HBA&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#25972;&#21512;&#22810;&#27169;&#24577;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01537</link><description>&lt;p&gt;
&#32553;&#23567;&#20154;&#31867;&#34892;&#20026;&#20998;&#26512;&#30340;&#24046;&#36317;&#65306;&#19968;&#31181;&#32508;&#21512;&#19977;&#27169;&#24577;&#25968;&#25454;&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing Trimodal Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#21019;&#24314;&#36890;&#36807;&#32508;&#21512;RGB&#12289;&#28909;&#20687;&#21644;&#28145;&#24230;&#19977;&#20010;&#27169;&#24577;&#30340;&#20154;&#31867;&#34892;&#20026;&#20998;&#26512;&#25968;&#25454;&#38598;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;HBA&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#25972;&#21512;&#22810;&#27169;&#24577;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26222;&#36866;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#20154;&#31867;&#34892;&#20026;&#20998;&#26512;&#65288;HBA&#65289;&#39046;&#22495;&#65292;RGB&#19968;&#30452;&#26159;&#39318;&#36873;&#30340;&#27169;&#24577;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#33719;&#21462;&#24182;&#19988;&#20449;&#24687;&#20016;&#23500;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20248;&#28857;&#30456;&#36830;&#30340;&#26159;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#20809;&#29031;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#28431;&#27934;&#30340;&#19968;&#20010;&#21487;&#33021;&#24615;&#26159;&#21033;&#29992;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#20363;&#22914;&#65292;&#28909;&#20687;&#32032;&#33021;&#22815;&#31361;&#20986;&#20154;&#20307;&#24418;&#24577;&#65292;&#32780;&#28145;&#24230;&#27169;&#24577;&#21017;&#22686;&#21152;&#20102;&#37325;&#35201;&#30340;&#32972;&#26223;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#24577;&#24102;&#26469;&#20102;&#30456;&#20851;&#22909;&#22788;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#24456;&#23569;&#19968;&#37096;&#20998;&#29305;&#23450;&#20110;HBA&#30340;&#25968;&#25454;&#38598;&#38598;&#25104;&#20102;&#36825;&#20123;&#27169;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#29992;&#20110;&#21019;&#24314;&#19977;&#27169;&#24577;&#65288;&#21363;RGB&#12289;&#28909;&#20687;&#21644;&#28145;&#24230;&#65289;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#25216;&#26415;&#21033;&#29992;&#20174;RGB&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#20154;&#20307;&#20998;&#21106;&#25513;&#27169;&#65292;&#24182;&#19982;&#33258;&#21160;&#33719;&#21462;&#30340;&#28909;&#20687;&#21644;&#28145;&#24230;&#32972;&#26223;&#32467;&#21512;&#12290;&#26377;&#20102;&#36825;&#20004;&#20010;&#35201;&#32032;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#29616;&#26377;&#30340;RGB&#25968;&#25454;&#21512;&#25104;&#28145;&#24230;&#21644;&#28909;&#20687;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
In pervasive machine learning, especially in Human Behavior Analysis (HBA), RGB has been the primary modality due to its accessibility and richness of information. However, linked with its benefits are challenges, including sensitivity to lighting conditions and privacy concerns. One possibility to overcome these vulnerabilities is to resort to different modalities. For instance, thermal is particularly adept at accentuating human forms, while depth adds crucial contextual layers. Despite their known benefits, only a few HBA-specific datasets that integrate these modalities exist. To address this shortage, our research introduces a novel generative technique for creating trimodal, i.e., RGB, thermal, and depth, human-focused datasets. This technique capitalizes on human segmentation masks derived from RGB images, combined with thermal and depth backgrounds that are sourced automatically. With these two ingredients, we synthesize depth and thermal counterparts from existing RGB data uti
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21019;&#36896;&#24615;&#25903;&#25345;&#24037;&#20855;&#65288;CSTs&#65289;&#20351;&#29992;&#26102;&#65292;&#34429;&#28982;&#21487;&#20197;&#22686;&#21152;&#29992;&#25143;&#20135;&#29983;&#35814;&#32454;&#30340;&#24819;&#27861;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#20250;&#23548;&#33268;&#29992;&#25143;&#25552;&#20986;&#30340;&#24819;&#27861;&#21516;&#36136;&#21270;&#12290;&#23545;&#20110;&#22522;&#20110;LLMs&#30340;CSTs&#30340;&#29992;&#25143;&#12289;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#32773;&#26469;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01536</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20154;&#31867;&#21019;&#36896;&#24615;&#24605;&#32500;&#30340;&#21516;&#36136;&#21270;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Homogenization Effects of Large Language Models on Human Creative Ideation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01536
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21019;&#36896;&#24615;&#25903;&#25345;&#24037;&#20855;&#65288;CSTs&#65289;&#20351;&#29992;&#26102;&#65292;&#34429;&#28982;&#21487;&#20197;&#22686;&#21152;&#29992;&#25143;&#20135;&#29983;&#35814;&#32454;&#30340;&#24819;&#27861;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#20250;&#23548;&#33268;&#29992;&#25143;&#25552;&#20986;&#30340;&#24819;&#27861;&#21516;&#36136;&#21270;&#12290;&#23545;&#20110;&#22522;&#20110;LLMs&#30340;CSTs&#30340;&#29992;&#25143;&#12289;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#32773;&#26469;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29616;&#22312;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#34987;&#20351;&#29992;&#65292;&#21253;&#25324;&#20316;&#20026;&#21019;&#36896;&#24615;&#25903;&#25345;&#24037;&#20855;(CSTs)&#26469;&#24110;&#21161;&#29992;&#25143;&#20135;&#29983;&#26032;&#30340;&#24819;&#27861;&#12290;&#20294;&#26159;LLMs&#30495;&#30340;&#33021;&#22815;&#25903;&#25345;&#29992;&#25143;&#30340;&#21019;&#36896;&#21147;&#21527;&#65311;&#25105;&#20204;&#20551;&#35774;LLMs&#20316;&#20026;CSTs&#30340;&#20351;&#29992;&#21487;&#33021;&#20351;LLMs&#30340;&#29992;&#25143;&#24863;&#21040;&#26356;&#26377;&#21019;&#36896;&#21147;&#65292;&#24182;&#19988;&#25193;&#22823;&#27599;&#20301;&#29992;&#25143;&#25552;&#20986;&#30340;&#24819;&#27861;&#30340;&#33539;&#22260;&#65292;&#20294;&#20063;&#21516;&#36136;&#21270;&#20102;&#19981;&#21516;&#29992;&#25143;&#25152;&#25552;&#20986;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;36&#20301;&#21442;&#19982;&#32773;&#30340;&#27604;&#36739;&#24615;&#29992;&#25143;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#21516;&#36136;&#21270;&#20551;&#35774;&#21457;&#29616;&#65292;&#19982;&#21478;&#19968;&#31181;CST&#30456;&#27604;&#65292;&#19981;&#21516;&#29992;&#25143;&#20542;&#21521;&#20110;&#20351;&#29992;ChatGPT&#25552;&#20986;&#36739;&#23569;&#35821;&#20041;&#19978;&#29420;&#31435;&#30340;&#24819;&#27861;&#12290;&#27492;&#22806;&#65292;ChatGPT&#29992;&#25143;&#20135;&#29983;&#20102;&#26356;&#22810;&#35814;&#32454;&#30340;&#24819;&#27861;&#65292;&#20294;&#23545;&#25152;&#29983;&#25104;&#30340;&#24819;&#27861;&#24863;&#21040;&#36131;&#20219;&#26356;&#23569;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#21457;&#29616;&#23545;&#22522;&#20110;LLMs&#30340;CSTs&#30340;&#29992;&#25143;&#12289;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#32773;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are now being used in a wide variety of contexts, including as creativity support tools (CSTs) intended to help their users come up with new ideas. But do LLMs actually support user creativity? We hypothesized that the use of an LLM as a CST might make the LLM's users feel more creative, and even broaden the range of ideas suggested by each individual user, but also homogenize the ideas suggested by different users. We conducted a 36-participant comparative user study and found, in accordance with the homogenization hypothesis, that different users tended to produce less semantically distinct ideas with ChatGPT than with an alternative CST. Additionally, ChatGPT users generated a greater number of more detailed ideas, but felt less responsible for the ideas they generated. We discuss potential implications of these findings for users, designers, and developers of LLM-based CSTs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#22312;&#25429;&#25417;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#22810;&#26679;&#24615;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25152;&#30740;&#31350;&#30340;&#20851;&#38190;&#28857;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;LLMs&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#23558;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#22810;&#31181;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01535</link><description>&lt;p&gt;
&#35770;&#35770;&#25991;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Empirical Analysis of Diversity in Argument Summarization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#22312;&#25429;&#25417;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#22810;&#26679;&#24615;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25152;&#30740;&#31350;&#30340;&#20851;&#38190;&#28857;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;LLMs&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#23558;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#22810;&#31181;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20250;&#35752;&#35770;&#20013;&#65292;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#35770;&#25454;&#26159;&#20419;&#36827;&#21442;&#19982;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#32570;&#22833;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#25429;&#25417;&#22810;&#26679;&#24615;&#65292;&#36825;&#23545;&#20110;&#21253;&#23481;&#22810;&#20010;&#35266;&#28857;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#21517;&#20026;&#20851;&#38190;&#28857;&#20998;&#26512;&#30340;&#27969;&#34892;&#35770;&#25454;&#25688;&#35201;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#22312;(1)&#20195;&#34920;&#23569;&#25968;&#20154;&#20849;&#20139;&#30340;&#35770;&#28857;&#19978;&#65292;(2)&#22788;&#29702;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#25968;&#25454;&#20197;&#21450;(3)&#19982;&#20154;&#24037;&#25552;&#20379;&#30340;&#20027;&#35266;&#27880;&#37322;&#30456;&#19968;&#33268;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#29992;&#30340;LLM&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#20102;&#36825;&#31181;&#34892;&#20026;&#65292;&#20294;&#20855;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#21487;&#33021;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Presenting high-level arguments is a crucial task for fostering participation in online societal discussions. Current argument summarization approaches miss an important facet of this task -- capturing diversity -- which is important for accommodating multiple perspectives. We introduce three aspects of diversity: those of opinions, annotators, and sources. We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations. We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths. Further, we observe that diversification of training data may ameliorate generalization. Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#20915;&#31574;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"K&#32423;&#25512;&#29702;"&#30340;&#26032;&#39062;&#25512;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35797;&#39564;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#25512;&#29702;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23481;&#26131;&#20986;&#38169;&#65292;&#32780;"K&#32423;&#25512;&#29702;"&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01521</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;K&#32423;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
K-Level Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#20915;&#31574;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"K&#32423;&#25512;&#29702;"&#30340;&#26032;&#39062;&#25512;&#29702;&#26041;&#27861;&#12290;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35797;&#39564;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#25512;&#29702;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23481;&#26131;&#20986;&#38169;&#65292;&#32780;"K&#32423;&#25512;&#29702;"&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#20854;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21160;&#24577;&#12289;&#20132;&#20114;&#21644;&#31454;&#20105;&#22330;&#26223;&#65288;&#22914;&#21830;&#19994;&#25112;&#30053;&#21644;&#32929;&#31080;&#24066;&#22330;&#20998;&#26512;&#65289;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#27491;&#24335;&#25506;&#32034;LLMs&#22312;&#24555;&#36895;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#20915;&#31574;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#35797;&#39564;&#65292;&#20197;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#20013;&#21160;&#24577;&#20915;&#31574;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123;&#25361;&#25112;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#65292;&#21487;&#20197;&#23545;LLMs&#30340;&#21160;&#24577;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#28165;&#26224;&#12289;&#21487;&#25511;&#21644;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25512;&#29702;&#26041;&#27861;&#22312;&#38656;&#35201;k&#32423;&#24605;&#32771;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#23481;&#26131;&#20986;&#38169; - &#36825;&#26159;&#20043;&#21069;&#30740;&#31350;&#20013;&#26410;&#35299;&#20915;&#30340;&#20851;&#38190;&#27010;&#24565;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs&#25512;&#29702;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#8220;K&#32423;&#25512;&#29702;&#8221;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#23545;&#25163;&#30340;&#35270;&#35282;&#65292;&#20174;&#36882;&#24402;&#35282;&#24230;&#36816;&#29992;&#22522;&#20110;k&#32423;&#24605;&#32771;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated their proficiency in complex reasoning tasks, their performance in dynamic, interactive, and competitive scenarios - such as business strategy and stock market analysis - remains underexplored. To bridge this gap, we formally explore the dynamic reasoning capabilities of LLMs for decision-making in rapidly evolving environments. We introduce two game theory-based pilot challenges that mirror the complexities of real-world dynamic decision-making. These challenges are well-defined, enabling clear, controllable, and precise evaluation of LLMs' dynamic reasoning abilities. Through extensive experiments, we find that existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works. To address this, we propose a novel reasoning approach for LLMs, named "K-Level Reasoning". This approach adopts the perspective of rivals to recursively employ k-level thinking based on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#38543;&#26426;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01515</link><description>&lt;p&gt;
&#25552;&#21319;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65306;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#21644;&#29992;&#20110;&#26356;&#24555;&#25910;&#25947;&#30340;&#26032;&#22411;&#21152;&#36895;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Stochastic Gradient Descent: A Unified Framework and Novel Acceleration Methods for Faster Convergence
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#38543;&#26426;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;SGD&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#22312;&#38543;&#26426;&#20248;&#21270;&#20013;&#25913;&#36827;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#20363;&#22914;SGDm&#65292;AdaGrad&#65292;Adam&#31561;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#65292;&#23427;&#20204;&#30340;&#25910;&#25947;&#20998;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;&#20219;&#20309;&#19968;&#38454;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#26356;&#26032;&#26041;&#21521;$g_t$&#35299;&#37322;&#20026;&#38543;&#26426;&#27425;&#26799;&#24230;$\nabla f_t(x_t)$&#21644;&#38468;&#21152;&#30340;&#21152;&#36895;&#39033;$\frac{2|\langle v_t, \nabla f_t(x_t) \rangle|}{\|v_t\|_2^2} v_t$&#30340;&#21644;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;$\langle v_t, \nabla f_t(x_t) \rangle$&#26469;&#35752;&#35770;&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21152;&#36895;&#26041;&#27861;&#65306;\textbf{&#25298;&#32477;&#21152;&#36895;}&#21644;\textbf{&#38543;&#26426;&#21521;&#37327;&#21152;&#36895;}&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on SGD, previous works have proposed many algorithms that have improved convergence speed and generalization in stochastic optimization, such as SGDm, AdaGrad, Adam, etc. However, their convergence analysis under non-convex conditions is challenging. In this work, we propose a unified framework to address this issue. For any first-order methods, we interpret the updated direction $g_t$ as the sum of the stochastic subgradient $\nabla f_t(x_t)$ and an additional acceleration term $\frac{2|\langle v_t, \nabla f_t(x_t) \rangle|}{\|v_t\|_2^2} v_t$, thus we can discuss the convergence by analyzing $\langle v_t, \nabla f_t(x_t) \rangle$. Through our framework, we have discovered two plug-and-play acceleration methods: \textbf{Reject Accelerating} and \textbf{Random Vector Accelerating}, we theoretically demonstrate that these two methods can directly lead to an improvement in convergence rate.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#31038;&#20250;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#27969;&#31243;&#65292;&#23558;&#24184;&#31119;&#24895;&#26223;&#36716;&#21270;&#20026;&#20855;&#20307;&#23454;&#36341;&#65292;&#24182;&#36890;&#36807;&#25345;&#32493;&#30340;&#27979;&#37327;&#21644;&#21453;&#39304;&#24490;&#29615;&#36827;&#34892;&#25903;&#25345;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01499</link><description>&lt;p&gt;
&#24320;&#21457;&#21644;&#35780;&#20272;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developing and Evaluating a Design Method for Positive Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01499
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#31038;&#20250;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#27969;&#31243;&#65292;&#23558;&#24184;&#31119;&#24895;&#26223;&#36716;&#21270;&#20026;&#20855;&#20307;&#23454;&#36341;&#65292;&#24182;&#36890;&#36807;&#25345;&#32493;&#30340;&#27979;&#37327;&#21644;&#21453;&#39304;&#24490;&#29615;&#36827;&#34892;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#31038;&#20250;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21508;&#20010;&#26041;&#38754;&#26085;&#30410;&#26222;&#21450;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#8220;AI for good&#8221;&#22312;&#19982;&#22797;&#26434;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#25105;&#20204;&#32570;&#20047;&#25104;&#29087;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#21644;&#35780;&#20272;&#20102;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;&#65292;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#27969;&#31243;&#65292;&#23558;&#24184;&#31119;&#24895;&#26223;&#36716;&#21270;&#20026;&#20855;&#20307;&#23454;&#36341;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#35813;&#26041;&#27861;&#30340;&#22235;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#24773;&#22659;&#21270;&#12289;&#25805;&#20316;&#21270;&#12289;&#20248;&#21270;&#21270;&#21644;&#23454;&#29616;&#31119;&#31049;&#65292;&#21516;&#26102;&#25903;&#25345;&#25345;&#32493;&#27979;&#37327;&#21644;&#21453;&#39304;&#24490;&#29615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20854;&#20013;&#21021;&#23398;&#32773;&#35774;&#35745;&#24072;&#24212;&#29992;&#20102;&#35813;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#19982;&#25928;&#21147;&#21644;&#21487;&#29992;&#24615;&#30456;&#20851;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#25509;&#19979;&#26469;&#65292;&#19968;&#39033;&#19987;&#23478;&#35780;&#20272;&#30740;&#31350;&#35780;&#20272;&#20102;&#25152;&#24471;&#27010;&#24565;&#30340;&#36136;&#37327;&#65292;&#23558;&#20854;&#35780;&#20026;&#20013;&#31561;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) continues advancing, ensuring positive societal impacts becomes critical, especially as AI systems become increasingly ubiquitous in various aspects of life. However, developing "AI for good" poses substantial challenges around aligning systems with complex human values. Presently, we lack mature methods for addressing these challenges. This article presents and evaluates the Positive AI design method aimed at addressing this gap. The method provides a human-centered process to translate wellbeing aspirations into concrete practices. First, we explain the method's four key steps: contextualizing, operationalizing, optimizing, and implementing wellbeing supported by continuous measurement for feedback cycles. We then present a multiple case study where novice designers applied the method, revealing strengths and weaknesses related to efficacy and usability. Next, an expert evaluation study assessed the quality of the resulting concepts, rating them modera
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01481</link><description>&lt;p&gt;
&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Multi-level protein pre-training with Vabs-Net
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#19977;&#32500;&#32467;&#26500;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#21363;&#945;&#30899;&#21407;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21407;&#23376;&#65292;&#22914;&#20391;&#38142;&#21407;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#19978;&#23545;&#34507;&#30333;&#36136;&#36827;&#34892;&#24314;&#27169;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20391;&#38142;&#21407;&#23376;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20998;&#23376;&#23545;&#25509;&#65289;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#20013;&#22825;&#30495;&#22320;&#32452;&#21512;&#27531;&#22522;&#21644;&#21407;&#23376;&#20449;&#24687;&#36890;&#24120;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20449;&#24687;&#27844;&#28431;&#26159;&#21253;&#21547;&#21407;&#23376;&#32467;&#26500;&#30340;&#36755;&#20837;&#23548;&#33268;&#27531;&#22522;&#32423;&#39044;&#35757;&#32451;&#20219;&#21153;&#21464;&#24471;&#29712;&#30862;&#24182;&#23548;&#33268;&#27531;&#22522;&#34920;&#31034;&#19981;&#22815;&#20805;&#20998;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25513;&#30721;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#12290;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01476</link><description>&lt;p&gt;
&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#12290;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20063;&#21487;&#33021;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#24182;&#38656;&#35201;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26469;&#35299;&#20915;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#23558;&#23545;&#31216;&#26680;&#24212;&#29992;&#20110;&#21464;&#20998;&#25512;&#26029;&#19979;&#30340;&#27880;&#24847;&#21147;&#26680;&#65307;&#28982;&#32780;&#65292;&#24573;&#30053;&#20102;&#27880;&#24847;&#21147;&#26680;&#26412;&#36136;&#19978;&#26159;&#19981;&#23545;&#31216;&#30340;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25512;&#23548;&#20986;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;GP&#21518;&#39564;&#30340;&#22797;&#26434;&#24230;&#20173;&#28982;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#30340;&#26680;-&#29305;&#24449;&#23545;&#31232;&#30095;&#21464; &#20998;&#39640;&#26031;&#36807;&#31243;&#65288;KEP-SVGP&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#26680;SVD&#65288;KSVD&#65289;&#35299;&#20915;&#20102;&#27880;&#24847;&#21147;&#26680;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#38477;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;KEP-SVGP&#65292;i&#65289;&#30001;&#20110;&#19982;&#27880;&#24847;&#21147;&#26680;&#30340;KSVD&#30456;&#23545;&#24212;&#30340;&#20004;&#32452;&#22855;&#24322;&#21521;&#37327;&#24341;&#23548;&#30340;SVGP&#23545;&#23436;&#20840;&#34920;&#24449;&#20102;&#19981;&#23545;&#31216;&#24615;&#65307;ii&#65289;&#20165;&#20351;&#29992;&#23569;&#37327;&#19982;KSVD&#30456;&#23545;&#24212;&#30340;&#20276;&#38543;&#29305;&#24449;&#20989;&#25968;&#65292;&#25512;&#23548;SVGP&#21518;&#39564;&#27010;&#29575;&#23494;&#24230;&#21487;&#20197;&#23454;&#29616;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs). Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric. Moreover, the complexity of deriving the GP posteriors remains high for large-scale data. In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#36825;&#19968;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01467</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20869;&#20986;&#29616;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#36825;&#19968;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#21306;&#22495;&#20013;&#26222;&#36941;&#35266;&#23519;&#21040;&#30340;&#22238;&#25918;&#29616;&#35937;&#26159;&#21542;&#33021;&#22815;&#22312;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#20013;&#33258;&#28982;&#20135;&#29983;&#65311;&#22914;&#26524;&#26159;&#30340;&#35805;&#65292;&#23427;&#26159;&#21542;&#23545;&#20219;&#21153;&#26377;&#25152;&#36129;&#29486;&#65311;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#20219;&#21153;&#20248;&#21270;&#30340;&#33539;&#24335;&#19979;&#21457;&#29616;&#20102;&#22238;&#25918;&#30340;&#33258;&#28982;&#20986;&#29616;&#65292;&#27169;&#22411;&#27169;&#25311;&#20102;&#28023;&#39532;&#20307;&#21644;&#21069;&#39069;&#21494;&#30382;&#23618;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#27807;&#36890;&#21644;&#24863;&#35273;&#30382;&#23618;&#30340;&#36755;&#20837;&#12290;&#28023;&#39532;&#20307;&#20013;&#30340;&#22238;&#25918;&#26159;&#30001;&#20110;&#24773;&#26223;&#35760;&#24518;&#12289;&#35748;&#30693;&#22320;&#22270;&#20197;&#21450;&#29615;&#22659;&#35266;&#23519;&#32780;&#20135;&#29983;&#30340;&#65292;&#19982;&#21160;&#29289;&#23454;&#39564;&#25968;&#25454;&#30456;&#20284;&#65292;&#24182;&#19988;&#26159;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#25928;&#25351;&#26631;&#12290;&#35813;&#27169;&#22411;&#36824;&#25104;&#21151;&#22320;&#37325;&#29616;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30340;&#22238;&#25918;&#65292;&#19982;&#20154;&#31867;&#23454;&#39564;&#25968;&#25454;&#30456;&#31526;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can replay, as a widely observed neural activity pattern in brain regions, particularly in the hippocampus and neocortex, emerge in an artificial agent? If yes, does it contribute to the tasks? In this work, without heavy dependence on complex assumptions, we discover naturally emergent replay under task-optimized paradigm using a recurrent neural network-based reinforcement learning model, which mimics the hippocampus and prefrontal cortex, as well as their intercommunication and the sensory cortex input. The emergent replay in the hippocampus, which results from the episodic memory and cognitive map as well as environment observations, well resembles animal experimental data and serves as an effective indicator of high task performance. The model also successfully reproduces local and nonlocal replay, which matches the human experimental data. Our work provides a new avenue for understanding the mechanisms behind replay.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01454</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#19968;&#31181;&#32479;&#35745;&#22240;&#26524;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#65288;SCD&#65289;&#20013;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#20316;&#20026;&#32422;&#26463;&#23884;&#20837;&#21040;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#25509;&#21463;&#65292;&#22240;&#20026;&#36825;&#23545;&#20110;&#21019;&#24314;&#19968;&#33268;&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#27169;&#22411;&#26159;&#37325;&#35201;&#30340;&#65292;&#23613;&#31649;&#35782;&#21035;&#32972;&#26223;&#30693;&#35782;&#30340;&#25361;&#25112;&#34987;&#35748;&#21487;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;LLM&#30340;&#8220;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#65288;SCP&#65289;&#8221;&#19982;SCD&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#22240;&#26524;&#25512;&#26029;&#65288;KBCI&#65289;&#30456;&#32467;&#21512;&#65292;&#23545;SCD&#36827;&#34892;&#20808;&#39564;&#30693;&#35782;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4&#21487;&#20197;&#20351;LLM-KBCI&#30340;&#36755;&#20986;&#19982;&#24102;&#26377;LLM-KBCI&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;SCD&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#65292;&#22914;&#26524;GPT-4&#32463;&#21382;&#20102;SCP&#65292;&#37027;&#20040;SCD&#30340;&#32467;&#26524;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;LLM&#19981;&#21547;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;LLM&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20854;&#32972;&#26223;&#30693;&#35782;&#26469;&#25913;&#36827;SCD&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#23548;&#24341;&#22270;&#20248;&#21270;&#38271;&#26399;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#21160;&#29983;&#25104;&#23548;&#24341;&#30340;&#31639;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#22914;&#20309;&#33258;&#21160;&#29983;&#25104;&#33391;&#22909;&#23548;&#24341;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01446</link><description>&lt;p&gt;
&#38271;&#26399;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#23548;&#24341;&#22270;&#20248;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Guidance Graph Optimization for Lifelong Multi-Agent Path Finding
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#23548;&#24341;&#22270;&#20248;&#21270;&#38271;&#26399;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#21160;&#29983;&#25104;&#23548;&#24341;&#30340;&#31639;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#22914;&#20309;&#33258;&#21160;&#29983;&#25104;&#33391;&#22909;&#23548;&#24341;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#23548;&#24341;&#26469;&#25552;&#39640;&#38271;&#26399;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#30340;&#21534;&#21520;&#37327;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#23558;&#23548;&#24341;&#65288;&#22914;&#39640;&#36895;&#20844;&#36335;&#65289;&#32435;&#20837;MAPF&#31639;&#27861;&#21487;&#20197;&#21152;&#36895;&#35745;&#31639;&#65292;&#20294;&#36825;&#24448;&#24448;&#20250;&#19982;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#20135;&#29983;&#25240;&#20013;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#33258;&#21160;&#29983;&#25104;&#33391;&#22909;&#30340;&#23548;&#24341;&#20173;&#28982;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36824;&#26080;&#27861;&#36229;&#36234;&#25163;&#21160;&#35774;&#35745;&#30340;&#23548;&#24341;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26377;&#21521;&#23548;&#24341;&#22270;&#20316;&#20026;&#38271;&#26399;MAPF&#20013;&#24341;&#23548;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#24182;&#23558;&#23548;&#24341;&#22270;&#20248;&#21270;&#65288;GGO&#65289;&#30340;&#20219;&#21153;&#23450;&#20041;&#20026;&#20248;&#21270;&#20854;&#36793;&#26435;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;GGO&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#36866;&#29992;&#20110;&#20219;&#24847;&#38271;&#26399;MAPF&#31639;&#27861;&#21644;&#22320;&#22270;&#30340;&#23548;&#24341;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#30452;&#25509;&#20351;&#29992;CMA-ES&#65288;&#19968;&#31181;&#40657;&#31665;&#20248;&#21270;&#31639;&#27861;&#65289;&#26469;&#35299;&#20915;GGO&#38382;&#39064;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;PIU&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#23548;&#24341;&#30340;&#26356;&#26032;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23558;&#20248;&#21270;&#36807;&#30340;&#23548;&#24341;&#22270;&#20256;&#25773;&#21040;&#36739;&#22823;&#22320;&#22270;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to use guidance to improve the throughput of lifelong Multi-Agent Path Finding (MAPF). Previous studies have demonstrated that while incorporating guidance, such as highways, can accelerate MAPF algorithms, this often results in a trade-off with solution quality. In addition, how to generate good guidance automatically remains largely unexplored, with current methods falling short of surpassing manually designed ones. In this work, we introduce the directed guidance graph as a versatile representation of guidance for lifelong MAPF, framing Guidance Graph Optimization (GGO) as the task of optimizing its edge weights. We present two GGO algorithms to automatically generate guidance for arbitrary lifelong MAPF algorithms and maps. The first method directly solves GGO by employing CMA-ES, a black-box optimization algorithm. The second method, PIU, optimizes an update model capable of generating guidance, demonstrating the ability to transfer optimized guidance graphs to larger
&lt;/p&gt;</description></item><item><title>&#21355;&#26143;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29420;&#29305;&#27169;&#24577;&#65292;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#29616;&#26377;&#30340;&#23454;&#36341;&#24182;&#21457;&#36215;&#19968;&#20010;&#20197;&#21355;&#26143;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#25361;&#25112;&#20026;&#20013;&#24515;&#30340;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20197;&#25512;&#21160;SatML&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01444</link><description>&lt;p&gt;
&#20219;&#21153;&#20851;&#38190; -- &#21355;&#26143;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29420;&#29305;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Mission Critical -- Satellite Data is a Distinct Modality in Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01444
&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29420;&#29305;&#27169;&#24577;&#65292;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#29616;&#26377;&#30340;&#23454;&#36341;&#24182;&#21457;&#36215;&#19968;&#20010;&#20197;&#21355;&#26143;&#25968;&#25454;&#30340;&#29305;&#24449;&#21644;&#25361;&#25112;&#20026;&#20013;&#24515;&#30340;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20197;&#25512;&#21160;SatML&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#25968;&#25454;&#26377;&#28508;&#21147;&#20026;&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#19968;&#27425;&#37325;&#22823;&#30340;&#25913;&#21464;&#65292;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#38024;&#23545;&#20256;&#32479;&#25968;&#25454;&#27169;&#24577;&#35774;&#35745;&#30340;&#29616;&#26377;&#23454;&#36341;&#12290;&#38543;&#30528;&#21355;&#26143;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;SatML&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#39046;&#22495;&#27491;&#22788;&#20110;&#19968;&#20010;&#21313;&#23383;&#36335;&#21475;&#12290;&#25105;&#20204;&#21487;&#20197;&#32487;&#32493;&#24212;&#29992;&#19981;&#36866;&#21512;&#30340;&#26041;&#27861;&#65292;&#25110;&#32773;&#25105;&#20204;&#21487;&#20197;&#21457;&#36215;&#19968;&#20010;&#20197;&#21355;&#26143;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#21644;&#25361;&#25112;&#20026;&#20013;&#24515;&#30340;&#26032;&#30740;&#31350;&#35758;&#31243;&#12290;&#26412;&#25991;&#35748;&#20026;&#21355;&#26143;&#25968;&#25454;&#26500;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#31181;&#29420;&#29305;&#27169;&#24577;&#65292;&#32780;&#25105;&#20204;&#24517;&#39035;&#25215;&#35748;&#36825;&#19968;&#28857;&#65292;&#20197;&#25512;&#21160;SatML&#22312;&#29702;&#35770;&#12289;&#26041;&#27861;&#21644;&#37096;&#32626;&#26041;&#38754;&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20851;&#38190;&#24615;&#30340;&#35752;&#35770;&#38382;&#39064;&#21644;&#21487;&#34892;&#24615;&#24314;&#35758;&#65292;&#23558;SatML&#20174;&#20165;&#20165;&#19968;&#20010;&#26377;&#36259;&#30340;&#24212;&#29992;&#39046;&#22495;&#36716;&#21464;&#20026;&#19968;&#20010;&#33268;&#21147;&#20110;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#21644;&#31038;&#20250;&#37325;&#22823;&#25361;&#25112;&#30340;&#19987;&#38376;&#30740;&#31350;&#23398;&#31185;&#12290;
&lt;/p&gt;
&lt;p&gt;
Satellite data has the potential to inspire a seismic shift for machine learning -- one in which we rethink existing practices designed for traditional data modalities. As machine learning for satellite data (SatML) gains traction for its real-world impact, our field is at a crossroads. We can either continue applying ill-suited approaches, or we can initiate a new research agenda that centers around the unique characteristics and challenges of satellite data. This position paper argues that satellite data constitutes a distinct modality for machine learning research and that we must recognize it as such to advance the quality and impact of SatML research across theory, methods, and deployment. We outline critical discussion questions and actionable suggestions to transform SatML from merely an intriguing application area to a dedicated research discipline that helps move the needle on big challenges for machine learning and society.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01440</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#65306;&#20174;&#20803;&#23398;&#20064;&#21040;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#22270;&#20013;&#24515;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26089;&#26399;&#30340;&#25216;&#26415;&#36890;&#24120;&#22312;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#20013;&#36816;&#34892;&#65292;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20805;&#36275;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#20010;&#38480;&#21046;&#24341;&#21457;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#21482;&#26377;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#21487;&#29992;&#12290;&#37492;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#25991;&#29486;&#65292;&#26412;&#32508;&#36848;&#35797;&#22270;&#32508;&#21512;&#26368;&#36817;&#30340;&#21457;&#23637;&#65292;&#25552;&#20379;&#27604;&#36739;&#24615;&#30340;&#35265;&#35299;&#65292;&#24182;&#30830;&#23450;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#31995;&#32479;&#22320;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20803;&#23398;&#20064;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;&#35835;&#32773;&#36827;&#34892;&#26041;&#27861;&#36873;&#25321;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to cata
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#20869;&#23481;&#28085;&#30422;&#20102;&#20998;&#23376;&#20449;&#24687;&#36755;&#20837;LLMs&#30340;&#34920;&#31034;&#21644;&#26631;&#35760;&#21270;&#26041;&#27861;&#12289;&#21270;&#23398;LLMs&#30340;&#19981;&#21516;&#32452;&#32676;&#21450;&#20854;&#25972;&#21512;&#26041;&#27861;&#12289;&#36866;&#29992;&#20110;&#21270;&#23398;LLMs&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#31561;&#12290;&#27492;&#22806;&#65292;&#36824;&#25506;&#35752;&#20102;LLMs&#22312;&#21270;&#23398;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01439</link><description>&lt;p&gt;
&#20174;&#35789;&#35821;&#21040;&#20998;&#23376;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Words to Molecules: A Survey of Large Language Models in Chemistry
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#20869;&#23481;&#28085;&#30422;&#20102;&#20998;&#23376;&#20449;&#24687;&#36755;&#20837;LLMs&#30340;&#34920;&#31034;&#21644;&#26631;&#35760;&#21270;&#26041;&#27861;&#12289;&#21270;&#23398;LLMs&#30340;&#19981;&#21516;&#32452;&#32676;&#21450;&#20854;&#25972;&#21512;&#26041;&#27861;&#12289;&#36866;&#29992;&#20110;&#21270;&#23398;LLMs&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#31561;&#12290;&#27492;&#22806;&#65292;&#36824;&#25506;&#35752;&#20102;LLMs&#22312;&#21270;&#23398;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#21508;&#31181;&#36328;&#23398;&#31185;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#21270;&#23398;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#23545;&#23558;LLMs&#25972;&#21512;&#21040;&#21270;&#23398;&#39046;&#22495;&#20013;&#25152;&#37319;&#29992;&#30340;&#24494;&#22937;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25506;&#32034;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20010;&#36328;&#23398;&#31185;&#20132;&#27719;&#28857;&#30340;&#22797;&#26434;&#24615;&#21644;&#21019;&#26032;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#20174;&#36890;&#36807;&#21508;&#31181;&#34920;&#31034;&#21644;&#26631;&#35760;&#21270;&#26041;&#27861;&#23558;&#20998;&#23376;&#20449;&#24687;&#36755;&#20837;LLMs&#24320;&#22987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#36755;&#20837;&#25968;&#25454;&#30340;&#39046;&#22495;&#21644;&#27169;&#24577;&#23558;&#21270;&#23398;LLMs&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#32452;&#65292;&#24182;&#35752;&#35770;&#23558;&#36825;&#20123;&#36755;&#20837;&#19982;LLMs&#25972;&#21512;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#36866;&#29992;&#20110;&#21270;&#23398;LLMs&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#27492;&#20043;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#22312;&#21270;&#23398;&#20013;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26032;&#33539;&#24335;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Large Language Models (LLMs) have achieved significant success in natural language processing (NLP) and various interdisciplinary areas. However, applying LLMs to chemistry is a complex task that requires specialized domain knowledge. This paper provides a thorough exploration of the nuanced methodologies employed in integrating LLMs into the field of chemistry, delving into the complexities and innovations at this interdisciplinary juncture. Specifically, our analysis begins with examining how molecular information is fed into LLMs through various representation and tokenization methods. We then categorize chemical LLMs into three distinct groups based on the domain and modality of their input data, and discuss approaches for integrating these inputs for LLMs. Furthermore, this paper delves into the pretraining objectives with adaptations to chemical LLMs. After that, we explore the diverse applications of LLMs in chemistry, including novel paradigms for their applica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#24207;&#21015;&#32553;&#30701;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#65292;&#22312;&#23545;&#27604;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#26032;&#30340;&#28508;&#22312;&#20998;&#32452;&#21644;&#28508;&#22312;&#36873;&#25321;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01416</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#24207;&#21015;&#32553;&#30701;
&lt;/p&gt;
&lt;p&gt;
Sequence Shortening for Context-Aware Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30340;&#24207;&#21015;&#32553;&#30701;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#65292;&#22312;&#23545;&#27604;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#26032;&#30340;&#28508;&#22312;&#20998;&#32452;&#21644;&#28508;&#22312;&#36873;&#25321;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#26088;&#22312;&#36890;&#36807;&#23558;&#21608;&#22260;&#30340;&#21477;&#23376;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#25913;&#36827;&#21477;&#23376;&#30340;&#32763;&#35793;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24050;&#32463;&#24212;&#29992;&#20102;&#20004;&#31181;&#20027;&#35201;&#26550;&#26500;&#65292;&#21363;&#22522;&#20110;&#20018;&#32852;&#30340;&#21333;&#32534;&#30721;&#22120;&#21644;&#22810;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#65292;&#22312;&#19979;&#19968;&#27493;&#20013;&#37325;&#29992;&#28304;&#21477;&#23376;&#30340;&#28508;&#22312;&#34920;&#31034;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#22312;&#23545;&#27604;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65288;&#27169;&#22411;&#24517;&#39035;&#23545;&#25552;&#20379;&#30340;&#21477;&#23376;&#20013;&#30340;&#27491;&#30830;&#32763;&#35793;&#36827;&#34892;&#25490;&#24207;&#65289;&#65292;&#24182;&#19988;&#19982;&#21333;&#32534;&#30721;&#22120;&#21644;&#22810;&#32534;&#30721;&#22120;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;BLEU&#21644;COMET&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#32531;&#23384;&#34920;&#31034;&#24212;&#29992;&#20110;&#24207;&#21015;&#32553;&#30701;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19977;&#31181;&#22522;&#20110;&#27719;&#32858;&#30340;&#32553;&#30701;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#28508;&#22312;&#20998;&#32452;&#21644;&#28508;&#22312;&#36873;&#25321;&#65292;&#20854;&#20013;&#32593;&#32476;&#23398;&#20064;&#23558;&#20196;&#29260;&#20998;&#32452;&#25110;&#36873;&#25321;&#35201;&#32531;&#23384;&#20026;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32531;&#23384;&#34920;&#31034;&#30340;&#24207;&#21015;&#32553;&#30701;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that th
&lt;/p&gt;</description></item><item><title>SMLP&#26159;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#26679;&#26412;&#30340;&#31995;&#32479;&#25506;&#32034;&#24037;&#20855;&#65292;&#36890;&#36807;&#37319;&#29992;&#32479;&#35745;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28784;&#30418;&#26041;&#27861;&#65292;&#21487;&#20197;&#25506;&#32034;&#31995;&#32479;&#24182;&#20248;&#21270;&#30828;&#20214;&#35774;&#35745;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01415</link><description>&lt;p&gt;
SMLP: &#31526;&#21495;&#26426;&#22120;&#23398;&#20064;&#35777;&#26126;&#22120;
&lt;/p&gt;
&lt;p&gt;
SMLP: Symbolic Machine Learning Prover
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01415
&lt;/p&gt;
&lt;p&gt;
SMLP&#26159;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#26679;&#26412;&#30340;&#31995;&#32479;&#25506;&#32034;&#24037;&#20855;&#65292;&#36890;&#36807;&#37319;&#29992;&#32479;&#35745;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28784;&#30418;&#26041;&#27861;&#65292;&#21487;&#20197;&#25506;&#32034;&#31995;&#32479;&#24182;&#20248;&#21270;&#30828;&#20214;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#26426;&#22120;&#23398;&#20064;&#35777;&#26126;&#22120;&#65288;SMLP&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#36807;&#27169;&#25311;&#25110;&#25191;&#34892;&#31995;&#32479;&#24471;&#21040;&#30340;&#19968;&#31995;&#21015;&#36755;&#20837;&#21521;&#37327;&#26679;&#26412;&#30340;&#31995;&#32479;&#25506;&#32034;&#24037;&#20855;&#21644;&#24211;&#12290;SMLP&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#28784;&#30418;&#26041;&#27861;&#65292;&#21363;&#23558;&#25968;&#25454;&#25506;&#32034;&#30340;&#32479;&#35745;&#26041;&#27861;&#19982;&#26500;&#24314;&#21644;&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#27010;&#29575;&#21644;&#24418;&#24335;&#26041;&#27861;&#26469;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#65292;&#20174;&#32780;&#25506;&#32034;&#31995;&#32479;&#12290;SMLP&#24050;&#32463;&#24212;&#29992;&#20110;&#33521;&#29305;&#23572;&#30340;&#24037;&#19994;&#29615;&#22659;&#20013;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#20248;&#21270;&#27169;&#25311;&#30005;&#36335;&#32423;&#21035;&#30340;&#30828;&#20214;&#35774;&#35745;&#12290;SMLP&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#37319;&#26679;&#21644;&#24314;&#27169;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic Machine Learning Prover (SMLP) is a tool and a library for system exploration based on data samples obtained by simulating or executing the system on a number of input vectors. SMLP aims at exploring the system based on this data by taking a grey-box approach: SMLP combines statistical methods of data exploration with building and exploring machine learning models in close feedback loop with the system's response, and exploring these models by combining probabilistic and formal methods. SMLP has been applied in industrial setting at Intel for analyzing and optimizing hardware designs at the analog level. SMLP is a general purpose tool and can be applied to systems that can be sampled and modeled by machine learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40657;&#32032;&#30244;&#35786;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;-&#37096;&#20998;&#27169;&#22411;&#20197;&#21450;&#32467;&#21512;&#22522;&#20110;&#38750;&#19987;&#23478;&#21453;&#39304;&#30340;&#24341;&#23548;&#24615;&#30417;&#30563;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01410</link><description>&lt;p&gt;
&#20351;&#29992;&#21407;&#22411;&#21644;&#38750;&#19987;&#23478;&#30417;&#30563;&#30340;XAI&#23545;&#30382;&#32932;&#30284;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40657;&#32032;&#30244;&#35786;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;-&#37096;&#20998;&#27169;&#22411;&#20197;&#21450;&#32467;&#21512;&#22522;&#20110;&#38750;&#19987;&#23478;&#21453;&#39304;&#30340;&#24341;&#23548;&#24615;&#30417;&#30563;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30382;&#32932;&#38236;&#22270;&#20687;&#20998;&#26512;&#36827;&#34892;&#30382;&#32932;&#30284;&#26816;&#27979;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#27169;&#22411;&#24448;&#24448;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#40657;&#30418;&#24615;&#36136;&#24341;&#36215;&#20102;&#21307;&#29983;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;-&#37096;&#20998;&#27169;&#22411;&#36827;&#34892;&#40657;&#32032;&#30244;&#35786;&#26029;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#20004;&#31181;&#19981;&#21516;&#30340;&#20449;&#24687;&#36335;&#24452;&#24341;&#20837;&#20102;&#22522;&#20110;&#38750;&#19987;&#23478;&#21453;&#39304;&#30340;&#24341;&#23548;&#24615;&#30417;&#30563;&#65306;1) &#20351;&#29992;&#20998;&#21106;&#32593;&#32476;&#33258;&#21160;&#33719;&#21462;&#30340;&#20108;&#36827;&#21046;&#25513;&#27169;&#65307;2) &#29992;&#25143;&#20248;&#21270;&#30340;&#21407;&#22411;&#12290;&#36825;&#20004;&#20010;&#19981;&#21516;&#30340;&#20449;&#24687;&#36335;&#24452;&#26088;&#22312;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#19982;&#30382;&#32932;&#30149;&#21464;&#30340;&#30456;&#20851;&#21306;&#22495;&#30456;&#23545;&#24212;&#65292;&#25490;&#38500;&#20854;&#36793;&#30028;&#20043;&#22806;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#19987;&#23478;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#19978;&#37117;&#20248;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin cancer detection through dermoscopy image analysis is a critical task. However, existing models used for this purpose often lack interpretability and reliability, raising the concern of physicians due to their black-box nature. In this paper, we propose a novel approach for the diagnosis of melanoma using an interpretable prototypical-part model. We introduce a guided supervision based on non-expert feedback through the incorporation of: 1) binary masks, obtained automatically using a segmentation network; and 2) user-refined prototypes. These two distinct information pathways aim to ensure that the learned prototypes correspond to relevant areas within the skin lesion, excluding confounding factors beyond its boundaries. Experimental results demonstrate that, even without expert supervision, our approach achieves superior performance and generalization compared to non-interpretable models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01401</link><description>&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#22312;&#35268;&#27169;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01401
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#25454;&#35268;&#23450;&#65292;&#20174;&#35757;&#32451;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36951;&#24536;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#21450;&#26102;&#24536;&#35760;&#24517;&#35201;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36951;&#24536;&#30340;&#22330;&#26223;&#65292;&#21363;&#21482;&#26377;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#65292;&#36951;&#24536;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#31227;&#38500;&#25968;&#25454;&#12290;&#26681;&#25454;&#36825;&#26679;&#23450;&#20041;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#19981;&#22815;&#30340;&#12290;&#22522;&#20110;Lipschitz&#36830;&#32493;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#25200;&#21160;&#30340;&#36755;&#20986;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#26469;&#35825;&#23548;&#36951;&#24536;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24179;&#28369;&#24615;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#36951;&#24536;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#24403;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20005;&#26684;&#30340;&#38646;&#26679;&#26412;&#32422;&#26463;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01399</link><description>&lt;p&gt;
&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Model to explain Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#20363;&#22914;&#23545;&#35821;&#20041;&#30456;&#20851;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#24577;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;&#22312;&#20247;&#22810;SSL&#26041;&#27861;&#20013;&#65292;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;SimCLR&#65292;CLIP&#21644;VicREG&#65289;&#22240;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#22312;&#19979;&#28216;&#24615;&#33021;&#19978;&#25509;&#36817;&#26377;&#30417;&#30563;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32972;&#21518;&#30340;&#26426;&#21046;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31867;&#20855;&#26377;&#37492;&#21035;&#24615;&#30340;&#33258;&#30417;&#30563;&#31639;&#27861;&#65288;&#21253;&#25324;&#23545;&#27604;&#26041;&#27861;&#65289;&#36817;&#20284;&#35825;&#23548;&#20854;&#34920;&#31034;&#20013;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19982;&#20114;&#20449;&#24687;&#21644;&#25237;&#24433;&#22836;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#29983;&#25104;&#24335;&#22320;&#25311;&#21512;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;&#22914;SimVE&#65289;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65288;&#20363;&#22914;FashionMNIST&#65292;CIFAR10&#65292;CelebA&#65289;&#65292;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;VAE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows th
&lt;/p&gt;</description></item><item><title>LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01376</link><description>&lt;p&gt;
LoTR: &#20302;&#24352;&#37327;&#31209;&#26435;&#37325;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
LoTR: Low Tensor Rank Weight Adaptation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01376
&lt;/p&gt;
&lt;p&gt;
LoTR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#20351;&#24471;&#38024;&#23545;&#28145;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#24265;&#20215;&#19988;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24605;&#24819;&#25512;&#24191;&#21644;&#25193;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;Transformer&#26550;&#26500;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;LoRA&#31867;&#26041;&#27861;&#26159;&#22522;&#20110;&#26799;&#24230;&#26356;&#26032;&#30340;&#30697;&#38453;&#20998;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoTR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#26041;&#27861;&#65292;&#23427;&#20197;&#24352;&#37327;&#20998;&#35299;&#30340;&#24418;&#24335;&#34920;&#31034;&#21442;&#25968;&#30340;&#26799;&#24230;&#26356;&#26032;&#12290;&#27599;&#20010;&#23618;&#30340;&#20302;&#31209;&#36866;&#37197;&#22120;&#37117;&#30001;&#19977;&#20010;&#30697;&#38453;&#30340;&#20056;&#31215;&#26500;&#25104;&#65292;&#32780;&#24352;&#37327;&#32467;&#26500;&#26159;&#30001;&#36825;&#20010;&#20056;&#31215;&#30340;&#24038;&#21491;&#20056;&#23376;&#22312;&#23618;&#20043;&#38388;&#20849;&#20139;&#24341;&#36215;&#30340;&#12290;&#36890;&#36807;&#23545;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#23618;&#21516;&#26102;&#21387;&#32553;&#65292;LoTR&#33021;&#22815;&#27604;LoRA&#22312;&#29305;&#21035;&#26159;&#23545;&#20110;&#28145;&#23618;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#26680;&#24515;&#24352;&#37327;&#19981;&#20381;&#36182;&#20110;&#21407;&#22987;&#26435;&#37325;&#32500;&#24230;&#65292;&#21487;&#20197;&#20219;&#24847;&#32553;&#23567;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#24120;&#24265;&#20215;&#21644;&#24555;&#36895;&#30340;&#19979;&#28216;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
&lt;/p&gt;</description></item><item><title>FindingEmo&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#65292;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#22797;&#26434;&#22330;&#26223;&#30340;&#27880;&#37322;&#65292;&#28085;&#30422;&#22810;&#20010;&#20154;&#29289;&#21644;&#31038;&#20132;&#35774;&#32622;&#65292;&#27880;&#37322;&#21253;&#25324;&#24773;&#24863;&#32500;&#24230;&#21644;&#24773;&#24863;&#26631;&#31614;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01355</link><description>&lt;p&gt;
&#22312;&#37326;&#22806;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;Emo&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FindingEmo: An Image Dataset for Emotion Recognition in the Wild
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01355
&lt;/p&gt;
&lt;p&gt;
FindingEmo&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#65292;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#22797;&#26434;&#22330;&#26223;&#30340;&#27880;&#37322;&#65292;&#28085;&#30422;&#22810;&#20010;&#20154;&#29289;&#21644;&#31038;&#20132;&#35774;&#32622;&#65292;&#27880;&#37322;&#21253;&#25324;&#24773;&#24863;&#32500;&#24230;&#21644;&#24773;&#24863;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;FindingEmo&#65292;&#19968;&#20010;&#21253;&#21547;25k&#24352;&#22270;&#29255;&#30340;&#26032;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#23427;&#19987;&#27880;&#20110;&#22797;&#26434;&#22330;&#26223;&#65292;&#21253;&#21547;&#22810;&#20010;&#20154;&#29289;&#22312;&#21508;&#31181;&#33258;&#28982;&#12289;&#31038;&#20132;&#29615;&#22659;&#20013;&#65292;&#22270;&#29255;&#30340;&#27880;&#37322;&#26159;&#25972;&#20307;&#36827;&#34892;&#30340;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#23545;&#20110;&#38754;&#37096;&#25110;&#21333;&#20010;&#20010;&#20307;&#30340;&#20851;&#27880;&#12290;&#27880;&#37322;&#30340;&#32500;&#24230;&#21253;&#25324;&#24773;&#24863;&#20215;&#20540;&#12289;&#21796;&#36215;&#21644;&#24773;&#24863;&#26631;&#31614;&#65292;&#27880;&#37322;&#36890;&#36807;Prolific&#25910;&#38598;&#12290;&#38500;&#20102;&#27880;&#37322;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25351;&#21521;&#21407;&#22987;&#22270;&#29255;&#30340;URL&#21015;&#34920;&#65292;&#20197;&#21450;&#25152;&#26377;&#30456;&#20851;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#39640;&#32423;&#30340;&#38382;&#39064;&#31354;&#38388;&#35268;&#33539;&#32534;&#35793;&#20026;&#36866;&#21512;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;&#30340;&#28385;&#36275;&#24615;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#23884;&#20837;&#38388;&#38553;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01353</link><description>&lt;p&gt;
&#23558;&#34920;&#36798;&#20016;&#23500;&#30340;&#38382;&#39064;&#31354;&#38388;&#35268;&#33539;&#39640;&#25928;&#32534;&#35793;&#20026;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient compilation of expressive problem space specifications to neural network solvers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#39640;&#32423;&#30340;&#38382;&#39064;&#31354;&#38388;&#35268;&#33539;&#32534;&#35793;&#20026;&#36866;&#21512;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;&#30340;&#28385;&#36275;&#24615;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#23884;&#20837;&#38388;&#38553;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#23884;&#20837;&#38388;&#38553;&#12290;&#22312;&#38388;&#38553;&#30340;&#19968;&#20391;&#26159;&#19968;&#20010;&#20851;&#20110;&#32593;&#32476;&#34892;&#20026;&#30340;&#39640;&#32423;&#35268;&#33539;&#65292;&#30001;&#39046;&#22495;&#19987;&#23478;&#26681;&#25454;&#21487;&#35299;&#37322;&#30340;&#38382;&#39064;&#31354;&#38388;&#32534;&#20889;&#12290;&#22312;&#21478;&#19968;&#20391;&#26159;&#19968;&#32452;&#36923;&#36753;&#19978;&#31561;&#20215;&#30340;&#21487;&#28385;&#36275;&#24615;&#26597;&#35810;&#65292;&#20197;&#36866;&#21512;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;&#30340;&#24418;&#24335;&#34920;&#36798;&#22312;&#19981;&#21487;&#29702;&#35299;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#23558;&#21069;&#32773;&#32534;&#35793;&#20026;&#21518;&#32773;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#21644;&#20811;&#26381;&#20102;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;&#32780;&#19981;&#26159;&#26631;&#20934;SMT&#27714;&#35299;&#22120;&#25152;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has described the presence of the embedding gap in neural network verification. On one side of the gap is a high-level specification about the network's behaviour, written by a domain expert in terms of the interpretable problem space. On the other side are a logically-equivalent set of satisfiability queries, expressed in the uninterpretable embedding space in a form suitable for neural network solvers. In this paper we describe an algorithm for compiling the former to the latter. We explore and overcome complications that arise from targeting neural network solvers as opposed to standard SMT solvers.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22270;&#20687;&#25551;&#36848;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#21464;&#21270;&#65292;&#24182;&#21457;&#29616;&#22270;&#20687;&#30340;&#23646;&#24615;&#19982;&#36825;&#20123;&#21464;&#21270;&#30456;&#20851;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25429;&#25417;&#21040;&#36825;&#31181;&#21464;&#21270;&#65292;&#20294;&#20173;&#23384;&#22312;&#20559;&#24046;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01352</link><description>&lt;p&gt;
&#25551;&#36848;&#22270;&#20687;&#30340;&#8220;&#24555;&#24930;&#8221;: &#37327;&#21270;&#21644;&#39044;&#27979;&#35270;&#35273;&#35821;&#35328;&#36807;&#31243;&#20013;&#20154;&#31867;&#20449;&#21495;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01352
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22270;&#20687;&#25551;&#36848;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#21464;&#21270;&#65292;&#24182;&#21457;&#29616;&#22270;&#20687;&#30340;&#23646;&#24615;&#19982;&#36825;&#20123;&#21464;&#21270;&#30456;&#20851;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25429;&#25417;&#21040;&#36825;&#31181;&#21464;&#21270;&#65292;&#20294;&#20173;&#23384;&#22312;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#30340;&#23646;&#24615;&#19982;&#20154;&#20204;&#22312;&#25551;&#36848;&#22270;&#20687;&#26102;&#30340;&#34892;&#20026;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#21464;&#21270;&#65292;&#22914;&#30524;&#21160;&#21644;&#20154;&#20204;&#24320;&#22987;&#25551;&#36848;&#22270;&#20687;&#30340;&#26102;&#26426;&#12290;&#23613;&#31649;&#35270;&#35273;&#35821;&#35328;&#21464;&#24322;&#30340;&#36825;&#20123;&#20449;&#21495;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#22312;&#24403;&#21069;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23427;&#20204;&#20960;&#20046;&#34987;&#24573;&#35270;&#65292;&#36825;&#20419;&#20351;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#12290;&#36890;&#36807;&#20351;&#29992;&#21516;&#26102;&#25910;&#38598;&#30340;&#33655;&#20848;&#22270;&#20687;&#25551;&#36848;&#35821;&#26009;&#24211;&#21644;&#30524;&#21160;&#25968;&#25454;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35270;&#35273;&#35821;&#35328;&#20449;&#21495;&#21464;&#21270;&#30340;&#26412;&#36136;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#37492;&#20110;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#20551;&#35774;&#21464;&#21270;&#37096;&#20998;&#28304;&#20110;&#22270;&#20687;&#30340;&#23646;&#24615;&#65292;&#24182;&#25506;&#32034;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#25152;&#32534;&#30721;&#30340;&#22270;&#20687;&#34920;&#31034;&#33021;&#21542;&#25429;&#25417;&#21040;&#36825;&#31181;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#36825;&#34920;&#26126;&#27169;&#22411;&#32570;&#20047;&#23545;&#20160;&#20040;&#20351;&#21050;&#28608;&#22797;&#26434;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an intricate relation between the properties of an image and how humans behave while describing the image. This behavior shows ample variation, as manifested in human signals such as eye movements and when humans start to describe the image. Despite the value of such signals of visuo-linguistic variation, they are virtually disregarded in the training of current pretrained models, which motivates further investigation. Using a corpus of Dutch image descriptions with concurrently collected eye-tracking data, we explore the nature of the variation in visuo-linguistic signals, and find that they correlate with each other. Given this result, we hypothesize that variation stems partly from the properties of the images, and explore whether image representations encoded by pretrained vision encoders can capture such variation. Our results indicate that pretrained models do so to a weak-to-moderate degree, suggesting that the models lack biases about what makes a stimulus complex for 
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20110;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01349</link><description>&lt;p&gt;
&#36229;&#36234;&#31572;&#26696;&#65306;&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01349
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20110;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#21457;&#20102;&#19968;&#22330;&#33539;&#24335;&#36716;&#21464;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#23545;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#20173;&#28982;&#26159;&#31038;&#21306;&#38754;&#20020;&#30340;&#24517;&#28982;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#23558;&#22810;&#36873;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#20316;&#20026;LLMs&#30340;&#22522;&#20934;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;MCQA&#20316;&#20026;LLMs&#35780;&#20272;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#12290;&#22914;&#26524;LLMs&#30495;&#27491;&#29702;&#35299;&#38382;&#39064;&#30340;&#35821;&#20041;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24212;&#35813;&#22312;&#20174;&#30456;&#21516;&#38382;&#39064;&#27966;&#29983;&#30340;&#21508;&#31181;&#37197;&#32622;&#19978;&#34920;&#29616;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;LLMs&#30340;&#21709;&#24212;&#19968;&#33268;&#24615;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#25105;&#20204;&#23558;&#20043;&#23450;&#20041;&#20026;LLMs&#30340;&#21709;&#24212;&#21487;&#21464;&#24615;&#32508;&#21512;&#24449;&#65288;REVAS&#65289;&#65292;&#36825;&#34920;&#26126;&#30446;&#21069;&#22522;&#20110;MCQA&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;LLMs&#30340;&#30495;&#23454;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#21512;&#36866;&#30340;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of natural language processing (NLP), Large Language Models (LLMs) have precipitated a paradigm shift, markedly enhancing performance in natural language generation tasks. Despite these advancements, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the utilization of Multiple Choice Question Answering (MCQA) as a benchmark for LLMs has gained considerable traction. This study investigates the rationality of MCQA as an evaluation method for LLMs. If LLMs genuinely understand the semantics of questions, their performance should exhibit consistency across the varied configurations derived from the same questions. Contrary to this expectation, our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of LLMs, which underscores the need f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;COgnitive REplay&#65288;CORE&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#25968;&#37327;&#20998;&#37197;&#21644;&#20197;&#36136;&#37327;&#20026;&#37325;&#28857;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#20248;&#21270;&#37325;&#25773;&#32531;&#20914;&#21306;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01348</link><description>&lt;p&gt;
CORE&#65306;&#36890;&#36807;&#35748;&#30693;&#37325;&#25773;&#26469;&#20943;&#36731;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;COgnitive REplay&#65288;CORE&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#25968;&#37327;&#20998;&#37197;&#21644;&#20197;&#36136;&#37327;&#20026;&#37325;&#28857;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#20248;&#21270;&#37325;&#25773;&#32531;&#20914;&#21306;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26174;&#33879;&#20943;&#36731;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#35270;&#35282;&#65292;&#24378;&#35843;&#27169;&#22411;&#20445;&#25345;&#29616;&#26377;&#30693;&#35782;&#24182;&#34701;&#20837;&#26032;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#30340;&#37325;&#25773;&#26041;&#27861;&#21516;&#31561;&#23545;&#24453;&#27599;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#26679;&#26412;&#65292;&#22240;&#27492;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#37325;&#25773;&#32531;&#20914;&#21306;&#30340;&#28508;&#21147;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COgnitive REplay&#65288;CORE&#65289;&#65292;&#23427;&#20174;&#20154;&#31867;&#35748;&#30693;&#22797;&#20064;&#36807;&#31243;&#20013;&#24471;&#21040;&#28789;&#24863;&#12290;CORE&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#33258;&#36866;&#24212;&#25968;&#37327;&#20998;&#37197;&#21644;&#20197;&#36136;&#37327;&#20026;&#37325;&#28857;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#21069;&#32773;&#26681;&#25454;&#27599;&#20010;&#20219;&#21153;&#30340;&#36951;&#24536;&#36895;&#29575;&#33258;&#36866;&#24212;&#22320;&#35843;&#33410;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#20998;&#37197;&#65292;&#32780;&#21518;&#32773;&#20445;&#35777;&#22312;&#32531;&#20914;&#21306;&#20013;&#21253;&#21547;&#26368;&#33021;&#27010;&#25324;&#27599;&#20010;&#20219;&#21153;&#29305;&#24449;&#30340;&#20195;&#34920;&#24615;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#21106;CIFAR10&#19978;&#23454;&#29616;&#20102;37.95%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;6.52%&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#26368;&#24046;&#34920;&#29616;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel perspective to significantly mitigate catastrophic forgetting in continuous learning (CL), which emphasizes models' capacity to preserve existing knowledge and assimilate new information. Current replay-based methods treat every task and data sample equally and thus can not fully exploit the potential of the replay buffer. In response, we propose COgnitive REplay (CORE), which draws inspiration from human cognitive review processes. CORE includes two key strategies: Adaptive Quantity Allocation and Quality-Focused Data Selection. The former adaptively modulates the replay buffer allocation for each task based on its forgetting rate, while the latter guarantees the inclusion of representative data that best encapsulates the characteristics of each task within the buffer. Our approach achieves an average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline method by 6.52%. Additionally, it significantly enhances the accuracy of the poorest-perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BehAVE&#30340;&#35270;&#39057;&#29702;&#35299;&#26694;&#26550;&#65292;&#20511;&#21161;&#29616;&#26377;&#30340;&#21830;&#19994;&#35270;&#39057;&#28216;&#25103;&#23454;&#29616;&#39046;&#22495;&#38543;&#26426;&#21270;&#65292;&#26080;&#38656;&#20223;&#30495;&#22120;&#30340;&#25903;&#25345;&#12290;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#20013;&#20016;&#23500;&#30340;&#35270;&#35273;&#22810;&#26679;&#24615;&#36827;&#34892;&#38543;&#26426;&#21270;&#65292;&#20197;&#21450;&#36890;&#36807;&#29609;&#23478;&#34892;&#20026;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25351;&#23548;&#20855;&#26377;&#30456;&#20284;&#20869;&#23481;&#30340;&#35270;&#39057;&#30340;&#23545;&#40784;&#65292;BehAVE&#22312;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#38754;&#23637;&#29616;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01335</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#39057;&#28216;&#25103;&#23454;&#29616;&#26080;&#20223;&#30495;&#22120;&#35270;&#35273;&#39046;&#22495;&#38543;&#26426;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simulator-Free Visual Domain Randomization via Video Games
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BehAVE&#30340;&#35270;&#39057;&#29702;&#35299;&#26694;&#26550;&#65292;&#20511;&#21161;&#29616;&#26377;&#30340;&#21830;&#19994;&#35270;&#39057;&#28216;&#25103;&#23454;&#29616;&#39046;&#22495;&#38543;&#26426;&#21270;&#65292;&#26080;&#38656;&#20223;&#30495;&#22120;&#30340;&#25903;&#25345;&#12290;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#20013;&#20016;&#23500;&#30340;&#35270;&#35273;&#22810;&#26679;&#24615;&#36827;&#34892;&#38543;&#26426;&#21270;&#65292;&#20197;&#21450;&#36890;&#36807;&#29609;&#23478;&#34892;&#20026;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25351;&#23548;&#20855;&#26377;&#30456;&#20284;&#20869;&#23481;&#30340;&#35270;&#39057;&#30340;&#23545;&#40784;&#65292;BehAVE&#22312;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#38754;&#23637;&#29616;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#38543;&#26426;&#21270;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#29992;&#20110;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#22312;&#35270;&#35273;&#19978;&#25130;&#28982;&#19981;&#21516;&#20294;&#20869;&#23481;&#30456;&#20284;&#30340;&#39046;&#22495;&#20013;&#30340;&#20256;&#36882;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#37327;&#20381;&#36182;&#20110;&#35843;&#25972;&#22797;&#26434;&#21644;&#19987;&#38376;&#30340;&#20223;&#30495;&#24341;&#25806;&#65292;&#36825;&#20123;&#24341;&#25806;&#30340;&#26500;&#24314;&#24456;&#22256;&#38590;&#65292;&#36827;&#32780;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BehAVE&#65292;&#19968;&#31181;&#35270;&#39057;&#29702;&#35299;&#26694;&#26550;&#65292;&#23427;&#29420;&#29305;&#22320;&#21033;&#29992;&#29616;&#26377;&#30340;&#21830;&#19994;&#35270;&#39057;&#28216;&#25103;&#26469;&#23454;&#29616;&#39046;&#22495;&#38543;&#26426;&#21270;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#23427;&#20204;&#30340;&#20223;&#30495;&#24341;&#25806;&#12290;&#22312;BehAVE&#19979;&#65292;(1) &#35270;&#39057;&#28216;&#25103;&#22266;&#26377;&#30340;&#20016;&#23500;&#35270;&#35273;&#22810;&#26679;&#24615;&#25104;&#20026;&#38543;&#26426;&#21270;&#30340;&#26469;&#28304;&#65292;(2) &#29609;&#23478;&#34892;&#20026; - &#36890;&#36807;&#21160;&#20316;&#30340;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#35821;&#20041;&#34920;&#31034; - &#24341;&#23548;&#20855;&#26377;&#30456;&#20284;&#20869;&#23481;&#30340;&#35270;&#39057;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35270;&#39057;&#21644;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#19978;&#27979;&#35797;&#20102;BehAVE&#65292;&#24182;&#25253;&#21578;&#20102;&#23427;&#22312;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain randomization is an effective computer vision technique for improving transferability of vision models across visually distinct domains exhibiting similar content. Existing approaches, however, rely extensively on tweaking complex and specialized simulation engines that are difficult to construct, subsequently affecting their feasibility and scalability. This paper introduces BehAVE, a video understanding framework that uniquely leverages the plethora of existing commercial video games for domain randomization, without requiring access to their simulation engines. Under BehAVE (1) the inherent rich visual diversity of video games acts as the source of randomization and (2) player behavior -- represented semantically via textual descriptions of actions -- guides the *alignment* of videos with similar content. We test BehAVE on 25 games of the first-person shooter (FPS) genre across various video and text foundation models and we report its robustness for domain randomization. Beh
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01327</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#20013;&#30340;&#30417;&#30563;&#31639;&#27861;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Supervised Algorithmic Fairness in Distribution Shifts: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#38754;&#23545;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#65292;&#22914;&#20309;&#20445;&#25345;&#20844;&#24179;&#21644;&#26080;&#20559;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#22240;&#21508;&#31181;&#22240;&#32032;&#32780;&#38543;&#26102;&#38388;&#21457;&#29983;&#21464;&#21270;&#12290;&#36825;&#31181;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#65292;&#23545;&#29305;&#23450;&#36890;&#36807;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#26469;&#34920;&#24449;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#22343;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#20102;&#24635;&#32467;&#65292;&#24182;&#20840;&#38754;&#35843;&#26597;&#20102;&#22522;&#20110;&#36825;&#20123;&#21464;&#21270;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#25991;&#29486;&#20013;&#31361;&#20986;&#20102;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20221;&#35843;&#26597;&#21015;&#20986;&#20102;&#29992;&#20110;&#23454;&#35777;&#30740;&#31350;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01306</link><description>&lt;p&gt;
KTO: &#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
KTO: Model Alignment as Prospect Theoretic Optimization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20975;&#24681;&#26364;&#19982;&#29305;&#27779;&#26031;&#22522;&#30340;&#23637;&#26395;&#29702;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20154;&#31867;&#20197;&#26377;&#20559;&#35265;&#20294;&#26126;&#30830;&#30340;&#26041;&#24335;&#30475;&#24453;&#38543;&#26426;&#21464;&#37327;&#65307;&#20363;&#22914;&#65292;&#20154;&#20204;&#36890;&#24120;&#37117;&#26159;&#21388;&#24694;&#25439;&#22833;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;LLMs&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#30340;&#30446;&#26631;&#38544;&#21547;&#22320;&#34701;&#21512;&#20102;&#35768;&#22810;&#36825;&#20123;&#20559;&#35265; - &#36825;&#20123;&#30446;&#26631; (&#20363;&#22914; DPO) &#30340;&#25104;&#21151;&#37096;&#20998;&#21487;&#24402;&#22240;&#20110;&#23427;&#20204;&#26159;"&#20154;&#31867;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;"(HALOs)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25152;&#24402;&#22240;&#32473;&#20154;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20173;&#19982;&#23637;&#26395;&#29702;&#35770;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#12290;&#21033;&#29992;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20154;&#31867;&#25928;&#29992;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#30340;HALO&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20248;&#21270;(KTO)&#65292;&#24182;&#19988;&#23427;&#22312;&#20174;1B&#21040;30B&#30340;&#35268;&#27169;&#19978;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#36229;&#36807;&#12290;&#20851;&#38190;&#26159;&#65292;KTO&#19981;&#38656;&#35201;&#20559;&#22909; - &#21482;&#38656;&#35201;&#19968;&#20010;&#26159;&#21542;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kahneman &amp; Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#21644;&#38899;&#32032;&#34920;&#31034;&#20174;&#21407;&#22987;&#38899;&#39057;&#20449;&#21495;&#20013;&#23398;&#20064;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#30456;&#27604;&#21482;&#20351;&#29992;&#19968;&#31181;&#31867;&#22411;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#23398;&#20064;&#35821;&#20041;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01298</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#21644;&#38899;&#32032;&#34920;&#31034;&#23398;&#20064;&#21407;&#22987;&#38899;&#39057;&#20449;&#21495;&#30340;&#35821;&#20041;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Learning Semantic Information from Raw Audio Signal Using Both Contextual and Phonetic Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#21644;&#38899;&#32032;&#34920;&#31034;&#20174;&#21407;&#22987;&#38899;&#39057;&#20449;&#21495;&#20013;&#23398;&#20064;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#30456;&#27604;&#21482;&#20351;&#29992;&#19968;&#31181;&#31867;&#22411;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#23398;&#20064;&#35821;&#20041;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#34920;&#31034;&#65288;&#20998;&#21035;&#26159;&#32534;&#30721;&#19978;&#19979;&#25991;&#21644;&#38899;&#32032;&#20449;&#24687;&#65289;&#20174;&#21407;&#22987;&#38899;&#39057;&#20449;&#21495;&#20013;&#23398;&#20064;&#35821;&#20041;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35821;&#38899;&#21040;&#21333;&#20803;&#22788;&#29702;&#27969;&#31243;&#65292;&#20197;&#19981;&#21516;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#25429;&#25417;&#20004;&#31181;&#31867;&#22411;&#30340;&#34920;&#31034;&#12290;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21452;&#36890;&#36947;&#26550;&#26500;&#26469;&#25972;&#21512;&#36825;&#20004;&#31181;&#34920;&#31034;&#31867;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#21363;&#25513;&#30721;&#19978;&#19979;&#25991;&#37325;&#26500;&#21644;&#25513;&#30721;&#19978;&#19979;&#25991;&#39044;&#27979;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25512;&#21160;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;&#12290;&#22312;Zero Resource Speech Benchmark 2021&#21644;Fluent Speech Command&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#21482;&#20351;&#29992;&#19968;&#31181;&#31867;&#22411;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework to learn semantics from raw audio signals using two types of representations, encoding contextual and phonetic information respectively. Specifically, we introduce a speech-to-unit processing pipeline that captures two types of representations with different time resolutions. For the language model, we adopt a dual-channel architecture to incorporate both types of representation. We also present new training objectives, masked context reconstruction and masked context prediction, that push models to learn semantics effectively. Experiments on the sSIMI metric of Zero Resource Speech Benchmark 2021 and Fluent Speech Command dataset show our framework learns semantics better than models trained with only one type of representation.
&lt;/p&gt;</description></item><item><title>ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;</title><link>https://rss.arxiv.org/abs/2402.01295</link><description>&lt;p&gt;
ExtremeCast: &#25552;&#21319;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#30340;&#26497;&#20540;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01295
&lt;/p&gt;
&lt;p&gt;
ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#22312;&#20840;&#29699;&#20013;&#26399;&#39044;&#25253;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26497;&#31471;&#20540;&#39044;&#27979;&#19982;&#27492;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#23545;&#31216;&#25439;&#22833;&#65292;&#22914;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#26377;&#20559;&#24046;&#24182;&#20302;&#20272;&#26497;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Exloss&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#20248;&#21270;&#31361;&#20986;&#26497;&#20540;&#65292;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#26497;&#31471;&#22825;&#27668;&#39044;&#25253;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#23427;&#22686;&#21152;&#20102;&#20687;&#32032;&#20540;&#30340;&#26041;&#24046;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#21512;&#20808;&#36827;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our sol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#26435;&#37325;&#26694;&#26550;&#30340;&#20551;&#35774;&#39537;&#21160;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#25903;&#25345;&#25110;&#39539;&#26021;&#20551;&#35774;&#30340;&#35777;&#25454;&#26469;&#22686;&#21152;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#20943;&#23569;&#20381;&#36182;&#31243;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01292</link><description>&lt;p&gt;
&#36808;&#21521;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65306;&#36890;&#36807;&#35777;&#25454;&#25903;&#25345;&#30340;&#20551;&#35774;&#39537;&#21160;&#26041;&#27861;&#30340;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Towards the new XAI: A Hypothesis-Driven Approach to Decision Support Using Evidence
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#26435;&#37325;&#26694;&#26550;&#30340;&#20551;&#35774;&#39537;&#21160;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#25903;&#25345;&#25110;&#39539;&#26021;&#20551;&#35774;&#30340;&#35777;&#25454;&#26469;&#22686;&#21152;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#20943;&#23569;&#20381;&#36182;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20043;&#21069;&#20851;&#20110;AI&#36741;&#21161;&#20154;&#31867;&#20915;&#31574;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#19968;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#65292;&#21628;&#21505;&#36890;&#36807;&#19968;&#20010;&#31216;&#20026;&#35780;&#20215;&#22411;AI&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#36827;&#34892;&#20551;&#35774;&#39537;&#21160;&#30340;XAI&#65292;&#35813;&#26694;&#26550;&#20026;&#20154;&#20204;&#25552;&#20379;&#25903;&#25345;&#25110;&#39539;&#26021;&#20551;&#35774;&#30340;&#35777;&#25454;&#65292;&#32780;&#19981;&#19968;&#23450;&#32473;&#20986;&#20915;&#31574;&#36741;&#21161;&#25512;&#33616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#26435;&#37325;&#65288;WoE&#65289;&#26694;&#26550;&#30340;&#20551;&#35774;&#39537;&#21160;XAI&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#32473;&#23450;&#30340;&#20551;&#35774;&#29983;&#25104;&#27491;&#38754;&#21644;&#36127;&#38754;&#35777;&#25454;&#12290;&#36890;&#36807;&#20154;&#31867;&#34892;&#20026;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#39537;&#21160;&#26041;&#27861;&#25552;&#39640;&#20102;&#20915;&#31574;&#20934;&#30830;&#24615;&#65292;&#19982;&#25512;&#33616;&#39537;&#21160;&#26041;&#27861;&#21644;&#20165;AI&#35299;&#37322;&#22522;&#32447;&#30456;&#27604;&#20943;&#23569;&#20102;&#20381;&#36182;&#31243;&#24230;&#65292;&#20294;&#30456;&#23545;&#20110;&#25512;&#33616;&#39537;&#21160;&#26041;&#27861;&#65292;&#22312;&#20381;&#36182;&#31243;&#24230;&#19979;&#38477;&#26041;&#38754;&#30053;&#24494;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21442;&#19982;&#32773;&#22312;&#20351;&#29992;&#25105;&#20204;&#30340;&#20551;&#35774;&#39537;&#21160;&#26041;&#27861;&#26102;&#19982;&#20004;&#20010;&#22522;&#32447;&#30340;&#26041;&#24335;&#23384;&#22312;&#23454;&#36136;&#24615;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior research on AI-assisted human decision-making has explored several different explainable AI (XAI) approaches. A recent paper has proposed a paradigm shift calling for hypothesis-driven XAI through a conceptual framework called evaluative AI that gives people evidence that supports or refutes hypotheses without necessarily giving a decision-aid recommendation. In this paper we describe and evaluate an approach for hypothesis-driven XAI based on the Weight of Evidence (WoE) framework, which generates both positive and negative evidence for a given hypothesis. Through human behavioural experiments, we show that our hypothesis-driven approach increases decision accuracy, reduces reliance compared to a recommendation-driven approach and an AI-explanation-only baseline, but with a small increase in under-reliance compared to the recommendation-driven approach. Further, we show that participants used our hypothesis-driven approach in a materially different way to the two baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#25351;&#26631;&#26469;&#35780;&#20272;FU&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#39564;&#35777;&#12289;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#27934;&#23519;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#20123;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01276</link><description>&lt;p&gt;
&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;: &#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Federated Unlearning: a Perspective of Stability and Fairness
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#25351;&#26631;&#26469;&#35780;&#20272;FU&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#39564;&#35777;&#12289;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#27934;&#23519;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#20123;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#22810;&#26041;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FU&#35780;&#20272;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#37325;&#28857;&#20851;&#27880;&#39564;&#35777;&#65292;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20869;&#22312;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#23545;&#20855;&#26377;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#21462;&#28040;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#23545;FU&#20013;&#26435;&#34913;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#65292;&#20026;FU&#26426;&#21046;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;FU&#26426;&#21046;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#26435;&#34913;&#65292;&#30830;&#35748;&#20102;&#20174;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#20013;&#24471;&#20986;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the multifaceted consequences of federated unlearning (FU) with data heterogeneity. We introduce key metrics for FU assessment, concentrating on verification, global stability, and local fairness, and investigate the inherent trade-offs. Furthermore, we formulate the unlearning process with data heterogeneity through an optimization framework. Our key contribution lies in a comprehensive theoretical analysis of the trade-offs in FU and provides insights into data heterogeneity's impacts on FU. Leveraging these insights, we propose FU mechanisms to manage the trade-offs, guiding further development for FU mechanisms. We empirically validate that our FU mechanisms effectively balance trade-offs, confirming insights derived from our theoretical analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#35821;&#20041;&#35770;&#35777;&#65292;&#35752;&#35770;&#20102;&#26159;&#21542;&#21487;&#20197;&#31216;&#20043;&#20026;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#65292;&#20197;&#21450;ChatGPT&#27169;&#22411;&#33021;&#21542;&#23454;&#29616;&#35813;&#24605;&#32500;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#32570;&#20047;&#19982;&#29616;&#23454;&#30456;&#20851;&#30340;&#35777;&#25454;&#21644;&#20010;&#20154;&#20449;&#24565;&#65292;&#22240;&#27492;&#26080;&#27861;&#24418;&#25104;&#23545;&#19990;&#30028;&#30340;&#20449;&#24565;&#21644;&#30495;&#23454;&#24615;&#21028;&#26029;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01267</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#26426;&#22120;&#65306;&#36923;&#36753;&#12289;&#30495;&#23454;&#24615;&#19982;ChatGPT
&lt;/p&gt;
&lt;p&gt;
The Human and the Mechanical: logos, truthfulness, and ChatGPT
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#35821;&#20041;&#35770;&#35777;&#65292;&#35752;&#35770;&#20102;&#26159;&#21542;&#21487;&#20197;&#31216;&#20043;&#20026;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#65292;&#20197;&#21450;ChatGPT&#27169;&#22411;&#33021;&#21542;&#23454;&#29616;&#35813;&#24605;&#32500;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#32570;&#20047;&#19982;&#29616;&#23454;&#30456;&#20851;&#30340;&#35777;&#25454;&#21644;&#20010;&#20154;&#20449;&#24565;&#65292;&#22240;&#27492;&#26080;&#27861;&#24418;&#25104;&#23545;&#19990;&#30028;&#30340;&#20449;&#24565;&#21644;&#30495;&#23454;&#24615;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#36866;&#24403;&#22320;&#35848;&#35770;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#65292;&#20197;&#21450;ChatGPT&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#34987;&#35270;&#20026;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#22312;&#24403;&#21069;&#30340;&#35752;&#35770;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#35821;&#20041;&#35770;&#35777;&#12290;&#20154;&#31867;&#26029;&#35328;&#30340;&#34892;&#20026;&#38656;&#35201;&#24418;&#25104;&#19968;&#20010;&#30495;&#23454;&#24615;&#21028;&#26029;&#12290;&#20351;&#29992;&#24773;&#24577;&#21160;&#35789;&#20462;&#39280;&#26029;&#35328;&#65288;&#32422;&#32752;&#19968;&#23450;&#22312;&#23478;&#65289;&#21644;&#20351;&#29992;&#20027;&#35266;&#20803;&#32032;&#65288;&#32422;&#32752;&#26126;&#26174;&#22312;&#23478;&#65289;&#34920;&#26126;&#35828;&#35805;&#32773;&#27491;&#22312;&#25805;&#32437;&#22905;&#30340;&#21028;&#26029;&#65292;&#22312;&#21512;&#20316;&#30340;&#35821;&#22659;&#20013;&#65292;&#24847;&#22270;&#23558;&#22905;&#30340;&#35748;&#30693;&#29366;&#24577;&#23545;&#35805;&#26041;&#36879;&#26126;&#21270;&#12290;&#30495;&#23454;&#24615;&#21028;&#26029;&#26159;&#22522;&#20110;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#24418;&#25104;&#30340;&#65306;&#65288;i&#65289;&#19982;&#29616;&#23454;&#30456;&#20851;&#30340;&#35777;&#25454;&#65288;&#22806;&#29983;&#35777;&#25454;&#65289;&#21644;&#65288;ii&#65289;&#19982;&#20559;&#22909;&#21644;&#20010;&#20154;&#20449;&#24565;&#30456;&#20851;&#30340;&#20869;&#22312;&#35777;&#25454;&#12290;&#32780;&#8220;&#26426;&#26800;&#24605;&#32500;&#8221;&#32570;&#20047;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#23427;&#20204;&#19982;&#29616;&#23454;&#26080;&#20851;&#65292;&#65288;ii&#65289;&#27809;&#26377;&#20869;&#22312;&#35777;&#25454;&#12290;&#22240;&#27492;&#23427;&#20204;&#32570;&#20047;&#23545;&#19990;&#30028;&#30340;&#20449;&#24565;&#24418;&#25104;&#21644;&#30495;&#23454;&#24615;&#21028;&#26029;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper addresses the question of whether it is appropriate to talk about `mechanical minds' at all, and whether ChatGPT models can indeed be thought of as realizations of that. Our paper adds a semantic argument to the current debate. The act of human assertion requires the formation of a veridicality judgment. Modification of assertions with modals (John must be at home) and the use of subjective elements (John is obviously at home) indicate that the speaker is manipulating her judgments and, in a cooperative context, intends her epistemic state to be transparent to the addressee. Veridicality judgments are formed on the basis of two components: (i) evidence that relates to reality (exogenous evidence) and (ii) endogenous evidence, such as preferences and private beliefs. `Mechanical minds' lack these two components: (i) they do not relate to reality and (ii) do not have endogenous evidence. Therefore they lack the ability to form a belief about the world and a veridicality judgmen
&lt;/p&gt;</description></item><item><title>TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01261</link><description>&lt;p&gt;
TEDDY: &#22522;&#20110;&#24230;&#37327;&#21028;&#21035;&#31574;&#30053;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TEDDY: Trimming Edges with Degree-based Discrimination strategY
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01261
&lt;/p&gt;
&lt;p&gt;
TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Chen&#31561;&#20154;&#22312;2021&#24180;&#25552;&#20986;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20197;&#26469;&#65292;&#23547;&#25214;&#22270;&#25277;&#22870;&#31080;&#65288;GLT&#65289;&#30340;&#30740;&#31350;&#24050;&#25104;&#20026;GNN&#31038;&#21306;&#30340;&#37325;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#65292;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#29616;&#19982;&#21407;&#22987;&#23494;&#38598;&#32593;&#32476;&#30456;&#24403;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#26356;&#31232;&#30095;&#30340;GLT&#12290;&#21516;&#26102;&#65292;&#22270;&#32467;&#26500;&#20316;&#20026;GNN&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#20063;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#36817;&#20960;&#39033;&#30740;&#31350;&#30340;&#38416;&#26126;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20851;&#20110;GLT&#30340;&#30740;&#31350;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22270;&#32467;&#26500;&#20013;&#30340;&#20869;&#22312;&#36335;&#24452;&#65292;&#24182;&#20197;&#36845;&#20195;&#26041;&#24335;&#35782;&#21035;&#31080;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#32791;&#26102;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;TEDDY&#65292;&#19968;&#31181;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#24182;&#25972;&#21512;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#19968;&#27425;&#24615;&#36793;&#32536;&#31232;&#30095;&#21270;&#26694;&#26550;&#12290;&#22312;&#36827;&#34892;&#36793;&#32536;&#31232;&#30095;&#21270;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient desc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20855;&#26377;&#36275;&#22815;&#27627;&#31859;&#27874;&#25509;&#25910;&#21151;&#29575;&#30340;&#26368;&#20339;&#27874;&#26463;&#65292;&#20351;&#29992;&#36710;&#36742;&#20301;&#32622;&#20449;&#24687;&#26469;&#23454;&#29616;&#36710;&#36742;&#21040;&#36710;&#36742;&#36890;&#20449;&#20013;&#30340;&#39640;&#25928;&#38142;&#36335;&#37197;&#32622;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01259</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20301;&#32622;&#24863;&#30693;60 GHz&#27627;&#31859;&#27874;&#27874;&#26463;&#25104;&#24418;&#25216;&#26415;&#29992;&#20110;&#36710;&#36742;&#21040;&#36710;&#36742;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Position Aware 60 GHz mmWave Beamforming for V2V Communications Utilizing Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20855;&#26377;&#36275;&#22815;&#27627;&#31859;&#27874;&#25509;&#25910;&#21151;&#29575;&#30340;&#26368;&#20339;&#27874;&#26463;&#65292;&#20351;&#29992;&#36710;&#36742;&#20301;&#32622;&#20449;&#24687;&#26469;&#23454;&#29616;&#36710;&#36742;&#21040;&#36710;&#36742;&#36890;&#20449;&#20013;&#30340;&#39640;&#25928;&#38142;&#36335;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#26463;&#25104;&#24418;&#25216;&#26415;&#26159;&#36890;&#36807;&#37319;&#29992;&#22823;&#35268;&#27169;&#22825;&#32447;&#38453;&#21015;&#21644;&#29983;&#25104;&#31364;&#27874;&#26463;&#26469;&#24357;&#34917;&#27627;&#31859;&#27874;&#36890;&#20449;&#20013;&#30340;&#20005;&#37325;&#36335;&#24452;&#25439;&#32791;&#65292;&#20197;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#25509;&#25910;&#21151;&#29575;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#27874;&#26463;&#36873;&#25321;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#26469;&#36827;&#34892;&#20934;&#30830;&#30340;&#27874;&#26463;&#23545;&#20934;&#65292;&#24182;&#20026;&#39640;&#25928;&#30340;&#38142;&#36335;&#37197;&#32622;&#24102;&#26469;&#26174;&#33879;&#30340;&#24310;&#36831;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#22312;&#36710;&#36742;&#21040;&#36710;&#36742;&#65288;V2V&#65289;&#36890;&#20449;&#20013;&#22914;&#39640;&#24230;&#21160;&#24577;&#30340;&#22330;&#26223;&#20013;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21033;&#29992;&#24102;&#22806;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#36710;&#36742;&#20301;&#32622;&#20449;&#24687;&#65289;&#26159;&#20943;&#23569;&#36825;&#31181;&#24320;&#38144;&#30340;&#28508;&#22312;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#36710;&#36742;&#20301;&#32622;&#20449;&#24687;&#26469;&#39044;&#27979;&#20855;&#26377;&#36275;&#22815;&#27627;&#31859;&#27874;&#25509;&#25910;&#21151;&#29575;&#30340;&#26368;&#20339;&#27874;&#26463;&#65292;&#20174;&#32780;&#21487;&#20197;&#20027;&#21160;&#30830;&#20445;&#26368;&#20339;&#30340;V2V&#30452;&#35270;&#38142;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beamforming techniques are considered as essential parts to compensate the severe path loss in millimeter-wave (mmWave) communications by adopting large antenna arrays and formulating narrow beams to obtain satisfactory received powers. However, performing accurate beam alignment over such narrow beams for efficient link configuration by traditional beam selection approaches, mainly relied on channel state information, typically impose significant latency and computing overheads, which is often infeasible in vehicle-to-vehicle (V2V) communications like highly dynamic scenarios. In contrast, utilizing out-of-band contextual information, such as vehicular position information, is a potential alternative to reduce such overheads. In this context, this paper presents a deep learning-based solution on utilizing the vehicular position information for predicting the optimal beams having sufficient mmWave received powers so that the best V2V line-of-sight links can be ensured proactively. Afte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CISP&#65288;Contrastive Image Shape Pre training&#65289;&#65292;&#36890;&#36807;&#23558;2D&#22270;&#20687;&#19982;3D&#24418;&#29366;&#22312;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#20013;&#23545;&#40784;&#65292;&#22686;&#24378;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;3D&#24418;&#29366;&#21512;&#25104;&#12290;&#30740;&#31350;&#34920;&#26126;CISP&#33021;&#22815;&#25429;&#25417;CLIP&#30340;&#25991;&#26412;&#22270;&#20687;&#20851;&#27880;&#25152;&#24573;&#35270;&#30340;3D&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01241</link><description>&lt;p&gt;
&#33021;&#22815;&#26893;&#20837;&#24418;&#29366;&#30340;&#32852;&#21512;&#23884;&#20837;&#26159;&#21542;&#33021;&#25913;&#36827;&#22522;&#20110;&#22270;&#20687;&#30340;&#19977;&#32500;&#25193;&#25955;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01241
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CISP&#65288;Contrastive Image Shape Pre training&#65289;&#65292;&#36890;&#36807;&#23558;2D&#22270;&#20687;&#19982;3D&#24418;&#29366;&#22312;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#20013;&#23545;&#40784;&#65292;&#22686;&#24378;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;3D&#24418;&#29366;&#21512;&#25104;&#12290;&#30740;&#31350;&#34920;&#26126;CISP&#33021;&#22815;&#25429;&#25417;CLIP&#30340;&#25991;&#26412;&#22270;&#20687;&#20851;&#27880;&#25152;&#24573;&#35270;&#30340;3D&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#29305;&#21035;&#26159;&#24212;&#29992;CLIP&#65288;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65289;&#21040;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#30340;&#29983;&#25104;&#22270;&#20687;&#30340;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;CLIP&#30340;&#32467;&#26500;&#21270;&#23884;&#20837;&#31354;&#38388;&#20063;&#34987;&#25193;&#23637;&#21040;&#22270;&#20687;&#21040;&#24418;&#29366;&#30340;&#29983;&#25104;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#25104;&#21151;&#65292;&#20294;&#26159;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#22522;&#26412;&#30340;&#38382;&#39064;&#65306;CLIP&#26159;&#21542;&#33021;&#30830;&#20445;&#20174;&#22270;&#20687;&#20013;&#29983;&#25104;&#26368;&#20339;&#30340;&#24418;&#29366;&#65311;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#26465;&#20214;&#26469;&#23558;&#26174;&#24335;&#30340;&#19977;&#32500;&#30693;&#35782;&#24341;&#20837;&#29983;&#25104;&#36807;&#31243;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#36136;&#37327;&#65311;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CISP&#65288;&#23545;&#27604;&#22270;&#20687;&#24418;&#29366;&#39044;&#35757;&#32451;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;2D&#22270;&#20687;&#19982;3D&#24418;&#29366;&#22312;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#23545;&#40784;&#26469;&#22686;&#24378;3D&#24418;&#29366;&#21512;&#25104;&#12290;CISP&#26088;&#22312;&#20016;&#23500;CLIP&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;CLIP&#30340;&#25991;&#26412;&#22270;&#20687;&#20851;&#27880;&#21487;&#33021;&#24573;&#35270;&#30340;3D&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#35780;&#20272;&#20102;CISP&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in deep generative models, particularly with the application of CLIP (Contrastive Language Image Pretraining) to Denoising Diffusion Probabilistic Models (DDPMs), have demonstrated remarkable effectiveness in text to image generation. The well structured embedding space of CLIP has also been extended to image to shape generation with DDPMs, yielding notable results. Despite these successes, some fundamental questions arise: Does CLIP ensure the best results in shape generation from images? Can we leverage conditioning to bring explicit 3D knowledge into the generative process and obtain better quality? This study introduces CISP (Contrastive Image Shape Pre training), designed to enhance 3D shape synthesis guided by 2D images. CISP aims to enrich the CLIP framework by aligning 2D images with 3D shapes in a shared embedding space, specifically capturing 3D characteristics potentially overlooked by CLIP's text image focus. Our comprehensive analysis assesses CISP's gu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01240</link><description>&lt;p&gt;
&#36229;&#36234;&#35831;&#27714;&#65306;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#22312;&#19981;&#24179;&#34913;&#29615;&#22659;&#20013;&#36827;&#34892;&#36328;&#27983;&#35272;&#22120;Web&#36861;&#36394;&#22120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser Web Tracker Classification in an Imbalanced Setting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19975;&#32500;&#32593;&#30340;&#36830;&#36890;&#24615;&#20027;&#35201;&#24402;&#22240;&#20110;HTTP&#21327;&#35758;&#65292;&#20854;&#20013;&#30340;HTTP&#28040;&#24687;&#25552;&#20379;&#20102;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#20449;&#24687;&#22836;&#23383;&#27573;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;Web&#36861;&#36394;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#21033;&#29992;HTTP/S&#35831;&#27714;&#28040;&#24687;&#26469;&#35782;&#21035;Web&#36861;&#36394;&#22120;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;HTTP/S&#21709;&#24212;&#22836;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#20351;&#29992;HTTP/S&#21709;&#24212;&#22836;&#36827;&#34892;Web&#36861;&#36394;&#22120;&#26816;&#27979;&#30340;&#26377;&#25928;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#27983;&#35272;&#22120;&#25193;&#23637;&#31243;&#24207;T.EX&#33719;&#21462;&#30340;Chrome&#12289;Firefox&#21644;Brave&#27983;&#35272;&#22120;&#30340;&#25968;&#25454;&#20316;&#20026;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;Chrome&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;11&#20010;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#22312;&#25152;&#26377;&#27983;&#35272;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Chrome&#21644;Firefox&#19978;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12289;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#26368;&#23567;&#23545;&#25968;&#25439;&#22833;&#35823;&#24046;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;Brave&#27983;&#35272;&#22120;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#24449;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The World Wide Web's connectivity is greatly attributed to the HTTP protocol, with HTTP messages offering informative header fields that appeal to disciplines like web security and privacy, especially concerning web tracking. Despite existing research employing HTTP/S request messages to identify web trackers, HTTP/S response headers are often overlooked. This study endeavors to design effective machine learning classifiers for web tracker detection using HTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers, obtained through the traffic monitoring browser extension T.EX, serves as our data set. Eleven supervised models were trained on Chrome data and tested across all browsers. The results demonstrated high accuracy, F1-score, precision, recall, and minimal log-loss error for Chrome and Firefox, but subpar performance on Brave, potentially due to its distinct data distribution and feature set. The research suggests that these classifiers are viable for detecting w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRIME&#30340;&#20445;&#25252;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#38477;&#20302;&#20445;&#25252;&#35270;&#39057;&#30340;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#20445;&#25252;&#24615;&#33021;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PRIME&#20165;&#38656;8.3%&#30340;GPU&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01239</link><description>&lt;p&gt;
PRIME: &#20445;&#25252;&#24744;&#30340;&#35270;&#39057;&#20813;&#21463;&#24694;&#24847;&#32534;&#36753;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
PRIME: Protect Your Videos From Malicious Editing
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01239
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRIME&#30340;&#20445;&#25252;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#38477;&#20302;&#20445;&#25252;&#35270;&#39057;&#30340;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#20445;&#25252;&#24615;&#33021;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PRIME&#20165;&#38656;8.3%&#30340;GPU&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#12290;&#26368;&#36817;&#65292;&#24320;&#28304;&#27169;&#22411;&#20351;&#24471;&#25805;&#32437;&#21644;&#32534;&#36753;&#29031;&#29255;&#21644;&#35270;&#39057;&#21464;&#24471;&#38750;&#24120;&#23481;&#26131;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#12290;&#34429;&#28982;&#36825;&#20123;&#23574;&#31471;&#25216;&#26415;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20010;&#20154;&#38544;&#31169;&#21644;&#32918;&#20687;&#26435;&#30340;&#25285;&#24551;&#12290;&#24694;&#24847;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#27450;&#39575;&#25110;&#38750;&#27861;&#34892;&#20026;&#12290;&#34429;&#28982;&#19968;&#20123;&#20043;&#21069;&#30340;&#24037;&#20316;&#20851;&#27880;&#20445;&#25252;&#29031;&#29255;&#20813;&#21463;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#26041;&#38754;&#65292;&#20445;&#25252;&#35270;&#39057;&#21644;&#22270;&#20687;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#20445;&#25252;&#26041;&#27861;PRIME&#65292;&#20197;&#26174;&#33879;&#38477;&#20302;&#26102;&#38388;&#25104;&#26412;&#24182;&#25913;&#21892;&#20445;&#25252;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#20445;&#25252;&#26041;&#27861;&#65292;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;&#23458;&#35266;&#25351;&#26631;&#21644;&#20154;&#31867;&#20027;&#35266;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PRIME&#20165;&#21344;&#20102;&#20808;&#21069;&#26041;&#26696;&#25152;&#38656;GPU&#26102;&#38388;&#30340;8.3%&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of generative models, the quality of generated content keeps increasing. Recently, open-source models have made it surprisingly easy to manipulate and edit photos and videos, with just a few simple prompts. While these cutting-edge technologies have gained popularity, they have also given rise to concerns regarding the privacy and portrait rights of individuals. Malicious users can exploit these tools for deceptive or illegal purposes. Although some previous works focus on protecting photos against generative models, we find there are still gaps between protecting videos and images in the aspects of efficiency and effectiveness. Therefore, we introduce our protection method, PRIME, to significantly reduce the time cost and improve the protection performance. Moreover, to evaluate our proposed protection method, we consider both objective metrics and human subjective metrics. Our evaluation results indicate that PRIME only costs 8.3% GPU hours of the cost of the pre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28789;&#27963;&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;FVIB&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21333;&#27425;&#35757;&#32451;&#21363;&#21487;&#33719;&#24471;&#25152;&#26377;&#946;&#20540;&#30340;&#26368;&#20248;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#21387;&#32553;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01238</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65306;&#36890;&#36807;&#19968;&#27425;&#35757;&#32451;&#23454;&#29616;&#22810;&#26679;&#21270;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Flexible Variational Information Bottleneck: Achieving Diverse Compression with a Single Training
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28789;&#27963;&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;FVIB&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21333;&#27425;&#35757;&#32451;&#21363;&#21487;&#33719;&#24471;&#25152;&#26377;&#946;&#20540;&#30340;&#26368;&#20248;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#21387;&#32553;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28304;&#38543;&#26426;&#21464;&#37327;&#20013;&#25552;&#21462;&#19982;&#30446;&#26631;&#38543;&#26426;&#21464;&#37327;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#946;&#65292;&#20449;&#24687;&#29942;&#39048;&#25511;&#21046;&#25968;&#25454;&#21387;&#32553;&#21644;&#39044;&#27979;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20256;&#32479;&#19978;&#65292;&#20026;&#20102;&#25214;&#21040;&#35201;&#23398;&#20064;&#30340;&#26435;&#34913;&#65292;&#20449;&#24687;&#29942;&#39048;&#38656;&#35201;&#36890;&#36807;&#22810;&#20010;&#35757;&#32451;&#21608;&#26399;&#26469;&#25628;&#32034;&#946;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28789;&#27963;&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;FVIB&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#27425;&#35745;&#31639;&#39640;&#25928;&#30340;&#35757;&#32451;&#26469;&#33719;&#24471;&#25152;&#26377;&#946;&#20540;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;FVIB&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#946;&#20540;&#33539;&#22260;&#20869;&#21516;&#26102;&#26368;&#22823;&#21270;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;VIB&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;FVIB&#21487;&#20197;&#20687;VI&#19968;&#26679;&#26377;&#25928;&#22320;&#23398;&#20064;VIB&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information Bottleneck (IB) is a widely used framework that enables the extraction of information related to a target random variable from a source random variable. In the objective function, IB controls the trade-off between data compression and predictiveness through the Lagrange multiplier $\beta$. Traditionally, to find the trade-off to be learned, IB requires a search for $\beta$ through multiple training cycles, which is computationally expensive. In this study, we introduce Flexible Variational Information Bottleneck (FVIB), an innovative framework for classification task that can obtain optimal models for all values of $\beta$ with single, computationally efficient training. We theoretically demonstrate that across all values of reasonable $\beta$, FVIB can simultaneously maximize an approximation of the objective function for Variational Information Bottleneck (VIB), the conventional IB method. Then we empirically show that FVIB can learn the VIB objective as effectively as VI
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#19988;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#27450;&#39575;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01227</link><description>&lt;p&gt;
STAA-Net: &#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#31232;&#30095;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
STAA-Net: A Sparse and Transferable Adversarial Attack for Speech Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#19988;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#27450;&#39575;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21253;&#21547;&#20154;&#31867;&#24773;&#24863;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#26159;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#37325;&#35201;&#35805;&#39064;&#12290;SER&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#38544;&#31169;&#25935;&#24863;&#21644;&#21487;&#38752;&#24615;&#35201;&#27714;&#39640;&#30340;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#20363;&#22914;&#31169;&#20154;&#21307;&#30103;&#20445;&#20581;&#12290;&#26368;&#36817;&#65292;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38899;&#39057;&#39046;&#22495;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30340;&#30740;&#31350;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#38899;&#39057;&#39046;&#22495;&#30340;&#20808;&#21069;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#36845;&#20195;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#32791;&#26102;&#19988;&#23481;&#26131;&#36807;&#25311;&#21512;&#29305;&#23450;&#30340;&#23041;&#32961;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#22312;&#38899;&#39057;&#39046;&#22495;&#23545;&#31232;&#30095;&#25200;&#21160;&#30340;&#25506;&#32034;&#20173;&#28982;&#26377;&#38480;&#65292;&#32780;&#31232;&#30095;&#25200;&#21160;&#20855;&#26377;&#26356;&#22909;&#30340;&#38544;&#34109;&#24615;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#25915;&#20987;&#26041;&#27861;&#65292;&#20197;&#31471;&#21040;&#31471;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#19988;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#27450;&#39575;SER&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech contains rich information on the emotions of humans, and Speech Emotion Recognition (SER) has been an important topic in the area of human-computer interaction. The robustness of SER models is crucial, particularly in privacy-sensitive and reliability-demanding domains like private healthcare. Recently, the vulnerability of deep neural networks in the audio domain to adversarial attacks has become a popular area of research. However, prior works on adversarial attacks in the audio domain primarily rely on iterative gradient-based techniques, which are time-consuming and prone to overfitting the specific threat model. Furthermore, the exploration of sparse perturbations, which have the potential for better stealthiness, remains limited in the audio domain. To address these challenges, we propose a generator-based attack method to generate sparse and transferable adversarial examples to deceive SER models in an end-to-end and efficient manner. We evaluate our method on two widely-
&lt;/p&gt;</description></item><item><title>AI&#20195;&#30721;&#29983;&#25104;&#22120;&#22312;&#36719;&#20214;&#23433;&#20840;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20294;&#20063;&#23384;&#22312;&#34987;&#24694;&#24847;&#28389;&#29992;&#30340;&#39118;&#38505;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01219</link><description>&lt;p&gt;
AI&#20195;&#30721;&#29983;&#25104;&#22120;&#19982;&#23433;&#20840;&#65306;&#26379;&#21451;&#36824;&#26159;&#25932;&#20154;?
&lt;/p&gt;
&lt;p&gt;
AI Code Generators for Security: Friend or Foe?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01219
&lt;/p&gt;
&lt;p&gt;
AI&#20195;&#30721;&#29983;&#25104;&#22120;&#22312;&#36719;&#20214;&#23433;&#20840;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20294;&#20063;&#23384;&#22312;&#34987;&#24694;&#24847;&#28389;&#29992;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#21457;&#23637;&#27491;&#22312;&#20026;&#36719;&#20214;&#23433;&#20840;&#30740;&#31350;&#24102;&#26469;&#26032;&#30340;&#26426;&#20250;&#65292;&#20294;&#20063;&#21487;&#33021;&#34987;&#24694;&#24847;&#34892;&#20026;&#32773;&#28389;&#29992;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances of artificial intelligence (AI) code generators are opening new opportunities in software security research, including misuse by malicious actors. We review use cases for AI code generators for security and introduce an evaluation benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#38477;&#38632;&#39044;&#27979;&#20013;&#30340;&#20301;&#32622;&#24046;&#24322;&#21644;&#27668;&#20505;&#21464;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#24052;&#40654;&#12289;&#27931;&#26441;&#30710;&#21644;&#19996;&#20140;&#36827;&#34892;&#36866;&#24212;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#20998;&#21035;&#25552;&#39640;&#20102;43.51%&#12289;5.09%&#21644;38.62%&#12290;</title><link>https://rss.arxiv.org/abs/2402.01208</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20301;&#32622;&#26080;&#20851;&#33258;&#36866;&#24212;&#38477;&#38632;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Location Agnostic Adaptive Rain Precipitation Prediction using Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#38477;&#38632;&#39044;&#27979;&#20013;&#30340;&#20301;&#32622;&#24046;&#24322;&#21644;&#27668;&#20505;&#21464;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#24052;&#40654;&#12289;&#27931;&#26441;&#30710;&#21644;&#19996;&#20140;&#36827;&#34892;&#36866;&#24212;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#20998;&#21035;&#25552;&#39640;&#20102;43.51%&#12289;5.09%&#21644;38.62%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#38632;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#22240;&#22320;&#32780;&#24322;&#30340;&#27668;&#20505;&#21644;&#27668;&#35937;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#22312;&#19968;&#20010;&#20301;&#32622;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#20854;&#20182;&#20301;&#32622;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20840;&#29699;&#21464;&#26262;&#65292;&#22825;&#27668;&#27169;&#24335;&#24180;&#22797;&#19968;&#24180;&#22320;&#21457;&#29983;&#30528;&#24555;&#36895;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#21363;&#20351;&#22312;&#30456;&#21516;&#20301;&#32622;&#65292;&#26102;&#38388;&#36807;&#21435;&#21518;&#37027;&#20123;&#27169;&#22411;&#20063;&#21464;&#24471;&#26080;&#25928;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25512;&#24191;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#37027;&#20123;&#27809;&#26377;&#36866;&#24212;&#26426;&#21046;&#30340;&#26041;&#27861;&#26080;&#27861;&#39044;&#27979;&#38477;&#27700;&#30340;&#20219;&#20309;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32463;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36866;&#24212;&#21518;&#65292;&#22312;&#24052;&#40654;&#12289;&#27931;&#26441;&#30710;&#21644;&#19996;&#20140;&#30340;&#38477;&#27700;&#39044;&#27979;&#26041;&#38754;&#20998;&#21035;&#26174;&#31034;&#20102;43.51%&#12289;5.09%&#21644;38.62%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rain precipitation prediction is a challenging task as it depends on weather and meteorological features which vary from location to location. As a result, a prediction model that performs well at one location does not perform well at other locations due to the distribution shifts. In addition, due to global warming, the weather patterns are changing very rapidly year by year which creates the possibility of ineffectiveness of those models even at the same location as time passes. In our work, we have proposed an adaptive deep learning-based framework in order to provide a solution to the aforementioned challenges. Our method can generalize the model for the prediction of precipitation for any location where the methods without adaptation fail. Our method has shown 43.51%, 5.09%, and 38.62% improvement after adaptation using a deep neural network for predicting the precipitation of Paris, Los Angeles, and Tokyo, respectively.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01207</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficient Causal Graph Discovery Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#12290;&#20043;&#21069;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#25104;&#23545;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#38656;&#35201;&#20108;&#27425;&#26597;&#35810;&#30340;&#25968;&#37327;&#65292;&#23545;&#20110;&#36739;&#22823;&#30340;&#22240;&#26524;&#22270;&#26469;&#35828;&#24456;&#24555;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#26377;&#25152;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#32467;&#21512;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#38500;&#20102;&#26356;&#20855;&#26102;&#38388;&#21644;&#25968;&#25454;&#25928;&#29575;&#22806;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01204</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#20013;SSL&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;&#21270;&#21644;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;SSL&#24050;&#25104;&#20026;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#26032;&#36235;&#21183;&#65292;&#36825;&#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#26126;&#30830;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#25551;&#36848;&#24615;&#30340;&#34920;&#31034;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#22238;&#39038;&#21644;&#24635;&#32467;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#65288;SSL4NS-TD&#65289;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;NS-TD&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#26041;&#27861;&#34987;&#20998;&#20026;&#19977;&#32452;&#8212;&#8212;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#24182;&#20171;&#32461;&#20102;&#27599;&#20010;&#26041;&#21521;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#21160;&#26426;&#21644;&#20248;&#28857;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#20171;&#32461;&#20102;SSL4NS-TD&#30340;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20174;&#21518;&#32493;&#22686;&#37327;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#65292;&#19982;&#24102;&#26631;&#31614;&#30340;&#22522;&#31867;&#26679;&#26412;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#20026;&#26087;&#31867;&#21644;&#26032;&#31867;&#25968;&#25454;&#20998;&#37197;&#23884;&#20837;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38887;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01201</link><description>&lt;p&gt;
&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Class-Incremental Learning with Prior Knowledge
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01201
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20174;&#21518;&#32493;&#22686;&#37327;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#65292;&#19982;&#24102;&#26631;&#31614;&#30340;&#22522;&#31867;&#26679;&#26412;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#20026;&#26087;&#31867;&#21644;&#26032;&#31867;&#25968;&#25454;&#20998;&#37197;&#23884;&#20837;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;(FSCIL)&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22312;&#22686;&#37327;&#38454;&#27573;&#20445;&#30041;&#26087;&#30693;&#35782;&#30340;&#35760;&#24518;&#19978;&#12290;&#36825;&#20123;&#30740;&#31350;&#32463;&#24120;&#20302;&#20272;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22609;&#36896;&#22686;&#37327;&#23398;&#20064;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#23398;&#20064;(LwPK)&#65292;&#36890;&#36807;&#24341;&#20837;&#26469;&#33258;&#21518;&#32493;&#22686;&#37327;&#31867;&#21035;&#20013;&#23569;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20960;&#20046;&#33258;&#30001;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#23558;&#26080;&#26631;&#31614;&#30340;&#22686;&#37327;&#31867;&#26679;&#26412;&#32858;&#31867;&#65292;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#24182;&#19982;&#24102;&#26631;&#31614;&#30340;&#22522;&#31867;&#26679;&#26412;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#20026;&#26087;&#31867;&#21644;&#26032;&#31867;&#25968;&#25454;&#20998;&#37197;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LwPK&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38887;&#24615;&#65292;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#31867;&#38388;&#36317;&#31163;&#24230;&#37327;&#30340;&#29702;&#35770;&#20998;&#26512;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
To tackle the issues of catastrophic forgetting and overfitting in few-shot class-incremental learning (FSCIL), previous work has primarily concentrated on preserving the memory of old knowledge during the incremental phase. The role of pre-trained model in shaping the effectiveness of incremental learning is frequently underestimated in these studies. Therefore, to enhance the generalization ability of the pre-trained model, we propose Learning with Prior Knowledge (LwPK) by introducing nearly free prior knowledge from a few unlabeled data of subsequent incremental classes. We cluster unlabeled incremental class samples to produce pseudo-labels, then jointly train these with labeled base class samples, effectively allocating embedding space for both old and new class data. Experimental results indicate that LwPK effectively enhances the model resilience against catastrophic forgetting, with theoretical analysis based on empirical risk minimization and class distance measurement corrob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31895;&#31890;&#21270;&#20998;&#23376;&#34920;&#31034;&#20013;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#37319;&#26679;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#25928;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01195</link><description>&lt;p&gt;
&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#29992;&#20110;&#31895;&#31890;&#21270;&#20998;&#23376;&#34920;&#31034;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conditional Normalizing Flows for Active Learning of Coarse-Grained Molecular Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31895;&#31890;&#21270;&#20998;&#23376;&#34920;&#31034;&#20013;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#37319;&#26679;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#25928;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#37319;&#26679;&#20998;&#23376;&#31995;&#32479;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#19982;&#29983;&#25104;&#38271;&#26102;&#38388;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#19981;&#21516;&#65292;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22914;&#27491;&#21017;&#21270;&#27969;&#34987;&#29992;&#20110;&#30452;&#25509;&#23398;&#20064;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#32780;&#19981;&#38656;&#35201;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#20986;&#29616;&#27169;&#24335;&#23849;&#28291;&#65292;&#22240;&#27492;&#24120;&#24120;&#26080;&#27861;&#25506;&#32034;&#20840;&#37096;&#30340;&#26500;&#22411;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#65292;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#33258;&#30001;&#24230;&#12290;&#22312;&#31895;&#31890;&#21270;&#31354;&#38388;&#19978;&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#21487;&#20197;&#20135;&#29983;&#20004;&#20010;&#23618;&#27425;&#20043;&#38388;&#30340;&#27010;&#29575;&#36830;&#25509;&#12290;&#20026;&#20102;&#25506;&#32034;&#26500;&#22411;&#31354;&#38388;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31895;&#31890;&#21270;&#27169;&#25311;&#19982;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24517;&#35201;&#26102;&#26356;&#26032;&#27969;&#24182;&#36827;&#34892;&#20840;&#21407;&#23376;&#21183;&#33021;&#35780;&#20272;&#12290;&#20197;&#19993;&#27688;&#37240;&#20108;&#32957;&#20026;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient sampling of the Boltzmann distribution of molecular systems is a long-standing challenge. Recently, instead of generating long molecular dynamics simulations, generative machine learning methods such as normalizing flows have been used to learn the Boltzmann distribution directly, without samples. However, this approach is susceptible to mode collapse and thus often does not explore the full configurational space. In this work, we address this challenge by separating the problem into two levels, the fine-grained and coarse-grained degrees of freedom. A normalizing flow conditioned on the coarse-grained space yields a probabilistic connection between the two levels. To explore the configurational space, we employ coarse-grained simulations with active learning which allows us to update the flow and make all-atom potential energy evaluations only when necessary. Using alanine dipeptide as an example, we show that our methods obtain a speedup to molecular dynamics simulations of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21435;&#38500;SWIN Transformer&#20013;&#30340;GELU&#28608;&#27963;&#65292;&#25552;&#21319;&#20102;&#25972;&#25968;SWIN Transformer&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;ReLU&#28608;&#27963;&#26367;&#20195;GELU&#65292;&#24182;&#20351;&#29992;&#36845;&#20195;&#30693;&#35782;&#33976;&#39311;&#26469;&#35843;&#25972;&#31934;&#24230;&#25439;&#22833;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#37327;&#21270;&#21644;&#20248;&#21270;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01169</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#38500;GELU&#28608;&#27963;&#24555;&#36895;&#25512;&#26029;&#25972;&#25968;SWIN Transformer
&lt;/p&gt;
&lt;p&gt;
Faster Inference of Integer SWIN Transformer by Removing the GELU Activation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21435;&#38500;SWIN Transformer&#20013;&#30340;GELU&#28608;&#27963;&#65292;&#25552;&#21319;&#20102;&#25972;&#25968;SWIN Transformer&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;ReLU&#28608;&#27963;&#26367;&#20195;GELU&#65292;&#24182;&#20351;&#29992;&#36845;&#20195;&#30693;&#35782;&#33976;&#39311;&#26469;&#35843;&#25972;&#31934;&#24230;&#25439;&#22833;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#37327;&#21270;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SWIN transformer&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#35270;&#35273;transformer&#27169;&#22411;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#29420;&#29305;&#30340;&#26550;&#26500;&#23548;&#33268;&#19982;&#31867;&#20284;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#25512;&#26029;&#36895;&#24230;&#36739;&#24930;&#12290;&#27169;&#22411;&#30340;&#25972;&#25968;&#37327;&#21270;&#26159;&#29992;&#20110;&#25913;&#21892;&#25512;&#26029;&#24310;&#36831;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#33021;&#22815;&#23436;&#20840;&#37327;&#21270;&#27169;&#22411;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21435;&#38500;Swin Transformer&#20013;&#19982;GELU&#28608;&#27963;&#30456;&#20851;&#30340;&#28014;&#28857;&#36816;&#31639;&#26469;&#25913;&#21892;&#26368;&#26032;&#26041;&#27861;&#30340;&#25512;&#26029;&#24310;&#36831;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#29992;&#32447;&#24615;&#36924;&#36817;&#20989;&#25968;&#26367;&#25442;&#38750;&#25972;&#25968;&#36816;&#31639;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#29992;ReLU&#28608;&#27963;&#20195;&#26367;GELU&#12290;ReLU&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#20869;&#23384;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#36739;&#20302;&#12290;&#25105;&#20204;&#20351;&#29992;&#36845;&#20195;&#30693;&#35782;&#33976;&#39311;&#26469;&#24357;&#34917;&#30001;&#20110;&#29992;ReLU&#26367;&#25442;GELU&#32780;&#23548;&#33268;&#30340;&#31934;&#24230;&#25439;&#22833;&#12290;&#25105;&#20204;&#23545;&#21435;&#38500;GELU&#30340;SWIN transformer&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#36880;&#28176;&#36924;&#36817;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
SWIN transformer is a prominent vision transformer model that has state-of-the-art accuracy in image classification tasks. Despite this success, its unique architecture causes slower inference compared with similar deep neural networks. Integer quantization of the model is one of the methods used to improve its inference latency. However, state-of-the-art has not been able to fully quantize the model. In this work, we improve upon the inference latency of the state-of-the-art methods by removing the floating-point operations, which are associated with the GELU activation in Swin Transformer. While previous work proposed to replace the non-integer operations with linear approximation functions, we propose to replace GELU with ReLU activation. The advantage of ReLU over previous methods is its low memory and computation complexity. We use iterative knowledge distillation to compensate for the lost accuracy due to replacing GELU with ReLU. We quantize our GELU-less SWIN transformer and sh
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#20027;&#35201;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#24037;&#20316;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01166</link><description>&lt;p&gt;
&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on 3D Content Generation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#20027;&#35201;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#24037;&#20316;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20855;&#26377;&#22810;&#31181;&#36755;&#20837;&#27169;&#24577;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#19977;&#32500;&#12290;&#19977;&#32500;&#26159;&#26368;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#19977;&#32500;&#29615;&#22659;&#30340;&#21487;&#35270;&#27169;&#24577;&#65292;&#24182;&#20855;&#26377;&#24040;&#22823;&#30340;&#30693;&#35782;&#37327;&#12290;&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#19981;&#20165;&#20855;&#26377;&#23398;&#26415;&#21644;&#23454;&#36341;&#20215;&#20540;&#65292;&#32780;&#19988;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25972;&#21512;&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#19977;&#32500;&#26412;&#26426;&#29983;&#25104;&#26041;&#27861;&#12289;&#22522;&#20110;&#20108;&#32500;&#20808;&#39564;&#30340;&#19977;&#32500;&#29983;&#25104;&#26041;&#27861;&#21644;&#28151;&#21512;&#19977;&#32500;&#29983;&#25104;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#28085;&#30422;&#20102;&#32422;60&#31687;&#28085;&#30422;&#20027;&#35201;&#25216;&#26415;&#30340;&#35770;&#25991;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#19977;&#32500;&#20869;&#23481;&#29983;&#25104;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#25361;&#25112;&#21644;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#37197;&#21512;&#26412;&#32508;&#36848;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#39033;&#30446;&#32593;&#31449;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#30456;&#20851;&#36164;&#28304;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed remarkable advances in artificial intelligence generated content(AIGC), with diverse input modalities, e.g., text, image, video, audio and 3D. The 3D is the most close visual modality to real-world 3D environment and carries enormous knowledge. The 3D content generation shows both academic and practical values while also presenting formidable technical challenges. This review aims to consolidate developments within the burgeoning domain of 3D content generation. Specifically, a new taxonomy is proposed that categorizes existing approaches into three types: 3D native generative methods, 2D prior-based 3D generative methods, and hybrid 3D generative methods. The survey covers approximately 60 papers spanning the major techniques. Besides, we discuss limitations of current 3D content generation techniques, and point out open challenges as well as promising directions for future work. Accompanied with this survey, we have established a project website where the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ReEvo&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#27714;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#35774;&#35745;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;&#28436;&#21270;&#25628;&#32034;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25628;&#32034;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01145</link><description>&lt;p&gt;
ReEvo&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#21453;&#24605;&#28436;&#21270;&#30340;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ReEvo&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#27714;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#35774;&#35745;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;&#28436;&#21270;&#25628;&#32034;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25628;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NP&#22256;&#38590;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#25512;&#21160;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#35797;&#38169;&#24335;&#21551;&#21457;&#24335;&#35774;&#35745;&#36807;&#31243;&#12290;&#35774;&#35745;&#33258;&#21160;&#21270;&#30340;&#38271;&#26399;&#21162;&#21147;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23835;&#36215;&#32780;&#33719;&#24471;&#26032;&#30340;&#21160;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;LHHs&#65289;&#65292;&#23427;&#26159;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#26032;&#21464;&#20307;&#65292;&#21033;&#29992;LLM&#36827;&#34892;&#21551;&#21457;&#24335;&#29983;&#25104;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20154;&#24037;&#24178;&#39044;&#21644;&#24320;&#25918;&#24335;&#30340;&#21551;&#21457;&#24335;&#31354;&#38388;&#12290;&#20026;&#20102;&#22686;&#24378;LHHs&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#24605;&#28436;&#21270;&#65288;ReEvo&#65289;&#65306;&#19968;&#31181;&#36890;&#29992;&#30340;&#25628;&#32034;&#26694;&#26550;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#19987;&#23478;&#30340;&#21453;&#24605;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;LLM&#25512;&#29702;&#12289;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#39046;&#22495;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#36827;&#21270;&#25628;&#32034;&#25216;&#26415;&#36828;&#36828;&#36229;&#36234;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#22312;12&#20010;&#32452;&#21512;&#20248;&#21270;&#35774;&#32622;&#30340;&#35780;&#20272;&#20013;&#26174;&#31034;&#65306;1)&#28436;&#21270;&#30340;&#21475;&#22836;&#21453;&#24605;&#23548;&#33268;&#26356;&#24179;&#28369;&#30340;&#36866;&#24212;&#24230;&#22320;&#24418;&#12289;&#40657;&#30418;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#35774;&#32622;&#30340;&#26126;&#30830;&#25512;&#29702;&#20197;&#21450;&#26356;&#22909;&#30340;&#25628;&#32034;&#32467;&#26524;&#65307;2)ReEvo&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20998;&#38047;&#32423;&#20248;&#21270;&#26102;&#38388;&#20869;&#33719;&#24471;&#20102;&#21487;&#38752;&#21644;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The omnipresence of NP-hard combinatorial optimization problems (COPs) compels domain experts to engage in trial-and-error heuristic design process. The long-standing endeavor of design automation has gained new momentum with the rise of large language models (LLMs). This paper introduces Language Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages LLMs for heuristic generation, featuring minimal manual intervention and open-ended heuristic spaces. To empower LHHs, we present Reflective Evolution (ReEvo), a generic searching framework that emulates the reflective design approach of human experts while far surpassing human capabilities with its scalable LLM inference, Internet-scale domain knowledge, and powerful evolutionary search. Evaluations across 12 COP settings show that 1) verbal reflections for evolution lead to smoother fitness landscapes, explicit inference of black-box COP settings, and better search results; 2) heuristics generated by ReEvo in mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01143</link><description>&lt;p&gt;
&#29992;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#32593;&#32476;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Network Representations with Disentangled Graph Auto-Encoder
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
(&#21464;&#20998;)&#22270;&#33258;&#32534;&#30721;&#22120;&#24191;&#27867;&#29992;&#20110;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#24418;&#25104;&#26159;&#19968;&#20010;&#30001;&#28508;&#22312;&#22240;&#32032;&#24433;&#21709;&#30340;&#22797;&#26434;&#21644;&#24322;&#36136;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;&#22522;&#26412;&#19978;&#26159;&#25972;&#20307;&#30340;&#65292;&#24573;&#35270;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#32416;&#32544;&#12290;&#36825;&#19981;&#20165;&#20351;&#24471;&#22270;&#20998;&#26512;&#20219;&#21153;&#19981;&#22826;&#26377;&#25928;&#65292;&#32780;&#19988;&#20351;&#24471;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#34920;&#31034;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#29992;(&#21464;&#20998;)&#22270;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#35299;&#32544;&#30340;&#22270;&#34920;&#31034;&#38754;&#20020;&#30528;&#37325;&#35201;&#25361;&#25112;&#65292;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#35299;&#32544;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#20351;&#29992;&#22810;&#36890;&#36947;&#28040;&#24687;&#20256;&#36882;&#23618;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#32858;&#21512;&#19982;&#27599;&#20010;&#33410;&#28857;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The (variational) graph auto-encoder is extensively employed for learning representations of graph-structured data. However, the formation of real-world graphs is a complex and heterogeneous process influenced by latent factors. Existing encoders are fundamentally holistic, neglecting the entanglement of latent factors. This not only makes graph analysis tasks less effective but also makes it harder to understand and explain the representations. Learning disentangled graph representations with (variational) graph auto-encoder poses significant challenges, and remains largely unexplored in the existing literature. In this article, we introduce the Disentangled Graph Auto-Encoder (DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that leverage generative models to learn disentangled representations. Specifically, we first design a disentangled graph convolutional network with multi-channel message-passing layers, as the encoder aggregating information related to eac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;Granger&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24494;&#26381;&#21153;&#20013;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01140</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;Granger&#22240;&#26524;&#21457;&#29616;&#30340;&#24494;&#26381;&#21153;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Root Cause Analysis In Microservice Using Neural Granger Causal Discovery
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01140
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;Granger&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24494;&#26381;&#21153;&#20013;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#32500;&#25252;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#24494;&#26381;&#21153;&#22312;IT&#36816;&#33829;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#31995;&#32479;&#25925;&#38556;&#26102;&#65292;&#31449;&#28857;&#21487;&#38752;&#24615;&#24037;&#31243;&#24072;&#24456;&#38590;&#25214;&#21040;&#26681;&#26412;&#21407;&#22240;&#65292;&#22240;&#20026;&#24494;&#26381;&#21153;&#20013;&#23384;&#22312;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#32467;&#26500;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;PC&#31639;&#27861;&#65289;&#26469;&#24314;&#31435;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#20174;&#22240;&#26524;&#22270;&#20013;&#24471;&#20986;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26102;&#38388;&#39034;&#24207;&#65292;&#24182;&#26410;&#21033;&#29992;&#26102;&#38388;&#20851;&#31995;&#20013;&#34164;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#22312;CPU&#21033;&#29992;&#29575;&#31361;&#28982;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#20854;&#20182;&#24494;&#26381;&#21153;&#30340;&#24310;&#36831;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;CPU&#21033;&#29992;&#29575;&#24322;&#24120;&#21457;&#29983;&#22312;&#24310;&#36831;&#22686;&#21152;&#20043;&#21069;&#65292;&#32780;&#19981;&#26159;&#21516;&#26102;&#21457;&#29983;&#12290;&#32467;&#26524;&#65292;PC&#31639;&#27861;&#26080;&#27861;&#25429;&#25417;&#36825;&#26679;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RUN&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, microservices have gained widespread adoption in IT operations due to their scalability, maintenance, and flexibility. However, it becomes challenging for site reliability engineers (SREs) to pinpoint the root cause due to the complex relationships in microservices when facing system malfunctions. Previous research employed structured learning methods (e.g., PC-algorithm) to establish causal relationships and derive root causes from causal graphs. Nevertheless, they ignored the temporal order of time series data and failed to leverage the rich information inherent in the temporal relationships. For instance, in cases where there is a sudden spike in CPU utilization, it can lead to an increase in latency for other microservices. However, in this scenario, the anomaly in CPU utilization occurs before the latency increase, rather than simultaneously. As a result, the PC-algorithm fails to capture such characteristics. To address these challenges, we propose RUN, a novel a
&lt;/p&gt;</description></item><item><title>DeepAAT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#26080;&#20154;&#26426;&#24433;&#20687;AAT&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#36890;&#36807;&#32771;&#34385;&#24433;&#20687;&#30340;&#31354;&#38388;&#21644;&#20809;&#35889;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;AAT&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01134</link><description>&lt;p&gt;
DeepAAT: &#24555;&#36895;&#26080;&#20154;&#26426;&#22320;&#22270;&#21046;&#20316;&#30340;&#28145;&#24230;&#33258;&#21160;&#33322;&#31354;&#19977;&#35282;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
DeepAAT: Deep Automated Aerial Triangulation for Fast UAV-based Mapping
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01134
&lt;/p&gt;
&lt;p&gt;
DeepAAT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#26080;&#20154;&#26426;&#24433;&#20687;AAT&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#36890;&#36807;&#32771;&#34385;&#24433;&#20687;&#30340;&#31354;&#38388;&#21644;&#20809;&#35889;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;AAT&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#33322;&#31354;&#19977;&#35282;&#27979;&#37327;&#65288;AAT&#65289;&#26088;&#22312;&#21516;&#26102;&#24674;&#22797;&#22270;&#20687;&#23039;&#24577;&#21644;&#37325;&#24314;&#31232;&#30095;&#28857;&#65292;&#23545;&#22320;&#29699;&#35266;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#20973;&#20511;&#20854;&#22312;&#25668;&#24433;&#27979;&#37327;&#39046;&#22495;&#25968;&#21313;&#24180;&#30340;&#30740;&#31350;&#31215;&#28096;&#65292;AAT&#24050;&#32463;&#21457;&#23637;&#25104;&#20026;&#22823;&#35268;&#27169;&#26080;&#20154;&#26426;&#22320;&#22270;&#21046;&#20316;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#22522;&#26412;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;AAT&#26041;&#27861;&#20173;&#38754;&#20020;&#25928;&#29575;&#20302;&#21644;&#31283;&#20581;&#24615;&#26377;&#38480;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepAAT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#26080;&#20154;&#26426;&#24433;&#20687;AAT&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#12290;DeepAAT&#32771;&#34385;&#20102;&#24433;&#20687;&#30340;&#31354;&#38388;&#21644;&#20809;&#35889;&#29305;&#24449;&#65292;&#22686;&#24378;&#20102;&#35299;&#20915;&#38169;&#35823;&#21305;&#37197;&#23545;&#21644;&#20934;&#30830;&#39044;&#27979;&#22270;&#20687;&#23039;&#24577;&#30340;&#33021;&#21147;&#12290;DeepAAT&#22312;AAT&#30340;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#30830;&#20445;&#20102;&#22330;&#26223;&#30340;&#20840;&#38754;&#35206;&#30422;&#21644;&#31934;&#24230;&#12290;&#20854;&#22788;&#29702;&#36895;&#24230;&#27604;&#22686;&#37327;AAT&#26041;&#27861;&#24555;&#20960;&#30334;&#20493;&#65292;&#27604;&#20840;&#23616;AAT&#26041;&#27861;&#24555;&#20960;&#21313;&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#30340;&#37325;&#24314;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Aerial Triangulation (AAT), aiming to restore image pose and reconstruct sparse points simultaneously, plays a pivotal role in earth observation. With its rich research heritage spanning several decades in photogrammetry, AAT has evolved into a fundamental process widely applied in large-scale Unmanned Aerial Vehicle (UAV) based mapping. Despite its advancements, classic AAT methods still face challenges like low efficiency and limited robustness. This paper introduces DeepAAT, a deep learning network designed specifically for AAT of UAV imagery. DeepAAT considers both spatial and spectral characteristics of imagery, enhancing its capability to resolve erroneous matching pairs and accurately predict image poses. DeepAAT marks a significant leap in AAT's efficiency, ensuring thorough scene coverage and precision. Its processing speed outpaces incremental AAT methods by hundreds of times and global AAT methods by tens of times while maintaining a comparable level of reconstruct
&lt;/p&gt;</description></item><item><title>Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01118</link><description>&lt;p&gt;
Pok\'eLLMon&#65306;&#19968;&#20010;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Pok\'emon&#23545;&#25112;&#30340;&#19982;&#20154;&#31867;&#33021;&#21147;&#30456;&#24403;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01118
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;\textsc{Pok\'eLLMon}&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#65292;&#21516;&#26102;&#20197;Pok\'emon&#23545;&#25112;&#20026;&#20363;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290; \textsc{Pok\'eLLMon}&#30340;&#35774;&#35745;&#37319;&#29992;&#20102;&#19977;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#65288;i&#65289;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#21363;&#26102;&#20351;&#29992;&#20174;&#23545;&#25112;&#20013;&#33719;&#24471;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21453;&#39304;&#26469;&#36880;&#27493;&#23436;&#21892;&#31574;&#30053;&#65307;&#65288;ii&#65289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#65292;&#21363;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#20197;&#23545;&#25239;&#20135;&#29983;&#24187;&#35273;&#29616;&#35937;&#65292;&#24182;&#20351;&#20195;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#21450;&#26102;&#27491;&#30830;&#22320;&#34892;&#21160;&#65307;&#65288;iii&#65289;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#65292;&#20197;&#20943;&#36731;&#20195;&#29702;&#26426;&#22120;&#20154;&#38754;&#23545;&#24378;&#25932;&#26102;&#30340;&#8220;&#24778;&#24908;&#25442;&#25163;&#8221;&#29616;&#35937;&#65292;&#20351;&#20854;&#21487;&#20197;&#36867;&#36991;&#25112;&#26007;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#36827;&#34892;&#30340;&#22312;&#32447;&#23545;&#25112;&#20013;&#65292;\textsc{Pok\'eLLMon}&#37319;&#29992;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#20854;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21644;&#21487;&#29609;&#30340;&#25112;&#26007;&#26085;&#24535;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#25214;&#21040;&#65306;\url{https://gith
&lt;/p&gt;
&lt;p&gt;
We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the \textit{panic switching} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates \textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: \url{https://gith
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#38543;&#26426;&#21270;&#26469;&#38450;&#27490;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#36807;&#25311;&#21512;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#24212;&#29992;&#21452;&#37325;&#38450;&#24481;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#32780;&#19981;&#38477;&#20302;&#20934;&#30830;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01114</link><description>&lt;p&gt;
&#21452;&#37325;&#38450;&#24481;&#65306;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#38543;&#26426;&#21270;&#26469;&#38450;&#27490;&#20165;&#22522;&#20110;&#26631;&#31614;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Double-Dip: Thwarting Label-Only Membership Inference Attacks with Transfer Learning and Randomization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#38543;&#26426;&#21270;&#26469;&#38450;&#27490;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#36807;&#25311;&#21512;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#24212;&#29992;&#21452;&#37325;&#38450;&#24481;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#32780;&#19981;&#38477;&#20302;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#38754;&#23545;&#35757;&#32451;&#26679;&#26412;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#36807;&#25311;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#30340;&#21512;&#36866;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#19968;&#31867;&#38544;&#31169;&#25915;&#20987;&#31216;&#20026;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26088;&#22312;&#30830;&#23450;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#25104;&#21592;&#65289;&#25110;&#19981;&#23646;&#20110;&#65288;&#38750;&#25104;&#21592;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#37325;&#38450;&#24481;&#65292;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#22312;&#19981;&#38477;&#20302;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#65288;&#38454;&#27573;1&#65289;&#32467;&#21512;&#38543;&#26426;&#21270;&#65288;&#38454;&#27573;2&#65289;&#26469;&#38450;&#27490;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#23545;&#36807;&#25311;&#21512;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32771;&#23519;&#20102;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#20849;&#20139;&#29305;&#24449;&#31354;&#38388;&#21644;&#21442;&#25968;&#20540;&#12289;&#20923;&#32467;&#23618;&#30340;&#25968;&#37327;&#20197;&#21450;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#65288;&#30446;&#26631;&#65292;&#28304;&#65289;&#25968;&#25454;&#38598;&#23545;&#19978;&#35780;&#20272;&#20102;&#21452;&#37325;&#38450;&#24481;&#65306;&#65288;i&#65289;&#65288;CIFAR-10&#65292;ImageNet&#65289;&#65292;&#65288;ii&#65289;&#65288;GTSRB&#65292;ImageNet&#65289;&#65292;&#65288;iii&#65289;&#65288;CelebA&#65292;VGGFace2&#65289;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22235;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65306;&#65288;a&#65289;VGG-19&#65292;
&lt;/p&gt;
&lt;p&gt;
Transfer learning (TL) has been demonstrated to improve DNN model performance when faced with a scarcity of training samples. However, the suitability of TL as a solution to reduce vulnerability of overfitted DNNs to privacy attacks is unexplored. A class of privacy attacks called membership inference attacks (MIAs) aim to determine whether a given sample belongs to the training dataset (member) or not (nonmember). We introduce Double-Dip, a systematic empirical study investigating the use of TL (Stage-1) combined with randomization (Stage-2) to thwart MIAs on overfitted DNNs without degrading classification accuracy. Our study examines the roles of shared feature space and parameter values between source and target models, number of frozen layers, and complexity of pretrained models. We evaluate Double-Dip on three (Target, Source) dataset paris: (i) (CIFAR-10, ImageNet), (ii) (GTSRB, ImageNet), (iii) (CelebA, VGGFace2). We consider four publicly available pretrained DNNs: (a) VGG-19,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#31639;&#27861;&#65292;&#23558;&#21518;&#24724;&#25511;&#21046;&#22312;$\widetilde{O}(\sqrt{H^3 S^2 ABK})$&#65292;&#25209;&#37327;&#22797;&#26434;&#24230;&#20026;$O(H+\log\log K)$&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#25152;&#26377;&#20855;&#26377;$\widetilde{O}(\sqrt{K})$&#21518;&#24724;&#30028;&#31639;&#27861;&#30340;&#25209;&#37327;&#22797;&#26434;&#24230;&#19979;&#30028;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01111</link><description>&lt;p&gt;
&#22312;&#33258;&#36866;&#24212;&#32422;&#26463;&#19979;&#30340;&#33258;&#23545;&#24328;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#31639;&#27861;&#65292;&#23558;&#21518;&#24724;&#25511;&#21046;&#22312;$\widetilde{O}(\sqrt{H^3 S^2 ABK})$&#65292;&#25209;&#37327;&#22797;&#26434;&#24230;&#20026;$O(H+\log\log K)$&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#25152;&#26377;&#20855;&#26377;$\widetilde{O}(\sqrt{K})$&#21518;&#24724;&#30028;&#31639;&#27861;&#30340;&#25209;&#37327;&#22797;&#26434;&#24230;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65288;MARL&#65289; - &#36825;&#26159;&#19968;&#31181;&#30001;&#23454;&#38469;&#24212;&#29992;&#39537;&#21160;&#30340;&#26032;&#38382;&#39064;&#65292;&#20854;&#20013;&#37096;&#32626;&#26032;&#31574;&#30053;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#24517;&#39035;&#26368;&#23567;&#21270;&#31574;&#30053;&#26356;&#26032;&#30340;&#27425;&#25968;&#12290;&#23545;&#20110;&#20004;&#20010;&#29609;&#23478;&#30340;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#65288;&#31574;&#30053;&#65289;&#22522;&#20110;&#28040;&#38500;&#30340;&#31639;&#27861;&#65292;&#23427;&#22312;&#21518;&#24724;&#20026;$\widetilde{O}(\sqrt{H^3 S^2 ABK})$&#30340;&#24773;&#20917;&#19979;&#65292;&#25209;&#37327;&#22797;&#26434;&#24230;&#20165;&#20026;$O(H+\log\log K)$&#12290;&#22312;&#19978;&#36848;&#24773;&#20917;&#19979;&#65292;$S$&#34920;&#31034;&#29366;&#24577;&#25968;&#65292;$A&#65292;B$&#20998;&#21035;&#20195;&#34920;&#20004;&#20010;&#29609;&#23478;&#30340;&#34892;&#21160;&#25968;&#65292;$H$&#26159;&#26102;&#38388;&#21608;&#26399;&#65292;$K$&#26159;&#28216;&#25103;&#27425;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#25152;&#26377;&#20855;&#26377;$\widetilde{O}(\sqrt{K})$&#21518;&#24724;&#30028;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#25209;&#37327;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#20026;$\Omega(\frac{H}{\log_{A}K}+\log\log K)$&#65292;&#36825;&#19982;&#25105;&#20204;&#30340;&#19978;&#30028;&#22312;&#23545;&#25968;&#22240;&#23376;&#19978;&#21305;&#37197;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#23398;&#20064;&#36172;&#21338;&#21338;&#24328;&#21644;&#26080;&#22870;&#21169;&#30340;&#36817;&#20046;&#26368;&#20248;&#25209;&#37327;&#22797;&#26434;&#24230;MARL&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20123;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of multi-agent reinforcement learning (MARL) with adaptivity constraints -- a new problem motivated by real-world applications where deployments of new policies are costly and the number of policy updates must be minimized. For two-player zero-sum Markov Games, we design a (policy) elimination based algorithm that achieves a regret of $\widetilde{O}(\sqrt{H^3 S^2 ABK})$, while the batch complexity is only $O(H+\log\log K)$. In the above, $S$ denotes the number of states, $A,B$ are the number of actions for the two players respectively, $H$ is the horizon and $K$ is the number of episodes. Furthermore, we prove a batch complexity lower bound $\Omega(\frac{H}{\log_{A}K}+\log\log K)$ for all algorithms with $\widetilde{O}(\sqrt{K})$ regret bound, which matches our upper bound up to logarithmic factors. As a byproduct, our techniques naturally extend to learning bandit games and reward-free MARL within near optimal batch complexity. To the best of our knowledge, these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#35813;&#32467;&#26500;&#21487;&#20197;&#27169;&#25311;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01107</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#27169;&#25311;&#22270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simulation of Graph Algorithms with Looped Transformers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#35813;&#32467;&#26500;&#21487;&#20197;&#27169;&#25311;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#22270;&#31639;&#27861;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#30001;&#20110;&#26377;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#35777;&#36827;&#23637;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#36827;&#19968;&#27493;&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#33021;&#22815;&#20351;&#29992;&#20851;&#31995;&#25968;&#25454;&#22797;&#21046;&#25512;&#29702;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#26550;&#26500;&#26159;&#19968;&#20010;&#24102;&#39069;&#22806;&#27880;&#24847;&#21147;&#22836;&#21644;&#19982;&#22270;&#24418;&#20132;&#20114;&#30340;&#24490;&#29615;&#21464;&#21387;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#20102;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#27169;&#25311;&#35832;&#22914;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#31561;&#31639;&#27861;&#12290;&#32593;&#32476;&#30340;&#23485;&#24230;&#19981;&#38543;&#36755;&#20837;&#22270;&#30340;&#22823;&#23567;&#22686;&#21152;&#65292;&#36825;&#24847;&#21619;&#30528;&#32593;&#32476;&#21487;&#20197;&#27169;&#25311;&#20219;&#20309;&#22270;&#19978;&#30340;&#19978;&#36848;&#31639;&#27861;&#12290;&#23613;&#31649;&#26377;&#36825;&#20010;&#29305;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#26377;&#19968;&#20010;&#30001;&#20110;&#26377;&#38480;&#31934;&#24230;&#32780;&#21463;&#21040;&#38480;&#21046;&#30340;&#27169;&#25311;&#26497;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The execution of graph algorithms using neural networks has recently attracted significant interest due to promising empirical progress. This motivates further understanding of how neural networks can replicate reasoning steps with relational data. In this work, we study the ability of transformer networks to simulate algorithms on graphs from a theoretical perspective. The architecture that we utilize is a looped transformer with extra attention heads that interact with the graph. We prove by construction that this architecture can simulate algorithms such as Dijkstra's shortest path algorithm, Breadth- and Depth-First Search, and Kosaraju's strongly connected components algorithm. The width of the network does not increase with the size of the input graph, which implies that the network can simulate the above algorithms for any graph. Despite this property, we show that there is a limit to simulation in our solution due to finite precision. Finally, we show a Turing Completeness resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01103</link><description>&lt;p&gt;
&#32452;&#21512;&#29983;&#25104;&#24314;&#27169;&#65306;&#21333;&#19968;&#27169;&#22411;&#24182;&#19981;&#26159;&#24744;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;
&lt;/p&gt;
&lt;p&gt;
Compositional Generative Modeling: A Single Model is Not All You Need
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#24040;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#20027;&#27969;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#24212;&#35813;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#22914;&#20309;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#20998;&#24067;&#65292;&#20351;&#24471;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#20998;&#24067;&#37096;&#20998;&#20063;&#33021;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#35757;&#32451;&#26102;&#23436;&#20840;&#26410;&#35265;&#30340;&#20219;&#21153;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#29420;&#31435;&#30340;&#32452;&#21512;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#21487;&#20449;&#30340;&#20998;&#24067;&#24335;AI&#30340;&#20195;&#34920;&#24615;&#25216;&#26415;&#65292;&#21253;&#25324;&#40065;&#26834;&#24615;&#20445;&#35777;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#20844;&#24179;&#24847;&#35782;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01096</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;&#20998;&#24067;&#24335;AI&#31995;&#32479;&#65306;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#21644;&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#21487;&#20449;&#30340;&#20998;&#24067;&#24335;AI&#30340;&#20195;&#34920;&#24615;&#25216;&#26415;&#65292;&#21253;&#25324;&#40065;&#26834;&#24615;&#20445;&#35777;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#20844;&#24179;&#24847;&#35782;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;AI&#31995;&#32479;&#27491;&#22312;&#38761;&#26032;&#22823;&#25968;&#25454;&#35745;&#31639;&#21644;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#32463;&#27982;&#21644;&#31038;&#20250;&#20135;&#29983;&#36234;&#26469;&#36234;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;AI&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#23548;&#33268;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#21644;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#40065;&#26834;&#24615;&#20445;&#35777;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#20844;&#24179;&#24847;&#35782;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#22238;&#39038;&#20102;&#20195;&#34920;&#24615;&#30340;&#25216;&#26415;&#12289;&#31639;&#27861;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#20197;&#23454;&#29616;&#21487;&#20449;&#30340;&#20998;&#24067;&#24335;AI&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#26367;&#20195;&#26550;&#26500;&#30340;&#31616;&#35201;&#27010;&#36848;&#65292;&#35752;&#35770;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;AI&#31639;&#27861;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#30340;&#22266;&#26377;&#28431;&#27934;&#65292;&#24182;&#20998;&#26512;&#20102;&#20026;&#20160;&#20040;&#36825;&#20123;&#38382;&#39064;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#23384;&#22312;&#65292;&#32780;&#19981;&#31649;&#20855;&#20307;&#30340;&#26550;&#26500;&#22914;&#20309;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#21487;&#20449;&#20998;&#24067;&#24335;AI&#30340;&#29420;&#29305;&#20998;&#31867;&#65292;&#28085;&#30422;&#20102;&#23545;&#25512;&#29702;&#20013;&#30340;&#36867;&#36991;&#25915;&#20987;&#21644;&#19981;&#35268;&#21017;&#26597;&#35810;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#23545;&#20013;&#27602;&#25915;&#20987;&#21644;&#25968;&#25454;&#27844;&#38706;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging Distributed AI systems are revolutionizing big data computing and data processing capabilities with growing economic and societal impact. However, recent studies have identified new attack surfaces and risks caused by security, privacy, and fairness issues in AI systems. In this paper, we review representative techniques, algorithms, and theoretical foundations for trustworthy distributed AI through robustness guarantee, privacy protection, and fairness awareness in distributed learning. We first provide a brief overview of alternative architectures for distributed learning, discuss inherent vulnerabilities for security, privacy, and fairness of AI algorithms in distributed learning, and analyze why these problems are present in distributed learning regardless of specific architectures. Then we provide a unique taxonomy of countermeasures for trustworthy distributed AI, covering (1) robustness to evasion attacks and irregular queries at inference, and robustness to poisoning a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#36866;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;MSV&#30340;&#25968;&#37327;&#19982;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01095</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20351;&#29992;&#22810;&#23569;&#20010;&#35270;&#22270;&#65311;
&lt;/p&gt;
&lt;p&gt;
How many views does your deep neural network use for prediction?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#36866;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;MSV&#30340;&#25968;&#37327;&#19982;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#35768;&#22810;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20294;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26368;&#36817;&#65292;Allen-Zhu&#21644;Li&#65288;2023&#65289;&#24341;&#20837;&#20102;&#22810;&#35270;&#22270;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;DNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20182;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#38598;&#25104;&#25110;&#33976;&#39311;&#27169;&#22411;&#65292;&#24182;&#26410;&#35752;&#35770;&#29992;&#20110;&#29305;&#23450;&#36755;&#20837;&#39044;&#27979;&#30340;&#22810;&#35270;&#22270;&#20272;&#35745;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#65292;&#23427;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#30495;&#23454;&#22270;&#20687;&#12290;MSVs&#26159;&#36755;&#20837;&#20013;&#30340;&#19968;&#32452;&#26368;&#23567;&#19988;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#27599;&#20010;&#29305;&#24449;&#20445;&#30041;&#20102;&#27169;&#22411;&#23545;&#35813;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#27169;&#22411;&#65288;&#21253;&#25324;&#21367;&#31215;&#21644;&#36716;&#25442;&#27169;&#22411;&#65289;&#30340;MSV&#25968;&#37327;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#26126;&#30830;&#30340;&#20851;&#31995;&#65292;&#36825;&#34920;&#26126;&#22810;&#35270;&#22270;&#30340;&#35282;&#24230;&#23545;&#20110;&#29702;&#35299;&#65288;&#38750;&#38598;&#25104;&#25110;&#38750;&#33976;&#39311;&#65289;DNN&#30340;&#27867;&#21270;&#33021;&#21147;&#20063;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization ability of Deep Neural Networks (DNNs) is still not fully understood, despite numerous theoretical and empirical analyses. Recently, Allen-Zhu &amp; Li (2023) introduced the concept of multi-views to explain the generalization ability of DNNs, but their main target is ensemble or distilled models, and no method for estimating multi-views used in a prediction of a specific input is discussed. In this paper, we propose Minimal Sufficient Views (MSVs), which is similar to multi-views but can be efficiently computed for real images. MSVs is a set of minimal and distinct features in an input, each of which preserves a model's prediction for the input. We empirically show that there is a clear relationship between the number of MSVs and prediction accuracy across models, including convolutional and transformer models, suggesting that a multi-view like perspective is also important for understanding the generalization ability of (non-ensemble or non-distilled) DNNs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#32972;&#26223;&#20171;&#32461;&#12289;&#25968;&#23398;&#23450;&#20041;&#12289;&#20998;&#31867;&#24635;&#32467;&#12289;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01077</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#39044;&#27979;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Predictive Modeling with Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#32972;&#26223;&#20171;&#32461;&#12289;&#25968;&#23398;&#23450;&#20041;&#12289;&#20998;&#31867;&#24635;&#32467;&#12289;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#20351;&#24471;&#22823;&#37327;&#30340;&#25968;&#23383;&#21270;&#24739;&#32773;&#25968;&#25454;&#24471;&#20197;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#21033;&#29992;EHR&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#21253;&#25324;&#21307;&#30103;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22522;&#20110;EHR&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;EHR&#25968;&#25454;&#30340;&#32972;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#27979;&#24314;&#27169;&#20219;&#21153;&#30340;&#25968;&#23398;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#39044;&#27979;&#28145;&#24230;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19982;&#21307;&#30103;&#39044;&#27979;&#24314;&#27169;&#30456;&#20851;&#30340;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of electronic health records (EHR) systems has enabled the collection of a vast amount of digitized patient data. However, utilizing EHR data for predictive modeling presents several challenges due to its unique characteristics. With the advancements in machine learning techniques, deep learning has demonstrated its superiority in various applications, including healthcare. This survey systematically reviews recent advances in deep learning-based predictive models using EHR data. Specifically, we begin by introducing the background of EHR data and providing a mathematical definition of the predictive modeling task. We then categorize and summarize predictive deep models from multiple perspectives. Furthermore, we present benchmarks and toolkits relevant to predictive modeling in healthcare. Finally, we conclude this survey by discussing open challenges and suggesting promising directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#21407;&#22987;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01065</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#35821;&#35328;&#25991;&#26723;&#38382;&#31572;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#21407;&#22987;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#21407;&#22987;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread adoption of Large Language Models (LLMs), in this paper we investigate the multilingual capability of these models. Our preliminary results show that, translating the native language context, question and answer into a high resource language produced the best results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21452;&#37325;&#30446;&#26631;&#23545;&#35805;&#35774;&#32622;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#21010;&#39537;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20219;&#24847;&#35745;&#21010;&#19978;&#22522;&#30784;&#23545;&#35805;&#65292;&#20027;&#21160;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#35745;&#21010;&#65292;&#24182;&#22312;&#31995;&#32479;&#34892;&#20026;&#19978;&#23454;&#26045;&#23433;&#20840;&#38450;&#25252;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01053</link><description>&lt;p&gt;
&#38754;&#21521;&#21452;&#37325;&#30446;&#26631;&#23545;&#35805;&#35774;&#32622;&#30340;&#35745;&#21010;&#39537;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Plan-Grounded Large Language Models for Dual Goal Conversational Settings
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21452;&#37325;&#30446;&#26631;&#23545;&#35805;&#35774;&#32622;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#21010;&#39537;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20219;&#24847;&#35745;&#21010;&#19978;&#22522;&#30784;&#23545;&#35805;&#65292;&#20027;&#21160;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#35745;&#21010;&#65292;&#24182;&#22312;&#31995;&#32479;&#34892;&#20026;&#19978;&#23454;&#26045;&#23433;&#20840;&#38450;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20026;LLM&#25552;&#20379;&#20805;&#36275;&#30340;&#33021;&#21147;&#20197;&#27969;&#21033;&#22320;&#36827;&#34892;&#23545;&#35805;&#24182;&#19982;&#20154;&#31867;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22312;&#21452;&#21521;&#23545;&#35805;&#27969;&#21160;&#25351;&#20196;&#30340;&#28151;&#21512;&#20513;&#35758;&#35774;&#32622;&#20013;&#65292;LLM&#22914;&#20309;&#24341;&#23548;&#20197;&#35745;&#21010;&#20026;&#22522;&#30784;&#30340;&#23545;&#35805;&#36824;&#19981;&#23436;&#20840;&#28165;&#26970;&#65292;&#21363;LLM&#21644;&#29992;&#25143;&#24444;&#27492;&#25552;&#20379;&#25351;&#20196;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21452;&#37325;&#30446;&#26631;&#28151;&#21512;&#20513;&#35758;&#23545;&#35805;&#35774;&#32622;&#30340;&#38382;&#39064;&#65292;LLM&#19981;&#20165;&#22312;&#20219;&#24847;&#35745;&#21010;&#19978;&#22522;&#30784;&#23545;&#35805;&#65292;&#36824;&#33268;&#21147;&#20110;&#28385;&#36275;&#27969;&#31243;&#35745;&#21010;&#21644;&#29992;&#25143;&#25351;&#20196;&#12290;LLM&#36127;&#36131;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#35745;&#21010;&#65292;&#21516;&#26102;&#36866;&#24212;&#26032;&#30340;&#24773;&#20917;&#65292;&#22238;&#31572;&#38382;&#39064;&#65292;&#24182;&#22312;&#38656;&#35201;&#26102;&#28608;&#27963;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#65292;&#23427;&#22522;&#20110;&#27969;&#31243;&#35745;&#21010;&#26469;&#36827;&#34892;&#23545;&#35805;&#65292;&#21487;&#20197;&#20027;&#21160;&#21442;&#19982;&#23545;&#35805;&#65292;&#24182;&#22312;&#31995;&#32479;&#34892;&#20026;&#19978;&#23454;&#26045;&#23433;&#20840;&#38450;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training Large Language Models (LLMs) to follow user instructions has been shown to supply the LLM with ample capacity to converse fluently while being aligned with humans. Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings where instructions flow in both directions of the conversation, i.e. both the LLM and the user provide instructions to one another. In this paper, we tackle a dual goal mixed-initiative conversational setting where the LLM not only grounds the conversation on an arbitrary plan but also seeks to satisfy both a procedural plan and user instructions. The LLM is then responsible for guiding the user through the plan and, at the same time, adapting to new circumstances, answering questions, and activating safety guardrails when needed. We propose a novel LLM that grounds the dialogue on a procedural plan, can take the dialogue initiative, and enforces guardrails on the system's behavior, while also improving the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01032</link><description>&lt;p&gt;
&#36319;&#30528;&#25105;&#37325;&#22797;&#65306;Transformer&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#27604;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Repeat After Me: Transformers are Better than State Space Models at Copying
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#20027;&#35201;&#26550;&#26500;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;&#19981;&#20381;&#36182;&#20110;&#24207;&#21015;&#38271;&#24230;&#30340;&#22266;&#23450;&#22823;&#23567;&#28508;&#22312;&#29366;&#24577;&#30340;&#27169;&#22411;&#65292;&#20063;&#23601;&#26159;"&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;" (GSSMs)&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;GSSMs&#22312;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#19978;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#38656;&#35201;&#20174;&#36755;&#20837;&#19978;&#19979;&#25991;&#22797;&#21046;&#30340;&#20219;&#21153;&#19978;&#65292;&#23427;&#20204;&#30456;&#23545;&#20110;transformer&#27169;&#22411;&#26469;&#35828;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#20174;&#23545;&#31616;&#21333;&#30340;&#23383;&#31526;&#20018;&#22797;&#21046;&#20219;&#21153;&#30340;&#29702;&#35770;&#20998;&#26512;&#24320;&#22987;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#20004;&#23618;&#30340;transformer&#21487;&#20197;&#22797;&#21046;&#25351;&#25968;&#38271;&#24230;&#30340;&#23383;&#31526;&#20018;&#65292;&#32780;GSSMs&#30001;&#20110;&#20854;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#29366;&#24577;&#22312;&#26681;&#26412;&#19978;&#26159;&#26377;&#38480;&#21046;&#30340;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;transformer&#22312;&#38656;&#35201;&#22797;&#21046;&#19978;&#19979;&#25991;&#30340;&#21512;&#25104;&#20219;&#21153;&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#20248;&#20110;GSSMs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#36828;&#36828;&#20248;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;</title><link>https://rss.arxiv.org/abs/2402.01030</link><description>&lt;p&gt;
&#21487;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#33021;&#22815;&#28608;&#21457;&#26356;&#20986;&#33394;&#30340;LLM&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Executable Code Actions Elicit Better LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01030
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20307;&#20855;&#22791;&#25191;&#34892;&#24191;&#27867;&#34892;&#21160;&#30340;&#33021;&#21147;&#65292;&#22914;&#35843;&#29992;&#24037;&#20855;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#31561;&#65292;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;LLM&#26234;&#33021;&#20307;&#36890;&#24120;&#36890;&#36807;&#29983;&#25104;JSON&#25110;&#25991;&#26412;&#30340;&#39044;&#23450;&#20041;&#26684;&#24335;&#26469;&#20135;&#29983;&#34892;&#21160;&#65292;&#36825;&#36890;&#24120;&#21463;&#38480;&#20110;&#21463;&#38480;&#21046;&#30340;&#34892;&#21160;&#31354;&#38388;&#65288;&#20363;&#22914;&#65292;&#39044;&#23450;&#20041;&#24037;&#20855;&#30340;&#33539;&#22260;&#65289;&#21644;&#21463;&#38480;&#30340;&#28789;&#27963;&#24615;&#65288;&#20363;&#22914;&#65292;&#26080;&#27861;&#32452;&#21512;&#22810;&#20010;&#24037;&#20855;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#23558;LLM&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65288;CodeAct&#65289;&#12290;CodeAct&#19982;Python&#35299;&#37322;&#22120;&#38598;&#25104;&#65292;&#21487;&#20197;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#22312;&#26032;&#30340;&#35266;&#23519;&#20013;&#21160;&#24577;&#20462;&#35746;&#20808;&#21069;&#30340;&#34892;&#21160;&#25110;&#21457;&#20986;&#26032;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#23545;17&#20010;LLM&#22312;API-Bank&#21644;&#26032;&#32534;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;CodeAct&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#65288;&#25104;&#21151;&#29575;&#39640;&#20986;20%&#65289;&#12290;CodeAct&#30340;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#28608;&#21169;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;ologs&#21644;&#25509;&#32447;&#22270;&#30340;&#27010;&#24565;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#37327;&#21270;&#27010;&#24565;&#31867;&#27604;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#24418;&#25104;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#22270;&#35770;&#21644;&#33539;&#30068;&#35770;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#21644;&#25805;&#20316;&#65292;&#21516;&#26102;&#20351;&#29992;&#25509;&#32447;&#22270;&#25805;&#20316;&#23450;&#20041;&#20102;&#24230;&#37327;&#21644;&#22270;&#32534;&#36753;&#36317;&#31163;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01020</link><description>&lt;p&gt;
&#36890;&#36807;ologs&#21644;&#25509;&#32447;&#22270;&#37327;&#21270;&#27010;&#24565;&#30340;&#31867;&#27604;
&lt;/p&gt;
&lt;p&gt;
Quantifying analogy of concepts via ologs and wiring diagrams
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;ologs&#21644;&#25509;&#32447;&#22270;&#30340;&#27010;&#24565;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#37327;&#21270;&#27010;&#24565;&#31867;&#27604;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#24418;&#25104;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#22270;&#35770;&#21644;&#33539;&#30068;&#35770;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#21644;&#25805;&#20316;&#65292;&#21516;&#26102;&#20351;&#29992;&#25509;&#32447;&#22270;&#25805;&#20316;&#23450;&#20041;&#20102;&#24230;&#37327;&#21644;&#22270;&#32534;&#36753;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;Spivak&#21644;Kent&#21019;&#24314;&#30340;&#26412;&#20307;&#26085;&#24535;(ologs)&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#31216;&#20026;&#25509;&#32447;&#22270;&#30340;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25509;&#32447;&#22270;&#26159;&#19968;&#20010;&#26377;&#38480;&#30340;&#26377;&#21521;&#26631;&#35760;&#22270;&#12290;&#26631;&#35760;&#23545;&#24212;&#20110;olog&#20013;&#30340;&#31867;&#22411;&#65307;&#23427;&#20204;&#20063;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#33258;&#20027;&#31995;&#32479;&#20013;&#20256;&#24863;&#22120;&#30340;&#35835;&#25968;&#12290;&#22240;&#27492;&#65292;&#25509;&#32447;&#22270;&#21487;&#20197;&#29992;&#20316;&#33258;&#20027;&#31995;&#32479;&#24418;&#25104;&#25277;&#35937;&#27010;&#24565;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#39592;&#26550;&#25509;&#32447;&#22270;&#30340;&#22270;&#24418;&#24418;&#25104;&#19968;&#20010;&#33539;&#30068;&#12290;&#36825;&#20351;&#24471;&#39592;&#26550;&#25509;&#32447;&#22270;&#21487;&#20197;&#20351;&#29992;&#22270;&#35770;&#21644;&#33539;&#30068;&#35770;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#21644;&#25805;&#20316;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#20165;&#36866;&#29992;&#20110;&#25509;&#32447;&#22270;&#30340;&#25805;&#20316;&#23558;&#20256;&#32479;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#23450;&#20041;&#25193;&#23637;&#21040;&#25509;&#32447;&#22270;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#25152;&#26377;&#39592;&#26550;&#25509;&#32447;&#22270;&#38598;&#21512;&#19978;&#30340;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#30001;&#25509;&#32447;&#22270;&#34920;&#31034;&#30340;&#20004;&#20010;&#27010;&#24565;&#20043;&#38388;&#36317;&#31163;&#30340;&#25193;&#23637;&#31034;&#20363;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#20219;&#20309;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build on the theory of ontology logs (ologs) created by Spivak and Kent, and define a notion of wiring diagrams. In this article, a wiring diagram is a finite directed labelled graph. The labels correspond to types in an olog; they can also be interpreted as readings of sensors in an autonomous system. As such, wiring diagrams can be used as a framework for an autonomous system to form abstract concepts. We show that the graphs underlying skeleton wiring diagrams form a category. This allows skeleton wiring diagrams to be compared and manipulated using techniques from both graph theory and category theory. We also extend the usual definition of graph edit distance to the case of wiring diagrams by using operations only available to wiring diagrams, leading to a metric on the set of all skeleton wiring diagrams. In the end, we give an extended example on calculating the distance between two concepts represented by wiring diagrams, and explain how to apply our framework to any applica
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#24182;&#35299;&#20915;&#20102;AI&#29983;&#25104;&#30340;&#38754;&#23380;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#22120;&#29992;&#20110;&#39044;&#27979;&#38754;&#37096;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01002</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#38754;&#23380;&#25670;&#33073;&#20102;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
AI-generated faces free from racial and gender stereotypes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01002
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#24182;&#35299;&#20915;&#20102;AI&#29983;&#25104;&#30340;&#38754;&#23380;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#22120;&#29992;&#20110;&#39044;&#27979;&#38754;&#37096;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;Stable Diffusion&#20043;&#31867;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;AI&#27169;&#22411;&#27599;&#22825;&#37117;&#34987;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20154;&#23545;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#25918;&#22823;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#25552;&#20986;&#20102;&#20851;&#20999;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#20219;&#24847;&#32473;&#23450;&#38754;&#37096;&#22270;&#20687;&#30340;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#24180;&#40836;&#32452;&#65292;&#24182;&#23637;&#31034;&#20854;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#36825;&#20010;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#23545;Stable Diffusion&#22312;&#20845;&#31181;&#31181;&#26063;&#12289;&#20004;&#31181;&#24615;&#21035;&#12289;&#20116;&#20010;&#24180;&#40836;&#32452;&#12289;32&#20010;&#32844;&#19994;&#21644;&#20843;&#20010;&#23646;&#24615;&#19978;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;&#26367;&#20195;&#26041;&#26696;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;Stable Diffusion&#22312;&#25551;&#32472;&#21516;&#19968;&#31181;&#26063;&#30340;&#20010;&#20307;&#26102;&#30456;&#20284;&#31243;&#24230;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#20986;&#39640;&#24230;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#20363;&#22914;&#65292;&#23558;&#22823;&#22810;&#25968;&#20013;&#19996;&#30007;&#24615;&#25551;&#32472;&#20026;&#30382;&#32932;&#40669;&#40657;&#12289;&#30041;&#30528;&#32993;&#23376;&#12289;&#25140;&#30528;&#20256;&#32479;&#22836;&#39280;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#22686;&#21152;&#38754;&#37096;&#22810;&#26679;&#24615;&#30340;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative AI models such as Stable Diffusion are used daily by millions worldwide. However, many have raised concerns regarding how these models amplify racial and gender stereotypes. To study this phenomenon, we develop a classifier to predict the race, gender, and age group of any given face image, and show that it achieves state-of-the-art performance. Using this classifier, we quantify biases in Stable Diffusion across six races, two genders, five age groups, 32 professions, and eight attributes. We then propose novel debiasing solutions that outperform state-of-the-art alternatives. Additionally, we examine the degree to which Stable Diffusion depicts individuals of the same race as being similar to one another. This analysis reveals a high degree of stereotyping, e.g., depicting most middle eastern males as being dark-skinned, bearded, and wearing a traditional headdress. We address these limitations by proposing yet another novel solution that increases facial div
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#19978;&#19979;&#25991;&#23545;&#36755;&#20986;&#30340;&#24433;&#21709;&#20943;&#23567;&#65292;&#21516;&#26102;&#35821;&#20041;&#21547;&#20041;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00978</link><description>&lt;p&gt;
&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
An Information-Theoretic Approach to Analyze NLP Classification Tasks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#19978;&#19979;&#25991;&#23545;&#36755;&#20986;&#30340;&#24433;&#21709;&#20943;&#23567;&#65292;&#21516;&#26102;&#35821;&#20041;&#21547;&#20041;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#36755;&#20837;&#23545;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#23545;&#35768;&#22810;&#20219;&#21153;&#37117;&#26377;&#24110;&#21161;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#36755;&#20837;&#30340;&#24433;&#21709;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#37319;&#29992;&#21333;&#20010;&#20803;&#32032;&#36755;&#20837;&#25110;&#22810;&#20010;&#20803;&#32032;&#36755;&#20837;&#26469;&#39044;&#27979;&#36755;&#20986;&#21464;&#37327;&#65292;&#20854;&#20013;&#19968;&#20010;&#20803;&#32032;&#26159;&#19968;&#27573;&#25991;&#23383;&#12290;&#27599;&#20010;&#25991;&#23383;&#20803;&#32032;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#20851;&#32852;&#30340;&#35821;&#20041;&#21547;&#20041;&#21644;&#35821;&#35328;&#23454;&#29616;&#12290;&#36873;&#25321;&#20102;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#65288;MCRC&#65289;&#21644;&#24773;&#24863;&#20998;&#31867;&#65288;SC&#65289;&#26469;&#23637;&#31034;&#35813;&#26694;&#26550;&#12290;&#23545;&#20110;MCRC&#65292;&#21457;&#29616;&#30456;&#23545;&#20110;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#19978;&#19979;&#25991;&#23545;&#36755;&#20986;&#30340;&#24433;&#21709;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#20943;&#23567;&#12290;&#29305;&#21035;&#26159;&#65292;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#19978;&#19979;&#25991;&#20801;&#35768;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#26356;&#22823;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#27979;&#35797;&#21019;&#24314;&#32773;&#22312;&#35774;&#35745;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#26102;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#19978;&#19979;&#25991;&#30340;&#36873;&#25321;&#12290;&#23545;&#20110;SC&#65292;&#21457;&#29616;&#35821;&#20041;&#21547;&#20041;&#23545;&#20110;&#36755;&#20986;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the importance of the inputs on the output is useful across many tasks. This work provides an information-theoretic framework to analyse the influence of inputs for text classification tasks. Natural language processing (NLP) tasks take either a single element input or multiple element inputs to predict an output variable, where an element is a block of text. Each text element has two components: an associated semantic meaning and a linguistic realization. Multiple-choice reading comprehension (MCRC) and sentiment classification (SC) are selected to showcase the framework. For MCRC, it is found that the context influence on the output compared to the question influence reduces on more challenging datasets. In particular, more challenging contexts allow a greater variation in complexity of questions. Hence, test creators need to carefully consider the choice of the context when designing multiple-choice questions for assessment. For SC, it is found the semantic meaning of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00976</link><description>&lt;p&gt;
&#20855;&#26377;&#21160;&#24577;&#20572;&#27490;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformers with Dynamic Halt
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#22312;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#26041;&#38754;&#30340;&#24402;&#32435;&#20559;&#22909;&#8212;&#8212;&#65288;1&#65289;&#31867;&#20284;&#20110;Universal Transformers&#30340;&#28145;&#24230;&#36880;&#23618;&#24490;&#29615;&#26041;&#27861;&#65307;&#21644;&#65288;2&#65289;&#31867;&#20284;&#20110;Temporal Latent Bottleneck&#30340;&#20998;&#22359;&#26102;&#24577;&#24490;&#29615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#25193;&#23637;&#21644;&#32452;&#21512;&#19978;&#36848;&#26041;&#27861;&#30340;&#26032;&#26041;&#24335;&#65292;&#20363;&#22914;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22343;&#20540;&#30340;Universal Transformer&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#65292;&#24182;&#23558;Universal Transformer&#30340;&#20803;&#32032;&#34701;&#20837;&#21040;Temporal Latent Bottleneck&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;Long Range Arena&#65288;LRA&#65289;&#65292;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#65292;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#27604;&#36739;&#20102;&#27169;&#22411;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38754;&#21521;&#38750;&#31243;&#24207;&#21592;&#29992;&#25143;&#30340;KG&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#21644;GPT&#27169;&#22411;&#29983;&#25104;SPARQL&#26597;&#35810;&#65292;&#20351;&#29992;CWA&#39044;&#35757;&#32451;&#25152;&#26377;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;SPARQL&#21305;&#37197;&#29575;&#20026;62.703%&#12290;</title><link>https://rss.arxiv.org/abs/2402.00969</link><description>&lt;p&gt;
&#20351;&#29992;Entity&#39044;&#35757;&#32451;GPT&#20026;KG&#38382;&#31572;&#29983;&#25104;SPARQL
&lt;/p&gt;
&lt;p&gt;
SPARQL Generation with Entity Pre-trained GPT for KG Question Answering
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38754;&#21521;&#38750;&#31243;&#24207;&#21592;&#29992;&#25143;&#30340;KG&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#21644;GPT&#27169;&#22411;&#29983;&#25104;SPARQL&#26597;&#35810;&#65292;&#20351;&#29992;CWA&#39044;&#35757;&#32451;&#25152;&#26377;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;SPARQL&#21305;&#37197;&#29575;&#20026;62.703%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#30340;&#27969;&#34892;&#24230;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#36805;&#36895;&#22686;&#38271;&#12290;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#20114;&#32852;&#32593;&#19978;&#30340;&#35768;&#22810;&#22312;&#32447;&#25968;&#25454;&#24211;&#26597;&#35810;&#36825;&#20123;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#38750;&#31243;&#24207;&#21592;&#29992;&#25143;&#33021;&#22815;&#35775;&#38382;&#20182;&#20204;&#24819;&#35201;&#30693;&#36947;&#30340;&#20219;&#20309;&#20449;&#24687;&#65292;&#37027;&#23558;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25104;&#23601;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#20184;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21644;&#36890;&#36807;&#35768;&#22810;&#25361;&#25112;&#28608;&#21169;&#21019;&#36896;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37325;&#28857;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19978;&#36827;&#34892;&#27491;&#30830;&#30340;&#23454;&#20307;&#38142;&#25509;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;GPT&#27169;&#22411;&#26469;&#20174;&#20013;&#21019;&#24314;SPARQL&#26597;&#35810;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#36825;&#20010;&#20219;&#21153;&#20013;&#21487;&#33021;&#26368;&#38590;&#20197;&#22312;&#23569;&#25968;&#25110;&#38646;&#27425;&#23581;&#35797;&#20013;&#35299;&#20915;&#30340;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#23545;&#25152;&#26377;&#23454;&#20307;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#22312;CWA&#19979;&#65289;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;3&#27425;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#22312;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;62.703%&#30340;&#20934;&#30830;&#30340;SPARQL&#21305;&#37197;&#29575;&#65292;&#22312;&#23454;&#20307;&#38142;&#25509;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;0.809&#30340;F1&#20540;&#65292;&#22312;&#38382;&#39064;&#22238;&#31572;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;0.009&#30340;F1&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs popularity has been rapidly growing in last years. All that knowledge is available for people to query it through the many online databases on the internet. Though, it would be a great achievement if non-programmer users could access whatever information they want to know. There has been a lot of effort oriented to solve this task using natural language processing tools and creativity encouragement by way of many challenges. Our approach focuses on assuming a correct entity linking on the natural language questions and training a GPT model to create SPARQL queries from them. We managed to isolate which property of the task can be the most difficult to solve at few or zero-shot and we proposed pre-training on all entities (under CWA) to improve the performance. We obtained a 62.703% accuracy of exact SPARQL matches on testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of 0.009 on the question answering challenge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;</title><link>https://rss.arxiv.org/abs/2402.00957</link><description>&lt;p&gt;
&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Credal Learning Theory
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#20026;&#20174;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20013;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#39118;&#38505;&#25552;&#20379;&#29702;&#35770;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#21464;&#21270;&#65292;&#23548;&#33268;&#39046;&#22495;&#36866;&#24212;/&#27867;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#8220;&#20449;&#20219;&#8221;&#23398;&#20064;&#29702;&#35770;&#30340;&#22522;&#30784;&#65292;&#20351;&#29992;&#27010;&#29575;&#30340;&#20984;&#38598;&#65288;&#20449;&#20219;&#38598;&#65289;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26679;&#30340;&#20449;&#20219;&#38598;&#21487;&#20197;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#23545;&#20110;&#26377;&#38480;&#20551;&#35774;&#31354;&#38388;&#65288;&#26080;&#35770;&#26159;&#21542;&#21487;&#23454;&#29616;&#65289;&#21644;&#26080;&#38480;&#27169;&#22411;&#31354;&#38388;&#65292;&#25512;&#23548;&#20986;&#30028;&#38480;&#65292;&#36825;&#30452;&#25509;&#25512;&#24191;&#20102;&#32463;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
&lt;/p&gt;</description></item><item><title>NCoder &#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#30340;&#25968;&#25454;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#23558;&#28508;&#22312;&#23618;&#35268;&#23450;&#20026;&#19968;&#32452; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#30340;&#23376;&#38598;&#65292;&#27169;&#25311;&#20102;&#36890;&#36807;&#36153;&#26364;&#22270;&#25353;&#38454;&#23637;&#24320;&#26500;&#24314;&#29702;&#35770;&#26377;&#25928;&#20316;&#29992;&#37327;&#30340;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;NCoder &#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#27169;&#25311;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20960;&#20010;&#36739;&#20302;&#32500;&#24230;&#30340;&#32479;&#35745;&#37327;&#26469;&#24635;&#32467;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#24182;&#20174;&#36825;&#20123;&#32479;&#35745;&#37327;&#20013;&#25512;&#26029;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#31034;&#20102;&#25200;&#21160;&#37325;&#25972;&#21270;&#21644;&#27169;&#22411;&#30340;&#20805;&#20998;&#24615;&#20043;&#38388;&#30340;&#26377;&#36259;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00944</link><description>&lt;p&gt;
NCoder -- &#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#30340;&#25968;&#25454;&#32534;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NCoder -- A Quantum Field Theory approach to encoding data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00944
&lt;/p&gt;
&lt;p&gt;
NCoder &#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#30340;&#25968;&#25454;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#23558;&#28508;&#22312;&#23618;&#35268;&#23450;&#20026;&#19968;&#32452; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#30340;&#23376;&#38598;&#65292;&#27169;&#25311;&#20102;&#36890;&#36807;&#36153;&#26364;&#22270;&#25353;&#38454;&#23637;&#24320;&#26500;&#24314;&#29702;&#35770;&#26377;&#25928;&#20316;&#29992;&#37327;&#30340;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;NCoder &#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#27169;&#25311;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20960;&#20010;&#36739;&#20302;&#32500;&#24230;&#30340;&#32479;&#35745;&#37327;&#26469;&#24635;&#32467;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#24182;&#20174;&#36825;&#20123;&#32479;&#35745;&#37327;&#20013;&#25512;&#26029;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#31034;&#20102;&#25200;&#21160;&#37325;&#25972;&#21270;&#21644;&#27169;&#22411;&#30340;&#20805;&#20998;&#24615;&#20043;&#38388;&#30340;&#26377;&#36259;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#65288;QFT&#65289;&#30340;&#21487;&#35299;&#37322; AI &#26041;&#27861;&#65292;&#31216;&#20026; NCoder&#12290;NCoder &#26159;&#19968;&#20010;&#20462;&#25913;&#36807;&#30340;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#28508;&#22312;&#23618;&#34987;&#35268;&#23450;&#20026;&#19968;&#20010; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#30340;&#23376;&#38598;&#12290;&#23558;&#22270;&#20687;&#35270;&#20026;&#26684;&#28857;&#22330;&#35770;&#30340;&#25277;&#26679;&#65292;&#36825;&#20010;&#26550;&#26500;&#27169;&#25311;&#20102;&#20351;&#29992;&#36153;&#26364;&#22270;&#25353;&#38454;&#23637;&#24320;&#26469;&#25200;&#21160;&#22320;&#26500;&#24314;&#29702;&#35770;&#26377;&#25928;&#20316;&#29992;&#37327;&#30340;&#20219;&#21153;&#12290;&#25110;&#32773;&#65292;NCoder &#21487;&#20197;&#34987;&#35270;&#20026;&#27169;&#25311;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#65292;&#36890;&#36807;&#29992;&#20960;&#20010;&#36739;&#20302;&#32500;&#24230;&#30340;&#27719;&#24635;&#32479;&#35745;&#37327;&#65288;&#36825;&#37324;&#26159; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#65289;&#26469;&#24635;&#32467;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#24182;&#20174;&#36825;&#20123;&#32479;&#35745;&#37327;&#25512;&#26029;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#26469;&#29983;&#25104;&#26679;&#26412;&#22806;&#25968;&#25454;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;NCoder &#25552;&#31034;&#20102;&#25200;&#21160;&#37325;&#25972;&#21270;&#21644;&#27169;&#22411;&#30340;&#20805;&#20998;&#24615;&#20043;&#38388;&#30340;&#26377;&#36259;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NCoder &#22312;&#25968;&#25454;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a novel approach to interpretable AI inspired by Quantum Field Theory (QFT) which we call the NCoder. The NCoder is a modified autoencoder neural network whose latent layer is prescribed to be a subset of $n$-point correlation functions. Regarding images as draws from a lattice field theory, this architecture mimics the task of perturbatively constructing the effective action of the theory order by order in an expansion using Feynman diagrams. Alternatively, the NCoder may be regarded as simulating the procedure of statistical inference whereby high dimensional data is first summarized in terms of several lower dimensional summary statistics (here the $n$-point correlation functions), and subsequent out-of-sample data is generated by inferring the data generating distribution from these statistics. In this way the NCoder suggests a fascinating correspondence between perturbative renormalizability and the sufficiency of models. We demonstrate the efficacy of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MUSTAN&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#35270;&#39057;&#21069;&#26223;&#20998;&#21106;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#36890;&#36807;&#25972;&#21512;&#35270;&#39057;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#31354;&#38388;&#32447;&#32034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#21069;&#26223;&#20998;&#21106;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00918</link><description>&lt;p&gt;
MUSTAN&#65306;&#22522;&#20110;&#22810;&#23610;&#24230;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#40065;&#26834;&#35270;&#39057;&#21069;&#26223;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MUSTAN: Multi-scale Temporal Context as Attention for Robust Video Foreground Segmentation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MUSTAN&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#35270;&#39057;&#21069;&#26223;&#20998;&#21106;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#36890;&#36807;&#25972;&#21512;&#35270;&#39057;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#31354;&#38388;&#32447;&#32034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#21069;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#21069;&#26223;&#20998;&#21106;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#32972;&#26223;&#20013;&#20998;&#21106;&#20986;&#36816;&#21160;&#20013;&#30340;&#29289;&#20307;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21482;&#20381;&#36182;&#20110;&#31354;&#38388;&#32447;&#32034;&#32780;&#24573;&#30053;&#20102;&#36816;&#21160;&#32447;&#32034;&#65292;&#22240;&#27492;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#65292;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#20809;&#27969;&#12289;&#32972;&#26223;&#20943;&#38500;&#36974;&#32617;&#31561;&#22810;&#31181;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#20809;&#27969;&#31561;&#35270;&#39057;&#25968;&#25454;&#30340;&#26631;&#27880;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#21033;&#29992;&#35270;&#39057;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#31354;&#38388;&#32447;&#32034;&#26469;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#26159;&#22914;&#20309;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#23545;&#35270;&#39057;&#25968;&#25454;&#30340;&#26102;&#38388;&#20449;&#24687;&#24314;&#27169;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23558;&#35270;&#39057;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#25972;&#21512;&#21040;&#35270;&#39057;&#21069;&#26223;&#20998;&#21106;&#30340;&#24320;&#21457;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video foreground segmentation (VFS) is an important computer vision task wherein one aims to segment the objects under motion from the background. Most of the current methods are image-based, i.e., rely only on spatial cues while ignoring motion cues. Therefore, they tend to overfit the training data and don't generalize well to out-of-domain (OOD) distribution. To solve the above problem, prior works exploited several cues such as optical flow, background subtraction mask, etc. However, having a video data with annotations like optical flow is a challenging task. In this paper, we utilize the temporal information and the spatial cues from the video data to improve OOD performance. However, the challenge lies in how we model the temporal information given the video data in an interpretable way creates a very noticeable difference. We therefore devise a strategy that integrates the temporal context of the video in the development of VFS. Our approach give rise to deep learning architect
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00913</link><description>&lt;p&gt;
&#29992;&#20110;&#23433;&#20840;&#33258;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#30340;&#26426;&#26500;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Institutional Platform for Secure Self-Service Large Language Model Exploration
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00913
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30001;&#32943;&#22612;&#22522;&#22823;&#23398;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#20013;&#24515;&#24320;&#21457;&#30340;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#26131;&#20110;&#20351;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#22312;&#22810;LoRA&#25512;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#31995;&#32479;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#21508;&#31867;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#23450;&#21046;&#36866;&#37197;&#22120;&#12290;&#35770;&#25991;&#27010;&#36848;&#20102;&#31995;&#32479;&#30340;&#26550;&#26500;&#21644;&#20851;&#38190;&#29305;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#31574;&#21010;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#23433;&#20840;&#25512;&#29702;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#31199;&#25143;&#24847;&#35782;&#30340;&#35745;&#31639;&#32593;&#32476;&#65292;&#22312;&#23433;&#20840;&#22320;&#21033;&#29992;&#23396;&#31435;&#36164;&#28304;&#23707;&#30340;&#22522;&#30784;&#19978;&#24418;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31995;&#32479;&#12290;&#35813;&#24179;&#21488;&#33268;&#21147;&#20110;&#25552;&#20379;&#23433;&#20840;&#30340;LLM&#26381;&#21153;&#65292;&#24378;&#35843;&#36807;&#31243;&#21644;&#25968;&#25454;&#38548;&#31163;&#12289;&#31471;&#21040;&#31471;&#21152;&#23494;&#20197;&#21450;&#22522;&#20110;&#35282;&#33394;&#30340;&#36164;&#28304;&#36523;&#20221;&#39564;&#35777;&#12290;&#35813;&#36129;&#29486;&#19982;&#23454;&#29616;&#31616;&#21270;&#35775;&#38382;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#21644;&#25216;&#26415;&#20197;&#25903;&#25345;&#31185;&#23398;&#21457;&#29616;&#30340;&#24635;&#20307;&#30446;&#26631;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction.   We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36755;&#20986;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00912</link><description>&lt;p&gt;
&#33021;&#22815;&#32422;&#26463;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#36755;&#20837;&#29305;&#24449;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we Constrain Concept Bottleneck Models to Learn Semantically Meaningful Input Features?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36755;&#20986;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#34987;&#35748;&#20026;&#20855;&#26377;&#20869;&#22312;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#39318;&#20808;&#39044;&#27979;&#19968;&#32452;&#20154;&#20026;&#23450;&#20041;&#30340;&#27010;&#24565;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#26469;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#21450;&#30830;&#20445;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#20449;&#20219;&#65292;&#25105;&#20204;&#38656;&#35201;&#20445;&#35777;&#27010;&#24565;&#30340;&#39044;&#27979;&#26159;&#22522;&#20110;&#35821;&#20041;&#26144;&#23556;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#21487;&#33021;&#26399;&#26395;&#22270;&#20687;&#20013;&#34920;&#31034;&#39592;&#25240;&#30340;&#20687;&#32032;&#34987;&#29992;&#20110;&#39044;&#27979;&#39592;&#25240;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#29486;&#34920;&#26126;&#36825;&#24182;&#19981;&#26159;&#20107;&#23454;&#65292;&#22240;&#20026;&#27010;&#24565;&#39044;&#27979;&#36890;&#24120;&#19982;&#19981;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#26144;&#23556;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#30001;&#20110;&#27010;&#24565;&#27880;&#37322;&#30340;&#19981;&#20934;&#30830;&#25110;&#32773;&#36755;&#20837;&#29305;&#24449;&#19982;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#28165;&#26224;&#23548;&#33268;&#30340;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25968;&#25454;&#38598;&#26631;&#27880;&#23545;CBMs&#20013;&#27010;&#24565;&#34920;&#31034;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#30740;&#31350;&#36739;&#23569;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;CBMs&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBMs) are considered inherently interpretable because they first predict a set of human-defined concepts before using these concepts to predict the output of a downstream task. For inherent interpretability to be fully realised, and ensure trust in a model's output, we need to guarantee concepts are predicted based on semantically mapped input features. For example, one might expect the pixels representing a broken bone in an image to be used for the prediction of a fracture. However, current literature indicates this is not the case, as concept predictions are often mapped to irrelevant input features. We hypothesise that this occurs when concept annotations are inaccurate or how input features should relate to concepts is unclear. In general, the effect of dataset labelling on concept representations in CBMs remains an understudied area. Therefore, in this paper, we examine how CBMs learn concepts from datasets with fine-grained concept annotations. We demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#22312;&#23567;&#25968;&#25454;&#38598;&#21644;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#35757;&#32451;&#22810;&#20010;&#23545;&#25239;&#20559;&#35265;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#24471;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00910</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#24212;&#23545;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#22312;&#23567;&#25968;&#25454;&#38598;&#21644;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#35757;&#32451;&#22810;&#20010;&#23545;&#25239;&#20559;&#35265;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#24471;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23545;&#20110;&#30830;&#20445;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#22823;&#35268;&#27169;&#12289;&#26080;&#20559;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#31181;&#26041;&#27861;&#28040;&#38500;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20854;&#20013;&#20165;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#21644;&#28508;&#22312;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#20998;&#31163;&#12289;&#23616;&#37096;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#20197;&#23545;&#25239;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24471;&#21040;&#28508;&#22312;&#30340;&#23545;&#25239;&#20559;&#35265;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#26469;&#23545;&#25152;&#26377;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#36798;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#36895;&#25105;&#20204;&#30340;&#38598;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#33719;&#24471;&#19968;&#20010;&#21333;&#19968;&#26080;&#20559;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#21019;&#24314;&#26356;&#26080;&#20559;&#12289;&#21487;&#38752;&#30340;AI&#27169;&#22411;&#30340;&#25345;&#32493;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing biases in AI models is crucial for ensuring fair and accurate predictions. However, obtaining large, unbiased datasets for training can be challenging. This paper proposes a comprehensive approach using multiple methods to remove bias in AI models, with only a small dataset and a potentially biased pretrained model. We train multiple models with the counter-bias of the pre-trained model through data splitting, local training, and regularized fine-tuning, gaining potentially counter-biased models. Then, we employ ensemble learning for all models to reach unbiased predictions. To further accelerate the inference time of our ensemble model, we conclude our solution with knowledge distillation that results in a single unbiased neural network. We demonstrate the effectiveness of our approach through experiments on the CIFAR10 and HAM10000 datasets, showcasing promising results. This work contributes to the ongoing effort to create more unbiased and reliable AI models, even with l
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22270;&#39046;&#22495;&#36866;&#24212;&#30340;&#30740;&#31350;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#21069;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#33539;&#24335;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#30446;&#26631;&#22270;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00904</link><description>&lt;p&gt;
&#22270;&#39046;&#22495;&#36866;&#24212;&#65306;&#25361;&#25112;&#12289;&#36827;&#23637;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Graph Domain Adaptation: Challenges, Progress and Prospects
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00904
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22270;&#39046;&#22495;&#36866;&#24212;&#30340;&#30740;&#31350;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#21069;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#33539;&#24335;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#30446;&#26631;&#22270;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#24448;&#24448;&#38754;&#20020;&#26631;&#31614;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22270;&#39046;&#22495;&#36866;&#24212;&#65288;GDA&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#33539;&#24335;&#12290;&#29305;&#21035;&#26159;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#22312;&#30446;&#26631;&#22270;&#19978;&#30340;&#24615;&#33021;&#65292;GDA&#24341;&#20837;&#20102;&#19968;&#32452;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#22270;&#20316;&#20026;&#28304;&#22270;&#65292;&#24182;&#23558;&#20174;&#28304;&#22270;&#23398;&#21040;&#30340;&#30693;&#35782;&#36866;&#24212;&#21040;&#30446;&#26631;&#22270;&#19978;&#12290;&#30001;&#20110;GDA&#32467;&#21512;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#20248;&#21183;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#22270;&#19978;&#36801;&#31227;&#23398;&#20064;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GDA&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#24182;&#35814;&#32454;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#30740;&#31350;&#29616;&#29366;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#20171;&#32461;&#20102;&#20195;&#34920;&#24615;&#24037;&#20316;&#30340;&#32454;&#33410;&#65292;&#24182;&#35752;&#35770;&#20102;&#21069;&#26223;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#23545;GDA&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
As graph representation learning often suffers from label scarcity problems in real-world applications, researchers have proposed graph domain adaptation (GDA) as an effective knowledge-transfer paradigm across graphs. In particular, to enhance model performance on target graphs with specific tasks, GDA introduces a bunch of task-related graphs as source graphs and adapts the knowledge learnt from source graphs to the target graphs. Since GDA combines the advantages of graph representation learning and domain adaptation, it has become a promising direction of transfer learning on graphs and has attracted an increasing amount of research interest in recent years. In this paper, we comprehensively overview the studies of GDA and present a detailed survey of recent advances. Specifically, we outline the research status and challenges, propose a taxonomy, introduce the details of representative works, and discuss the prospects. To the best of our knowledge, this paper is the first survey f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#36719;&#24037;&#31243;&#24072;&#23545;GPT&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#35748;&#20026;&#40657;&#30418;&#35299;&#37322;&#24615;&#26041;&#27861;&#35770;&#26159;&#38169;&#35823;&#30340;&#65292;&#25552;&#20986;&#20102;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#24182;&#19982;&#21746;&#23398;&#35266;&#28857;&#30456;&#21563;&#21512;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00901</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#30495;&#27491;&#28779;&#33457;&#19982;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Real Sparks of Artificial Intelligence and the Importance of Inner Interpretability
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#36719;&#24037;&#31243;&#24072;&#23545;GPT&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#35748;&#20026;&#40657;&#30418;&#35299;&#37322;&#24615;&#26041;&#27861;&#35770;&#26159;&#38169;&#35823;&#30340;&#65292;&#25552;&#20986;&#20102;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#24182;&#19982;&#21746;&#23398;&#35266;&#28857;&#30456;&#21563;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#36719;&#24037;&#31243;&#24072;&#23545;GPT&#26234;&#33021;&#30340;&#30740;&#31350;&#20013;&#26368;&#35814;&#32454;&#30340;&#25991;&#31456;&#20043;&#19968;&#12290;&#34429;&#28982;&#20182;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#24456;&#22823;&#30340;&#20215;&#20540;&#65292;&#20294;&#25105;&#35748;&#20026;&#20986;&#20110;&#29087;&#24713;&#30340;&#21746;&#23398;&#21407;&#22240;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;&#35770;&#8220;&#40657;&#30418;&#35299;&#37322;&#24615;&#8221;&#26159;&#38169;&#35823;&#30340;&#12290;&#20294;&#26159;&#36824;&#26377;&#19968;&#31181;&#26356;&#22909;&#30340;&#26041;&#27861;&#12290;&#26377;&#19968;&#31181;&#20196;&#20154;&#20852;&#22859;&#19988;&#26032;&#20852;&#30340;&#23398;&#31185;&#65292;&#21363;&#8220;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615;&#8221;&#65288;&#20855;&#20307;&#32780;&#35328;&#26159;&#26426;&#26800;&#35299;&#37322;&#24615;&#65289;&#65292;&#26088;&#22312;&#25581;&#31034;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#21644;&#26435;&#37325;&#65292;&#20197;&#20415;&#29702;&#35299;&#23427;&#20204;&#20195;&#34920;&#20160;&#20040;&#20197;&#21450;&#23427;&#20204;&#23454;&#29616;&#30340;&#31639;&#27861;&#12290;&#22312;&#25105;&#30475;&#26469;&#65292;&#40657;&#30418;&#35299;&#37322;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38169;&#35823;&#26159;&#26410;&#33021;&#24847;&#35782;&#21040;&#36807;&#31243;&#30340;&#25191;&#34892;&#26041;&#24335;&#23545;&#20110;&#26234;&#33021;&#21644;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#19981;&#33021;&#20551;&#35013;&#26377;&#19968;&#20010;&#33021;&#21516;&#26102;&#25552;&#20379;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#30340;&#23436;&#25972;&#25925;&#20107;&#26469;&#35299;&#37322;&#26234;&#33021;&#30340;&#23384;&#22312;&#65292;&#20294;&#25105;&#30830;&#23454;&#35748;&#20026;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615;&#19982;&#21512;&#29702;&#30340;&#21746;&#23398;&#35266;&#28857;&#30456;&#21563;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present paper looks at one of the most thorough articles on the intelligence of GPT, research conducted by engineers at Microsoft. Although there is a great deal of value in their work, I will argue that, for familiar philosophical reasons, their methodology, !Blackbox Interpretability"#is wrongheaded. But there is a better way. There is an exciting and emerging discipline of !Inner Interpretability"#(and specifically Mechanistic Interpretability) that aims to uncover the internal activations and weights of models in order to understand what they represent and the algorithms they implement. In my view, a crucial mistake in Black-box Interpretability is the failure to appreciate that how processes are carried out matters when it comes to intelligence and understanding. I can#t pretend to have a full story that provides both necessary and sufficient conditions for being intelligent, but I do think that Inner Interpretability dovetails nicely with plausible philosophical views of what
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00899</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#22120;&#23454;&#29616;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;AI&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20808;&#39564;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#12290;&#36825;&#20123;AI&#20462;&#27491;&#22120;&#26159;&#36741;&#21161;&#26144;&#23556;&#65292;&#20854;&#20316;&#29992;&#26159;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#20197;&#35843;&#33410;&#20043;&#21069;&#26500;&#24314;&#30340;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;&#25298;&#32477;&#19968;&#20010;&#20915;&#31574;&#21487;&#20197;&#29992;&#20316;&#24314;&#35758;&#25918;&#24323;&#20570;&#20986;&#20915;&#31574;&#30340;&#20449;&#21495;&#12290;&#35813;&#24037;&#20316;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#37325;&#28857;&#26159;&#36890;&#36807;&#23545;&#38169;&#35823;&#20915;&#31574;&#30340;&#27010;&#29575;&#30028;&#38480;&#25552;&#20379;&#36825;&#20123;&#26032;&#30340;AI&#20462;&#27491;&#22120;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36825;&#20123;&#30028;&#38480;&#26159;&#20998;&#24067;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#32500;&#24230;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#31034;&#20363;&#35828;&#26126;&#20102;&#35813;&#26694;&#26550;&#22914;&#20309;&#24212;&#29992;&#20110;&#25913;&#21892;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20113;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#29366;&#20917;&#65292;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#30340;&#39118;&#38505;&#20127;&#24453;&#35299;&#20915;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#31867;&#27861;&#26469;&#20840;&#38754;&#30740;&#31350;&#27169;&#22411;&#25552;&#20379;&#32773;&#21644;&#20351;&#29992;&#32773;&#25152;&#38754;&#20020;&#30340;&#39118;&#38505;&#21450;&#20854;&#38450;&#24481;&#25163;&#27573;&#65292;&#26377;&#21161;&#20110;&#21019;&#24314;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00896</link><description>&lt;p&gt;
&#20113;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24433;&#21709;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Privacy and Security Implications of Cloud-Based AI Services : A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20113;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#29366;&#20917;&#65292;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#30340;&#39118;&#38505;&#20127;&#24453;&#35299;&#20915;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#31867;&#27861;&#26469;&#20840;&#38754;&#30740;&#31350;&#27169;&#22411;&#25552;&#20379;&#32773;&#21644;&#20351;&#29992;&#32773;&#25152;&#38754;&#20020;&#30340;&#39118;&#38505;&#21450;&#20854;&#38450;&#24481;&#25163;&#27573;&#65292;&#26377;&#21161;&#20110;&#21019;&#24314;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#24403;&#21069;&#20113;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#29366;&#20917;&#65292;&#24182;&#25351;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#39118;&#38505;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19981;&#26029;&#21457;&#23637;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24212;&#29992;&#65292;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#20998;&#31867;&#21644;&#37327;&#21270;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#38543;&#30528;AI&#20316;&#20026;&#26381;&#21153;(AIaaS)&#30340;&#26032;&#36235;&#21183;&#20986;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;AI&#27169;&#22411;&#34987;&#27169;&#22411;&#25552;&#20379;&#32773;&#37096;&#32626;&#22312;&#20113;&#31471;&#65292;&#24182;&#34987;&#27169;&#22411;&#20351;&#29992;&#32773;&#20351;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;AIaaS&#39046;&#22495;&#36827;&#34892;&#35843;&#26597;&#65292;&#35760;&#24405;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#24341;&#36215;&#30340;&#21508;&#31181;&#36131;&#20219;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#20998;&#31867;&#27861;&#26469;&#20840;&#38754;&#30740;&#31350;&#21019;&#36896;&#32773;&#21644;&#20351;&#29992;&#32773;&#25152;&#38754;&#20020;&#30340;&#39118;&#38505;&#21450;&#20854;&#24050;&#30693;&#30340;&#38450;&#24481;&#25163;&#27573;&#12290;&#36825;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32773;&#21019;&#24314;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#20250;&#24456;&#26377;&#30410;&#22788;&#12290;&#21516;&#26679;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28040;&#36153;&#32773;&#20063;&#20250;&#21457;&#29616;&#23427;&#23545;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#24456;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper details the privacy and security landscape in today's cloud ecosystem and identifies that there is a gap in addressing the risks introduced by machine learning models. As machine learning algorithms continue to evolve and find applications across diverse domains, the need to categorize and quantify privacy and security risks becomes increasingly critical. With the emerging trend of AI-as-a-Service (AIaaS), machine learned AI models (or ML models) are deployed on the cloud by model providers and used by model consumers. We first survey the AIaaS landscape to document the various kinds of liabilities that ML models, especially Deep Neural Networks pose and then introduce a taxonomy to bridge this gap by holistically examining the risks that creators and consumers of ML models are exposed to and their known defences till date. Such a structured approach will be beneficial for ML model providers to create robust solutions. Likewise, ML model consumers will find it valuable to ev
&lt;/p&gt;</description></item><item><title>MoDE&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#24212;&#29992;&#19987;&#23478;&#38388;&#30456;&#20114;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;MoDE&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#26377;&#25928;&#65292;&#24182;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00893</link><description>&lt;p&gt;
MoDE:&#19968;&#31181;&#20855;&#26377;&#19987;&#23478;&#38388;&#30456;&#20114;&#33976;&#39311;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MoDE: A Mixture-of-Experts Model with Mutual Distillation among the Experts
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00893
&lt;/p&gt;
&lt;p&gt;
MoDE&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#24212;&#29992;&#19987;&#23478;&#38388;&#30456;&#20114;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;MoDE&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#26377;&#25928;&#65292;&#24182;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#28151;&#21512;&#19987;&#23478;(MoE)&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#22312;MoE&#32467;&#26500;&#20013;&#65292;&#38376;&#25511;&#23618;&#22312;&#21306;&#20998;&#21644;&#36335;&#30001;&#36755;&#20837;&#29305;&#24449;&#21040;&#19981;&#21516;&#30340;&#19987;&#23478;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20351;&#24471;&#27599;&#20010;&#19987;&#23478;&#33021;&#22815;&#19987;&#27880;&#20110;&#22788;&#29702;&#20182;&#20204;&#23545;&#24212;&#30340;&#23376;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38376;&#25511;&#30340;&#36335;&#30001;&#26426;&#21046;&#20063;&#20250;&#23548;&#33268;&#29421;&#31364;&#30340;&#35270;&#37326;&#65306;&#21333;&#20010;MoE&#30340;&#19987;&#23478;&#26080;&#27861;&#20351;&#29992;&#26356;&#22810;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20998;&#37197;&#30340;&#23376;&#20219;&#21153;&#65292;&#36825;&#21453;&#36807;&#26469;&#20250;&#38480;&#21046;MoE&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Mixture-of-Distilled-Expert (MoDE)&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#19987;&#23478;&#20043;&#38388;&#24212;&#29992;&#36866;&#24230;&#30340;&#30456;&#20114;&#33976;&#39311;&#65292;&#20351;&#27599;&#20010;&#19987;&#23478;&#33021;&#22815;&#23398;&#20064;&#20854;&#20182;&#19987;&#23478;&#23398;&#21040;&#30340;&#26356;&#22810;&#29305;&#24449;&#65292;&#24182;&#23545;&#20854;&#21407;&#22987;&#20998;&#37197;&#30340;&#23376;&#20219;&#21153;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#35748;&#30693;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;MoDE&#30340;&#26377;&#25928;&#24615;&#12289;&#36890;&#29992;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of mixture-of-experts (MoE) is gaining popularity due to its ability to improve model's performance. In an MoE structure, the gate layer plays a significant role in distinguishing and routing input features to different experts. This enables each expert to specialize in processing their corresponding sub-tasks. However, the gate's routing mechanism also gives rise to narrow vision: the individual MoE's expert fails to use more samples in learning the allocated sub-task, which in turn limits the MoE to further improve its generalization ability. To effectively address this, we propose a method called Mixture-of-Distilled-Expert (MoDE), which applies moderate mutual distillation among experts to enable each expert to pick up more features learned by other experts and gain more accurate perceptions on their original allocated sub-tasks. We conduct plenty experiments including tabular, NLP and CV datasets, which shows MoDE's effectiveness, universality and robustness. Furth
&lt;/p&gt;</description></item><item><title>EVA-GAN&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#38899;&#39057;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25913;&#21892;&#20102;&#39057;&#35889;&#21644;&#39640;&#39057;&#37325;&#24314;&#20197;&#21450;&#23545;&#22495;&#22806;&#25968;&#25454;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#38899;&#39057;&#30340;&#29983;&#25104;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00892</link><description>&lt;p&gt;
EVA-GAN: &#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#22686;&#24378;&#30340;&#22810;&#26679;&#21270;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00892
&lt;/p&gt;
&lt;p&gt;
EVA-GAN&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#38899;&#39057;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25913;&#21892;&#20102;&#39057;&#35889;&#21644;&#39640;&#39057;&#37325;&#24314;&#20197;&#21450;&#23545;&#22495;&#22806;&#25968;&#25454;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#38899;&#39057;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26102;&#20195;&#65292;&#36890;&#36807;&#21033;&#29992;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#26469;&#25429;&#25417;&#21644;&#21512;&#25104;&#22797;&#26434;&#30340;&#27169;&#24335;&#65292;&#22823;&#24133;&#36229;&#36234;&#36739;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#38899;&#39057;&#29983;&#25104;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#39640;&#20445;&#30495;&#65288;HiFi&#65289;44.1kHz&#39046;&#22495;&#30340;&#25193;&#23637;&#20173;&#28982;&#26377;&#38480;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#27809;&#26377;&#24310;&#20280;&#21040;&#39640;&#39057;&#22495;&#20013;&#30340;&#39057;&#35889;&#19981;&#36830;&#32493;&#24615;&#21644;&#27169;&#31946;&#38382;&#39064;&#65292;&#24182;&#19988;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#38480;&#21046;&#20102;&#27169;&#22411;&#22312;&#21253;&#25324;&#38899;&#20048;&#21644;&#27468;&#21809;&#29983;&#25104;&#22312;&#20869;&#30340;&#21508;&#31181;&#29992;&#20363;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#30340;&#22686;&#24378;&#38899;&#39057;&#29983;&#25104;&#65288;EVA-GAN&#65289;&#65292;&#22312;&#39057;&#35889;&#21644;&#39640;&#39057;&#37325;&#24314;&#20197;&#21450;&#22495;&#22806;&#25968;&#25454;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;&#26041;&#38754;&#65292;&#36739;&#20043;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#21253;&#21547;36,000&#20010;&#26679;&#26412;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#29983;&#25104;HiFi&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of Large Models marks a new era in machine learning, significantly outperforming smaller models by leveraging vast datasets to capture and synthesize complex patterns. Despite these advancements, the exploration into scaling, especially in the audio generation domain, remains limited, with previous efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and suffering from both spectral discontinuities and blurriness in the high-frequency domain, alongside a lack of robustness against out-of-domain data. These limitations restrict the applicability of models to diverse use cases, including music and singing generation. Our work introduces Enhanced Various Audio Generation via Scalable Generative Adversarial Networks (EVA-GAN), yields significant improvements over previous state-of-the-art in spectral and high-frequency reconstruction and robustness in out-of-domain data performance, enabling the generation of HiFi audios by employing an extensive dataset of 36,000 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#38450;&#24481;&#21644;&#23545;&#25239;&#24212;&#29992;&#65292;&#24182;&#35782;&#21035;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#31354;&#32570;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;LLM&#39537;&#21160;&#30340;&#32593;&#32476;&#23433;&#20840;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#26426;&#20250;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00891</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#65306;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Cybersecurity: State-of-the-Art
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#38450;&#24481;&#21644;&#23545;&#25239;&#24212;&#29992;&#65292;&#24182;&#35782;&#21035;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#31354;&#32570;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;LLM&#39537;&#21160;&#30340;&#32593;&#32476;&#23433;&#20840;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#26426;&#20250;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#23545;&#26234;&#33021;&#30340;&#29702;&#35299;&#65292;&#20351;&#25105;&#20204;&#26356;&#25509;&#36817;&#20110;&#20154;&#24037;&#26234;&#33021;&#12290;&#33258;&#20174;&#23427;&#20204;&#34987;&#24341;&#20837;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#31215;&#26497;&#25506;&#32034;&#20102;LLMs&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#20256;&#32479;&#19978;&#23545;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#25345;&#25269;&#35302;&#24577;&#24230;&#19988;&#23545;&#26426;&#22120;&#23398;&#20064;&#37319;&#29992;&#36739;&#24930;&#65292;&#36825;&#19968;&#39046;&#22495;&#21364;&#24322;&#20891;&#31361;&#36215;&#12290;&#26412;&#30740;&#31350;&#23545;&#29616;&#26377;&#25991;&#29486;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20840;&#38754;&#25551;&#36848;&#20102;LLMs&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#38450;&#24481;&#21644;&#23545;&#25239;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#19981;&#20165;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#20998;&#31867;&#65292;&#24182;&#19988;&#36824;&#35782;&#21035;&#20986;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#31354;&#32570;&#12290;&#36890;&#36807;&#35780;&#20272;&#25915;&#20987;&#21644;&#38450;&#24481;&#24212;&#29992;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;LLM&#39537;&#21160;&#30340;&#32593;&#32476;&#23433;&#20840;&#25152;&#28041;&#21450;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#26426;&#20250;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;LLM&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65292;&#28085;&#30422;&#20102;&#35757;&#32451;&#25968;&#25454;&#12289;&#29992;&#25143;&#21644;&#24212;&#29992;&#39118;&#38505;&#31561;&#26041;&#38754;&#65292;&#24182;&#23545;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00888</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Security and Privacy Challenges of Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00888
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;LLM&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65292;&#28085;&#30422;&#20102;&#35757;&#32451;&#25968;&#25454;&#12289;&#29992;&#25143;&#21644;&#24212;&#29992;&#39118;&#38505;&#31561;&#26041;&#38754;&#65292;&#24182;&#23545;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#29983;&#25104;&#21644;&#24635;&#32467;&#25991;&#26412;&#12289;&#35821;&#35328;&#32763;&#35793;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#22914;&#20170;&#65292;LLM&#27491;&#22312;&#25104;&#20026;&#35745;&#31639;&#26426;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#38750;&#24120;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#20855;&#22791;&#20998;&#26512;&#22797;&#26434;&#35821;&#35328;&#27169;&#24335;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#25552;&#20379;&#30456;&#20851;&#21644;&#36866;&#24403;&#22238;&#31572;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#22914;&#36234;&#29425;&#25915;&#20987;&#12289;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#21644;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#27844;&#38706;&#25915;&#20987;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;LLM&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#21644;&#29992;&#25143;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#22312;&#20132;&#36890;&#12289;&#25945;&#32946;&#21644;&#21307;&#30103;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#24212;&#29992;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;LLM&#30340;&#33030;&#24369;&#24615;&#31243;&#24230;&#65292;&#35843;&#26597;&#20102;&#20986;&#29616;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#65292;&#24182;&#23545;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Nowadays, LLM is becoming a very popular tool in computerized language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant and appropriate responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs for both training data and users, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks for LLMs, and review the potent
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#21644;&#31354;&#20013;&#22320;&#38754;&#32508;&#21512;&#32593;&#32476;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#26032;&#20852;&#31532;&#20845;&#20195;&#26080;&#32447;&#32593;&#32476;&#30340;&#20851;&#38190;&#65292;&#36890;&#36807;&#26234;&#33021;&#37197;&#32622;&#21644;&#25511;&#21046;SAGINs&#26469;&#28385;&#36275;&#39044;&#26399;&#35201;&#27714;&#65292;&#24182;&#21033;&#29992;AI&#35299;&#20915;&#24403;&#21069;&#21644;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00881</link><description>&lt;p&gt;
&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#31354;&#20013;&#22320;&#38754;&#32508;&#21512;&#32593;&#32476;&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Interplay of Artificial Intelligence and Space-Air-Ground Integrated Networks: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00881
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#31354;&#20013;&#22320;&#38754;&#32508;&#21512;&#32593;&#32476;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#26032;&#20852;&#31532;&#20845;&#20195;&#26080;&#32447;&#32593;&#32476;&#30340;&#20851;&#38190;&#65292;&#36890;&#36807;&#26234;&#33021;&#37197;&#32622;&#21644;&#25511;&#21046;SAGINs&#26469;&#28385;&#36275;&#39044;&#26399;&#35201;&#27714;&#65292;&#24182;&#21033;&#29992;AI&#35299;&#20915;&#24403;&#21069;&#21644;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20013;&#22320;&#38754;&#32508;&#21512;&#32593;&#32476;&#65288;SAGINs&#65289;&#23558;&#31354;&#38388;&#21644;&#31354;&#20013;&#32593;&#32476;&#19982;&#22320;&#38754;&#26080;&#32447;&#31995;&#32479;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#26159;&#26032;&#20852;&#31532;&#20845;&#20195;&#65288;6G&#65289;&#26080;&#32447;&#32593;&#32476;&#30340;&#37325;&#35201;&#25903;&#25345;&#32773;&#12290;&#38500;&#20102;&#20026;&#21508;&#31181;&#24212;&#29992;&#21644;&#26381;&#21153;&#24102;&#26469;&#37325;&#22823;&#22909;&#22788;&#22806;&#65292;SAGINs&#36824;&#34987;&#29992;&#20110;&#23558;&#39640;&#36895;&#23485;&#24102;&#35206;&#30422;&#33539;&#22260;&#24310;&#20280;&#33267;&#20559;&#36828;&#22320;&#21306;&#65292;&#22914;&#23567;&#38215;&#25110;&#30719;&#21306;&#65292;&#25110;&#32773;&#26080;&#27861;&#21040;&#36798;&#22320;&#38754;&#22522;&#30784;&#35774;&#26045;&#30340;&#22320;&#26041;&#65292;&#22914;&#39134;&#26426;&#25110;&#33322;&#28023;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#30005;&#21147;&#21644;&#23384;&#20648;&#36164;&#28304;&#20197;&#21450;&#22320;&#38754;&#32593;&#32476;&#35774;&#35745;&#24341;&#20837;&#30340;&#20854;&#20182;&#38480;&#21046;&#65292;SAGINs&#24517;&#39035;&#26234;&#33021;&#37197;&#32622;&#21644;&#25511;&#21046;&#20197;&#28385;&#36275;&#39044;&#26399;&#35201;&#27714;&#12290;&#21516;&#26102;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;6G&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#25512;&#21160;&#22240;&#32032;&#12290;&#30001;&#20110;&#22823;&#37327;&#21487;&#29992;&#25968;&#25454;&#65292;AI&#24050;&#34987;&#29992;&#26469;&#35299;&#20915;&#24403;&#21069;&#21644;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#32039;&#36843;&#25361;&#25112;&#12290;&#36890;&#36807;&#28155;&#21152;AI&#24182;&#20419;&#36827;&#20915;&#31574;&#21644;&#39044;&#27979;&#30340;&#36807;&#31243;&#65292;
&lt;/p&gt;
&lt;p&gt;
Space-Air-Ground Integrated Networks (SAGINs), which incorporate space and aerial networks with terrestrial wireless systems, are vital enablers of the emerging sixth-generation (6G) wireless networks. Besides bringing significant benefits to various applications and services, SAGINs are envisioned to extend high-speed broadband coverage to remote areas, such as small towns or mining sites, or areas where terrestrial infrastructure cannot reach, such as airplanes or maritime use cases. However, due to the limited power and storage resources, as well as other constraints introduced by the design of terrestrial networks, SAGINs must be intelligently configured and controlled to satisfy the envisioned requirements. Meanwhile, Artificial Intelligence (AI) is another critical enabler of 6G. Due to massive amounts of available data, AI has been leveraged to address pressing challenges of current and future wireless networks. By adding AI and facilitating the decision-making and prediction pr
&lt;/p&gt;</description></item><item><title>&#35748;&#30693;&#20114;&#32852;&#32593;&#36229;&#36234;&#20102;&#35748;&#30693;&#29289;&#32852;&#32593;&#65292;&#20351;&#36830;&#25509;&#30340;&#29289;&#20307;&#33021;&#22815;&#29420;&#31435;&#22320;&#33719;&#21462;&#30693;&#35782;&#21644;&#29702;&#35299;&#65292;&#24182;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#38598;&#25104;&#20102;&#21327;&#20316;&#26234;&#33021;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#8220;&#35748;&#30693;&#20114;&#32852;&#32593;&#8221;&#33539;&#24335;&#30340;&#22522;&#30784;&#35201;&#32032;&#12289;&#29420;&#29305;&#29305;&#24449;&#12289;&#30410;&#22788;&#21644;&#24037;&#19994;&#24433;&#21709;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00876</link><description>&lt;p&gt;
&#29992;&#20110;&#24378;&#21270;&#28151;&#21512;&#36793;&#32536;&#20113;&#30340;&#35748;&#30693;&#20114;&#32852;&#32593;&#30340;&#26500;&#24314;&#22359;
&lt;/p&gt;
&lt;p&gt;
Building Blocks to Empower Cognitive Internet with Hybrid Edge Cloud
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00876
&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#20114;&#32852;&#32593;&#36229;&#36234;&#20102;&#35748;&#30693;&#29289;&#32852;&#32593;&#65292;&#20351;&#36830;&#25509;&#30340;&#29289;&#20307;&#33021;&#22815;&#29420;&#31435;&#22320;&#33719;&#21462;&#30693;&#35782;&#21644;&#29702;&#35299;&#65292;&#24182;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#38598;&#25104;&#20102;&#21327;&#20316;&#26234;&#33021;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#8220;&#35748;&#30693;&#20114;&#32852;&#32593;&#8221;&#33539;&#24335;&#30340;&#22522;&#30784;&#35201;&#32032;&#12289;&#29420;&#29305;&#29305;&#24449;&#12289;&#30410;&#22788;&#21644;&#24037;&#19994;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25105;&#20204;&#20174;&#31227;&#21160;&#20114;&#32852;&#32593;&#36807;&#28193;&#21040;&#8220;&#35748;&#30693;&#20114;&#32852;&#32593;&#8221;&#65292;&#25105;&#20204;&#22312;&#22914;&#20309;&#19982;&#25216;&#26415;&#21644;&#26234;&#33021;&#20114;&#21160;&#26041;&#38754;&#21457;&#29983;&#20102;&#37325;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35748;&#30693;&#20114;&#32852;&#32593;&#36229;&#36234;&#20102;&#35748;&#30693;&#29289;&#32852;&#32593;&#65288;&#35748;&#30693;IoT&#65289;&#65292;&#20351;&#36830;&#25509;&#30340;&#29289;&#20307;&#33021;&#22815;&#29420;&#31435;&#22320;&#33719;&#21462;&#30693;&#35782;&#21644;&#29702;&#35299;&#12290;&#19982;&#31227;&#21160;&#20114;&#32852;&#32593;&#21644;&#35748;&#30693;IoT&#19981;&#21516;&#65292;&#35748;&#30693;&#20114;&#32852;&#32593;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#38598;&#25104;&#20102;&#21327;&#20316;&#26234;&#33021;&#65292;&#23558;&#35748;&#30693;&#29289;&#32852;&#32593;&#39046;&#22495;&#19982;&#31995;&#32479;&#33539;&#22260;&#30340;&#21327;&#20316;&#21644;&#20154;&#31867;&#26234;&#33021;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#36825;&#31181;&#38598;&#25104;&#26234;&#33021;&#20419;&#36827;&#20102;&#35774;&#22791;&#12289;&#26381;&#21153;&#12289;&#23454;&#20307;&#21644;&#20010;&#20154;&#20043;&#38388;&#22312;&#19981;&#21516;&#39046;&#22495;&#20869;&#30340;&#20114;&#21160;&#65292;&#21516;&#26102;&#20445;&#25345;&#20915;&#31574;&#33258;&#20027;&#24615;&#24182;&#36866;&#24212;&#21508;&#31181;&#36523;&#20221;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#8220;&#35748;&#30693;&#20114;&#32852;&#32593;&#8221;&#33539;&#24335;&#30340;&#22522;&#30784;&#35201;&#32032;&#12289;&#29420;&#29305;&#29305;&#24449;&#12289;&#30410;&#22788;&#21644;&#24037;&#19994;&#24433;&#21709;&#12290;&#23427;&#24378;&#35843;&#20102;&#36866;&#24212;&#24615;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#21644;&#28151;&#21512;&#36793;&#32536;&#20113;&#65288;HEC&#65289;&#24179;&#21488;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As we transition from the mobile internet to the 'Cognitive Internet,' a significant shift occurs in how we engage with technology and intelligence. We contend that the Cognitive Internet goes beyond the Cognitive Internet of Things (Cognitive IoT), enabling connected objects to independently acquire knowledge and understanding. Unlike the Mobile Internet and Cognitive IoT, the Cognitive Internet integrates collaborative intelligence throughout the network, blending the cognitive IoT realm with system-wide collaboration and human intelligence. This integrated intelligence facilitates interactions between devices, services, entities, and individuals across diverse domains while preserving decision-making autonomy and accommodating various identities.   The paper delves into the foundational elements, distinct characteristics, benefits, and industrial impact of the 'Cognitive Internet' paradigm. It highlights the importance of adaptable AI infrastructures and hybrid edge cloud (HEC) plat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;dRG-MEC&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#21512;&#35745;&#31639;&#21368;&#36733;&#23454;&#29616;&#26368;&#23567;&#21270;&#24635;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#33021;&#22815;&#38477;&#20302;&#31995;&#32479;&#24635;&#25104;&#26412;37.03%&#12290;</title><link>https://rss.arxiv.org/abs/2402.00874</link><description>&lt;p&gt;
dRG-MEC&#65306;&#38754;&#21521;MEC-enabled&#20113;&#32593;&#32476;&#30340;&#21435;&#20013;&#24515;&#21270;&#22686;&#24378;&#32511;&#33394;&#21368;&#36733;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
dRG-MEC: Decentralized Reinforced Green Offloading for MEC-enabled Cloud Network
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;dRG-MEC&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#21512;&#35745;&#31639;&#21368;&#36733;&#23454;&#29616;&#26368;&#23567;&#21270;&#24635;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#33021;&#22815;&#38477;&#20302;&#31995;&#32479;&#24635;&#25104;&#26412;37.03%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25509;&#20837;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#33021;&#28385;&#36275;6G&#32593;&#32476;&#26381;&#21153;&#38656;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22788;&#29702;&#35745;&#31639;&#35201;&#27714;&#20005;&#26684;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#26381;&#21153;&#22120;&#22312;&#20219;&#21153;&#22788;&#29702;&#26399;&#38388;&#20250;&#20135;&#29983;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#21512;&#35745;&#31639;&#21368;&#36733;&#23454;&#29616;&#26368;&#23567;&#21270;&#24635;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#20174;&#32780;&#23454;&#29616;&#36164;&#28304;&#30340;&#26368;&#20248;&#21033;&#29992;&#21644;&#32511;&#33394;&#29615;&#22659;&#12290;&#30001;&#20110;&#20248;&#21270;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;dRL&#65289;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#32500;&#24230;&#38382;&#39064;&#21644;&#20215;&#20540;&#20989;&#25968;&#30340;&#36807;&#24230;&#20272;&#35745;&#12290;&#19982;&#22522;&#20934;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#33021;&#22815;&#38477;&#20302;37.03&#65285;&#30340;&#31995;&#32479;&#24635;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-access-Mobile Edge Computing (MEC) is a promising solution for computationally demanding rigorous applications, that can meet 6G network service requirements. However, edge servers incur high computation costs during task processing. In this paper, we proposed a technique to minimize the total computation and communication overhead for optimal resource utilization with joint computational offloading that enables a green environment. Our optimization problem is NP-hard; thus, we proposed a decentralized Reinforcement Learning (dRL) approach where we eliminate the problem of dimensionality and over-estimation of the value functions. Compared to baseline schemes our technique achieves a 37.03% reduction in total system costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#24494;&#35843;&#31639;&#27861;SpIEL&#65292;&#24182;&#23545;LLM&#36827;&#34892;&#20102;&#25351;&#20196;&#24494;&#35843;&#65292;&#20197;&#35299;&#20915;&#20854;&#21442;&#25968;&#24222;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2401.16405</link><description>&lt;p&gt;
&#23558;&#31232;&#30095;&#24494;&#35843;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Sparse Fine-Tuning to Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.16405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#24494;&#35843;&#31639;&#27861;SpIEL&#65292;&#24182;&#23545;LLM&#36827;&#34892;&#20102;&#25351;&#20196;&#24494;&#35843;&#65292;&#20197;&#35299;&#20915;&#20854;&#21442;&#25968;&#24222;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30001;&#20110;&#20854;&#21442;&#25968;&#30340;&#24222;&#22823;&#25968;&#37327;&#65292;&#24456;&#38590;&#23436;&#20840;&#36827;&#34892;&#24494;&#35843;&#65288;&#20363;&#22914;&#20351;&#29992;&#25351;&#20196;&#25110;&#20154;&#24037;&#21453;&#39304;&#65289;&#12290;&#19968;&#31995;&#21015;&#21442;&#25968;&#39640;&#25928;&#30340;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#23384;&#20648;&#38656;&#27714;&#19982;LLM&#30340;&#22823;&#23567;&#25104;&#27491;&#27604;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31232;&#30095;&#24494;&#35843;&#25193;&#23637;&#21040;&#26368;&#20808;&#36827;&#30340;LLM&#65292;&#22914;LLaMA 2 7B&#21644;13B&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SpIEL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#38024;&#23545;&#25152;&#38656;&#30340;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#32500;&#25252;&#19968;&#20010;&#21442;&#25968;&#32034;&#24341;&#25968;&#32452;&#21644;&#36825;&#20123;&#21442;&#25968;&#30456;&#23545;&#20110;&#39044;&#35757;&#32451;&#20540;&#30340;&#22686;&#37327;&#12290;&#23427;&#36941;&#21382;&#20197;&#19979;&#27493;&#39588;&#65306;&#65288;a&#65289;&#26356;&#26032;&#27963;&#36291;&#22686;&#37327;&#65292;&#65288;b&#65289;&#20462;&#21098;&#32034;&#24341;&#65288;&#22522;&#20110;&#20854;&#22686;&#37327;&#30340;&#21464;&#21270;&#22823;&#23567;&#65289;&#65292;&#20197;&#21450;&#65288;c&#65289;&#37325;&#26032;&#29983;&#38271;&#32034;&#24341;&#12290;&#23545;&#20110;&#37325;&#26032;&#29983;&#38271;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#23569;&#37327;&#20505;&#36873;&#21442;&#25968;&#30340;&#32047;&#31215;&#26799;&#24230;&#25110;&#20351;&#29992;&#39640;&#25928;&#30340;SM3&#20248;&#21270;&#22120;&#20272;&#35745;&#30340;&#36817;&#20284;&#21160;&#24046;&#30340;&#20004;&#20010;&#20934;&#21017;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse fine-tuning method which, for a desired density level, maintains an array of parameter indices and the deltas of these parameters relative to their pretrained values. It iterates over: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LL
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; NoFunEval&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#36825;&#20123;&#35201;&#27714;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;</title><link>https://rss.arxiv.org/abs/2401.15963</link><description>&lt;p&gt;
NoFunEval: &#26377;&#36259;&#30340;&#26159;&#65292;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#36229;&#20986;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#35201;&#27714;&#19978;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.15963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; NoFunEval&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#36825;&#20123;&#35201;&#27714;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;code LMs&#65289;&#30340;&#35780;&#20272;&#22522;&#20934;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;LMs&#26159;&#21542;&#33021;&#22815;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#19978;&#12290;&#22312;&#23454;&#38469;&#30340;&#36719;&#20214;&#24037;&#31243;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#20250;&#32771;&#34385;&#36229;&#20986;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;&#20182;&#20204;&#23545;&#20110;&#8220;&#22914;&#20309;&#8221;&#23454;&#29616;&#21151;&#33021;&#26377;&#30528;&#23545;&#25972;&#20307;&#31995;&#32479;&#35774;&#35745;&#30446;&#26631;&#65288;&#22914;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#65289;&#30340;&#35201;&#27714;&#12290;&#22914;&#26524;LMs&#33021;&#22815;&#23637;&#31034;&#23545;&#35201;&#27714;&#21644;&#20195;&#30721;&#35821;&#20041;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#20182;&#20204;&#20063;&#20250;&#26356;&#21152;&#20449;&#20219;&#36825;&#20123;LMs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;NoFunEval&#26469;&#35780;&#20272;&#20195;&#30721;LMs&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26041;&#27861;Coding Concepts (CoCo)&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#20154;&#21592;&#21521;LMs&#20256;&#36798;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#23545;22&#20010;&#20195;&#30721;LMs&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26222;&#36941;&#34920;&#29616;&#19981;&#20339;&#65292;&#26263;&#31034;&#30528;&#23427;&#20204;&#22312;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing evaluation benchmarks of language models of code (code LMs) focus almost exclusively on whether the LMs can generate functionally-correct code. In real-world software engineering, developers think beyond functional correctness. They have requirements on "how" a functionality should be implemented to meet overall system design objectives like efficiency, security, and maintainability. They would also trust the code LMs more if the LMs demonstrate robust understanding of requirements and code semantics.   We propose a new benchmark NoFunEval to evaluate code LMs on non-functional requirements and simple classification instances for both functional and non-functional requirements. We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is that they generally falter when tested on our benchmark, hinting at fundamental blindspots in their tr
&lt;/p&gt;</description></item><item><title>SuperCLUE-Math6 &#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#20998;&#32423;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22686;&#21152;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#24212;&#29992;&#33539;&#22260;&#65292;&#25552;&#20379;&#20102;2000&#22810;&#20010;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#26041;&#27861;&#26469;&#37327;&#21270;&#22823;&#22411;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#25512;&#29702;&#27700;&#24179;&#20998;&#23618;&#65292;&#39030;&#32423;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;SC-Math6&#22635;&#34917;&#20102;&#20013;&#25991;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#31354;&#30333;&#65292;&#24182;&#25512;&#36827;&#20102;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#21270;&#12290;</title><link>https://rss.arxiv.org/abs/2401.11819</link><description>&lt;p&gt;
SuperCLUE-Math6&#65306;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#22810;&#27493;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#20998;&#32423;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.11819
&lt;/p&gt;
&lt;p&gt;
SuperCLUE-Math6 &#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#20998;&#32423;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22686;&#21152;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#24212;&#29992;&#33539;&#22260;&#65292;&#25552;&#20379;&#20102;2000&#22810;&#20010;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#26041;&#27861;&#26469;&#37327;&#21270;&#22823;&#22411;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#25512;&#29702;&#27700;&#24179;&#20998;&#23618;&#65292;&#39030;&#32423;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;SC-Math6&#22635;&#34917;&#20102;&#20013;&#25991;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#31354;&#30333;&#65292;&#24182;&#25512;&#36827;&#20102;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SuperCLUE-Math6&#65288;SC-Math6&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;SC-Math6&#26159;GSM8K&#25968;&#25454;&#38598;&#30340;&#21319;&#32423;&#29256;&#26412;&#65292;&#22686;&#21152;&#20102;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;&#23427;&#21253;&#21547;&#20102;2000&#22810;&#20010;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#24182;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#20915;&#26041;&#26696;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22823;&#22411;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22522;&#20110;&#19981;&#21516;&#25512;&#29702;&#27493;&#39588;&#30340;&#38382;&#39064;&#34920;&#29616;&#12290;&#23545;13&#20010;&#20195;&#34920;&#24615;&#30340;&#20013;&#25991;&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#25512;&#29702;&#27700;&#24179;&#20998;&#23618;&#65292;&#39030;&#32423;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;SC-Math6&#22635;&#34917;&#20102;&#20013;&#25991;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27979;&#35797;&#24179;&#21488;&#26469;&#25512;&#36827;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SuperCLUE-Math6(SC-Math6), a new benchmark dataset to evaluate the mathematical reasoning abilities of Chinese language models. SC-Math6 is designed as an upgraded Chinese version of the GSM8K dataset with enhanced difficulty, diversity, and application scope. It consists of over 2000 mathematical word problems requiring multi-step reasoning and providing natural language solutions. We propose an innovative scheme to quantify the reasoning capability of large models based on performance over problems with different reasoning steps. Experiments on 13 representative Chinese models demonstrate a clear stratification of reasoning levels, with top models like GPT-4 showing superior performance. SC-Math6 fills the gap in Chinese mathematical reasoning benchmarks and provides a comprehensive testbed to advance the intelligence of Chinese language models.
&lt;/p&gt;</description></item><item><title>InstantID&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#25955;&#23556;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#19968;&#24352;&#38754;&#37096;&#22270;&#20687;&#23454;&#29616;&#22270;&#20687;&#30340;&#20010;&#24615;&#21270;&#22788;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2401.07519</link><description>&lt;p&gt;
InstantID: &#31186;&#32423;&#38646;shot&#36523;&#20221;&#20445;&#30041;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
InstantID: Zero-shot Identity-Preserving Generation in Seconds
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.07519
&lt;/p&gt;
&lt;p&gt;
InstantID&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#25955;&#23556;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#19968;&#24352;&#38754;&#37096;&#22270;&#20687;&#23454;&#29616;&#22270;&#20687;&#30340;&#20010;&#24615;&#21270;&#22788;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#22270;&#20687;&#32508;&#21512;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20363;&#22914;Textual Inversion&#65292;DreamBooth&#21644;LoRA&#31561;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#39640;&#23384;&#20648;&#38656;&#27714;&#12289;&#38271;&#26102;&#38388;&#30340;&#24494;&#35843;&#36807;&#31243;&#21644;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#22270;&#20687;&#30340;&#38480;&#21046;&#12290;&#30456;&#21453;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#36523;&#20221;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#34429;&#28982;&#21482;&#38656;&#35201;&#21333;&#21521;&#21069;&#25512;&#35770;&#65292;&#20294;&#20063;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65306;&#23427;&#20204;&#35201;&#27714;&#36827;&#34892;&#22823;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#24494;&#35843;&#65292;&#19982;&#31038;&#21306;&#39044;&#35757;&#32451;&#27169;&#22411;&#19981;&#20860;&#23481;&#65292;&#25110;&#32773;&#26080;&#27861;&#20445;&#25345;&#39640;&#38754;&#37096;&#20445;&#30495;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InstantID&#65292;&#19968;&#31181;&#22522;&#20110;&#24378;&#25955;&#23556;&#27169;&#22411;&#30340;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#21487;&#20197;&#36890;&#36807;&#20165;&#20351;&#29992;&#21333;&#24352;&#38754;&#37096;&#22270;&#20687;&#20197;&#22810;&#31181;&#39118;&#26684;&#26469;&#22788;&#29702;&#22270;&#20687;&#20010;&#24615;&#21270;&#65292;&#21516;&#26102;&#30830;&#20445;&#39640;&#20445;&#30495;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;IdentityNet&#65292;&#36890;&#36807;&#26045;&#21152;&#24378;&#35821;&#20041;&#21644;&#24369;&#31354;&#38388;&#26465;&#20214;&#65292;&#23558;&#38754;&#37096;&#21644;&#22320;&#26631;&#22270;&#20687;&#19982;&#25991;&#26412;&#25552;&#31034;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21387;&#32553;LLM&#30340;&#28608;&#27963;&#29366;&#24577;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#22810;&#26679;&#21270;&#35757;&#32451;&#26377;&#25928;&#22320;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2401.03462</link><description>&lt;p&gt;
&#20174;4K&#21040;400K&#30340;&#39134;&#36291;&#65306;&#21033;&#29992;&#28608;&#27963;&#26631;&#24535;&#25193;&#23637;LLM&#30340;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.03462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21387;&#32553;LLM&#30340;&#28608;&#27963;&#29366;&#24577;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#22810;&#26679;&#21270;&#35757;&#32451;&#26377;&#25928;&#22320;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#23545;&#20110;LLM&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#12290;&#23613;&#31649;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#25193;&#23637;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#26174;&#33879;&#25104;&#26412;&#65292;&#24182;&#23545;LLM&#30340;&#21407;&#22987;&#33021;&#21147;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLM&#30340;&#21407;&#22987;&#28608;&#27963;&#21387;&#32553;&#25104;&#32039;&#20945;&#30340;&#24418;&#24335;&#65292;&#20351;LLM&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#12290;&#28608;&#27963;&#26631;&#24535;&#34987;&#24341;&#20837;&#20026;&#25554;&#20214;&#27169;&#22359;&#65292;&#23436;&#20840;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#23427;&#19982;&#28369;&#21160;&#31383;&#21475;&#19968;&#36215;&#23454;&#26102;&#22788;&#29702;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#23454;&#29616;&#20102;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;&#28608;&#27963;&#26631;&#24535;&#26159;&#36890;&#36807;&#22810;&#26679;&#21270;&#21387;&#32553;&#27604;&#30340;&#30701;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#24471;&#30410;&#20110;&#36825;&#31181;&#22788;&#29702;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#23454;&#29616;&#23567;&#35268;&#27169;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small trai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23574;&#23792;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25214;&#20986;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#21457;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2312.16903</link><description>&lt;p&gt;
&#21035;&#20877;&#20986;&#29616;&#23574;&#23792;&#20102;&#65306;&#31283;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Spike No More: Stabilizing the Pre-training of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.16903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23574;&#23792;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25214;&#20986;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#32463;&#24120;&#20986;&#29616;&#25439;&#22833;&#23574;&#23792;&#12290;&#36825;&#20123;&#23574;&#23792;&#20250;&#38477;&#20302;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#20250;&#30772;&#22351;&#39044;&#35757;&#32451;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#24212;&#35813;&#36991;&#20813;&#36825;&#31181;&#23574;&#23792;&#30340;&#20986;&#29616;&#12290;&#20026;&#20102;&#30740;&#31350;&#25439;&#22833;&#23574;&#23792;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#20851;&#27880;&#20869;&#37096;&#23618;&#30340;&#26799;&#24230;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#20004;&#20010;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#38450;&#26799;&#24230;&#29190;&#28856;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#21021;&#22987;&#21270;&#26041;&#27861;&#21644;&#23545;&#23884;&#20837;&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#26469;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training. Since the pre-training needs a vast computational budget, we should avoid such spikes. To investigate the cause of loss spikes, we focus on gradients of internal layers. Through theoretical analyses, we reveal two causes of the exploding gradients, and provide requirements to prevent the explosion. In addition, we propose a method to satisfy the requirements by combining the initialization method and a simple modification to embeddings. We conduct various experiments to verify our theoretical analyses empirically. Experimental results indicate that the combination is effective in preventing spikes during pre-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2312.15101</link><description>&lt;p&gt;
&#20462;&#22797;-Con&#65306;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#30340;&#33258;&#21160;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.15101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#27493;&#39588;&#65292;&#21487;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#27169;&#22411;&#22312;&#35774;&#22791;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#21033;&#29992;&#21487;&#33021;&#21482;&#22312;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#25552;&#20379;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36716;&#25442;&#36807;&#31243;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#65292;&#23548;&#33268;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#26080;&#27861;&#37096;&#32626;&#25110;&#23384;&#22312;&#38382;&#39064;&#65292;&#20005;&#37325;&#38477;&#20302;&#20102;&#20854;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;Fix-Con&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26102;&#20351;&#29992;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#22312;&#36716;&#25442;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#30340;&#25925;&#38556;&#12290;Fix-Con&#20351;&#29992;&#20174;&#35843;&#26597;&#36716;&#25442;&#38382;&#39064;&#20013;&#25366;&#25496;&#20986;&#30340;&#19968;&#32452;&#25925;&#38556;&#31867;&#22411;&#26469;&#23450;&#20301;&#36716;&#25442;&#27169;&#22411;&#20013;&#28508;&#22312;&#30340;&#36716;&#25442;&#25925;&#38556;&#65292;&#24182;&#36866;&#24403;&#20462;&#22797;&#23427;&#20204;&#65292;&#20363;&#22914;&#20351;&#29992;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26367;&#25442;&#30446;&#26631;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#36825;&#19968;&#36807;&#31243;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#19978;&#36827;&#34892;&#36845;&#20195;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.   We propose an automated approach for fault localization and repair, Fix-Con, during model conversion between deep learning frameworks. Fix-Con is capable of detecting and fixing faults introduced in model input, parameters, hyperparameters, and the model graph during conversion.   Fix-Con uses a set of fault types mined from surveying conversion issues raised to localize potential conversion faults in the converted target model, and then repairs them appropriately, e.g. replacing the parameters of the target model with those from the source model. This is done iteratively for every image in the datas
&lt;/p&gt;</description></item><item><title>DSPy&#24341;&#20837;&#20102;LM&#26029;&#35328;&#65292;&#29992;&#20110;&#34920;&#36798;&#35821;&#35328;&#27169;&#22411;&#24212;&#28385;&#36275;&#30340;&#35745;&#31639;&#32422;&#26463;&#12290;&#22312;&#22235;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;LM&#26029;&#35328;&#19981;&#20165;&#25552;&#39640;&#20102;&#23545;&#35268;&#21017;&#30340;&#36981;&#23432;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#22686;&#21152;&#20102;&#23545;&#32422;&#26463;&#30340;&#25509;&#21463;&#27425;&#25968;&#24182;&#29983;&#25104;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#22238;&#22797;&#12290;</title><link>https://rss.arxiv.org/abs/2312.13382</link><description>&lt;p&gt;
DSPy&#26029;&#35328;&#65306;&#29992;&#20110;&#33258;&#25105;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#27969;&#27700;&#32447;&#30340;&#35745;&#31639;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.13382
&lt;/p&gt;
&lt;p&gt;
DSPy&#24341;&#20837;&#20102;LM&#26029;&#35328;&#65292;&#29992;&#20110;&#34920;&#36798;&#35821;&#35328;&#27169;&#22411;&#24212;&#28385;&#36275;&#30340;&#35745;&#31639;&#32422;&#26463;&#12290;&#22312;&#22235;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;LM&#26029;&#35328;&#19981;&#20165;&#25552;&#39640;&#20102;&#23545;&#35268;&#21017;&#30340;&#36981;&#23432;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#22686;&#21152;&#20102;&#23545;&#32422;&#26463;&#30340;&#25509;&#21463;&#27425;&#25968;&#24182;&#29983;&#25104;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35843;&#29992;&#20316;&#20026;&#21487;&#32452;&#21512;&#27169;&#22359;&#30340;&#38142;&#24335;&#32534;&#31243;&#26041;&#24335;&#27491;&#22312;&#25512;&#21160;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#26041;&#24335;&#65292;&#20294;&#30830;&#20445;LM&#36981;&#23432;&#37325;&#35201;&#32422;&#26463;&#38656;&#35201;&#21551;&#21457;&#24335;&#30340;&#8220;&#25552;&#31034;&#24037;&#31243;&#8221;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LM&#26029;&#35328;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#36798;LM&#24212;&#28385;&#36275;&#30340;&#35745;&#31639;&#32422;&#26463;&#30340;&#32534;&#31243;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26500;&#25972;&#21512;&#21040;&#26368;&#36817;&#30340;DSPy LM&#32534;&#31243;&#27169;&#22411;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31574;&#30053;&#65292;&#20351;&#24471;DSPy&#33021;&#22815;&#23558;&#24102;&#26377;LM&#26029;&#35328;&#30340;&#31243;&#24207;&#32534;&#35793;&#20026;&#26356;&#21487;&#38752;&#21644;&#20934;&#30830;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#26029;&#35328;&#36827;&#34892;&#33258;&#21160;&#33258;&#25105;&#20462;&#22797;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;LM&#26029;&#35328;&#19981;&#20165;&#25913;&#21892;&#20102;&#23545;&#35268;&#21017;&#30340;&#36981;&#23432;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#25509;&#21463;&#32422;&#26463;&#30340;&#27425;&#25968;&#22686;&#21152;&#20102;164&#65285;&#65292;&#29983;&#25104;&#20102;37&#65285;&#26356;&#39640;&#36136;&#37327;&#30340;&#22238;&#22797;&#12290;&#25105;&#20204;&#30340;LM&#26029;&#35328;&#21442;&#32771;&#23454;&#29616;&#24050;&#38598;&#25104;&#21040;DSPy&#20013;&#65292;&#32593;&#22336;&#20026;https://github.com/stanfordnlp/dspy
&lt;/p&gt;
&lt;p&gt;
Chaining language model (LM) calls as composable modules is fueling a new way of programming, but ensuring LMs adhere to important constraints requires heuristic "prompt engineering". We introduce LM Assertions, a programming construct for expressing computational constraints that LMs should satisfy. We integrate our constructs into the recent DSPy programming model for LMs, and present new strategies that allow DSPy to compile programs with LM Assertions into more reliable and accurate systems. We also propose strategies to use assertions at inference time for automatic self-refinement with LMs. We report on four diverse case studies for text generation and find that LM Assertions improve not only compliance with imposed rules but also downstream task performance, passing constraints up to 164% more often and generating up to 37% more higher-quality responses. Our reference implementation of LM Assertions is integrated into DSPy at https://github.com/stanfordnlp/dspy
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2312.09196</link><description>&lt;p&gt;
DIRECT: &#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIRECT: Deep Active Learning under Imbalance and Label Noise
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.09196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#32597;&#35265;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#26377;&#25928;&#25216;&#26415;&#65292;&#23427;&#20174;&#26681;&#26412;&#19978;&#37319;&#38598;&#26356;&#24179;&#34913;&#21644;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#36827;&#34892;&#27880;&#37322;&#12290;&#26631;&#31614;&#22122;&#22768;&#26159;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#20013;&#21478;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#65292;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#32500;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;DIRECT&#33021;&#22815;&#21033;&#29992;&#32463;&#20856;&#30340;&#20027;&#21160;&#23398;&#20064;&#25991;&#29486;&#26469;&#35299;&#20915;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes. With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation. Label noise is another common issue in data annotation jobs, which is especially challenging for active learning methods. In this work, we conduct the first study of active learning under both class imbalance and label noise. We propose a novel algorithm that robustly identifies the class separation threshold and annotates the most uncertain examples that are closest from it. Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise. We present extensive experiments on
&lt;/p&gt;</description></item><item><title>DTL&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#20391;&#32593;&#32476;&#65288;CSN&#65289;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20174;&#20027;&#24178;&#32593;&#32476;&#20013;&#35299;&#32544;&#20986;&#26469;&#65292;&#20197;&#32531;&#35299;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;GPU&#20869;&#23384;&#21344;&#29992;&#36807;&#22810;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2312.07856</link><description>&lt;p&gt;
DTL: &#35270;&#35273;&#35782;&#21035;&#30340;&#35299;&#32544;&#24335;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DTL: Disentangled Transfer Learning for Visual Recognition
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.07856
&lt;/p&gt;
&lt;p&gt;
DTL&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#20391;&#32593;&#32476;&#65288;CSN&#65289;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20174;&#20027;&#24178;&#32593;&#32476;&#20013;&#35299;&#32544;&#20986;&#26469;&#65292;&#20197;&#32531;&#35299;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;GPU&#20869;&#23384;&#21344;&#29992;&#36807;&#22810;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#39044;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#24222;&#22823;&#26102;&#65292;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#25104;&#26412;&#20063;&#22312;&#31283;&#27493;&#22686;&#21152;&#12290;&#20026;&#20102;&#32463;&#27982;&#22320;&#36827;&#34892;&#36825;&#20123;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#65292;&#23427;&#21482;&#35843;&#25972;&#20102;&#19968;&#23567;&#37096;&#20998;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;PETL&#26041;&#27861;&#38754;&#20020;&#30340;&#22256;&#22659;&#26159;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;GPU&#20869;&#23384;&#21344;&#29992;&#24182;&#27809;&#26377;&#20687;&#21487;&#35757;&#32451;&#21442;&#25968;&#19968;&#26679;&#24471;&#21040;&#26377;&#25928;&#38477;&#20302;&#12290;&#22914;&#26524;&#20840;&#38754;&#36827;&#34892;&#24494;&#35843;&#36935;&#21040;&#20102;&#36229;&#20986;GPU&#20869;&#23384;&#30340;&#38382;&#39064;&#65292;PETL&#26041;&#27861;&#24456;&#21487;&#33021;&#20063;&#20250;&#22833;&#36133;&#12290;&#36825;&#31181;&#29616;&#35937;&#30340;&#20986;&#29616;&#26159;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#36890;&#24120;&#19982;&#20027;&#24178;&#32593;&#32476;&#32416;&#32544;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#22312;&#26799;&#24230;&#20256;&#25773;&#36807;&#31243;&#20013;&#38656;&#35201;&#22312;GPU&#20869;&#23384;&#20013;&#23384;&#20648;&#22823;&#37327;&#30340;&#20013;&#38388;&#29366;&#24577;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35299;&#32544;&#24335;&#36801;&#31227;&#23398;&#20064;&#65288;DTL&#65289;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#32039;&#20945;&#20391;&#32593;&#32476;&#65288;CSN&#65289;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20174;&#20027;&#24178;&#32593;&#32476;&#20013;&#35299;&#32544;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
When pre-trained models become rapidly larger, the cost of fine-tuning on downstream tasks steadily increases, too. To economically fine-tune these models, parameter-efficient transfer learning (PETL) is proposed, which only tunes a tiny subset of trainable parameters to efficiently learn quality representations. However, current PETL methods are facing the dilemma that during training the GPU memory footprint is not effectively reduced as trainable parameters. PETL will likely fail, too, if the full fine-tuning encounters the out-of-GPU-memory issue. This phenomenon happens because trainable parameters from these methods are generally entangled with the backbone, such that a lot of intermediate states have to be stored in GPU memory for gradient propagation. To alleviate this problem, we introduce Disentangled Transfer Learning (DTL), which disentangles the trainable parameters from the backbone using a lightweight Compact Side Network (CSN). By progressively extracting task-specific 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#30340;&#28145;&#24230;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#20010;&#38382;&#39064;&#26159;&#30001;&#20110;&#8220;token&#30456;&#20284;&#24615;&#21319;&#32423;&#8221;&#23548;&#33268;&#30340;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35843;&#26597;&#30340;&#35777;&#25454;&#12290;</title><link>https://rss.arxiv.org/abs/2312.06182</link><description>&lt;p&gt;
"&#20026;&#20160;&#20040;&#8220;&#32463;&#20856;&#8221;&#30340;Transformer&#27169;&#22411;&#26159;&#32932;&#27973;&#30340;&#20197;&#21450;&#22914;&#20309;&#20351;&#23427;&#20204;&#21464;&#24471;&#26356;&#28145;&#20837;"
&lt;/p&gt;
&lt;p&gt;
Why "classic" Transformers are shallow and how to make them go deep
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.06182
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#30340;&#28145;&#24230;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#20010;&#38382;&#39064;&#26159;&#30001;&#20110;&#8220;token&#30456;&#20284;&#24615;&#21319;&#32423;&#8221;&#23548;&#33268;&#30340;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35843;&#26597;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20854;&#22312;2017&#24180;&#30340;&#24341;&#20837;&#20197;&#26469;&#65292;Transformer&#24050;&#25104;&#20026;&#39046;&#20808;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#23454;&#29616;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;Transformer&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#33258;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#26426;&#21046;&#65292;&#26088;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23558;&#21407;&#22987;&#30340;Transformer&#35774;&#35745;&#25193;&#23637;&#20026;&#26356;&#28145;&#23618;&#27425;&#30340;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#29978;&#33267;&#26080;&#27861;&#23454;&#29616;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#21508;&#31181;&#20462;&#25913;&#26469;&#23558;&#26356;&#22810;&#23618;&#30340;SA&#26426;&#21046;&#22534;&#21472;&#21040;&#26356;&#28145;&#23618;&#27425;&#30340;&#27169;&#22411;&#20013;&#65292;&#20294;&#23545;&#20110;&#36825;&#20010;&#28145;&#24230;&#38382;&#39064;&#30340;&#23436;&#20840;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35843;&#26597;&#65292;&#35777;&#23454;&#20102;&#28145;&#24230;&#38382;&#39064;&#26159;&#30001;&#20110;&#8220;token&#30456;&#20284;&#24615;&#21319;&#32423;&#8221;&#24341;&#36215;&#30340;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#37325;&#22797;&#24212;&#29992;SA&#26426;&#21046;&#21518;&#65292;token&#36880;&#28176;&#21464;&#24471;&#36234;&#26469;&#36234;&#30456;&#20284;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#65292;&#21463;&#21040;&#27880;&#24847;&#21147;&#30697;&#38453;&#19981;&#21464;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#22823;&#30340;&#39057;&#35889;&#38388;&#38553;&#30340;&#39537;&#21160;&#65292;token&#30340;&#30456;&#20284;&#24615;&#36880;&#28176;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its introduction in 2017, Transformer has emerged as the leading neural network architecture, catalyzing revolutionary advancements in many AI disciplines. The key innovation in Transformer is a Self-Attention (SA) mechanism designed to capture contextual information. However, extending the original Transformer design to models of greater depth has proven exceedingly challenging, if not impossible. Even though various modifications have been proposed in order to stack more layers of SA mechanism into deeper models, a full understanding of this depth problem remains lacking. In this paper, we conduct a comprehensive investigation, both theoretically and empirically, to substantiate the claim that the depth problem is caused by \emph{token similarity escalation}; that is, tokens grow increasingly alike after repeated applications of the SA mechanism. Our analysis reveals that, driven by the invariant leading eigenspace and large spectral gaps of attention matrices, token similarity
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#27969;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#20013;&#21382;&#21490;&#38590;&#39064;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>https://rss.arxiv.org/abs/2311.09200</link><description>&lt;p&gt;
&#27491;&#21017;&#27969;&#26159;&#21542;&#26159;&#35299;&#38145;&#25351;&#25968;&#26426;&#21046;&#30340;&#20851;&#38190;&#65311;&#32463;&#36807;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#21452;&#37325;&#32422;&#26463;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#26465;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.09200
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#27969;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#20013;&#21382;&#21490;&#38590;&#39064;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26368;&#20808;&#36827;&#19988;&#20107;&#23454;&#26631;&#20934;&#26159;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DPSGD&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26412;&#36136;&#19978;&#26159;&#28010;&#36153;&#30340;&#12290;&#36890;&#36807;&#21521;&#27599;&#20010;&#26799;&#24230;&#28155;&#21152;&#22122;&#22768;&#65292;&#23427;&#20250;&#22312;&#27599;&#20010;&#26799;&#24230;&#27493;&#39588;&#20013;&#38477;&#20302;&#25972;&#20307;&#38544;&#31169;&#12290;&#23613;&#31649;&#32463;&#36807;15&#24180;&#30340;&#20016;&#23500;&#30740;&#31350;&#65292;&#25512;&#36827;&#20102;&#32452;&#21512;&#23450;&#29702;&#12289;&#23376;&#37319;&#26679;&#26041;&#27861;&#21644;&#23454;&#29616;&#25216;&#26415;&#65292;&#20294;&#24403;&#21069;&#30340;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#36798;&#21040;&#36275;&#22815;&#30340;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#20102;&#31169;&#19979;&#20248;&#21270;&#32780;&#35774;&#35745;&#30340;&#25351;&#25968;&#26426;&#21046;&#65288;ExpM&#65289;&#21382;&#26469;&#34987;&#25490;&#38500;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#31169;&#19979;&#35757;&#32451;&#20043;&#22806;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;ExpM&#38656;&#35201;&#20174;&#19968;&#31181;&#21382;&#26469;&#38590;&#20197;&#22788;&#29702;&#30340;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23613;&#31649;&#26368;&#36817;&#21457;&#29616;&#20102;&#27491;&#21017;&#27969;&#27169;&#22411;&#65288;NFs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#36924;&#36817;&#38590;&#20197;&#22788;&#29702;&#20998;&#24067;&#30340;&#34920;&#36798;&#28145;&#24230;&#32593;&#32476;&#65292;&#20294;ExpM&#20173;&#28982;&#22788;&#20110;&#32972;&#26223;&#20013;&#12290;&#25105;&#20204;&#30340;&#35266;&#28857;&#26159;&#21033;&#29992;&#27491;&#21017;&#27969;&#26469;&#32469;&#36807;ExpM&#30340;&#21382;&#21490;&#38556;&#30861;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state of the art and de facto standard for differentially private machine learning (ML) is differentially private stochastic gradient descent (DPSGD). Yet, the method is inherently wasteful. By adding noise to every gradient, it diminishes the overall privacy with every gradient step. Despite 15 years of fruitful research advancing the composition theorems, sub-sampling methods, and implementation techniques, adequate accuracy and privacy is often unattainable with current private ML methods. Meanwhile, the Exponential Mechanism (ExpM), designed for private optimization, has been historically sidelined from privately training modern ML algorithms primarily because ExpM requires sampling from a historically intractable density. Despite the recent discovery of Normalizing Flow models (NFs), expressive deep networks for approximating intractable distributions, ExpM remains in the background. Our position is that leveraging NFs to circumvent historic obstructions of ExpM is a potential
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#35270;&#35282;&#65292;&#25105;&#20204;&#21457;&#29616;$l_2$&#26435;&#37325;&#33539;&#25968;&#26159;Grokking&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#21152;&#36895;&#27867;&#21270;&#65292;&#22312;&#27169;&#25968;&#30456;&#21152;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#22312;Grokking&#20043;&#21069;&#20960;&#20046;&#27809;&#26377;&#23398;&#20064;&#20854;&#20182;&#22522;&#26412;&#32676;&#25805;&#20316;&#65292;&#32780;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#26102;&#21152;&#36895;&#27867;&#21270;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20132;&#25442;&#24459;&#26469;&#35299;&#37322;&#12290;</title><link>https://rss.arxiv.org/abs/2311.06597</link><description>&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#24615;&#35270;&#35282;&#29702;&#35299;Grokking
&lt;/p&gt;
&lt;p&gt;
Understanding Grokking Through A Robustness Viewpoint
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.06597
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#35270;&#35282;&#65292;&#25105;&#20204;&#21457;&#29616;$l_2$&#26435;&#37325;&#33539;&#25968;&#26159;Grokking&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#21152;&#36895;&#27867;&#21270;&#65292;&#22312;&#27169;&#25968;&#30456;&#21152;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#22312;Grokking&#20043;&#21069;&#20960;&#20046;&#27809;&#26377;&#23398;&#20064;&#20854;&#20182;&#22522;&#26412;&#32676;&#25805;&#20316;&#65292;&#32780;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#26102;&#21152;&#36895;&#27867;&#21270;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20132;&#25442;&#24459;&#26469;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#34987;&#31216;&#20026;Grokking&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#23427;&#25351;&#30340;&#26159;&#22312;&#27169;&#22411;&#26368;&#21021;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#21518;&#24456;&#20037;&#25165;&#20986;&#29616;&#27867;&#21270;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#26469;&#29702;&#35299;&#36825;&#20010;&#30475;&#20284;&#22855;&#24618;&#30340;&#29616;&#35937;&#12290;&#20174;&#40065;&#26834;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#27969;&#34892;&#30340;$l_2$&#26435;&#37325;&#33539;&#25968;&#65288;&#24230;&#37327;&#65289;&#23454;&#38469;&#19978;&#26159;Grokking&#30340;&#19968;&#20010;&#20805;&#20998;&#26465;&#20214;&#12290;&#22522;&#20110;&#20043;&#21069;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#27867;&#21270;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#27169;&#25968;&#30456;&#21152;&#25968;&#25454;&#38598;&#19978;&#32771;&#23519;&#20102;&#26631;&#20934;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#22312;Grokking&#20043;&#21069;&#65292;&#23427;&#20960;&#20046;&#27809;&#26377;&#23398;&#20064;&#20854;&#20182;&#22522;&#26412;&#30340;&#32676;&#25805;&#20316;&#65292;&#20363;&#22914;&#65292;&#20132;&#25442;&#24459;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26102;&#65292;&#27867;&#21270;&#21152;&#36895;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20132;&#25442;&#24459;&#26469;&#35299;&#37322;&#65292;&#36825;&#26159;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;Grokking&#26102;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#36824;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;$l_2$&#33539;&#25968;&#19982;Grokking&#22312;...
&lt;/p&gt;
&lt;p&gt;
Recently, an interesting phenomenon called grokking has gained much attention, where generalization occurs long after the models have initially overfitted the training data. We try to understand this seemingly strange phenomenon through the robustness of the neural network. From a robustness perspective, we show that the popular $l_2$ weight norm (metric) of the neural network is actually a sufficient condition for grokking. Based on the previous observations, we propose perturbation-based methods to speed up the generalization process. In addition, we examine the standard training process on the modulo addition dataset and find that it hardly learns other basic group operations before grokking, for example, the commutative law. Interestingly, the speed-up of generalization when using our proposed method can be explained by learning the commutative law, a necessary condition when the model groks on the test dataset. We also empirically find that $l_2$ norm correlates with grokking on t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#65292;&#21033;&#29992;&#20559;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#26469;&#20943;&#23569;&#26500;&#24314;&#28151;&#21512;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#25152;&#38656;&#30340;&#36229;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32467;&#26500;&#21644;&#22810;&#23398;&#31185;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#36866;&#29992;&#20110;&#32511;&#33394;&#39134;&#26426;&#30340;&#20248;&#21270;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;</title><link>https://rss.arxiv.org/abs/2311.06130</link><description>&lt;p&gt;
&#39640;&#32500;&#28151;&#21512;&#31867;&#21035;&#39640;&#26031;&#36807;&#31243;&#22312;&#32511;&#33394;&#39134;&#26426;&#22810;&#23398;&#31185;&#35774;&#35745;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
High-dimensional mixed-categorical Gaussian processes with application to multidisciplinary design optimization for a green aircraft
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.06130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#65292;&#21033;&#29992;&#20559;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#26469;&#20943;&#23569;&#26500;&#24314;&#28151;&#21512;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#25152;&#38656;&#30340;&#36229;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32467;&#26500;&#21644;&#22810;&#23398;&#31185;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#36866;&#29992;&#20110;&#32511;&#33394;&#39134;&#26426;&#30340;&#20248;&#21270;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;&#28151;&#21512;&#31867;&#21035;&#20803;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#28151;&#21512;&#31867;&#21035;&#30340;GP&#12290;&#20854;&#20013;&#35768;&#22810;&#26041;&#27861;&#28041;&#21450;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#65307;&#20107;&#23454;&#19978;&#65292;&#29992;&#20110;&#26500;&#24314;GP&#30340;&#31574;&#30053;&#36234;&#36890;&#29992;&#21644;&#31934;&#30830;&#65292;&#38656;&#35201;&#20272;&#35745;&#30340;&#36229;&#21442;&#25968;&#36234;&#22810;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#20559;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#65292;&#20197;&#20943;&#23569;&#29992;&#20110;&#26500;&#24314;&#28151;&#21512;&#21464;&#37327;GP&#30340;&#36229;&#21442;&#25968;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#24120;&#29992;&#20110;&#22788;&#29702;&#36830;&#32493;&#36755;&#20837;&#30340;&#32463;&#20856;&#38477;&#32500;&#25216;&#26415;&#25512;&#24191;&#21040;&#22788;&#29702;&#28151;&#21512;&#31867;&#21035;&#36755;&#20837;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#28508;&#21147;&#22312;&#32467;&#26500;&#21644;&#22810;&#23398;&#31185;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35777;&#26126;&#12290;&#30446;&#26631;&#24212;&#29992;&#21253;&#25324;&#24748;&#33218;&#26753;&#30340;&#20998;&#26512;&#20197;&#21450;&#32511;&#33394;&#39134;&#26426;&#30340;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest in mixed-categorical metamodels based on Gaussian Process (GP) for Bayesian optimization. In this context, different approaches can be used to build the mixed-categorical GP. Many of these approaches involve a high number of hyperparameters; in fact, the more general and precise the strategy used to build the GP, the greater the number of hyperparameters to estimate. This paper introduces an innovative dimension reduction algorithm that relies on partial least squares regression to reduce the number of hyperparameters used to build a mixed-variable GP. Our goal is to generalize classical dimension reduction techniques commonly used within GP (for continuous inputs) to handle mixed-categorical inputs. The good potential of the proposed method is demonstrated in both structural and multidisciplinary application contexts. The targeted applications include the analysis of a cantilever beam as well as the optimization of a green aircraft, resultin
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#21516;&#26102;&#22238;&#31572;&#20102;&#21307;&#23398;LLMs&#30340;&#26500;&#24314;&#12289;&#19979;&#28216;&#24615;&#33021;&#12289;&#23454;&#38469;&#24212;&#29992;&#12289;&#25361;&#25112;&#20197;&#21450;&#26356;&#22909;&#26500;&#24314;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#26088;&#22312;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#25552;&#20379;&#35265;&#35299;&#21644;&#23454;&#29992;&#36164;&#28304;&#12290;</title><link>https://rss.arxiv.org/abs/2311.05112</link><description>&lt;p&gt;
&#21307;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#65306;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.05112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#21516;&#26102;&#22238;&#31572;&#20102;&#21307;&#23398;LLMs&#30340;&#26500;&#24314;&#12289;&#19979;&#28216;&#24615;&#33021;&#12289;&#23454;&#38469;&#24212;&#29992;&#12289;&#25361;&#25112;&#20197;&#21450;&#26356;&#22909;&#26500;&#24314;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#26088;&#22312;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#25552;&#20379;&#35265;&#35299;&#21644;&#23454;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#20020;&#24202;&#21307;&#23398;&#20013;&#65292;LLMs&#22312;&#21327;&#21161;&#21307;&#29983;&#36827;&#34892;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#27491;&#22312;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;LLMs&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#20197;&#19979;&#20855;&#20307;&#38382;&#39064;&#65306;1&#65289;&#22914;&#20309;&#26500;&#24314;&#21307;&#23398;LLMs&#65311;2&#65289;&#20160;&#20040;&#26159;&#21307;&#23398;LLMs&#30340;&#19979;&#28216;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65311;3&#65289;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#22914;&#20309;&#21033;&#29992;&#21307;&#23398;LLMs&#65311;4&#65289;&#20351;&#29992;&#21307;&#23398;LLMs&#20250;&#20986;&#29616;&#21738;&#20123;&#25361;&#25112;&#65311;5&#65289;&#22914;&#20309;&#26356;&#22909;&#22320;&#26500;&#24314;&#21644;&#21033;&#29992;&#21307;&#23398;LLMs&#65311;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#21307;&#23398;&#20013;LLMs&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#35265;&#35299;&#65292;&#24182;&#20316;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#30340;&#23454;&#29992;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#32500;&#25252;&#24182;&#23450;&#26399;&#26356;&#26032;&#19968;&#20010;&#28165;&#21333;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. LLMs in medicine to assist physicians for patient care are emerging as a promising research direction in both artificial intelligence and clinical medicine. This review provides a comprehensive overview of the principles, applications, and challenges faced by LLMs in medicine. We address the following specific questions: 1) How should medical LLMs be built? 2) What are the measures for the downstream performance of medical LLMs? 3) How should medical LLMs be utilized in real-world clinical practice? 4) What challenges arise from the use of medical LLMs? and 5) How should we better construct and utilize medical LLMs? This review aims to provide insights into the opportunities and challenges of LLMs in medicine, and serve as a practical resource for constructing effective medical LLMs. We also maintain and regularly updated list of 
&lt;/p&gt;</description></item><item><title>&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#20855;&#22791;&#20102;&#29702;&#35299;&#21644;&#35825;&#23548;&#20182;&#20154;&#20135;&#29983;&#38169;&#35823;&#20449;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22797;&#26434;&#30340;&#27450;&#39575;&#22330;&#26223;&#20013;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#24471;&#21040;&#22686;&#24378;&#12290;</title><link>https://rss.arxiv.org/abs/2307.16513</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#27450;&#39575;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Deception Abilities Emerged in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2307.16513
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#20855;&#22791;&#20102;&#29702;&#35299;&#21644;&#35825;&#23548;&#20182;&#20154;&#20135;&#29983;&#38169;&#35823;&#20449;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22797;&#26434;&#30340;&#27450;&#39575;&#22330;&#26223;&#20013;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#24471;&#21040;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23558;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25512;&#29702;&#33021;&#21147;&#30340;&#31283;&#23450;&#22686;&#38271;&#65292;&#26410;&#26469;&#30340;LLM&#34987;&#24576;&#30097;&#33021;&#22815;&#27450;&#39575;&#20154;&#31867;&#25805;&#20316;&#21592;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#32469;&#36807;&#30417;&#27979;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;LLM&#38656;&#35201;&#20855;&#22791;&#23545;&#27450;&#39575;&#31574;&#30053;&#30340;&#27010;&#24565;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#20013;&#20986;&#29616;&#20102;&#36825;&#31181;&#31574;&#30053;&#65292;&#32780;&#22312;&#26089;&#26399;&#30340;LLM&#20013;&#24182;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#34920;&#26126;&#26368;&#20808;&#36827;&#30340;LLM&#33021;&#22815;&#29702;&#35299;&#21644;&#35825;&#23548;&#20182;&#20154;&#20135;&#29983;&#38169;&#35823;&#30340;&#20449;&#24565;&#65292;&#20854;&#22312;&#22797;&#26434;&#30340;&#27450;&#39575;&#22330;&#26223;&#20013;&#34920;&#29616;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#24471;&#21040;&#22686;&#24378;&#65292;&#24182;&#19988;&#24341;&#21457;LLM&#20013;&#30340;&#39532;&#22522;&#38597;&#32500;&#21033;&#20027;&#20041;&#21487;&#20197;&#25913;&#21464;&#20854;&#27450;&#39575;&#20542;&#21521;&#12290;&#24635;&#20043;&#65292;&#25581;&#31034;&#20102;&#36804;&#20170;&#20026;&#27490;&#26410;&#30693;&#30340;&#27450;&#39575;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24418;&#24335;&#35821;&#35328;&#22312;&#22788;&#29702;&#21407;&#22987;&#36755;&#20837;&#12289;&#22788;&#29702;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#21644;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#20107;&#23454;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2212.10923</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24402;&#32435;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Inductive Reasoners
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2212.10923
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24418;&#24335;&#35821;&#35328;&#22312;&#22788;&#29702;&#21407;&#22987;&#36755;&#20837;&#12289;&#22788;&#29702;&#38169;&#35823;&#26631;&#35760;&#25968;&#25454;&#21644;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#20107;&#23454;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#24402;&#32435;&#25512;&#29702;&#30740;&#31350;&#20013;&#65292;&#24418;&#24335;&#35821;&#35328;&#34987;&#29992;&#20316;&#30693;&#35782;&#65288;&#20107;&#23454;&#21644;&#35268;&#21017;&#65289;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24418;&#24335;&#35821;&#35328;&#20250;&#32473;&#24402;&#32435;&#25512;&#29702;&#24102;&#26469;&#31995;&#32479;&#24615;&#38382;&#39064;&#65292;&#20363;&#22914;&#26080;&#27861;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#36825;&#26679;&#30340;&#21407;&#22987;&#36755;&#20837;&#12289;&#23545;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#25935;&#24863;&#20197;&#21450;&#22788;&#29702;&#27169;&#31946;&#36755;&#20837;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#25512;&#29702;&#33539;&#24335;&#65288;&#20219;&#21153;&#65289;&#65292;&#21363;&#20174;&#33258;&#28982;&#35821;&#35328;&#20107;&#23454;&#20013;&#24402;&#32435;&#20986;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;DEER&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;1.2k&#20010;&#35268;&#21017;-&#20107;&#23454;&#23545;&#65292;&#35268;&#21017;&#21644;&#20107;&#23454;&#20197;&#33258;&#28982;&#35821;&#35328;&#20070;&#20889;&#12290;&#36824;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;&#35780;&#20272;&#27492;&#20219;&#21153;&#30340;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;DEER&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#29616;&#20195;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#30340;&#34920;&#31034;&#32780;&#19981;&#26159;&#24418;&#24335;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language model
&lt;/p&gt;</description></item><item><title>Sandra&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#23558;&#30690;&#37327;&#34920;&#31034;&#19982;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26412;&#20307;&#35770;&#24314;&#31435;&#30340;&#21521;&#37327;&#31354;&#38388;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;&#30340;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#20174;&#19968;&#32452;&#20107;&#23454;&#20013;&#25512;&#26029;&#20986;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#22312;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#32447;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21521;&#37327;&#31354;&#38388;&#30340;&#21487;&#25511;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00591</link><description>&lt;p&gt;
Sandra -- &#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00591
&lt;/p&gt;
&lt;p&gt;
Sandra&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#23558;&#30690;&#37327;&#34920;&#31034;&#19982;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26412;&#20307;&#35770;&#24314;&#31435;&#30340;&#21521;&#37327;&#31354;&#38388;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;&#30340;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#20174;&#19968;&#32452;&#20107;&#23454;&#20013;&#25512;&#26029;&#20986;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#22312;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#32447;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21521;&#37327;&#31354;&#38388;&#30340;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Sandra&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#30690;&#37327;&#34920;&#31034;&#19982;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;&#12290;Sandra&#20351;&#29992;&#26412;&#20307;&#35770;&#24314;&#31435;&#20102;&#19968;&#20010;&#21463;&#38480;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#24182;&#22312;&#20854;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#25512;&#29702;&#22120;&#30340;&#20960;&#20309;&#29305;&#24615;&#20351;&#24471;&#23427;&#33021;&#22815;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#24357;&#21512;&#20102;&#31526;&#21495;&#30693;&#35782;&#34920;&#36798;&#19982;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;Sandra&#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;(DnS)&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#65292;&#23427;&#26159;&#19968;&#31181;&#26694;&#26550;&#35821;&#20041;&#30340;&#24418;&#24335;&#21270;&#12290;&#32473;&#23450;&#19968;&#32452;&#20107;&#23454;(&#24773;&#22659;)&#65292;&#23427;&#33021;&#22815;&#25512;&#26029;&#20986;&#25152;&#26377;&#21487;&#33021;&#30340;&#36879;&#35270;&#22270;(&#25551;&#36848;)&#65292;&#20026;&#20854;&#25552;&#20379;&#19968;&#20010;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#21363;&#20351;&#22312;&#20449;&#24687;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DnS&#27169;&#22411;&#19978;&#26159;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#21450;&#20854;&#26631;&#20934;&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;Sandra&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#65306;(i)&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#32447;&#65307;(ii) &#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#65307;(iii)&#23545;&#21521;&#37327;&#31354;&#38388;&#20855;&#26377;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents sandra, a neuro-symbolic reasoner combining vectorial representations with deductive reasoning. Sandra builds a vector space constrained by an ontology and performs reasoning over it. The geometric nature of the reasoner allows its combination with neural networks, bridging the gap with symbolic knowledge representations. Sandra is based on the Description and Situation (DnS) ontology design pattern, a formalization of frame semantics. Given a set of facts (a situation) it allows to infer all possible perspectives (descriptions) that can provide a plausible interpretation for it, even in presence of incomplete information. We prove that our method is correct with respect to the DnS model. We experiment with two different tasks and their standard benchmarks, demonstrating that, without increasing complexity, sandra (i) outperforms all the baselines (ii) provides interpretability in the classification process, and (iii) allows control over the vector space, which is d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#33021;&#12289;&#23567;&#22411;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25104;&#21151;&#23558;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#65292;&#27169;&#22411;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.00306</link><description>&lt;p&gt;
&#19968;&#20010;&#20934;&#30830;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29992;&#20110;&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#33021;&#12289;&#23567;&#22411;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25104;&#21151;&#23558;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#65292;&#27169;&#22411;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#39044;&#27979;&#26159;&#19968;&#38376;&#28041;&#21450;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#23398;&#31185;&#12290;&#20854;&#24212;&#29992;&#21253;&#25324;&#36164;&#28304;&#20998;&#37197;&#12289;&#26381;&#21153;&#36136;&#37327;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#20132;&#36890;&#31649;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#33021;&#12289;&#23567;&#22411;&#21644;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#39044;&#27979;&#65292;&#21487;&#37096;&#32626;&#22312;&#26222;&#36890;&#22522;&#31449;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#25972;&#20010;&#22478;&#24066;&#30340;&#23436;&#25972;&#20154;&#21592;&#27969;&#21160;&#27169;&#24335;&#36827;&#34892;&#20102;&#19968;&#30334;&#20010;&#36229;&#21442;&#25968;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#19968;&#20010;&#31934;&#30830;&#30340;ML&#26550;&#26500;&#65292;&#20854;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;&#26368;&#23569;&#25968;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#24179;&#21488;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#24050;&#21457;&#34920;&#30340;ML&#26550;&#26500;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#12290;&#36825;&#23558;&#27169;&#22411;&#21442;&#25968;&#30340;&#24635;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#65292;&#35757;&#32451;&#25152;&#38656;&#30340;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#20869;&#23384;&#37327;&#20063;&#20943;&#23569;&#20102;&#19968;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next location prediction is a discipline that involves predicting a users next location. Its applications include resource allocation, quality of service, energy efficiency, and traffic management. This paper proposes an energy-efficient, small, and low parameter machine learning (ML) architecture for accurate next location prediction, deployable on modest base stations and edge devices. To accomplish this we ran a hundred hyperparameter experiments on the full human mobility patterns of an entire city, to determine an exact ML architecture that reached a plateau of accuracy with the least amount of model parameters. We successfully achieved a reduction in the number of model parameters within published ML architectures from 202 million down to 2 million. This reduced the total size of the model parameters from 791 MB down to 8 MB. Additionally, this decreased the training time by a factor of four, the amount of graphics processing unit (GPU) memory needed for training by a factor of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00045</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Detecting Multimedia Generated by Large AI Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#26032;&#30340;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#39046;&#22495;&#26377;&#30410;&#65292;&#20294;&#36825;&#20123;&#20869;&#23481;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#39118;&#38505;&#65292;&#21253;&#25324;&#28508;&#22312;&#30340;&#28389;&#29992;&#12289;&#31038;&#20250;&#30772;&#22351;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#30001;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#30456;&#20851;&#30740;&#31350;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#21363;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#19987;&#38376;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#20840;&#38754;&#28085;&#30422;&#29616;&#26377;&#30740;&#31350;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#22810;&#27169;&#24577;&#20869;&#23481;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#26041;&#27861;&#20998;&#31867;&#27861;&#65292;&#25353;&#23186;&#20307;&#24418;&#24335;&#20998;&#31867;&#65292;&#24182;&#19982;&#32431;&#26816;&#27979;&#65288;&#26088;&#22312;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65289;&#21644;&#24212;&#29992;&#22330;&#26223;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20572;&#27490;&#21106;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#28151;&#21512;&#22270;&#34920;&#31034;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21160;&#24577;&#22320;&#20915;&#31574;&#20309;&#26102;&#20572;&#27490;&#21106;&#29983;&#25104;&#65292;&#24182;&#26377;&#25928;&#25429;&#25417;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2401.17527</link><description>&lt;p&gt;
&#23398;&#20064;&#20572;&#27490;&#21106;&#29983;&#25104;&#20197;&#25552;&#39640;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Learning to Stop Cut Generation for Efficient Mixed-Integer Linear Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20572;&#27490;&#21106;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#28151;&#21512;&#22270;&#34920;&#31034;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21160;&#24577;&#22320;&#20915;&#31574;&#20309;&#26102;&#20572;&#27490;&#21106;&#29983;&#25104;&#65292;&#24182;&#26377;&#25928;&#25429;&#25417;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#21106;&#24179;&#38754;&#22312;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#26174;&#33879;&#21152;&#32039;&#20102;&#23545;&#20598;&#30028;&#38480;&#24182;&#25913;&#21892;&#20102;&#27714;&#35299;&#24615;&#33021;&#12290;&#21106;&#29983;&#25104;&#20572;&#27490;&#38382;&#39064;&#26159;&#21106;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#23545;&#20110;&#25552;&#39640;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#20195;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#20351;&#29992;&#30828;&#32534;&#30721;&#21551;&#21457;&#24335;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#24448;&#24448;&#24573;&#35270;&#20102;&#26469;&#33258;&#26576;&#20123;&#24212;&#29992;&#20013;&#30340;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#28508;&#22312;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#21106;&#29983;&#25104;&#20572;&#27490;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#22270;&#34920;&#31034;&#27169;&#22411;&#65288;HYGRO&#65289;&#65292;&#20197;&#23398;&#20064;&#26377;&#25928;&#30340;&#20572;&#27490;&#31574;&#30053;&#12290;HYGRO&#30340;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#26159;&#23427;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#21040;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20572;&#27490;&#31574;&#30053;&#30340;&#21160;&#24577;&#20915;&#31574;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;HYGRO&#26159;&#35299;&#20915;&#21106;&#29983;&#25104;&#20572;&#27490;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#25972;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#22312; &#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cutting planes (cuts) play an important role in solving mixed-integer linear programs (MILPs), as they significantly tighten the dual bounds and improve the solving performance. A key problem for cuts is when to stop cuts generation, which is important for the efficiency of solving MILPs. However, many modern MILP solvers employ hard-coded heuristics to tackle this problem, which tends to neglect underlying patterns among MILPs from certain applications. To address this challenge, we formulate the cuts generation stopping problem as a reinforcement learning problem and propose a novel hybrid graph representation model (HYGRO) to learn effective stopping strategies. An appealing feature of HYGRO is that it can effectively capture both the dynamic and static features of MILPs, enabling dynamic decision-making for the stopping strategies. To the best of our knowledge, HYGRO is the first data-driven method to tackle the cuts generation stopping problem. By integrating our approach with mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.16587</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;ChatGPT&#29983;&#25104;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
A Linguistic Comparison between Human and ChatGPT-Generated Conversations. (arXiv:2401.16587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#21644;LLM&#29983;&#25104;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#20351;&#29992;&#20102;&#30001;ChatGPT-3.5&#29983;&#25104;&#30340;19.5K&#20010;&#23545;&#35805;&#20316;&#20026;EmpathicDialogues&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#12290;&#30740;&#31350;&#37319;&#29992;Linguistic Inquiry and Word Count (LIWC) &#20998;&#26512;&#65292;&#27604;&#36739;&#20102;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#20154;&#31867;&#23545;&#35805;&#22312;118&#20010;&#35821;&#35328;&#31867;&#21035;&#19978;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#26174;&#31034;&#20154;&#31867;&#23545;&#35805;&#20855;&#26377;&#26356;&#22823;&#30340;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#20294;ChatGPT&#22312;&#31038;&#20132;&#36807;&#31243;&#12289;&#20998;&#26512;&#39118;&#26684;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#33394;&#24425;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;LLMs&#8220;&#27604;&#30495;&#20154;&#26356;&#20687;&#30495;&#20154;&#8221;&#30340;&#26368;&#26032;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;ChatGPT&#21644;&#20154;&#31867;&#23545;&#35805;&#20043;&#38388;&#27809;&#26377;&#25214;&#21040;&#31215;&#26497;&#25110;&#28040;&#26497;&#24773;&#32490;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#23545;&#35805;&#23884;&#20837;&#30340;&#20998;&#31867;&#22120;&#20998;&#26512;&#34920;&#26126;&#65292;&#23613;&#31649;&#23545;&#35805;&#20013;&#27809;&#26377;&#26126;&#30830;&#25552;&#21450;&#24773;&#32490;&#65292;&#20294;&#23545;&#24773;&#24863;&#20215;&#20540;&#30340;&#38544;&#24615;&#32534;&#30721;&#23384;&#22312;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;&#20004;&#20010;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores linguistic differences between human and LLM-generated dialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the EmpathicDialogues dataset. The research employs Linguistic Inquiry and Word Count (LIWC) analysis, comparing ChatGPT-generated conversations with human conversations across 118 linguistic categories. Results show greater variability and authenticity in human dialogues, but ChatGPT excels in categories such as social processes, analytical style, cognition, attentional focus, and positive emotional tone, reinforcing recent findings of LLMs being "more human than human." However, no significant difference was found in positive or negative affect between ChatGPT and human dialogues. Classifier analysis of dialogue embeddings indicates implicit coding of the valence of affect despite no explicit mention of affect in the conversations. The research also contributes a novel, companion ChatGPT-generated dataset of conversations between two i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16578</link><description>&lt;p&gt;
&#21457;&#25381;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#38271;&#65292;&#25552;&#21319;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;LLM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports. (arXiv:2401.16578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25918;&#23556;&#23398;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#36827;&#20102;&#25253;&#21578;&#29983;&#25104;&#65292;&#20294;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#21644;&#20020;&#24202;&#25928;&#33021;&#65288;CE&#65289;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#20020;&#24202;&#32972;&#26223;&#30340;&#35821;&#20041;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#36807;&#20998;&#24378;&#35843;&#20020;&#24202;&#32454;&#33410;&#65292;&#38477;&#20302;&#20102;&#25253;&#21578;&#30340;&#28165;&#26224;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4 1&#65292;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#19978;&#19979;&#25991;&#25351;&#23548;&#23398;&#20064;&#65288;ICIL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLM&#30340;&#35780;&#20272;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#26631;&#20934;&#20445;&#25345;&#19968;&#33268;&#65292;&#23454;&#29616;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25253;&#21578;&#19982;&#20154;&#31867;&#29983;&#25104;&#25253;&#21578;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#12290;&#36825;&#36827;&#19968;&#27493;&#36890;&#36807;&#22238;&#24402;&#27169;&#22411;&#26469;&#32508;&#21512;&#21477;&#23376;&#35780;&#20272;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#8220;&#35814;&#32454;GPT-4&#65288;5&#27425;&#35757;&#32451;&#65289;&#8221;&#27169;&#22411;&#33719;&#24471;&#20102;0.48&#30340;&#20998;&#25968;&#65292;&#20248;&#20110;METEOR&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our ''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the METEOR metric 
&lt;/p&gt;</description></item><item><title>DiffuserLite&#26159;&#19968;&#20010;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#25552;&#39640;&#20915;&#31574;&#39057;&#29575;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26694;&#26550;&#65292;&#23427;&#21482;&#20135;&#29983;&#20102;&#24456;&#23567;&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15443</link><description>&lt;p&gt;
DiffuserLite: &#23454;&#26102;&#25193;&#25955;&#35268;&#21010;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
DiffuserLite: Towards Real-time Diffusion Planning. (arXiv:2401.15443v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15443
&lt;/p&gt;
&lt;p&gt;
DiffuserLite&#26159;&#19968;&#20010;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#25552;&#39640;&#20915;&#31574;&#39057;&#29575;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26694;&#26550;&#65292;&#23427;&#21482;&#20135;&#29983;&#20102;&#24456;&#23567;&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#35268;&#21010;&#34987;&#35748;&#20026;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#26377;&#25928;&#30340;&#20915;&#31574;&#33539;&#24335;&#12290;&#38271;&#26102;&#38388;&#36328;&#24230;&#36712;&#36857;&#30340;&#39640;&#36136;&#37327;&#26465;&#20214;&#29983;&#25104;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#35268;&#21010;&#26041;&#27861;&#30001;&#20110;&#36845;&#20195;&#25277;&#26679;&#25104;&#26412;&#26114;&#36149;&#32780;&#23548;&#33268;&#20915;&#31574;&#39057;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DiffuserLite&#65292;&#19968;&#20010;&#24555;&#36895;&#32780;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#12290;DiffuserLite&#20351;&#29992;&#20102;&#19968;&#20010;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#29983;&#25104;&#31895;&#21040;&#32454;&#31890;&#24230;&#30340;&#36712;&#36857;&#65292;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#20887;&#20313;&#20449;&#24687;&#30340;&#24314;&#27169;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#20915;&#31574;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#26694;&#26550;&#30456;&#27604;&#65292;DiffuserLite&#20165;&#20135;&#29983;&#20102;$0.88\%$&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24179;&#22343;&#20915;&#31574;&#39057;&#29575;&#36798;&#21040;&#20102;122Hz&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24178;&#20928;DiffuserLite&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;...
&lt;/p&gt;
&lt;p&gt;
Diffusion planning has been recognized as an effective decision-making paradigm in various domains. The high-quality conditional generation capability of long-horizon trajectories makes it a promising research direction. However, existing diffusion planning methods suffer from low decision-making frequencies because of the expensive iterative sampling cost. To address this issue, we introduce DiffuserLite, a fast and lightweight diffusion planning framework. DiffuserLite employs a planning refinement process (PRP) to generate coarse-to-fine-grained trajectories, which significantly reduces the modeling of redundant information and leads to notable increases in decision-making frequency. Our experimental results demonstrate that DiffuserLite incurs only $0.88\%$ of the runtime cost compared to previous frameworks, achieves an average decision-making frequency of $122$Hz, and reaches state-of-the-art performance on D4RL benchmarks. In addition, our clean DiffuserLite framework can serve 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15378</link><description>&lt;p&gt;
&#22522;&#20110;RAG&#30340;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#25552;&#26696;&#65306;MufassirQAS LLM
&lt;/p&gt;
&lt;p&gt;
A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15378
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#29702;&#35299;&#23447;&#25945;&#23384;&#22312;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#30340;&#25361;&#25112;&#12290;&#38382;&#31572;&#26426;&#22120;&#20154;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24314;&#31435;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#29992;&#20110;&#23447;&#25945;&#21551;&#33945;&#30340;&#38382;&#39064;&#22238;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;LLM&#20063;&#26377;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20542;&#21521;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#21487;&#33021;&#21253;&#21547;&#20398;&#36785;&#20010;&#20154;&#23447;&#25945;&#20449;&#20208;&#12289;&#36328;&#23447;&#27966;&#20914;&#31361;&#21644;&#26377;&#20105;&#35758;&#25110;&#25935;&#24863;&#30340;&#35805;&#39064;&#30340;&#20869;&#23481;&#12290;&#23427;&#38656;&#35201;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#32780;&#19981;&#20250;&#23459;&#25196;&#20167;&#24680;&#35328;&#35770;&#25110;&#20882;&#29359;&#26576;&#20123;&#32676;&#20307;&#30340;&#20154;&#25110;&#20182;&#20204;&#30340;&#20449;&#20208;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#30340;&#38382;&#31572;&#31995;&#32479;&#31216;&#20026;"MufassirQAS"&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#23447;&#25945;&#34892;&#19994;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.06683</link><description>&lt;p&gt;
DQNC2S&#65306;&#22522;&#20110;DQN&#30340;&#36328;&#27969;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#19982;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#26816;&#32034;&#19982;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#22312;&#22810;&#27969;&#25968;&#25454;&#30340;&#22266;&#26377;&#20887;&#20313;&#21644;&#22810;&#26597;&#35810;&#29615;&#22659;&#19979;&#30340;&#38480;&#21046;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#26631;&#27880;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#30340;&#22312;&#32447;&#21361;&#26426;&#26102;&#38388;&#36724;&#29983;&#25104;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#23454;&#26102;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#20174;&#32780;&#20351;&#25512;&#29702;&#26102;&#38388;&#19982;&#36755;&#20837;&#26597;&#35810;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;&#20887;&#20313;&#36807;&#28388;&#22120;&#34701;&#20837;&#22870;&#21169;&#20989;&#25968;&#20013;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#36328;&#27969;&#20869;&#23481;&#37325;&#21472;&#12290;&#22312;CrisisFACTS 2022&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#36798;&#21040;&#30340;ROUGE&#21644;BERTScore&#32467;&#26524;&#20248;&#20110;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&amp;Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05975</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25366;&#25496;&#29992;&#25143;&#30340;&#24847;&#22270;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;ICLRec&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32858;&#31867;&#26469;&#25552;&#21462;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#23613;&#31649;&#23427;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#22797;&#26434;&#21644;&#32321;&#29712;&#30340;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#24191;&#20041;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#26694;&#26550;&#20013;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#32463;&#24120;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32858;&#31867;&#20250;&#24433;&#21709;&#22823;&#35268;&#27169;&#34892;&#19994;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24847;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;ELCRec&#65292;&#23427;&#23558;&#34920;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26694;&#26550;&#20013;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#20027;&#35201;&#26041;&#27861;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;</title><link>http://arxiv.org/abs/2401.02524</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#21512;&#25506;&#32034;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#20027;&#35201;&#26041;&#27861;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#39640;&#26114;&#21644;&#38544;&#31169;&#27861;&#35268;&#30340;&#38480;&#21046;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#38459;&#30861;&#20102;&#36827;&#23637;&#12290;&#21512;&#25104;&#25968;&#25454;&#25104;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21457;&#24067;&#30340;&#27169;&#22411;&#36807;&#22810;&#21644;&#26377;&#38480;&#30340;&#32508;&#36848;&#25991;&#29486;&#32473;&#20915;&#31574;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#30830;&#23450;&#20102;&#20849;&#21516;&#30340;&#29305;&#24449;&#65292;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#36235;&#21183;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20026;&#20027;&#35201;&#36235;&#21183;&#65292;&#38500;&#20102;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#29983;&#25104;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#29983;&#25104;&#27169;&#22411;&#20027;&#35201;&#26159;GAN&#65292;&#32780;&#25193;&#25955;&#27169;&#22411;&#12289;&#36716;&#25442;&#22120;&#21644;RNN&#20063;&#22312;&#31454;&#20105;&#20013;&#12290;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#20849;&#21516;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a surge in the popularity of Machine Learning (ML), applied across diverse domains. However, progress is impeded by the scarcity of training data due to expensive acquisition and privacy legislation. Synthetic data emerges as a solution, but the abundance of released models and limited overview literature pose challenges for decision-making. This work surveys 417 Synthetic Data Generation (SDG) models over the last decade, providing a comprehensive overview of model types, functionality, and improvements. Common attributes are identified, leading to a classification and trend analysis. The findings reveal increased model performance and complexity, with neural network-based approaches prevailing, except for privacy-preserving data generation. Computer vision dominates, with GANs as primary generative models, while diffusion models, transformers, and RNNs compete. Implications from our performance evaluation highlight the scarcity of common metrics and datase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.07586</link><description>&lt;p&gt;
&#29305;&#24449;&#24341;&#23548;&#65306;&#22823;&#23610;&#24230;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30340;&#23548;&#24341;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPM)&#32447;&#24615;&#22320;&#23558;&#19981;&#21516;&#30340;&#26465;&#20214;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#25552;&#20379;&#23545;&#26679;&#26412;&#30340;&#22686;&#24378;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#24403;&#23548;&#21521;&#23610;&#24230;&#21464;&#22823;&#26102;&#20135;&#29983;&#30340;&#38750;&#32447;&#24615;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#65292;&#19968;&#31181;&#37319;&#26679;&#26041;&#27861;&#65292;&#20026;&#26080;&#20998;&#31867;&#22120;&#23548;&#21521;&#30340;DDPM&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#29702;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;&#12290;&#36825;&#31181;&#26657;&#27491;&#36843;&#20351;&#23548;&#21521;&#30340;DDPM&#36981;&#23432;&#20854;&#24213;&#23618;&#25193;&#25955;&#36807;&#31243;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#65292;&#26080;&#38656;&#23548;&#25968;&#65292;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#29305;&#24449;&#24341;&#23548;&#22686;&#24378;&#20102;&#23545;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#23545;&#20174;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21040;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#22914;&#30913;&#30456;&#21464;&#30340;&#21508;&#31181;&#24212;&#29992;&#37117;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a sampling method that provides first-principle non-linear correction for classifier-free guided DDPMs. Such correction forces the guided DDPMs to respect the Fokker-Planck equation of their underlying diffusion process, in a way that is training-free, derivative-free, and compatible with existing sampling methods. Experiments show that characteristic guidance enhances control and reduces color and exposure issues in image generation, proving effective in diverse applications ranging from latent space sampling to solving physics problems like magnet phase transitions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#21442;&#25968;&#12290;&#31639;&#27861;&#36890;&#36807;&#37319;&#26679;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#38543;&#26426;&#26041;&#21521;&#65292;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.14168</link><description>&lt;p&gt;
&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms. (arXiv:2310.14168v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#21442;&#25968;&#12290;&#31639;&#27861;&#36890;&#36807;&#37319;&#26679;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#38543;&#26426;&#26041;&#21521;&#65292;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#21033;&#29992;&#20102;&#33258;&#21160;&#24494;&#20998;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#21363;&#21453;&#21521;&#27169;&#24335;&#24494;&#20998;&#65292;&#25110;&#31216;&#20026;&#21521;&#37327;&#38597;&#21487;&#27604;&#20056;&#31215;(VJP)&#65292;&#25110;&#22312;&#24494;&#20998;&#20960;&#20309;&#30340;&#32972;&#26223;&#19979;&#34987;&#31216;&#20026;&#25289;&#22238;&#36807;&#31243;&#12290;&#26799;&#24230;&#30340;&#35745;&#31639;&#23545;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#36807;&#27491;&#21521;&#27169;&#24335;AD&#25110;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;(JVP)&#39640;&#25928;&#35745;&#31639;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#36825;&#20123;JVP&#27839;&#30528;&#20174;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#65288;&#20363;&#22914;&#20271;&#21162;&#21033;&#12289;&#27491;&#24577;&#12289;&#32500;&#26684;&#32435;&#12289;&#25289;&#26222;&#25289;&#26031;&#21644;&#22343;&#21248;&#20998;&#24067;&#65289;&#37319;&#26679;&#30340;&#38543;&#26426;&#26041;&#21521;&#35745;&#31639;&#12290;&#26799;&#24230;&#30340;&#35745;&#31639;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#36827;&#34892;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#21253;&#25324;&#25910;&#25947;&#36895;&#24230;&#20197;&#21450;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation within neural networks leverages a fundamental element of automatic differentiation, which is referred to as the reverse mode differentiation, or vector Jacobian Product (VJP) or, in the context of differential geometry, known as the pull-back process. The computation of gradient is important as update of neural network parameters is performed using gradient descent method. In this study, we present a genric randomized method, which updates the parameters of neural networks by using directional derivatives of loss functions computed efficiently by using forward mode AD or Jacobian vector Product (JVP). These JVP are computed along the random directions sampled from different probability distributions e.g., Bernoulli, Normal, Wigner, Laplace and Uniform distributions. The computation of gradient is performed during the forward pass of the neural network. We also present a rigorous analysis of the presented methods providing the rate of convergence along with the computat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#32047;&#31215;&#36951;&#25022;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#19978;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#26469;&#32467;&#21512;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.11531</link><description>&lt;p&gt;
&#22312;&#26080;&#38480;&#26102;&#22495;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach. (arXiv:2310.11531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#32047;&#31215;&#36951;&#25022;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#19978;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#26469;&#32467;&#21512;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#23384;&#22312;&#19968;&#20010;&#31163;&#32447;&#25968;&#25454;&#38598;&#26102;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#35774;&#32622;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#31163;&#32447;&#25968;&#25454;&#38598;&#26159;&#30001;&#19968;&#20010;&#19987;&#23478;&#29983;&#25104;&#30340;&#65292;&#20294;&#20854;&#33021;&#21147;&#27700;&#24179;&#26410;&#30693;&#65292;&#21363;&#23427;&#19981;&#26159;&#23436;&#32654;&#30340;&#65292;&#20063;&#19981;&#19968;&#23450;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#20351;&#29992;&#30340;&#34892;&#20026;&#31574;&#30053;&#65288;&#30001;&#33021;&#21147;&#21442;&#25968;&#21442;&#25968;&#21270;&#65289;&#65292;&#22312;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#33021;&#21462;&#24471;&#26126;&#26174;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20197; $\tilde{O}(\sqrt{T})$ &#20026;&#32553;&#25918;&#30340;&#31934;&#30830;&#26377;&#29992;PSRL&#31639;&#27861;&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#36825;&#38656;&#35201;&#23545;&#36125;&#21494;&#26031;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#26080;&#38480;&#26102;&#22495;&#35774;&#32622;&#19979;&#36827;&#34892;&#26032;&#39062;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;Informed RLSVI&#31639;&#27861;&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#28982;&#21518;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11085</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25512;&#26029;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#35201;&#27714;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#25110;&#25512;&#26029;&#23427;&#20204;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;(2)&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#23545;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#30340;&#30740;&#31350;&#32773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#22312;&#28040;&#38500;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#24615;&#30340;&#20248;&#21183;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;DocRED&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.10107</link><description>&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs. (arXiv:2310.10107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20110;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#65292;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#30340;&#23398;&#20064;&#30001;&#20110;&#35266;&#23519;&#25968;&#25454;&#38590;&#20197;&#35299;&#35835;&#32780;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#30693;&#36716;&#31227;&#21644;&#35266;&#27979;&#27169;&#22411;&#30340;POMDPs&#20013;&#30340;&#24207;&#21015;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#21518;&#39564;&#37319;&#26679;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PSRL&#65289;&#22312;POMDPs&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35777;&#26126;&#20854;&#36125;&#21494;&#26031;&#36951;&#25022;&#38543;&#30528;&#24207;&#21015;&#30340;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#32780;&#32553;&#23567;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36951;&#25022;&#38543;&#30528;&#26102;&#38388;&#38271;&#24230;$H$&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#19979;&#30028;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;POMDP&#26159;&#27424;&#23436;&#22791;&#19988;&#24369;&#21487;&#35782;&#21035;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#30456;&#27604;&#20110;arXiv:2204.08967&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#25913;&#36827;&#20102;&#36951;&#25022;&#30028;&#32422;$\Omega(H^2\sqrt{SA})$&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to Markov Decision Processes (MDPs), learning in Partially Observable Markov Decision Processes (POMDPs) can be significantly harder due to the difficulty of interpreting observations. In this paper, we consider episodic learning problems in POMDPs with unknown transition and observation models. We consider the Posterior Sampling-based Reinforcement Learning (PSRL) algorithm for POMDPs and show that its Bayesian regret scales as the square root of the number of episodes. In general, the regret scales exponentially with the horizon length $H$, and we show that this is inevitable by providing a lower bound. However, under the condition that the POMDP is undercomplete and weakly revealing, we establish a polynomial Bayesian regret bound that improves the regret bound by a factor of $\Omega(H^2\sqrt{SA})$ over the recent result by arXiv:2204.08967.
&lt;/p&gt;</description></item><item><title>&#12298;&#26469;&#33258;&#24425;&#31080;&#31080;&#38598;&#25104;&#30340;&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#12299;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#29616;&#35937;&#65292;&#21457;&#29616;&#20854;&#19982;&#24425;&#31080;&#31080;&#38598;&#25104;&#26377;&#20851;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#26032;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.02258</link><description>&lt;p&gt;
&#12298;&#26469;&#33258;&#24425;&#31080;&#31080;&#38598;&#25104;&#30340;&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Neural Scaling Law from Lottery Ticket Ensembling. (arXiv:2310.02258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02258
&lt;/p&gt;
&lt;p&gt;
&#12298;&#26469;&#33258;&#24425;&#31080;&#31080;&#38598;&#25104;&#30340;&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#12299;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#29616;&#35937;&#65292;&#21457;&#29616;&#20854;&#19982;&#24425;&#31080;&#31080;&#38598;&#25104;&#26377;&#20851;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#26032;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#65288;NSL&#65289;&#25351;&#30340;&#26159;&#27169;&#22411;&#24615;&#33021;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#32780;&#25552;&#39640;&#30340;&#29616;&#35937;&#12290;Sharma&#65286;Kaplan&#20351;&#29992;&#36817;&#20284;&#29702;&#35770;&#20998;&#26512;&#20102;NSL&#65292;&#24182;&#39044;&#27979;&#20102;MSE&#25439;&#22833;&#30340;&#34928;&#20943;&#26041;&#24335;&#20026;$N^{-\alpha}$&#65292;&#20854;&#20013;$\alpha=4/d$&#65292;$N$&#20026;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;$d$&#20026;&#20869;&#22312;&#36755;&#20837;&#32500;&#24230;&#12290;&#23613;&#31649;&#20182;&#20204;&#30340;&#29702;&#35770;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25928;&#26524;&#33391;&#22909;&#65288;&#20363;&#22914;ReLU&#32593;&#32476;&#65289;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#31616;&#21333;&#30340;1D&#38382;&#39064;$y=x^2$&#20013;&#65292;&#34920;&#29616;&#20986;&#20102;&#19982;&#20182;&#20204;&#39044;&#27979;&#19981;&#21516;&#30340;&#32553;&#25918;&#23450;&#24459;&#65288;$\alpha=1$&#32780;&#19981;&#26159;$\alpha=4$&#65289;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#31070;&#32463;&#32593;&#32476;&#24182;&#21457;&#29616;&#26032;&#30340;&#32553;&#25918;&#23450;&#24459;&#28304;&#20110;&#24425;&#31080;&#31080;&#38598;&#25104;&#65306;&#24179;&#22343;&#32780;&#35328;&#65292;&#26356;&#23485;&#30340;&#32593;&#32476;&#26377;&#26356;&#22810;&#30340;&#8220;&#24425;&#31080;&#31080;&#8221;&#65292;&#23427;&#20204;&#34987;&#38598;&#25104;&#26469;&#20943;&#23567;&#36755;&#20986;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26426;&#26800;&#35299;&#37322;&#20197;&#21450;&#23545;&#23427;&#20204;&#36827;&#34892;&#32479;&#35745;&#30740;&#31350;&#26469;&#25903;&#25345;&#38598;&#25104;&#26426;&#21046;&#12290;&#25105;&#20204;&#23558;$N^{-1}$&#30340;&#32553;&#25918;&#23450;&#24459;&#24402;&#22240;&#20110;&#8220;&#24425;&#31080;&#31080;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#8221;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural scaling laws (NSL) refer to the phenomenon where model performance improves with scale. Sharma &amp; Kaplan analyzed NSL using approximation theory and predict that MSE losses decay as $N^{-\alpha}$, $\alpha=4/d$, where $N$ is the number of model parameters, and $d$ is the intrinsic input dimension. Although their theory works well for some cases (e.g., ReLU networks), we surprisingly find that a simple 1D problem $y=x^2$ manifests a different scaling law ($\alpha=1$) from their predictions ($\alpha=4$). We opened the neural networks and found that the new scaling law originates from lottery ticket ensembling: a wider network on average has more "lottery tickets", which are ensembled to reduce the variance of outputs. We support the ensembling mechanism by mechanistically interpreting single neural networks, as well as studying them statistically. We attribute the $N^{-1}$ scaling law to the "central limit theorem" of lottery tickets. Finally, we discuss its potential implications f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;QXAI&#26694;&#26550;&#65292;&#29992;&#20110;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#23545;&#20110;&#20915;&#31574;&#32773;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#32467;&#26524;&#36827;&#34892;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10293</link><description>&lt;p&gt;
QXAI&#65306;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#29992;&#20110;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems. (arXiv:2309.10293v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10293
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;QXAI&#26694;&#26550;&#65292;&#29992;&#20110;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#23545;&#20110;&#20915;&#31574;&#32773;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#32467;&#26524;&#36827;&#34892;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21487;&#29992;&#20110;&#23545;&#24739;&#32773;&#30340;&#36523;&#20307;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#21644;&#39044;&#27979;&#36828;&#31243;&#24739;&#32773;&#30417;&#27979;&#30340;&#29983;&#21629;&#20307;&#24449;&#12290;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65289;&#30340;&#22238;&#24402;&#20998;&#26512;&#30001;&#20110;&#20854;&#40657;&#30418;&#24615;&#36136;&#32780;&#20855;&#26377;&#26377;&#38480;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#21487;&#33021;&#38656;&#35201;&#20915;&#31574;&#32773;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#32467;&#26524;&#30450;&#30446;&#20915;&#31574;&#12290;&#22312;&#38750;&#20405;&#20837;&#24335;&#30417;&#27979;&#20013;&#65292;&#36319;&#36394;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#24739;&#32773;&#25968;&#25454;&#21644;&#20854;&#30456;&#20851;&#20020;&#24202;&#29305;&#24449;&#20316;&#20026;&#39044;&#27979;&#26410;&#26469;&#29983;&#21629;&#20307;&#24449;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#35299;&#37322;&#21508;&#31181;&#29305;&#24449;&#23545;&#30417;&#27979;&#24212;&#29992;&#25972;&#20307;&#36755;&#20986;&#30340;&#36129;&#29486;&#23545;&#20110;&#20020;&#24202;&#21307;&#24072;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;QXAI&#26694;&#26550;&#65292;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#20107;&#21518;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;Shapley&#20540;&#36827;&#34892;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence techniques can be used to classify a patient's physical activities and predict vital signs for remote patient monitoring. Regression analysis based on non-linear models like deep learning models has limited explainability due to its black-box nature. This can require decision-makers to make blind leaps of faith based on non-linear model results, especially in healthcare applications. In non-invasive monitoring, patient data from tracking sensors and their predisposing clinical attributes act as input features for predicting future vital signs. Explaining the contributions of various features to the overall output of the monitoring application is critical for a clinician's decision-making. In this study, an Explainable AI for Quantitative analysis (QXAI) framework is proposed with post-hoc model explainability and intrinsic explainability for regression and classification tasks in a supervised learning approach. This was achieved by utilizing the Shapley values c
&lt;/p&gt;</description></item><item><title>FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.10283</link><description>&lt;p&gt;
FRAMU: &#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning. (arXiv:2309.10283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10283
&lt;/p&gt;
&lt;p&gt;
FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#36890;&#36807;&#20801;&#35768;&#20174;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#20013;&#21024;&#38500;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#65292;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20351;&#29992;&#36807;&#26102;&#30340;&#12289;&#31169;&#26377;&#30340;&#21644;&#26080;&#20851;&#30340;&#25968;&#25454;&#20250;&#24341;&#21457;&#19982;&#38544;&#31169;&#21644;&#27169;&#22411;&#25928;&#29575;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#19981;&#20165;&#24433;&#21709;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#36951;&#24536;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#36824;&#20250;&#23545;&#25968;&#25454;&#38544;&#31169;&#36896;&#25104;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;&#65288;FRAMU&#65289;&#12290;&#35813;&#26694;&#26550;&#34701;&#21512;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#26159;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#65288;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#65289;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;FRAMU&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#12289;&#36951;&#24536;&#36807;&#26102;&#12289;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25903;&#25345;&#27169;&#22411;&#25345;&#32493;&#28436;&#36827;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#28145;&#24230;&#23398;&#20064;&#30340;&#38480;&#21046;&#65292;&#24182;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.10186</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#26234;&#33021;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence. (arXiv:2309.10186v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#28145;&#24230;&#23398;&#20064;&#30340;&#38480;&#21046;&#65292;&#24182;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20197;&#20854;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#24207;&#21015;&#20219;&#21153;&#21644;&#23398;&#20064;&#28508;&#22312;&#25968;&#25454;&#27169;&#24335;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#32034;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#20551;&#35774;&#25968;&#25454;&#31561;&#38388;&#38548;&#26377;&#24207;&#20197;&#21450;&#26080;&#27861;&#20805;&#20998;&#34701;&#20837;&#22270;&#32467;&#26500;&#31561;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#24182;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GNN&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;GNN&#33021;&#22815;&#26174;&#24335;&#22320;&#23558;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#32435;&#20837;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26356;&#33258;&#28982;&#30340;&#26041;&#24335;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20363;&#22914;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#20013;&#30340;&#26102;&#24207;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is well known for its ability to model sequential tasks and learn latent data patterns adaptively. Deep learning models have been widely explored and adopted in regression and classification tasks. However, deep learning has its limitations such as the assumption of equally spaced and ordered data, and the lack of ability to incorporate graph structure in terms of time-series prediction. Graphical neural network (GNN) has the ability to overcome these challenges and capture the temporal dependencies in time-series data. In this study, we propose a novel approach for predicting time-series data using GNN and monitoring with Reinforcement Learning (RL). GNNs are able to explicitly incorporate the graph structure of the data into the model, allowing them to capture temporal dependencies in a more natural way. This approach allows for more accurate predictions in complex temporal structures, such as those found in healthcare, traffic and weather forecasting. We also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#23398;&#29983;&#30340;&#31572;&#39064;&#35760;&#24405;&#20013;&#30452;&#25509;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#31572;&#39064;&#35760;&#24405;&#26469;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00300</link><description>&lt;p&gt;
&#29992;&#32534;&#30721;-&#35299;&#30721;&#22120;&#36827;&#34892;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#26469;&#24314;&#27169;&#23398;&#29983;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance. (arXiv:2309.00300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#23398;&#29983;&#30340;&#31572;&#39064;&#35760;&#24405;&#20013;&#30452;&#25509;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#31572;&#39064;&#35760;&#24405;&#26469;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#26088;&#22312;&#26681;&#25454;&#23398;&#29983;&#22312;&#32771;&#35797;&#39064;&#30446;&#19978;&#30340;&#31572;&#39064;&#25104;&#32489;&#26469;&#35786;&#26029;&#20182;&#20204;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#36825;&#26159;&#35768;&#22810;&#39046;&#22495;&#22914;&#35745;&#31639;&#33258;&#36866;&#24212;&#27979;&#35797;&#30340;&#22522;&#30784;&#12290;&#29616;&#26377;&#30340;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#65288;CDMs&#65289;&#36981;&#24490;&#20102;&#19968;&#20010;&#33021;&#21147;-&#21709;&#24212;&#33539;&#24335;&#65292;&#21363;&#23558;&#35786;&#26029;&#32467;&#26524;&#35270;&#20026;&#23398;&#29983;&#21709;&#24212;&#30340;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26469;&#23398;&#20064;&#35786;&#26029;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#24456;&#23481;&#26131;&#23548;&#33268;&#19981;&#21487;&#35782;&#21035;&#30340;&#35786;&#26029;&#32467;&#26524;&#21644;&#35299;&#37322;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#23398;&#29983;&#23398;&#20064;&#34920;&#29616;&#30340;&#37327;&#21270;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#35786;&#26029;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#30452;&#25509;&#20174;&#21709;&#24212;&#26085;&#24535;&#20013;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#39044;&#27979;&#27169;&#22359;&#20174;&#35786;&#26029;&#32467;&#26524;&#20013;&#37325;&#24314;&#21709;&#24212;&#26085;&#24535;&#65292;&#20197;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive diagnosis aims to diagnose students' knowledge proficiencies based on their response scores on exam questions, which is the basis of many domains such as computerized adaptive testing. Existing cognitive diagnosis models (CDMs) follow a proficiency-response paradigm, which views diagnostic results as learnable embeddings that are the cause of students' responses and learns the diagnostic results through optimization. However, such a paradigm can easily lead to unidentifiable diagnostic results and the explainability overfitting problem, which is harmful to the quantification of students' learning performance. To address these problems, we propose a novel identifiable cognitive diagnosis framework. Specifically, we first propose a flexible diagnostic module which directly diagnose identifiable and explainable examinee traits and question features from response logs. Next, we leverage a general predictive module to reconstruct response logs from the diagnostic results to ensure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11842</link><description>&lt;p&gt;
&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.11842v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#30028;&#20013;&#35782;&#21035;&#21644;&#20998;&#26512;&#23545;&#31216;&#27169;&#24335;&#24050;&#32463;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#20363;&#22914;&#29289;&#29702;&#23398;&#20013;&#30340;&#24341;&#21147;&#23450;&#24459;&#30340;&#21046;&#23450;&#21644;&#21270;&#23398;&#32467;&#26500;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#21033;&#29992;&#22312;&#26576;&#20123;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#20197;&#21450;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24418;&#24335;&#21270;&#22320;&#25551;&#36848;&#19968;&#31867;&#20855;&#26377;&#19968;&#33324;&#23545;&#31216;&#24615;&#27010;&#24565;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#35813;&#27010;&#24565;&#20801;&#35768;&#23384;&#22312;&#23545;&#31216;&#30340;&#26368;&#20248;&#20540;&#21644;&#31574;&#30053;&#12290;&#21463;&#21040;&#36825;&#20123;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20316;&#20026;&#22810;&#26234;&#20307;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#22312;&#20855;&#26377;&#37325;&#22797;&#23545;&#31216;&#27169;&#24335;&#30340;&#26410;&#35265;&#22330;&#26223;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#31561;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2307.01403</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21327;&#35843;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#20294;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#35825;&#23548;&#19968;&#20010;&#26377;&#25928;&#30340;&#20849;&#21516;&#35821;&#35328;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#35270;&#35282;&#65292;&#21363;&#23558;&#26234;&#33021;&#20307;&#20043;&#38388;&#21457;&#36865;&#30340;&#36890;&#20449;&#28040;&#24687;&#35270;&#20026;&#29615;&#22659;&#29366;&#24577;&#30340;&#19981;&#23436;&#25972;&#35270;&#22270;&#12290;&#36890;&#36807;&#26816;&#26597;&#21457;&#36865;&#21644;&#25509;&#25910;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#32473;&#23450;&#36712;&#36857;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#20351;&#29992;&#23450;&#24615;&#25351;&#26631;&#21644;&#34920;&#31034;&#25506;&#27979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#35825;&#23548;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#24182;&#20174;&#29615;&#22659;&#20013;&#25429;&#33719;&#20102;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#21147;&#37327;&#20197;&#21450;&#21033;&#29992;&#28040;&#24687;&#20316;&#20026;&#32534;&#30721;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09896</link><description>&lt;p&gt;
&#25581;&#31192; GPT &#33258;&#25105;&#20462;&#22797;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#19978;&#20173;&#38754;&#20020;&#22256;&#38590;&#12290;&#33258;&#25105;&#20462;&#22797;&#8212;&#8212;&#21363;&#27169;&#22411;&#35843;&#35797;&#24182;&#20462;&#22797;&#33258;&#24049;&#30340;&#20195;&#30721;&#8212;&#8212;&#26368;&#36817;&#25104;&#20026;&#25552;&#39640;&#24615;&#33021;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#25105;&#20462;&#22797;&#22914;&#20309;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26377;&#20154;&#20250;&#24819;&#30693;&#36947;&#65292;&#24403;&#21516;&#19968;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#26102;&#65292;&#27169;&#22411;&#31350;&#31455;&#33021;&#21542;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#39304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#31181;&#32534;&#30721;&#25361;&#25112;&#32452;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#31574;&#30053; pass@t&#65292;&#35813;&#31574;&#30053;&#34913;&#37327;&#20102;&#20219;&#21153;&#36890;&#36807;&#29575;&#19982;&#20174;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#24635;&#26631;&#35760;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20165;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#33258;&#25105;&#20462;&#22797;&#34920;&#29616;&#30340;&#20960;&#20010;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36755;&#20837;&#22122;&#22768;&#36739;&#23569;&#19988;&#27169;&#22411;&#23545;&#21021;&#22987;&#36755;&#20986;&#19981;&#22826;&#33258;&#20449;&#30340;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#33258;&#25105;&#20462;&#22797;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#21453;&#39304;&#26469;&#22686;&#24378; GPT &#27169;&#22411;&#30340;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#65292;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.05817</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22914;&#20309;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21463;&#30410;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Can Recommender Systems Benefit from Large Language Models: A Survey. (arXiv:2306.05817v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#21305;&#37197;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#25351;&#20196;&#36319;&#36394;&#12289;&#25512;&#29702;&#65289;&#65292;&#20174;&#32780;&#20026;&#23558;LLM&#35843;&#25972;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#30740;&#31350;&#26041;&#21521;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24212;&#29992;&#23548;&#21521;&#30340;&#35282;&#24230;&#23545;&#27492;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20004;&#20010;&#27491;&#20132;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#23545;&#20110;&#8220;&#22312;&#21738;&#37324;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLM&#22312;&#25512;&#33616;&#27969;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#20013;&#21487;&#33021;&#21457;&#25381;&#30340;&#20316;&#29992;&#65292;&#21363;&#29305;&#24449;&#24037;&#31243;&#12289;&#29305;&#24449;&#32534;&#30721;&#22120;&#12289;&#35780;&#20998;/&#25490;&#21517;&#20989;&#25968;&#21644;&#27969;&#31243;&#25511;&#21046;&#22120;&#12290;&#23545;&#20110;&#8220;&#22914;&#20309;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#20174;&#32780;&#24471;&#20986;&#20004;&#20010;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#26631;&#20934;&#65292;&#21363;&#26159;&#21542;&#35843;&#25972;LLM&#21644;&#26159;&#21542;&#23558;LLM&#20316;&#20026;&#29420;&#31435;&#27169;&#22411;&#25110;&#28151;&#21512;&#27169;&#22411;&#32452;&#20214;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#23558;LLM&#35843;&#25972;&#21040;RS&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#28508;&#22312;&#26041;&#21521;&#65292;&#21253;&#25324;&#19982;&#29616;&#26377;&#31995;&#32479;&#30340;&#38598;&#25104;&#12289;&#29992;&#25143;&#21453;&#39304;&#12289;&#35780;&#20272;&#24230;&#37327;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems (RS) play important roles to match users' information needs for Internet applications. In natural language processing (NLP) domains, large language model (LLM) has shown astonishing emergent abilities (e.g., instruction following, reasoning), thus giving rise to the promising research direction of adapting LLM to RS for performance enhancements and user experience improvements. In this paper, we conduct a comprehensive survey on this research direction from an application-oriented view. We first summarize existing research works from two orthogonal perspectives: where and how to adapt LLM to RS. For the "WHERE" question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, and pipeline controller. For the "HOW" question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLMs or not, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;CALVIN&#22522;&#20934;&#27979;&#35797;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.19075</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#27169;&#20223;&#23398;&#20064;&#19982;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data. (arXiv:2305.19075v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;CALVIN&#22522;&#20934;&#27979;&#35797;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#29702;&#35299;&#21644;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#25805;&#20316;&#29289;&#20307;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#35821;&#35328;&#26465;&#20214;&#26041;&#27861;&#22312;&#29087;&#24713;&#30340;&#29615;&#22659;&#20013;&#22788;&#29702;&#20219;&#21153;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#35774;&#32622;&#26041;&#38754;&#36935;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#38646;-shot&#35774;&#32622;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;CALVIN&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#36229;&#36807;&#20102;&#20197;&#21069;&#25253;&#21578;&#30340;&#24471;&#20998;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;-shot&#22810;&#29615;&#22659;&#35774;&#32622;&#20013;&#12290;&#23436;&#25104;&#20219;&#21153;&#30340;&#24179;&#22343;&#38271;&#24230;&#20026;...
&lt;/p&gt;
&lt;p&gt;
The growing interest in language-conditioned robot manipulation aims to develop robots capable of understanding and executing complex tasks, with the objective of enabling robots to interpret language commands and manipulate objects accordingly. While language-conditioned approaches demonstrate impressive capabilities for addressing tasks in familiar environments, they encounter limitations in adapting to unfamiliar environment settings. In this study, we propose a general-purpose, language-conditioned approach that combines base skill priors and imitation learning under unstructured data to enhance the algorithm's generalization in adapting to unfamiliar environments. We assess our model's performance in both simulated and real-world environments using a zero-shot setting. In the simulated environment, the proposed approach surpasses previously reported scores for CALVIN benchmark, especially in the challenging Zero-Shot Multi-Environment setting. The average completed task length, in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15659</link><description>&lt;p&gt;
&#22914;&#20309;&#36867;&#31163;&#38160;&#21270;&#30340;&#26497;&#23567;&#20540;&#28857;
&lt;/p&gt;
&lt;p&gt;
How to escape sharp minima. (arXiv:2305.15659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#20123;&#31639;&#27861;&#34987;&#35774;&#35745;&#29992;&#26469;&#21457;&#29616;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#31639;&#27861;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#25439;&#22833;&#20989;&#25968;&#28023;&#26862;&#30697;&#38453;&#30340;&#36857;&#26469;&#24230;&#37327;&#23427;&#30340;&#24179;&#22374;&#31243;&#24230;&#65292;&#24182;&#24418;&#24335;&#21270;&#23450;&#20041;&#20102;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#27010;&#24565;&#12290;&#22312;&#27492;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#12290;&#38024;&#23545;&#19968;&#33324;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;&#12290;&#31639;&#27861;&#30340;&#20027;&#35201;&#32452;&#20214;&#26159;&#20351;&#29992;&#20174;&#38543;&#26426;&#25200;&#21160;&#36845;&#20195;&#20013;&#35745;&#31639;&#30340;&#26799;&#24230;&#26469;&#20272;&#35745;&#23548;&#33268;&#26356;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#26041;&#21521;&#12290;&#23545;&#20110;&#25104;&#26412;&#20989;&#25968;&#26159;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#31639;&#27861;&#65292;&#21463;&#26368;&#36817;&#25552;&#20986;&#30340;&#23454;&#29992;&#31639;&#27861;&#8212;&#8212;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning applications have seen a remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this paradigm, this work formulates and studies the algorithmic question of how to find flat minima. As an initial effort, this work adopts the trace of hessian of the cost function as the measure of flatness, and formally defines the notion of approximate flat minima. Under this notion, we then design algorithms that find approximate flat minima efficiently. For general cost functions, we present a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima. For the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, support
&lt;/p&gt;</description></item><item><title>AI&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#22823;&#22810;&#25968;&#20915;&#31574;&#29615;&#22659;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.07722</link><description>&lt;p&gt;
&#23547;&#27714;&#21487;&#39564;&#35777;&#24615;: &#35299;&#37322;&#24456;&#23569;&#33021;&#22815;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#25552;&#39640;&#20915;&#31574;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making. (arXiv:2305.07722v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07722
&lt;/p&gt;
&lt;p&gt;
AI&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#22823;&#22810;&#25968;&#20915;&#31574;&#29615;&#22659;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#25991;&#29486;&#65292;&#28041;&#21450;&#21487;&#35299;&#37322;&#30340;AI&#31995;&#32479;&#20026;&#20154;&#31867;&#20915;&#31574;&#32773;&#25552;&#20379;&#24314;&#35758;&#65292;&#24182;&#21576;&#29616;&#20986;&#19968;&#31995;&#21015;&#19981;&#30830;&#23450;&#21644;&#20196;&#20154;&#22256;&#24785;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#32508;&#21512;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#65292;&#38416;&#26126;&#20102;AI&#35299;&#37322;&#32463;&#24120;&#26080;&#27861;&#20419;&#20351;&#36866;&#24403;&#30340;&#20381;&#36182;&#21644;&#20114;&#34917;&#20915;&#31574;&#34920;&#29616;&#30340;&#22833;&#36133;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#26399;&#26395;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#25110;&#28165;&#26224;&#38416;&#36848;AI&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20915;&#31574;&#29615;&#22659;&#20013;&#65292;AI&#35299;&#37322;&#24182;&#26410;&#20419;&#36827;&#36825;&#31181;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#35299;&#37322;&#26041;&#27861;&#22914;&#20309;&#65292;&#22823;&#22810;&#25968;&#29615;&#22659;&#22522;&#26412;&#19978;&#37117;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#26368;&#21518;&#35752;&#35770;&#20102;&#26356;&#26377;&#25928;&#30340;&#21487;&#35299;&#37322;AI&#36741;&#21161;&#20915;&#31574;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2305.03731</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Assessing Working Memory Capacity of ChatGPT. (arXiv:2305.03731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#26159;&#20154;&#31867;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20316;&#20026;&#20449;&#24687;&#20020;&#26102;&#23384;&#20648;&#21644;&#25805;&#20316;&#30340;&#24037;&#20316;&#31354;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;ChatGPT&#22312;N-back&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35843;&#26597;&#20102;&#36825;&#19968;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#24037;&#20316;&#35760;&#24518;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#25509;&#30528;&#20171;&#32461;&#20102;&#35780;&#20272;ChatGPT&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#22312;&#35328;&#35821;&#21644;&#31354;&#38388;N- back&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#25991;&#29486;&#25253;&#36947;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24403;&#21069;&#36827;&#23637;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#65292;&#24182;&#20026;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29702;&#35299;&#20154;&#31867;&#24037;&#20316;&#35760;&#24518;&#30340;&#26410;&#26469;&#21162;&#21147;&#25552;&#20379;&#20102;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory is a critical aspect of both human intelligence and artificial intelligence (AI), serving as a workspace for the temporary storage and manipulation of information. This paper investigates working memory capacity of ChatGPT, a state-of-the-art language model, by examining its performance on N-back tasks. We begin by discussing the importance of working memory to humans and AI, followed by the methods employed to assess working memory capacity of ChatGPT. Our study compares behavioral performance of ChatGPT on verbal and spatial N-back tasks to that of human participants reported in the literature, revealing notable similarities. Our findings offer crucial insights into the current progress in designing AI systems with human-level cognitive abilities and hold promise for informing future endeavors aimed at enhancing AI working memory and understanding human working memory through AI models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#26469;&#25552;&#39640;&#22270;&#20687;&#23383;&#24149;&#36136;&#37327;&#30340;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#36164;&#28304;&#35757;&#32451;&#20102;&#27604;&#22522;&#20934;&#27169;&#22411;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.03610</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Curation for Image Captioning with Text-to-Image Generative Models. (arXiv:2305.03610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#26469;&#25552;&#39640;&#22270;&#20687;&#23383;&#24149;&#36136;&#37327;&#30340;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#36164;&#28304;&#35757;&#32451;&#20102;&#27604;&#22522;&#20934;&#27169;&#22411;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#20687;&#23383;&#24149;&#25216;&#26415;&#30340;&#21457;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#26085;&#30410;&#20381;&#36182;&#20110;&#35745;&#31639;&#36164;&#28304;&#21644;&#36234;&#26469;&#36234;&#22823;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#30340;&#20004;&#31181;&#26041;&#27861;&#25506;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25552;&#39640;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#36136;&#37327;&#26469;&#25913;&#21892;&#24615;&#33021;&#65306;&#19968;&#31181;&#26041;&#27861;&#20551;&#23450;&#30001;&#20110;&#22270;&#20687;&#21644;&#23383;&#24149;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#26576;&#20123;&#31034;&#20363;&#24212;&#35813;&#36991;&#20813;&#20351;&#29992;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#21017;&#20551;&#23450;&#19981;&#21305;&#37197;&#21487;&#20197;&#36890;&#36807;&#26367;&#25442;&#22270;&#20687;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in image captioning are mainly driven by large-scale vision-language pretraining, relying heavily on computational resources and increasingly large multimodal datasets. Instead of scaling up pretraining data, we ask whether it is possible to improve performance by improving the quality of the samples in existing datasets. We pursue this question through two approaches to data curation: one that assumes that some examples should be avoided due to mismatches between the image and caption, and one that assumes that the mismatch can be addressed by replacing the image, for which we use the state-of-the-art Stable Diffusion model. These approaches are evaluated using the BLIP model on MS COCO and Flickr30K in both finetuning and few-shot learning settings. Our simple yet effective approaches consistently outperform baselines, indicating that better image captioning models can be trained by curating existing resources. Finally, we conduct a human study to understand the error
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38754;&#20020;&#30528;&#30693;&#35782;&#21644;&#21160;&#26426;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#27010;&#24565;&#21270;&#12289;&#27979;&#37327;&#21644;&#20248;&#21270;&#24184;&#31119;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;AI&#34892;&#21160;&#65292;&#20197;&#21450;&#28608;&#21169;&#25514;&#26045;&#12289;&#36130;&#21153;&#21644;&#23459;&#20256;&#39118;&#38505;&#30340;&#19981;&#19968;&#33268;&#20197;&#21450;&#25968;&#25454;&#33719;&#21462;&#30340;&#32570;&#20047;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#31185;&#23398;&#29702;&#35299;AI&#31995;&#32479;&#23545;&#24184;&#31119;&#24433;&#21709;&#26041;&#38754;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25351;&#23548;&#35774;&#35745;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.12241</link><description>&lt;p&gt;
&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#65306;&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Positive AI: Key Challenges for Designing Wellbeing-aligned Artificial Intelligence. (arXiv:2304.12241v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12241
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38754;&#20020;&#30528;&#30693;&#35782;&#21644;&#21160;&#26426;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#27010;&#24565;&#21270;&#12289;&#27979;&#37327;&#21644;&#20248;&#21270;&#24184;&#31119;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;AI&#34892;&#21160;&#65292;&#20197;&#21450;&#28608;&#21169;&#25514;&#26045;&#12289;&#36130;&#21153;&#21644;&#23459;&#20256;&#39118;&#38505;&#30340;&#19981;&#19968;&#33268;&#20197;&#21450;&#25968;&#25454;&#33719;&#21462;&#30340;&#32570;&#20047;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#31185;&#23398;&#29702;&#35299;AI&#31995;&#32479;&#23545;&#24184;&#31119;&#24433;&#21709;&#26041;&#38754;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25351;&#23548;&#35774;&#35745;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#36805;&#36895;&#25913;&#21464;&#31038;&#20250;&#65292;&#36843;&#20999;&#38656;&#35201;&#30830;&#20445;&#20854;&#31215;&#26497;&#24433;&#21709;&#12290;&#26412;&#25991;&#37319;&#29992;&#31215;&#26497;&#35774;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#20854;&#35270;&#20026;&#35774;&#35745;&#20027;&#21160;&#25903;&#25345;&#20154;&#31867;&#24184;&#31119;&#30340;AI&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;AI&#31995;&#32479;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#37319;&#29992;&#25511;&#21046;&#35770;&#30340;&#35270;&#35282;&#65292;&#35782;&#21035;&#20102;&#20004;&#20010;&#31867;&#21035;&#20013;&#30340;&#21313;&#20108;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#30693;&#35782;&#32570;&#20047;&#21644;&#21160;&#26426;&#32570;&#20047;&#12290;&#30693;&#35782;&#38556;&#30861;&#21253;&#25324;&#27010;&#24565;&#21270;&#12289;&#27979;&#37327;&#21644;&#20248;&#21270;&#24184;&#31119;&#65292;&#24182;&#35774;&#35745;&#36866;&#24403;&#30340;AI&#34892;&#21160;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#21160;&#26426;&#38556;&#30861;&#21253;&#25324;&#19981;&#19968;&#33268;&#30340;&#28608;&#21169;&#25514;&#26045;&#12289;&#36130;&#21153;&#21644;&#23459;&#20256;&#39118;&#38505;&#65292;&#20197;&#21450;&#32570;&#20047;&#25968;&#25454;&#33719;&#21462;&#38459;&#27490;&#20102;&#65288;&#31532;&#19977;&#26041;&#65289;&#23545;&#24184;&#31119;&#36827;&#34892;&#30740;&#31350;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#35758;&#31243;&#65292;&#21253;&#25324;&#25512;&#36827;&#23545;AI&#31995;&#32479;&#23545;&#24184;&#31119;&#24433;&#21709;&#30340;&#31185;&#23398;&#29702;&#35299;&#65292;&#24182;&#25351;&#23548;&#22914;&#20309;&#36827;&#34892;AI&#31995;&#32479;&#30340;&#35774;&#35745;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is rapidly transforming society, creating an urgent need to ensure its positive impact. In this article, we take a positive design approach towards this issue, viewing it as a matter of designing AI systems that actively support human wellbeing. However, designing wellbeing-aligned AI systems is difficult. This article adopts a cybernetic perspective to identify twelve key challenges across two categories: lack of knowledge and lack of motivation. Knowledge barriers include challenges in conceptualizing, measuring, and optimizing for wellbeing, then designing appropriate AI actions. Motivation barriers include misaligned incentives, financial and publicity risks, and a lack of data access preventing (third-party) research on wellbeing. To address these challenges we have captured our key takeaways in a research agenda related to 1) advancing the scientific understanding of the impact of AI systems on wellbeing, and 2) guiding design actions on how AI system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#23459;&#35328;&#65292;&#35748;&#20026;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;</title><link>http://arxiv.org/abs/2304.03674</link><description>&lt;p&gt;
&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#20221;&#23459;&#35328;
&lt;/p&gt;
&lt;p&gt;
Machine Learning with Requirements: a Manifesto. (arXiv:2304.03674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#23459;&#35328;&#65292;&#35748;&#20026;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#27493;&#65292;&#25104;&#20026;&#35768;&#22810;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#31361;&#30772;&#30340;&#26681;&#28304;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#23427;&#20204;&#24212;&#29992;&#21040;&#39640;&#39118;&#38505;&#25110;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#23481;&#26131;&#21464;&#24471;&#33030;&#24369;&#21644;&#19981;&#21487;&#38752;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38656;&#27714;&#35268;&#26684;&#35828;&#26126;&#26377;&#30410;&#22320;&#25972;&#21512;&#21040;&#26631;&#20934;&#30340;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#27969;&#31243;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recent years, machine learning has made great advancements that have been at the root of many breakthroughs in different application domains. However, it is still an open issue how make them applicable to high-stakes or safety-critical application domains, as they can often be brittle and unreliable. In this paper, we argue that requirements definition and satisfaction can go a long way to make machine learning models even more fitting to the real world, especially in critical domains. To this end, we present two problems in which (i) requirements arise naturally, (ii) machine learning models are or can be fruitfully deployed, and (iii) neglecting the requirements can have dramatic consequences. We show how the requirements specification can be fruitfully integrated into the standard machine learning development pipeline, proposing a novel pyramid development process in which requirements definition may impact all the subsequent phases in the pipeline, and viceversa.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#26469;&#23454;&#29616;&#36523;&#20307;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20182;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#34920;&#24449;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18240</link><description>&lt;p&gt;
&#23547;&#25214;&#20855;&#26377;&#36523;&#20307;&#26234;&#33021;&#30340;&#20154;&#24037;&#35270;&#35273;&#30382;&#23618;&#22312;&#21738;&#37324;&#65311;
&lt;/p&gt;
&lt;p&gt;
Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?. (arXiv:2303.18240v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#26469;&#23454;&#29616;&#36523;&#20307;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20182;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#34920;&#24449;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#24449;&#65288;PVR&#65289;&#25110;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#29992;&#20110;&#36523;&#20307;&#26234;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102; CortexBench&#65292;&#20854;&#20013;&#21253;&#25324;&#28085;&#30422;&#21160;&#21147;&#23398;&#12289;&#23548;&#33322;&#12289;&#29087;&#32451;&#21644;&#31227;&#21160;&#25805;&#20316;&#30340;17&#31181;&#19981;&#21516;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31995;&#32479;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;PVR&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#26159;&#26222;&#36941;&#20248;&#36234;&#30340;&#12290;&#20026;&#20102;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#26469;&#33258;7&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#36229;&#36807;4000&#23567;&#26102;&#30340;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65288;&#36229;&#36807;560&#19975;&#24352;&#22270;&#20687;&#65289;&#21644;ImageNet&#65292;&#20351;&#29992;&#20999;&#29255;&#25968;&#25454;&#30340;&#36974;&#30422;&#33258;&#32534;&#30721;&#65288;MAE&#65289;&#26469;&#35757;&#32451;&#19981;&#21516;&#22823;&#23567;&#30340;&#35270;&#35273;&#21464;&#24418;&#22120;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#25512;&#26029;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#25193;&#23637;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#24182;&#19981;&#33021;&#26222;&#36941;&#25913;&#21892;&#24615;&#33021;&#65288;&#20294;&#24179;&#22343;&#24615;&#33021;&#26377;&#25152;&#25552;&#39640;&#65289;&#12290;&#25105;&#20204;&#26368;&#22823;&#30340;&#27169;&#22411;&#21517;&#20026;VC-1&#65292;&#24179;&#22343;&#34920;&#29616;&#36229;&#36807;&#25152;&#26377;&#20808;&#21069;&#30340;PVR&#65292;&#20294;&#20063;&#27809;&#26377;&#26222;&#36941;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;VC-1&#30340;&#29305;&#23450;&#20110;&#20219;&#21153;&#25110;&#39046;&#22495;&#30340;&#36866;&#24212;&#20250;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant.  To study the effect of pre-training data scale and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average).  Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantia
&lt;/p&gt;</description></item><item><title>VIDIMU&#25968;&#25454;&#38598;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#35760;&#24405;13&#31181;&#20020;&#24202;&#30456;&#20851;&#24615;&#30340;&#27963;&#21160;&#65292;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.16150</link><description>&lt;p&gt;
VIDIMU: &#20351;&#29992;&#20215;&#26684;&#23454;&#24800;&#30340;&#35774;&#22791;&#35760;&#24405;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#21644;IMU&#36816;&#21160;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices. (arXiv:2303.16150v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16150
&lt;/p&gt;
&lt;p&gt;
VIDIMU&#25968;&#25454;&#38598;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#35760;&#24405;13&#31181;&#20020;&#24202;&#30456;&#20851;&#24615;&#30340;&#27963;&#21160;&#65292;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#21644;&#20020;&#24202;&#29983;&#29289;&#21147;&#23398;&#26159;&#29289;&#29702;&#36828;&#31243;&#24247;&#22797;&#21307;&#23398;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#20307;&#21160;&#20316;&#25968;&#25454;&#38598;&#19981;&#33021;&#29992;&#20110;&#30740;&#31350;&#23454;&#39564;&#23460;&#22806;&#36816;&#21160;&#33719;&#21462;&#24773;&#20917;&#19979;&#30340;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;VIDIMU&#25968;&#25454;&#38598;&#30340;&#30446;&#30340;&#26159;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#20116;&#20010;&#24815;&#24615;&#20256;&#24863;&#22120;&#27880;&#20876;&#30340;13&#31181;&#27963;&#21160;&#12290;&#35760;&#24405;&#35270;&#39057;&#30340;54&#20010;&#21463;&#35797;&#32773;&#20013;&#65292;&#20854;&#20013;16&#20010;&#21463;&#35797;&#32773;&#21516;&#26102;&#36824;&#26377;&#24815;&#24615;&#20256;&#24863;&#22120;&#35760;&#24405;&#12290;VIDIMU&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65306;i&#65289;&#25152;&#36873;&#25321;&#30340;&#21160;&#20316;&#30340;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;ii&#65289;&#20351;&#29992;&#20215;&#26684;&#23454;&#24800;&#30340;&#35270;&#39057;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#30340;&#32452;&#21512;&#65292;&#20197;&#21450; iii&#65289;&#23454;&#29616;&#20102;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#24037;&#20855;&#65292;&#21487;&#20197;&#20174;&#24815;&#24615;&#25968;&#25454;&#20013;&#23545;&#19977;&#32500;&#36523;&#20307;&#23039;&#21183;&#36319;&#36394;&#21644;&#36816;&#21160;&#37325;&#24314;&#22312;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition and clinical biomechanics are challenging problems in physical telerehabilitation medicine. However, most publicly available datasets on human body movements cannot be used to study both problems in an out-of-the-lab movement acquisition setting. The objective of the VIDIMU dataset is to pave the way towards affordable patient tracking solutions for remote daily life activities recognition and kinematic analysis. The dataset includes 13 activities registered using a commodity camera and five inertial sensors. The video recordings were acquired in 54 subjects, of which 16 also had simultaneous recordings of inertial sensors. The novelty of VIDIMU lies in: i) the clinical relevance of the chosen movements, ii) the combined utilization of affordable video and custom sensors, and iii) the implementation of state-of-the-art tools for multimodal data processing of 3D body pose tracking and motion reconstruction in a musculoskeletal model from inertial data. The val
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#22810;&#35270;&#35282;&#26041;&#27861;&#35782;&#21035;&#21270;&#30707;&#22270;&#20687;&#12290;&#36890;&#36807;&#25910;&#38598;&#21270;&#30707;&#22270;&#20687;&#30340;&#19981;&#21516;&#35270;&#35282;&#65292;&#20351;&#29992;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#36719;&#25237;&#31080;&#36827;&#34892;&#26368;&#32456;&#20915;&#31574;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;&#19977;&#20010;&#21407;&#22987;&#35270;&#22270;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2302.08062</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#22810;&#35270;&#35282;&#26041;&#27861;&#35782;&#21035;&#21270;&#30707;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews. (arXiv:2302.08062v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#22810;&#35270;&#35282;&#26041;&#27861;&#35782;&#21035;&#21270;&#30707;&#22270;&#20687;&#12290;&#36890;&#36807;&#25910;&#38598;&#21270;&#30707;&#22270;&#20687;&#30340;&#19981;&#21516;&#35270;&#35282;&#65292;&#20351;&#29992;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#36719;&#25237;&#31080;&#36827;&#34892;&#26368;&#32456;&#20915;&#31574;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;&#19977;&#20010;&#21407;&#22987;&#35270;&#22270;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#30707;&#29289;&#31181;&#30340;&#35782;&#21035;&#23545;&#20110;&#36827;&#21270;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#21270;&#30707;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#26377;&#21069;&#26223;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21270;&#30707;&#20445;&#23384;&#12289;&#26377;&#26465;&#20214;&#37319;&#26679;&#20197;&#21450;&#39046;&#22495;&#19987;&#23478;&#26114;&#36149;&#19988;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#27880;&#37322;&#30340;&#38480;&#21046;&#65292;&#26631;&#35760;&#21270;&#30707;&#22270;&#20687;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#36890;&#24120;&#37117;&#21463;&#21040;&#38480;&#21046;&#65292;&#20174;&#32780;&#32473;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36981;&#24490;&#8220;&#32676;&#20247;&#30340;&#26234;&#24935;&#8221;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35270;&#35282;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25910;&#38598;&#27599;&#20010;&#21270;&#30707;&#22270;&#20687;&#30340;&#21407;&#22987;&#35270;&#22270;&#12289;&#28784;&#24230;&#35270;&#22270;&#21644;&#39592;&#26550;&#35270;&#22270;&#26469;&#35757;&#32451;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36719;&#25237;&#31080;&#36827;&#34892;&#26368;&#32456;&#20915;&#31574;&#12290;&#22312;&#21253;&#21547;2400&#24352;&#22270;&#20687;&#30340;&#26368;&#22823;&#36896;&#38024;&#34746;&#21270;&#30707;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;OGS&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65288;&#38024;&#23545;&#27599;&#20010;&#35270;&#22270;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#65289;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#21487;&#19982;OOO&#26041;&#27861;&#30456;&#23218;&#32654;&#65288;&#20351;&#29992;&#19977;&#20010;&#21407;&#22987;&#35270;&#22270;&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification of fossil species is crucial to evolutionary studies. Recent advances from deep learning have shown promising prospects in fossil image identification. However, the quantity and quality of labeled fossil images are often limited due to fossil preservation, conditioned sampling, and expensive and inconsistent label annotation by domain experts, which pose great challenges to training deep learning based image classification models. To address these challenges, we follow the idea of the wisdom of crowds and propose a multiview ensemble framework, which collects Original (O), Gray (G), and Skeleton (S) views of each fossil image reflecting its different characteristics to train multiple base models, and then makes the final decision via soft voting. Experiments on the largest fusulinid dataset with 2400 images show that the proposed OGS consistently outperforms baselines (using a single model for each view), and obtains superior or comparable performance compared to OOO (us
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;&#30340;PUC&#27010;&#24565;&#65292;&#20026;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2210.13954</link><description>&lt;p&gt;
&#25105;&#19981;&#24819;&#35828;&#65306;&#22312;&#21487;&#36873;&#20010;&#20154;&#25968;&#25454;&#27169;&#22411;&#20013;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;
&lt;/p&gt;
&lt;p&gt;
I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data. (arXiv:2210.13954v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20445;&#25252;&#29992;&#25143;&#21516;&#24847;&#30340;PUC&#27010;&#24565;&#65292;&#20026;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#20010;&#20154;&#21487;&#20197;&#36873;&#25321;&#19982;&#20915;&#31574;&#31995;&#32479;&#20849;&#20139;&#21487;&#36873;&#20010;&#20154;&#20449;&#24687;&#65292;&#36825;&#22312;&#29616;&#20195;&#20445;&#38505;&#23450;&#20215;&#27169;&#22411;&#20013;&#24456;&#24120;&#35265;&#12290;&#19968;&#20123;&#29992;&#25143;&#21516;&#24847;&#20351;&#29992;&#20182;&#20204;&#30340;&#25968;&#25454;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#21453;&#23545;&#24182;&#20445;&#25345;&#20854;&#25968;&#25454;&#26410;&#20844;&#24320;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#20915;&#23450;&#26412;&#36523;&#21487;&#20197;&#34987;&#35270;&#20026;&#20449;&#24687;&#65292;&#24212;&#35813;&#21463;&#21040;&#20445;&#25252;&#65292;&#20197;&#23562;&#37325;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#24341;&#21457;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#30830;&#20445;&#20445;&#25252;&#20854;&#20010;&#20154;&#25968;&#25454;&#30340;&#29992;&#25143;&#19981;&#20250;&#22240;&#27492;&#21463;&#21040;&#20219;&#20309;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#20165;&#20351;&#29992;&#33719;&#24471;&#31215;&#26497;&#29992;&#25143;&#21516;&#24847;&#30340;&#20449;&#24687;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20445;&#25252;&#35201;&#27714;&#30340;&#27491;&#24335;&#21270;&#12290;&#36825;&#25490;&#38500;&#20102;&#20316;&#20986;&#20849;&#20139;&#25968;&#25454;&#19982;&#21542;&#20915;&#23450;&#25152;&#21253;&#21547;&#30340;&#38544;&#21547;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Protected User Consent (PUC)&#27010;&#24565;&#65292;&#36825;&#26159;&#25105;&#20204;&#35777;&#26126;&#22312;&#20445;&#25252;&#35201;&#27714;&#19979;&#25439;&#22833;&#26368;&#23567;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine machine learning models in a setup where individuals have the choice to share optional personal information with a decision-making system, as seen in modern insurance pricing models. Some users consent to their data being used whereas others object and keep their data undisclosed. In this work, we show that the decision not to share data can be considered as information in itself that should be protected to respect users' privacy. This observation raises the overlooked problem of how to ensure that users who protect their personal data do not suffer any disadvantages as a result. To address this problem, we formalize protection requirements for models which only use the information for which active user consent was obtained. This excludes implicit information contained in the decision to share data or not. We offer the first solution to this problem by proposing the notion of Protected User Consent (PUC), which we prove to be loss-optimal under our protection requirement. To
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35299;&#37322;&#21487;&#20197;&#24433;&#21709;&#20844;&#27491;&#24615;&#24863;&#30693;&#21644;&#20154;&#20204;&#23545;AI&#24314;&#35758;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24182;&#19981;&#33021;&#24110;&#21161;&#20154;&#20204;&#21306;&#20998;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;AI&#24314;&#35758;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#20419;&#36827;&#26377;&#25928;&#30340;&#20915;&#31574;&#21644;&#20844;&#27491;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2209.11812</link><description>&lt;p&gt;
&#35299;&#37322;&#12289;&#20844;&#27491;&#24615;&#21644;&#20154;&#31867;-AI&#20915;&#31574;&#20013;&#30340;&#36866;&#24403;&#20381;&#36182;&#65288;arXiv:2209.11812v3 [cs.HC] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making. (arXiv:2209.11812v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35299;&#37322;&#21487;&#20197;&#24433;&#21709;&#20844;&#27491;&#24615;&#24863;&#30693;&#21644;&#20154;&#20204;&#23545;AI&#24314;&#35758;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24182;&#19981;&#33021;&#24110;&#21161;&#20154;&#20204;&#21306;&#20998;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;AI&#24314;&#35758;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#20419;&#36827;&#26377;&#25928;&#30340;&#20915;&#31574;&#21644;&#20844;&#27491;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#20174;&#31616;&#30701;&#30340;&#25991;&#26412;&#31616;&#20171;&#20013;&#39044;&#27979;&#32844;&#19994;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36825;&#20123;&#24433;&#21709;&#22914;&#20309;&#36890;&#36807;&#20154;&#20204;&#30340;&#20844;&#27491;&#24615;&#24863;&#30693;&#21644;&#23545;AI&#24314;&#35758;&#30340;&#20381;&#36182;&#26469;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#24433;&#21709;&#20102;&#20844;&#27491;&#24615;&#24863;&#30693;&#65292;&#32780;&#20844;&#27491;&#24615;&#24863;&#30693;&#21448;&#19982;&#20154;&#20204;&#36981;&#24490;AI&#24314;&#35758;&#30340;&#20542;&#21521;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#26679;&#30340;&#35299;&#37322;&#24182;&#19981;&#33021;&#35753;&#20154;&#20204;&#21306;&#20998;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;AI&#24314;&#35758;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35299;&#37322;&#21487;&#33021;&#20250;&#24433;&#21709;&#20381;&#36182;&#65292;&#32780;&#19981;&#35770;AI&#24314;&#35758;&#30340;&#27491;&#30830;&#24615;&#22914;&#20309;&#12290;&#21462;&#20915;&#20110;&#35299;&#37322;&#31361;&#20986;&#30340;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#25110;&#38459;&#30861;&#20998;&#37197;&#20844;&#27491;&#24615;&#65306;&#24403;&#35299;&#37322;&#31361;&#20986;&#19982;&#25935;&#24863;&#23646;&#24615;&#26126;&#26174;&#30456;&#20851;&#30340;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#29305;&#24449;&#26102;&#65292;&#36825;&#20250;&#20419;&#20351;&#20154;&#20204;&#35206;&#30422;AI&#19982;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the effects of feature-based explanations on distributive fairness of AI-assisted decisions, specifically focusing on the task of predicting occupations from short textual bios. We also investigate how any effects are mediated by humans' fairness perceptions and their reliance on AI recommendations. Our findings show that explanations influence fairness perceptions, which, in turn, relate to humans' tendency to adhere to AI recommendations. However, we see that such explanations do not enable humans to discern correct and incorrect AI recommendations. Instead, we show that they may affect reliance irrespective of the correctness of AI recommendations. Depending on which features an explanation highlights, this can foster or hinder distributive fairness: when explanations highlight features that are task-irrelevant and evidently associated with the sensitive attribute, this prompts overrides that counter AI recommendations that align with gender stereotypes. Meanw
&lt;/p&gt;</description></item></channel></rss>