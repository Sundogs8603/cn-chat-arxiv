<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31243;&#24207;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;3D&#25193;&#25955;&#20248;&#20808;&#32423;&#21035;&#21152;&#19978;&#26032;&#39062;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#38450;&#27490;NeRF&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#22270;&#24418;&#20266;&#24433;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.10532</link><description>&lt;p&gt;
Nerfbusters&#65306;&#20174;&#38543;&#24847;&#25429;&#33719;&#30340;NeRF&#20013;&#21435;&#38500;&#24189;&#28789;&#20284;&#30340;&#22270;&#20687;&#20266;&#24433;
&lt;/p&gt;
&lt;p&gt;
Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs. (arXiv:2304.10532v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10532
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31243;&#24207;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;3D&#25193;&#25955;&#20248;&#20808;&#32423;&#21035;&#21152;&#19978;&#26032;&#39062;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#38450;&#27490;NeRF&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#22270;&#24418;&#20266;&#24433;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#24847;&#25429;&#33719;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#22312;&#28210;&#26579;&#25668;&#20687;&#26426;&#36712;&#36857;&#20043;&#22806;&#26102;&#20250;&#20986;&#29616;&#28014;&#28857;&#38169;&#35823;&#25110;&#26377;&#32570;&#38519;&#30340;&#20960;&#20309;&#22270;&#24418;&#31561;&#20266;&#24433;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#21327;&#35758;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#36825;&#20123;&#25928;&#24212;&#65292;&#22240;&#20026;&#36890;&#24120;&#20165;&#22312;&#35757;&#32451;&#25235;&#21462;&#30340;&#27599;&#20010;&#31532;&#20843;&#24103;&#35780;&#20272;&#22270;&#20687;&#36136;&#37327;&#12290;&#20026;&#20102;&#25512;&#21160;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31243;&#24207;&#65292;&#20854;&#20013;&#35760;&#24405;&#20102;&#22330;&#26223;&#30340;&#20004;&#20010;&#25668;&#20687;&#26426;&#36712;&#36857;&#65306;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#35268;&#21017;&#19981;&#20165;&#19981;&#33021;&#21435;&#38500;&#28014;&#28857;&#38169;&#35823;&#65292;&#32780;&#19988;&#20063;&#19981;&#33021;&#25913;&#21892;&#22330;&#26223;&#20960;&#20309;&#24418;&#29366;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26412;&#22320;3D&#20808;&#39564;&#21644;&#26032;&#39062;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#25439;&#22833;&#65292;&#22312;NeRF&#20248;&#21270;&#36807;&#31243;&#20013;&#38450;&#27490;&#20986;&#29616;&#20266;&#20687;&#29616;&#35937;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#20808;&#32423;&#21035;&#33021;&#22815;&#21435;&#38500;&#28014;&#28857;&#38169;&#35823;&#24182;&#25913;&#21892;&#38543;&#24847;&#25429;&#33719;&#30340;&#22330;&#26223;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such as floaters or flawed geometry when rendered outside the camera trajectory. Existing evaluation protocols often do not capture these effects, since they usually only assess image quality at every 8th frame of the training capture. To push forward progress in novel-view synthesis, we propose a new dataset and evaluation procedure, where two camera trajectories are recorded of the scene: one used for training, and the other for evaluation. In this more challenging in-the-wild setting, we find that existing hand-crafted regularizers do not remove floaters nor improve scene geometry. Thus, we propose a 3D diffusion-based method that leverages local 3D priors and a novel density-based score distillation sampling loss to discourage artifacts during NeRF optimization. We show that this data-driven prior removes floaters and improves scene geometry for casual captures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;(MAE-CT)&#65292;&#21033;&#29992;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#22312;&#26631;&#35760;&#25968;&#25454;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19979;&#28216;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#22270;&#20687;&#30340;&#20016;&#23500;&#29305;&#24449;&#32858;&#31867;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.10520</link><description>&lt;p&gt;
&#23545;&#27604;&#35843;&#33410;: &#24110;&#21161;&#36951;&#24536;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#19968;&#28857;&#23567;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;(MAE-CT)&#65292;&#21033;&#29992;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#22312;&#26631;&#35760;&#25968;&#25454;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19979;&#28216;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#22270;&#20687;&#30340;&#20016;&#23500;&#29305;&#24449;&#32858;&#31867;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#22914;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36755;&#20837;&#30340;&#20016;&#23500;&#34920;&#31034;&#12290;&#20294;&#26159;&#65292;&#20026;&#20102;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#20016;&#23500;&#30340;&#29305;&#24449;&#19981;&#20165;&#25429;&#33719;&#20102;&#23545;&#35937;&#32780;&#19988;&#36824;&#21253;&#25324;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#32972;&#26223;&#65292;&#22240;&#27492;&#23427;&#20204;&#38656;&#35201;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23454;&#20363;&#36776;&#21035;&#26041;&#27861;&#20391;&#37325;&#20110;&#23545;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23558;MIM&#30340;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#19982;ID&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#32570;&#23569;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#19979;&#28216;&#20998;&#31867;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;&#65288;MAE-CT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39034;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#24212;&#29992;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;MAE&#12290;MAE-CT&#35843;&#25972;&#20102;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#20351;&#23427;&#20204;&#24418;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#26631;&#31614;&#12290;&#24212;&#29992;&#20110;&#22823;&#22411;&#21644;&#24040;&#22411;Vision Transformer&#65288;ViT&#65289;&#27169;&#22411;&#26102;&#65292;MAE-CT&#22312;&#32447;&#24615;&#25506;&#27979;&#65292;k-&#22343;&#20540;&#32858;&#31867;&#21644;&#21322;&#30417;&#30563;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#21305;&#37197;&#25110;&#36229;&#36234;&#20102;&#22312;ImageNet&#19978;&#35757;&#32451;&#30340;&#20808;&#21069;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE), efficiently learn a rich representation of the input. However, for adapting to downstream tasks, they require a sufficient amount of labeled data since their rich features capture not only objects but also less relevant image background. In contrast, Instance Discrimination (ID) methods focus on objects. In this work, we study how to combine the efficiency and scalability of MIM with the ability of ID to perform downstream classification in the absence of large amounts of labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning (MAE-CT), a sequential approach that applies Nearest Neighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes the rich features such that they form semantic clusters of objects without using any labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CT matches or excels previous self-supervised methods trained on ImageNet in linear probing, k
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.10517</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#65306;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#27880;&#37322;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#33719;&#21462;&#25104;&#26412;&#65292;&#35757;&#32451;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;Segment Anything Model&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#22522;&#30784;&#27169;&#22411;&#65292;&#32463;&#36807;&#36229;&#36807;10&#20159;&#20010;&#27880;&#37322;&#30340;&#35757;&#32451;&#65292;&#20027;&#35201;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;&#65292;&#26088;&#22312;&#33021;&#22815;&#20197;&#20132;&#20114;&#26041;&#24335;&#20998;&#21106;&#29992;&#25143;&#23450;&#20041;&#30340;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;SAM&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#19981;&#28165;&#26970;&#35813;&#27169;&#22411;&#22312;&#36716;&#25442;&#21040;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#26102;&#20250;&#21463;&#21040;&#22810;&#22823;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;SAM&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#30340;11&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#29983;&#25104;&#28857;&#25552;&#31034;&#26469;&#27169;&#25311;&#20132;&#20114;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAM&#22522;&#20110;&#21333;&#28857;&#25552;&#31034;&#30340;&#34920;&#29616;&#22312;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26041;&#38754;&#39640;&#24230;&#21464;&#21270;&#65292;&#21363;&#20174;&#33034;&#26609;MRI&#25968;&#25454;&#38598;&#30340;0.1135&#21040;&#39627;&#20851;&#33410;X&#23556;&#32447;&#25968;&#25454;&#38598;&#30340;0.8650&#12290;
&lt;/p&gt;
&lt;p&gt;
Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20307;&#31215;&#25968;&#25454;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#25216;&#26415;&#65292;&#32467;&#21512;&#21040;&#21453;&#24212;&#24335;&#32534;&#31243;&#31995;&#32479;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#21407;&#20301;&#26102;&#38388;&#32531;&#23384;&#31995;&#32479;&#65292;&#24182;&#22312;Ascent&#22522;&#30784;&#26550;&#26500;&#20013;&#20351;&#29992;&#23454;&#38469;&#27169;&#25311;&#35780;&#20272;&#20102;&#20854;100&#20493;&#23481;&#37327;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10516</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#31070;&#32463;&#34920;&#31034;&#25216;&#26415;&#29992;&#20110;&#21453;&#24212;&#24335;&#21407;&#20301;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distributed Neural Representation for Reactive in situ Visualization. (arXiv:2304.10516v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20307;&#31215;&#25968;&#25454;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#25216;&#26415;&#65292;&#32467;&#21512;&#21040;&#21453;&#24212;&#24335;&#32534;&#31243;&#31995;&#32479;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#21407;&#20301;&#26102;&#38388;&#32531;&#23384;&#31995;&#32479;&#65292;&#24182;&#22312;Ascent&#22522;&#30784;&#26550;&#26500;&#20013;&#20351;&#29992;&#23454;&#38469;&#27169;&#25311;&#35780;&#20272;&#20102;&#20854;100&#20493;&#23481;&#37327;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21453;&#24212;&#24335;&#32534;&#31243;&#23454;&#29616;&#35745;&#31639;&#27169;&#22411;&#30340;&#21407;&#20301;&#21487;&#35270;&#21270;&#21644;&#25511;&#21046;&#21313;&#20998;&#39640;&#25928;&#65292;&#23427;&#21033;&#29992;&#26102;&#38388;&#25277;&#35937;&#21644;&#25968;&#25454;&#32531;&#23384;&#26426;&#21046;&#26469;&#21019;&#24314;&#21160;&#24577;&#24037;&#20316;&#27969;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#27169;&#25311;&#65292;&#23454;&#29616;&#26102;&#38388;&#32531;&#23384;&#21487;&#33021;&#23384;&#22312;&#25361;&#25112;&#12290;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#22312;&#21387;&#32553;&#22823;&#22411;&#25968;&#25454;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#36824;&#27809;&#26377;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20307;&#31215;&#25968;&#25454;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#32467;&#21512;&#21040;DIVA&#21453;&#24212;&#24335;&#32534;&#31243;&#31995;&#32479;&#20013;&#12290;&#36825;&#31181;&#23454;&#29616;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#21407;&#20301;&#26102;&#38388;&#32531;&#23384;&#31995;&#32479;&#65292;&#20854;&#23481;&#37327;&#27604;&#20197;&#21069;&#30340;&#23481;&#37327;&#22823;100&#20493;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#38598;&#25104;&#21040;Ascent&#22522;&#30784;&#26550;&#26500;&#20013;&#65292;&#24182;&#20351;&#29992;&#23454;&#38469;&#27169;&#25311;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In situ visualization and steering of computational modeling can be effectively achieved using reactive programming, which leverages temporal abstraction and data caching mechanisms to create dynamic workflows. However, implementing a temporal cache for large-scale simulations can be challenging. Implicit neural networks have proven effective in compressing large volume data. However, their application to distributed data has yet to be fully explored. In this work, we develop an implicit neural representation for distributed volume data and incorporate it into the DIVA reactive programming system. This implementation enables us to build an in situ temporal caching system with a capacity 100 times larger than previously achieved. We integrate our implementation into the Ascent infrastructure and evaluate its performance using real-world simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20154;&#33041;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#30340;&#31867;&#33041;&#35774;&#35745;&#21407;&#21017;&#20197;&#25351;&#23548;CNN&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#32593;&#32476;&#24067;&#32447;&#27169;&#24335;&#21644;&#21367;&#31215;&#25805;&#20316;&#20013;&#23454;&#29616;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;CP-CNN&#26550;&#26500;&#65292;&#23427;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.10515</link><description>&lt;p&gt;
CP-CNN: &#22522;&#20110;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#25351;&#23548;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CP-CNN: Core-Periphery Principle Guided Convolutional Neural Network. (arXiv:2304.10515v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20154;&#33041;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#30340;&#31867;&#33041;&#35774;&#35745;&#21407;&#21017;&#20197;&#25351;&#23548;CNN&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#32593;&#32476;&#24067;&#32447;&#27169;&#24335;&#21644;&#21367;&#31215;&#25805;&#20316;&#20013;&#23454;&#29616;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;CP-CNN&#26550;&#26500;&#65292;&#23427;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#30340;&#21457;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#20854;&#26550;&#26500;&#35774;&#35745;&#65292;&#21363;&#32593;&#32476;&#24067;&#32447;&#27169;&#24335;&#12290;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;(NAS)&#36890;&#36807;&#33258;&#21160;&#21270;&#25628;&#32034;&#26368;&#20248;&#32593;&#32476;&#26550;&#26500;&#25512;&#36827;&#20102;&#36825;&#19968;&#36827;&#23637;&#65292;&#20294;&#25152;&#24471;&#21040;&#30340;&#32593;&#32476;&#23454;&#20363;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#21487;&#33021;&#26080;&#27861;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25506;&#32034;&#36328;&#20219;&#21153;&#36890;&#29992;&#30340;&#32593;&#32476;&#35774;&#35745;&#21407;&#21017;&#26159;&#26356;&#23454;&#38469;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#20154;&#33041;&#32593;&#32476;&#30340;&#26680;&#24515;-&#21608;&#36793;&#23646;&#24615;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31867;&#33041;&#35774;&#35745;&#21407;&#21017;&#65292;&#20197;&#25351;&#23548;CNN&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20174;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#36825;&#20123;&#30740;&#31350;&#34920;&#26126;&#20154;&#24037;&#21644;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#22312;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;&#26041;&#38754;&#21487;&#33021;&#20855;&#26377;&#20849;&#21516;&#30340;&#21407;&#21017;&#12290;&#25105;&#20204;&#22312;&#32593;&#32476;&#24067;&#32447;&#27169;&#24335;&#30340;&#35774;&#35745;&#21644;&#21367;&#31215;&#25805;&#20316;&#30340;&#31232;&#30095;&#21270;&#20013;&#23454;&#29616;&#20102;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#12290;&#32467;&#26524;&#65292;&#20351;&#29992;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#25351;&#23548;&#30340;CNN (CP-CNN)&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of convolutional neural networks (CNNs) can be largely attributed to the design of its architecture, i.e., the network wiring pattern. Neural architecture search (NAS) advances this by automating the search for the optimal network architecture, but the resulting network instance may not generalize well in different tasks. To overcome this, exploring network design principles that are generalizable across tasks is a more practical solution. In this study, We explore a novel brain-inspired design principle based on the core-periphery property of the human brain network to guide the design of CNNs. Our work draws inspiration from recent studies suggesting that artificial and biological neural networks may have common principles in optimizing network architecture. We implement the core-periphery principle in the design of network wiring patterns and the sparsification of the convolution operation. The resulting core-periphery principle guided CNNs (CP-CNNs) are evaluated on t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22833;&#35823;&#65292;&#24402;&#32435;&#24182;&#30830;&#23450;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#31867;&#22411;&#21644;&#20851;&#38190;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10513</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;ChatGPT&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Does ChatGPT Fall Short in Answering Questions Faithfully?. (arXiv:2304.10513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22833;&#35823;&#65292;&#24402;&#32435;&#24182;&#30830;&#23450;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#31867;&#22411;&#21644;&#20851;&#38190;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20986;&#23545;&#20154;&#31867;&#29983;&#27963;&#21508;&#26041;&#38754;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;ChatGPT&#22312;&#35802;&#23454;&#24615;&#31561;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#38382;&#31572;&#31995;&#32479;&#20026;&#20195;&#34920;&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#20026;&#20160;&#20040;ChatGPT&#22312;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#26377;&#25152;&#19981;&#36275;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35797;&#22270;&#20998;&#26512;ChatGPT&#22312;&#22797;&#26434;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#22833;&#36133;&#30340;&#21407;&#22240;&#65292;&#24182;&#30830;&#23450;&#19982;&#36825;&#20123;&#22833;&#36133;&#26377;&#20851;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;ChatGPT&#30340;&#22833;&#36133;&#24402;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#29702;&#35299;&#12289;&#20107;&#23454;&#24615;&#12289;&#20855;&#20307;&#24615;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#19982;QA&#22833;&#36133;&#26377;&#20851;&#30340;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#30693;&#35782;&#35760;&#24518;&#12289;&#30693;&#35782;&#20851;&#32852;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22260;&#32469;&#36825;&#20123;&#33021;&#21147;&#30340;&#23454;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21521;&#27169;&#22411;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#22806;&#37096;&#30693;&#35782;&#12289;&#32473;&#20104;&#25552;&#31034;&#26469;&#24110;&#21161;&#23427;&#32858;&#28966;&#24182;&#21152;&#24378;&#20851;&#38190;&#33021;&#21147;&#65292;&#36825;&#37117;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models, such as ChatGPT, have demonstrated significant potential to impact various aspects of human life. However, ChatGPT still faces challenges in aspects like faithfulness. Taking question answering as a representative application, we seek to understand why ChatGPT falls short in answering questions faithfully. To address this question, we attempt to analyze the failures of ChatGPT in complex open-domain question answering and identifies the abilities under the failures. Specifically, we categorize ChatGPT's failures into four types: comprehension, factualness, specificity, and inference. We further pinpoint three critical abilities associated with QA failures: knowledge memorization, knowledge association, and knowledge reasoning. Additionally, we conduct experiments centered on these abilities and propose potential approaches to enhance faithfulness. The results indicate that furnishing the model with fine-grained external knowledge, hints for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OutCenTR&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#34987;&#21033;&#29992;&#30340;&#28431;&#27934;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#38477;&#32500;&#25216;&#26415;OutCenTR&#65292;&#21487;&#20197;&#22686;&#24378;&#22522;&#26412;&#30340;&#31163;&#32676;&#28857;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;OutCenTR&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.10511</link><description>&lt;p&gt;
OutCenTR: &#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#39044;&#27979;&#28431;&#27934;&#21033;&#29992;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OutCenTR: A novel semi-supervised framework for predicting exploits of vulnerabilities in high-dimensional datasets. (arXiv:2304.10511v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OutCenTR&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#34987;&#21033;&#29992;&#30340;&#28431;&#27934;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#38477;&#32500;&#25216;&#26415;OutCenTR&#65292;&#21487;&#20197;&#22686;&#24378;&#22522;&#26412;&#30340;&#31163;&#32676;&#28857;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;OutCenTR&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#37117;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#28431;&#27934;&#34987;&#25253;&#36947;&#65292;&#28982;&#32780;&#36825;&#20123;&#28431;&#27934;&#24182;&#19981;&#37117;&#26159;&#30456;&#21516;&#30340;&#65292;&#26377;&#20123;&#27604;&#20854;&#20182;&#30340;&#26356;&#20855;&#26377;&#38024;&#23545;&#24615;&#12290;&#27491;&#30830;&#20272;&#35745;&#28431;&#27934;&#34987;&#21033;&#29992;&#30340;&#21487;&#33021;&#24615;&#26159;&#31995;&#32479;&#31649;&#29702;&#21592;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#21033;&#29992;&#31163;&#32676;&#28857;&#26816;&#27979;&#25216;&#26415;&#26469;&#39044;&#27979;&#39640;&#24230;&#19981;&#24179;&#34913;&#21644;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#34987;&#21033;&#29992;&#30340;&#28431;&#27934;&#65292;&#20363;&#22914;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#25216;&#26415;OutCenTR&#65292;&#21487;&#20197;&#22686;&#24378;&#22522;&#26412;&#30340;&#31163;&#32676;&#28857;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;4&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;12&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;OutCenTR&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;PCA&#21644;GRP&#31561;&#26368;&#20808;&#36827;&#30340;&#38477;&#32500;&#25216;&#26415;&#30456;&#27604;&#65292;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ever-growing number of vulnerabilities are reported every day. Yet these vulnerabilities are not all the same; Some are more targeted than others. Correctly estimating the likelihood of a vulnerability being exploited is a critical task for system administrators. This aids the system administrators in prioritizing and patching the right vulnerabilities. Our work makes use of outlier detection techniques to predict vulnerabilities that are likely to be exploited in highly imbalanced and high-dimensional datasets such as the National Vulnerability Database. We propose a dimensionality reduction technique, OutCenTR, that enhances the baseline outlier detection models. We further demonstrate the effectiveness and efficiency of OutCenTR empirically with 4 benchmark and 12 synthetic datasets. The results of our experiments show on average a 5-fold improvement of F1 score in comparison with state-of-the-art dimensionality reduction techniques such as PCA and GRP.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#25439;&#22833;&#30340;GAN&#28508;&#31354;&#38388;&#35821;&#20041;&#32534;&#36753;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#20998;&#31867;&#22120;&#30340;&#20381;&#36182;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10508</link><description>&lt;p&gt;
GAN&#28508;&#31354;&#38388;&#35821;&#20041;&#32534;&#36753;&#20013;&#30340;Wasserstein Loss
&lt;/p&gt;
&lt;p&gt;
Wasserstein Loss for Semantic Editing in the Latent Space of GANs. (arXiv:2304.10508v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#25439;&#22833;&#30340;GAN&#28508;&#31354;&#38388;&#35821;&#20041;&#32534;&#36753;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#20998;&#31867;&#22120;&#30340;&#20381;&#36182;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GAN&#30340;&#28508;&#22312;&#31354;&#38388;&#21253;&#21547;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#12290;&#19981;&#21516;&#30340;&#26041;&#27861;&#25552;&#20986;&#22312;&#28508;&#31354;&#38388;&#20013;&#23398;&#20064;&#19982;&#35821;&#20041;&#23646;&#24615;&#30456;&#23545;&#24212;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;&#20174;&#32780;&#21487;&#20197;&#20462;&#25913;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#30417;&#30563;&#26041;&#27861;&#20381;&#36182;&#20110;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#26469;&#20135;&#29983;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#20998;&#31867;&#22120;&#21487;&#33021;&#20250;&#23548;&#33268;&#36229;&#20986;&#20998;&#24067;&#21306;&#22495;&#65292;&#21516;&#26102;&#34987;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#25439;&#22833;&#30340;&#26367;&#20195;&#20844;&#24335;&#65292;&#36991;&#20813;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#19982;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#25105;&#20204;&#20351;&#29992;StyleGAN2&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;&#25968;&#23383;&#21644;&#20154;&#33080;&#65289;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latent space of GANs contains rich semantics reflecting the training data. Different methods propose to learn edits in latent space corresponding to semantic attributes, thus allowing to modify generated images. Most supervised methods rely on the guidance of classifiers to produce such edits. However, classifiers can lead to out-of-distribution regions and be fooled by adversarial samples. We propose an alternative formulation based on the Wasserstein loss that avoids such problems, while maintaining performance on-par with classifier-based approaches. We demonstrate the effectiveness of our method on two datasets (digits and faces) using StyleGAN2.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#39057;&#39044;&#35757;&#32451;Transformer&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#23558;&#35270;&#39057;&#36716;&#25442;&#25104;&#36890;&#29992;&#34920;&#31034;&#65292;&#21487;&#20197;&#32467;&#21512;&#22810;&#31181;&#27169;&#24335;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10505</link><description>&lt;p&gt;
&#35270;&#39057;&#39044;&#35757;&#32451;Transformer: &#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#19987;&#23478;&#28151;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts. (arXiv:2304.10505v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#39057;&#39044;&#35757;&#32451;Transformer&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#23558;&#35270;&#39057;&#36716;&#25442;&#25104;&#36890;&#29992;&#34920;&#31034;&#65292;&#21487;&#20197;&#32467;&#21512;&#22810;&#31181;&#27169;&#24335;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35270;&#39057;&#39044;&#35757;&#32451;Transformer (VPT)&#12290;VPT&#20351;&#29992;&#26469;&#33258;&#20043;&#21069;&#24037;&#20316;&#30340;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#23558;&#35270;&#39057;&#36716;&#25442;&#25104;&#32039;&#20945;&#30340;&#23884;&#20837;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#39592;&#24178;&#32593;&#32476;&#22522;&#20110;&#21442;&#32771;Flan-T5-11B&#26550;&#26500;&#65292;&#23398;&#20064;&#35270;&#39057;&#30340;&#36890;&#29992;&#34920;&#31034;&#24418;&#24335;&#65292;&#36825;&#26159;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24635;&#21644;&#12290;&#23427;&#20351;&#29992;&#33258;&#22238;&#24402;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#36890;&#36807;&#39044;&#27979;YouTube&#35270;&#39057;&#20013;&#35762;&#35805;&#30340;&#21333;&#35789;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#27599;&#20010;&#20219;&#21153;&#35757;&#32451;&#20840;&#36830;&#25509;&#39044;&#27979;&#22836;&#65292;&#22312;&#26631;&#20934;&#30340;&#19979;&#28216;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#8220;&#23884;&#20837;-&gt;&#39592;&#24178;&#32593;&#32476;-&gt;&#39044;&#27979;&#22836;&#8221;&#35774;&#35745;&#27169;&#24335;&#20013;&#20351;&#29992;&#22810;&#20010;&#20923;&#32467;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#30740;&#31350;&#65292;&#32780;&#20854;&#20182;&#30740;&#31350;&#37117;&#26159;&#35757;&#32451;&#33258;&#24049;&#30340;&#32852;&#21512;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#26174;&#24335;&#30340;&#22330;&#26223;&#22270;&#20449;&#24687;&#65292;&#21253;&#21547;&#20102;&#27604;&#24403;&#21069;SOTA Merlot Reserve&#26356;&#22810;&#30340;&#27169;&#24577;&#12290;&#20986;&#20110;&#36825;&#20004;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#30456;&#20449;&#23427;&#21487;&#20197;&#23558;&#19990;&#30028;&#19978;&#26368;&#22909;&#30340;&#24320;&#28304;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Video Pre-trained Transformer. VPT uses four SOTA encoder models from prior work to convert a video into a sequence of compact embeddings. Our backbone, based on a reference Flan-T5-11B architecture, learns a universal representation of the video that is a non-linear sum of the encoder models. It learns using an autoregressive causal language modeling loss by predicting the words spoken in YouTube videos. Finally, we evaluate on standard downstream benchmarks by training fully connected prediction heads for each task. To the best of our knowledge, this is the first use of multiple frozen SOTA models as encoders in an "embedding -&gt; backbone -&gt; prediction head" design pattern - all others have trained their own joint encoder models. Additionally, we include more modalities than the current SOTA, Merlot Reserve, by adding explicit Scene Graph information. For these two reasons, we believe it could combine the world's best open-source models to achieve SOTA performance. Initial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KERMIT&#65292;&#19968;&#31181;&#22823;&#25968;&#25454;&#33258;&#20027;&#26550;&#26500;&#31995;&#32479;&#12290;KERMIT&#33021;&#22815;&#33258;&#21160;&#35843;&#20248;Apache Spark&#21644;Hadoop&#65292;&#24182;&#22312;&#20154;&#24037;&#32463;&#39564;&#20248;&#21270;&#19978;&#36798;&#21040;&#24555;30%&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#19982;&#31351;&#20030;&#25628;&#32034;&#30340;&#24615;&#33021;&#20248;&#21270;&#26041;&#27861;&#30456;&#27604;&#24555;92%&#12290;KERMIT&#33021;&#22815;&#39640;&#36798;99%&#30340;&#20934;&#30830;&#29575;&#26816;&#27979;&#37325;&#35201;&#30340;&#24037;&#20316;&#36127;&#36733;&#21464;&#21270;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#24037;&#20316;&#36127;&#36733;&#31867;&#22411;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;96%&#12290;</title><link>http://arxiv.org/abs/2304.10503</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#24615;&#33021;&#20248;&#21270;&#30340;&#33258;&#20027;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Autonomic Architecture for Big Data Performance Optimization. (arXiv:2304.10503v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KERMIT&#65292;&#19968;&#31181;&#22823;&#25968;&#25454;&#33258;&#20027;&#26550;&#26500;&#31995;&#32479;&#12290;KERMIT&#33021;&#22815;&#33258;&#21160;&#35843;&#20248;Apache Spark&#21644;Hadoop&#65292;&#24182;&#22312;&#20154;&#24037;&#32463;&#39564;&#20248;&#21270;&#19978;&#36798;&#21040;&#24555;30%&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#19982;&#31351;&#20030;&#25628;&#32034;&#30340;&#24615;&#33021;&#20248;&#21270;&#26041;&#27861;&#30456;&#27604;&#24555;92%&#12290;KERMIT&#33021;&#22815;&#39640;&#36798;99%&#30340;&#20934;&#30830;&#29575;&#26816;&#27979;&#37325;&#35201;&#30340;&#24037;&#20316;&#36127;&#36733;&#21464;&#21270;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#24037;&#20316;&#36127;&#36733;&#31867;&#22411;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;96%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Apache Spark&#21644;Hadoop&#24320;&#21457;&#30340;&#22823;&#25968;&#25454;&#36719;&#20214;&#26632;&#24050;&#25104;&#20026;&#35768;&#22810;&#20225;&#19994;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;Spark&#21644;Hadoop&#20316;&#19994;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#35768;&#22810;&#37197;&#32622;&#35774;&#32622;&#12290;&#25163;&#21160;&#35843;&#25972;&#26159;&#26114;&#36149;&#19988;&#26131;&#30862;&#30340;&#12290;&#36807;&#21435;&#24050;&#32463;&#21162;&#21147;&#24320;&#21457;&#22312;&#32447;&#21644;&#31163;&#32447;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#65292;&#20351;&#22823;&#25968;&#25454;&#26632;&#19981;&#20877;&#20381;&#36182;&#20110;&#25163;&#21160;&#35843;&#20248;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#33021;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#38750;&#24120;&#31616;&#21333;&#30340;&#21333;&#29992;&#25143;&#24037;&#20316;&#36127;&#36733;&#65292;&#21512;&#29702;&#24615;&#33021;&#20248;&#21270;&#26041;&#38754;&#36129;&#29486;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KERMIT - &#22823;&#25968;&#25454;&#33258;&#20027;&#26550;&#26500;&#31995;&#32479;&#65292;&#33021;&#22815;&#33258;&#21160;&#35843;&#20248;Apache Spark&#21644;Hadoop&#65292;&#24182;&#23454;&#29616;&#27604;&#20154;&#24037;&#31649;&#29702;&#21592;&#30340;&#32463;&#39564;&#20248;&#21270;&#25928;&#26524;&#24555;30%&#65292;&#19982;&#25191;&#34892;&#35843;&#20248;&#21442;&#25968;&#31354;&#38388;&#30340;&#31351;&#20030;&#25628;&#32034;&#30340;&#24615;&#33021;&#20248;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#23454;&#29616;&#39640;&#36798;92%&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;KERMIT&#33021;&#22815;&#39640;&#36798;99%&#30340;&#20934;&#30830;&#29575;&#26816;&#27979;&#37325;&#35201;&#30340;&#24037;&#20316;&#36127;&#36733;&#21464;&#21270;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#24037;&#20316;&#36127;&#36733;&#31867;&#22411;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;96%&#12290;
&lt;/p&gt;
&lt;p&gt;
The big data software stack based on Apache Spark and Hadoop has become mission critical in many enterprises. Performance of Spark and Hadoop jobs depends on a large number of configuration settings. Manual tuning is expensive and brittle. There have been prior efforts to develop on-line and off-line automatic tuning approaches to make the big data stack less dependent on manual tuning. These, however, demonstrated only modest performance improvements with very simple, single-user workloads on small data sets. This paper presents KERMIT - the autonomic architecture for big data capable of automatically tuning Apache Spark and Hadoop on-line, and achieving performance results 30% faster than rule-of-thumb tuning by a human administrator and up to 92% as fast as the fastest possible tuning established by performing an exhaustive search of the tuning parameter space. KERMIT can detect important workload changes with up to 99% accuracy, and predict future workload types with up to 96% accu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20351;&#29992; Transformer &#27169;&#22411;&#36827;&#34892;&#24418;&#24335;&#35821;&#35328;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26368;&#22522;&#26412;&#30340;&#29305;&#24615;&#8212;&#8212;&#26415;&#35821;&#21644;&#31867;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Transformer &#30340;&#31867;&#22411;&#25512;&#29702;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TTI &#27169;&#22411;&#22312;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10500</link><description>&lt;p&gt;
&#20351;&#29992; Transformer &#27169;&#22411;&#36827;&#34892; Simply Typed Lambda Calculus &#30340;&#31867;&#22411;&#25512;&#29702;&#65306;&#28145;&#24230;&#23398;&#20064;&#22312;&#20195;&#30721;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformer Models for Type Inference in the Simply Typed Lambda Calculus: A Case Study in Deep Learning for Code. (arXiv:2304.10500v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20351;&#29992; Transformer &#27169;&#22411;&#36827;&#34892;&#24418;&#24335;&#35821;&#35328;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26368;&#22522;&#26412;&#30340;&#29305;&#24615;&#8212;&#8212;&#26415;&#35821;&#21644;&#31867;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Transformer &#30340;&#31867;&#22411;&#25512;&#29702;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TTI &#27169;&#22411;&#22312;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#20132;&#21449;&#39046;&#22495;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#31995;&#32479;&#24615;&#22320;&#25506;&#32034;&#20351;&#29992; Transformer &#27169;&#22411;&#25512;&#29702;&#26377;&#31867;&#22411; lambda &#28436;&#31639;&#26041;&#38754;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#36825;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26377;&#20004;&#20010;&#21407;&#22240;&#12290;&#39318;&#20808;&#65292;&#26377;&#31867;&#22411;&#30340; lambda &#28436;&#31639;&#26159;&#32534;&#31243;&#35821;&#35328;&#30340;&#22522;&#30784;&#12290;&#23558;&#21508;&#31181;&#26377;&#31867;&#22411; lambda &#28436;&#31639;&#19982;&#26377;&#25928;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#20851;&#32852;&#30340;&#19968;&#32452;&#21551;&#21457;&#24335;&#26041;&#27861;&#20250;&#25552;&#20379;&#19968;&#31181;&#23558;&#35821;&#35328;&#29305;&#24449;&#65288;&#20363;&#22914;&#22810;&#24577;&#24615;&#65292;&#23376;&#31867;&#22411;&#65292;&#32487;&#25215;&#31561;&#65289;&#26144;&#23556;&#21040;&#26550;&#26500;&#36873;&#25321;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;Transformer &#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#24212;&#29992;&#20110;&#20195;&#30721;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#20294;&#20854;&#35774;&#35745;&#21644;&#36229;&#21442;&#25968;&#31354;&#38388;&#22312;&#32534;&#31243;&#35821;&#35328;&#24212;&#29992;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#20063;&#35768;&#26159;&#32534;&#31243;&#35821;&#35328;&#26368;&#31616;&#21333;&#21644;&#26368;&#22522;&#26412;&#30340;&#29305;&#24615;&#65292;&#21363;&#26415;&#35821;&#21644;&#31867;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26469;&#25506;&#32034;&#36825;&#19968;&#28857;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite a growing body of work at the intersection of deep learning and formal languages, there has been relatively little systematic exploration of transformer models for reasoning about typed lambda calculi. This is an interesting area of inquiry for two reasons. First, typed lambda calculi are the lingua franc of programming languages. A set of heuristics that relate various typed lambda calculi to effective neural architectures would provide a systematic method for mapping language features (e.g., polymorphism, subtyping, inheritance, etc.) to architecture choices. Second, transformer models are widely used in deep learning architectures applied to code, but the design and hyperparameter space for them is large and relatively unexplored in programming language applications. Therefore, we suggest a benchmark that allows us to explore exactly this through perhaps the simplest and most fundamental property of a programming language: the relationship between terms and types. Consequent
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26500;&#35937;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#21463;&#21040;&#36136;&#30097;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20173;&#26159;&#19968;&#20010;&#26377;&#21147;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2304.10494</link><description>&lt;p&gt;
&#26080;&#38480;&#29289;&#29702;&#29492;&#23376;&#65306;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26500;&#35937;&#29983;&#25104;&#20013;&#30495;&#30340;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Infinite Physical Monkey: Do Deep Learning Methods Really Perform Better in Conformation Generation?. (arXiv:2304.10494v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10494
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26500;&#35937;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#21463;&#21040;&#36136;&#30097;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20173;&#26159;&#19968;&#20010;&#26377;&#21147;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#35937;&#29983;&#25104;&#26159;&#33647;&#29289;&#21457;&#29616;&#21644;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#26377;&#26426;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#65292;&#29305;&#21035;&#26159;&#22312;&#30495;&#31354;&#21644;&#34507;&#30333;&#36136;&#21475;&#34955;&#29615;&#22659;&#20013;&#65292;&#19982;&#33647;&#29289;&#35774;&#35745;&#26368;&#30456;&#20851;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#26696;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#19968;&#39046;&#22495;&#65292;&#21253;&#25324;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#65288;&#22312;&#30495;&#31354;&#20013;&#65289;&#21644;&#32467;&#21512;&#20301;&#23039;&#29983;&#25104;&#65288;&#22312;&#34507;&#30333;&#36136;&#21475;&#34955;&#20013;&#65289;&#12290;&#21069;&#32773;&#25171;&#36133;&#20102;&#20256;&#32479;&#30340;ETKDG&#26041;&#27861;&#65292;&#32780;&#21518;&#32773;&#36798;&#21040;&#20102;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#20998;&#23376;&#23545;&#25509;&#36719;&#20214;&#30456;&#20284;&#30340;&#31934;&#24230;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#36136;&#30097;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#26159;&#21542;&#36890;&#36807;&#26080;&#21442;&#25968;&#26041;&#27861;&#26356;&#22909;&#22320;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#20013;&#34920;&#29616;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20182;&#20204;&#35774;&#35745;&#30340;&#26041;&#27861;&#26377;&#28857;&#31867;&#20284;&#20110;&#33879;&#21517;&#30340;&#26080;&#38480;&#29492;&#23376;&#23450;&#29702;&#65292;&#36825;&#20123;&#29492;&#23376;&#29978;&#33267;&#35013;&#22791;&#20102;&#29289;&#29702;&#25945;&#32946;&#12290;&#26412;&#25991;&#26088;&#22312;&#35752;&#35770;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#31616;&#21333;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#20173;&#28982;&#26159;&#26500;&#35937;&#29983;&#25104;&#38382;&#39064;&#20013;&#30340;&#26377;&#21147;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformation Generation is a fundamental problem in drug discovery and cheminformatics. And organic molecule conformation generation, particularly in vacuum and protein pocket environments, is most relevant to drug design. Recently, with the development of geometric neural networks, the data-driven schemes have been successfully applied in this field, both for molecular conformation generation (in vacuum) and binding pose generation (in protein pocket). The former beats the traditional ETKDG method, while the latter achieves similar accuracy compared with the widely used molecular docking software. Although these methods have shown promising results, some researchers have recently questioned whether deep learning (DL) methods perform better in molecular conformation generation via a parameter-free method. To our surprise, what they have designed is some kind analogous to the famous infinite monkey theorem, the monkeys that are even equipped with physics education. To discuss the feasib
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#39640;&#26102;&#38388;&#24046;&#35823;&#24046;&#30340;&#39564;&#35777;&#38598;&#19978;&#20986;&#29616;&#20102;&#20005;&#37325;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10466</link><description>&lt;p&gt;
&#39640;&#25928;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#25233;&#21046;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Efficient Deep Reinforcement Learning Requires Regulating Overfitting. (arXiv:2304.10466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10466
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#39640;&#26102;&#38388;&#24046;&#35823;&#24046;&#30340;&#39564;&#35777;&#38598;&#19978;&#20986;&#29616;&#20102;&#20005;&#37325;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#25910;&#38598;&#26377;&#38480;&#30340;&#25968;&#25454;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#38656;&#35201;&#27491;&#30830;&#30340;&#27491;&#21017;&#21270;&#25216;&#24039;&#25165;&#33021;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#21033;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#39564;&#20960;&#31181;&#20551;&#35774;&#65292;&#22914;&#38750;&#31283;&#24577;&#24615;&#12289;&#36807;&#24230;&#21160;&#20316;&#20998;&#24067;&#20559;&#31227;&#21644;&#36807;&#25311;&#21512;&#31561;&#65292;&#35797;&#22270;&#29702;&#35299;&#22312;&#26679;&#26412;&#39640;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20027;&#35201;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#23545;DeepMind&#25511;&#21046;&#22871;&#20214;&#65288;DMC&#65289;&#20219;&#21153;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#19968;&#31181;&#26377;&#25511;&#21046;&#12289;&#31995;&#32479;&#30340;&#26041;&#24335;&#23637;&#31034;&#20102;&#23545;&#36716;&#25442;&#30340;&#39564;&#35777;&#38598;&#30340;&#39640;&#26102;&#38388;&#24046;&#65288;TD&#65289;&#35823;&#24046;&#26159;&#20005;&#37325;&#24433;&#21709;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30340;&#20027;&#35201;&#32618;&#39745;&#31096;&#39318;&#65292;&#32780;&#20808;&#21069;&#30340;&#26041;&#27861;......(&#26410;&#23436;&#25972;&#32763;&#35793;)
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms that learn policies by trial-and-error must learn from limited amounts of data collected by actively interacting with the environment. While many prior works have shown that proper regularization techniques are crucial for enabling data-efficient RL, a general understanding of the bottlenecks in data-efficient RL has remained unclear. Consequently, it has been difficult to devise a universal technique that works well across all domains. In this paper, we attempt to understand the primary bottleneck in sample-efficient deep RL by examining several potential hypotheses such as non-stationarity, excessive action distribution shift, and overfitting. We perform thorough empirical analysis on state-based DeepMind control suite (DMC) tasks in a controlled and systematic way to show that high temporal-difference (TD) error on the validation set of transitions is the main culprit that severely affects the performance of deep RL algorithms, and prior method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#23398;&#20064;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20132;&#20114;&#28857;&#24182;&#22686;&#24378;&#21608;&#22260;&#29305;&#24449;&#65292;&#23454;&#29616;&#20004;&#24103;&#35270;&#39057;&#30340;&#38544;&#24335;&#23545;&#40784;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#26102;&#38388;&#24314;&#27169;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.10465</link><description>&lt;p&gt;
&#21487;&#23398;&#20064;&#23545;&#40784;&#30340;&#38544;&#24335;&#26102;&#38388;&#24314;&#27169;&#29992;&#20110;&#35270;&#39057;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Implicit Temporal Modeling with Learnable Alignment for Video Recognition. (arXiv:2304.10465v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#23398;&#20064;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20132;&#20114;&#28857;&#24182;&#22686;&#24378;&#21608;&#22260;&#29305;&#24449;&#65292;&#23454;&#29616;&#20004;&#24103;&#35270;&#39057;&#30340;&#38544;&#24335;&#23545;&#40784;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#26102;&#38388;&#24314;&#27169;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;(CLIP)&#22312;&#21508;&#31181;&#22270;&#20687;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#25193;&#23637;CLIP&#20197;&#36827;&#34892;&#26102;&#38388;&#24314;&#27169;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#19988;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#23398;&#20064;&#23545;&#40784;(ILA)&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26102;&#38388;&#24314;&#27169;&#30340;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26497;&#39640;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#19968;&#24103;&#23545;&#65292;&#27599;&#24103;&#37117;&#39044;&#27979;&#19968;&#20010;&#20132;&#20114;&#28857;&#65292;&#20316;&#20026;&#24444;&#27492;&#20449;&#24687;&#20016;&#23500;&#30340;&#21306;&#22495;&#12290;&#36890;&#36807;&#22686;&#24378;&#20132;&#20114;&#28857;&#21608;&#22260;&#30340;&#29305;&#24449;&#65292;&#20004;&#24103;&#34987;&#38544;&#24335;&#23545;&#40784;&#12290;&#23545;&#40784;&#30340;&#29305;&#24449;&#28982;&#21518;&#34987;&#27719;&#38598;&#25104;&#19968;&#20010;&#20196;&#29260;&#65292;&#29992;&#20110;&#21518;&#32493;&#30340;&#31354;&#38388;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive language-image pretraining (CLIP) has demonstrated remarkable success in various image tasks. However, how to extend CLIP with effective temporal modeling is still an open and crucial problem. Existing factorized or joint spatial-temporal modeling trades off between the efficiency and performance. While modeling temporal information within straight through tube is widely adopted in literature, we find that simple frame alignment already provides enough essence without temporal attention. To this end, in this paper, we proposed a novel Implicit Learnable Alignment (ILA) method, which minimizes the temporal modeling effort while achieving incredibly high performance. Specifically, for a frame pair, an interactive point is predicted in each frame, serving as a mutual information rich region. By enhancing the features around the interactive point, two frames are implicitly aligned. The aligned features are then pooled into a single token, which is leveraged in the subsequent sp
&lt;/p&gt;</description></item><item><title>Phoenix &#26159;&#19968;&#27454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#27665;&#20027;&#21270;&#65292;&#19981;&#20165;&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20063;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471; ChatGPT &#22312;&#26356;&#22810;&#30340;&#22269;&#23478;&#21644;&#22320;&#21306;&#21464;&#24471;&#26356;&#21152;&#21487;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.10453</link><description>&lt;p&gt;
Phoenix: &#23454;&#29616; ChatGPT &#30340;&#36328;&#35821;&#35328;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Phoenix: Democratizing ChatGPT across Languages. (arXiv:2304.10453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10453
&lt;/p&gt;
&lt;p&gt;
Phoenix &#26159;&#19968;&#27454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#27665;&#20027;&#21270;&#65292;&#19981;&#20165;&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20063;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471; ChatGPT &#22312;&#26356;&#22810;&#30340;&#22269;&#23478;&#21644;&#22320;&#21306;&#21464;&#24471;&#26356;&#21152;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#21162;&#21147;&#23454;&#29616; ChatGPT &#36328;&#35821;&#35328;&#27665;&#20027;&#21270;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#27454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8220;Phoenix&#8221;&#65292;&#22312;&#24320;&#28304;&#30340;&#33521;&#25991;&#21644;&#20013;&#25991;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#65288;&#21253;&#25324;&#25289;&#19969;&#21644;&#38750;&#25289;&#19969;&#35821;&#35328;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21033;&#20110;&#35753; ChatGPT &#22312;&#26356;&#22810;&#30340;&#22269;&#23478;&#21644;&#22320;&#21306;&#21464;&#24471;&#26356;&#21152;&#21487;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#30001;&#20110;OpenAI&#25110;&#24403;&#22320;&#25919;&#24220;&#30340;&#38480;&#21046;&#32780;&#26080;&#27861;&#20351;&#29992;ChatGPT&#30340;&#22269;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our efforts to democratize ChatGPT across language. We release a large language model "Phoenix", achieving competitive performance among open-source English and Chinese models while excelling in languages with limited resources (covering both Latin and non-Latin languages). We believe this work will be beneficial to make ChatGPT more accessible, especially in countries where people cannot use ChatGPT due to restrictions from OpenAI or local goverments. Our data, code, and models are available at https://github.com/FreedomIntelligence/LLMZoo.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32852;&#37030;&#29983;&#29289;&#20449;&#24687;&#23398;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#19982;&#39046;&#22495;&#19987;&#23478;&#23545;&#35805;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36328;&#25968;&#25454;&#38598;&#30340;&#26597;&#35810;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.10427</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32852;&#37030;&#29983;&#29289;&#20449;&#24687;&#23398;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#28508;&#21147;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
On the Potential of Artificial Intelligence Chatbots for Data Exploration of Federated Bioinformatics Knowledge Graphs. (arXiv:2304.10427v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32852;&#37030;&#29983;&#29289;&#20449;&#24687;&#23398;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#19982;&#39046;&#22495;&#19987;&#23478;&#23545;&#35805;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36328;&#25968;&#25454;&#38598;&#30340;&#26597;&#35810;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#30446;&#21069;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;&#20363;&#22914;ChatGPT&#65289;&#22312;&#32852;&#37030;&#21270;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#35775;&#38382;&#20013;&#30340;&#20316;&#29992;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;&#25552;&#20379;&#20102;&#29983;&#29289;&#20449;&#24687;&#23398;&#39046;&#22495;&#30340;&#31034;&#20363;&#65292;&#35828;&#26126;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#25551;&#36848;&#25968;&#25454;&#38598;&#20197;&#21450;&#29983;&#25104;&#21644;&#35299;&#37322;&#65288;&#32852;&#37030;&#21270;&#65289;&#36328;&#25968;&#25454;&#38598;&#26597;&#35810;&#30340;&#28508;&#22312;&#29992;&#36884;&#65292;&#20197;&#20415;&#20026;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present work in progress on the role of artificial intelligence (AI) chatbots, such as ChatGPT, in facilitating data access to federated knowledge graphs. In particular, we provide examples from the field of bioinformatics, to illustrate the potential use of Conversational AI to describe datasets, as well as generate and explain (federated) queries across datasets for the benefit of domain experts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23436;&#20840;&#33258;&#20027;&#32534;&#31243;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Synthesize&#12289;Execute&#12289;Debug&#65288;SED&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31243;&#24207;&#29983;&#25104;&#30340;&#20302;&#20934;&#30830;&#24230;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#35843;&#35797;&#31574;&#30053;&#21644;&#25552;&#31034;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10423</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23436;&#20840;&#33258;&#20027;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Fully Autonomous Programming with Large Language Models. (arXiv:2304.10423v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23436;&#20840;&#33258;&#20027;&#32534;&#31243;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Synthesize&#12289;Execute&#12289;Debug&#65288;SED&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31243;&#24207;&#29983;&#25104;&#30340;&#20302;&#20934;&#30830;&#24230;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#35843;&#35797;&#31574;&#30053;&#21644;&#25552;&#31034;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31243;&#24207;&#32508;&#21512;&#26041;&#27861;&#23384;&#22312;&#8220;&#20960;&#20046;&#25104;&#21151;&#32508;&#21512;&#8221;&#30340;&#38382;&#39064;&#65306;&#23427;&#20204;&#29983;&#25104;&#30340;&#31243;&#24207;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#27491;&#30830;&#31572;&#26696;&#65288;&#36890;&#36807;&#25991;&#26412;&#30456;&#20284;&#24615;&#24230;&#37327;&#25110;&#20154;&#24037;&#35780;&#20272;&#26469;&#34913;&#37327;&#65289;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#36755;&#20986;&#26684;&#24335;&#38169;&#35823;&#31561;&#32454;&#24494;&#24046;&#24322;&#65292;&#23545;&#20110;&#21333;&#20803;&#27979;&#35797;&#32780;&#35328;&#30340;&#20934;&#30830;&#24615;&#24456;&#20302;&#29978;&#33267;&#20026;&#38646;&#12290;&#36825;&#38656;&#35201;&#19968;&#31181;&#31216;&#20026;Synthesize&#12289;Execute&#12289;Debug&#65288;SED&#65289;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#33609;&#31295;&#65292;&#28982;&#21518;&#36827;&#34892;&#31243;&#24207;&#20462;&#22797;&#38454;&#27573;&#20197;&#35299;&#20915;&#26410;&#36890;&#36807;&#30340;&#27979;&#35797;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#25351;&#20196;&#39537;&#21160;&#30340;LLMs&#65292;&#38656;&#35201;&#30830;&#23450;&#21738;&#20123;&#25552;&#31034;&#20316;&#20026;LLMs&#30340;&#25351;&#20196;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#22312;&#20462;&#22797;&#26410;&#25104;&#21151;&#30340;&#31243;&#24207;&#21644;&#29992;&#26032;&#29983;&#25104;&#30340;&#31243;&#24207;&#26367;&#25442;&#23427;&#20204;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20197;&#26367;&#25442;&#20026;&#37325;&#28857;&#12289;&#20197;&#20462;&#22797;&#20026;&#37325;&#28857;&#21644;&#28151;&#21512;&#35843;&#35797;&#31574;&#30053;&#20197;&#21450;&#19981;&#21516;&#30340;&#22522;&#20110;&#27169;&#26495;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#25552;&#31034;&#29983;&#25104;&#26041;&#27861;&#26469;&#23454;&#35777;&#25506;&#35752;&#36825;&#20123;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to program synthesis with Large Language Models (LLMs) exhibit a "near miss syndrome": they tend to generate programs that semantically resemble the correct answer (as measured by text similarity metrics or human evaluation), but achieve a low or even zero accuracy as measured by unit tests due to small imperfections, such as the wrong input or output format. This calls for an approach known as Synthesize, Execute, Debug (SED), whereby a draft of the solution is generated first, followed by a program repair phase addressing the failed tests. To effectively apply this approach to instruction-driven LLMs, one needs to determine which prompts perform best as instructions for LLMs, as well as strike a balance between repairing unsuccessful programs and replacing them with newly generated ones. We explore these trade-offs empirically, comparing replace-focused, repair-focused, and hybrid debug strategies, as well as different template-based and model-based prompt-generati
&lt;/p&gt;</description></item><item><title>MAHH&#26041;&#27861;&#36890;&#36807;&#35299;&#20915;&#36339;&#36291;&#20989;&#25968;&#22312;&#23567;&#30340;&#38388;&#38553;&#22823;&#23567;&#19979;&#30340;&#34920;&#29616;&#38382;&#39064;&#65292;&#26174;&#30528;&#20248;&#20110;&#31616;&#21333;&#30340;&#31934;&#33521;&#36827;&#21270;&#31639;&#27861;&#12290;&#24403;&#22788;&#29702;&#23485;&#24748;&#23830;&#38382;&#39064;&#26102;&#65292;MAHH&#20173;&#28982;&#26159;&#35299;&#20915;&#23616;&#37096;&#26368;&#20248;&#35299;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10414</link><description>&lt;p&gt;
&#36816;&#21160;&#25509;&#21463;&#36229;&#21551;&#21457;&#24335;&#26041;&#27861;&#22914;&#20309;&#24212;&#23545;&#23616;&#37096;&#26368;&#20248;&#35299;&#38382;&#39064;&#65306;&#36291;&#36801;&#19982;&#24748;&#23830;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
How the Move Acceptance Hyper-Heuristic Copes With Local Optima: Drastic Differences Between Jumps and Cliffs. (arXiv:2304.10414v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10414
&lt;/p&gt;
&lt;p&gt;
MAHH&#26041;&#27861;&#36890;&#36807;&#35299;&#20915;&#36339;&#36291;&#20989;&#25968;&#22312;&#23567;&#30340;&#38388;&#38553;&#22823;&#23567;&#19979;&#30340;&#34920;&#29616;&#38382;&#39064;&#65292;&#26174;&#30528;&#20248;&#20110;&#31616;&#21333;&#30340;&#31934;&#33521;&#36827;&#21270;&#31639;&#27861;&#12290;&#24403;&#22788;&#29702;&#23485;&#24748;&#23830;&#38382;&#39064;&#26102;&#65292;MAHH&#20173;&#28982;&#26159;&#35299;&#20915;&#23616;&#37096;&#26368;&#20248;&#35299;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;Lissovoi&#12289;Oliveto&#21644;Warwicker&#65288;&#12298;&#20154;&#24037;&#26234;&#33021;&#12299;&#65288;2023&#65289;&#65289;&#35777;&#26126;&#20102;&#36816;&#21160;&#25509;&#21463;&#36229;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;MAHH&#65289;&#20197;&#26174;&#30528;&#30340;&#25928;&#29575;&#31163;&#24320;&#22810;&#23792;&#24748;&#23830;&#22522;&#20934;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#23545;&#20110;&#20960;&#20046;&#25152;&#26377;&#24748;&#23830;&#23485;&#24230;$d$&#65292;MAHH&#30340;$O(n^3)$&#36816;&#34892;&#26102;&#38388;&#22823;&#22823;&#20248;&#20110;&#31616;&#21333;&#31934;&#33521;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#30340;$\Theta(n^d)$&#36816;&#34892;&#26102;&#38388;&#12290;&#23545;&#20110;&#26368;&#20027;&#35201;&#30340;&#22810;&#23792;&#22522;&#20934;&#36339;&#36291;&#20989;&#25968;&#65292;&#23545;&#20110;&#38388;&#38553;&#22823;&#23567;$m\geq 2$&#30340;&#36816;&#34892;&#26102;&#38388;&#20272;&#35745;&#20026;$O(n^{2m}m^{-\Theta(m)})$&#21644;$\Omega(2^{\Omega(m)})$&#65292;&#20004;&#32773;&#30340;&#24046;&#36317;&#24456;&#22823;&#65292;MAHH&#30340;&#23454;&#38469;&#34920;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;MAHH&#36873;&#25321;&#21442;&#25968;$p$&#30340;&#36873;&#25321;&#65292;MAHH&#22312;&#20855;&#26377;&#38388;&#38553;&#22823;&#23567;$m = o(n^{1/2})$&#30340;&#36339;&#36291;&#20989;&#25968;&#19978;&#30340;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#33267;&#23569;&#20026;$\Omega(n^{2m-1}/(2m-1)!)$&#12290;&#36825;&#20351;&#24471;MAHH&#27604;&#31616;&#21333;&#30340;&#31934;&#33521;&#36827;&#21270;&#31639;&#27861;&#26356;&#24930;&#65292;&#21518;&#32773;&#36890;&#24120;&#26377;$O(n^m)$&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent work, Lissovoi, Oliveto, and Warwicker (Artificial Intelligence (2023)) proved that the Move Acceptance Hyper-Heuristic (MAHH) leaves the local optimum of the multimodal cliff benchmark with remarkable efficiency. With its $O(n^3)$ runtime, for almost all cliff widths $d,$ the MAHH massively outperforms the $\Theta(n^d)$ runtime of simple elitist evolutionary algorithms (EAs). For the most prominent multimodal benchmark, the jump functions, the given runtime estimates of $O(n^{2m} m^{-\Theta(m)})$ and $\Omega(2^{\Omega(m)})$, for gap size $m \ge 2$, are far apart and the real performance of MAHH is still an open question.  In this work, we resolve this question. We prove that for any choice of the MAHH selection parameter~$p$, the expected runtime of the MAHH on a jump function with gap size $m = o(n^{1/2})$ is at least $\Omega(n^{2m-1} / (2m-1)!)$. This renders the MAHH much slower than simple elitist evolutionary algorithms with their typical $O(n^m)$ runtime.  We also show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#25351;&#21335;&#65292;&#20027;&#35201;&#20851;&#27880;&#24863;&#30693;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#12290;&#38647;&#36798;&#21644;&#30456;&#26426;&#36890;&#36807;&#20114;&#34917;&#30340;&#26041;&#24335;&#25552;&#20379;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#21608;&#22260;&#29615;&#22659;&#24863;&#30693;&#65292;&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#22788;&#29702;&#21644;&#34920;&#31034;&#65292;&#35814;&#32454;&#24635;&#32467;&#20102;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#65292;&#24182;&#35299;&#20915;&#20102;&#35768;&#22810;&#26041;&#27861;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10410</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#65306;&#32508;&#36848;&#19982;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review. (arXiv:2304.10410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#25351;&#21335;&#65292;&#20027;&#35201;&#20851;&#27880;&#24863;&#30693;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#12290;&#38647;&#36798;&#21644;&#30456;&#26426;&#36890;&#36807;&#20114;&#34917;&#30340;&#26041;&#24335;&#25552;&#20379;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#21608;&#22260;&#29615;&#22659;&#24863;&#30693;&#65292;&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#22788;&#29702;&#21644;&#34920;&#31034;&#65292;&#35814;&#32454;&#24635;&#32467;&#20102;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#65292;&#24182;&#35299;&#20915;&#20102;&#35768;&#22810;&#26041;&#27861;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25512;&#21160;&#65292;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#24863;&#30693;&#25216;&#26415;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#20026;&#20102;&#23454;&#29616;&#31934;&#20934;&#21644;&#31283;&#20581;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#36890;&#24120;&#37197;&#22791;&#22810;&#31181;&#20256;&#24863;&#22120;&#65292;&#20351;&#24471;&#20256;&#24863;&#22120;&#34701;&#21512;&#25104;&#20026;&#24863;&#30693;&#31995;&#32479;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#22312;&#36825;&#20123;&#20256;&#24863;&#22120;&#20013;&#65292;&#38647;&#36798;&#21644;&#30456;&#26426;&#36890;&#36807;&#20114;&#34917;&#30340;&#26041;&#24335;&#65292;&#22312;&#20219;&#20309;&#20809;&#29031;&#21644;&#22825;&#27668;&#26465;&#20214;&#19979;&#37117;&#33021;&#22815;&#25552;&#20379;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#21608;&#22260;&#29615;&#22659;&#24863;&#30693;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20221;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#29305;&#21035;&#20851;&#27880;&#19982;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#30456;&#20851;&#30340;&#24863;&#30693;&#20219;&#21153;&#12290;&#22522;&#20110;&#38647;&#36798;&#21644;&#30456;&#26426;&#20256;&#24863;&#22120;&#30340;&#21407;&#29702;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#25968;&#25454;&#22788;&#29702;&#36807;&#31243;&#21644;&#34920;&#31034;&#65292;&#25509;&#30528;&#35814;&#32454;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#12290;&#22312;&#25506;&#35752;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#30340;&#26041;&#27861;&#23398;&#26102;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#35768;&#22810;&#38382;&#39064;&#65292;&#21253;&#25324;&#8220;&#20026;&#20160;&#20040;&#34701;&#21512;&#8221;&#65292;&#8220;&#34701;&#21512;&#20160;&#20040;&#8221;&#65292;&#8220;&#22312;&#21738;&#37324;&#34701;&#21512;&#8221;
&lt;/p&gt;
&lt;p&gt;
Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years. To achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the perception system. Among these fused sensors, radars and cameras enable a complementary and cost-effective perception of the surrounding environment regardless of lighting and weather conditions. This review aims to provide a comprehensive guideline for radar-camera fusion, particularly concentrating on perception tasks related to object detection and semantic segmentation. Based on the principles of the radar and camera sensors, we delve into the data processing process and representations, followed by an in-depth analysis and summary of radar-camera fusion datasets. In the review of methodologies in radar-camera fusion, we address interrogative questions, including "why to fuse", "what to fuse", "where to fus
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;CPDL+&#65292;&#23427;&#26159;&#19968;&#31181;&#34920;&#36798;&#24335;&#24378;&#22823;&#30340;&#36923;&#36753;&#31995;&#32479;&#65292;&#25193;&#23637;&#20102;PDL&#30340;&#20132;&#38598;&#21644;&#36716;&#32622;&#65292;&#36229;&#36234;&#20102;CQ&#12289;CRPQ&#21644;&#24050;&#30693;&#25193;&#23637;&#30340;&#34920;&#36798;&#21147;&#12290;&#26641;&#23485;&#20026;2&#30340;CPDL+&#20844;&#24335;&#31561;&#20215;&#20110;ICPDL&#65292;&#20294;&#22686;&#21152;&#26641;&#23485;&#20250;&#22686;&#21152;&#34920;&#36798;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10381</link><description>&lt;p&gt;
PDL&#22312;&#25299;&#23637;&#19978;&#26356;&#19978;&#19968;&#23618;&#27004;&#65306;&#20855;&#26377;&#20132;&#38598;&#21644;&#36716;&#32622;&#30340;PDL&#34920;&#36798;&#30340;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
PDL on Steroids: on Expressive Extensions of PDL with Intersection and Converse. (arXiv:2304.10381v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10381
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;CPDL+&#65292;&#23427;&#26159;&#19968;&#31181;&#34920;&#36798;&#24335;&#24378;&#22823;&#30340;&#36923;&#36753;&#31995;&#32479;&#65292;&#25193;&#23637;&#20102;PDL&#30340;&#20132;&#38598;&#21644;&#36716;&#32622;&#65292;&#36229;&#36234;&#20102;CQ&#12289;CRPQ&#21644;&#24050;&#30693;&#25193;&#23637;&#30340;&#34920;&#36798;&#21147;&#12290;&#26641;&#23485;&#20026;2&#30340;CPDL+&#20844;&#24335;&#31561;&#20215;&#20110;ICPDL&#65292;&#20294;&#22686;&#21152;&#26641;&#23485;&#20250;&#22686;&#21152;&#34920;&#36798;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CPDL+&#65292;&#36825;&#26159;&#19968;&#26063;&#34920;&#36798;&#24335;&#24378;&#22823;&#30340;&#36923;&#36753;&#31995;&#32479;&#65292;&#26681;&#26893;&#20110;&#21629;&#39064;&#21160;&#24577;&#36923;&#36753;&#65288;PDL&#65289;&#12290;&#22312;&#34920;&#36798;&#21147;&#19978;&#65292;CPDL+&#20005;&#26684;&#21253;&#21547;&#25193;&#23637;&#20102;&#20132;&#38598;&#21644;&#36716;&#32622;&#30340;PDL&#65288;&#21363;ICPDL&#65289;&#65292;&#20197;&#21450;&#36830;&#25509;&#26597;&#35810;&#65288;CQ&#65289;&#12289;&#36830;&#36890;&#27491;&#21017;&#36335;&#24452;&#26597;&#35810;&#65288;CRPQ&#65289;&#25110;&#26576;&#20123;&#24050;&#30693;&#30340;&#25193;&#23637;&#65288;&#27491;&#21017;&#26597;&#35810;&#21644;CQPDL&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;CPDL+&#30340;&#34920;&#36798;&#21147;&#12289;&#21452;&#27169;&#25311;&#30340;&#29305;&#24449;&#12289;&#21487;&#28385;&#36275;&#24615;&#21644;&#27169;&#22411;&#26816;&#26597;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;CPDL+&#30340;&#33258;&#28982;&#23376;&#38598;&#21487;&#20197;&#29992;&#20844;&#24335;&#30340;&#22522;&#30784;&#22270;&#30340;&#26641;&#23485;&#26469;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26641;&#23485;&#20026;2&#30340;CPDL+&#20844;&#24335;&#30340;&#31867;&#31561;&#20215;&#20110;ICPDL&#65292;&#23427;&#20063;&#19982;&#26641;&#23485;&#20026;1&#30340;CPDL+&#20844;&#24335;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;&#26641;&#23485;&#22823;&#20110;2&#30340;&#24773;&#20917;&#19979;&#65292;&#22686;&#21152;&#26641;&#23485;&#20005;&#26684;&#22686;&#21152;&#20102;&#34920;&#36798;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24102;&#30707;&#23376;&#30340;&#21452;&#27169;&#25311;&#21338;&#24328;&#26469;&#34920;&#24449;&#20102;&#27599;&#20010;&#22266;&#23450;&#26641;&#23485;&#20844;&#24335;&#31867;&#30340;&#34920;&#36798;&#21147;&#12290;&#22522;&#20110;&#36825;&#20010;&#34920;&#24449;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31639;&#27861;&#65292;&#29992;&#20110;&#21028;&#26029;&#20855;&#26377;&#26368;&#23567;&#25110;&#29305;&#23450;&#26641;&#23485;&#30340;&#20844;&#24335;&#30340;&#21487;&#28385;&#36275;&#24615;&#21644;&#27169;&#22411;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CPDL+, a family of expressive logics rooted in Propositional Dynamic Logic (PDL). In terms of expressive power, CPDL+ strictly contains PDL extended with intersection and converse (a.k.a. ICPDL) as well as Conjunctive Queries (CQ), Conjunctive Regular Path Queries (CRPQ), or some known extensions thereof (Regular Queries and CQPDL). We investigate the expressive power, characterization of bisimulation, satisfiability, and model checking for CPDL+.  We argue that natural subclasses of CPDL+ can be defined in terms of the tree-width of the underlying graphs of the formulas. We show that the class of CPDL+ formulas of tree-width 2 is equivalent to ICPDL, and that it also coincides with CPDL+ formulas of tree-width 1. However, beyond tree-width 2, incrementing the tree-width strictly increases the expressive power. We characterize the expressive power for every class of fixed tree-width formulas in terms of a bisimulation game with pebbles. Based on this characterization, we s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#28436;&#21592;&#26550;&#26500;&#65292;&#21033;&#29992;&#26174;&#33879;&#24615;&#21521;&#37327;&#37325;&#29992;&#29615;&#22659;&#30340;&#26465;&#20214;&#29366;&#24577;&#26469;&#25552;&#39640;&#26465;&#20214;&#21327;&#21516;&#34892;&#20026;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20195;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10375</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#26465;&#20214;&#21327;&#21516;&#34892;&#20026;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning. (arXiv:2304.10375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10375
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#28436;&#21592;&#26550;&#26500;&#65292;&#21033;&#29992;&#26174;&#33879;&#24615;&#21521;&#37327;&#37325;&#29992;&#29615;&#22659;&#30340;&#26465;&#20214;&#29366;&#24577;&#26469;&#25552;&#39640;&#26465;&#20214;&#21327;&#21516;&#34892;&#20026;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20195;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#31216;&#20026;&#22522;&#20110;&#26465;&#20214;&#27880;&#24847;&#21147;(DA6-X)&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#28436;&#21592;&#26550;&#26500;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#26465;&#20214;&#21327;&#21516;&#34892;&#20026;&#21487;&#35299;&#37322;&#24615;&#12290;&#20854;&#22522;&#26412;&#21407;&#29702;&#28041;&#21450;&#37325;&#29992;&#26174;&#33879;&#24615;&#21521;&#37327;&#65292;&#35813;&#21521;&#37327;&#34920;&#31034;&#29615;&#22659;&#30340;&#26465;&#20214;&#29366;&#24577;&#65292;&#20363;&#22914;&#20195;&#29702;&#30340;&#20840;&#23616;&#20301;&#32622;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#23884;&#20837;&#20854;&#31574;&#30053;&#20013;&#30340;DA6-X&#28789;&#27963;&#24615;&#30340;&#20195;&#29702;&#36890;&#36807;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#32771;&#34385;&#26465;&#20214;&#29366;&#24577;&#20013;&#30340;&#38468;&#21152;&#20449;&#24687;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#23545;&#35937;&#25910;&#38598;&#28216;&#25103;&#20013;&#23558;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;DA6-X&#30340;&#27880;&#24847;&#26435;&#37325;&#65292;&#25105;&#20204;&#30830;&#35748;&#20195;&#29702;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#24773;&#22659;&#20381;&#36182;&#24615;&#21327;&#21516;&#34892;&#20026;&#65292;&#36890;&#36807;&#27491;&#30830;&#35782;&#21035;&#21508;&#31181;&#26465;&#20214;&#29366;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model-free reinforcement learning architecture, called distributed attentional actor architecture after conditional attention (DA6-X), to provide better interpretability of conditional coordinated behaviors. The underlying principle involves reusing the saliency vector, which represents the conditional states of the environment, such as the global position of agents. Hence, agents with DA6-X flexibility built into their policy exhibit superior performance by considering the additional information in the conditional states during the decision-making process. The effectiveness of the proposed method was experimentally evaluated by comparing it with conventional methods in an objects collection game. By visualizing the attention weights from DA6-X, we confirmed that agents successfully learn situation-dependent coordinated behaviors by correctly identifying various conditional states, leading to improved interpretability of agents along with superior performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26102;&#31354;&#24207;&#36143;&#20915;&#31574;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;&#31574;&#30053;&#30340;&#24322;&#27493;&#34892;&#21160;&#21327;&#35843;&#65292;&#24182;&#22312;&#21442;&#25968;&#20849;&#20139;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19981;&#21516;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2304.10351</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#31354;&#24207;&#36143;&#20915;&#31574;&#21046;&#23548;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;&#30340;&#20135;&#29983;
&lt;/p&gt;
&lt;p&gt;
Inducing Stackelberg Equilibrium through Spatio-Temporal Sequential Decision-Making in Multi-Agent Reinforcement Learning. (arXiv:2304.10351v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26102;&#31354;&#24207;&#36143;&#20915;&#31574;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;&#31574;&#30053;&#30340;&#24322;&#27493;&#34892;&#21160;&#21327;&#35843;&#65292;&#24182;&#22312;&#21442;&#25968;&#20849;&#20139;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19981;&#21516;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#20013;&#65292;&#33258;&#21033;&#30340;&#26234;&#33021;&#20307;&#35797;&#22270;&#24314;&#31435;&#22343;&#34913;&#24182;&#26681;&#25454;&#28216;&#25103;&#32467;&#26500;&#23454;&#29616;&#21327;&#35843;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MARL&#26041;&#27861;&#22823;&#22810;&#21463;&#21046;&#20110;&#25152;&#26377;&#26234;&#33021;&#20307;&#22312;&#39532;&#21487;&#22827;&#21338;&#24328;(MG)&#26694;&#26550;&#20013;&#30340;&#21516;&#26102;&#34892;&#21160;&#65292;&#24456;&#23569;&#26377;&#20316;&#21697;&#32771;&#34385;&#36890;&#36807;&#24322;&#27493;&#34892;&#21160;&#21327;&#35843;&#24418;&#25104;&#22343;&#34913;&#31574;&#30053;&#12290;&#37492;&#20110;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;(SE)&#30456;&#23545;&#32435;&#20160;&#22343;&#34913;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20174;MG&#23548;&#20986;&#30340;&#26102;&#31354;&#24207;&#36143;&#20915;&#31574;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25152;&#26377;&#26234;&#33021;&#20307;&#20849;&#20139;&#30340;&#26465;&#20214;&#36229;&#32593;&#32476;&#30340;N&#32423;&#31574;&#30053;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#19981;&#23545;&#31216;&#35757;&#32451;&#21644;&#23545;&#31216;&#25191;&#34892;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21709;&#24212;&#20110;&#19978;&#32423;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#26368;&#20248;&#21453;&#24212;&#12290;&#26234;&#33021;&#20307;&#21487;&#20197;&#23398;&#20064;&#19981;&#21516;&#30340;SE&#31574;&#30053;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;&#21442;&#25968;&#20849;&#20139;&#65292;&#36825;&#23548;&#33268;&#20102;&#23398;&#20064;&#21644;&#23384;&#20648;&#25104;&#26412;&#30340;&#38477;&#20302;&#20197;&#21450;&#25193;&#23637;&#24615;&#30340;&#25552;&#39640;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent reinforcement learning (MARL), self-interested agents attempt to establish equilibrium and achieve coordination depending on game structure. However, existing MARL approaches are mostly bound by the simultaneous actions of all agents in the Markov game (MG) framework, and few works consider the formation of equilibrium strategies via asynchronous action coordination. In view of the advantages of Stackelberg equilibrium (SE) over Nash equilibrium, we construct a spatio-temporal sequential decision-making structure derived from the MG and propose an N-level policy model based on a conditional hypernetwork shared by all agents. This approach allows for asymmetric training with symmetric execution, with each agent responding optimally conditioned on the decisions made by superior agents. Agents can learn heterogeneous SE policies while still maintaining parameter sharing, which leads to reduced cost for learning and storage and enhanced scalability as the number of agents in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24102;&#20551;&#35774;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#65288;NSRwH&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#23450;&#20041;&#30340;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#21040;&#29983;&#25104;&#20998;&#26512;&#34920;&#36798;&#24335;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#30446;&#26631;&#34920;&#36798;&#24335;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#31526;&#21495;&#31867;&#22411;&#21644;&#33539;&#22260;&#65292;&#24182;&#23545;&#34920;&#36798;&#24335;&#30340;&#22797;&#26434;&#24615;&#26045;&#21152;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2304.10336</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Controllable Neural Symbolic Regression. (arXiv:2304.10336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24102;&#20551;&#35774;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#65288;NSRwH&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#23450;&#20041;&#30340;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#21040;&#29983;&#25104;&#20998;&#26512;&#34920;&#36798;&#24335;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#30446;&#26631;&#34920;&#36798;&#24335;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#31526;&#21495;&#31867;&#22411;&#21644;&#33539;&#22260;&#65292;&#24182;&#23545;&#34920;&#36798;&#24335;&#30340;&#22797;&#26434;&#24615;&#26045;&#21152;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31526;&#21495;&#22238;&#24402;&#20013;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#25968;&#23398;&#31526;&#21495;&#65288;&#20363;&#22914;&#36816;&#31639;&#31526;&#12289;&#21464;&#37327;&#21644;&#24120;&#37327;&#65289;&#30340;&#24773;&#20917;&#19979;&#31934;&#30830;&#22320;&#25311;&#21512;&#23454;&#39564;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#34920;&#36798;&#24335;&#30340;&#32452;&#21512;&#31354;&#38388;&#24456;&#22823;&#65292;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#24456;&#38590;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25214;&#21040;&#27491;&#30830;&#30340;&#34920;&#36798;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#65288;NSR&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#24182;&#29983;&#25104;&#20998;&#26512;&#34920;&#36798;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30446;&#21069;&#36824;&#32570;&#20047;&#23558;&#29992;&#25143;&#23450;&#20041;&#30340;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#20854;&#20013;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#31181;&#33021;&#21147;&#22312;&#33258;&#28982;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#36890;&#24120;&#26159;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21517;&#20026;&#24102;&#20551;&#35774;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#65288;NSRwH&#65289;&#65292;&#23427;&#20801;&#35768;&#26174;&#24335;&#22320;&#23558;&#20851;&#20110;&#30495;&#23454;&#34920;&#36798;&#24335;&#26399;&#26395;&#32467;&#26500;&#30340;&#20551;&#35774;&#32435;&#20837;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In symbolic regression, the goal is to find an analytical expression that accurately fits experimental data with the minimal use of mathematical symbols such as operators, variables, and constants. However, the combinatorial space of possible expressions can make it challenging for traditional evolutionary algorithms to find the correct expression in a reasonable amount of time. To address this issue, Neural Symbolic Regression (NSR) algorithms have been developed that can quickly identify patterns in the data and generate analytical expressions. However, these methods, in their current form, lack the capability to incorporate user-defined prior knowledge, which is often required in natural sciences and engineering fields. To overcome this limitation, we propose a novel neural symbolic regression method, named Neural Symbolic Regression with Hypothesis (NSRwH) that enables the explicit incorporation of assumptions about the expected structure of the ground-truth expression into the pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#22330;&#26223;&#65292;&#25552;&#20986;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#22238;&#39038;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;&#20869;&#23384;&#25928;&#29575;&#21464;&#20307;&#21487;&#26377;&#25928;&#22320;&#20445;&#25345;&#19968;&#23450;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10335</link><description>&lt;p&gt;
&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#20013;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#32447;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A baseline on continual learning methods for video action recognition. (arXiv:2304.10335v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#22330;&#26223;&#65292;&#25552;&#20986;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#22238;&#39038;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;&#20869;&#23384;&#25928;&#29575;&#21464;&#20307;&#21487;&#26377;&#25928;&#22320;&#20445;&#25345;&#19968;&#23450;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36830;&#32493;&#23398;&#20064;&#21560;&#24341;&#20102;&#30740;&#31350;&#30028;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26088;&#22312;&#35299;&#20915;&#32463;&#20856;&#30417;&#30563;&#27169;&#22411;&#30340;&#38271;&#26399;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#37117;&#26159;&#38024;&#23545;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#36827;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#29366;&#24577;&#19979;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;&#38500;&#20102;&#30001;&#20110;&#26102;&#38388;&#32500;&#24230;&#32780;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#22806;&#65292;&#22312;&#35270;&#39057;&#29615;&#22659;&#20013;&#65292;&#20026;&#20102;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#30340;&#22238;&#39038;&#26041;&#27861;&#23545;&#35745;&#31639;&#36164;&#28304;&#35201;&#27714;&#26356;&#39640;&#12290;&#20026;&#20102;&#23545;&#25239;&#22686;&#21152;&#30340;&#20869;&#23384;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#26041;&#27861;&#37117;&#36890;&#29992;&#30340;&#22238;&#39038;&#26041;&#27861;&#21464;&#20307;&#65292;&#21033;&#29992;&#27169;&#22411;&#32622;&#20449;&#24230;&#25110;&#25968;&#25454;&#20449;&#24687;&#30340;&#25351;&#26631;&#26469;&#36873;&#25321;&#21487;&#35760;&#24518;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#25991;&#29486;&#20013;&#39044;&#26399;&#30340;&#19968;&#26679;&#65292;&#22238;&#39038;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65307;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#20869;&#23384;&#25928;&#29575;&#21464;&#20307;&#34987;&#35777;&#26126;&#22312;&#20445;&#25345;&#19968;&#23450;&#27700;&#24179;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning has recently attracted attention from the research community, as it aims to solve long-standing limitations of classic supervisedly-trained models. However, most research on this subject has tackled continual learning in simple image classification scenarios. In this paper, we present a benchmark of state-of-the-art continual learning methods on video action recognition. Besides the increased complexity due to the temporal dimension, the video setting imposes stronger requirements on computing resources for top-performing rehearsal methods. To counteract the increased memory requirements, we present two method-agnostic variants for rehearsal methods, exploiting measures of either model confidence or data information to select memorable samples. Our experiments show that, as expected from the literature, rehearsal methods outperform other approaches; moreover, the proposed memory-efficient variants are shown to be effective at retaining a certain level of performance 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;&#34913;&#37327;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#20182;&#20204;&#20351;&#29992;&#20102;&#34892;&#20026;&#35266;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.10327</link><description>&lt;p&gt;
&#21521;&#30528;&#20154;&#31867;&#21644;&#26426;&#22120;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards a Benchmark for Scientific Understanding in Humans and Machines. (arXiv:2304.10327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;&#34913;&#37327;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#20182;&#20204;&#20351;&#29992;&#20102;&#34892;&#20026;&#35266;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#29702;&#35299;&#26159;&#31185;&#23398;&#30340;&#22522;&#26412;&#30446;&#26631;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#35299;&#37322;&#19990;&#30028;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#22909;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#20195;&#29702;&#20154;&#30340;&#31185;&#23398;&#29702;&#35299;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#20154;&#31867;&#36824;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#32570;&#20047;&#28165;&#26224;&#30340;&#22522;&#20934;&#65292;&#38590;&#20197;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#22312;&#27492;&#36335;&#32447;&#22270;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#31185;&#23398;&#21746;&#23398;&#24037;&#20855;&#21019;&#24314;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#37319;&#29992;&#34892;&#20026;&#35266;&#24565;&#65292;&#35748;&#20026;&#30495;&#27491;&#30340;&#29702;&#35299;&#24212;&#35813;&#34987;&#35748;&#20026;&#26159;&#25191;&#34892;&#26576;&#20123;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#32452;&#38382;&#39064;&#26469;&#25193;&#23637;&#36825;&#20010;&#27010;&#24565;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#65292;&#23433;&#25490;&#20449;&#24687;&#20197;&#29983;&#25104;&#35299;&#37322;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#25512;&#26029;&#20107;&#29289;&#20250;&#26377;&#21738;&#20123;&#19981;&#21516;&#12290;Scientific Understanding Benchmark&#65288;SUB&#65289;&#30001;
&lt;/p&gt;
&lt;p&gt;
Scientific understanding is a fundamental goal of science, allowing us to explain the world. There is currently no good way to measure the scientific understanding of agents, whether these be humans or Artificial Intelligence systems. Without a clear benchmark, it is challenging to evaluate and compare different levels of and approaches to scientific understanding. In this Roadmap, we propose a framework to create a benchmark for scientific understanding, utilizing tools from philosophy of science. We adopt a behavioral notion according to which genuine understanding should be recognized as an ability to perform certain tasks. We extend this notion by considering a set of questions that can gauge different levels of scientific understanding, covering information retrieval, the capability to arrange information to produce an explanation, and the ability to infer how things would be different under different circumstances. The Scientific Understanding Benchmark (SUB), which is formed by 
&lt;/p&gt;</description></item><item><title>LA3 &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#65292;&#20026;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#20998;&#21035;&#23398;&#20064;&#22686;&#24378;&#31574;&#30053;&#65292;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10310</link><description>&lt;p&gt;
LA3: &#39640;&#25928;&#30340;&#26631;&#31614;&#24863;&#30693;&#33258;&#21160;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
LA3: Efficient Label-Aware AutoAugment. (arXiv:2304.10310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10310
&lt;/p&gt;
&lt;p&gt;
LA3 &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#65292;&#20026;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#20998;&#21035;&#23398;&#20064;&#22686;&#24378;&#31574;&#30053;&#65292;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#22686;&#24378;&#25216;&#26415;&#26159;&#19968;&#31181;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#20852;&#26377;&#25928;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#37096;&#20998;&#24037;&#20316;&#20391;&#37325;&#20110;&#26500;&#24314;&#36866;&#29992;&#20110;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#30340;&#32479;&#19968;&#22686;&#24378;&#31574;&#30053;&#65292;&#32780;&#24573;&#30053;&#20102;&#26679;&#26412;&#25110;&#31867;&#21035;&#20043;&#38388;&#30340;&#24046;&#24322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861; LA3&#65292;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#65292;&#38024;&#23545;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#20998;&#21035;&#23398;&#20064;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated augmentation is an emerging and effective technique to search for data augmentation policies to improve generalizability of deep neural network training. Most existing work focuses on constructing a unified policy applicable to all data samples in a given dataset, without considering sample or class variations. In this paper, we propose a novel two-stage data augmentation algorithm, named Label-Aware AutoAugment (LA3), which takes advantage of the label information, and learns augmentation policies separately for samples of different labels. LA3 consists of two learning stages, where in the first stage, individual augmentation methods are evaluated and ranked for each label via Bayesian Optimization aided by a neural predictor, which allows us to identify effective augmentation techniques for each label under a low search cost. And in the second stage, a composite augmentation policy is constructed out of a selection of effective as well as complementary augmentations, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FIANCEE&#65292;&#36890;&#36807;&#28155;&#21152;&#26089;&#26399;&#36864;&#20986;&#20998;&#25903;&#24182;&#26681;&#25454;&#36755;&#20986;&#38590;&#24230;&#21160;&#24577;&#20999;&#25442;&#35745;&#31639;&#36335;&#24452;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#23545;&#25239;&#24615;&#32593;&#32476;&#25512;&#26029;&#65292;&#32463;&#36807;&#39564;&#35777;&#35813;&#26041;&#27861;&#23545;&#20110;&#29983;&#25104;&#20219;&#21153;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#29305;&#21035;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.10306</link><description>&lt;p&gt;
FIANCEE: &#36890;&#36807;&#26465;&#20214;&#26089;&#26399;&#36864;&#20986;&#21152;&#36895;&#23545;&#25239;&#24615;&#32593;&#32476;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits. (arXiv:2304.10306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FIANCEE&#65292;&#36890;&#36807;&#28155;&#21152;&#26089;&#26399;&#36864;&#20986;&#20998;&#25903;&#24182;&#26681;&#25454;&#36755;&#20986;&#38590;&#24230;&#21160;&#24577;&#20999;&#25442;&#35745;&#31639;&#36335;&#24452;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#23545;&#25239;&#24615;&#32593;&#32476;&#25512;&#26029;&#65292;&#32463;&#36807;&#39564;&#35777;&#35813;&#26041;&#27861;&#23545;&#20110;&#29983;&#25104;&#20219;&#21153;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#29305;&#21035;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#22270;&#20687;&#21512;&#25104;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#35745;&#31639;&#36127;&#36733;&#30340;&#38480;&#21046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38024;&#23545;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#65288;&#20363;&#22914;&#22312;&#29305;&#23450;&#29305;&#24449;&#33539;&#22260;&#20869;&#29983;&#25104;&#20154;&#33080;&#65289;&#65292;&#19981;&#21516;&#29305;&#24449;&#19979;&#30340;&#22270;&#20687;&#36136;&#37327;&#20250;&#23384;&#22312;&#24046;&#24322;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#26576;&#20123;&#23454;&#20363;&#19978;&#38480;&#21046;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#65292;&#20445;&#25345;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#26550;&#26500;&#20013;&#28155;&#21152;&#25152;&#35859;&#30340;&#26089;&#26399;&#36864;&#20986;&#20998;&#25903;&#65292;&#24182;&#26681;&#25454;&#36755;&#20986;&#38590;&#24230;&#21160;&#24577;&#20999;&#25442;&#35745;&#31639;&#36335;&#24452;&#65292;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#25191;&#34892;&#29983;&#25104;&#20219;&#21153;&#65306;&#20174;&#35821;&#20041;&#26144;&#23556;&#20013;&#29983;&#25104;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#20132;&#21449;&#37325;&#29616;&#65292;&#34920;&#26126;&#23427;&#33021;&#22815;&#36755;&#20986;&#20855;&#26377;&#33258;&#23450;&#20041;&#20302;&#36136;&#37327;&#38408;&#20540;&#30340;&#22270;&#20687;&#12290;&#23545;&#20110;LPIPS &lt;=0.1&#30340;&#38408;&#20540;&#65292;&#25105;&#20204;&#23558;&#35745;&#31639;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#12290;&#36825;&#23545;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#23588;&#20854;&#37325;&#35201;&#65292;&#20854;&#20013;&#24555;&#36895;&#25512;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative DNNs are a powerful tool for image synthesis, but they are limited by their computational load. On the other hand, given a trained model and a task, e.g. faces generation within a range of characteristics, the output image quality will be unevenly distributed among images with different characteristics. It follows, that we might restrain the models complexity on some instances, maintaining a high quality. We propose a method for diminishing computations by adding so-called early exit branches to the original architecture, and dynamically switching the computational path depending on how difficult it will be to render the output. We apply our method on two different SOTA models performing generative tasks: generation from a semantic map, and cross-reenactment of face expressions; showing it is able to output images with custom lower-quality thresholds. For a threshold of LPIPS &lt;=0.1, we diminish their computations by up to a half. This is especially relevant for real-time app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;SARF&#65292;&#36890;&#36807;&#21033;&#29992;&#21516;&#20041;&#20851;&#31995;&#36741;&#21161;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#20851;&#31995;&#25512;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#65292;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#36229;&#36234;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10297</link><description>&lt;p&gt;
SARF: &#21033;&#29992;&#21516;&#20041;&#20851;&#31995;&#36741;&#21161;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning. (arXiv:2304.10297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;SARF&#65292;&#36890;&#36807;&#21033;&#29992;&#21516;&#20041;&#20851;&#31995;&#36741;&#21161;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#20851;&#31995;&#25512;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#65292;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#36229;&#36234;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#38024;&#23545;&#23569;&#37327;&#25968;&#25454;&#12289;&#38271;&#23614;&#30340;&#20851;&#31995;&#25512;&#29702;&#65288;FS-KGR&#65289;&#36817;&#24180;&#26469;&#22240;&#20854;&#23454;&#29992;&#24615;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#26500;&#24314;&#20803;&#20851;&#31995;&#38598;&#26469;&#39044;&#35757;&#32451;&#65292;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#21171;&#21160;&#25104;&#26412;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#34987;&#35270;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#20294;&#22312;FS-KGR&#20219;&#21153;&#20013;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#21033;&#29992;&#19982;&#30446;&#26631;&#25968;&#25454;&#31232;&#23569;&#20851;&#31995;&#20855;&#26377;&#31867;&#20284;&#19978;&#19979;&#25991;&#35821;&#20041;&#30340;&#21516;&#20041;&#20851;&#31995;&#65288;AR&#65289;&#30340;&#26377;&#30410;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21516;&#20041;&#20851;&#31995;&#36741;&#21161;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;SARF&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#35745;&#20102;&#22235;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;SSL&#25512;&#29702;&#27169;&#22359;&#12289;AR&#36741;&#21161;&#26426;&#21046;&#12289;&#34701;&#21512;&#27169;&#22359;&#21644;&#35780;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#39318;&#20808;&#20197;&#29983;&#25104;&#24335;&#30340;&#26041;&#24335;&#29983;&#25104;&#20849;&#29616;&#27169;&#24335;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#35774;&#35745;AR&#36741;&#21161;&#26426;&#21046;&#26469;&#25429;&#25417;&#25968;&#25454;&#31232;&#23569;&#20851;&#31995;&#30340;&#30456;&#20284;&#19978;&#19979;&#25991;&#35821;&#20041;&#12290;&#28982;&#21518;&#65292;&#36825;&#20004;&#20010;&#34920;&#31034;&#34987;&#34701;&#21512;&#36215;&#26469;&#29983;&#25104;&#32508;&#21512;&#29305;&#24449;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26469;&#39044;&#27979;&#22522;&#20110;&#27492;&#29305;&#24449;&#34920;&#31034;&#30340;&#30446;&#26631;&#20851;&#31995;&#12290;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SARF&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot relation reasoning on knowledge graphs (FS-KGR) aims to infer long-tail data-poor relations, which has drawn increasing attention these years due to its practicalities. The pre-training of previous methods needs to manually construct the meta-relation set, leading to numerous labor costs. Self-supervised learning (SSL) is treated as a solution to tackle the issue, but still at an early stage for FS-KGR task. Moreover, most of the existing methods ignore leveraging the beneficial information from aliasing relations (AR), i.e., data-rich relations with similar contextual semantics to the target data-poor relation. Therefore, we proposed a novel Self-Supervised Learning model by leveraging Aliasing Relations to assist FS-KGR, termed SARF. Concretely, four main components are designed in our model, i.e., SSL reasoning module, AR-assisted mechanism, fusion module, and scoring function. We first generate the representation of the co-occurrence patterns in a generative manner. Meanwh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20197;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#37117;&#20114;&#30456;&#20851;&#32852;&#30340;&#22240;&#32032;&#26469;&#34920;&#24449;&#19968;&#20010;&#23454;&#20363;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#20272;&#35745;&#20998;&#31867;&#38169;&#35823;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.10284</link><description>&lt;p&gt;
&#19968;&#31181;&#20272;&#31639;&#24182;&#35299;&#37322;&#20998;&#31867;&#22120;&#19981;&#30830;&#23450;&#24615;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Meta-heuristic Approach to Estimate and Explain Classifier Uncertainty. (arXiv:2304.10284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20197;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#37117;&#20114;&#30456;&#20851;&#32852;&#30340;&#22240;&#32032;&#26469;&#34920;&#24449;&#19968;&#20010;&#23454;&#20363;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#20272;&#35745;&#20998;&#31867;&#38169;&#35823;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#26159;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37319;&#29992;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#23450;&#24615;&#30740;&#31350;&#34920;&#26126;&#65292;&#32456;&#31471;&#29992;&#25143;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#38656;&#35201;&#33021;&#22815;&#22312;&#20915;&#31574;&#26102;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#27169;&#22411;&#65292;&#20197;&#20351;&#29992;&#25143;&#30693;&#36947;&#20309;&#26102;&#24573;&#30053;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37327;&#21270;&#20915;&#31574;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#19981;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#23601;&#26159;&#20381;&#36182;&#20110;&#19981;&#23481;&#26131;&#35753;&#26222;&#36890;&#20154;&#25110;&#32456;&#31471;&#29992;&#25143;&#29702;&#35299;&#30340;&#22797;&#26434;&#32479;&#35745;&#25512;&#23548;&#65292;&#36825;&#20351;&#23427;&#20204;&#22312;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#26102;&#19981;&#22826;&#26377;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#31867;&#29420;&#31435;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#37117;&#20114;&#30456;&#20851;&#32852;&#30340;&#22240;&#32032;&#26469;&#34920;&#24449;&#19968;&#20010;&#23454;&#20363;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123;&#24230;&#37327;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#35813;&#26694;&#26550;&#20272;&#35745;&#20102;&#20998;&#31867;&#38169;&#35823;&#39118;&#38505;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#37492;&#21035;&#37027;&#20123;&#26377;&#21487;&#33021;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#23454;&#20363;&#26041;&#38754;&#65292;&#34920;&#29616;&#20248;&#20110;&#39044;&#27979;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust is a crucial factor affecting the adoption of machine learning (ML) models. Qualitative studies have revealed that end-users, particularly in the medical domain, need models that can express their uncertainty in decision-making allowing users to know when to ignore the model's recommendations. However, existing approaches for quantifying decision-making uncertainty are not model-agnostic, or they rely on complex statistical derivations that are not easily understood by laypersons or end-users, making them less useful for explaining the model's decision-making process. This work proposes a set of class-independent meta-heuristics that can characterize the complexity of an instance in terms of factors are mutually relevant to both human and ML decision-making. The measures are integrated into a meta-learning framework that estimates the risk of misclassification. The proposed framework outperformed predicted probabilities in identifying instances at risk of being misclassified. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38480;&#30340;&#12289;&#26080;&#32447;&#30340;&#20803;&#23431;&#23449;&#35270;&#37326;&#65292;&#23558;&#20803;&#23431;&#23449;&#27987;&#32553;&#20026;&#19971;&#20010;&#19990;&#30028;&#21644;&#20307;&#39564;&#30340;&#20132;&#21449;&#28857;&#65292;&#25903;&#25345;&#20154;&#31867;&#21644;&#34394;&#25311;&#21270;&#35282;&#33394;&#20043;&#38388;&#20197;&#21450;&#36830;&#25509;&#30340;&#26234;&#33021;&#31995;&#32479;&#21644;&#20854;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#26032;&#39062;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.10282</link><description>&lt;p&gt;
&#26080;&#32447;&#20803;&#23431;&#23449;&#30340;&#19971;&#20010;&#19990;&#30028;&#21644;&#20307;&#39564;&#65306;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
The Seven Worlds and Experiences of the Wireless Metaverse: Challenges and Opportunities. (arXiv:2304.10282v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38480;&#30340;&#12289;&#26080;&#32447;&#30340;&#20803;&#23431;&#23449;&#35270;&#37326;&#65292;&#23558;&#20803;&#23431;&#23449;&#27987;&#32553;&#20026;&#19971;&#20010;&#19990;&#30028;&#21644;&#20307;&#39564;&#30340;&#20132;&#21449;&#28857;&#65292;&#25903;&#25345;&#20154;&#31867;&#21644;&#34394;&#25311;&#21270;&#35282;&#33394;&#20043;&#38388;&#20197;&#21450;&#36830;&#25509;&#30340;&#26234;&#33021;&#31995;&#32479;&#21644;&#20854;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#26032;&#39062;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#20803;&#23431;&#23449;&#23558;&#20250;&#22312;&#23454;&#20307;&#12289;&#25968;&#23383;&#21644;&#34394;&#25311;&#19990;&#30028;&#30340;&#20132;&#21449;&#28857;&#21019;&#36896;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#36825;&#20123;&#20307;&#39564;&#23558;&#20250;&#20419;&#36827;&#36825;&#19977;&#20010;&#19990;&#30028;&#30340;&#26500;&#25104;&#35201;&#32032;&#65288;&#22914;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#29992;&#25143;&#21644;&#34394;&#25311;&#21270;&#35282;&#33394;&#65289;&#20043;&#38388;&#30340;&#26032;&#39062;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#30633;&#30446;&#30340;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#27809;&#26377;&#19968;&#20010;&#20840;&#38754;&#30340;&#24895;&#26223;&#33021;&#22815;&#30830;&#23450;&#20803;&#23431;&#23449;&#30340;&#20840;&#37096;&#19990;&#30028;&#12289;&#26500;&#25104;&#35201;&#32032;&#21644;&#20307;&#39564;&#30340;&#38598;&#21512;&#65292;&#20197;&#21450;&#23427;&#20204;&#30456;&#20851;&#20114;&#21160;&#23545;&#26410;&#26469;&#36890;&#20449;&#21644;&#35745;&#31639;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#23613;&#30340;&#12289;&#26080;&#32447;&#30340;&#12289;&#32508;&#21512;&#30340;&#20803;&#23431;&#23449;&#35270;&#37326;&#65292;&#23558;&#20803;&#23431;&#23449;&#27987;&#32553;&#20026;&#19971;&#20010;&#19990;&#30028;&#21644;&#20307;&#39564;&#30340;&#20132;&#21449;&#28857;&#65292;&#21253;&#25324;&#65306;i&#65289;&#23454;&#20307;&#12289;&#25968;&#23383;&#21644;&#34394;&#25311;&#19990;&#30028;&#65292;&#20197;&#21450;ii&#65289;&#32593;&#32476;&#12289;&#25193;&#23637;&#12289;&#23454;&#26102;&#21644;&#24179;&#34892;&#30340;&#20307;&#39564;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#36825;&#20123;&#32463;&#39564;&#22914;&#20309;&#21576;&#29616;&#19981;&#21516;&#30340;&#20803;&#23431;&#23449;&#26500;&#25104;&#35201;&#32032;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21363;a&#65289;&#20154;&#31867;&#21644;&#34394;&#25311;&#21270;&#35282;&#33394;&#65292;&#21644;b&#65289;&#36830;&#25509;&#30340;&#26234;&#33021;&#31995;&#32479;&#21450;&#20854;&#25968;&#23383;&#23402;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wireless metaverse will create diverse user experiences at the intersection of the physical, digital, and virtual worlds. These experiences will enable novel interactions between the constituents (e.g., extended reality (XR) users and avatars) of the three worlds. However, remarkably, to date, there is no holistic vision that identifies the full set of metaverse worlds, constituents, and experiences, and the implications of their associated interactions on next-generation communication and computing systems. In this paper, we present a holistic vision of a limitless, wireless metaverse that distills the metaverse into an intersection of seven worlds and experiences that include the: i) physical, digital, and virtual worlds, along with the ii) cyber, extended, live, and parallel experiences. We then articulate how these experiences bring forth interactions between diverse metaverse constituents, namely, a) humans and avatars and b) connected intelligence systems and their digital tw
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#19968;&#33268;&#24615;&#29983;&#25104;&#23545;&#25239;&#26041;&#27861;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#36712;&#36857;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;DATI&#65292;&#21487;&#20197;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#20195;&#34920;&#24615;&#36712;&#36857;&#65292;&#35782;&#21035;&#20132;&#36890;&#20013;&#24322;&#24120;&#36816;&#21160;&#27169;&#24335;&#65292;&#24182;&#22312;&#21508;&#31181;&#21512;&#25104;&#30340;&#21442;&#32771;&#36712;&#36857;&#23478;&#26063;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.10260</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#20223;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#20195;&#34920;&#24615;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Learning Representative Trajectories of Dynamical Systems via Domain-Adaptive Imitation. (arXiv:2304.10260v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#19968;&#33268;&#24615;&#29983;&#25104;&#23545;&#25239;&#26041;&#27861;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#36712;&#36857;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;DATI&#65292;&#21487;&#20197;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#20195;&#34920;&#24615;&#36712;&#36857;&#65292;&#35782;&#21035;&#20132;&#36890;&#20013;&#24322;&#24120;&#36816;&#21160;&#27169;&#24335;&#65292;&#24182;&#22312;&#21508;&#31181;&#21512;&#25104;&#30340;&#21442;&#32771;&#36712;&#36857;&#23478;&#26063;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#30340;&#36712;&#36857;&#27169;&#20223;&#26159;&#19968;&#31181;&#26576;&#20123;&#25429;&#39135;&#21160;&#29289;&#20026;&#29983;&#23384;&#23398;&#20064;&#30340;&#25216;&#33021;&#65292;&#36890;&#36807;&#23558;&#21160;&#24577;&#20449;&#24687;&#20174;&#19968;&#20010;&#22495;&#65288;&#23427;&#20204;&#30340;&#36895;&#24230;&#21644;&#36716;&#21521;&#26041;&#21521;&#65289;&#26144;&#23556;&#21040;&#19981;&#21516;&#30340;&#22495;&#65288;&#31227;&#21160;&#29454;&#29289;&#30340;&#24403;&#21069;&#20301;&#32622;&#65289;&#12290;&#20855;&#26377;&#27492;&#25216;&#33021;&#30340;&#26234;&#33021;&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#23398;&#20064;&#27169;&#20223;&#20195;&#34920;&#24615;&#36712;&#36857;&#21518;&#35782;&#21035;&#20132;&#36890;&#20013;&#30340;&#24322;&#24120;&#36816;&#21160;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;DATI&#65292;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#19968;&#33268;&#24615;&#29983;&#25104;&#23545;&#25239;&#26041;&#27861;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#36712;&#36857;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21512;&#25104;&#30340;&#21442;&#32771;&#36712;&#36857;&#23478;&#26063;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DATI&#22312;&#27169;&#20223;&#23398;&#20064;&#21644;&#26368;&#20248;&#25511;&#21046;&#20013;&#30340;&#22522;&#20934;&#26041;&#27861;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#20445;&#25345;&#30456;&#21516;&#30340;&#27599;&#19968;&#39033;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#12290;&#36890;&#36807;&#21457;&#29616;&#28023;&#19978;&#20132;&#36890;&#30340;&#24322;&#24120;&#36816;&#21160;&#27169;&#24335;&#65292;&#23427;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#24050;&#24471;&#21040;&#35777;&#26126;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#22312;&#39046;&#22495;&#36866;&#24212;&#36712;&#36857;&#27169;&#20223;&#20013;&#30340;&#24212;&#29992;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-adaptive trajectory imitation is a skill that some predators learn for survival, by mapping dynamic information from one domain (their speed and steering direction) to a different domain (current position of the moving prey). An intelligent agent with this skill could be exploited for a diversity of tasks, including the recognition of abnormal motion in traffic once it has learned to imitate representative trajectories. Towards this direction, we propose DATI, a deep reinforcement learning agent designed for domain-adaptive trajectory imitation using a cycle-consistent generative adversarial method. Our experiments on a variety of synthetic families of reference trajectories show that DATI outperforms baseline methods for imitation learning and optimal control in this setting, keeping the same per-task hyperparameters. Its generalization to a real-world scenario is shown through the discovery of abnormal motion patterns in maritime traffic, opening the door for the use of deep r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;3D&#28857;&#20113;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#32593;&#32476;MvNet&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#29616;&#26377;&#30340;2D&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#32531;&#35299;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#23545;&#22823;&#35268;&#27169;&#27880;&#37322;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10224</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#35270;&#35273;&#25552;&#31034;&#34701;&#21512;&#32593;&#32476;&#65306;2D&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#21542;&#22686;&#24378;3D&#28857;&#20113;&#25968;&#25454;&#31232;&#32570;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Multi-view Vision-Prompt Fusion Network: Can 2D Pre-trained Model Boost 3D Point Cloud Data-scarce Learning?. (arXiv:2304.10224v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;3D&#28857;&#20113;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#32593;&#32476;MvNet&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#29616;&#26377;&#30340;2D&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#32531;&#35299;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#23545;&#22823;&#35268;&#27169;&#27880;&#37322;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#30340;3D&#28145;&#24230;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#23478;&#24237;&#26426;&#22120;&#20154;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#35270;&#22270;&#35270;&#35273;&#25552;&#31034;&#34701;&#21512;&#32593;&#32476;&#65288;MvNet&#65289;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;3D&#28857;&#20113;&#20998;&#31867;&#65292;&#28789;&#24863;&#28304;&#33258;&#20110;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25552;&#31034;&#24615;&#23398;&#20064;&#12290;MvNet &#25506;&#35752;&#20102;&#21033;&#29992;&#29616;&#26377;2D&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#23569;&#26679;&#26412;&#20998;&#31867;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#21487;&#20197;&#32531;&#35299;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#23545;&#22823;&#35268;&#27169;&#27880;&#37322;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MvNet&#39318;&#20808;&#23558;3D&#28857;&#20113;&#32534;&#30721;&#25104;&#22810;&#35270;&#22270;&#22270;&#20687;&#29305;&#24449;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#22270;&#25552;&#31034;&#34701;&#21512;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#20449;&#24687;&#65292;&#20197;&#24357;&#21512;3D&#28857;&#20113;&#25968;&#25454;&#21644;2D&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#21518;&#21487;&#20197;&#27966;&#29983;&#19968;&#32452;2D&#22270;&#20687;&#25552;&#31034;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#36866;&#24403;&#30340;&#20808;&#39564;&#30693;&#35782;&#20197;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point cloud based 3D deep model has wide applications in many applications such as autonomous driving, house robot, and so on. Inspired by the recent prompt learning in natural language processing, this work proposes a novel Multi-view Vision-Prompt Fusion Network (MvNet) for few-shot 3D point cloud classification. MvNet investigates the possibility of leveraging the off-the-shelf 2D pre-trained models to achieve the few-shot classification, which can alleviate the over-dependence issue of the existing baseline models towards the large-scale annotated 3D point cloud data. Specifically, MvNet first encodes a 3D point cloud into multi-view image features for a number of different views. Then, a novel multi-view prompt fusion module is developed to effectively fuse information from different views to bridge the gap between 3D point cloud data and 2D pre-trained models. A set of 2D image prompts can then be derived to better describe the suitable prior knowledge for a large-scale pre-train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#27493;&#26426;&#21046;&#23454;&#29616;MC-dropout&#30340;&#39640;&#25928;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20026;&#31070;&#32463;&#24418;&#24577;&#23398;&#30828;&#20214;&#30340;&#33410;&#33021;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.10191</link><description>&lt;p&gt;
&#22522;&#20110;MC-dropout&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Uncertainty Estimation in Spiking Neural Networks via MC-dropout. (arXiv:2304.10191v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#27493;&#26426;&#21046;&#23454;&#29616;MC-dropout&#30340;&#39640;&#25928;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20026;&#31070;&#32463;&#24418;&#24577;&#23398;&#30828;&#20214;&#30340;&#33410;&#33021;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNN)&#20316;&#20026;&#29983;&#29289;&#31070;&#32463;&#20803;&#31232;&#30095;&#21644;&#20107;&#20214;&#39537;&#21160;&#36890;&#20449;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#24050;&#24341;&#36215;&#20851;&#27880;&#65292;&#24182;&#22240;&#27492;&#22312;&#31070;&#32463;&#24418;&#24577;&#23398;&#30828;&#20214;&#20013;&#26174;&#31034;&#20986;&#36234;&#26469;&#36234;&#22810;&#30340;&#33410;&#33021;&#24212;&#29992;&#21069;&#26223;&#12290;&#19982;&#32463;&#20856;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#19968;&#26679;&#65292;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12289;&#21307;&#23398;&#35786;&#26029;&#21644;&#39640;&#39057;&#20132;&#26131;&#65289;&#30340;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;SNNs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35752;&#35770;&#24456;&#23569;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;SNNs&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MC-dropout&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;SNNs&#30340;&#26102;&#38388;&#27493;&#26426;&#21046;&#65292;&#20197;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#24335;&#23454;&#29616;MC-dropout&#65292;&#19981;&#20250;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#24341;&#20837;&#37325;&#22823;&#24320;&#38144;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#39640;&#20934;&#30830;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have gained attention as models of sparse and event-driven communication of biological neurons, and as such have shown increasing promise for energy-efficient applications in neuromorphic hardware. As with classical artificial neural networks (ANNs), predictive uncertainties are important for decision making in high-stakes applications, such as autonomous vehicles, medical diagnosis, and high frequency trading. Yet, discussion of uncertainty estimation in SNNs is limited, and approaches for uncertainty estimation in artificial neural networks (ANNs) are not directly applicable to SNNs. Here, we propose an efficient Monte Carlo(MC)-dropout based approach for uncertainty estimation in SNNs. Our approach exploits the time-step mechanism of SNNs to enable MC-dropout in a computationally efficient manner, without introducing significant overheads during training and inference while demonstrating high accuracy and uncertainty quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25506;&#32034;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#22312;&#24314;&#31569;&#35774;&#35745;&#26089;&#26399;&#38454;&#27573;&#25903;&#25345;&#21019;&#36896;&#21147;&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#22120;&#25903;&#25345;&#24847;&#22806;&#30340;&#21019;&#24847;&#21457;&#29616;&#21644;&#23500;&#26377;&#24819;&#35937;&#21147;&#30340;&#24515;&#24577;&#65292;&#20016;&#23500;&#20102;&#35774;&#35745;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.10182</link><description>&lt;p&gt;
&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#36827;&#34892;&#24314;&#31569;&#35774;&#35745;&#26500;&#24605;
&lt;/p&gt;
&lt;p&gt;
Using Text-to-Image Generation for Architectural Design Ideation. (arXiv:2304.10182v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25506;&#32034;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#22312;&#24314;&#31569;&#35774;&#35745;&#26089;&#26399;&#38454;&#27573;&#25903;&#25345;&#21019;&#36896;&#21147;&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#22120;&#25903;&#25345;&#24847;&#22806;&#30340;&#21019;&#24847;&#21457;&#29616;&#21644;&#23500;&#26377;&#24819;&#35937;&#21147;&#30340;&#24515;&#24577;&#65292;&#20016;&#23500;&#20102;&#35774;&#35745;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24471;&#21040;&#20102;&#35748;&#21487;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#27425;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#22312;&#24314;&#31569;&#35774;&#35745;&#26089;&#26399;&#38454;&#27573;&#25903;&#25345;&#21019;&#36896;&#21147;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#19982;17&#21517;&#24314;&#31569;&#23398;&#29983;&#36827;&#34892;&#20102;&#23454;&#39564;&#23460;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#8212;&#8212;Midjourney&#12289;&#31283;&#23450;&#25193;&#25955;&#21644;DALL-E&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#21270;&#20013;&#24515;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#26631;&#20934;&#38382;&#21367;&#21644;&#23567;&#32452;&#35775;&#35848;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#20180;&#32454;&#32771;&#34385;&#35774;&#35745;&#32422;&#26463;&#26102;&#65292;&#22270;&#20687;&#29983;&#25104;&#21487;&#20197;&#25104;&#20026;&#35774;&#35745;&#36807;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#29983;&#25104;&#24037;&#20855;&#25903;&#25345;&#24847;&#22806;&#30340;&#21019;&#24847;&#21457;&#29616;&#21644;&#23500;&#26377;&#24819;&#35937;&#21147;&#30340;&#24515;&#24577;&#65292;&#20016;&#23500;&#20102;&#35774;&#35745;&#36807;&#31243;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#20687;&#29983;&#25104;&#22120;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#36719;&#20214;&#24320;&#21457;&#21644;&#25945;&#32946;&#24037;&#20316;&#32773;&#30340;&#32771;&#34385;&#65292;&#20197;&#25903;&#25345;&#21019;&#36896;&#21147;&#24182;&#24378;&#35843;&#35774;&#35745;&#24072;&#30340;&#24819;&#35937;&#21147;&#12290;&#36890;&#36807;&#20102;&#35299;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#38480;&#21046;&#21644;&#28508;&#21147;&#65292;&#24314;&#31569;&#24072;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#39033;&#25216;&#26415;&#26469;&#20419;&#36827;&#21019;&#36896;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress of text-to-image generation has been recognized in architectural design. Our study is the first to investigate the potential of text-to-image generators in supporting creativity during the early stages of the architectural design process. We conducted a laboratory study with 17 architecture students, who developed a concept for a culture center using three popular text-to-image generators: Midjourney, Stable Diffusion, and DALL-E. Through standardized questionnaires and group interviews, we found that image generation could be a meaningful part of the design process when design constraints are carefully considered. Generative tools support serendipitous discovery of ideas and an imaginative mindset, enriching the design process. We identified several challenges of image generators and provided considerations for software development and educators to support creativity and emphasize designers' imaginative mindset. By understanding the limitations and potential of tex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26435;&#37325;&#38170;&#23450;&#26469;&#23454;&#29616;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#36991;&#20813;&#22312;&#20248;&#21270;&#29615;&#22659;&#19979;&#24573;&#30053;&#25110;&#24536;&#35760;&#26399;&#26395;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10176</link><description>&lt;p&gt;
&#36890;&#36807;&#26435;&#37325;&#38170;&#23450;&#23454;&#29616;&#40065;&#26834;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Robust Deep Reinforcement Learning Scheduling via Weight Anchoring. (arXiv:2304.10176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26435;&#37325;&#38170;&#23450;&#26469;&#23454;&#29616;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#36991;&#20813;&#22312;&#20248;&#21270;&#29615;&#22659;&#19979;&#24573;&#30053;&#25110;&#24536;&#35760;&#26399;&#26395;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23398;&#20064;&#26041;&#27861;&#20174;&#20223;&#30495;&#21040;&#29616;&#23454;&#20013;&#36328;&#36234;&#40511;&#27807;&#26102;&#65292;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26435;&#37325;&#38170;&#23450;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#24050;&#30693;&#65292;&#24182;&#29992;&#20110;&#22521;&#20859;&#21644;&#22266;&#23450;&#31070;&#32463;&#32593;&#32476;&#20013;&#26399;&#26395;&#30340;&#34892;&#20026;&#12290;&#21487;&#20197;&#20351;&#29992;&#26435;&#37325;&#38170;&#23450;&#26041;&#27861;&#25214;&#21040;&#19968;&#20010;&#19982;&#21478;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#25509;&#36817;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#12290;&#36825;&#26679;&#65292;&#22312;&#20248;&#21270;&#29615;&#22659;&#19979;&#36827;&#34892;&#23398;&#20064;&#26102;&#65292;&#19981;&#20250;&#24573;&#30053;&#25110;&#24536;&#35760;&#26399;&#26395;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#30340;QoS&#39640;&#25928;&#31163;&#25955;&#36164;&#28304;&#35843;&#24230;&#19982;&#19981;&#39057;&#32321;&#30340;&#20248;&#20808;&#28040;&#24687;&#30340;&#31034;&#20363;&#26469;&#28436;&#31034;&#27492;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19982;&#22686;&#21152;&#20223;&#30495;&#29615;&#22659;&#30340;&#29616;&#26377;&#25216;&#26415;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Questions remain on the robustness of data-driven learning methods when crossing the gap from simulation to reality. We utilize weight anchoring, a method known from continual learning, to cultivate and fixate desired behavior in Neural Networks. Weight anchoring may be used to find a solution to a learning problem that is nearby the solution of another learning problem. Thereby, learning can be carried out in optimal environments without neglecting or unlearning desired behavior. We demonstrate this approach on the example of learning mixed QoS-efficient discrete resource scheduling with infrequent priority messages. Results show that this method provides performance comparable to the state of the art of augmenting a simulation environment, alongside significantly increased robustness and steerability.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#26368;&#32456;&#24635;&#32467;&#20102;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.10159</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Using Hybrid Quantum Neural Network. (arXiv:2304.10159v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10159
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#26368;&#32456;&#24635;&#32467;&#20102;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#23545;&#20110;&#20419;&#36827;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#26356;&#39640;&#25968;&#25454;&#32500;&#24230;&#25110;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24635;&#20307;&#35757;&#32451;&#21442;&#25968;&#30340;&#38480;&#21046;&#20855;&#26377;&#24378;&#28872;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230; Q-Learning &#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#24182;&#22521;&#35757;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#26032;&#30340; Qiskit &#21644; PyTorch &#26694;&#26550;&#30340;&#26032;&#22411; PQC&#65292;&#20197;&#19982;&#23436;&#20840;&#32463;&#20856;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#24102;&#25110;&#19981;&#24102;&#38598;&#25104; PQC&#12290;&#30740;&#31350;&#26368;&#21518;&#24635;&#32467;&#20102;&#20854;&#20851;&#20110;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#35299;&#20915;&#36855;&#23467;&#38382;&#39064;&#25110;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computation has a strong implication for advancing the current limitation of machine learning algorithms to deal with higher data dimensions or reducing the overall training parameters for a deep neural network model. Based on a gate-based quantum computer, a parameterized quantum circuit was designed to solve a model-free reinforcement learning problem with the deep-Q learning method. This research has investigated and evaluated its potential. Therefore, a novel PQC based on the latest Qiskit and PyTorch framework was designed and trained to compare with a full-classical deep neural network with and without integrated PQC. At the end of the research, the research draws its conclusion and prospects on developing deep quantum learning in solving a maze problem or other reinforcement learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.10145</link><description>&lt;p&gt;
ChatGPT&#33021;&#21542;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#65311;&#23545;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks. (arXiv:2304.10145v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#21457;&#24067;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21462;&#20195;&#20154;&#31867;&#26234;&#24935;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;ChatGPT&#26159;&#21542;&#26377;&#28508;&#21147;&#22312;&#31038;&#20132;&#35745;&#31639;&#20219;&#21153;&#20013;&#22797;&#21046;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#27880;&#37322;&#12290;&#36825;&#26679;&#30340;&#25104;&#23601;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#31038;&#20132;&#35745;&#31639;&#30740;&#31350;&#30340;&#25104;&#26412;&#21644;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#37325;&#26032;&#26631;&#35760;&#20102;&#20116;&#20010;&#20855;&#26377;&#37324;&#31243;&#30865;&#24847;&#20041;&#30340;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#31435;&#22330;&#26816;&#27979;&#65288;2&#20010;&#65289;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#26426;&#22120;&#20154;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#26377;&#28508;&#21147;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#65292;&#23613;&#31649;&#20173;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;ChatGPT&#33719;&#24471;&#20102;&#24179;&#22343;&#31934;&#24230;0.609&#12290; ChatGPT&#23545;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#26368;&#20339;&#65292;&#27491;&#30830;&#27880;&#37322;&#20102;64.9&#65285;&#30340;&#25512;&#25991;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26174;&#31034;&#24615;&#33021;&#22312;&#19981;&#21516;&#26631;&#31614;&#20043;&#38388;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#39033;&#24037;&#20316;&#21487;&#20197;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#32447;&#36335;&#65292;&#24182;&#20316;&#20026;&#26410;&#26469;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT has uncovered a range of possibilities whereby large language models (LLMs) can substitute human intelligence. In this paper, we seek to understand whether ChatGPT has the potential to reproduce human-generated label annotations in social computing tasks. Such an achievement could significantly reduce the cost and complexity of social computing research. As such, we use ChatGPT to re-label five seminal datasets covering stance detection (2x), sentiment analysis, hate speech, and bot detection. Our results highlight that ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain. ChatGPT obtains an average precision 0.609. Performance is highest for the sentiment analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we show that performance varies substantially across individual labels. We believe this work can open up new lines of analysis and act as a basis for future research into the exploi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#12290;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;&#21069;&#21521;&#35757;&#32451;&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#26469;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#65292;&#20174;&#32780;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2304.10126</link><description>&lt;p&gt;
&#35299;&#32806;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#31616;&#21333;&#30340;GNN&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decouple Graph Neural Networks: Train Multiple Simple GNNs Simultaneously Instead of One. (arXiv:2304.10126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#12290;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;&#21069;&#21521;&#35757;&#32451;&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#26469;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#65292;&#20174;&#32780;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23384;&#22312;&#20005;&#37325;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#33410;&#28857;&#20381;&#36182;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#36825;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#20351;&#24471;GNN&#30340;&#35757;&#32451;&#36890;&#24120;&#24456;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#22810;&#23618;GNN&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#30001;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#65288;FT&#65289;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#65288;BT&#65289;&#32452;&#25104;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19979;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;FT&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#19981;&#20250;&#25197;&#26354;&#22270;&#24418;&#20449;&#24687;&#12290;&#20026;&#36991;&#20813;FT&#30340;&#21482;&#21333;&#21521;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#65292;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#12290;&#36825;&#31181;&#21453;&#21521;&#35757;&#32451;&#24341;&#20837;&#20102;&#21453;&#21521;&#20449;&#24687;&#20256;&#36882;&#21040;&#35299;&#32806;&#27169;&#22359;&#20013;&#65292;&#21516;&#26102;&#20063;&#20250;&#26377;&#21069;&#21521;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNN) suffer from severe inefficiency. It is mainly caused by the exponential growth of node dependency with the increase of layers. It extremely limits the application of stochastic optimization algorithms so that the training of GNN is usually time-consuming. To address this problem, we propose to decouple a multi-layer GNN as multiple simple modules for more efficient training, which is comprised of classical forward training (FT)and designed backward training (BT). Under the proposed framework, each module can be trained efficiently in FT by stochastic algorithms without distortion of graph information owing to its simplicity. To avoid the only unidirectional information delivery of FT and sufficiently train shallow modules with the deeper ones, we develop a backward training mechanism that makes the former modules perceive the latter modules. The backward training introduces the reversed information delivery into the decoupled modules as well as the forward i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#36827;&#21270;&#35757;&#32451;&#65288;AET&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#35843;&#25972;&#65288;ADA&#65289;&#21644;&#29615;&#22659;&#38543;&#26426;&#21270;&#65288;ER&#65289;&#20248;&#21270;AET&#36807;&#31243;&#65292;&#20351;&#24471;AI&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;AMP&#28216;&#25103;&#20013;&#20987;&#36133;&#39030;&#32423;&#20154;&#31867;&#29609;&#23478;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#20154;&#31867;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.10124</link><description>&lt;p&gt;
&#29992;&#22810;&#26234;&#33021;&#20307;&#38750;&#23545;&#31216;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#25484;&#25569;&#38750;&#23545;&#31216;&#22810;&#20154;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Mastering Asymmetrical Multiplayer Game with Multi-Agent Asymmetric-Evolution Reinforcement Learning. (arXiv:2304.10124v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10124
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#36827;&#21270;&#35757;&#32451;&#65288;AET&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#35843;&#25972;&#65288;ADA&#65289;&#21644;&#29615;&#22659;&#38543;&#26426;&#21270;&#65288;ER&#65289;&#20248;&#21270;AET&#36807;&#31243;&#65292;&#20351;&#24471;AI&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;AMP&#28216;&#25103;&#20013;&#20987;&#36133;&#39030;&#32423;&#20154;&#31867;&#29609;&#23478;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#20154;&#31867;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#23545;&#31216;&#22810;&#20154;&#28216;&#25103;&#65288;AMP&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28216;&#25103;&#31867;&#22411;&#65292;&#28041;&#21450;&#22810;&#31181;&#31867;&#22411;&#30340;&#20195;&#29702;&#22312;&#28216;&#25103;&#20013;&#30456;&#20114;&#31454;&#20105;&#25110;&#21512;&#20316;&#12290;&#30001;&#20110;&#38750;&#23545;&#31216;&#29615;&#22659;&#20013;&#26377;&#19981;&#24179;&#34913;&#30340;&#29305;&#24615;&#65292;&#22240;&#27492;&#20351;&#29992;&#20856;&#22411;&#30340;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#26041;&#27861;&#35757;&#32451;&#24378;&#22823;&#30340;&#20195;&#29702;&#20197;&#20987;&#36133;&#39030;&#32423;&#20154;&#31867;&#29609;&#23478;&#22312;AMP&#28216;&#25103;&#20013;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#23545;&#31216;&#36827;&#21270;&#35757;&#32451;&#65288;AET&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35757;&#32451;&#22810;&#31181;&#20195;&#29702;&#22312;AMP&#28216;&#25103;&#20013;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#25968;&#25454;&#35843;&#25972;&#65288;ADA&#65289;&#21644;&#29615;&#22659;&#38543;&#26426;&#21270;&#65288;ER&#65289;&#26469;&#20248;&#21270;AET&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;Tom&#65286;Jerry&#30340;&#22797;&#26434;AMP&#28216;&#25103;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;&#30340;AI&#23545;65&#20010;&#27604;&#36187;&#21462;&#24471;&#20102;98.5&#65285;&#30340;&#32988;&#29575;&#65292;&#32780;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#20154;&#31867;&#25968;&#25454;&#12290;&#28040;&#34701;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22359;&#26377;&#30410;&#20110;&#35813;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asymmetrical multiplayer (AMP) game is a popular game genre which involves multiple types of agents competing or collaborating with each other in the game. It is difficult to train powerful agents that can defeat top human players in AMP games by typical self-play training method because of unbalancing characteristics in their asymmetrical environments. We propose asymmetric-evolution training (AET), a novel multi-agent reinforcement learning framework that can train multiple kinds of agents simultaneously in AMP game. We designed adaptive data adjustment (ADA) and environment randomization (ER) to optimize the AET process. We tested our method in a complex AMP game named Tom \&amp; Jerry, and our AIs trained without using any human data can achieve a win rate of 98.5% against top human players over 65 matches. The ablation experiments indicated that the proposed modules are beneficial to the framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37327;&#23376;&#28216;&#36208;&#30340;&#31639;&#27861;&#35299;&#20915;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#23558;&#25506;&#32034;&#21644;&#21033;&#29992;&#19982;&#37327;&#23376;&#28216;&#36208;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#65292;&#30456;&#27604;&#20110;&#32463;&#20856;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10118</link><description>&lt;p&gt;
&#19968;&#20010;&#30001;&#32463;&#20856;&#38543;&#26426;&#28216;&#36208;&#21644;&#37327;&#23376;&#28216;&#36208;&#39537;&#21160;&#30340;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bandit Algorithm Driven by a Classical Random Walk and a Quantum Walk. (arXiv:2304.10118v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37327;&#23376;&#28216;&#36208;&#30340;&#31639;&#27861;&#35299;&#20915;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#23558;&#25506;&#32034;&#21644;&#21033;&#29992;&#19982;&#37327;&#23376;&#28216;&#36208;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#65292;&#30456;&#27604;&#20110;&#32463;&#20856;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#28216;&#36208;&#20855;&#26377;&#32463;&#20856;&#38543;&#26426;&#28216;&#36208;&#25152;&#19981;&#20855;&#22791;&#30340;&#23646;&#24615;&#8212;&#8212;&#32447;&#24615;&#20256;&#25773;&#21644;&#23616;&#37096;&#21270;&#20849;&#23384;&#8212;&#8212;&#24182;&#19988;&#36825;&#31181;&#23646;&#24615;&#34987;&#29992;&#20110;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#28216;&#36208;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#31639;&#27861;&#65292;&#23558;&#20351;&#36172;&#21338;&#38382;&#39064;&#22256;&#38590;&#30340;&#20004;&#31181;&#25805;&#20316;&#8212;&#8212;&#25506;&#32034;&#21644;&#21033;&#29992;&#8212;&#8212;&#19982;&#36825;&#20004;&#31181;&#37327;&#23376;&#28216;&#36208;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#22522;&#20110;&#37327;&#23376;&#28216;&#36208;&#30340;&#26032;&#31574;&#30053;&#30456;&#27604;&#20110;&#30456;&#24212;&#30340;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum walks (QWs) have the property that classical random walks (RWs) do not possess -- coexistence of linear spreading and localization -- and this property is utilized to implement various kinds of applications. This paper proposes a quantum-walk-based algorithm for multi-armed-bandit (MAB) problems by associating the two operations that make MAB problems difficult -exploration and exploitation -- with these two behaviors of QWs. We show that this new policy based on the QWs realizes high performance compared with the corresponding RW-based one.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#24471;&#21040;&#30340;&#19981;&#21516;&#20294;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#26631;&#31614;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#25429;&#33719;&#12290;</title><link>http://arxiv.org/abs/2304.10045</link><description>&lt;p&gt;
ID-MixGCL: &#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ID-MixGCL: Identity Mixup for Graph Contrastive Learning. (arXiv:2304.10045v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#24471;&#21040;&#30340;&#19981;&#21516;&#20294;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#26631;&#31614;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#25429;&#33719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#23637;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#26159;&#27604;&#36739;&#21516;&#19968;&#20010;&#22270;&#24418;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#8220;&#35270;&#22270;&#8221;&#20197;&#23398;&#20064;&#33410;&#28857;/&#22270;&#24418;&#34920;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#65292;&#21487;&#20197;&#29983;&#25104;&#20960;&#20010;&#32467;&#26500;&#19981;&#21516;&#20294;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#22240;&#27492;&#21407;&#22987;&#21644;&#22686;&#24378;&#30340;&#22270;&#24418;/&#33410;&#28857;&#30340;&#36523;&#20221;&#26631;&#31614;&#24212;&#35813;&#26159;&#30456;&#21516;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#65292;&#20363;&#22914;&#20998;&#23376;&#22270;&#20013;&#23545;&#33410;&#28857;&#25110;&#36793;&#30340;&#20219;&#20309;&#25200;&#21160;&#37117;&#20250;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25913;&#21464;&#22270;&#24418;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#22686;&#24378;&#22270;&#24418;&#32467;&#26500;&#24212;&#35813;&#20276;&#38543;&#30528;&#23545;&#23545;&#27604;&#25439;&#22833;&#20351;&#29992;&#30340;&#26631;&#31614;&#30340;&#36866;&#24212;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ID-MixGCL&#65292;&#23427;&#20801;&#35768;&#21516;&#26102;&#35843;&#33410;&#36755;&#20837;&#22270;&#24418;&#21644;&#30456;&#24212;&#30340;&#36523;&#20221;&#26631;&#31614;&#65292;&#20855;&#26377;&#21487;&#25511;&#30340;&#25913;&#21464;&#31243;&#24230;&#65292;&#20174;&#32780;&#25429;&#33719;&#32454;&#31890;&#24230;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed graph contrastive learning (GCL) approaches compare two different "views" of the same graph in order to learn node/graph representations. The core assumption of these approaches is that by graph augmentation, it is possible to generate several structurally different but semantically similar graph structures, and therefore, the identity labels of the original and augmented graph/nodes should be identical. However, in this paper, we observe that this assumption does not always hold, for example, any perturbation to nodes or edges in a molecular graph will change the graph labels to some degree. Therefore, we believe that augmenting the graph structure should be accompanied by an adaptation of the labels used for the contrastive loss. Based on this idea, we propose ID-MixGCL, which allows for simultaneous modulation of both the input graph and the corresponding identity labels, with a controllable degree of change, leading to the capture of fine-grained representations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#25351;&#23548;&#30340; Actor-Critic &#27169;&#22359;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;&#32508;&#21512;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#26368;&#20248;&#22791;&#20221;&#39034;&#24207;&#26469;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.10041</link><description>&lt;p&gt;
&#38754;&#21521;&#20855;&#26377;&#26102;&#38388;&#30446;&#26631;&#30340;&#36830;&#32493;&#31995;&#32479;&#30340;&#25299;&#25169;&#25351;&#23548;&#30340; Actor-Critic &#27169;&#22359;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Topological Guided Actor-Critic Modular Learning of Continuous Systems with Temporal Objectives. (arXiv:2304.10041v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#25351;&#23548;&#30340; Actor-Critic &#27169;&#22359;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;&#32508;&#21512;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#26368;&#20248;&#22791;&#20221;&#39034;&#24207;&#26469;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#20013;&#32473;&#23450;&#39640;&#32423;&#35268;&#33539;&#30340;&#36830;&#32493;&#29366;&#24577;&#38543;&#26426;&#21160;&#24577;&#31995;&#32479;&#30340;&#27491;&#24335;&#31574;&#30053;&#32508;&#21512;&#12290;&#20026;&#20102;&#23398;&#20064;&#26368;&#22823;&#21270;&#28385;&#36275;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#25105;&#20204;&#23545;&#21160;&#24577;&#31995;&#32479;&#21644;&#32763;&#35793;&#20986;&#30340;&#33258;&#21160;&#26426;&#36827;&#34892;&#20102;&#20056;&#31215;&#26500;&#36896;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#20010;&#20056;&#31215;&#31995;&#32479;&#65292;&#28982;&#21518;&#22312;&#20854;&#20013;&#35299;&#20915;&#20102;&#26368;&#20248;&#35745;&#21010;&#38382;&#39064;&#12290;&#30001;&#20110;&#20056;&#31215;&#31995;&#32479;&#20855;&#26377;&#28151;&#21512;&#20056;&#31215;&#29366;&#24577;&#31354;&#38388;&#65292;&#23548;&#33268;&#22870;&#21169;&#31232;&#30095;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#26368;&#20248;&#22791;&#20221;&#39034;&#24207;&#65292;&#19982;&#25299;&#25169;&#39034;&#24207;&#30456;&#21453;&#65292;&#20197;&#25351;&#23548;&#20540;&#22791;&#20221;&#24182;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#24191;&#20041;&#26368;&#20248;&#22791;&#20221;&#39034;&#24207;&#36827;&#34892;&#26368;&#20248;&#35745;&#21010;&#38382;&#39064;&#30340;&#26368;&#20248;&#24615;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#39034;&#24207;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20808;&#36827;&#30340;&#25968;&#23398;&#25216;&#26415;&#65292;&#24182;&#20855;&#26377;&#36229;&#21442;&#25968;&#33258;&#35843;&#25972;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#21644;&#25910;&#25947;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the formal policy synthesis of continuous-state stochastic dynamic systems given high-level specifications in linear temporal logic. To learn an optimal policy that maximizes the satisfaction probability, we take a product between a dynamic system and the translated automaton to construct a product system on which we solve an optimal planning problem. Since this product system has a hybrid product state space that results in reward sparsity, we introduce a generalized optimal backup order, in reverse to the topological order, to guide the value backups and accelerate the learning process. We provide the optimality proof for using the generalized optimal backup order in this optimal planning problem. Further, this paper presents an actor-critic reinforcement learning algorithm when topological order applies. This algorithm leverages advanced mathematical techniques and enjoys the property of hyperparameter self-tuning. We provide proof of the optimality and conver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#20998;&#24067;&#22806;&#26816;&#27979;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#20998;&#35299;&#25104;&#20219;&#21153;&#20869;&#39044;&#27979;&#21644;&#20219;&#21153; ID &#39044;&#27979;&#65292;&#24182;&#19988;&#20219;&#21153; ID &#39044;&#27979;&#19982;&#20998;&#24067;&#22806;&#26816;&#27979;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2304.10038</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#25345;&#32493;&#23398;&#20064;&#65306;&#32479;&#19968;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Open-World Continual Learning: Unifying Novelty Detection and Continual Learning. (arXiv:2304.10038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#20998;&#24067;&#22806;&#26816;&#27979;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#20998;&#35299;&#25104;&#20219;&#21153;&#20869;&#39044;&#27979;&#21644;&#20219;&#21153; ID &#39044;&#27979;&#65292;&#24182;&#19988;&#20219;&#21153; ID &#39044;&#27979;&#19982;&#20998;&#24067;&#22806;&#26816;&#27979;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528; AI agent &#22312;&#26410;&#30693;&#25110;&#26032;&#22855;&#30340;&#30495;&#23454;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#23427;&#20204;&#38656;&#35201;&#20855;&#22791; (1) &#35748;&#35782;&#24050;&#32463;&#23398;&#20064;&#36807;&#30340;&#29289;&#20307;&#21644;&#26816;&#27979;&#21040;&#20043;&#21069;&#26410;&#35265;&#25110;&#23398;&#20064;&#30340;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450; (2) &#22686;&#37327;&#22320;&#23398;&#20064;&#26032;&#29289;&#21697;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#26377;&#30693;&#35782;&#21644;&#26356;&#24378;&#22823;&#12290; (1) &#31216;&#20026;&#26032;&#39062;&#24615;&#26816;&#27979;&#25110;&#20998;&#24067;&#22806; (OOD) &#26816;&#27979;&#65292;&#32780; (2) &#31216;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064; (CIL)&#65292;&#26159;&#25345;&#32493;&#23398;&#20064; (CL) &#30340;&#19968;&#31181;&#35774;&#32622;&#12290;&#22312;&#29616;&#26377;&#30340;&#30740;&#31350;&#20013;&#65292;OOD &#26816;&#27979;&#21644; CIL &#34987;&#35270;&#20026;&#20004;&#20010;&#23436;&#20840;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102; OOD &#26816;&#27979;&#23454;&#38469;&#19978;&#23545;&#20110; CIL &#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034; CIL &#21487;&#20197;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65306;&#20219;&#21153;&#20869;&#39044;&#27979; (WP) &#21644;&#20219;&#21153; ID &#39044;&#27979;(TP)&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102; TP &#19982; OOD &#26816;&#27979;&#30456;&#20851;&#12290;&#20851;&#38190;&#30340;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#26080;&#35770; WP &#21644; OOD &#26816;&#27979;&#65288;&#25110; TP&#65289;&#26159;&#21542;&#30001; CIL &#31639;&#27861;&#26174;&#24335;&#25110;&#38544;&#24335;&#22320;&#23450;&#20041;&#65292;&#22909;&#30340; WP &#21644;&#33391;&#22909;&#30340; OOD &#26816;&#27979;&#25110; TP &#24635;&#26159;&#23384;&#22312;&#23884;&#20837;&#22312;&#20219;&#20309; CIL &#31639;&#27861;&#20013;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI agents are increasingly used in the real open world with unknowns or novelties, they need the ability to (1) recognize objects that (i) they have learned and (ii) detect items that they have not seen or learned before, and (2) learn the new items incrementally to become more and more knowledgeable and powerful. (1) is called novelty detection or out-of-distribution (OOD) detection and (2) is called class incremental learning (CIL), which is a setting of continual learning (CL). In existing research, OOD detection and CIL are regarded as two completely different problems. This paper theoretically proves that OOD detection actually is necessary for CIL. We first show that CIL can be decomposed into two sub-problems: within-task prediction (WP) and task-id prediction (TP). We then prove that TP is correlated with OOD detection. The key theoretical result is that regardless of whether WP and OOD detection (or TP) are defined explicitly or implicitly by a CIL algorithm, good WP and go
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32473;&#20986;&#20102;&#23545;&#20113;&#12289;&#36793;&#32536;&#21644;&#31471;&#35774;&#22791;&#19978;DNN&#20998;&#21306;&#26041;&#27861;&#30340;&#20840;&#38754;&#32508;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20116;&#32500;&#20998;&#31867;&#26694;&#26550;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#24403;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#19988;&#25968;&#25454;&#36828;&#31243;&#20256;&#36755;&#20195;&#20215;&#39640;&#26114;&#26102;&#25552;&#39640;DNN&#25512;&#29702;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10020</link><description>&lt;p&gt;
&#20113;&#12289;&#36793;&#32536;&#21644;&#32456;&#31471;&#35774;&#22791;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#21306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Neural Network Partition over Cloud, Edge and End Devices. (arXiv:2304.10020v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#23545;&#20113;&#12289;&#36793;&#32536;&#21644;&#31471;&#35774;&#22791;&#19978;DNN&#20998;&#21306;&#26041;&#27861;&#30340;&#20840;&#38754;&#32508;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20116;&#32500;&#20998;&#31867;&#26694;&#26550;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#24403;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#19988;&#25968;&#25454;&#36828;&#31243;&#20256;&#36755;&#20195;&#20215;&#39640;&#26114;&#26102;&#25552;&#39640;DNN&#25512;&#29702;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20998;&#21306;&#26159;&#19968;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#28041;&#21450;&#23558;DNN&#25286;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#24182;&#23558;&#23427;&#20204;&#21368;&#36733;&#21040;&#29305;&#23450;&#20301;&#32622;&#12290;&#30001;&#20110;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#21644;&#36793;&#32536;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24403;&#36793;&#32536;&#21644;&#32456;&#31471;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#19988;&#20174;&#36825;&#20123;&#35774;&#22791;&#21040;&#20113;&#30340;&#36828;&#31243;&#20256;&#36755;&#25968;&#25454;&#20195;&#20215;&#39640;&#26114;&#26102;&#65292;DNN&#20998;&#21306;&#34987;&#35748;&#20026;&#26159;&#25552;&#39640;DNN&#25512;&#29702;&#24615;&#33021;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#30456;&#20851;&#25991;&#29486;&#30340;&#35814;&#32454;&#25991;&#29486;&#25910;&#38598;&#65292;&#25552;&#20379;&#20102;&#23545;&#20113;&#12289;&#36793;&#32536;&#21644;&#31471;&#35774;&#22791;&#19978;DNN&#20998;&#21306;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;DNN&#20998;&#21306;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24037;&#20316;&#26041;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;DNN&#20998;&#21306;&#38382;&#39064;&#30340;&#32479;&#19968;&#25968;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;DNN&#20998;&#21306;&#26041;&#27861;&#30340;&#20116;&#32500;&#20998;&#31867;&#26694;&#26550;&#65292;&#21253;&#25324;&#37096;&#32626;&#20301;&#32622;&#12289;&#20998;&#21306;&#31890;&#24230;&#12289;&#20998;&#21306;&#32422;&#26463;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#36890;&#20449;&#21327;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;DNN&#20998;&#21306;&#21644;&#20854;&#19982;&#20113;&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#38598;&#25104;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) partition is a research problem that involves splitting a DNN into multiple parts and offloading them to specific locations. Because of the recent advancement in multi-access edge computing and edge intelligence, DNN partition has been considered as a powerful tool for improving DNN inference performance when the computing resources of edge and end devices are limited and the remote transmission of data from these devices to clouds is costly. This paper provides a comprehensive survey on the recent advances and challenges in DNN partition approaches over the cloud, edge, and end devices based on a detailed literature collection. We review how DNN partition works in various application scenarios, and provide a unified mathematical model of the DNN partition problem. We developed a five-dimensional classification framework for DNN partition approaches, consisting of deployment locations, partition granularity, partition constraints, optimization objectives, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#20998;&#31163;&#24615;&#20551;&#35774;&#21644;&#19978;&#19979;&#25991;&#24615;&#30340;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#19982;&#26694;&#26550;&#38382;&#39064;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#21518;&#32773;&#30340;&#37327;&#23376;&#27169;&#25311;&#21644;&#19981;&#21487;&#21028;&#23450;&#24615;&#35777;&#26126;&#65292;&#21516;&#26102;&#34920;&#26126;&#19978;&#19979;&#25991;&#24615;&#22312;&#29366;&#24577;&#20934;&#22791;&#21644;&#27979;&#37327;&#20013;&#22914;&#20309;&#34987;&#24341;&#23548;&#65292;&#24182;&#25351;&#20986;&#32454;&#24494;&#30340;&#35843;&#25972;&#20551;&#35774;&#22312;&#38750;&#19978;&#19979;&#25991;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2304.10010</link><description>&lt;p&gt;
&#21487;&#20998;&#24615;&#12289;&#19978;&#19979;&#25991;&#24615;&#21450;&#37327;&#23376;&#26694;&#26550;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Separability, Contextuality, and the Quantum Frame Problem. (arXiv:2304.10010v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#20998;&#31163;&#24615;&#20551;&#35774;&#21644;&#19978;&#19979;&#25991;&#24615;&#30340;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#19982;&#26694;&#26550;&#38382;&#39064;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#21518;&#32773;&#30340;&#37327;&#23376;&#27169;&#25311;&#21644;&#19981;&#21487;&#21028;&#23450;&#24615;&#35777;&#26126;&#65292;&#21516;&#26102;&#34920;&#26126;&#19978;&#19979;&#25991;&#24615;&#22312;&#29366;&#24577;&#20934;&#22791;&#21644;&#27979;&#37327;&#20013;&#22914;&#20309;&#34987;&#24341;&#23548;&#65292;&#24182;&#25351;&#20986;&#32454;&#24494;&#30340;&#35843;&#25972;&#20551;&#35774;&#22312;&#38750;&#19978;&#19979;&#25991;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29366;&#24577;&#21487;&#20998;&#31163;&#24615;&#20551;&#35774;&#19982;&#20934;&#22791;&#21644;&#27979;&#37327;&#19978;&#19979;&#25991;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#26694;&#26550;&#38382;&#39064;&#8212;&#8212;&#39044;&#27979;&#30001;&#20110;&#21160;&#20316;&#19981;&#20250;&#25913;&#21464;&#30340;&#20869;&#23481;&#30340;&#38382;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#32773;&#30340;&#37327;&#23376;&#27169;&#25311;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30340;&#19981;&#21487;&#21028;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29366;&#24577;&#20934;&#22791;&#21644;&#27979;&#37327;&#20013;&#19978;&#19979;&#25991;&#24615;&#26159;&#22914;&#20309;&#36890;&#36807;&#22522;&#36873;&#25321;&#12289;&#28909;&#21147;&#23398;&#20132;&#25442;&#21644;&#20808;&#39564;&#22240;&#26524;&#27169;&#22411;&#30340;&#24378;&#21046;&#23454;&#26045;&#26222;&#36941;&#24341;&#23548;&#30340;&#65292;&#24182;&#19988;&#22914;&#20309;&#35843;&#25972;&#20551;&#35774;&#22312;&#34987;&#25551;&#36848;&#20026;&#26080;&#29615;&#22659;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the relationship between assumptions of state separability and both preparation and measurement contextuality, and the relationship of both of these to the frame problem, the problem of predicting what does not change in consequence of an action. We state a quantum analog of the latter and prove its undecidability. We show how contextuality is generically induced in state preparation and measurement by basis choice, thermodynamic exchange, and the imposition of a priori causal models, and how fine-tuning assumptions appear ubiquitously in settings characterized as non-contextual.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#25913;&#36827;&#30340;&#24130;&#24459;&#27169;&#24335;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#21457;&#29616;&#25552;&#39640;&#20102;&#39044;&#27979;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#31934;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#19978;&#24471;&#21040;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ML&#22522;&#20934;&#36828;&#26410;&#39281;&#21644;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25913;&#36827;&#20855;&#26377;&#31361;&#28982;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10004</link><description>&lt;p&gt;
&#36895;&#36890;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24130;&#24459;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Power Law Trends in Speedrunning and Machine Learning. (arXiv:2304.10004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10004
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#25913;&#36827;&#30340;&#24130;&#24459;&#27169;&#24335;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#21457;&#29616;&#25552;&#39640;&#20102;&#39044;&#27979;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#31934;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#19978;&#24471;&#21040;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ML&#22522;&#20934;&#36828;&#26410;&#39281;&#21644;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25913;&#36827;&#20855;&#26377;&#31361;&#28982;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#30340;&#25913;&#36827;&#20013;&#23384;&#22312;&#24130;&#24459;&#27169;&#24335;&#12290;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26410;&#35299;&#20915;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#39044;&#27979;&#26576;&#20010;&#26102;&#38388;&#36328;&#24230;&#65288;&#22914;&#19968;&#20010;&#26376;&#65289;&#20869;&#30340;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#26102;&#65292;&#25552;&#39640;&#22522;&#32447;&#39044;&#27979;&#19981;&#25913;&#36827;&#30340;&#31934;&#24230;&#65311;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25928;&#24212;&#27169;&#22411;&#65292;&#22312;&#39044;&#27979;&#26679;&#26412;&#22806;&#30340;&#19990;&#30028;&#32426;&#24405;&#25913;&#36827;&#30340;&#30456;&#23545;&#22343;&#26041;&#35823;&#24046;&#19978;&#65292;&#25105;&#20204;&#22312;$p&lt;10^{-5}$&#30340;&#26174;&#33879;&#24615;&#27700;&#24179;&#19978;&#25552;&#39640;&#20102;&#22522;&#32447;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#23613;&#31649;&#20351;&#29992;&#30340;&#25968;&#25454;&#28857;&#36828;&#23569;&#20110;&#20808;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#39044;&#27979;&#27169;&#22411;&#65292;&#20294;&#30456;&#21516;&#30340;&#35774;&#32622;&#22312;$p=0.15$&#30340;&#26174;&#33879;&#24615;&#27700;&#24179;&#19978;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24182;&#21462;&#24471;&#20102;&#36229;&#36807;&#22522;&#32447;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35299;&#37322;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35748;&#20026;1&#65289;ML&#22522;&#20934;&#36828;&#26410;&#39281;&#21644;&#65292;2&#65289;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31361;&#28982;&#22823;&#24133;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
We find that improvements in speedrunning world records follow a power law pattern. Using this observation, we answer an outstanding question from previous work: How do we improve on the baseline of predicting no improvement when forecasting speedrunning world records out to some time horizon, such as one month? Using a random effects model, we improve on this baseline for relative mean square error made on predicting out-of-sample world record improvements as the comparison metric at a $p &lt; 10^{-5}$ significance level. The same set-up improves \textit{even} on the ex-post best exponential moving average forecasts at a $p = 0.15$ significance level while having access to substantially fewer data points. We demonstrate the effectiveness of this approach by applying it to Machine Learning benchmarks and achieving forecasts that exceed a baseline. Finally, we interpret the resulting model to suggest that 1) ML benchmarks are far from saturation and 2) sudden large improvements in Machine 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-RNN&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;CNN&#21644;RNN&#27169;&#22411;&#65292;&#22312;&#31934;&#24230;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.09994</link><description>&lt;p&gt;
&#22522;&#20110;LSTM-DeepLabv3+&#21644;&#26102;&#31354;&#29305;&#24449;&#34701;&#21512;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion. (arXiv:2304.09994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-RNN&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;CNN&#21644;RNN&#27169;&#22411;&#65292;&#22312;&#31934;&#24230;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22240;&#20854;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#36880;&#28176;&#25104;&#20026;&#27969;&#34892;&#30340;&#27946;&#27700;&#39044;&#27979;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#29420;&#30340;&#31354;&#38388;&#25110;&#26102;&#38388;&#29305;&#24449;&#20998;&#26512;&#65292;&#24182;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#32500;&#24230;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CNN-RNN&#30340;&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#65292;&#23558;CNN&#22312;&#22788;&#29702;&#31354;&#38388;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;RNN&#22312;&#20998;&#26512;&#19981;&#21516;&#32500;&#24230;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#30340;&#20248;&#21183;&#25972;&#21512;&#36215;&#26469;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#36827;&#34892;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#27946;&#27700;&#39044;&#27979;&#12290;&#24212;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#30830;&#23450;&#19971;&#20010;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#27946;&#27700;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#26368;&#20339;&#32452;&#21512;&#31574;&#30053;&#12290;&#36890;&#36807;&#32467;&#21512;&#22235;&#20010;CNN&#65288;FCN&#65292;UNet&#65292;SegNet&#65292;DeepLabv3+&#65289;&#21644;&#19977;&#20010;RNN&#65288;LSTM&#65292;BiLSTM&#65292;GRU&#65289;&#65292;&#26368;&#20248;&#28151;&#21512;&#27169;&#22411;&#34987;&#30830;&#23450;&#20026;LSTM-DeepLabv3+&#12290;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65288;MAE&#12289;RMSE&#12289;NSE&#21644;KGE&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have become increasingly popular for flood prediction due to their superior accuracy and efficiency compared to traditional methods. However, current machine learning methods often rely on separate spatial or temporal feature analysis and have limitations on the types, number, and dimensions of input data. This study presented a CNN-RNN hybrid feature fusion modelling approach for urban flood prediction, which integrated the strengths of CNNs in processing spatial features and RNNs in analyzing different dimensions of time sequences. This approach allowed for both static and dynamic flood predictions. Bayesian optimization was applied to identify the seven most influential flood-driven factors and determine the best combination strategy. By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM, BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. This model achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE wer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.09991</link><description>&lt;p&gt;
&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#23457;&#35745;LLM&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#37319;&#35775;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#25991;&#29486;&#30340;&#30740;&#31350;&#65292;&#22686;&#24378;&#20102;&#8220;AdaTest&#8221;&#23457;&#35745;&#24037;&#20855;&#65292;&#36825;&#20010;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#21327;&#21516;&#20248;&#21183;&#65292;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#37096;&#32626;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#21644;&#26222;&#21450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;&#20998;&#31867;&#36824;&#26159;&#29983;&#25104;&#65292;&#37117;&#34920;&#29616;&#20986;&#26377;&#20559;&#24046;&#21644;&#19981;&#36127;&#36131;&#20219;&#30340;&#34892;&#20026;&#65292;&#23545;&#20154;&#31867;&#36896;&#25104;&#20102;&#35268;&#27169;&#24615;&#30340;&#20260;&#23475;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#23457;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23457;&#35745;&#24037;&#20855;&#21033;&#29992;&#20154;&#21644;&#25110;AI&#26469;&#21457;&#29616;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21644;&#24863;&#30693;&#30340;&#25991;&#29486;&#65292;&#24182;&#37319;&#35775;&#20102;&#23433;&#20840;&#21644;&#20844;&#27491;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#19987;&#23478;&#65292;&#20197;&#22686;&#24378;&#23457;&#35745;&#24037;&#20855;&#8220;AdaTest&#8221;&#65288;Ribeiro&#21644;Lundberg&#65292;2022&#65289;&#65292;&#35813;&#24037;&#20855;&#30001;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#12290;&#36890;&#36807;&#35774;&#35745;&#36807;&#31243;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24863;&#30693;&#21644;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#22312;&#21327;&#20316;&#23457;&#35745;&#20013;&#21033;&#29992;&#20154;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#20114;&#34917;&#20248;&#21183;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#22686;&#24378;&#24037;&#20855;AdaTest ++&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65292;&#20351;&#21442;&#19982;&#32773;&#36827;&#34892;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously. Existing auditing tools leverage either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and conduct interviews with research experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro and Lundberg, 2022), which is powered by a generative large language model (LLM). Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of the augmented tool, AdaTest++, we conduct user studies with participants audit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32473;Transformer&#27169;&#22411;&#22686;&#21152;&#20004;&#20010;&#31616;&#21333;&#30340;&#24402;&#32435;&#23398;&#20064;&#20559;&#35265;&#25506;&#31350;&#20102;&#23398;&#20064;&#21644;&#39044;&#27979;&#31616;&#21333;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#36825;&#20123;&#20559;&#35265;&#30340;&#24110;&#21161;&#65292;&#21516;&#26102;&#25351;&#20986;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#26377;&#21161;&#20110;&#20154;&#31867;&#22312;&#22806;&#25512;&#33021;&#21147;&#26041;&#38754;&#30340;&#24402;&#32435;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.09979</link><description>&lt;p&gt;
&#36229;&#36234;Transformer&#30340;&#20989;&#25968;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Transformers for Function Learning. (arXiv:2304.09979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32473;Transformer&#27169;&#22411;&#22686;&#21152;&#20004;&#20010;&#31616;&#21333;&#30340;&#24402;&#32435;&#23398;&#20064;&#20559;&#35265;&#25506;&#31350;&#20102;&#23398;&#20064;&#21644;&#39044;&#27979;&#31616;&#21333;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#36825;&#20123;&#20559;&#35265;&#30340;&#24110;&#21161;&#65292;&#21516;&#26102;&#25351;&#20986;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#26377;&#21161;&#20110;&#20154;&#31867;&#22312;&#22806;&#25512;&#33021;&#21147;&#26041;&#38754;&#30340;&#24402;&#32435;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#39044;&#27979;&#31616;&#21333;&#20989;&#25968;&#30340;&#33021;&#21147;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#24320;&#22987;&#20351;&#29992;Transformer&#26550;&#26500;&#26469;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#28982;&#32780;&#20173;&#19981;&#28165;&#26970;&#36825;&#26159;&#21542;&#36275;&#20197;&#37325;&#29616;&#20154;&#20204;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22686;&#21152;&#20004;&#20010;&#31616;&#21333;&#30340;&#24402;&#32435;&#23398;&#20064;&#20559;&#35265;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#36825;&#20123;&#20559;&#35265;&#30452;&#25509;&#26469;&#33258;&#20110;&#35748;&#30693;&#31185;&#23398;&#20013;&#26368;&#36817;&#30340;&#25277;&#35937;&#25512;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#25253;&#36947;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#20123;&#20559;&#35265;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#21487;&#33021;&#26377;&#21161;&#20110;&#20154;&#31867;&#22806;&#25512;&#33021;&#21147;&#30340;&#24402;&#32435;&#23398;&#20064;&#20559;&#35265;&#30340;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn and predict simple functions is a key aspect of human intelligence. Recent works have started to explore this ability using transformer architectures, however it remains unclear whether this is sufficient to recapitulate the extrapolation abilities of people in this domain. Here, we propose to address this gap by augmenting the transformer architecture with two simple inductive learning biases, that are directly adapted from recent models of abstract reasoning in cognitive science. The results we report demonstrate that these biases are helpful in the context of large neural network models, as well as shed light on the types of inductive learning biases that may contribute to human abilities in extrapolation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32958;&#31227;&#26893;&#38382;&#39064;&#65292;&#21487;&#20197;&#36817;&#20284;&#35299;&#20915;NP&#38590;&#38382;&#39064;&#65292;&#24182;&#19988;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.09975</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32958;&#31227;&#26893;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving the Kidney-Exchange Problem via Graph Neural Networks with No Supervision. (arXiv:2304.09975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32958;&#31227;&#26893;&#38382;&#39064;&#65292;&#21487;&#20197;&#36817;&#20284;&#35299;&#20915;NP&#38590;&#38382;&#39064;&#65292;&#24182;&#19988;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36817;&#20284;&#35299;&#20915;&#20102;&#22270;&#19978;&#30340;&#32958;&#31227;&#26893;&#38382;&#39064;&#65288;KEP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#28041;&#21450;&#21040;&#20174;&#19968;&#32452;&#32958;&#33039;&#20379;&#20307;&#21644;&#31561;&#24453;&#32958;&#33039;&#25424;&#36192;&#30340;&#24739;&#32773;&#20013;&#65292;&#36873;&#25321;&#19968;&#32452;&#25424;&#36192;&#20197;&#26368;&#22823;&#21270;&#31227;&#26893;&#25968;&#37327;&#21644;&#36136;&#37327;&#65292;&#21516;&#26102;&#36981;&#23432;&#19968;&#20123;&#26377;&#20851;&#36825;&#20123;&#25424;&#36192;&#23433;&#25490;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#35813;&#25216;&#26415;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#31532;&#19968;&#27493;&#26159;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65307;&#31532;&#20108;&#27493;&#26159;&#30830;&#23450;&#24615;&#30340;&#38750;&#23398;&#20064;&#25628;&#32034;&#21551;&#21457;&#24335;&#65292;&#23427;&#21033;&#29992;GNN&#30340;&#36755;&#20986;&#26469;&#26597;&#25214;&#36335;&#24452;&#21644;&#24490;&#29615;&#12290;&#20026;&#20102;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#23454;&#29616;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26041;&#27861;&#65292;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#12289;&#20004;&#20010;&#36138;&#24515;&#25628;&#32034;&#21551;&#21457;&#24335;&#65292;&#19981;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22359;&#30340;GNN&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#22522;&#20110;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new learning-based approach for approximately solving the Kidney-Exchange Problem (KEP), an NP-hard problem on graphs. The problem consists of, given a pool of kidney donors and patients waiting for kidney donations, optimally selecting a set of donations to optimize the quantity and quality of transplants performed while respecting a set of constraints about the arrangement of these donations. The proposed technique consists of two main steps: the first is a Graph Neural Network (GNN) trained without supervision; the second is a deterministic non-learned search heuristic that uses the output of the GNN to find paths and cycles. To allow for comparisons, we also implemented and tested an exact solution method using integer programming, two greedy search heuristics without the machine learning module, and the GNN alone without a heuristic. We analyze and compare the methods and conclude that the learning-based two-stage approach is the best solution quality, outp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35821;&#35328;-&#35270;&#35273;GPT&#27169;&#22411;&#65292;&#22686;&#24378;GPT2&#27169;&#22411;&#20197;&#21253;&#25324;&#35270;&#35273;&#36755;&#20837;&#65292;&#28982;&#21518;&#24494;&#35843;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#22312;&#25163;&#26415;VQA&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#35270;&#35273;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2304.09974</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;SurgicalGPT&#65306;&#31471;&#21040;&#31471;&#30340;&#35821;&#35328;-&#35270;&#35273;GPT&#27169;&#22411;&#29992;&#20110;&#25163;&#26415;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery. (arXiv:2304.09974v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35821;&#35328;-&#35270;&#35273;GPT&#27169;&#22411;&#65292;&#22686;&#24378;GPT2&#27169;&#22411;&#20197;&#21253;&#25324;&#35270;&#35273;&#36755;&#20837;&#65292;&#28982;&#21518;&#24494;&#35843;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#22312;&#25163;&#26415;VQA&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#35270;&#35273;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#20351;&#29992;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#36825;&#20123;&#33258;&#22238;&#24402;LLM&#21487;&#20197;&#29983;&#25104;&#38271;&#32780;&#36830;&#36143;&#30340;&#27573;&#33853;&#65292;&#20294;&#26159;&#22312;&#38656;&#35201;&#21516;&#26102;&#22788;&#29702;&#35270;&#35273;&#21450;&#35821;&#35328;&#20449;&#24687;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#38656;&#35201;&#21452;&#21521;&#27880;&#24847;&#21147;&#25110;&#34701;&#21512;&#25216;&#26415;&#26469;&#25429;&#33719;&#22810;&#31181;&#27169;&#24577;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;&#30001;&#20110;GPT&#26412;&#36523;&#19981;&#25903;&#25345;&#35270;&#35273;&#26631;&#35760;&#22788;&#29702;&#65292;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;GPT&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#25163;&#26415;VQA&#20013;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#35821;&#35328;-&#35270;&#35273;GPT&#65288;LV-GPT&#65289;&#27169;&#22411;&#65292;&#23558;GPT2&#27169;&#22411;&#25193;&#23637;&#20026;&#21253;&#25324;&#35270;&#35273;&#36755;&#20837;&#65288;&#22270;&#20687;&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;LV-GPT&#21253;&#25324;&#21151;&#33021;&#25552;&#21462;&#22120;&#65288;&#35270;&#35273;&#26631;&#35760;&#22120;&#65289;&#21644;&#35270;&#35273;&#26631;&#35760;&#23884;&#20837;&#65288;&#26631;&#35760;&#31867;&#22411;&#21644;&#23039;&#24577;&#65289;&#12290;&#37492;&#20110;GPT&#27169;&#22411;&#20013;&#21333;&#21521;&#20851;&#27880;&#30340;&#38480;&#21046;&#20197;&#21450;&#29983;&#25104;&#36830;&#36143;&#38271;&#27573;&#33853;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;GPT&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21152;&#20837;&#21452;&#21521;&#20851;&#27880;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25163;&#26415;VQA&#20219;&#21153;&#20013;&#30340;&#35270;&#35273;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in GPT-based large language models (LLMs) are revolutionizing natural language processing, exponentially increasing its use across various domains. Incorporating uni-directional attention, these autoregressive LLMs can generate long and coherent paragraphs. However, for visual question answering (VQA) tasks that require both vision and language processing, models with bi-directional attention or models employing fusion techniques are often employed to capture the context of multiple modalities all at once. As GPT does not natively process vision tokens, to exploit the advancements in GPT models for VQA in robotic surgery, we design an end-to-end trainable Language-Vision GPT (LV-GPT) model that expands the GPT2 model to include vision input (image). The proposed LV-GPT incorporates a feature extractor (vision tokenizer) and vision token embedding (token type and pose). Given the limitations of unidirectional attention in GPT models and their ability to generate coherent long p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#65292;&#20855;&#26377;&#20248;&#20110;&#24120;&#35265;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09970</link><description>&lt;p&gt;
&#23398;&#20064;&#31574;&#30053;&#22312;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning policies for resource allocation in business processes. (arXiv:2304.09970v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#65292;&#20855;&#26377;&#20248;&#20110;&#24120;&#35265;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#20998;&#37197;&#26159;&#23558;&#36164;&#28304;&#20998;&#37197;&#21040;&#24517;&#39035;&#22312;&#36816;&#34892;&#26102;&#21051;&#25191;&#34892;&#30340;&#19994;&#21153;&#27969;&#31243;&#27963;&#21160;&#20013;&#12290;&#34429;&#28982;&#36164;&#28304;&#20998;&#37197;&#22312;&#21046;&#36896;&#31561;&#20854;&#20182;&#39046;&#22495;&#20013;&#24050;&#32463;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#21364;&#21482;&#23384;&#22312;&#23569;&#37327;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#20225;&#19994;&#27969;&#31243;&#30340;&#24212;&#29992;&#25110;&#26159;&#21482;&#38024;&#23545;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#30340;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#12290;&#22312;&#20195;&#34920;&#20856;&#22411;&#19994;&#21153;&#27969;&#31243;&#32467;&#26500;&#30340;&#19968;&#32452;&#24773;&#26223;&#20197;&#21450;&#22312;&#20195;&#34920;&#29616;&#23454;&#19994;&#21153;&#27969;&#31243;&#30340;&#23436;&#25972;&#32593;&#32476;&#19978;&#65292;&#23558;&#20004;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#20013;&#20248;&#20110;&#25110;&#19982;&#24120;&#35265;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource allocation is the assignment of resources to activities that must be executed in a business process at a particular moment at run-time. While resource allocation is well-studied in other fields, such as manufacturing, there exist only a few methods in business process management. Existing methods are not suited for application in large business processes or focus on optimizing resource allocation for a single case rather than for all cases combined. To fill this gap, this paper proposes two learning-based methods for resource allocation in business processes: a deep reinforcement learning-based approach and a score-based value function approximation approach. The two methods are compared against existing heuristics in a set of scenarios that represent typical business process structures and on a complete network that represents a realistic business process. The results show that our learning-based methods outperform or are competitive with common heuristics in most scenarios a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.09960</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#28508;&#22312;&#31354;&#38388;&#29702;&#35770;&#23545;&#24212;&#26032;&#20852;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24182;&#19981;&#26159;&#38543;&#26426;&#29983;&#25104;&#65292;&#32780;&#26159;&#20026;&#20102;&#20256;&#36882;&#20449;&#24687;&#12290;&#35821;&#35328;&#19982;&#20854;&#24213;&#23618;&#21547;&#20041;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#20851;&#32852;&#65292;&#22312;&#20854;&#30456;&#20851;&#24615;&#26041;&#38754;&#26377;&#30528;&#20005;&#37325;&#20559;&#24046;&#30340;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31232;&#30095;&#24615;&#65292;&#36825;&#20123;&#39640;&#23792;&#20540;&#24688;&#22909;&#19982;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#21305;&#37197;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#22823;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;LLMs&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#25506;&#32034;&#32852;&#21512;&#20998;&#24067;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#26377;&#25928;&#25512;&#29702;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#20998;&#31867;&#20026;&#26126;&#30830;&#19982;{\epsilon}-&#27169;&#31946;&#65292;&#24182;&#25552;&#20986;&#23450;&#37327;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#65289;&#37117;&#21487;&#20197;&#24402;&#22240;&#20110;&#23545;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3&#27169;&#22411;&#65292;&#27604;&#20197;&#24448;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#24615;&#33021;&#26356;&#20986;&#33394;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#34394;&#20551;&#35780;&#35770;&#19982;&#30495;&#23454;&#35780;&#35770;&#22312;&#35821;&#35328;&#34920;&#36798;&#19978;&#23384;&#22312;&#30340;&#24046;&#24322;&#65292;&#20998;&#26512;&#20102;&#34394;&#20551;&#35780;&#35770;&#30340;&#27169;&#24335;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.09948</link><description>&lt;p&gt;
&#25235;&#20303;&#20320;&#30340;&#35805;&#35821;&#65306;&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#35782;&#21035;&#34394;&#20551;&#21307;&#24072;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers. (arXiv:2304.09948v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3&#27169;&#22411;&#65292;&#27604;&#20197;&#24448;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#24615;&#33021;&#26356;&#20986;&#33394;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#34394;&#20551;&#35780;&#35770;&#19982;&#30495;&#23454;&#35780;&#35770;&#22312;&#35821;&#35328;&#34920;&#36798;&#19978;&#23384;&#22312;&#30340;&#24046;&#24322;&#65292;&#20998;&#26512;&#20102;&#34394;&#20551;&#35780;&#35770;&#30340;&#27169;&#24335;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#29983;&#34394;&#20551;&#35780;&#35770;&#30340;&#27867;&#28389;&#21487;&#33021;&#20250;&#23545;&#24739;&#32773;&#31119;&#21033;&#20135;&#29983;&#28508;&#22312;&#21361;&#23475;&#65292;&#24182;&#24341;&#36215;&#28040;&#36153;&#32773;&#20445;&#25252;&#32452;&#32455;&#21644;&#30417;&#31649;&#26426;&#26500;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24050;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#34394;&#20551;&#35780;&#35770;&#19982;&#30495;&#23454;&#35780;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#29305;&#24449;&#36824;&#26159;&#29702;&#35299;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#35760;&#22909;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;38048&#20010;&#21307;&#24072;&#35780;&#35770;&#65292;&#20197;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35780;&#35770;&#20998;&#31867;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#19982;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;GPT4&#65292;GPT&#31995;&#21015;&#20013;&#26368;&#26032;&#30340;&#27169;&#22411;&#65292;&#26469;&#25581;&#31034;&#34394;&#20551;&#21644;&#30495;&#23454;&#21307;&#29983;&#35780;&#35770;&#30340;&#20851;&#38190;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;GPT-3&#22312;&#36825;&#20010;&#39046;&#22495;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#34394;&#20551;&#35780;&#35770;&#19982;&#30495;&#23454;&#35780;&#35770;&#22312;&#35821;&#35328;&#34920;&#36798;&#19978;&#30340;&#24046;&#24322;&#65292; &#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#34394;&#20551;&#35780;&#35770;&#30340;&#27169;&#24335;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of fake reviews of doctors has potentially detrimental consequences for patient well-being and has prompted concern among consumer protection groups and regulatory bodies. Yet despite significant advancements in the fields of machine learning and natural language processing, there remains limited comprehension of the characteristics differentiating fraudulent from authentic reviews. This study utilizes a novel pre-labeled dataset of 38048 physician reviews to establish the effectiveness of large language models in classifying reviews. Specifically, we compare the performance of traditional ML models, such as logistic regression and support vector machines, to generative pre-trained transformer models. Furthermore, we use GPT4, the newest model in the GPT family, to uncover the key dimensions along which fake and genuine physician reviews differ. Our findings reveal significantly superior performance of GPT-3 over traditional ML models in this context. Additionally, ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#36923;&#36753;&#32534;&#31243;&#30340;&#35282;&#24230;&#35299;&#20915;&#28436;&#32462;&#25968;&#25454;&#24211;&#23436;&#25972;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#35777;&#26126;&#26641;&#20195;&#26367;SLDNF&#26641;&#26356;&#26041;&#20415;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#32452;&#26368;&#23567;&#30340;&#26465;&#20214;&#25351;&#23450;&#30693;&#35782;&#24211;&#20013;&#26356;&#25913;&#23545;&#23436;&#25972;&#24615;&#32422;&#26463;&#30340;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.09944</link><description>&lt;p&gt;
&#22312;&#28436;&#32462;&#25968;&#25454;&#24211;&#20013;&#39564;&#35777;&#23436;&#25972;&#24615;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
\"Uberpr\"ufung von Integrit\"atsbedingungen in Deduktiven Datenbanken. (arXiv:2304.09944v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#36923;&#36753;&#32534;&#31243;&#30340;&#35282;&#24230;&#35299;&#20915;&#28436;&#32462;&#25968;&#25454;&#24211;&#23436;&#25972;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#35777;&#26126;&#26641;&#20195;&#26367;SLDNF&#26641;&#26356;&#26041;&#20415;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#32452;&#26368;&#23567;&#30340;&#26465;&#20214;&#25351;&#23450;&#30693;&#35782;&#24211;&#20013;&#26356;&#25913;&#23545;&#23436;&#25972;&#24615;&#32422;&#26463;&#30340;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#30340;&#24320;&#21457;&#12290;&#36825;&#20123;&#30693;&#35782;&#24211;&#23481;&#26131;&#20986;&#29616;&#30683;&#30462;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22810;&#20010;&#19987;&#23478;&#26102;&#12290;&#20026;&#20102;&#20445;&#35777;&#21464;&#21270;&#26399;&#38388;&#30340;&#23436;&#25972;&#24615;&#65292;&#38656;&#35201;&#19968;&#20123;&#31243;&#24207;&#12290;&#26412;&#25991;&#20174;&#36923;&#36753;&#32534;&#31243;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23436;&#25972;&#24615;&#36829;&#35268;&#21487;&#34987;&#35299;&#37322;&#20026;&#23436;&#25972;&#24615;&#32422;&#26463;&#35777;&#26126;&#19978;&#30340;&#29305;&#27530;&#25805;&#20316;&#65292;&#20854;&#20013;SLDNF&#35777;&#26126;&#26159;&#37325;&#28857;&#12290;&#25105;&#20204;&#23558;&#35777;&#26126;&#26641;&#23450;&#20041;&#20026;&#19968;&#31181;&#29305;&#27530;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#36825;&#26679;&#19968;&#26869;&#26641;&#28436;&#31034;&#23384;&#22312;SLDNF&#35777;&#26126;&#30340;&#21547;&#20041;&#12290;&#35777;&#26126;&#26641;&#27604;SLDNF&#26641;&#26356;&#26041;&#20415;&#65292;&#24182;&#20801;&#35768;&#23545;&#35777;&#26126;&#36827;&#34892;&#38598;&#21512;&#21462;&#21521;&#30340;&#32771;&#34385;&#12290;&#23427;&#20204;&#20063;&#26356;&#28165;&#26224;&#22320;&#23637;&#31034;&#20102;&#35777;&#26126;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36827;&#19968;&#27493;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#27492;&#32467;&#26500;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#26368;&#23567;&#30340;&#26465;&#20214;&#38598;&#65292;&#25351;&#23450;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#26356;&#25913;&#24433;&#21709;&#23436;&#25972;&#24615;&#32422;&#26463;&#30340;&#26377;&#25928;&#24615;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#37325;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in computer science and AI lead to the development of larger, more complex knowledge bases. These are susceptible to contradictions, particularly when multiple experts are involved. To ensure integrity during changes, procedures are needed. This work addresses the problem from a logical programming perspective. Integrity violations can be interpreted as special operations on proofs of integrity constraints, with SLDNF proofs being the focus. We define a proof tree as a special data structure and demonstrate the implication of the existence of an SLDNF proof through such a tree. Proof trees are more convenient than SLDNF trees and allow set-oriented considerations of proofs. They also present the proof structure more clearly, enabling further applications. Using this structure, we determine a minimal set of conditions that specify when a change in the knowledge base affects the validity of an integrity constraint. Additionally, this approach allows for the reuse of large pa
&lt;/p&gt;</description></item><item><title>LARD&#25968;&#25454;&#38598;&#26159;&#29992;&#20110;&#30528;&#38470;&#21644;&#36827;&#22330;&#38454;&#27573;&#30340;&#36305;&#36947;&#26816;&#27979;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#33322;&#25293;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23427;&#30001;&#22823;&#37327;&#21512;&#25104;&#22270;&#20687;&#21644;&#19968;&#20123;&#20154;&#24037;&#26631;&#27880;&#30340;&#23454;&#38469;&#30528;&#38470;&#38236;&#22836;&#22270;&#20687;&#32452;&#25104;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#22120;&#20197;&#20135;&#29983;&#27492;&#31867;&#21512;&#25104;&#22270;&#20687;&#24182;&#33258;&#21160;&#27880;&#37322;&#36305;&#36947;&#25296;&#35282;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.09938</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#30528;&#38470;&#25968;&#25454;&#38598;-- LARD&#65288;Landing Approach Runway Detection Dataset&#65289;
&lt;/p&gt;
&lt;p&gt;
LARD -- Landing Approach Runway Detection -- Dataset for Vision Based Landing. (arXiv:2304.09938v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09938
&lt;/p&gt;
&lt;p&gt;
LARD&#25968;&#25454;&#38598;&#26159;&#29992;&#20110;&#30528;&#38470;&#21644;&#36827;&#22330;&#38454;&#27573;&#30340;&#36305;&#36947;&#26816;&#27979;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#33322;&#25293;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23427;&#30001;&#22823;&#37327;&#21512;&#25104;&#22270;&#20687;&#21644;&#19968;&#20123;&#20154;&#24037;&#26631;&#27880;&#30340;&#23454;&#38469;&#30528;&#38470;&#38236;&#22836;&#22270;&#20687;&#32452;&#25104;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#22120;&#20197;&#20135;&#29983;&#27492;&#31867;&#21512;&#25104;&#22270;&#20687;&#24182;&#33258;&#21160;&#27880;&#37322;&#36305;&#36947;&#25296;&#35282;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#33258;&#20027;&#31995;&#32479;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;&#25910;&#38598;&#36275;&#22815;&#19988;&#20195;&#34920;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#23613;&#31649;&#33322;&#31354;&#33322;&#22825;&#39046;&#22495;&#30340;&#33258;&#20027;&#30528;&#38470;&#31995;&#32479;&#20855;&#26377;&#24378;&#28872;&#30340;&#23454;&#29992;&#21644;&#21830;&#19994;&#20215;&#20540;&#65292;&#20294;&#32570;&#20047;&#24320;&#28304;&#30340;&#33322;&#31354;&#24433;&#20687;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#33322;&#25293;&#22270;&#20687;&#25968;&#25454;&#38598;-LARD&#65292;&#29992;&#20110;&#30528;&#38470;&#21644;&#36827;&#22330;&#38454;&#27573;&#30340;&#36305;&#36947;&#26816;&#27979;&#20219;&#21153;&#12290;&#25968;&#25454;&#38598;&#22823;&#37096;&#20998;&#30001;&#21512;&#25104;&#22270;&#20687;&#32452;&#25104;&#65292;&#20294;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20154;&#24037;&#26631;&#27880;&#30340;&#23454;&#38469;&#30528;&#38470;&#38236;&#22836;&#22270;&#20687;&#65292;&#20197;&#25193;&#23637;&#26816;&#27979;&#20219;&#21153;&#21040;&#26356;&#29616;&#23454;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#20135;&#29983;&#36825;&#26679;&#30340;&#21512;&#25104;&#21069;&#35270;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#20960;&#20309;&#21464;&#25442;&#23454;&#29616;&#36305;&#36947;&#25296;&#35282;&#28857;&#30340;&#33258;&#21160;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#22914;&#25968;&#25454;&#38598;&#36136;&#37327;&#20998;&#26512;&#25110;&#24320;&#21457;&#27169;&#22411;&#24212;&#23545;&#26816;&#27979;&#20219;&#21153;&#65292;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the interest in autonomous systems continues to grow, one of the major challenges is collecting sufficient and representative real-world data. Despite the strong practical and commercial interest in autonomous landing systems in the aerospace field, there is a lack of open-source datasets of aerial images. To address this issue, we present a dataset-lard-of high-quality aerial images for the task of runway detection during approach and landing phases. Most of the dataset is composed of synthetic images but we also provide manually labelled images from real landing footages, to extend the detection task to a more realistic setting. In addition, we offer the generator which can produce such synthetic front-view images and enables automatic annotation of the runway corners through geometric transformations. This dataset paves the way for further research such as the analysis of dataset quality or the development of models to cope with the detection tasks. Find data, code and more up-to
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;eBible&#30340;&#22307;&#32463;&#32763;&#35793;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;1009&#20010;&#22307;&#32463;&#37096;&#20998;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;833&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#65292;&#20998;&#24067;&#22312;75&#20010;&#35821;&#35328;&#23478;&#26063;&#20013;&#12290;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;NLLB&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#22307;&#32463;&#32763;&#35793;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09919</link><description>&lt;p&gt;
The eBible&#35821;&#26009;&#24211;&#65306;&#29992;&#20110;&#38754;&#21521;&#20302;&#36164;&#28304;&#35821;&#35328;&#22280;&#30340;&#22307;&#32463;&#32763;&#35793;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
The eBible Corpus: Data and Model Benchmarks for Bible Translation for Low-Resource Languages. (arXiv:2304.09919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09919
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;eBible&#30340;&#22307;&#32463;&#32763;&#35793;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;1009&#20010;&#22307;&#32463;&#37096;&#20998;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;833&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#65292;&#20998;&#24067;&#22312;75&#20010;&#35821;&#35328;&#23478;&#26063;&#20013;&#12290;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;NLLB&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#22307;&#32463;&#32763;&#35793;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#37319;&#29992;&#25163;&#21160;&#12289;&#33258;&#21160;&#25110;&#20004;&#32773;&#32467;&#21512;&#30340;&#31574;&#30053;&#65292;&#39640;&#25928;&#20934;&#30830;&#22320;&#23558;&#35821;&#26009;&#24211;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#35768;&#22810;&#22522;&#30563;&#25945;&#32452;&#32455;&#33268;&#21147;&#20110;&#23558;&#22307;&#32463;&#32763;&#35793;&#25104;&#32570;&#20047;&#29616;&#20195;&#32763;&#35793;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;eBible&#35821;&#26009;&#24211;&#65306;&#19968;&#20010;&#21253;&#21547;1009&#20010;&#22307;&#32463;&#37096;&#20998;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;833&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#65292;&#20998;&#24067;&#22312;75&#20010;&#35821;&#35328;&#23478;&#26063;&#20013;&#12290;&#38500;&#20102;&#22307;&#32463;&#32763;&#35793;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#22522;&#20110;No Language Left Behind&#65288;NLLB&#65289;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#30340;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22307;&#32463;&#32763;&#35793;&#39046;&#22495;&#29305;&#26377;&#30340;&#33509;&#24178;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#24050;&#24314;&#31435;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#22522;&#20934;&#22914;&#20309;&#29992;&#20110;&#26410;&#26469;&#30340;&#32763;&#35793;&#24037;&#20316;&#12290;&#23545;&#20110;&#20351;&#29992;NLLB&#36827;&#34892;&#35757;&#32451;&#30340;BT&#20219;&#21153;&#65292;&#21335;&#23707;&#21644;&#26032;&#20960;&#20869;&#20122;&#20256;&#36755;&#35821;&#31995;&#30340;&#35821;&#35328;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#21360;&#27431;&#35821;&#31995;&#21644;&#38750;&#27954;&#20122;&#27954;&#35821;&#31995;&#30340;&#35821;&#35328;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently and accurately translating a corpus into a low-resource language remains a challenge, regardless of the strategies employed, whether manual, automated, or a combination of the two. Many Christian organizations are dedicated to the task of translating the Holy Bible into languages that lack a modern translation. Bible translation (BT) work is currently underway for over 3000 extremely low resource languages. We introduce the eBible corpus: a dataset containing 1009 translations of portions of the Bible with data in 833 different languages across 75 language families. In addition to a BT benchmarking dataset, we introduce model performance benchmarks built on the No Language Left Behind (NLLB) neural machine translation (NMT) models. Finally, we describe several problems specific to the domain of BT and consider how the established data and model benchmarks might be used for future translation efforts. For a BT task trained with NLLB, Austronesian and Trans-New Guinea languag
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MARS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24369;&#30417;&#30563;&#25968;&#25454;&#20013;&#33258;&#21160;&#21435;&#38500;&#26377;&#20559;&#23545;&#35937;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2304.09913</link><description>&lt;p&gt;
MARS: &#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#36827;&#34892;&#27169;&#22411;&#26080;&#20851;&#30340;&#26377;&#20559;&#23545;&#35937;&#21435;&#38500;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation. (arXiv:2304.09913v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09913
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MARS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24369;&#30417;&#30563;&#25968;&#25454;&#20013;&#33258;&#21160;&#21435;&#38500;&#26377;&#20559;&#23545;&#35937;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24369;&#30417;&#30563;&#65288;&#22914;&#22270;&#20687;&#32423;&#21035;&#31867;&#21035;&#26631;&#31614;&#65289;&#35757;&#32451;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#26469;&#38477;&#20302;&#26631;&#31614;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#20135;&#29983;&#20934;&#30830;&#30340;&#23450;&#20301;&#22270;&#21644;&#22312;&#31867;&#30456;&#20851;&#32972;&#26223;&#65288;&#22914;&#26816;&#27979;&#20855;&#26377;&#28779;&#36710;&#31867;&#30340;&#38081;&#36335;&#65289;&#20013;&#36973;&#21463;&#20551;&#39044;&#27979;&#30340;&#26377;&#20559;&#23545;&#35937;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290; &#26368;&#36817;&#30340;&#21435;&#38500;&#20559;&#35265;&#23545;&#35937;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#30417;&#30563;&#20197;&#25163;&#21160;&#35782;&#21035;&#27599;&#20010;&#26377;&#38382;&#39064;&#30340;&#31867;&#30340;&#26377;&#20559;&#23545;&#35937;&#24182;&#36890;&#36807;&#23457;&#26597;&#39044;&#27979;&#25910;&#38598;&#23427;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#22810;&#20010;&#26631;&#31614;&#21644;&#22797;&#26434;&#20851;&#31995;&#20197;&#20559;&#20506;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#31532;&#19968;&#20010;&#35266;&#23519;&#21040;&#20559;&#35265;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#23558;&#20559;&#35265;&#23545;&#35937;&#19982;&#30456;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#32972;&#26223;&#21305;&#37197;&#26469;&#20998;&#31163;&#21644;&#28040;&#38500;&#30340;&#35266;&#23519;&#20043;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;/&#27169;&#22411;&#26080;&#20851;&#30340;&#26377;&#20559;&#21435;&#38500;&#26694;&#26550;MARS&#65288;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#26377;&#20559;&#23545;&#35937;&#21435;&#38500;&#65289;&#65292;&#23427;&#21033;&#29992;&#24369;&#30417;&#30563;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20559;&#20506;&#27169;&#24335;&#20197;&#26377;&#25928;&#21435;&#38500;&#26377;&#20559;&#23545;&#35937;&#65292;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised semantic segmentation aims to reduce labeling costs by training semantic segmentation models using weak supervision, such as image-level class labels. However, most approaches struggle to produce accurate localization maps and suffer from false predictions in class-related backgrounds (i.e., biased objects), such as detecting a railroad with the train class. Recent methods that remove biased objects require additional supervision for manually identifying biased objects for each problematic class and collecting their datasets by reviewing predictions, limiting their applicability to the real-world dataset with multiple labels and complex relationships for biasing. Following the first observation that biased features can be separated and eliminated by matching biased objects with backgrounds in the same dataset, we propose a fully-automatic/model-agnostic biased removal framework called MARS (Model-Agnostic biased object Removal without additional Supervision), which ut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.09875</link><description>&lt;p&gt;
GREAT&#20998;&#25968;&#65306;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models. (arXiv:2304.09875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#20110;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32858;&#21512;&#19968;&#32452;&#25968;&#25454;&#26679;&#26412;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#32467;&#26524;&#19978;&#65292;&#20197;&#35780;&#20272;&#21644;&#25490;&#21517;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23616;&#37096;&#32479;&#35745;&#37327;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#20195;&#34920;&#22522;&#30784;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#30495;&#27491;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;GREAT&#20998;&#25968;&#27491;&#24335;&#20855;&#26377;&#19968;&#20010;&#20840;&#23616;&#32479;&#35745;&#37327;&#30340;&#29289;&#29702;&#24847;&#20041;&#65292;&#25429;&#25417;&#26469;&#33258;&#29983;&#25104;&#27169;&#22411;&#30340;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#12290;&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26679;&#26412;&#22343;&#20540;&#19982;&#30495;&#23454;&#22343;&#20540;&#20043;&#38388;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;GREAT&#20998;&#25968;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#20351;&#29992;GREAT&#20998;&#25968;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#39640;&#25928;&#32780;&#19988;&#35268;&#27169;&#21487;&#25193;&#23637;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current studies on adversarial robustness mainly focus on aggregating local robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true global robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called GREAT Score , for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;ChatGPT&#20316;&#20026;&#24515;&#29702;&#27835;&#30103;&#24072;&#30340;&#21161;&#25163;&#65292;&#21487;&#20197;&#31215;&#26497;&#21442;&#19982;&#23545;&#35805;&#12289;&#25972;&#29702;&#20449;&#24687;&#12289;&#25552;&#20379;&#39564;&#35777;&#21644;&#28508;&#22312;&#24212;&#23545;&#31574;&#30053;&#65292;&#24182;&#24110;&#21161;&#27835;&#30103;&#24072;&#20174;&#22810;&#27425;&#23545;&#35805;&#20013;&#21457;&#29616;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.09873</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#27835;&#30103;&#24072;&#21161;&#25163;&#30340;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ChatGPT as a Therapist Assistant: A Suitability Study. (arXiv:2304.09873v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;ChatGPT&#20316;&#20026;&#24515;&#29702;&#27835;&#30103;&#24072;&#30340;&#21161;&#25163;&#65292;&#21487;&#20197;&#31215;&#26497;&#21442;&#19982;&#23545;&#35805;&#12289;&#25972;&#29702;&#20449;&#24687;&#12289;&#25552;&#20379;&#39564;&#35777;&#21644;&#28508;&#22312;&#24212;&#23545;&#31574;&#30053;&#65292;&#24182;&#24110;&#21161;&#27835;&#30103;&#24072;&#20174;&#22810;&#27425;&#23545;&#35805;&#20013;&#21457;&#29616;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;ChatGPT&#20316;&#20026;&#24515;&#29702;&#27835;&#30103;&#24072;&#30340;&#21161;&#25163;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;ChatGPT&#21487;&#20197;&#20316;&#20026;&#24739;&#32773;&#20449;&#24687;&#30340;&#25910;&#38598;&#22120;&#65292;&#22312;&#27835;&#30103;&#20250;&#35805;&#20043;&#38388;&#38506;&#20276;&#24739;&#32773;&#65292;&#24182;&#25972;&#29702;&#25910;&#38598;&#30340;&#20449;&#24687;&#20379;&#27835;&#30103;&#24072;&#20351;&#29992;&#65292;&#20419;&#36827;&#27835;&#30103;&#36807;&#31243;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#20116;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;&#29992;&#20110;&#24494;&#35843;&#21161;&#25163;&#30340;&#26377;&#29992;&#25552;&#31034;&#65292;&#36825;&#34920;&#26126;ChatGPT&#21487;&#20197;&#31215;&#26497;&#21442;&#19982;&#23545;&#35805;&#65292;&#32838;&#21548;&#20840;&#31070;&#36143;&#27880;&#65292;&#25552;&#20379;&#39564;&#35777;&#21644;&#28508;&#22312;&#24212;&#23545;&#31574;&#30053;&#65292;&#32780;&#19981;&#25552;&#20379;&#26126;&#30830;&#30340;&#21307;&#30103;&#24314;&#35758;&#65292;&#24182;&#24110;&#21161;&#27835;&#30103;&#24072;&#20174;&#22810;&#27425;&#19982;&#21516;&#19968;&#24739;&#32773;&#30340;&#23545;&#35805;&#20013;&#21457;&#29616;&#26032;&#30340;&#35265;&#35299;&#12290;&#23558;ChatGPT&#29992;&#20316;&#24515;&#29702;&#27835;&#30103;&#24072;&#30340;&#21161;&#25163;&#23384;&#22312;&#22810;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25216;&#26415;&#21644;&#20197;&#20154;&#20026;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes using ChatGPT, an innovative technology with various applications, as an assistant for psychotherapy. ChatGPT can serve as a patient information collector, a companion for patients in between therapy sessions, and an organizer of gathered information for therapists to facilitate treatment processes. The research identifies five research questions and discovers useful prompts for fine-tuning the assistant, which shows that ChatGPT can participate in positive conversations, listen attentively, offer validation and potential coping strategies without providing explicit medical advice, and help therapists discover new insights from multiple conversations with the same patient. Using ChatGPT as an assistant for psychotherapy poses several challenges that need to be addressed, including technical as well as human-centric challenges which are discussed.
&lt;/p&gt;</description></item><item><title>Adam&#20248;&#21270;&#31639;&#27861;&#22312;&#22823;&#25209;&#37327;&#30340;&#35757;&#32451;&#19979;&#23481;&#26131;&#20986;&#29616;&#19981;&#31283;&#23450;&#29616;&#35937;&#65292;&#24182;&#23548;&#33268;&#35757;&#32451;&#24322;&#24120;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35813;&#29616;&#35937;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.09871</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;Adam&#19981;&#31283;&#23450;&#24615;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Theory on Adam Instability in Large-Scale Machine Learning. (arXiv:2304.09871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09871
&lt;/p&gt;
&lt;p&gt;
Adam&#20248;&#21270;&#31639;&#27861;&#22312;&#22823;&#25209;&#37327;&#30340;&#35757;&#32451;&#19979;&#23481;&#26131;&#20986;&#29616;&#19981;&#31283;&#23450;&#29616;&#35937;&#65292;&#24182;&#23548;&#33268;&#35757;&#32451;&#24322;&#24120;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35813;&#29616;&#35937;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20043;&#21069;&#26410;&#34987;&#35299;&#37322;&#30340;&#29616;&#35937;&#30340;&#29702;&#35770;&#65292;&#35813;&#29616;&#35937;&#20986;&#29616;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#21457;&#25955;&#34892;&#20026;&#20013;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#20027;&#27969;&#30340;&#20248;&#21270;&#31639;&#27861; Adam &#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040; Adam &#21487;&#33021;&#20250;&#36827;&#20837;&#19968;&#31181;&#29366;&#24577;&#65292;&#20854;&#20013;&#21442;&#25968;&#26356;&#26032;&#21521;&#37327;&#26377;&#27604;&#36739;&#22823;&#30340;&#33539;&#25968;&#65292;&#24182;&#19988;&#19982;&#35757;&#32451;&#25439;&#22833;&#26223;&#35266;&#19979;&#30340;&#19979;&#38477;&#26041;&#21521;&#22522;&#26412;&#26080;&#20851;&#65292;&#20174;&#32780;&#23548;&#33268;&#21457;&#25955;&#12290;&#36825;&#31181;&#29616;&#35937;&#26356;&#23481;&#26131;&#22312;&#22823;&#25209;&#37327;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#36825;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20856;&#22411;&#35774;&#32622;&#12290;&#20026;&#20102;&#35777;&#26126;&#35813;&#29702;&#35770;&#65292;&#25105;&#20204;&#23545;&#35268;&#27169;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;70&#20159;&#65292;300&#20159;&#65292;650&#20159;&#21644;5460&#20159;&#21442;&#25968;&#65289;&#36827;&#34892;&#20102;&#35757;&#32451;&#36816;&#34892;&#30340;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theory for the previously unexplained divergent behavior noticed in the training of large language models. We argue that the phenomenon is an artifact of the dominant optimization algorithm used for training, called Adam. We observe that Adam can enter a state in which the parameter update vector has a relatively large norm and is essentially uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This artifact is more likely to be observed in the training of a deep model with a large batch size, which is the typical setting of large-scale language model training. To argue the theory, we present observations from the training runs of the language models of different scales: 7 billion, 30 billion, 65 billion, and 546 billion parameters.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#38480;&#21046;&#65292;&#21516;&#26102;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#65292;&#24314;&#31435;&#20102;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#21450;&#20854;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335; HATRPO &#21644; HAPPO&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09870</link><description>&lt;p&gt;
&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous-Agent Reinforcement Learning. (arXiv:2304.09870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#38480;&#21046;&#65292;&#21516;&#26102;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#65292;&#24314;&#31435;&#20102;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#21450;&#20854;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335; HATRPO &#21644; HAPPO&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21442;&#25968;&#20849;&#20139;&#65292;&#36825;&#23558;&#23427;&#20204;&#38480;&#21046;&#22312;&#21516;&#36136;&#24322;&#26500;&#26234;&#33021;&#20307;&#35774;&#32622;&#19979;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#32570;&#20047;&#25910;&#25947;&#20445;&#35777;&#12290;&#20026;&#20102;&#22312;&#19968;&#33324;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#35774;&#32622;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26680;&#24515;&#26159;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#12290;&#22522;&#20110;&#36825;&#20123;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32463;&#36807;&#39564;&#35777;&#30340;&#26080;&#21442;&#25968;&#20849;&#20139;&#32422;&#26463;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335;&#24471;&#20986;&#20102;HATRPO&#21644;HAPPO&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL) that is free of parameter-sharing constraint, and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#38543;&#26426;&#25490;&#21517;&#33258;&#36866;&#24212;&#24179;&#34913;&#22870;&#21169;&#21644;&#32422;&#26463;&#36829;&#35268;&#65292;&#24182;&#21516;&#26102;&#36890;&#36807;&#32500;&#25252;&#19968;&#32452;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#31995;&#25968;&#21450;&#19968;&#20010;&#32422;&#26463;&#32531;&#20914;&#21306;&#26469;&#38480;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09869</link><description>&lt;p&gt;
&#36827;&#21270;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Evolving Constrained Reinforcement Learning Policy. (arXiv:2304.09869v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#38543;&#26426;&#25490;&#21517;&#33258;&#36866;&#24212;&#24179;&#34913;&#22870;&#21169;&#21644;&#32422;&#26463;&#36829;&#35268;&#65292;&#24182;&#21516;&#26102;&#36890;&#36807;&#32500;&#25252;&#19968;&#32452;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#31995;&#25968;&#21450;&#19968;&#20010;&#32422;&#26463;&#32531;&#20914;&#21306;&#26469;&#38480;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#24050;&#34987;&#29992;&#20110;&#28436;&#21270;&#20986;&#19968;&#32452;&#25191;&#34892;&#32773;&#65292;&#20197;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#20307;&#39564;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#20174;&#32780;&#35299;&#20915;&#26102;&#38388;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#24182;&#25552;&#39640;&#25506;&#32034;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#32422;&#26463;&#38382;&#39064;&#26102;&#65292;&#24456;&#38590;&#24179;&#34913;&#22870;&#21169;&#21644;&#32422;&#26463;&#36829;&#35268;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#65288;ECRL&#65289;&#31639;&#27861;&#65292;&#37319;&#29992;&#38543;&#26426;&#25490;&#21517;&#33258;&#36866;&#24212;&#24179;&#34913;&#22870;&#21169;&#21644;&#32422;&#26463;&#36829;&#35268;&#65292;&#24182;&#21516;&#26102;&#36890;&#36807;&#32500;&#25252;&#19968;&#32452;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#31995;&#25968;&#21450;&#19968;&#20010;&#32422;&#26463;&#32531;&#20914;&#21306;&#26469;&#38480;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ECRL&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#28040;&#34701;&#20998;&#26512;&#34920;&#26126;&#24341;&#20837;&#38543;&#26426;&#25490;&#21517;&#21644;&#32422;&#26463;&#32531;&#20914;&#21306;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms have been used to evolve a population of actors to generate diverse experiences for training reinforcement learning agents, which helps to tackle the temporal credit assignment problem and improves the exploration efficiency. However, when adapting this approach to address constrained problems, balancing the trade-off between the reward and constraint violation is hard. In this paper, we propose a novel evolutionary constrained reinforcement learning (ECRL) algorithm, which adaptively balances the reward and constraint violation with stochastic ranking, and at the same time, restricts the policy's behaviour by maintaining a set of Lagrange relaxation coefficients with a constraint buffer. Extensive experiments on robotic control benchmarks show that our ECRL achieves outstanding performance compared to state-of-the-art algorithms. Ablation analysis shows the benefits of introducing stochastic ranking and constraint buffer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.09868</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?. (arXiv:2304.09868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20294;&#26159;&#30001;&#20110;&#20854;&#35745;&#31639;&#26114;&#36149;&#30340;&#31751;&#20998;&#37197;&#27493;&#39588;&#65292;&#23427;&#38754;&#20020;&#30528;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#21387;&#32553;&#25104;&#23569;&#37327;&#35889;&#34920;&#31034;&#30340;&#32858;&#21512;&#25968;&#25454;&#28857;&#65292;&#28982;&#21518;&#22312;&#21387;&#32553;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#26631;&#20934;&#30340;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#26368;&#21518;&#23558;&#21387;&#32553;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#32467;&#26524;&#26144;&#23556;&#22238;&#21407;&#22987;&#25968;&#25454;&#38598;&#20197;&#21457;&#29616;&#31751;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support vector clustering is an important clustering method. However, it suffers from a scalability issue due to its computational expensive cluster assignment step. In this paper we accelertate the support vector clustering via spectrum-preserving data compression. Specifically, we first compress the original data set into a small amount of spectrally representative aggregated data points. Then, we perform standard support vector clustering on the compressed data set. Finally, we map the clustering results of the compressed data set back to discover the clusters in the original data set. Our extensive experimental results on real-world data set demonstrate dramatically speedups over standard support vector clustering without sacrificing clustering quality.
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#35770;&#20026;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24314;&#26500;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#22810;&#20803;&#30340;&#35270;&#35282;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#19982;&#27495;&#35270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09867</link><description>&lt;p&gt;
&#24341;&#20837;&#26500;&#24314;&#35770;&#20316;&#20026;&#21253;&#23481;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26631;&#20934;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Introducing Construct Theory as a Standard Methodology for Inclusive AI Models. (arXiv:2304.09867v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09867
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#35770;&#20026;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24314;&#26500;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#22810;&#20803;&#30340;&#35270;&#35282;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#19982;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#24515;&#29702;&#23398;&#23478;George Kelly&#24320;&#21457;&#30340;&#26500;&#24314;&#35770;&#26159;&#19968;&#31181;&#29992;&#26469;&#39044;&#27979;&#21644;&#39044;&#26399;&#20107;&#20214;&#30340;&#24515;&#29702;&#26500;&#24314;&#12290;&#26500;&#24314;&#26159;&#20154;&#31867;&#35299;&#37322;&#12289;&#31579;&#36873;&#12289;&#39044;&#27979;&#21644;&#39564;&#35777;&#25968;&#25454;&#21644;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30446;&#21069;AI&#23384;&#22312;&#20559;&#35265;&#65292;&#22240;&#20026;&#23427;&#26159;&#26681;&#25454;&#35757;&#32451;&#25968;&#25454;&#26631;&#31614;&#25152;&#23450;&#20041;&#30340;&#29421;&#31364;&#26500;&#24314;&#26469;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#30446;&#21069;&#20154;&#33080;&#35782;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#30528;&#23545;&#36739;&#26263;&#32932;&#33394;&#30340;&#20154;&#30340;&#27495;&#35270;&#65292;&#24182;&#19988;&#22312;Buolamwini&#12289;Joy &#21644; Timnit Gebru&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#35770;&#25991;&#12298;Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification&#12299;&#20013;&#65292;&#25552;&#20986;&#20102;&#37319;&#29992;&#34920;&#22411;&#26631;&#31614;&#20316;&#20026;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26500;&#24314;&#35770;&#20013;&#65292;&#34920;&#22411;&#21482;&#26159;&#26500;&#24314;&#20986;&#33080;&#37096;&#29305;&#24449;&#30340;&#35768;&#22810;&#23376;&#20803;&#32032;&#20043;&#19968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#33080;&#37096;&#26500;&#24314;&#30340;15&#20010;&#20027;&#35201;&#20803;&#32032;&#65292;50&#20010;&#23376;&#20803;&#32032;&#65292;&#24182;&#20351;&#29992;&#30446;&#21069;&#21253;&#25324;7&#20010;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#24180;&#40836;&#25968;&#25454;&#30340;FairFace&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;Google Cloud Vision API&#21644;Microsoft Cognitive Services API&#12290;
&lt;/p&gt;
&lt;p&gt;
Construct theory in social psychology, developed by George Kelly are mental constructs to predict and anticipate events. Constructs are how humans interpret, curate, predict and validate data; information. AI today is biased because it is trained with a narrow construct as defined by the training data labels. Machine Learning algorithms for facial recognition discriminate against darker skin colors and in the ground breaking research papers (Buolamwini, Joy and Timnit Gebru. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. FAT (2018), the inclusion of phenotypic labeling is proposed as a viable solution. In Construct theory, phenotype is just one of the many subelements that make up the construct of a face. In this paper, we present 15 main elements of the construct of face, with 50 subelements and tested Google Cloud Vision API and Microsoft Cognitive Services API using FairFace dataset that currently has data for 7 races, genders and ages, and w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#35299;&#20915;&#32769;&#24180;&#20154;&#30340;&#23396;&#29420;&#21644;&#31038;&#20132;&#23396;&#31435;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#23545;&#35805;&#20276;&#20387;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#21021;&#27493;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#29983;&#25104;&#19982;&#32769;&#24180;&#20154;&#30456;&#20851;&#30340;&#22238;&#24212;&#65292;&#20294;&#38656;&#27880;&#24847;&#20854;&#23616;&#38480;&#24615;&#21644;&#20262;&#29702;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.09866</link><description>&lt;p&gt;
&#38754;&#21521;&#32769;&#24180;&#20154;&#30340;ChatGPT&#23545;&#35805;&#20276;&#20387;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Towards Designing a ChatGPT Conversational Companion for Elderly People. (arXiv:2304.09866v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#35299;&#20915;&#32769;&#24180;&#20154;&#30340;&#23396;&#29420;&#21644;&#31038;&#20132;&#23396;&#31435;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#23545;&#35805;&#20276;&#20387;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#21021;&#27493;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#29983;&#25104;&#19982;&#32769;&#24180;&#20154;&#30456;&#20851;&#30340;&#22238;&#24212;&#65292;&#20294;&#38656;&#27880;&#24847;&#20854;&#23616;&#38480;&#24615;&#21644;&#20262;&#29702;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23396;&#29420;&#21644;&#31038;&#20132;&#23396;&#31435;&#26159;&#32769;&#24180;&#20154;&#26222;&#36941;&#23384;&#22312;&#30340;&#20005;&#37325;&#38382;&#39064;&#65292;&#20250;&#24433;&#21709;&#20854;&#36523;&#20307;&#21644;&#24515;&#29702;&#20581;&#24247;&#12289;&#29983;&#27963;&#36136;&#37327;&#21644;&#23551;&#21629;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#32769;&#24180;&#20154;&#23545;&#35805;&#20276;&#20387;&#31995;&#32479;&#65292;&#26088;&#22312;&#20026;&#32769;&#24180;&#20154;&#25552;&#20379;&#38506;&#20276;&#65292;&#24182;&#24110;&#21161;&#20943;&#23569;&#23396;&#29420;&#21644;&#31038;&#20132;&#23396;&#31435;&#24863;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#21021;&#27493;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#29983;&#25104;&#19982;&#21019;&#24314;&#30340;&#32769;&#24180;&#20154;&#24418;&#35937;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;ChatGPT&#30340;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#28508;&#22312;&#30340;&#20559;&#35265;&#21644;&#19981;&#20934;&#30830;&#24615;&#65292;&#24182;&#32771;&#34385;&#20351;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20276;&#20387;&#31995;&#32479;&#23545;&#32769;&#24180;&#20154;&#30340;&#20262;&#29702;&#24433;&#21709;&#65292;&#21253;&#25324;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loneliness and social isolation are serious and widespread problems among older people, affecting their physical and mental health, quality of life, and longevity. In this paper, we propose a ChatGPT-based conversational companion system for elderly people. The system is designed to provide companionship and help reduce feelings of loneliness and social isolation. The system was evaluated with a preliminary study. The results showed that the system was able to generate responses that were relevant to the created elderly personas. However, it is essential to acknowledge the limitations of ChatGPT, such as potential biases and misinformation, and to consider the ethical implications of using AI-based companionship for the elderly, including privacy concerns.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#22411;AI&#31995;&#32479;&#31649;&#29702;&#23545;&#29992;&#25143;&#20307;&#39564;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24230;&#30340;&#31649;&#29702;&#21487;&#20197;&#25552;&#39640;&#29992;&#25143;&#30041;&#23384;&#29575;&#65292;&#35777;&#26126;&#20102;&#37319;&#29992;AI&#23433;&#20840;&#26426;&#21046;&#26500;&#24314;&#33021;&#35753;&#29992;&#25143;&#21916;&#24742;&#30340;&#23545;&#35805;&#22411;AI&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09865</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#23545;&#35805;&#22411;AI&#20316;&#20026;&#29992;&#25143;&#21916;&#24742;&#30340;&#28304;&#27849;
&lt;/p&gt;
&lt;p&gt;
Safer Conversational AI as a Source of User Delight. (arXiv:2304.09865v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#22411;AI&#31995;&#32479;&#31649;&#29702;&#23545;&#29992;&#25143;&#20307;&#39564;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24230;&#30340;&#31649;&#29702;&#21487;&#20197;&#25552;&#39640;&#29992;&#25143;&#30041;&#23384;&#29575;&#65292;&#35777;&#26126;&#20102;&#37319;&#29992;AI&#23433;&#20840;&#26426;&#21046;&#26500;&#24314;&#33021;&#35753;&#29992;&#25143;&#21916;&#24742;&#30340;&#23545;&#35805;&#22411;AI&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31649;&#29702;&#23545;&#35805;&#22411;AI&#31995;&#32479;&#23545;&#29992;&#25143;&#20307;&#39564;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#36817;&#26399;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24050;&#32463;&#20351;&#24471;&#23545;&#35805;&#22411;AI&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23545;AI&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#20197;&#21450;&#38656;&#35201;&#23545;&#31995;&#32479;&#36827;&#34892;&#31649;&#29702;&#20197;&#40723;&#21169;&#23433;&#20840;&#29992;&#35821;&#21644;&#38450;&#27490;&#21361;&#23475;&#30340;&#38656;&#27714;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#29992;&#25143;&#35748;&#20026;&#24403;&#21069;&#30340;&#31649;&#29702;&#26041;&#27861;&#38480;&#21046;&#20102;&#25216;&#26415;&#30340;&#21457;&#23637;&#12289;&#22949;&#21327;&#20102;&#33258;&#30001;&#34920;&#36798;&#24182;&#19988;&#38480;&#21046;&#20102;&#25216;&#26415;&#25152;&#25552;&#20379;&#30340;&#20215;&#20540;&#12290;&#26412;&#30740;&#31350;&#37319;&#21462;&#23458;&#35266;&#30340;&#31435;&#22330;&#24182;&#26174;&#31034;&#20986;&#31649;&#29702;&#24182;&#19981;&#19968;&#23450;&#20250;&#24433;&#21709;&#21040;&#29992;&#25143;&#20307;&#39564;&#12290;&#31895;&#26292;&#30340;&#31649;&#29702;&#26041;&#24335;&#30830;&#23454;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#20294;&#26159;&#36890;&#36807;&#31649;&#29702;&#30340;&#26041;&#27861;&#20351;AI&#26356;&#23433;&#20840;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#23545;&#35805;&#22411;AI&#37096;&#32626;&#22312;Chai&#24179;&#21488;&#20013;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#36866;&#24230;&#31649;&#29702;&#21644;&#23433;&#20840;&#31995;&#32479;&#35774;&#35745;&#21487;&#20197;&#25552;&#39640;&#29992;&#25143;&#30041;&#23384;&#29575;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#37319;&#29992;AI&#23433;&#20840;&#26426;&#21046;&#26500;&#24314;&#33021;&#35753;&#29992;&#25143;&#21916;&#24742;&#30340;&#23545;&#35805;&#22411;AI&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores the impact of moderation on users' enjoyment of conversational AI systems. While recent advancements in Large Language Models (LLMs) have led to highly capable conversational AIs that are increasingly deployed in real-world settings, there is a growing concern over AI safety and the need to moderate systems to encourage safe language and prevent harm. However, some users argue that current approaches to moderation limit the technology, compromise free expression, and limit the value delivered by the technology. This study takes an unbiased stance and shows that moderation does not necessarily detract from user enjoyment. Heavy handed moderation does seem to have a nefarious effect, but models that are moderated to be safer can lead to a better user experience. By deploying various conversational AIs in the Chai platform, the study finds that user retention can increase with a level of moderation and safe system design. These results demonstrate the importance of appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#32456;&#31471;&#29992;&#25143;&#24320;&#21457;&#65288;EUD&#65289;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#30446;&#30340;&#26159;&#20351;&#38750;&#25216;&#26415;&#29992;&#25143;&#30452;&#25509;&#20197;&#28385;&#36275;&#20854;&#38656;&#27714;&#30340;&#26041;&#24335;&#21442;&#19982;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#23450;&#20041;&#21644;&#20010;&#24615;&#21270;&#20013;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35780;&#20272;&#20102;EUD&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#28508;&#22312;&#30340;&#22909;&#22788;&#20197;&#21450;&#23558;&#20854;&#25972;&#21512;&#21040;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20013;&#30340;&#26410;&#26469;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.09863</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#32456;&#31471;&#29992;&#25143;&#24320;&#21457;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
End-User Development for Artificial Intelligence: A Systematic Literature Review. (arXiv:2304.09863v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#32456;&#31471;&#29992;&#25143;&#24320;&#21457;&#65288;EUD&#65289;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#30446;&#30340;&#26159;&#20351;&#38750;&#25216;&#26415;&#29992;&#25143;&#30452;&#25509;&#20197;&#28385;&#36275;&#20854;&#38656;&#27714;&#30340;&#26041;&#24335;&#21442;&#19982;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#23450;&#20041;&#21644;&#20010;&#24615;&#21270;&#20013;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35780;&#20272;&#20102;EUD&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#28508;&#22312;&#30340;&#22909;&#22788;&#20197;&#21450;&#23558;&#20854;&#25972;&#21512;&#21040;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20013;&#30340;&#26410;&#26469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#25105;&#20204;&#31038;&#20250;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20960;&#20046;&#24635;&#26159;IT&#21644;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#19987;&#23646;&#26435;&#21033;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#32456;&#31471;&#29992;&#25143;&#24320;&#21457;&#65288;EUD&#65289;&#65292;&#20351;&#24471;&#38750;&#25216;&#26415;&#20154;&#21592;&#21487;&#20197;&#30452;&#25509;&#21442;&#19982;&#23450;&#20041;&#21644;&#20010;&#24615;&#21270;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#24378;AI&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#38416;&#36848;&#20102;&#24403;&#21069;EUD&#23545;AI&#31995;&#32479;&#30340;&#21457;&#23637;&#26684;&#23616;&#65292;&#21363;&#22914;&#20309;&#20351;&#29992;&#25143;&#22312;&#27809;&#26377;&#20154;&#24037;&#26234;&#33021;&#21644;/&#25110;&#32534;&#31243;&#25216;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#23450;&#21046;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#26469;&#28385;&#36275;&#20182;&#20204;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;EUD&#38754;&#23545;&#30340;&#24403;&#21069;&#25361;&#25112;&#12289;&#28508;&#22312;&#30340;&#22909;&#22788;&#20197;&#21450;&#23558;EUD&#25972;&#21512;&#21040;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#26410;&#26469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Artificial Intelligence has become more and more relevant in our society. Creating AI systems is almost always the prerogative of IT and AI experts. However, users may need to create intelligent solutions tailored to their specific needs. In this way, AI systems can be enhanced if new approaches are devised to allow non-technical users to be directly involved in the definition and personalization of AI technologies. End-User Development (EUD) can provide a solution to these problems, allowing people to create, customize, or adapt AI-based systems to their own needs. This paper presents a systematic literature review that aims to shed the light on the current landscape of EUD for AI systems, i.e., how users, even without skills in AI and/or programming, can customize the AI behavior to their needs. This study also discusses the current challenges of EUD for AI, the potential benefits, and the future implications of integrating EUD into the overall AI development process
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NRTS&#30340;Android&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#26032;&#29983;&#20799;&#22797;&#33487;&#27169;&#25311;&#35838;&#31243;&#26399;&#38388;&#35760;&#24405;&#30340;&#25152;&#26377;&#25968;&#25454;&#20174;&#21307;&#38498;&#30340;&#26032;&#29983;&#20799;&#37325;&#30151;&#30417;&#25252;&#23460;&#21457;&#36865;&#21040;&#20301;&#20110;&#24847;&#22823;&#21033;&#30382;&#22467;&#33945;&#29305;&#19996;&#37096;&#22823;&#23398;&#30340;&#26381;&#21153;&#22120;&#19978;&#12290;&#21307;&#23398;&#25945;&#32451;&#21487;&#20197;&#20351;&#29992;&#35813;&#24212;&#29992;&#31243;&#24207;&#26597;&#30475;&#26377;&#20851;&#27169;&#25311;&#32451;&#20064;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20197;&#23545;&#28041;&#21450;&#27169;&#25311;&#22330;&#26223;&#30340;&#22810;&#23398;&#31185;&#22242;&#38431;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.09860</link><description>&lt;p&gt;
NRTS&#65306;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#26032;&#29983;&#20799;&#22797;&#33487;&#27169;&#25311;&#22330;&#26223;&#20013;&#22810;&#23398;&#31185;&#22242;&#38431;&#25968;&#25454;&#35760;&#24405;&#12289;&#20256;&#36755;&#21644;&#35780;&#20272;&#30340;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
NRTS: A Client-Server architecture for supporting data recording, transmission and evaluation of multidisciplinary teams during the neonatal resuscitation simulation scenario. (arXiv:2304.09860v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09860
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NRTS&#30340;Android&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#26032;&#29983;&#20799;&#22797;&#33487;&#27169;&#25311;&#35838;&#31243;&#26399;&#38388;&#35760;&#24405;&#30340;&#25152;&#26377;&#25968;&#25454;&#20174;&#21307;&#38498;&#30340;&#26032;&#29983;&#20799;&#37325;&#30151;&#30417;&#25252;&#23460;&#21457;&#36865;&#21040;&#20301;&#20110;&#24847;&#22823;&#21033;&#30382;&#22467;&#33945;&#29305;&#19996;&#37096;&#22823;&#23398;&#30340;&#26381;&#21153;&#22120;&#19978;&#12290;&#21307;&#23398;&#25945;&#32451;&#21487;&#20197;&#20351;&#29992;&#35813;&#24212;&#29992;&#31243;&#24207;&#26597;&#30475;&#26377;&#20851;&#27169;&#25311;&#32451;&#20064;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20197;&#23545;&#28041;&#21450;&#27169;&#25311;&#22330;&#26223;&#30340;&#22810;&#23398;&#31185;&#22242;&#38431;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#26032;&#29983;&#20799;&#22797;&#33487;&#22521;&#35757;&#27169;&#25311;&#22120;&#65288;NRTS&#65289;&#65292;&#19968;&#27454;&#26088;&#22312;&#25903;&#25345;&#21307;&#23398;&#19987;&#23478;&#22312;&#26032;&#29983;&#20799;&#22797;&#33487;&#30340;&#39640;&#20445;&#30495;&#27169;&#25311;&#35838;&#31243;&#20013;&#36755;&#20837;&#12289;&#20256;&#36755;&#21644;&#35760;&#24405;&#25968;&#25454;&#30340;Android&#31227;&#21160;&#24212;&#29992;&#12290;&#35813;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20801;&#35768;&#33258;&#21160;&#21457;&#36865;&#25152;&#26377;&#35760;&#24405;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#26469;&#33258;&#24847;&#22823;&#21033;Casale Monferrato&#20799;&#31461;&#21307;&#38498;&#30340;&#26032;&#29983;&#20799;&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;NICU&#65289;&#30340;&#25968;&#25454;&#65292;&#21040;&#20301;&#20110;&#24847;&#22823;&#21033;&#30382;&#22467;&#33945;&#29305;&#19996;&#37096;&#22823;&#23398;&#31185;&#23398;&#19982;&#25216;&#26415;&#21019;&#26032;&#31995;&#65288;DiSIT&#65289;&#30340;&#26381;&#21153;&#22120;&#19978;&#12290;&#26368;&#21518;&#65292;&#21307;&#23398;&#25945;&#32451;&#21487;&#20197;&#22312;&#27169;&#25311;&#28436;&#20064;&#26399;&#38388;&#26597;&#30475;&#32479;&#35745;&#20449;&#24687;&#65292;&#36825;&#20123;&#32479;&#35745;&#20449;&#24687;&#21487;&#29992;&#20110;&#22810;&#23398;&#31185;&#22242;&#38431;&#22312;&#27169;&#25311;&#22330;&#26223;&#20013;&#30340;&#35780;&#20272;&#21644;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical report, we describe Neonatal Resuscitation Training Simulator (NRTS), an Android mobile app designed to support medical experts to input, transmit and record data during a High-Fidelity Simulation course for neonatal resuscitation. This mobile app allows one to automatically send all the recorded data from "Neonatal Intensive Care Unit" (NICU) of Casale Monferrato Children's Hospital, (Italy) to a server located at the Department of Science and Technological Innovation (DiSIT), University of Piemonte Orientale (Italy). Finally, the medical instructor can view statistics on a simulation exercise that may be used during the de-briefing phase for the evaluation of multidisciplinary teams involved in the simulation scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#21450;&#22522;&#20110;&#32844;&#19994;&#30693;&#35782;&#22270;&#35889;&#24320;&#21457;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#39044;&#27979;&#20102;ChatGPT&#23545;&#26410;&#26469;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#30446;&#21069;&#32422;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09823</link><description>&lt;p&gt;
ChatGPT&#21551;&#29992;&#30340;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#26410;&#26469;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Future of ChatGPT-enabled Labor Market: A Preliminary Study. (arXiv:2304.09823v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#21450;&#22522;&#20110;&#32844;&#19994;&#30693;&#35782;&#22270;&#35889;&#24320;&#21457;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#39044;&#27979;&#20102;ChatGPT&#23545;&#26410;&#26469;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#30446;&#21069;&#32422;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#38750;&#20961;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;ChatGPT&#22312;&#21508;&#31181;&#29616;&#23454;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25104;&#21151;&#24182;&#36234;&#26469;&#36234;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#21644;&#24037;&#20316;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;ChatGPT&#26679;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#26159;&#21542;&#20250;&#21462;&#20195;&#20154;&#31867;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#32780;&#19981;&#26159;&#23545;&#31435;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;ChatGPT&#21551;&#29992;&#30340;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#26410;&#26469;&#30340;&#21021;&#27493;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#20013;&#22269;&#26368;&#22823;&#30340;&#22312;&#32447;&#25307;&#32856;&#24179;&#21488;BOSS&#30452;&#32856;&#20013;&#30340;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#21171;&#21160;&#21147;&#24066;&#22330;&#32422;&#26377;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#20110;&#32844;&#19994;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#20197;&#39044;&#27979;&#26410;&#26469;&#32844;&#19994;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a phenomenal large language model, ChatGPT has achieved unparalleled success in various real-world tasks and increasingly plays an important role in our daily lives and work. However, extensive concerns are also raised about the potential ethical issues, especially about whether ChatGPT-like artificial general intelligence (AGI) will replace human jobs. To this end, in this paper, we introduce a preliminary data-driven study on the future of ChatGPT-enabled labor market from the view of Human-AI Symbiosis instead of Human-AI Confrontation. To be specific, we first conduct an in-depth analysis of large-scale job posting data in BOSS Zhipin, the largest online recruitment platform in China. The results indicate that about 28% of occupations in the current labor market require ChatGPT-related skills. Furthermore, based on a large-scale occupation-centered knowledge graph, we develop a semantic information enhanced collaborative filtering algorithm to predict the future occupation-skill
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SCAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#65292;&#35299;&#20915;&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.08495</link><description>&lt;p&gt;
&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#32676;&#20307;&#25928;&#29992;&#20248;&#21270;&#65306;&#19968;&#31181;&#31574;&#30053;&#24615;&#21644;&#20247;&#21253;&#24847;&#35782;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Group Utility in Itinerary Planning: A Strategic and Crowd-Aware Approach. (arXiv:2304.08495v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SCAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#65292;&#35299;&#20915;&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#31243;&#25512;&#33616;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#22797;&#26434;&#30340;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#24403;&#32771;&#34385;&#21040;&#20248;&#21270;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#26102;&#65292;&#36825;&#39033;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#35832;&#22810;&#21442;&#25968;&#65292;&#22914;&#26223;&#28857;&#21463;&#27426;&#36814;&#31243;&#24230;&#12289;&#25490;&#38431;&#26102;&#38388;&#12289;&#27493;&#34892;&#26102;&#38388;&#21644;&#33829;&#19994;&#26102;&#38388;&#31561;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#38598;&#20013;&#22312;&#21333;&#20154;&#35270;&#35282;&#19978;&#65292;&#26410;&#33021;&#35299;&#20915;&#33258;&#28982;&#20154;&#32676;&#34892;&#20026;&#24341;&#36215;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#22914;&#36138;&#23146;&#36335;&#30001;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25112;&#30053;&#21644;&#20247;&#21253;&#24847;&#35782;&#34892;&#31243;&#25512;&#33616;&#65288;SCAIR&#65289;&#8221;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#12290;&#25105;&#20204;&#23558;&#36335;&#32447;&#25512;&#33616;&#31574;&#30053;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29366;&#24577;&#32534;&#30721;&#26426;&#21046;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#23454;&#29616;&#23454;&#26102;&#35268;&#21010;&#21644;&#20998;&#37197;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#39064;&#20844;&#22253;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#21508;&#31181;&#31454;&#20105;&#24615;&#21644;&#29616;&#23454;&#30340;&#22522;&#32447;&#27979;&#35797;&#65292;&#35777;&#26126;SCAIR&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Itinerary recommendation is a complex sequence prediction problem with numerous real-world applications. This task becomes even more challenging when considering the optimization of multiple user queuing times and crowd levels, as well as numerous involved parameters, such as attraction popularity, queuing time, walking time, and operating hours. Existing solutions typically focus on single-person perspectives and fail to address real-world issues resulting from natural crowd behavior, like the Selfish Routing problem. In this paper, we introduce the Strategic and Crowd-Aware Itinerary Recommendation (SCAIR) algorithm, which optimizes group utility in real-world settings. We model the route recommendation strategy as a Markov Decision Process and propose a State Encoding mechanism that enables real-time planning and allocation in linear time. We evaluate our algorithm against various competitive and realistic baselines using a theme park dataset, demonstrating that SCAIR outperforms th
&lt;/p&gt;</description></item><item><title>CornerFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19981;&#21516;&#24314;&#27169;&#31574;&#30053;&#20110;&#21333;&#20010;&#27169;&#22411;&#20013;&#34701;&#21512;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#26469;&#25552;&#21319;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07072</link><description>&lt;p&gt;
CornerFormer: &#25552;&#21319;&#35282;&#28857;&#34920;&#24449;&#20197;&#36827;&#34892;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction. (arXiv:2304.07072v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07072
&lt;/p&gt;
&lt;p&gt;
CornerFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19981;&#21516;&#24314;&#27169;&#31574;&#30053;&#20110;&#21333;&#20010;&#27169;&#22411;&#20013;&#34701;&#21512;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#26469;&#25552;&#21319;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#37325;&#24314;&#26159;&#19968;&#31181;&#38750;&#24179;&#20961;&#30340;&#23494;&#38598;&#39044;&#27979;&#38382;&#39064;&#65292;&#23427;&#20174;&#26629;&#26684;&#22270;&#20687;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#24314;&#31569;&#35282;&#28857;&#21644;&#36793;&#32536;&#65289;&#65292;&#28982;&#21518;&#30456;&#24212;&#22320;&#37325;&#24314;&#20026;&#20108;&#32500;&#24179;&#38754;&#22270;&#12290;&#19982;&#24120;&#35265;&#30340;&#20998;&#21106;&#25110;&#26816;&#27979;&#38382;&#39064;&#30456;&#27604;&#65292;&#23427;&#26174;&#33879;&#20381;&#36182;&#20110;&#21033;&#29992;&#25972;&#20307;&#20960;&#20309;&#20449;&#24687;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;transformer&#30340;&#26041;&#27861;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#31532;&#19968;&#20010;&#27169;&#22411;&#20013;&#26816;&#27979;&#35282;&#28857;&#65292;&#24182;&#22312;&#31532;&#20108;&#20010;&#27169;&#22411;&#20013;&#20998;&#31867;&#25311;&#35758;&#36793;&#32536;&#65288;&#35282;&#23545;&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23558;&#20004;&#20010;&#38454;&#27573;&#20998;&#24320;&#25104;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#20849;&#20139;&#20027;&#24178;&#32534;&#30721;&#22120;&#12290;&#19982;&#29616;&#26377;&#30340;&#24314;&#27169;&#31574;&#30053;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;&#35282;&#28857;&#34920;&#31034;&#26041;&#27861;&#65306;1&#65289;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#31890;&#24230;&#20013;&#20849;&#20139;&#29305;&#24449;&#65292;&#23427;&#22312;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#20043;&#38388;&#34701;&#21512;&#30693;&#35782;&#65307;2&#65289;&#35282;&#28857;&#20505;&#36873;&#32773;&#26681;&#25454;&#20854;&#26041;&#21521;&#20316;&#20026;&#22235;&#20010;&#28909;&#22270;&#36890;&#36947;&#25552;&#20986;&#12290;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#22343;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;CornerFormer&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;transformer-based&#27169;&#22411;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured reconstruction is a non-trivial dense prediction problem, which extracts structural information (\eg, building corners and edges) from a raster image, then reconstructs it to a 2D planar graph accordingly. Compared with common segmentation or detection problems, it significantly relays on the capability that leveraging holistic geometric information for structural reasoning. Current transformer-based approaches tackle this challenging problem in a two-stage manner, which detect corners in the first model and classify the proposed edges (corner-pairs) in the second model. However, they separate two-stage into different models and only share the backbone encoder. Unlike the existing modeling strategies, we present an enhanced corner representation method: 1) It fuses knowledge between the corner detection and edge prediction by sharing feature in different granularity; 2) Corner candidates are proposed in four heatmap channels w.r.t its direction. Both qualitative and quantita
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;&#30340;&#31532;&#20108;&#23626;&#27604;&#36187;&#32467;&#26524;&#65292;&#39640;&#36136;&#37327;&#30340;SYNS-Patches&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;&#27604;&#36187;&#38590;&#24230;&#65292;&#25152;&#26377;&#25552;&#20132;&#20316;&#21697;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.07051</link><description>&lt;p&gt;
&#31532;&#20108;&#23626;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
The Second Monocular Depth Estimation Challenge. (arXiv:2304.07051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;&#30340;&#31532;&#20108;&#23626;&#27604;&#36187;&#32467;&#26524;&#65292;&#39640;&#36136;&#37327;&#30340;SYNS-Patches&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;&#27604;&#36187;&#38590;&#24230;&#65292;&#25152;&#26377;&#25552;&#20132;&#20316;&#21697;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;&#65288;MDEC&#65289;&#30340;&#31532;&#20108;&#23626;&#27604;&#36187;&#32467;&#26524;&#12290;&#26412;&#27425;&#27604;&#36187;&#25509;&#21463;&#20219;&#20309;&#24418;&#24335;&#26041;&#24335;&#30340;&#30417;&#30563;&#65292;&#21253;&#25324;&#20840;&#30417;&#30563;&#12289;&#33258;&#30417;&#30563;&#12289;&#22810;&#20219;&#21153;&#25110;&#20195;&#29702;&#28145;&#24230;&#12290;&#27604;&#36187;&#30340;&#25968;&#25454;&#38598;&#22522;&#20110;SYNS-Patches&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#36136;&#37327;&#30340;&#23494;&#38598;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#29615;&#22659;&#22810;&#26679;&#24615;&#12290;&#27604;&#36187;&#25910;&#21040;&#20102;8&#20010;&#29420;&#29305;&#30340;&#25552;&#20132;&#20316;&#21697;&#65292;&#25152;&#26377;&#22522;&#20110;&#28857;&#20113;&#25110;&#22522;&#20110;&#22270;&#20687;&#30340;&#25351;&#26631;&#34920;&#29616;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#27700;&#24179;&#12290;&#26368;&#20339;&#30417;&#30563;&#25552;&#20132;&#20316;&#21697;&#30340;&#30456;&#23545;F-&#20998;&#25968;&#25552;&#39640;&#20102;27.62&#65285;&#65292;&#32780;&#26368;&#20339;&#33258;&#30417;&#30563;&#25552;&#20132;&#20316;&#21697;&#25552;&#39640;&#20102;16.61&#65285;&#65292;&#36825;&#20123;&#32467;&#26524;&#20195;&#34920;&#20102;&#30495;&#27491;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the results for the second edition of the Monocular Depth Estimation Challenge (MDEC). This edition was open to methods using any form of supervision, including fully-supervised, self-supervised, multi-task or proxy depth. The challenge was based around the SYNS-Patches dataset, which features a wide diversity of environments with high-quality dense ground-truth. This includes complex natural environments, e.g. forests or fields, which are greatly underrepresented in current benchmarks.  The challenge received eight unique submissions that outperformed the provided SotA baseline on any of the pointcloud- or image-based metrics. The top supervised submission improved relative F-Score by 27.62%, while the top self-supervised improved it by 16.61%. Supervised submissions generally leveraged large collections of datasets to improve data diversity. Self-supervised submissions instead updated the network architecture and pretrained backbones. These results represent a si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.06377</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#31526;&#21495;&#30340;&#20986;&#29616;&#19982;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#21019;&#36896;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#65292;&#24182;&#29087;&#32451;&#22320;&#23558;&#23427;&#20204;&#29992;&#20110;&#26356;&#39640;&#30340;&#35748;&#30693;&#21151;&#33021;&#65292;&#22914;&#20132;&#27969;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#31561;&#65292;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#37325;&#35201;&#21644;&#29420;&#29305;&#20043;&#22788;&#12290; &#30446;&#21069;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#21019;&#36896;&#31526;&#21495;&#36827;&#34892;&#36825;&#20123;&#39640;&#32423;&#35748;&#30693;&#21151;&#33021;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31526;&#21495;&#21019;&#36896;&#12289;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#33021;&#21147;&#12290;SEA-net&#29983;&#25104;&#21160;&#24577;&#37197;&#32622;&#32593;&#32476;&#20197;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#31526;&#21495;&#12290;&#36825;&#20123;&#31526;&#21495;&#25429;&#25417;&#20102;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#36890;&#36807;&#32431;&#31526;&#21495;&#25805;&#20316;&#25110;&#20132;&#27969;&#33719;&#24471;&#26032;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#30340;&#31526;&#21495;&#21576;&#29616;&#20986;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#34920;&#26126;&#22312;&#20154;&#31867;&#22823;&#33041;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#29983;&#25104;&#21644;&#29702;&#35299;&#31526;&#21495;&#30340;&#20849;&#21516;&#26694;&#26550;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#23558;&#25104;&#20026;&#23558;&#26469;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#30340;&#21161;&#25512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to create meaningful symbols and proficiently use them for higher cognitive functions such as communication, reasoning, planning, etc., is essential and unique for human intelligence. Current deep neural networks are still far behind human's ability to create symbols for such higher cognitive functions. Here we propose a solution, named SEA-net, to endow neural networks with ability of symbol creation, semantic understanding and communication. SEA-net generates symbols that dynamically configure the network to perform specific tasks. These symbols capture compositional semantic information that enables the system to acquire new functions purely by symbolic manipulation or communication. In addition, we found that these self-generated symbols exhibit an intrinsic structure resembling that of natural language, suggesting a common framework underlying the generation and understanding of symbols in both human brains and artificial neural networks. We hope that it will be instrum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;KG&#23545;&#40784;&#26041;&#27861;DAAKG&#65292;&#21487;&#20197;&#32852;&#21512;&#23545;&#40784;&#19981;&#20165;&#23454;&#20307;&#65292;&#36824;&#21253;&#25324;&#20851;&#31995;&#21644;&#31867;&#21035;&#65307;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#25209;&#27425;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#36234;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04389</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#21644;&#26550;&#26500;&#30340;&#28145;&#24230;&#20027;&#21160;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Deep Active Alignment of Knowledge Graph Entities and Schemata. (arXiv:2304.04389v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;KG&#23545;&#40784;&#26041;&#27861;DAAKG&#65292;&#21487;&#20197;&#32852;&#21512;&#23545;&#40784;&#19981;&#20165;&#23454;&#20307;&#65292;&#36824;&#21253;&#25324;&#20851;&#31995;&#21644;&#31867;&#21035;&#65307;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#25209;&#27425;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#36234;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23384;&#20648;&#20102;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#20016;&#23500;&#20107;&#23454;&#12290;&#26412;&#25991;&#30740;&#31350;KG&#23545;&#40784;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#30340;KG&#20013;&#25214;&#21040;&#19981;&#20165;&#23454;&#20307;&#65292;&#36824;&#21253;&#25324;&#20851;&#31995;&#21644;&#31867;&#21035;&#30340;&#23545;&#40784;&#12290;&#23454;&#20307;&#32423;&#21035;&#30340;&#23545;&#40784;&#21487;&#20197;&#20419;&#36827;&#26550;&#26500;&#32423;&#21035;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KG&#23545;&#40784;&#26041;&#27861;&#65292;&#31216;&#20026;DAAKG&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#65292;&#23427;&#23398;&#20064;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#31867;&#21035;&#30340;&#23884;&#20837;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#26041;&#24335;&#19979;&#32852;&#21512;&#23545;&#40784;&#23427;&#20204;&#65307;&#32780;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#65292;&#23427;&#20272;&#35745;&#23454;&#20307;&#12289;&#20851;&#31995;&#25110;&#31867;&#21035;&#23545;&#30340;&#25512;&#26029;&#21487;&#33021;&#24615;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#25209;&#27425;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#36817;&#20284;&#31639;&#27861;&#20197;&#39640;&#25928;&#36873;&#25321;&#25209;&#27425;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;&#20102;DAAKG&#30340;&#20248;&#36234;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#25152;&#26377;&#27169;&#22359;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) store rich facts about the real world. In this paper, we study KG alignment, which aims to find alignment between not only entities but also relations and classes in different KGs. Alignment at the entity level can cross-fertilize alignment at the schema level. We propose a new KG alignment approach, called DAAKG, based on deep learning and active learning. With deep learning, it learns the embeddings of entities, relations and classes, and jointly aligns them in a semi-supervised manner. With active learning, it estimates how likely an entity, relation or class pair can be inferred, and selects the best batch for human labeling. We design two approximation algorithms for efficient solution to batch selection. Our experiments on benchmark datasets show the superior accuracy and generalization of DAAKG and validate the effectiveness of all its modules.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.03439</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#21644;GPT-4&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;4&#65288;GPT-4&#65289;&#30340;&#21457;&#24067;&#65292;&#25105;&#20204;&#28212;&#26395;&#20102;&#35299;GPT-4&#22312;&#21508;&#31181;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;LogiQA&#21644;ReClor&#31561;&#24120;&#29992;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#20687;AR-LSAT&#36825;&#26679;&#30340;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#38656;&#35201;&#36923;&#36753;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#27979;&#35797;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;ChatGPT&#21644;GPT-4&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;ChatGPT&#21644;GPT-4&#20043;&#38388;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;&#34920;&#29616;&#36828;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#12290;GPT-4&#22312;&#25105;&#20204;&#30340;&#25163;&#21160;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#39640;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#21644;GPT-4&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#20026;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. GPT-4 shows even higher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 do relatively w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#36873;&#32508;&#36848;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#25991;&#31456;&#65292;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#20248;&#21183;&#65292;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2304.00086</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#32463;&#27982;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65306;&#20309;&#26102;&#12289;&#20160;&#20040;&#21644;&#22914;&#20309;&#36816;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Economics Research: When What and How?. (arXiv:2304.00086v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#36873;&#32508;&#36848;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#25991;&#31456;&#65292;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#20248;&#21183;&#65292;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#37325;&#35201;&#32463;&#27982;&#26399;&#21002;&#19978;&#21457;&#34920;&#30340;&#25991;&#31456;&#36827;&#34892;&#20102;&#31934;&#36873;&#32508;&#36848;&#12290;&#32508;&#36848;&#22238;&#31572;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20309;&#26102;&#22312;&#32463;&#27982;&#23398;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#65288;2&#65289;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#20160;&#20040;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#23558;&#23427;&#20204;&#29992;&#20110;&#32463;&#27982;&#24212;&#29992;&#12290;&#32508;&#36848;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#38750;&#20256;&#32479;&#25968;&#25454;&#65292;&#32780;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#20256;&#32479;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#35745;&#37327;&#32463;&#27982;&#23398;&#27169;&#22411;&#22312;&#20998;&#26512;&#20302;&#22797;&#26434;&#24615;&#25968;&#25454;&#26102;&#21487;&#33021;&#36275;&#22815;&#65292;&#20294;&#30001;&#20110;&#24555;&#36895;&#25968;&#23383;&#21270;&#21644;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#65292;&#32463;&#27982;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#27491;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a curated review of selected papers published in prominent economics journals that use machine learning (ML) tools for research and policy analysis. The review focuses on three key questions: (1) when ML is used in economics, (2) what ML models are commonly preferred, and (3) how they are used for economic applications. The review highlights that ML is particularly used in processing nontraditional and unstructured data, capturing strong nonlinearity, and improving prediction accuracy. Deep learning models are suitable for nontraditional data, whereas ensemble learning models are preferred for traditional datasets. While traditional econometric models may suffice for analyzing low-complexity data, the increasing complexity of economic data due to rapid digitalization and the growing literature suggest that ML is becoming an essential addition to the econometrician's toolbox.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13769</link><description>&lt;p&gt;
&#26410;&#30693;&#21957;&#25506;&#22120;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#65306;&#19981;&#35201;&#23545;&#26410;&#30693;&#23545;&#35937;&#35270;&#32780;&#19981;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects. (arXiv:2303.13769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25552;&#20986;&#30340;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#21644;&#24320;&#25918;&#38598;&#26816;&#27979;&#22312;&#23547;&#25214;&#20174;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24182;&#23558;&#20854;&#19982;&#24050;&#30693;&#31867;&#21035;&#21306;&#20998;&#24320;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#23545;&#20174;&#24050;&#30693;&#31867;&#21035;&#21521;&#26410;&#30693;&#31867;&#21035;&#30340;&#30693;&#35782;&#20256;&#36882;&#30340;&#30740;&#31350;&#38656;&#35201;&#26356;&#28145;&#20837;&#65292;&#20174;&#32780;&#23548;&#33268;&#25506;&#27979;&#38544;&#34255;&#22312;&#32972;&#26223;&#20013;&#30340;&#26410;&#30693;&#29289;&#20307;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#26469;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#65292;&#20165;&#20351;&#29992;&#24050;&#30693;&#31867;&#21035;&#26679;&#26412;&#36827;&#34892;&#30417;&#30563;&#21644;&#36991;&#20813;&#22312;&#32972;&#26223;&#20013;&#19981;&#36866;&#24403;&#22320;&#21387;&#21046;&#26410;&#30693;&#29289;&#20307;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24050;&#30693;&#29289;&#20307;&#23398;&#20064;&#21040;&#30340;&#36825;&#31181;&#32622;&#20449;&#24230;&#20998;&#25968;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#29289;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#36827;&#19968;&#27493;&#38480;&#21046;&#32972;&#26223;&#20013;&#38750;&#29289;&#20307;&#26679;&#26412;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30001;&#20110;&#32570;&#20047;&#23427;&#20204;&#22312;&#35757;&#32451;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#30340;&#26368;&#20339;&#26694;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The recently proposed open-world object and open-set detection achieve a breakthrough in finding never-seen-before objects and distinguishing them from class-known ones. However, their studies on knowledge transfer from known classes to unknown ones need to be deeper, leading to the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses class-known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from class-known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further limit the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this is
&lt;/p&gt;</description></item><item><title>MCTS-GEB&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#37325;&#20889;&#31995;&#32479;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#26500;&#24314;&#26368;&#20248;&#30340;E&#22270;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#20013;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04651</link><description>&lt;p&gt;
MCTS-GEB&#65306;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26159;&#19968;&#20010;&#22909;&#30340;E&#22270;&#26500;&#24314;&#22120;
&lt;/p&gt;
&lt;p&gt;
MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder. (arXiv:2303.04651v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04651
&lt;/p&gt;
&lt;p&gt;
MCTS-GEB&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#37325;&#20889;&#31995;&#32479;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#26500;&#24314;&#26368;&#20248;&#30340;E&#22270;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#20013;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#20889;&#31995;&#32479;&#24191;&#27867;&#20351;&#29992;&#31561;&#24335;&#39281;&#21644;&#25216;&#26415;&#26469;&#20248;&#21270;&#37325;&#20889;&#39034;&#24207;&#65292;&#20294;&#26159;&#24403;E&#22270;&#27809;&#26377;&#39281;&#21644;&#26102;&#65292;&#26080;&#27861;&#20195;&#34920;&#25152;&#26377;&#21487;&#33021;&#30340;&#37325;&#20889;&#26426;&#20250;&#65292;&#20250;&#37325;&#26032;&#24341;&#20837;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MCTS-GEB&#65292;&#19968;&#20010;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#20110;E&#22270;&#26500;&#24314;&#30340;&#36890;&#29992;&#37325;&#20889;&#31995;&#32479;&#12290;MCTS-GEB&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#39640;&#25928;&#35268;&#21010;&#26368;&#20248;&#30340;E&#22270;&#26500;&#24314;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#38454;&#27573;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;MCTS-GEB&#37117;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rewrite systems [6, 10, 12] have been widely employing equality saturation [9], which is an optimisation methodology that uses a saturated e-graph to represent all possible sequences of rewrite simultaneously, and then extracts the optimal one. As such, optimal results can be achieved by avoiding the phase-ordering problem. However, we observe that when the e-graph is not saturated, it cannot represent all possible rewrite opportunities and therefore the phase-ordering problem is re-introduced during the construction phase of the e-graph. To address this problem, we propose MCTS-GEB, a domain-general rewrite system that applies reinforcement learning (RL) to e-graph construction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3] to efficiently plan for the optimal e-graph construction, and therefore it can effectively eliminate the phase-ordering problem at the construction phase and achieve better performance within a reasonable time. Evaluation in two different domains 
&lt;/p&gt;</description></item><item><title>CUREE&#26159;&#19968;&#31181;&#27700;&#19979;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#26426;&#22120;&#20154;&#34892;&#20026;&#21644;&#24863;&#30693;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#32452;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#20302;&#31354;&#35270;&#35273;&#35843;&#26597;&#12289;&#22768;&#26223;&#35843;&#26597;&#12289;&#26646;&#24687;&#22320;&#34920;&#24449;&#21644;&#21160;&#29289;&#36319;&#36394;&#12290;&#22312;&#29642;&#29786;&#30977;&#19978;&#30340;&#20004;&#27425;&#23454;&#22320;&#37096;&#32626;&#20013;&#65292;CUREE&#25104;&#21151;&#35782;&#21035;&#20102;&#25293;&#25668;&#34430;&#30340;&#39318;&#36873;&#26646;&#24687;&#22320;&#31867;&#22411;&#65292;&#24182;&#36861;&#36394;&#21644;&#36319;&#36394;&#20102;&#19968;&#32676;&#35269;&#39135;&#30340;&#22806;&#31185;&#21307;&#29983;&#40060;&#65292;&#25910;&#38598;&#20102;&#23427;&#20204;&#30340;&#34892;&#20026;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.03943</link><description>&lt;p&gt;
CUREE: &#19968;&#31181;&#29992;&#20110;&#29983;&#24577;&#31995;&#32479;&#25506;&#32034;&#30340;&#22909;&#22855;&#27700;&#19979;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
CUREE: A Curious Underwater Robot for Ecosystem Exploration. (arXiv:2303.03943v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03943
&lt;/p&gt;
&lt;p&gt;
CUREE&#26159;&#19968;&#31181;&#27700;&#19979;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#26426;&#22120;&#20154;&#34892;&#20026;&#21644;&#24863;&#30693;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#32452;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#20302;&#31354;&#35270;&#35273;&#35843;&#26597;&#12289;&#22768;&#26223;&#35843;&#26597;&#12289;&#26646;&#24687;&#22320;&#34920;&#24449;&#21644;&#21160;&#29289;&#36319;&#36394;&#12290;&#22312;&#29642;&#29786;&#30977;&#19978;&#30340;&#20004;&#27425;&#23454;&#22320;&#37096;&#32626;&#20013;&#65292;CUREE&#25104;&#21151;&#35782;&#21035;&#20102;&#25293;&#25668;&#34430;&#30340;&#39318;&#36873;&#26646;&#24687;&#22320;&#31867;&#22411;&#65292;&#24182;&#36861;&#36394;&#21644;&#36319;&#36394;&#20102;&#19968;&#32676;&#35269;&#39135;&#30340;&#22806;&#31185;&#21307;&#29983;&#40060;&#65292;&#25910;&#38598;&#20102;&#23427;&#20204;&#30340;&#34892;&#20026;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25506;&#32034;&#21644;&#30417;&#27979;&#22797;&#26434;&#30340;&#27700;&#19979;&#29983;&#24577;&#31995;&#32479;&#65288;&#22914;&#29642;&#29786;&#30977;&#65289;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#28508;&#27700;&#21592;&#25163;&#25345;&#25110;&#38745;&#24577;&#30456;&#26426;&#36827;&#34892;&#35843;&#26597;&#65292;&#25110;&#32773;&#37096;&#32626;&#20256;&#24863;&#22120;&#28014;&#26631;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#19981;&#21516;&#29642;&#29786;&#30977;&#29983;&#29289;&#21450;&#20854;&#26646;&#24687;&#22320;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#20840;&#37096;&#21464;&#21270;&#21644;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CUREE&#24179;&#21488;&#65292;&#23427;&#36890;&#36807;&#26426;&#22120;&#20154;&#34892;&#20026;&#21644;&#24863;&#30693;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#32452;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#20351;&#31185;&#23398;&#23478;&#33021;&#22815;&#25506;&#32034;&#29983;&#24577;&#31995;&#32479;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#36825;&#20123;&#33021;&#21147;&#30340;&#31034;&#20363;&#21253;&#25324;&#20302;&#31354;&#35270;&#35273;&#35843;&#26597;&#12289;&#22768;&#26223;&#35843;&#26597;&#12289;&#26646;&#24687;&#22320;&#34920;&#24449;&#21644;&#21160;&#29289;&#36319;&#36394;&#12290;&#25105;&#20204;&#36890;&#36807;&#25551;&#36848;&#22312;&#32654;&#22269;&#32500;&#23572;&#20140;&#32676;&#23707;&#30340;&#29642;&#29786;&#30977;&#19978;&#36827;&#34892;&#30340;&#20004;&#27425;&#23454;&#22320;&#37096;&#32626;&#26469;&#23637;&#31034;&#36825;&#20123;&#33021;&#21147;&#12290;&#22312;&#31532;&#19968;&#27425;&#37096;&#32626;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CUREE&#21487;&#20197;&#36890;&#36807;&#35270;&#35273;&#35843;&#26597;&#12289;&#26646;&#24687;&#22320;&#34920;&#24449;&#21644;&#22768;&#26223;&#35843;&#26597;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#35782;&#21035;&#20986;&#29642;&#29786;&#30977;&#20013;&#25293;&#25668;&#34430;&#30340;&#39318;&#36873;&#26646;&#24687;&#22320;&#31867;&#22411;&#12290;&#22312;&#31532;&#20108;&#27425;&#37096;&#32626;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CUREE&#36861;&#36394;&#21644;&#36319;&#36394;&#19968;&#32676;&#35269;&#39135;&#30340;&#22806;&#31185;&#21307;&#29983;&#40060;&#24182;&#25910;&#38598;&#20854;&#34892;&#20026;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current approach to exploring and monitoring complex underwater ecosystems, such as coral reefs, is to conduct surveys using diver-held or static cameras, or deploying sensor buoys. These approaches often fail to capture the full variation and complexity of interactions between different reef organisms and their habitat. The CUREE platform presented in this paper provides a unique set of capabilities in the form of robot behaviors and perception algorithms to enable scientists to explore different aspects of an ecosystem. Examples of these capabilities include low-altitude visual surveys, soundscape surveys, habitat characterization, and animal following. We demonstrate these capabilities by describing two field deployments on coral reefs in the US Virgin Islands. In the first deployment, we show that CUREE can identify the preferred habitat type of snapping shrimp in a reef through a combination of a visual survey, habitat characterization, and a soundscape survey. In the second d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;</title><link>http://arxiv.org/abs/2303.00396</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning. (arXiv:2303.00396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20219;&#21153;&#65292;&#23398;&#20064;&#29305;&#23450;&#20110;&#24207;&#25968;&#20998;&#31867;&#30340;&#33391;&#22909;&#32467;&#26500;&#21270;&#29305;&#24449;&#31354;&#38388;&#26377;&#21161;&#20110;&#24688;&#24403;&#22320;&#25429;&#25417;&#31867;&#20043;&#38388;&#30340;&#24207;&#25968;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20026;&#27599;&#20010;&#24207;&#25968;&#31867;&#23398;&#20064;&#19968;&#20010;&#20195;&#29702;&#65292;&#28982;&#21518;&#36890;&#36807;&#38480;&#21046;&#36825;&#20123;&#20195;&#29702;&#26469;&#35843;&#25972;&#31867;&#30340;&#20840;&#23616;&#24067;&#23616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#30828;&#24067;&#23616;&#32422;&#26463;&#21644;&#36719;&#24067;&#23616;&#32422;&#26463;&#12290;&#30828;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#30452;&#25509;&#25511;&#21046;&#20195;&#29702;&#30340;&#29983;&#25104;&#26469;&#23454;&#29616;&#65292;&#20197;&#24378;&#21046;&#23558;&#20854;&#25918;&#32622;&#22312;&#20005;&#26684;&#30340;&#32447;&#24615;&#24067;&#23616;&#25110;&#21322;&#22278;&#24418;&#24067;&#23616;&#65288;&#21363;&#20005;&#26684;&#24207;&#25968;&#24067;&#23616;&#30340;&#20004;&#31181;&#23454;&#20363;&#65289;&#20013;&#12290;&#36719;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#26469;&#23454;&#29616;&#65292;&#35813;&#39033;&#24809;&#32602;&#20559;&#31163;&#29702;&#24819;&#24207;&#25968;&#24067;&#23616;&#30340;&#24773;&#20917;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;CPL&#26041;&#27861;&#22312;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.07267</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24515;&#29702;&#23398;&#20013;&#30340;&#8220;&#27491;&#30830;&#31572;&#26696;&#8221;
&lt;/p&gt;
&lt;p&gt;
"Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#12290;&#36825;&#31181;AI&#31995;&#32479;&#30340;&#19968;&#20010;&#25552;&#20986;&#30340;&#24212;&#29992;&#26159;&#25903;&#25345;&#31038;&#20250;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#30446;&#21069;&#23436;&#32654;&#30340;&#23454;&#39564;&#25511;&#21046;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#22823;&#35268;&#27169;&#12289;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#65288;&#20439;&#31216;GPT3.5&#65289;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#39033;&#30740;&#31350;&#30340;&#35843;&#26597;&#20316;&#20026;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;GPT3.5&#30340;&#40664;&#35748;&#35774;&#32622;&#20013;&#25910;&#38598;&#20102;&#21709;&#24212;&#12290;&#22312;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#30340;&#20843;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;GPT&#26679;&#26412;&#22797;&#21046;&#20102;&#21407;&#22987;&#32467;&#26524;&#30340;37.5%&#20197;&#21450;Many Labs 2&#32467;&#26524;&#30340;37.5%&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#26080;&#27861;&#20687;&#39044;&#20808;&#27880;&#20876;&#30340;&#35745;&#21010;&#37027;&#26679;&#20998;&#26512;&#21097;&#19979;&#30340;&#20845;&#39033;&#30740;&#31350;&#12290;&#36825;&#26159;&#22240;&#20026;&#23545;&#20110;&#36825;&#20845;&#39033;&#30740;&#31350;&#20013;&#30340;&#27599;&#19968;&#39033;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65288;&#26080;&#35770;&#26159;&#22240;&#21464;&#37327;&#36824;&#26159;&#26465;&#20214;&#21464;&#37327;&#65289;&#65306;&#19968;&#20010;&#26410;&#30693;&#30340;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20248;&#36136;&#22810;&#26679;&#24615;&#31639;&#27861;&#36827;&#21270;Minecraft&#28216;&#25103;&#20013;&#30340;&#39134;&#34892;&#22120;&#65292;&#27604;&#20256;&#32479;&#36866;&#24212;&#24230;&#31639;&#27861;&#26356;&#21487;&#38752;&#22320;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2302.00782</link><description>&lt;p&gt;
&#22312;Minecraft&#20013;&#20351;&#29992;&#20248;&#36136;&#22810;&#26679;&#24615;&#36827;&#21270;&#39134;&#34892;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evolving Flying Machines in Minecraft Using Quality Diversity. (arXiv:2302.00782v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20248;&#36136;&#22810;&#26679;&#24615;&#31639;&#27861;&#36827;&#21270;Minecraft&#28216;&#25103;&#20013;&#30340;&#39134;&#34892;&#22120;&#65292;&#27604;&#20256;&#32479;&#36866;&#24212;&#24230;&#31639;&#27861;&#26356;&#21487;&#38752;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Minecraft&#26159;&#19968;&#20010;&#26497;&#22909;&#30340;&#20154;&#31867;&#21019;&#36896;&#21147;&#27979;&#35797;&#24179;&#21488;&#65292;&#21551;&#21457;&#20102;&#21508;&#31181;&#32467;&#26500;&#29978;&#33267;&#21253;&#25324;&#39134;&#34892;&#22120;&#30340;&#35774;&#35745;&#12290;EvoCraft&#26159;&#19968;&#20010;&#21487;&#32534;&#31243;&#29983;&#25104;Minecraft&#32467;&#26500;&#30340;API&#65292;&#20294;&#26159;&#35813;&#39046;&#22495;&#30340;&#21021;&#27493;&#24037;&#20316;&#24182;&#19981;&#33021;&#29983;&#25104;&#39134;&#34892;&#22120;&#12290;&#26412;&#25991;&#24212;&#29992;&#22522;&#20110;&#36866;&#24212;&#24230;&#30340;&#36827;&#21270;&#21644;&#20248;&#36136;&#22810;&#26679;&#24615;&#25628;&#32034;&#65292;&#20197;&#36827;&#21270;&#39134;&#34892;&#22120;&#12290;&#34429;&#28982;&#20165;&#20165;&#24212;&#29992;&#36866;&#24212;&#24230;&#26377;&#26102;&#21487;&#20197;&#20135;&#29983;&#39134;&#34892;&#22120;&#65292;&#20294;&#30001;&#20110;&#20351;&#29992;&#20102;&#27604;&#20197;&#21069;&#26356;&#22797;&#26434;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#65292;&#20248;&#36136;&#22810;&#26679;&#24615;&#31639;&#27861;MAP-Elites&#33021;&#22815;&#26356;&#21487;&#38752;&#22320;&#21457;&#29616;&#39134;&#34892;&#22120;&#65292;&#33267;&#23569;&#22312;&#20351;&#29992;&#36866;&#24403;&#30340;&#34892;&#20026;&#34920;&#24449;&#26469;&#25351;&#23548;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#25628;&#32034;&#26102;&#26159;&#21487;&#20197;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minecraft is a great testbed for human creativity that has inspired the design of various structures and even functioning machines, including flying machines. EvoCraft is an API for programmatically generating structures in Minecraft, but the initial work in this domain was not capable of evolving flying machines. This paper applies fitness-based evolution and quality diversity search in order to evolve flying machines. Although fitness alone can occasionally produce flying machines, thanks in part to a more sophisticated fitness function than was used previously, the quality diversity algorithm MAP-Elites is capable of discovering flying machines much more reliably, at least when an appropriate behavior characterization is used to guide the search for diverse solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SLOTAlign&#30340;&#26080;&#30417;&#30563;&#22270;&#23545;&#40784;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#32467;&#26500;&#23398;&#20064;&#21644;&#26368;&#20248;&#36755;&#36816;&#26469;&#35299;&#20915;&#22270;&#20043;&#38388;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#23545;&#40784;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12721</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#23398;&#20064;&#21644;&#26368;&#20248;&#36755;&#36816;&#30340;&#40065;&#26834;&#23646;&#24615;&#22270;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Attributed Graph Alignment via Joint Structure Learning and Optimal Transport. (arXiv:2301.12721v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SLOTAlign&#30340;&#26080;&#30417;&#30563;&#22270;&#23545;&#40784;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#32467;&#26500;&#23398;&#20064;&#21644;&#26368;&#20248;&#36755;&#36816;&#26469;&#35299;&#20915;&#22270;&#20043;&#38388;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#23545;&#40784;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22270;&#20043;&#38388;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#19981;&#19968;&#33268;&#24615;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21313;&#20998;&#26222;&#36941;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SLOTAlign&#30340;&#26080;&#30417;&#30563;&#22270;&#23545;&#40784;&#26694;&#26550;&#65292;&#20854;&#20013;&#23558;&#22270;&#23545;&#40784;&#36716;&#21270;&#20026;&#20004;&#20010;&#20869;&#37096;&#22270;&#30697;&#38453;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#21516;&#26102;&#32467;&#21512;&#22810;&#35270;&#35282;&#32467;&#26500;&#23398;&#20064;&#20197;&#22686;&#24378;&#20854;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#23545;&#40784;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph alignment, which aims at identifying corresponding entities across multiple networks, has been widely applied in various domains. As the graphs to be aligned are usually constructed from different sources, the inconsistency issues of structures and features between two graphs are ubiquitous in real-world applications. Most existing methods follow the ``embed-then-cross-compare'' paradigm, which computes node embeddings in each graph and then processes node correspondences based on cross-graph embedding comparison. However, we find these methods are unstable and sub-optimal when structure or feature inconsistency appears. To this end, we propose SLOTAlign, an unsupervised graph alignment framework that jointly performs Structure Learning and Optimal Transport Alignment. We convert graph alignment to an optimal transport problem between two intra-graph matrices without the requirement of cross-graph comparison. We further incorporate multi-view structure learning to enhance graph r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14454</link><description>&lt;p&gt;
MEAformer: &#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid. (arXiv:2212.14454v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#21464;&#20307;&#65292;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#26088;&#22312;&#21457;&#29616;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#20855;&#26377;&#30456;&#20851;&#22270;&#20687;&#30340;&#30456;&#21516;&#23454;&#20307;&#12290; &#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#24403;&#21069;&#30340;MMEA&#31639;&#27861;&#37117;&#20840;&#23616;&#37319;&#29992;KG&#32423;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24335;&#23454;&#20307;&#34920;&#31034;&#65292;&#20294;&#24573;&#30053;&#20102;&#20010;&#20307;&#23454;&#20307;&#30340;&#27169;&#24577;&#20559;&#22909;&#21464;&#21270;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#23545;&#27169;&#24577;&#65288;&#20363;&#22914;&#27169;&#31946;&#22270;&#20687;&#21644;&#20851;&#31995;&#65289;&#20013;&#28508;&#22312;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MEAformer&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#65288;&#21253;&#25324;&#26377;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#12289;&#36845;&#20195;&#21644;&#20302;&#36164;&#28304;&#35774;&#32622;&#65289;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#21033;&#29992;&#27169;&#24577;&#20559;&#22909;&#21464;&#21270;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an important variant of entity alignment (EA), multi-modal entity alignment (MMEA) aims to discover identical entities across different knowledge graphs (KGs) with relevant images attached. We noticed that current MMEA algorithms all globally adopt the KG-level modality fusion strategies for multi-modal entity representation but ignore the variation in modality preferences for individual entities, hurting the robustness to potential noise involved in modalities (e.g., blurry images and relations). In this paper, we present MEAformer, a multi-modal entity alignment transformer approach for meta modality hybrid, which dynamically predicts the mutual correlation coefficients among modalities for entity-level feature aggregation. A modal-aware hard entity replay strategy is further proposed for addressing vague entity details. Experimental results show that our model not only achieves SOTA performance on multiple training scenarios including supervised, unsupervised, iterative, and low 
&lt;/p&gt;</description></item><item><title>MegaCRN&#26159;&#19968;&#20010;&#20351;&#29992;&#26102;&#31354;&#20803;&#22270;&#23398;&#20064;&#26426;&#21046;&#30340;&#20803;&#22270;&#21367;&#31215;&#36882;&#24402;&#32593;&#32476;&#65292;&#23545;&#20110;&#26102;&#31354;&#24314;&#27169;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.05989</link><description>&lt;p&gt;
MegaCRN&#65306;&#29992;&#20110;&#26102;&#31354;&#24314;&#27169;&#30340;&#20803;&#22270;&#21367;&#31215;&#36882;&#24402;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MegaCRN: Meta-Graph Convolutional Recurrent Network for Spatio-Temporal Modeling. (arXiv:2212.05989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05989
&lt;/p&gt;
&lt;p&gt;
MegaCRN&#26159;&#19968;&#20010;&#20351;&#29992;&#26102;&#31354;&#20803;&#22270;&#23398;&#20064;&#26426;&#21046;&#30340;&#20803;&#22270;&#21367;&#31215;&#36882;&#24402;&#32593;&#32476;&#65292;&#23545;&#20110;&#26102;&#31354;&#24314;&#27169;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#24314;&#27169;&#26159;AI&#31038;&#21306;&#20013;&#37325;&#35201;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#31354;&#20803;&#22270;&#23398;&#20064;&#20316;&#20026;&#22788;&#29702;&#22270;&#27969;&#20013;&#30340;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#30340;&#26032;&#22411;&#22270;&#32467;&#26500;&#23398;&#20064;&#26426;&#21046;&#12290;&#36890;&#36807;&#23558;&#20803;&#22270;&#23398;&#20064;&#22120;&#23884;&#20837;GCRN&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#24605;&#24819;&#65292;&#24182;&#21019;&#24314;&#20102;Meta-Graph Convolutional Recurrent Network (MegaCRN)&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;METR-LA&#21644;PEMS-BAY&#65289;&#20197;&#21450;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#38750;&#24179;&#31283;&#29616;&#35937;&#30340;&#22823;&#35268;&#27169;&#26102;&#31354;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#37117;&#22823;&#20026;&#20248;&#36234;&#65288;&#35823;&#24046;&#29575;&#36229;&#36807;27&#65285; MAE&#21644;34&#65285; RMSE&#65289;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#22320;&#35299;&#24320;&#20855;&#26377;&#19981;&#21516;&#27169;&#24335;&#30340;&#20301;&#32622;&#21644;&#26102;&#38388;&#25554;&#27133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal modeling as a canonical task of multivariate time series forecasting has been a significant research topic in AI community. To address the underlying heterogeneity and non-stationarity implied in the graph streams, in this study, we propose Spatio-Temporal Meta-Graph Learning as a novel Graph Structure Learning mechanism on spatio-temporal data. Specifically, we implement this idea into Meta-Graph Convolutional Recurrent Network (MegaCRN) by plugging the Meta-Graph Learner powered by a Meta-Node Bank into GCRN encoder-decoder. We conduct a comprehensive evaluation on two benchmark datasets (METR-LA and PEMS-BAY) and a large-scale spatio-temporal dataset that contains a variaty of non-stationary phenomena. Our model outperformed the state-of-the-arts to a large degree on all three datasets (over 27% MAE and 34% RMSE). Besides, through a series of qualitative evaluations, we demonstrate that our model can explicitly disentangle locations and time slots with different patt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35780;&#35770;&#32773;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;(SN)&#21487;&#20197;&#20351;&#35780;&#35770;&#23478;&#26356;&#31283;&#20581;&#22320;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#22330;&#26223;&#19979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35268;&#33539;&#21270;&#35780;&#35770;&#23478;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#24555;&#36895;&#36731;&#26494;&#22320;&#20174;&#37027;&#20123;&#22797;&#26434;&#24773;&#22659;&#20013;&#23398;&#20064;&#65292;&#36825;&#19968;&#28857;&#23545;&#20110;&#23454;&#29616;&#31283;&#23450;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2212.05331</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35889;&#24402;&#19968;&#21270;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Effects of Spectral Normalization in Multi-agent Reinforcement Learning. (arXiv:2212.05331v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35780;&#35770;&#32773;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;(SN)&#21487;&#20197;&#20351;&#35780;&#35770;&#23478;&#26356;&#31283;&#20581;&#22320;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#22330;&#26223;&#19979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35268;&#33539;&#21270;&#35780;&#35770;&#23478;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#24555;&#36895;&#36731;&#26494;&#22320;&#20174;&#37027;&#20123;&#22797;&#26434;&#24773;&#22659;&#20013;&#23398;&#20064;&#65292;&#36825;&#19968;&#28857;&#23545;&#20110;&#23454;&#29616;&#31283;&#23450;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31232;&#30095;&#22870;&#21169;&#22330;&#26223;&#19979;&#65292;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#35770;&#32773;&#23545;&#20110;&#22312;&#31574;&#30053;&#19978;&#23454;&#29616;&#28436;&#21592;-&#35780;&#35770;&#32773;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#22240;&#32032;&#65292;&#23398;&#20064;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#35770;&#32773;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;1&#65289;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#32852;&#21512;&#20316;&#29992;&#31354;&#38388;&#21576;&#25351;&#25968;&#22686;&#38271;&#65307;2&#65289;&#36825;&#20010;&#22240;&#32032;&#32467;&#21512;&#22870;&#21169;&#31232;&#30095;&#21644;&#29615;&#22659;&#22122;&#22768;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#25968;&#25165;&#33021;&#23454;&#29616;&#20934;&#30830;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#29992;&#35889;&#24402;&#19968;&#21270;(SN)&#23545;&#35780;&#35770;&#23478;&#36827;&#34892;&#35268;&#33539;&#21270;&#65292;&#20351;&#23427;&#33021;&#22815;&#22312;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#19978;&#30340;&#31232;&#30095;&#22870;&#21169;&#22330;&#26223;&#20013;&#26356;&#21152;&#31283;&#20581;&#22320;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35268;&#33539;&#21270;&#30340;&#35780;&#35770;&#23478;&#33021;&#22815;&#24555;&#36895;&#22320;&#20174;&#22797;&#26434;&#30340;SMAC&#21644;RWARE&#39046;&#22495;&#30340;&#31232;&#32570;&#22870;&#21169;&#32463;&#21382;&#20013;&#23398;&#20064;&#21040;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#35780;&#35770;&#23478;&#35268;&#33539;&#21270;&#22312;&#31283;&#23450;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A reliable critic is central to on-policy actor-critic learning. But it becomes challenging to learn a reliable critic in a multi-agent sparse reward scenario due to two factors: 1) The joint action space grows exponentially with the number of agents 2) This, combined with the reward sparseness and environment noise, leads to large sample requirements for accurate learning. We show that regularising the critic with spectral normalization (SN) enables it to learn more robustly, even in multi-agent on-policy sparse reward scenarios. Our experiments show that the regularised critic is quickly able to learn from the sparse rewarding experience in the complex SMAC and RWARE domains. These findings highlight the importance of regularisation in the critic for stable learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PowRL&#26694;&#26550;&#26469;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#26377;&#25928;&#24212;&#23545;&#30005;&#21147;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24773;&#20917;&#65292;&#24182;&#20445;&#25345;&#30005;&#32593;&#30340;&#21487;&#38752;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2212.02397</link><description>&lt;p&gt;
PowRL&#65306;&#29992;&#20110;&#31283;&#20581;&#31649;&#29702;&#30005;&#21147;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PowRL: A Reinforcement Learning Framework for Robust Management of Power Networks. (arXiv:2212.02397v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PowRL&#26694;&#26550;&#26469;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#26377;&#25928;&#24212;&#23545;&#30005;&#21147;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24773;&#20917;&#65292;&#24182;&#20445;&#25345;&#30005;&#32593;&#30340;&#21487;&#38752;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#21508;&#22320;&#30340;&#30005;&#21147;&#32593;&#32476;&#36890;&#36807;&#20026;&#22810;&#20010;&#34892;&#19994;&#12289;&#20225;&#19994;&#21644;&#23478;&#24237;&#28040;&#36153;&#32773;&#25552;&#20379;&#19981;&#38388;&#26029;&#12289;&#21487;&#38752;&#21644;&#26080;&#26242;&#24577;&#30005;&#21147;&#21457;&#25381;&#30528;&#37325;&#35201;&#30340;&#31038;&#20250;&#21644;&#32463;&#27982;&#20316;&#29992;&#12290;&#38543;&#30528;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#30005;&#21160;&#36710;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#21457;&#30005;&#21644;&#39640;&#24230;&#21160;&#24577;&#36127;&#36733;&#38656;&#27714;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#30636;&#24577;&#31283;&#23450;&#38382;&#39064;&#31649;&#29702;&#26469;&#30830;&#20445;&#30005;&#21147;&#32593;&#32476;&#30340;&#31283;&#20581;&#36816;&#34892;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#24182;&#23558;&#20572;&#30005;&#20107;&#20214;&#38480;&#21046;&#22312;&#22320;&#26041;&#33539;&#22260;&#20869;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;PowRL&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20197;&#32531;&#35299;&#24847;&#22806;&#32593;&#32476;&#20107;&#20214;&#30340;&#24433;&#21709;&#65292;&#24182;&#21487;&#38752;&#22320;&#22312;&#32593;&#32476;&#19978;&#38543;&#26102;&#32500;&#25345;&#30005;&#21147;&#12290;PowRL&#21033;&#29992;&#26032;&#39062;&#30340;&#36229;&#36127;&#33655;&#31649;&#29702;&#21551;&#21457;&#24335;&#20197;&#21450;&#22522;&#20110;RL&#25552;&#20379;&#30340;&#26368;&#20248;&#25299;&#25169;&#36873;&#25321;&#20915;&#31574;&#65292;&#20197;&#30830;&#20445;&#30005;&#32593;&#22312;&#21464;&#21270;&#21644;&#19981;&#30830;&#23450;&#26465;&#20214;&#19979;&#23433;&#20840;&#12289;&#21487;&#38752;&#22320;&#36816;&#34892;&#65288;&#26080;&#36229;&#36733;&#32447;&#36335;&#21644;&#26080;&#20572;&#30005;&#20107;&#20214;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Power grids, across the world, play an important societal and economical role by providing uninterrupted, reliable and transient-free power to several industries, businesses and household consumers. With the advent of renewable power resources and EVs resulting into uncertain generation and highly dynamic load demands, it has become ever so important to ensure robust operation of power networks through suitable management of transient stability issues and localize the events of blackouts. In the light of ever increasing stress on the modern grid infrastructure and the grid operators, this paper presents a reinforcement learning (RL) framework, PowRL, to mitigate the effects of unexpected network events, as well as reliably maintain electricity everywhere on the network at all times. The PowRL leverages a novel heuristic for overload management, along with the RL-guided decision making on optimal topology selection to ensure that the grid is operated safely and reliably (with no overloa
&lt;/p&gt;</description></item><item><title>CEC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#65292;&#20854;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#27604;&#26368;&#20808;&#36827;&#30340;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15183</link><description>&lt;p&gt;
&#36830;&#32493;&#24773;&#26223;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continuous Episodic Control. (arXiv:2211.15183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15183
&lt;/p&gt;
&lt;p&gt;
CEC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#65292;&#20854;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#27604;&#26368;&#20808;&#36827;&#30340;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#21487;&#20197;&#29992;&#20110;&#24555;&#36895;&#38145;&#23450;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#39640;&#22870;&#21169;&#30340;&#32463;&#39564;&#12290;&#19982;&#21442;&#25968;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#21442;&#25968;&#38656;&#35201;&#32531;&#24930;&#22320;&#21453;&#21521;&#20256;&#36882;&#22870;&#21169;&#20449;&#21495;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#38656;&#35201;&#21457;&#29616;&#19968;&#27425;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#21518;&#23601;&#21487;&#20197;&#21453;&#22797;&#35299;&#20915;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24773;&#26223;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#23384;&#20648;&#22312;&#31163;&#25955;&#34920;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#36804;&#20170;&#21482;&#24212;&#29992;&#20110;&#31163;&#25955;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#24773;&#26223;&#25511;&#21046;&#65288;CEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#12290;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#38271;&#26399;&#24615;&#33021;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;CEC&#21487;&#20197;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-parametric episodic memory can be used to quickly latch onto high-rewarded experience in reinforcement learning tasks. In contrast to parametric deep reinforcement learning approaches in which reward signals need to be back-propagated slowly, these methods only need to discover the solution once, and may then repeatedly solve the task. However, episodic control solutions are stored in discrete tables, and this approach has so far only been applied to discrete action space problems. Therefore, this paper introduces Continuous Episodic Control (CEC), a novel non-parametric episodic memory algorithm for sequential decision making in problems with a continuous action space. Results on several sparse-reward continuous control environments show that our proposed method learns faster than state-of-the-art model-free RL and memory-augmented RL algorithms, while maintaining good long-run performance as well. In short, CEC can be a fast approach for learning in continuous control tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#39044;&#27979;&#20989;&#25968;&#19968;&#36215;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#39044;&#27979;&#20989;&#25968;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#27169;&#22411;&#27531;&#24046;&#12290;&#24341;&#20837;&#20102;Cohort Shapley&#30340;&#31215;&#20998;&#26799;&#24230;&#29256;&#26412;&#65288;IGCS&#65289;&#65292;&#20351;&#24471;&#22312;&#20108;&#20803;&#39044;&#27979;&#22120;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#20351;&#29992;IG&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.08414</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#26080;&#27169;&#22411;&#21464;&#37327;&#37325;&#35201;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model free variable importance for high dimensional data. (arXiv:2211.08414v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08414
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#39044;&#27979;&#20989;&#25968;&#19968;&#36215;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#39044;&#27979;&#20989;&#25968;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#27169;&#22411;&#27531;&#24046;&#12290;&#24341;&#20837;&#20102;Cohort Shapley&#30340;&#31215;&#20998;&#26799;&#24230;&#29256;&#26412;&#65288;IGCS&#65289;&#65292;&#20351;&#24471;&#22312;&#20108;&#20803;&#39044;&#27979;&#22120;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#20351;&#29992;IG&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26041;&#27861;&#21487;&#19982;&#20219;&#24847;&#39044;&#27979;&#20989;&#25968;&#19968;&#36215;&#20351;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#39044;&#27979;&#20989;&#25968;&#12290;&#36825;&#22312;&#39044;&#27979;&#20989;&#25968;&#26159;&#19987;&#26377;&#30340;&#19988;&#19981;&#21487;&#29992;&#25110;&#26497;&#20854;&#26114;&#36149;&#26102;&#24456;&#26377;&#29992;&#12290;&#24403;&#23545;&#27169;&#22411;&#30340;&#27531;&#24046;&#36827;&#34892;&#30740;&#31350;&#26102;&#20063;&#24456;&#26377;&#29992;&#12290;Cohort Shapley&#65288;CS&#65289;&#26041;&#27861;&#26159;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#20294;&#22312;&#36755;&#20837;&#31354;&#38388;&#30340;&#32500;&#25968;&#19978;&#20855;&#26377;&#25351;&#25968;&#25104;&#26412;&#12290;Frye&#31561;&#20154;&#65288;2020&#65289;&#30340;&#30417;&#30563;&#27969;&#24418;&#19978;Shapley&#26041;&#27861;&#20063;&#26159;&#26080;&#27169;&#22411;&#30340;&#65292;&#20294;&#35201;&#27714;&#36755;&#20837;&#31532;&#20108;&#20010;&#40657;&#21283;&#23376;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#24517;&#39035;&#20026;Shapley&#20540;&#38382;&#39064;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Cohort Shapley&#30340;&#31215;&#20998;&#26799;&#24230;&#65288;IG&#65289;&#29256;&#26412;&#65292;&#31216;&#20026;IGCS&#65292;&#25104;&#26412;&#20026;$\mathcal{O}(nd)$&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#32477;&#22823;&#22810;&#25968;&#30456;&#20851;&#21333;&#20803;&#30340;&#31435;&#26041;&#20307;&#19978;&#65292;IGCS&#20540;&#20989;&#25968;&#25509;&#36817;&#22810;&#32447;&#24615;&#20989;&#25968;&#65292;&#20854;&#20013;IGCS&#21305;&#37197;CS&#12290;IGCS&#30340;&#21478;&#19968;&#20010;&#22909;&#22788;&#26159;&#23427;&#20801;&#35768;&#20351;&#29992;&#20108;&#20803;&#39044;&#27979;&#22120;&#36827;&#34892;IG&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20123;&#38754;&#31215;...
&lt;/p&gt;
&lt;p&gt;
A model-agnostic variable importance method can be used with arbitrary prediction functions. Here we present some model-free methods that do not require access to the prediction function. This is useful when that function is proprietary and not available, or just extremely expensive. It is also useful when studying residuals from a model. The cohort Shapley (CS) method is model-free but has exponential cost in the dimension of the input space. A supervised on-manifold Shapley method from Frye et al. (2020) is also model free but requires as input a second black box model that has to be trained for the Shapley value problem. We introduce an integrated gradient (IG) version of cohort Shapley, called IGCS, with cost $\mathcal{O}(nd)$. We show that over the vast majority of the relevant unit cube that the IGCS value function is close to a multilinear function for which IGCS matches CS. Another benefit of IGCS is that is allows IG methods to be used with binary predictors. We use some area 
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#25104;&#20026;&#28909;&#38376;&#26041;&#27861;&#65292;&#20294;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20250;&#38754;&#20020;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;"&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;</title><link>http://arxiv.org/abs/2211.02658</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#22788;&#29702;&#23398;&#20064;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02658
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#25104;&#20026;&#28909;&#38376;&#26041;&#27861;&#65292;&#20294;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20250;&#38754;&#20020;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;"&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064; (ML) &#24050;&#25104;&#20026;&#25903;&#25345;&#33258;&#36866;&#24212;&#30340;&#28909;&#38376;&#26041;&#27861;&#12290;ML &#24050;&#34987;&#29992;&#26469;&#22788;&#29702;&#33258;&#36866;&#24212;&#20013;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#32500;&#25252;&#26368;&#26032;&#30340;&#36816;&#34892;&#26102;&#27169;&#22411;&#21644;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992; ML &#23384;&#22312;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#38754;&#21521;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31995;&#32479;&#30340;&#19968;&#20010;&#29305;&#21035;&#37325;&#35201;&#30340;&#25361;&#25112;&#65306;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#36866;&#24212;&#31354;&#38388;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#33258;&#36866;&#24212;&#31995;&#32479;&#22312;&#26576;&#19968;&#29305;&#23450;&#26102;&#38388;&#21487;&#20197;&#36873;&#25321;&#30340;&#36866;&#24212;&#36873;&#39033;&#30340;&#38598;&#21512;&#65292;&#20197;&#26681;&#25454;&#36866;&#24212;&#36873;&#39033;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#36866;&#24212;&#12290;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#28304;&#20110;&#24433;&#21709;&#36866;&#24212;&#36873;&#39033;&#36136;&#37327;&#23646;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#28418;&#31227;&#21487;&#33021;&#24847;&#21619;&#30528;&#26368;&#32456;&#27809;&#26377;&#36866;&#24212;&#36873;&#39033;&#33021;&#22815;&#28385;&#36275;&#26368;&#21021;&#30340;&#36866;&#24212;&#30446;&#26631;&#65292;&#20174;&#32780;&#38477;&#20302;&#31995;&#32479;&#30340;&#36136;&#37327;&#65292;&#25110;&#32773;&#21487;&#33021;&#20986;&#29616;&#20801;&#35768;&#22686;&#24378;&#36866;&#24212;&#30446;&#26631;&#30340;&#36866;&#24212;&#36873;&#39033;&#12290;&#22312; ML &#20013;&#65292;&#36825;&#31181;&#28418;&#31227;&#36890;&#24120;&#34987;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#25110;&#23454;&#20363;&#28418;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#23545; ML powered self-adaptation &#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this paper, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space we refer to the set of adaptation options a self-adaptive system can select from at a given time to adapt based on the estimated quality properties of the adaptation options. Drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that eventually no adaptation option can satisfy the initial set of the adaptation goals, deteriorating the quality of the system, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such shift cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#30913;&#38567;&#36947;&#32467;&#23558;&#22810;&#39057;&#29575;RF&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25104;&#20026;&#23454;&#29616;&#23884;&#20837;&#24335;&#23556;&#39057;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2211.01131</link><description>&lt;p&gt;
&#21033;&#29992;&#30913;&#38567;&#36947;&#32467;&#23558;&#22810;&#39057;&#29575;&#23556;&#39057;&#20449;&#21495;&#36827;&#34892;&#26497;&#38480;&#23398;&#20064;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of multi-frequency RF signals by extreme learning, using magnetic tunnel junctions as neurons and synapses. (arXiv:2211.01131v2 [cond-mat.mes-hall] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#30913;&#38567;&#36947;&#32467;&#23558;&#22810;&#39057;&#29575;RF&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25104;&#20026;&#23454;&#29616;&#23884;&#20837;&#24335;&#23556;&#39057;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35832;&#22914;&#38647;&#36798;&#21644;&#20581;&#24247;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#20197;&#20302;&#33021;&#32791;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20174;&#23556;&#39057;(RF)&#20449;&#21495;&#20013;&#25552;&#21462;&#20449;&#24687;&#26159;&#19968;&#39033;&#20851;&#38190;&#38656;&#27714;&#12290;&#36825;&#20123;RF&#36755;&#20837;&#30001;&#22810;&#20010;&#39057;&#29575;&#32452;&#25104;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#30913;&#38567;&#36947;&#32467;&#21487;&#20197;&#24182;&#34892;&#22788;&#29702;&#20855;&#26377;&#22810;&#20010;&#39057;&#29575;&#30340;&#27169;&#25311;RF&#36755;&#20837;&#24182;&#25191;&#34892;&#31361;&#35302;&#25805;&#20316;&#12290;&#20351;&#29992;&#19968;&#31181;&#26080;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#27861;&#31216;&#20026;&#26497;&#38480;&#23398;&#20064;&#65292;&#25105;&#20204;&#21033;&#29992;&#20316;&#20026;&#31361;&#35302;&#21644;&#31070;&#32463;&#20803;&#30340;&#30913;&#38567;&#36947;&#32467;&#30340;&#23454;&#39564;&#25968;&#25454;&#65292;&#23545;&#30001;RF&#20449;&#21495;&#32534;&#30721;&#30340;&#22122;&#22768;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#31561;&#25928;&#36719;&#20214;&#31070;&#32463;&#32593;&#32476;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#23884;&#20837;&#24335;&#23556;&#39057;&#20154;&#24037;&#26234;&#33021;&#36808;&#20986;&#20102;&#20851;&#38190;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting information from radiofrequency (RF) signals using artificial neural networks at low energy cost is a critical need for a wide range of applications from radars to health. These RF inputs are composed of multiples frequencies. Here we show that magnetic tunnel junctions can process analogue RF inputs with multiple frequencies in parallel and perform synaptic operations. Using a backpropagation-free method called extreme learning, we classify noisy images encoded by RF signals, using experimental data from magnetic tunnel junctions functioning as both synapses and neurons. We achieve the same accuracy as an equivalent software neural network. These results are a key step for embedded radiofrequency artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#20272;&#35745;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#20998;&#37197;&#36793;&#32536;&#30340;&#24863;&#30693;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20174;&#32780;&#20351;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35745;&#31639;&#20013;&#23454;&#29616;&#26356;&#20339;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.02166</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35745;&#31639;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#26234;&#33021;&#20256;&#24863;&#65306;&#35745;&#31639;&#36824;&#26159;&#19981;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
To Compute or not to Compute? Adaptive Smart Sensing in Resource-Constrained Edge Computing. (arXiv:2209.02166v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#20272;&#35745;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#20998;&#37197;&#36793;&#32536;&#30340;&#24863;&#30693;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20174;&#32780;&#20351;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35745;&#31639;&#20013;&#23454;&#29616;&#26356;&#20339;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#65292;&#24212;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#20013;&#65292;&#23545;&#24863;&#20852;&#36259;&#30340;&#20449;&#21495;&#36827;&#34892;&#37319;&#26679;&#24182;&#21521;&#22522;&#31449;&#21457;&#36865;&#26356;&#26032;&#20197;&#36827;&#34892;&#36828;&#31243;&#20840;&#23616;&#30417;&#25511;&#12290;&#20256;&#24863;&#22120;&#37197;&#22791;&#26377;&#24863;&#30693;&#21644;&#35745;&#31639;&#21151;&#33021;&#65292;&#21487;&#20197;&#22312;&#20256;&#36755;&#20043;&#21069;&#22312;&#26495;&#19978;&#22788;&#29702;&#21407;&#22987;&#25968;&#25454;&#25110;&#30452;&#25509;&#21457;&#36865;&#21407;&#22987;&#25968;&#25454;&#12290;&#36793;&#32536;&#30340;&#30828;&#20214;&#36164;&#28304;&#26377;&#38480;&#65292;&#20135;&#29983;&#20102;&#22522;&#26412;&#30340;&#24310;&#36831; - &#31934;&#24230;&#26435;&#34913;&#65306;&#21407;&#22987;&#27979;&#37327;&#19981;&#20934;&#30830;&#20294;&#21450;&#26102;&#65292;&#32780;&#32463;&#36807;&#35745;&#31639;&#24310;&#36831;&#21518;&#65292;&#20934;&#30830;&#30340;&#22788;&#29702;&#26356;&#26032;&#23601;&#21487;&#29992;&#12290;&#21478;&#22806;&#65292;&#22914;&#26524;&#20256;&#24863;&#22120;&#26495;&#19978;&#22788;&#29702;&#28041;&#21450;&#25968;&#25454;&#21387;&#32553;&#65292;&#21017;&#30001;&#20110;&#26080;&#32447;&#36890;&#20449;&#24341;&#36215;&#30340;&#24310;&#36831;&#21487;&#33021;&#20250;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20915;&#23450;&#20309;&#26102;&#20256;&#24863;&#22120;&#24212;&#35813;&#20256;&#36755;&#21407;&#22987;&#27979;&#37327;&#25968;&#25454;&#25110;&#20381;&#36182;&#26412;&#22320;&#22788;&#29702;&#20197;&#26368;&#22823;&#21270;&#25972;&#20307;&#32593;&#32476;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20256;&#24863;&#35774;&#35745;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#23884;&#20837;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#30340;&#20272;&#35745;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#20998;&#37197;&#36793;&#32536;&#30340;&#24863;&#30693;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#23454;&#29616;&#26356;&#20302;&#30340;&#20272;&#35745;&#35823;&#24046;&#12289;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#21644;&#26356;&#20302;&#30340;&#33021;&#32791;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#20256;&#36755;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a network of smart sensors for edge computing application that sample a signal of interest and send updates to a base station for remote global monitoring. Sensors are equipped with sensing and compute, and can either send raw data or process them on-board before transmission. Limited hardware resources at the edge generate a fundamental latency-accuracy trade-off: raw measurements are inaccurate but timely, whereas accurate processed updates are available after computational delay. Also, if sensor on-board processing entails data compression, latency caused by wireless communication might be higher for raw measurements. Hence, one needs to decide when sensors should transmit raw measurements or rely on local processing to maximize overall network performance. To tackle this sensing design problem, we model an estimation-theoretic optimization framework that embeds computation and communication delays, and propose a Reinforcement Learning-based approach to dynamically alloc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36127;&#38754;&#20154;&#26435;&#22914;&#20309;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#21407;&#21017;&#65292;&#20026;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#25216;&#26415;&#23433;&#20840;&#32422;&#26463;&#25552;&#20379;&#22522;&#30784;&#65292;&#24182;&#25903;&#25345;&#22269;&#38469;&#30417;&#31649;&#20307;&#31995;&#30340;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2208.14788</link><description>&lt;p&gt;
&#20197;&#36127;&#38754;&#20154;&#26435;&#20026;&#22522;&#30784;&#30340;&#38271;&#26399;AI&#23433;&#20840;&#21644;&#30417;&#31649;
&lt;/p&gt;
&lt;p&gt;
Negative Human Rights as a Basis for Long-term AI Safety and Regulation. (arXiv:2208.14788v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36127;&#38754;&#20154;&#26435;&#22914;&#20309;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#21407;&#21017;&#65292;&#20026;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#25216;&#26415;&#23433;&#20840;&#32422;&#26463;&#25552;&#20379;&#22522;&#30784;&#65292;&#24182;&#25903;&#25345;&#22269;&#38469;&#30417;&#31649;&#20307;&#31995;&#30340;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#33258;&#20027;&#30340;AI&#31995;&#32479;&#35201;&#22312;&#26032;&#39046;&#22495;&#20869;&#20445;&#25345;&#21487;&#38752;&#30340;&#23433;&#20840;&#24615;&#65292;&#23427;&#20204;&#38656;&#35201;&#34701;&#20837;&#19968;&#20123;&#36890;&#29992;&#21407;&#21017;&#26469;&#25351;&#23548;&#20854;&#35782;&#21035;&#21644;&#36991;&#20813;&#26377;&#23475;&#34892;&#20026;&#12290;&#36825;&#20123;&#21407;&#21017;&#21487;&#33021;&#38656;&#35201;&#30001;&#19968;&#22871;&#26377;&#32422;&#26463;&#21147;&#30340;&#30417;&#31649;&#20307;&#31995;&#25903;&#25345;&#65292;&#32780;&#30417;&#31649;&#20307;&#31995;&#38656;&#35201;&#20855;&#22791;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#22522;&#30784;&#24615;&#21407;&#21017;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#21407;&#21017;&#20063;&#38656;&#35201;&#20855;&#20307;&#21040;&#21487;&#25216;&#26415;&#23454;&#29616;&#30340;&#23618;&#38754;&#12290;&#26412;&#25991;&#20174;&#27861;&#24459;&#30340;&#35282;&#24230;&#38416;&#36848;&#20102;&#36127;&#38754;&#20154;&#26435;&#22914;&#20309;&#32988;&#20219;&#36825;&#20123;&#21407;&#21017;&#30340;&#35282;&#33394;&#65292;&#19981;&#20165;&#33021;&#22815;&#20316;&#20026;&#22269;&#38469;&#30417;&#31649;&#20307;&#31995;&#30340;&#22522;&#30784;&#65292;&#32780;&#19988;&#33021;&#26500;&#24314;&#20026;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#25216;&#26415;&#23433;&#20840;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
If autonomous AI systems are to be reliably safe in novel situations, they will need to incorporate general principles guiding them to recognize and avoid harmful behaviours. Such principles may need to be supported by a binding system of regulation, which would need the underlying principles to be widely accepted. They should also be specific enough for technical implementation. Drawing inspiration from law, this article explains how negative human rights could fulfil the role of such principles and serve as a foundation both for an international regulatory system and for building technical safety constraints for future AI systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.07886</link><description>&lt;p&gt;
&#22522;&#20110;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07886
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#19981;&#26029;&#35775;&#38382;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#36807;&#31243;&#36890;&#24120;&#20250;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#65288;&#20363;&#22914;CPU&#65292;GPU&#65289;&#22312;&#20869;&#23384;&#21333;&#20803;&#21644;&#22788;&#29702;&#21333;&#20803;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#26041;&#38754;&#23384;&#22312;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#36825;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#33021;&#37327;&#21644;&#25191;&#34892;&#21608;&#26399;&#12290;&#20855;&#26377;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#21151;&#33021;&#30340;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20102;&#35299;&#29616;&#20195;&#36890;&#29992;PIM&#26550;&#26500;&#21152;&#36895;ML&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#65288;1&#65289;&#22312;&#23454;&#38469;&#36890;&#29992;PIM&#26550;&#26500;&#19978;&#23454;&#29616;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#20256;&#32479;ML&#31639;&#27861;&#65288;&#21363;&#32447;&#24615;&#22238;&#24402;&#65292;&#36923;&#36753;&#22238;&#24402;&#65292;&#20915;&#31574;&#26641;&#65292;K-Means&#32858;&#31867;&#65289;&#65292;&#65288;2&#65289;&#20005;&#26684;&#35780;&#20272;&#21644;&#34920;&#24449;&#36825;&#20123;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24615;&#33021;&#21644;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#65288;3&#65289;&#19982;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#30340;&#30456;&#24212;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#30456;&#24212;&#30340;CPU&#21644;GPU&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#39033;&#24335;Zonotopes&#36827;&#34892;&#24320;&#29615;&#21644;&#38381;&#29615;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#12290;&#20854;&#20027;&#35201;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#38750;&#20984;&#24615;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2207.02715</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#39033;&#24335;Zonotopes&#36827;&#34892;&#24320;&#29615;&#21644;&#38381;&#29615;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Open- and Closed-Loop Neural Network Verification using Polynomial Zonotopes. (arXiv:2207.02715v2 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#39033;&#24335;Zonotopes&#36827;&#34892;&#24320;&#29615;&#21644;&#38381;&#29615;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#12290;&#20854;&#20027;&#35201;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#38750;&#20984;&#24615;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#39033;&#24335;Zonotopes&#39640;&#25928;&#22320;&#35745;&#31639;&#20855;&#26377;ReLU&#12289;sigmoid&#25110;&#21452;&#26354;&#27491;&#20999;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#22270;&#20687;&#30340;&#32039;&#23494;&#38750;&#20984;&#21253;&#22260;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22810;&#39033;&#24335;&#36924;&#36817;&#26469;&#25277;&#35937;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22810;&#39033;&#24335;Zonotopes&#20197;&#38598;&#21512;&#30340;&#26041;&#24335;&#36827;&#34892;&#27714;&#20540;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#36866;&#29992;&#20110;&#24320;&#29615;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#20294;&#25105;&#20204;&#30340;&#20027;&#35201;&#24212;&#29992;&#26159;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#20854;&#20013;&#22810;&#39033;&#24335;Zonotopes&#33021;&#22815;&#25429;&#25417;&#30001;&#31070;&#32463;&#32593;&#32476;&#21450;&#31995;&#32479;&#21160;&#24577;&#24341;&#36215;&#30340;&#38750;&#20984;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#65292;&#36825;&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to efficiently compute tight non-convex enclosures of the image through neural networks with ReLU, sigmoid, or hyperbolic tangent activation functions. In particular, we abstract the input-output relation of each neuron by a polynomial approximation, which is evaluated in a set-based manner using polynomial zonotopes. While our approach can also can be beneficial for open-loop neural network verification, our main application is reachability analysis of neural network controlled systems, where polynomial zonotopes are able to capture the non-convexity caused by the neural network as well as the system dynamics. This results in a superior performance compared to other methods, as we demonstrate on various benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#20197;&#26816;&#27979;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.07813</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Search-Based Testing Approach for Deep Reinforcement Learning Agents. (arXiv:2206.07813v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#20197;&#26816;&#27979;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#29983;&#21629;&#23433;&#20840;&#29615;&#22659;&#20013;&#32463;&#24120;&#34920;&#29616;&#20986;&#38169;&#35823;&#34892;&#20026;&#65292;&#23548;&#33268;&#28508;&#22312;&#30340;&#37325;&#22823;&#38169;&#35823;&#65292;&#22240;&#27492;&#23427;&#20204;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#23545;&#23427;&#20204;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#25925;&#38556;&#30340;&#25925;&#38556;&#12290;&#36825;&#23601;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#25105;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#27979;&#35797;DRL&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#30340;&#27491;&#30830;&#24615;&#21644;&#36981;&#23432;&#23433;&#20840;&#35201;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#20195;&#29702;&#29983;&#25104;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#30340;&#29366;&#24577;&#24207;&#21015;&#21464;&#21270;&#65292;&#20197;&#25506;&#32034;&#29615;&#22659;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;DRL&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#20445;&#25345;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#27979;&#35797;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One way to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Their main goal is to test the robustness of DRL agents rather than testing the compliance of agents' policies with respect to requirements. Due to the huge state spac
&lt;/p&gt;</description></item><item><title>&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2205.14590</link><description>&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14590
&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#29609;&#23478;&#19981;&#20102;&#35299;&#28216;&#25103;&#27169;&#22411;&#65292;&#20063;&#19981;&#33021;&#36827;&#34892;&#21327;&#35843;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#29609;&#23478;&#36890;&#36807;&#24322;&#27493;&#26041;&#24335;&#26356;&#26032;&#20182;&#20204;&#30340;&#25171;&#25200;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#65292;&#35813;&#20989;&#25968;&#26681;&#25454;&#23454;&#29616;&#30340;&#19968;&#38454;&#27573;&#22870;&#21169;&#35780;&#20272;&#20182;&#20204;&#30340;&#24635;&#20307;&#26465;&#20214;&#20184;&#27454;&#12290;&#28982;&#21518;&#65292;&#29609;&#23478;&#36890;&#36807;&#23558;&#22522;&#20110;&#20272;&#35745;Q&#20989;&#25968;&#30340;&#24179;&#28369;&#26368;&#20248;&#19968;&#38454;&#27573;&#20559;&#24046;&#31574;&#30053;&#32435;&#20837;&#20854;&#31574;&#30053;&#20013;&#26469;&#29420;&#31435;&#22320;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#23398;&#20064;&#21160;&#24577;&#30340;&#20851;&#38190;&#29305;&#24449;&#26159;Q&#20989;&#25968;&#20272;&#35745;&#26159;&#20197;&#27604;&#31574;&#30053;&#26356;&#24555;&#30340;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#26356;&#26032;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#21160;&#24577;&#24341;&#23548;&#30340;&#31574;&#30053;&#22312;&#27010;&#29575;1&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#31616;&#21333;&#23398;&#20064;&#21160;&#24577;&#22312;&#36798;&#21040;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#21363;&#20351;&#26159;&#22312;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#20195;&#29702;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-agent reinforcement learning dynamics, and analyze its convergence in infinite-horizon discounted Markov potential games. We focus on the independent and decentralized setting, where players do not have knowledge of the game model and cannot coordinate. In each stage, players update their estimate of a perturbed Q-function that evaluates their total contingent payoff based on the realized one-stage reward in an asynchronous manner. Then, players independently update their policies by incorporating a smoothed optimal one-stage deviation strategy based on the estimated Q-function. A key feature of the learning dynamics is that the Q-function estimates are updated at a faster timescale than the policies. We prove that the policies induced by our learning dynamics converge to a stationary Nash equilibrium in Markov potential games with probability 1. Our results highlight the efficacy of simple learning dynamics in reaching a stationary Nash equilibrium even in environme
&lt;/p&gt;</description></item><item><title>FLEX&#26159;&#19968;&#31181;&#26032;&#22411;KGR&#26694;&#26550;&#65292;&#33021;&#22815;&#30495;&#27491;&#22788;&#29702;&#21253;&#25324;&#21512;&#21462;&#12289;&#26512;&#21462;&#12289;&#21542;&#23450;&#31561;&#25152;&#26377;FOL&#25805;&#20316;&#24182;&#25903;&#25345;&#21508;&#31181;&#29305;&#24449;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FLEX&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.11039</link><description>&lt;p&gt;
FLEX: &#29992;&#20110;&#22797;&#26434;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#29305;&#24449;&#36923;&#36753;&#23884;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FLEX: Feature-Logic Embedding Framework for CompleX Knowledge Graph Reasoning. (arXiv:2205.11039v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11039
&lt;/p&gt;
&lt;p&gt;
FLEX&#26159;&#19968;&#31181;&#26032;&#22411;KGR&#26694;&#26550;&#65292;&#33021;&#22815;&#30495;&#27491;&#22788;&#29702;&#21253;&#25324;&#21512;&#21462;&#12289;&#26512;&#21462;&#12289;&#21542;&#23450;&#31561;&#25152;&#26377;FOL&#25805;&#20316;&#24182;&#25903;&#25345;&#21508;&#31181;&#29305;&#24449;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FLEX&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65288;KGR&#65289;&#30340;&#24403;&#21069;&#26368;&#20339;&#27169;&#22411;&#24341;&#20837;&#20960;&#20309;&#23545;&#35937;&#25110;&#27010;&#29575;&#20998;&#24067;&#23558;&#23454;&#20307;&#21644;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#26597;&#35810;&#23884;&#20837;&#21040;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#23427;&#20204;&#21487;&#20197;&#24635;&#32467;&#20026;&#19968;&#20010;&#20013;&#24515;-&#23610;&#23544;&#26694;&#26550;&#65288;&#28857;/&#30418;&#24335;/&#38181;&#24418;&#65292;Beta/Gaussian&#20998;&#24067;&#31561;&#65289;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#36923;&#36753;&#23884;&#20837;&#26694;&#26550;&#65288;FLEX&#65289;&#30340;&#26032;&#22411;KGR&#26694;&#26550;&#65292;&#23427;&#26159;&#39318;&#20010;&#30495;&#27491;&#33021;&#22815;&#22788;&#29702;&#21253;&#25324;&#21512;&#21462;&#12289;&#26512;&#21462;&#12289;&#21542;&#23450;&#31561;&#25152;&#26377;FOL&#25805;&#20316;&#24182;&#25903;&#25345;&#21508;&#31181;&#29305;&#24449;&#31354;&#38388;&#30340;KGR&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#29305;&#24449;&#36923;&#36753;&#26694;&#26550;&#30340;&#36923;&#36753;&#37096;&#20998;&#22522;&#20110;&#21521;&#37327;&#36923;&#36753;&#65292;&#33258;&#28982;&#22320;&#24314;&#27169;&#20102;&#25152;&#26377;FOL&#25805;&#20316;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FLEX&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current best performing models for knowledge graph reasoning (KGR) introduce geometry objects or probabilistic distributions to embed entities and first-order logical (FOL) queries into low-dimensional vector spaces. They can be summarized as a center-size framework (point/box/cone, Beta/Gaussian distribution, etc.). However, they have limited logical reasoning ability. And it is difficult to generalize to various features, because the center and size are one-to-one constrained, unable to have multiple centers or sizes. To address these challenges, we instead propose a novel KGR framework named Feature-Logic Embedding framework, FLEX, which is the first KGR framework that can not only TRULY handle all FOL operations including conjunction, disjunction, negation and so on, but also support various feature spaces. Specifically, the logic part of feature-logic framework is based on vector logic, which naturally models all FOL operations. Experiments demonstrate that FLEX significantly outp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FedCAMS&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#37325;&#22797;&#30340;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#21516;&#27493;&#32780;&#20135;&#29983;&#30340;&#22823;&#37327;&#36890;&#20449;&#24320;&#38144;&#21644;&#22522;&#20110; SGD &#30340;&#27169;&#22411;&#26356;&#26032;&#32570;&#20047;&#36866;&#24212;&#24615;&#31561;&#35832;&#22810;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.02719</link><description>&lt;p&gt;
&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Adaptive Federated Learning. (arXiv:2205.02719v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FedCAMS&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#37325;&#22797;&#30340;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#21516;&#27493;&#32780;&#20135;&#29983;&#30340;&#22823;&#37327;&#36890;&#20449;&#24320;&#38144;&#21644;&#22522;&#20110; SGD &#30340;&#27169;&#22411;&#26356;&#26032;&#32570;&#20047;&#36866;&#24212;&#24615;&#31561;&#35832;&#22810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#24335;&#65292;&#20351;&#24471;&#23458;&#25143;&#31471;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20173;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#30001;&#20110;&#37325;&#22797;&#30340;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#21516;&#27493;&#32780;&#20135;&#29983;&#30340;&#22823;&#37327;&#36890;&#20449;&#24320;&#38144;&#20197;&#21450;&#22522;&#20110; SGD &#30340;&#27169;&#22411;&#26356;&#26032;&#32570;&#20047;&#36866;&#24212;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#26799;&#24230;&#21387;&#32553;&#25110;&#37327;&#21270;&#26469;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;FedAdam&#31561;&#32852;&#37030;&#29256;&#26412;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#26469;&#22686;&#21152;&#26356;&#22810;&#30340;&#36866;&#24212;&#24615;&#65292;&#20294;&#24403;&#21069;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20173;&#26080;&#27861;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#25152;&#26377;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FedCAMS&#65289;&#65292;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a machine learning training paradigm that enables clients to jointly train models without sharing their own localized data. However, the implementation of federated learning in practice still faces numerous challenges, such as the large communication overhead due to the repetitive server-client synchronization and the lack of adaptivity by SGD-based model updates. Despite that various methods have been proposed for reducing the communication cost by gradient compression or quantization, and the federated versions of adaptive optimizers such as FedAdam are proposed to add more adaptivity, the current federated learning framework still cannot solve the aforementioned challenges all at once. In this paper, we propose a novel communication-efficient adaptive federated learning method (FedCAMS) with theoretical convergence guarantees. We show that in the nonconvex stochastic optimization setting, our proposed FedCAMS achieves the same convergence rate of $O(\frac{1}{\s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#26500;&#24314;&#20165;&#20351;&#29992;&#27491;&#24120;&#30340;&#26085;&#24535;&#26465;&#30446;&#26469;&#35757;&#32451;&#26032;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#24418;&#24335;&#30340;&#25200;&#21160;&#26469;&#22686;&#24378;&#26085;&#24535;&#65292;&#20197;&#24212;&#23545;&#26085;&#24535;&#26469;&#28304;&#30340;&#24322;&#26500;&#24615;&#21644;&#32570;&#20047;&#26631;&#31614;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#38480;&#21046;&#65307;&#35813;&#27169;&#22411;&#36827;&#19968;&#27493;&#20351;&#29992;&#19968;&#31181;&#26377;&#38480;&#30340;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2203.10960</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;&#26085;&#24535;&#20998;&#26512;&#22120;: &#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AI based Log Analyser: A Practical Approach. (arXiv:2203.10960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#26500;&#24314;&#20165;&#20351;&#29992;&#27491;&#24120;&#30340;&#26085;&#24535;&#26465;&#30446;&#26469;&#35757;&#32451;&#26032;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#24418;&#24335;&#30340;&#25200;&#21160;&#26469;&#22686;&#24378;&#26085;&#24535;&#65292;&#20197;&#24212;&#23545;&#26085;&#24535;&#26469;&#28304;&#30340;&#24322;&#26500;&#24615;&#21644;&#32570;&#20047;&#26631;&#31614;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#38480;&#21046;&#65307;&#35813;&#27169;&#22411;&#36827;&#19968;&#27493;&#20351;&#29992;&#19968;&#31181;&#26377;&#38480;&#30340;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#20998;&#26512;&#26159;&#36827;&#34892;&#25925;&#38556;&#25110;&#32593;&#32476;&#20107;&#20214;&#26816;&#27979;&#12289;&#35843;&#26597;&#21644;&#25216;&#26415;&#21462;&#35777;&#20998;&#26512;&#20197;&#23454;&#29616;&#31995;&#32479;&#21644;&#32593;&#32476;&#38887;&#24615;&#30340;&#37325;&#35201;&#27963;&#21160;&#12290;AI&#31639;&#27861;&#22312;&#26085;&#24535;&#20998;&#26512;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#21487;&#20197;&#22686;&#24378;&#36825;&#31181;&#22797;&#26434;&#21644;&#32321;&#37325;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#26085;&#24535;&#26469;&#28304;&#30340;&#24322;&#26500;&#24615;&#21644;&#27809;&#26377;&#25110;&#24456;&#23569;&#26631;&#31614;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#24403;&#36825;&#20123;&#26631;&#31614;&#21464;&#24471;&#21487;&#29992;&#26102;&#65292;&#38656;&#35201;&#26356;&#26032;&#20998;&#31867;&#22120;&#12290;&#36825;&#39033;&#22522;&#20110;&#23454;&#36341;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Transformer&#26500;&#24314;&#26469;&#20165;&#20351;&#29992;&#27491;&#24120;&#30340;&#26085;&#24535;&#26465;&#30446;&#35757;&#32451;&#26032;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#22810;&#31181;&#24418;&#24335;&#30340;&#25200;&#21160;&#26469;&#22686;&#24378;&#26085;&#24535;&#65292;&#20197;&#20316;&#20026;&#33258;&#30417;&#30563;&#35757;&#32451;&#29992;&#20110;&#29305;&#24449;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#36827;&#19968;&#27493;&#20351;&#29992;&#19968;&#31181;&#26377;&#38480;&#30340;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#27169;&#20223;&#23454;&#38469;&#24773;&#20917;&#19979;&#26377;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of logs is a vital activity undertaken for fault or cyber incident detection, investigation and technical forensics analysis for system and cyber resilience. The potential application of AI algorithms for Log analysis could augment such complex and laborious tasks. However, such solution has its constraints the heterogeneity of log sources and limited to no labels for training a classifier. When such labels become available, the need for the classifier to be updated. This practice-based research seeks to address these challenges with the use of Transformer construct to train a new model with only normal log entries. Log augmentation through multiple forms of perturbation is applied as a form of self-supervised training for feature learning. The model is further finetuned using a form of reinforcement learning with a limited set of label samples to mimic real-world situation with the availability of labels. The experimental results of our model construct show promise with c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20013;&#36807;&#20110;&#33258;&#20449;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#26469;&#36991;&#20813;&#20256;&#32479;&#30340;&#39044;&#27979;&#23618;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;</title><link>http://arxiv.org/abs/2112.01360</link><description>&lt;p&gt;
&#36335;&#29992;&#25143;&#26816;&#27979;&#30340;&#27010;&#29575;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Approach for Road-Users Detection. (arXiv:2112.01360v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20013;&#36807;&#20110;&#33258;&#20449;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#26469;&#36991;&#20813;&#20256;&#32479;&#30340;&#39044;&#27979;&#23618;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#24847;&#21619;&#30528;&#23545;&#35821;&#20041;&#23545;&#35937;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#36890;&#24120;&#26159;&#22478;&#24066;&#39550;&#39542;&#29615;&#22659;&#30340;&#29305;&#33394;&#65292;&#22914;&#34892;&#20154;&#21644;&#36710;&#36742;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#24102;&#26377;&#36807;&#20110;&#33258;&#20449;&#30340;&#24471;&#20998;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20854;&#20182;&#20851;&#38190;&#30340;&#26426;&#22120;&#20154;&#24863;&#30693;&#39046;&#22495;&#65292;&#36825;&#26159;&#38750;&#24120;&#19981;&#24076;&#26395;&#30475;&#21040;&#30340;&#65292;&#22240;&#20026;&#28041;&#21450;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#20013;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#65292;&#21521;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#20013;&#28155;&#21152;&#36825;&#31181;&#27010;&#29575;&#23618;&#12290;&#24314;&#35758;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#20256;&#32479;&#30340;Sigmoid&#25110;Softmax&#39044;&#27979;&#23618;&#65292;&#36825;&#20123;&#23618;&#36890;&#24120;&#20250;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#30495;&#38451;&#24615;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;2D-KITTI&#30446;&#26631;&#26816;&#27979;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#20351;&#29992;&#20102;YOLOV4&#21644;S&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection in autonomous driving applications implies that the detection and tracking of semantic objects are commonly native to urban driving environments, as pedestrians and vehicles. One of the major challenges in state-of-the-art deep-learning based object detection are false positives which occur with overconfident scores. This is highly undesirable in autonomous driving and other critical robotic-perception domains because of safety concerns. This paper proposes an approach to alleviate the problem of overconfident predictions by introducing a novel probabilistic layer to deep object detection networks in testing. The suggested approach avoids the traditional Sigmoid or Softmax prediction layer which often produces overconfident predictions. It is demonstrated that the proposed technique reduces overconfidence in the false positives without degrading the performance on the true positives. The approach is validated on the 2D-KITTI objection detection through the YOLOV4 and S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BGNNs&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#19977;&#32500;&#39063;&#31890;&#27969;&#21160;&#30340;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2106.11299</link><description>&lt;p&gt;
&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110; 3D &#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Boundary Graph Neural Networks for 3D Simulations. (arXiv:2106.11299v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BGNNs&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#19977;&#32500;&#39063;&#31890;&#27969;&#21160;&#30340;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#25968;&#25454;&#30340;&#20986;&#29616;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#28982;&#31185;&#23398;&#21644;&#24037;&#31243;&#23398;&#26041;&#38754;&#20855;&#26377;&#20102;&#21487;&#35266;&#30340;&#21160;&#21147;&#65292;&#28982;&#32780;&#23545;&#29289;&#29702;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#21035;&#26840;&#25163;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#34920;&#31034;&#20960;&#20309;&#36793;&#30028;&#12290;&#19977;&#35282;&#21270;&#30340;&#20960;&#20309;&#36793;&#30028;&#22312;&#24037;&#31243;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#29702;&#35299;&#21644;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23427;&#20204;&#30340;&#23610;&#23544;&#21644;&#26041;&#21521;&#30340;&#24322;&#36136;&#24615;&#65292;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#36890;&#24120;&#21313;&#20998;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#29702;&#35770;&#26469;&#24314;&#27169;&#31890;&#23376;&#19982;&#36793;&#30028;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25512;&#23548;&#20986;&#20102;&#25105;&#20204;&#30340;&#26032;&#22411;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BGNNs&#65289;&#65292;&#35813;&#32593;&#32476;&#21160;&#24577;&#22320;&#20462;&#25913;&#22270;&#32467;&#26500;&#20197;&#28385;&#36275;&#36793;&#30028;&#26465;&#20214;&#12290;&#26032;&#30340; BGNNs &#22312;&#22797;&#26434;&#30340;&#19977;&#32500;&#39063;&#31890;&#27969;&#21160;&#36807;&#31243;&#65288;&#22914;&#28431;&#26007;&#12289;&#26059;&#36716;&#40723;&#21644;&#25605;&#25292;&#22120;&#65289;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36825;&#20123;&#36807;&#31243;&#37117;&#26159;&#29616;&#20195;&#24037;&#19994;&#26426;&#26800;&#30340;&#26631;&#20934;&#32452;&#20214;&#65292;&#20294;&#20854;&#20960;&#20309;&#24418;&#29366;&#20173;&#28982;&#21313;&#20998;&#22797;&#26434;&#12290;BGNNs &#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#29575;&#26041;&#38754;&#37117;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abundance of data has given machine learning considerable momentum in natural sciences and engineering, though modeling of physical processes is often difficult. A particularly tough problem is the efficient representation of geometric boundaries. Triangularized geometric boundaries are well understood and ubiquitous in engineering applications. However, it is notoriously difficult to integrate them into machine learning approaches due to their heterogeneity with respect to size and orientation. In this work, we introduce an effective theory to model particle-boundary interactions, which leads to our new Boundary Graph Neural Networks (BGNNs) that dynamically modify graph structures to obey boundary conditions. The new BGNNs are tested on complex 3D granular flow processes of hoppers, rotating drums and mixers, which are all standard components of modern industrial machinery but still have complicated geometry. BGNNs are evaluated in terms of computational efficiency as well as pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#26412;&#20307;&#22312;FOLE&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#25552;&#20379;&#20102;ERA&#25968;&#25454;&#27169;&#22411;&#30340;&#20005;&#26684;&#25968;&#23398;&#34920;&#31034;&#65292;&#20316;&#20026;&#26412;&#20307;&#35770;&#30340;&#22522;&#30784;&#25506;&#35752;&#12290;</title><link>http://arxiv.org/abs/1512.07430</link><description>&lt;p&gt;
FOLE ERA&#65306;&#22522;&#30784;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
The ERA of FOLE: Foundation. (arXiv:1512.07430v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1512.07430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#26412;&#20307;&#22312;FOLE&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#25552;&#20379;&#20102;ERA&#25968;&#25454;&#27169;&#22411;&#30340;&#20005;&#26684;&#25968;&#23398;&#34920;&#31034;&#65292;&#20316;&#20026;&#26412;&#20307;&#35770;&#30340;&#22522;&#30784;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#26412;&#20307;&#22312;FOLE&#65288;Kent 2013&#65289;&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#30340;&#34920;&#31034;&#12290;&#26412;&#20307;&#23450;&#20041;&#20102;&#29992;&#20110;&#20026;&#35805;&#35821;&#31038;&#21306;&#24314;&#27169;&#30693;&#35782;&#36164;&#28304;&#30340;&#21407;&#35821;&#65288;Gruber 2009&#65289;&#12290;&#36825;&#20123;&#21407;&#35821;&#21253;&#25324;&#31867;&#12289;&#20851;&#31995;&#21644;&#23646;&#24615;&#65292;&#30001;&#23454;&#20307;-&#20851;&#31995;-&#23646;&#24615;&#65288;ERA&#65289;&#25968;&#25454;&#27169;&#22411;&#65288;Chen 1976&#65289;&#34920;&#31034;&#12290;&#26412;&#25991;&#26159;&#19977;&#31687;&#35770;&#25991;&#20013;&#30340;&#31532;&#19968;&#31687;&#65292;&#23427;&#22312;FOLE&#30340;&#31532;&#19968;&#38454;&#36923;&#36753;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;ERA&#25968;&#25454;&#27169;&#22411;&#30340;&#20005;&#26684;&#25968;&#23398;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#26412;&#20307;&#35770;&#30340;&#22522;&#30784;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the representation of ontologies in the first-order logical environment FOLE (Kent 2013). An ontology defines the primitives with which to model the knowledge resources for a community of discourse (Gruber 2009). These primitives, consisting of classes, relationships and properties, are represented by the entity-relationship-attribute ERA data model (Chen 1976). An ontology uses formal axioms to constrain the interpretation of these primitives. In short, an ontology specifies a logical theory. This paper is the first in a series of three papers that provide a rigorous mathematical representation for the ERA data model in particular, and ontologies in general, within the first-order logical environment FOLE. The first two papers show how FOLE represents the formalism and semantics of (many-sorted) first-order logic in a classification form corresponding to ideas discussed in the Information Flow Framework (IFF). In particular, this first paper provides a foundation 
&lt;/p&gt;</description></item></channel></rss>