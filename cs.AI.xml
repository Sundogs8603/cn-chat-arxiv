<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.06101</link><description>&lt;p&gt;
Prodigy: &#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#38646;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#20013;&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20004;&#31181;&#25216;&#26415;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#35777;&#26126;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20197;&#20415;&#26368;&#20248;&#35774;&#32622;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#22522;&#20110;&#23398;&#20064;&#29575;&#33258;&#30001;&#30340;D-Adaptation&#26041;&#27861;&#30340;&#20462;&#25913;&#65292;&#24182;&#36890;&#36807;$O(\sqrt{\log(D/d_0)})$&#30340;&#22240;&#23376;&#25552;&#39640;&#20102;D-Adaptation&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$d_0$&#26159;$D$&#30340;&#21021;&#22987;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;12&#20010;&#24120;&#35265;&#30340;&#36923;&#36753;&#22238;&#24402;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;VGG11&#21644;ResNet-50&#12289;&#22312;Imagenet&#19978;&#35757;&#32451;&#30340;ViT&#12289;&#22312;IWSLT14&#19978;&#35757;&#32451;&#30340;LSTM&#12289;&#22312;Criteo&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DLRM&#12289;&#22312;Knee MRI&#25968;&#25454;&#38598;&#19978;&#30340;VarNet&#65292;&#20197;&#21450;&#22312;BookWiki&#19978;&#35757;&#32451;&#30340;RoBERTa&#21644;GPT transformer&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;D-Adaptation&#65292;&#24182;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.06094</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;&#39537;&#21160;&#30340;&#22270;&#20687;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding. (arXiv:2306.06094v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#24320;&#21457;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#24615;&#26041;&#27861;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;XML&#30340;SVG&#34920;&#36848;&#30340;&#25991;&#26412;&#25551;&#36848;&#32780;&#19981;&#26159;&#20809;&#26629;&#22270;&#20687;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;LLMs&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#27861;&#22312;&#21028;&#21035;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;(i)&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#65292;(ii)&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#23454;&#29616;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20197;&#21450;(iii)&#22270;&#20687;&#26434;&#20081;&#31243;&#24230;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) have made significant advancements in natural language understanding and generation. However, their potential in computer vision remains largely unexplored. In this paper, we introduce a new, exploratory approach that enables LLMs to process images using the Scalable Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions of SVG representations instead of raster images, we aim to bridge the gap between the visual and textual modalities, allowing LLMs to directly understand and manipulate images without the need for parameterized visual components. Our method facilitates simple image classification, generation, and in-context learning using only LLM capabilities. We demonstrate the promise of our approach across discriminative and generative tasks, highlighting its (i) robustness against distribution shift, (ii) substantial improvements achieved by tapping into the in-context learning abilities of LLMs, and (iii) image unders
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#19981;&#36827;&#34892;&#27450;&#39575;&#30340;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#26234;&#33021;&#32929;&#31080;&#20132;&#26131;&#20195;&#29702;&#30340;&#27450;&#35784;&#34892;&#20026;&#35782;&#21035;&#21644;&#36991;&#20813;&#12290;</title><link>http://arxiv.org/abs/2306.06087</link><description>&lt;p&gt;
&#23398;&#20250;&#19981;&#36827;&#34892;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Learning Not to Spoof. (arXiv:2306.06087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#19981;&#36827;&#34892;&#27450;&#39575;&#30340;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#26234;&#33021;&#32929;&#31080;&#20132;&#26131;&#20195;&#29702;&#30340;&#27450;&#35784;&#34892;&#20026;&#35782;&#21035;&#21644;&#36991;&#20813;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26234;&#33021;&#20132;&#26131;&#20195;&#29702;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30830;&#20445;RL&#20195;&#29702;&#36981;&#23432;&#27861;&#24459;&#12289;&#27861;&#35268;&#21644;&#20154;&#31867;&#34892;&#20026;&#26399;&#26395;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20854;&#20013;&#26234;&#33021;&#32929;&#31080;&#20132;&#26131;&#20195;&#29702;&#26368;&#22823;&#21270;&#21033;&#28070;&#65292;&#20294;&#21487;&#33021;&#26080;&#24847;&#20013;&#23398;&#20250;&#27450;&#39575;&#20854;&#21442;&#19982;&#30340;&#24066;&#22330;&#12290;&#26412;&#25991;&#39318;&#20808;&#24341;&#20837;&#25163;&#21160;&#32534;&#30721;&#30340;&#27450;&#35784;&#20195;&#29702;&#21040;&#19968;&#20010;&#22810;&#20195;&#29702;&#24066;&#22330;&#27169;&#25311;&#20013;&#65292;&#24182;&#23398;&#20064;&#35782;&#21035;&#27450;&#35784;&#27963;&#21160;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#23558;&#25163;&#21160;&#32534;&#30721;&#30340;&#27450;&#39575;&#20132;&#26131;&#21592;&#26367;&#25442;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#26368;&#22823;&#21270;&#21033;&#28070;&#30340;RL&#20195;&#29702;&#65292;&#35266;&#23519;&#23427;&#26159;&#21542;&#20250;&#29420;&#31435;&#22320;&#23398;&#20064;&#27450;&#35784;&#34892;&#20026;&#65292;&#24182;&#23581;&#35797;&#36991;&#20813;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
As intelligent trading agents based on reinforcement learning (RL) gain prevalence, it becomes more important to ensure that RL agents obey laws, regulations, and human behavioral expectations. There is substantial literature concerning the aversion of obvious catastrophes like crashing a helicopter or bankrupting a trading account, but little around the avoidance of subtle non-normative behavior for which there are examples, but no programmable definition. Such behavior may violate legal or regulatory, rather than physical or monetary, constraints.  In this article, I consider a series of experiments in which an intelligent stock trading agent maximizes profit but may also inadvertently learn to spoof the market in which it participates. I first inject a hand-coded spoofing agent to a multi-agent market simulation and learn to recognize spoofing activity sequences. Then I replace the hand-coded spoofing trader with a simple profit-maximizing RL agent and observe that it independently 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22916;&#24819;&#30151;&#65292;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06085</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#25429;&#25417;LLM&#22916;&#24819;&#30151;
&lt;/p&gt;
&lt;p&gt;
Trapping LLM Hallucinations Using Tagged Context Prompts. (arXiv:2306.06085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22916;&#24819;&#30151;&#65292;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20363;&#22914;ChatGPT&#65292;&#24050;&#32463;&#23548;&#33268;&#39640;&#24230;&#22797;&#26434;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#21463;&#21040;&#8220;&#22916;&#24819;&#30151;&#8221;&#30340;&#22256;&#25200;&#65292;&#21363;&#27169;&#22411;&#29983;&#25104;&#34394;&#20551;&#25110;&#25423;&#36896;&#30340;&#20449;&#24687;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#38750;&#24120;&#20851;&#38190;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#20010;&#39046;&#22495;&#37319;&#29992;AI&#39537;&#21160;&#30340;&#24179;&#21488;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#26631;&#35760;LLM&#22312;&#20854;&#39046;&#22495;&#30693;&#35782;&#33539;&#22260;&#20043;&#22806;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#29992;&#25143;&#33719;&#24471;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32467;&#21512;&#23884;&#20837;&#30340;&#26631;&#35760;&#21644;&#19978;&#19979;&#25991;&#26469;&#23545;&#25239;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22916;&#24819;&#30151;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#23545;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#30340;URL&#20316;&#20026;&#26131;&#20110;&#27979;&#35797;&#30340;&#25423;&#36896;&#25968;&#25454;&#25351;&#26631;&#65292;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#25552;&#31034;-&#21709;&#24212;&#23545;&#30340;&#24773;&#20917;&#19979;&#22522;&#32447;&#20381;&#36182;&#24615;&#22916;&#24819;&#39057;&#29575;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#20026;&#27979;&#35797;&#30340;&#29983;&#25104;&#24341;&#25806;&#25552;&#20379;&#38382;&#39064;&#25552;&#31034;&#26102;&#25552;&#20379;&#19978;&#19979;&#25991;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24635;&#20307;&#22916;&#24819;&#30151;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#19978;&#19979;&#25991;&#25552;&#31034;&#20013;&#25918;&#32622;&#26631;&#35760;&#22914;&#20309;&#24433;&#21709;&#26816;&#27979;&#22916;&#24819;&#30151;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;LLM&#20013;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#25552;&#39640;&#20102;&#26816;&#27979;&#22916;&#24819;&#30151;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs), such as ChatGPT, have led to highly sophisticated conversation agents. However, these models suffer from "hallucinations," where the model generates false or fabricated information. Addressing this challenge is crucial, particularly with AI-driven platforms being adopted across various sectors. In this paper, we propose a novel method to recognize and flag instances when LLMs perform outside their domain knowledge, and ensuring users receive accurate information.  We find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. To do this, we baseline hallucination frequency in no-context prompt-response pairs using generated URLs as easily-tested indicators of fabricated data. We observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. Lastly, we evaluated how placing tags within cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.06081</link><description>&lt;p&gt;
CARSO: &#23545;&#25239;&#24615;&#21512;&#25104;&#35266;&#27979;&#30340;&#21453;&#23545;&#25239;&#24615;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#32447;&#32034;&#12290;&#35813;&#26041;&#27861;&#19982;&#23545;&#25239;&#35757;&#32451;&#20855;&#26377;&#21327;&#21516;&#20114;&#34917;&#24615;&#65292;&#24182;&#20381;&#36182;&#20110;&#34987;&#25915;&#20987;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#35813;&#26041;&#27861;&#37319;&#26679;&#36755;&#20837;&#30340;&#37325;&#26500;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#22312;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CARSO&#33021;&#22815;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#8212;&#8212;&#21516;&#26102;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#28165;&#27905;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#20307;&#31995;&#32467;&#26500;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06077</link><description>&lt;p&gt;
&#35270;&#35273;&#35789;&#27719;&#25551;&#36848;&#25552;&#21319;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Visually-Grounded Descriptions Improve Zero-Shot Image Classification. (arXiv:2306.06077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#22914;CLIP&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#20219;&#21153;&#65288;&#20363;&#22914;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;ZSIC&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20855;&#20307;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#31867;&#21035;&#25551;&#36848;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#31890;&#24230;&#21644;&#26631;&#31614;&#27495;&#20041;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;V-GLOSS&#65306;Visual Glosses&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#26469;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;ZSIC&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;ImageNet&#21644;STL-10&#65289;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#26469;&#23637;&#31034;V-GLOSS&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;V-GLOSS&#29983;&#25104;&#30340;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20854;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-vision models like CLIP have made significant progress in zero-shot vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive class descriptions remains a major challenge. Existing approaches suffer from granularity and label ambiguity issues. To tackle these challenges, we propose V-GLOSS: Visual Glosses, a novel method leveraging modern language models and semantic knowledge bases to produce visually-grounded class descriptions. We demonstrate V-GLOSS's effectiveness by achieving state-of-the-art results on benchmark ZSIC datasets including ImageNet and STL-10. In addition, we introduce a silver dataset with class descriptions generated by V-GLOSS, and show its usefulness for vision tasks. We make available our code and dataset.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#20809;&#23398;&#21644;SAR&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#36827;&#34892;&#27946;&#27700;&#22270;&#21046;&#20316;&#65292;&#24182;&#22312;&#24052;&#22522;&#26031;&#22374;2022&#24180;&#30340;&#27946;&#27700;&#20013;&#24212;&#29992;&#65292;&#21487;&#20934;&#30830;&#35782;&#21035;&#21463;&#28798;&#20154;&#21475;&#21644;&#22522;&#30784;&#35774;&#26045;&#65292;&#20197;&#24110;&#21161;&#21046;&#23450;&#29702;&#26234;&#30340;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.06074</link><description>&lt;p&gt;
&#20351;&#29992;Sentinel-1, Sentinel-2&#21644;Landsat-9&#22270;&#20687;&#34701;&#21512;&#25913;&#36827;&#27946;&#27700;&#22270;&#21046;&#20316;&#20197;&#35782;&#21035;&#21463;&#28798;&#20154;&#21475;&#21644;&#22522;&#30784;&#35774;&#26045;
&lt;/p&gt;
&lt;p&gt;
Improved flood mapping for efficient policy design by fusion of Sentinel-1, Sentinel-2, and Landsat-9 imagery to identify population and infrastructure exposed to floods. (arXiv:2306.06074v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06074
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#20809;&#23398;&#21644;SAR&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#36827;&#34892;&#27946;&#27700;&#22270;&#21046;&#20316;&#65292;&#24182;&#22312;&#24052;&#22522;&#26031;&#22374;2022&#24180;&#30340;&#27946;&#27700;&#20013;&#24212;&#29992;&#65292;&#21487;&#20934;&#30830;&#35782;&#21035;&#21463;&#28798;&#20154;&#21475;&#21644;&#22522;&#30784;&#35774;&#26045;&#65292;&#20197;&#24110;&#21161;&#21046;&#23450;&#29702;&#26234;&#30340;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#19988;&#24265;&#20215;&#30340;&#27946;&#27700;&#27700;&#20301;&#20272;&#35745;&#24037;&#20855;&#26377;&#21161;&#20110;&#26377;&#25928;&#30340;&#28798;&#23475;&#31649;&#29702;&#12290;&#20809;&#23398;&#21644;SAR&#22270;&#20687;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#25193;&#23637;&#21487;&#29992;&#24615;&#21644;&#22686;&#24378;&#27946;&#27700;&#22270;&#21046;&#20316;&#21487;&#38752;&#24615;&#30340;&#25163;&#27573;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36825;&#20004;&#31181;&#22270;&#20687;&#21512;&#24182;&#21040;&#19968;&#20010;&#20849;&#21516;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#24052;&#22522;&#26031;&#22374;2022&#24180;&#27946;&#27700;&#20013;&#35782;&#21035;&#21463;&#28798;&#20154;&#21475;&#21644;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20809;&#23398;&#21644;SAR&#25968;&#25454;&#30340;&#21512;&#24182;&#20026;&#25105;&#20204;&#22312;&#20113;&#23618;&#23494;&#38598;&#21306;&#22495;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#35266;&#27979;&#65292;&#28982;&#21518;&#29992;&#20110;&#33719;&#24471;&#26356;&#22810;&#30340;&#27946;&#27700;&#22270;&#21046;&#20316;&#24212;&#29992;&#35265;&#35299;&#12290;&#20351;&#29992;WorldPop&#21644;OSM&#30340;&#24320;&#25918;&#28304;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#29992;&#20110;&#20154;&#21475;&#21644;&#36947;&#36335;&#65292;&#20351;&#35813;&#32451;&#20064;&#20855;&#26377;&#20840;&#29699;&#22797;&#21046;&#30340;&#33021;&#21147;&#12290;&#27946;&#27700;&#22270;&#19982;&#20154;&#21475;&#21644;&#22522;&#30784;&#35774;&#26045;&#30340;&#31354;&#38388;&#25968;&#25454;&#30340;&#25972;&#21512;&#26377;&#21161;&#20110;&#21046;&#23450;&#26126;&#26234;&#30340;&#25919;&#31574;&#12290;&#25105;&#20204;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#24052;&#22522;&#26031;&#22374;&#20449;&#24503;&#30465;&#20116;&#20010;&#21463;&#27946;&#27700;&#24433;&#21709;&#26368;&#20005;&#37325;&#30340;&#22320;&#21306;&#20013;&#65292;&#21463;&#24433;&#21709;&#30340;&#20154;&#21475;&#21644;&#22522;&#30784;&#35774;&#26045;&#28085;&#30422;&#29575;&#30340;&#21069;&#20116;&#21517;&#21306;&#22495;&#26377;&#24456;&#22823;&#30340;&#39034;&#24207;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A reliable yet inexpensive tool for the estimation of flood water spread is conducive for efficient disaster management. The application of optical and SAR imagery in tandem provides a means of extended availability and enhanced reliability of flood mapping. We propose a methodology to merge these two types of imagery into a common data space and demonstrate its use in the identification of affected populations and infrastructure for the 2022 floods in Pakistan. The merging of optical and SAR data provides us with improved observations in cloud-prone regions; that is then used to gain additional insights into flood mapping applications. The use of open source datasets from WorldPop and OSM for population and roads respectively makes the exercise globally replicable. The integration of flood maps with spatial data on population and infrastructure facilitates informed policy design. We have shown that within the top five flood-affected districts in Sindh province, Pakistan, the affected 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#31574;&#30053;&#21644;&#33945;&#29305;&#21345;&#32599;&#35268;&#21010;&#30340;POTMMCP&#31639;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#31867;&#22411;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.06067</link><description>&lt;p&gt;
&#32467;&#21512;&#20803;&#31574;&#30053;&#21644;&#33945;&#29305;&#21345;&#32599;&#35268;&#21010;&#65292;&#23454;&#29616;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#31867;&#22411;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Combining a Meta-Policy and Monte-Carlo Planning for Scalable Type-Based Reasoning in Partially Observable Environments. (arXiv:2306.06067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#31574;&#30053;&#21644;&#33945;&#29305;&#21345;&#32599;&#35268;&#21010;&#30340;POTMMCP&#31639;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#31867;&#22411;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#35774;&#35745;&#33021;&#22815;&#19982;&#20854;&#20182;&#20195;&#29702;&#26377;&#25928;&#20132;&#20114;&#32780;&#26080;&#38656;&#20107;&#20808;&#21327;&#35843;&#30340;&#33258;&#20027;&#20195;&#29702;&#26159;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#22522;&#20110;&#31867;&#22411;&#30340;&#25512;&#29702;&#26041;&#27861;&#36890;&#36807;&#32500;&#25252;&#23545;&#20854;&#20182;&#20195;&#29702;&#30340;&#19968;&#32452;&#28508;&#22312;&#34892;&#20026;&#20449;&#24565;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#35748;&#20026;&#20854;&#20182;&#20195;&#29702;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#26159;&#23436;&#20840;&#21487;&#35266;&#27979;&#30340;&#65292;&#25110;&#32773;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#38382;&#39064;&#21644;&#26356;&#38271;&#30340;&#35745;&#21010;&#26102;&#32570;&#20047;&#39640;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#31867;&#22411;&#30340;&#20803;&#33945;&#29305;&#21345;&#32599;&#35268;&#21010;(POTMMCP)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31867;&#22411;&#25512;&#29702;&#22823;&#35268;&#27169;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#35268;&#21010;&#26041;&#27861;&#12290;POTMMCP&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#31574;&#30053;&#26469;&#25351;&#23548;&#25628;&#32034;&#21644;&#35780;&#20272;&#20449;&#24565;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#35268;&#21010;&#26102;&#38388;&#26356;&#26377;&#25928;&#22320;&#25628;&#32034;&#26356;&#38271;&#30340;&#26102;&#38388;&#38388;&#38548;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26497;&#38480;&#29366;&#24577;&#19979;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#33021;&#22815;&#26377;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of autonomous agents that can interact effectively with other agents without prior coordination is a core problem in multi-agent systems. Type-based reasoning methods achieve this by maintaining a belief over a set of potential behaviours for the other agents. However, current methods are limited in that they assume full observability of the state and actions of the other agent or do not scale efficiently to larger problems with longer planning horizons. Addressing these limitations, we propose Partially Observable Type-based Meta Monte-Carlo Planning (POTMMCP) - an online Monte-Carlo Tree Search based planning method for type-based reasoning in large partially observable environments. POTMMCP incorporates a novel meta-policy for guiding search and evaluating beliefs, allowing it to search more effectively to longer horizons using less planning time. We show that our method converges to the optimal solution in the limit and empirically demonstrate that it effectively adapts 
&lt;/p&gt;</description></item><item><title>SNeL&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#31070;&#32463;&#31526;&#21495;&#35821;&#35328;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#31526;&#21495;&#25512;&#29702;&#19982;&#20302;&#32423;&#31070;&#32463;&#22788;&#29702;&#23545;&#40784;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20998;&#27495;&#65292;&#21487;&#20197;&#24191;&#27867;&#30340;&#24212;&#29992;&#20110;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#29992;&#20110;&#23454;&#20307;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#22330;&#26223;&#29702;&#35299;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.06036</link><description>&lt;p&gt;
SNeL&#65306;&#29992;&#20110;&#22522;&#20110;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#22330;&#26223;&#29702;&#35299;&#30340;&#32467;&#26500;&#21270;&#31070;&#32463;&#31526;&#21495;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
SNeL: A Structured Neuro-Symbolic Language for Entity-Based Multimodal Scene Understanding. (arXiv:2306.06036v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06036
&lt;/p&gt;
&lt;p&gt;
SNeL&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#31070;&#32463;&#31526;&#21495;&#35821;&#35328;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#31526;&#21495;&#25512;&#29702;&#19982;&#20302;&#32423;&#31070;&#32463;&#22788;&#29702;&#23545;&#40784;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20998;&#27495;&#65292;&#21487;&#20197;&#24191;&#27867;&#30340;&#24212;&#29992;&#20110;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#29992;&#20110;&#23454;&#20307;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#22330;&#26223;&#29702;&#35299;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#30340;&#20170;&#22825;&#65292;&#22810;&#27169;&#24577;&#21644;&#31070;&#32463;&#31526;&#21495;&#23398;&#33539;&#24335;&#22788;&#20110;&#21069;&#27839;&#22320;&#20301;&#65292;&#29305;&#21035;&#24378;&#35843;&#22312;&#19981;&#21516;&#27169;&#24577;&#19979;&#35782;&#21035;&#21644;&#20132;&#20114;&#23454;&#20307;&#21450;&#20854;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32972;&#26223;&#19979;&#22797;&#26434;&#26597;&#35810;&#21644;&#20132;&#20114;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SNeL&#65288;&#32467;&#26500;&#21270;&#31070;&#32463;&#31526;&#21495;&#35821;&#35328;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#26597;&#35810;&#35821;&#35328;&#65292;&#26088;&#22312;&#20419;&#36827;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24494;&#22937;&#20132;&#20114;&#12290;SNeL&#30340;&#34920;&#36798;&#25509;&#21475;&#21487;&#20197;&#26500;&#24314;&#22797;&#26434;&#26597;&#35810;&#65292;&#25903;&#25345;&#36923;&#36753;&#21644;&#31639;&#26415;&#36816;&#31639;&#31526;&#12289;&#27604;&#36739;&#22120;&#12289;&#23884;&#22871;&#31561;&#12290;&#36825;&#20801;&#35768;&#29992;&#25143;&#23450;&#20301;&#29305;&#23450;&#23454;&#20307;&#12289;&#25351;&#23450;&#20854;&#23646;&#24615;&#24182;&#38480;&#21046;&#32467;&#26524;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20174;&#22330;&#26223;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#36890;&#36807;&#23558;&#39640;&#32423;&#31526;&#21495;&#25512;&#29702;&#19982;&#20302;&#32423;&#31070;&#32463;&#22788;&#29702;&#23545;&#40784;&#65292;SNeL&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20998;&#27495;&#12290;&#35813;&#35821;&#35328;&#30340;&#22810;&#21151;&#33021;&#24615;&#25193;&#23637;&#21040;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#65292;&#20351;&#20854;&#25104;&#20026;&#22522;&#20110;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#22330;&#26223;&#29702;&#35299;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving landscape of artificial intelligence, multimodal and Neuro-Symbolic paradigms stand at the forefront, with a particular emphasis on the identification and interaction with entities and their relations across diverse modalities. Addressing the need for complex querying and interaction in this context, we introduce SNeL (Structured Neuro-symbolic Language), a versatile query language designed to facilitate nuanced interactions with neural networks processing multimodal data. SNeL's expressive interface enables the construction of intricate queries, supporting logical and arithmetic operators, comparators, nesting, and more. This allows users to target specific entities, specify their properties, and limit results, thereby efficiently extracting information from a scene. By aligning high-level symbolic reasoning with low-level neural processing, SNeL effectively bridges the Neuro-Symbolic divide. The language's versatility extends to a variety of data types, including imag
&lt;/p&gt;</description></item><item><title>HiTZ@Antidote&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#35770;&#35777;&#29702;&#35770;&#65292;&#25552;&#20379;&#25968;&#23383;&#21307;&#30103;&#39046;&#22495;&#39640;&#36136;&#37327;&#30340;&#21487;&#35299;&#37322;AI&#39044;&#27979;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.06029</link><description>&lt;p&gt;
HiTZ@Antidote: &#38754;&#21521;&#25968;&#23383;&#21307;&#30103;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine. (arXiv:2306.06029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06029
&lt;/p&gt;
&lt;p&gt;
HiTZ@Antidote&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#35770;&#35777;&#29702;&#35770;&#65292;&#25552;&#20379;&#25968;&#23383;&#21307;&#30103;&#39046;&#22495;&#39640;&#36136;&#37327;&#30340;&#21487;&#35299;&#37322;AI&#39044;&#27979;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;AI&#39044;&#27979;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#20219;&#21153;&#12290;&#35201;&#20351;&#20854;&#27491;&#24120;&#24037;&#20316;&#65292;&#38656;&#35201;&#36873;&#25321;&#36866;&#24403;&#30340;&#35299;&#37322;&#27867;&#21270;/&#32454;&#21270;&#27700;&#24179;&#65307;&#32771;&#34385;&#35299;&#37322;&#21463;&#30410;&#32773;&#23545;&#25152;&#32771;&#34385;&#30340;AI&#20219;&#21153;&#30340;&#29087;&#24713;&#31243;&#24230;&#30340;&#20551;&#35774;&#65307;&#28041;&#21450;&#21040;&#23545;&#20419;&#25104;&#20915;&#31574;&#30340;&#20855;&#20307;&#20803;&#32032;&#30340;&#24341;&#29992;&#65307;&#21033;&#29992;&#21487;&#33021;&#19981;&#26159;&#39044;&#27979;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#30340;&#20854;&#20182;&#30693;&#35782;&#65288;&#20363;&#22914;&#19987;&#23478;&#35777;&#25454;&#65289;&#65307;&#24182;&#20197;&#28165;&#26224;&#26131;&#25026;&#12289;&#21487;&#33021;&#20855;&#35828;&#26381;&#21147;&#30340;&#26041;&#24335;&#34920;&#36848;&#35299;&#37322;&#12290;&#37492;&#20110;&#36825;&#20123;&#32771;&#34385;&#22240;&#32032;&#65292;ANTIDOTE&#22312;&#21487;&#35299;&#37322;AI&#26041;&#38754;&#22521;&#32946;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#35270;&#35282;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#36807;&#31243;&#30340;&#20302;&#32423;&#29305;&#24449;&#19982;&#20154;&#31867;&#35770;&#35777;&#33021;&#21147;&#30340;&#26356;&#39640;&#32423;&#21035;&#26041;&#26696;&#30456;&#32467;&#21512;&#12290;ANTIDOTE&#23558;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#35770;&#35777;&#29702;&#35770;&#30340;&#36328;&#23398;&#31185;&#31454;&#20105;&#20248;&#21183;&#65292;&#35299;&#20915;&#25968;&#23383;&#21307;&#23398;&#39046;&#22495;&#25552;&#20379;&#39640;&#36136;&#37327;AI&#39044;&#27979;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing high quality explanations for AI predictions based on machine learning is a challenging and complex task. To work well it requires, among other factors: selecting a proper level of generality/specificity of the explanation; considering assumptions about the familiarity of the explanation beneficiary with the AI task under consideration; referring to specific elements that have contributed to the decision; making use of additional knowledge (e.g. expert evidence) which might not be part of the prediction process; and providing evidence supporting negative hypothesis. Finally, the system needs to formulate the explanation in a clearly interpretable, and possibly convincing, way. Given these considerations, ANTIDOTE fosters an integrated vision of explainable AI, where low-level characteristics of the deep learning process are combined with higher level schemes proper of the human argumentation capacity. ANTIDOTE will exploit cross-disciplinary competences in deep learning and a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#35745;&#31639;&#30340;Metaverse&#21160;&#24577;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#21160;&#24577;&#35843;&#25972;&#21368;&#36733;&#31574;&#30053;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#23376;&#20219;&#21153;&#30340;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20102;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#20998;&#21035;&#38024;&#23545;&#29992;&#25143;&#31471;&#30340;&#20219;&#21153;&#20999;&#20998;&#38382;&#39064;&#21644;COIN&#31471;&#30340;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#12290;&#36825;&#38656;&#35201;&#20351;&#29992;&#24207;&#25968;&#28508;&#22312;&#21338;&#24328;&#65288;OPG&#65289;&#21644;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#25552;&#20986;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21452;&#37325;&#28145;&#24230;Q&#32593;&#32476;&#65288;DDQN&#65289;&#26469;&#35299;&#20915;&#26368;&#20248;&#21368;&#36733;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06022</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#35745;&#31639;&#30340;Metaverse&#21160;&#24577;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Dynamic Partial Computation Offloading for the Metaverse in In-Network Computing. (arXiv:2306.06022v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#35745;&#31639;&#30340;Metaverse&#21160;&#24577;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#21160;&#24577;&#35843;&#25972;&#21368;&#36733;&#31574;&#30053;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#23376;&#20219;&#21153;&#30340;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20102;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#20998;&#21035;&#38024;&#23545;&#29992;&#25143;&#31471;&#30340;&#20219;&#21153;&#20999;&#20998;&#38382;&#39064;&#21644;COIN&#31471;&#30340;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#12290;&#36825;&#38656;&#35201;&#20351;&#29992;&#24207;&#25968;&#28508;&#22312;&#21338;&#24328;&#65288;OPG&#65289;&#21644;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#25552;&#20986;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21452;&#37325;&#28145;&#24230;Q&#32593;&#32476;&#65288;DDQN&#65289;&#26469;&#35299;&#20915;&#26368;&#20248;&#21368;&#36733;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32593;&#32476;&#35745;&#31639;&#65288;COIN&#65289;&#33539;&#24335;&#26159;&#19968;&#20010;&#21033;&#29992;&#26410;&#20351;&#29992;&#30340;&#32593;&#32476;&#36164;&#28304;&#26469;&#25191;&#34892;&#26576;&#20123;&#20219;&#21153;&#20197;&#28385;&#36275;&#35745;&#31639;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#65288;&#22914;Metaverse&#65289;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;COIN&#29615;&#22659;&#20013;&#38024;&#23545;Metaverse&#30340;&#22810;&#20010;&#23376;&#20219;&#21153;&#30340;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#38382;&#39064;&#65292;&#20197;&#22312;&#21160;&#24577;&#35843;&#25972;&#21368;&#36733;&#31574;&#30053;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#33021;&#32791;&#21644;&#24310;&#36831;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#26159;NP&#38590;&#39064;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65306;&#29992;&#25143;&#31471;&#30340;&#20219;&#21153;&#20999;&#20998;&#38382;&#39064;&#65288;TSP&#65289;&#21644;COIN&#31471;&#30340;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65288;TOP&#65289;&#12290;&#25105;&#20204;&#23558;TSP&#24314;&#27169;&#20026;&#24207;&#25968;&#28508;&#22312;&#21338;&#24328;&#65288;OPG&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#31639;&#27861;&#26469;&#33719;&#21462;&#20854;&#32435;&#20160;&#24179;&#34913;&#65288;NE&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;TOP&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#25552;&#20986;&#20102;&#21452;&#37325;&#28145;&#24230;Q&#32593;&#32476;&#65288;DDQN&#65289;&#26469;&#35299;&#20915;&#26368;&#20248;&#21368;&#36733;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;DDQN&#31639;&#27861;&#19981;&#21516;&#65292;&#26234;&#33021;&#20195;&#29702;&#22312;&#27492;&#19981;&#20197;&#38543;&#26426;&#26041;&#24335;&#25277;&#26679;&#21368;&#36733;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
The In-Network Computing (COIN) paradigm is a promising solution that leverages unused network resources to perform some tasks to meet up with computation-demanding applications, such as metaverse. In this vein, we consider the metaverse partial computation offloading problem for multiple subtasks in a COIN environment to minimise energy consumption and delay while dynamically adjusting the offloading policy based on the changing computation resources status. We prove that the problem is NP and thus transformed it into two subproblems: task splitting problem (TSP) on the user side and task offloading problem (TOP) on the COIN side. We modelled the TSP as an ordinal potential game (OPG) and proposed a decentralised algorithm to obtain its Nash Equilibrium (NE). Then, we model the TOP as Markov Decision Process (MDP) proposed double deep Q-network (DDQN) to solve for the optimal offloading policy. Unlike the conventional DDQN algorithm, where intelligent agents sample offloading decision
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30697;&#38453;&#21152;&#26435;&#32447;&#24615;&#20272;&#35745;&#22120;&#32467;&#21512;&#35266;&#27979;&#25968;&#25454;&#21644;&#24178;&#39044;&#25968;&#25454;&#26469;&#25512;&#26029;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#26080;&#38480;&#26679;&#26412;&#26497;&#38480;&#19979;&#26159;&#28176;&#36817;&#26080;&#20559;&#30340;&#12290;&#22312;&#28151;&#28102;&#20005;&#37325;&#19988;&#35266;&#27979;&#25968;&#25454;&#21644;&#24178;&#39044;&#25968;&#25454;&#30340;&#27604;&#20363;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06002</link><description>&lt;p&gt;
&#36890;&#36807;&#30697;&#38453;&#21152;&#26435;&#32447;&#24615;&#20272;&#35745;&#22120;&#20174;&#35266;&#27979;&#25968;&#25454;&#21644;&#24178;&#39044;&#25968;&#25454;&#25512;&#26029;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Causal Effect Estimation from Observational and Interventional Data Through Matrix Weighted Linear Estimators. (arXiv:2306.06002v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30697;&#38453;&#21152;&#26435;&#32447;&#24615;&#20272;&#35745;&#22120;&#32467;&#21512;&#35266;&#27979;&#25968;&#25454;&#21644;&#24178;&#39044;&#25968;&#25454;&#26469;&#25512;&#26029;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#26080;&#38480;&#26679;&#26412;&#26497;&#38480;&#19979;&#26159;&#28176;&#36817;&#26080;&#20559;&#30340;&#12290;&#22312;&#28151;&#28102;&#20005;&#37325;&#19988;&#35266;&#27979;&#25968;&#25454;&#21644;&#24178;&#39044;&#25968;&#25454;&#30340;&#27604;&#20363;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20803;&#27835;&#30103;&#30340;&#28151;&#21512;&#35266;&#27979;&#25968;&#25454;&#21644;&#24178;&#39044;&#25968;&#25454;&#22312;&#28151;&#28102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#32467;&#21512;&#20174;&#35266;&#27979;&#21644;&#24178;&#39044;&#29366;&#24577;&#19979;&#24471;&#20986;&#30340;&#20272;&#35745;&#22120;&#65292;&#26399;&#26395;&#22343;&#26041;&#35823;&#24046;&#30340;&#32479;&#35745;&#25928;&#29575;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#30697;&#38453;&#21152;&#26435;&#32447;&#24615;&#20272;&#35745;&#22120;&#25512;&#23548;&#20102;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26080;&#38480;&#26679;&#26412;&#26497;&#38480;&#19979;&#26159;&#28176;&#36817;&#26080;&#20559;&#30340;&#12290;&#30456;&#27604;&#20110;&#20351;&#29992;&#24178;&#39044;&#25968;&#25454;&#21644;&#35266;&#27979;&#25968;&#25454;&#24182;&#38598;&#20316;&#20026;&#27719;&#24635;&#20272;&#35745;&#22120;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#30340;&#25913;&#36827;&#65292;&#22240;&#20026;&#21482;&#26377;&#24403;&#35266;&#27979;&#24178;&#39044;&#25968;&#25454;&#27604;&#30340;&#27604;&#20363;&#36235;&#36817;&#20110;&#38646;&#26102;&#65292;&#20854;&#20559;&#24046;&#25165;&#20250;&#28040;&#22833;&#12290;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#22312;&#28151;&#28102;&#20005;&#37325;&#19988;&#35266;&#27979;&#25968;&#25454;&#21644;&#24178;&#39044;&#25968;&#25454;&#30340;&#27604;&#20363;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#20248;&#20110;Stein&#22411;&#20272;&#35745;&#22120;&#21644;&#20854;&#20182;&#22522;&#20934;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study causal effect estimation from a mixture of observational and interventional data in a confounded linear regression model with multivariate treatments. We show that the statistical efficiency in terms of expected squared error can be improved by combining estimators arising from both the observational and interventional setting. To this end, we derive methods based on matrix weighted linear estimators and prove that our methods are asymptotically unbiased in the infinite sample limit. This is an important improvement compared to the pooled estimator using the union of interventional and observational data, for which the bias only vanishes if the ratio of observational to interventional data tends to zero. Studies on synthetic data confirm our theoretical findings. In settings where confounding is substantial and the ratio of observational to interventional data is large, our estimators outperform a Stein-type estimator and various other baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;S$^{3}$&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#36755;&#20986;&#24207;&#21015;&#38271;&#24230;&#24182;&#35843;&#24230;&#29983;&#25104;&#26597;&#35810;&#65292;&#20197;&#25552;&#39640;&#35774;&#22791;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#21534;&#21520;&#37327;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;6.49&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.06000</link><description>&lt;p&gt;
S$^{3}$: &#25552;&#39640;&#29983;&#25104;&#25512;&#26029; GPU &#21033;&#29992;&#29575;&#30340;&#26041;&#27861;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;
&lt;/p&gt;
&lt;p&gt;
S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput. (arXiv:2306.06000v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;S$^{3}$&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#36755;&#20986;&#24207;&#21015;&#38271;&#24230;&#24182;&#35843;&#24230;&#29983;&#25104;&#26597;&#35810;&#65292;&#20197;&#25552;&#39640;&#35774;&#22791;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#21534;&#21520;&#37327;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;6.49&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#30340;&#20869;&#23384;&#12290;&#38500;&#20102;&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#22806;&#65292;&#20445;&#23384;&#24207;&#21015;&#20013;&#21069;&#38754;&#26631;&#35760;&#20449;&#24687;&#30340;&#38190;/&#20540;&#65288;KV&#65289;&#32531;&#23384;&#20063;&#21487;&#33021;&#27604;&#27169;&#22411;&#26412;&#36523;&#26356;&#22823;&#12290;&#22312;&#24403;&#21069;&#30340;LLM&#26381;&#21153;&#26694;&#26550;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#34987;&#21152;&#21095;&#65292;&#22240;&#20026;&#20854;&#23558;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#30340;&#20869;&#23384;&#20445;&#30041;&#32473;KV&#32531;&#23384;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#23436;&#25972;&#30340;&#24207;&#21015;&#65292;&#20294;&#36755;&#20986;&#24207;&#21015;&#38271;&#24230;&#26159;&#26410;&#30693;&#30340;&#12290;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#65292;&#23548;&#33268;GPU&#21033;&#29992;&#29575;&#38477;&#20302;&#65292;&#21534;&#21520;&#37327;&#20063;&#38477;&#20302;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35774;&#35745;&#19968;&#20010;&#20855;&#26377;&#20808;&#39564;&#36755;&#20986;&#24207;&#21015;&#38271;&#24230;&#30340;&#31995;&#32479;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; S$^{3}$&#65292;&#23427;&#39044;&#27979;&#36755;&#20986;&#24207;&#21015;&#38271;&#24230;&#65292;&#22522;&#20110;&#39044;&#27979;&#35843;&#24230;&#29983;&#25104;&#26597;&#35810;&#20197;&#25552;&#39640;&#35774;&#22791;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#21534;&#21520;&#37327;&#65292;&#24182;&#22788;&#29702;&#39044;&#27979;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#21534;&#21520;&#37327;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;6.49&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself. This problem is exacerbated in one of the current LLM serving frameworks which reserves the maximum sequence length of memory for the KV cache to guarantee generating a complete sequence as they do not know the output sequence length. This restricts us to use a smaller batch size leading to lower GPU utilization and above all, lower throughput. We argue that designing a system with a priori knowledge of the output sequence can mitigate this problem. To this end, we propose S$^{3}$, which predicts the output sequence length, schedules generation queries based on the prediction to increase device resource utilization and throughput, and handle mispredictions. Our proposed method achieves 6.49$\times$ throughput over those sy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#22791;&#30340;&#26597;&#35810;&#37325;&#20889;&#31639;&#23376;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#20123;&#36830;&#25509;&#26597;&#35810;&#26080;&#27861;&#36827;&#34892;UCQ&#37325;&#20889;&#12290;&#23545;&#20110;&#26144;&#23556;&#38382;&#39064;&#65292;&#21017;&#21457;&#29616;&#20854;&#21028;&#23450;&#26159;&#21542;&#38656;&#35201;&#32463;&#36807;UC&#65279;Q&#37325;&#20889;&#36890;&#24120;&#26159;&#20010;coNP&#38590;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05973</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#31163;&#23384;&#22312;&#35268;&#21017;&#21644;&#26144;&#23556;&#30340;&#26597;&#35810;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Query Rewriting with Disjunctive Existential Rules and Mappings. (arXiv:2306.05973v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#22791;&#30340;&#26597;&#35810;&#37325;&#20889;&#31639;&#23376;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#20123;&#36830;&#25509;&#26597;&#35810;&#26080;&#27861;&#36827;&#34892;UCQ&#37325;&#20889;&#12290;&#23545;&#20110;&#26144;&#23556;&#38382;&#39064;&#65292;&#21017;&#21457;&#29616;&#20854;&#21028;&#23450;&#26159;&#21542;&#38656;&#35201;&#32463;&#36807;UC&#65279;Q&#37325;&#20889;&#36890;&#24120;&#26159;&#20010;coNP&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#24102;&#26377;&#20998;&#31163;&#23384;&#22312;&#35268;&#21017;&#21644;&#26144;&#23556;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#32852;&#21512;&#30340;&#36830;&#25509;&#26597;&#35810;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#22791;&#30340;&#26597;&#35810;&#37325;&#20889;&#31639;&#23376;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#19968;&#20010;&#36861;&#36394;&#27493;&#39588;&#21644;&#37325;&#20889;&#27493;&#39588;&#20043;&#38388;&#30340;&#32039;&#23494;&#32852;&#31995;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#34920;&#26126;&#23545;&#20110;&#20219;&#20309;&#8220;&#30495;&#27491;&#30340;&#20998;&#31163;&#8221;&#38750;&#36882;&#24402;&#35268;&#21017;&#65292;&#37117;&#23384;&#22312;&#19968;&#31181;&#36830;&#25509;&#26597;&#35810;&#27809;&#26377;UCQ&#37325;&#20889;&#30340;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#38024;&#23545;&#26144;&#23556;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20998;&#31163;&#26144;&#23556;&#26469;&#30830;&#23450;UCQ&#26159;&#21542;&#36890;&#36807;UCQ&#37325;&#20889;&#30340;&#38382;&#39064;&#36890;&#24120;&#26159;&#19968;&#20010;coNP&#38590;&#38382;&#39064;&#65292;&#22240;&#27492;&#19981;&#22826;&#21487;&#33021;&#26377;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the issue of answering unions of conjunctive queries (UCQs) with disjunctive existential rules and mappings. While this issue has already been well studied from a chase perspective, query rewriting within UCQs has hardly been addressed yet. We first propose a sound and complete query rewriting operator, which has the advantage of establishing a tight relationship between a chase step and a rewriting step. The associated breadth-first query rewriting algorithm outputs a minimal UCQ-rewriting when one exists. Second, we show that for any ``truly disjunctive'' nonrecursive rule, there exists a conjunctive query that has no UCQ-rewriting. It follows that the notion of finite unification sets (fus), which denotes sets of existential rules such that any UCQ admits a UCQ-rewriting, seems to have little relevance in this setting. Finally, turning our attention to mappings, we show that the problem of determining whether a UCQ admits a UCQ-rewriting through a disjunctive mapping is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;</title><link>http://arxiv.org/abs/2306.05965</link><description>&lt;p&gt;
&#22312;&#22240;&#23376;&#22270;&#20013;&#33258;&#21160;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#65292;&#36125;&#21494;&#26031;&#29366;&#24577;&#21644;&#21442;&#25968;&#20272;&#35745;&#24050;&#32463;&#34987;&#26377;&#25928;&#33258;&#21160;&#21270;&#65292;&#20294;&#23545;&#20110;&#27169;&#22411;&#27604;&#36739;&#23578;&#26410;&#22914;&#27492;&#65292;&#22240;&#27492;&#20173;&#38656;&#35201;&#23481;&#26131;&#20986;&#38169;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#25512;&#23548;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#27604;&#36739;&#32463;&#24120;&#34987;&#24573;&#35270;&#21644;&#24573;&#30053;&#65292;&#23613;&#31649;&#23427;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;Forney&#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#19978;&#20351;&#29992;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#26469;&#39640;&#25928;&#22320;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#12290;&#36827;&#32780;&#21487;&#20351;&#29992;&#32553;&#25918;&#22240;&#23376;&#21516;&#26102;&#25191;&#34892;&#21442;&#25968;&#21644;&#29366;&#24577;&#25512;&#26029;&#20197;&#21450;&#27169;&#22411;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#65292;&#21516;&#26102;&#20801;&#35768;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#20998;&#23618;&#21644;&#26102;&#38388;&#27169;&#22411;&#20808;&#39564;&#65292;&#20197;&#36866;&#24212;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#21464;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian state and parameter estimation have been automated effectively in the literature, however, this has not yet been the case for model comparison, which therefore still requires error-prone and time-consuming manual derivations. As a result, model comparison is often overlooked and ignored, despite its importance. This paper efficiently automates Bayesian model averaging, selection, and combination by message passing on a Forney-style factor graph with a custom mixture node. Parameter and state inference, and model comparison can then be executed simultaneously using message passing with scale factors. This approach shortens the model design cycle and allows for the straightforward extension to hierarchical and temporal model priors to accommodate for modeling complicated time-varying processes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#26041;&#27861;&#26469;&#35780;&#20272;&#20219;&#20309;&#27169;&#24577;&#30340;&#29983;&#25104;AI&#31995;&#32479;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#20998;&#20026;&#22522;&#30784;&#31995;&#32479;&#21644;&#31038;&#20250;&#26041;&#38754;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;7&#20010;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#38544;&#31169;&#20445;&#25252;&#12289;&#29615;&#22659;&#25104;&#26412;&#31561;&#12290;</title><link>http://arxiv.org/abs/2306.05949</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;AI&#31995;&#32479;&#22312;&#31995;&#32479;&#21644;&#31038;&#20250;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Social Impact of Generative AI Systems in Systems and Society. (arXiv:2306.05949v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#26041;&#27861;&#26469;&#35780;&#20272;&#20219;&#20309;&#27169;&#24577;&#30340;&#29983;&#25104;AI&#31995;&#32479;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#20998;&#20026;&#22522;&#30784;&#31995;&#32479;&#21644;&#31038;&#20250;&#26041;&#38754;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;7&#20010;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#38544;&#31169;&#20445;&#25252;&#12289;&#29615;&#22659;&#25104;&#26412;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#31995;&#32479;&#36328;&#36234;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#31561;&#22810;&#31181;&#27169;&#24577;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#20294;&#30446;&#21069;&#19981;&#23384;&#22312;&#23448;&#26041;&#26631;&#20934;&#26469;&#35780;&#20272;&#36825;&#20123;&#24433;&#21709;&#21644;&#24212;&#35813;&#35780;&#20272;&#21738;&#20123;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#26041;&#27861;&#26469;&#35780;&#20272;&#20219;&#20309;&#27169;&#24577;&#30340;&#29983;&#25104;AI&#31995;&#32479;&#65292;&#20998;&#20026;&#20004;&#22823;&#31867;&#21035;&#65306;&#23545;&#20110;&#27809;&#26377;&#39044;&#23450;&#24212;&#29992;&#30340;&#22522;&#30784;&#31995;&#32479;&#21487;&#20197;&#35780;&#20272;&#20160;&#20040;&#65292;&#20197;&#21450;&#21487;&#20197;&#22312;&#31038;&#20250;&#20013;&#35780;&#20272;&#20160;&#20040;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#22522;&#30784;&#25216;&#26415;&#31995;&#32479;&#12289;&#20154;&#27665;&#21644;&#31038;&#20250;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#31995;&#32479;&#26694;&#26550;&#23450;&#20041;&#20102;&#19971;&#20010;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#65306;&#20559;&#35265;&#12289;&#21051;&#26495;&#21360;&#35937;&#21644;&#34920;&#29616;&#24615;&#20260;&#23475;&#65307;&#25991;&#21270;&#20215;&#20540;&#21644;&#25935;&#24863;&#20869;&#23481;&#65307;&#19981;&#23545;&#31561;&#30340;&#24615;&#33021;&#65307;&#38544;&#31169;&#21644;&#25968;&#25454;&#20445;&#25252;&#65307;&#36130;&#21153;&#25104;&#26412;&#65307;&#29615;&#22659;&#25104;&#26412;&#65307;&#20197;&#21450;&#25968;&#25454;&#21644;&#20869;&#23481;&#30417;&#31649;&#21171;&#21160;&#25104;&#26412;&#12290;&#24314;&#35758;&#30340;&#35780;&#20272;&#26041;&#27861;&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#24577;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. We move toward a standard approach in evaluating a generative AI system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. We describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to all modalities and analyses of the li
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05880</link><description>&lt;p&gt;
&#22522;&#20110;Implicit Neural Representations&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#29992;&#20110;&#25554;&#20540;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26102;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;Implicit Neural Representations (INR)&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25110;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#35843;&#21046;INR&#21442;&#25968;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#35265;&#26679;&#26412;&#21644;&#26102;&#38388;&#31383;&#21475;&#31227;&#20301;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#26041;&#38754;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;FedWon&#65292;&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.05879</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#26159;&#21542;&#31163;&#19981;&#24320;&#26631;&#20934;&#21270;?
&lt;/p&gt;
&lt;p&gt;
Is Normalization Indispensable for Multi-domain Federated Learning?. (arXiv:2306.05879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;FedWon&#65292;&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20998;&#25955;&#22312;&#23458;&#25143;&#31471;&#19978;&#30340;&#21327;&#20316;&#24335;&#20869;&#37096;&#35757;&#32451;&#22686;&#24378;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65288;non-i.i.d&#65289;&#23548;&#33268;&#30340;&#28508;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#25910;&#25947;&#21463;&#38459;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#19968;&#20010;&#20851;&#38190;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#8212;&#8212;&#22810;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#28304;&#20110;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#21508;&#31181;&#39046;&#22495;&#65292;&#32780;&#19981;&#26159;&#26631;&#31614;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#31216;&#20026;&#19981;&#20351;&#29992;&#35268;&#33539;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FedWon&#65289;&#12290;FedWon&#20174;&#19968;&#20010;&#35266;&#23519;&#20986;&#21457;&#65292;&#21363;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#22312;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#20010;&#39046;&#22495;&#30340;&#32479;&#35745;&#20449;&#24687;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#26367;&#20195;&#35268;&#33539;&#21270;&#25216;&#26415;&#20855;&#26377;&#33258;&#36523;&#30340;&#23616;&#38480;&#24615;&#12290;FedWon&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to non-independent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior studies predominantly addressed the issue of skewed label distribution, our research addresses a crucial yet frequently overlooked problem known as multi-domain FL. In this scenario, clients' data originate from diverse domains with distinct feature distributions, as opposed to label distributions. To address the multi-domain problem in FL, we propose a novel method called Federated learning Without normalizations (FedWon). FedWon draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains, while alternative normalization techniques possess their own limitations. In order to address these issues, FedWon elimi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#38750;&#40065;&#26834;&#26041;&#21521;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23616;&#22495;&#20108;&#27425;&#36924;&#36817;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#20026;&#23433;&#20840;&#35266;&#27979;&#21644;&#23545;&#25239;&#24615;&#35266;&#27979;&#20043;&#38388;&#30340;&#22522;&#26412;&#25130;&#27490;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;&#24182;&#19988;&#35813;&#26041;&#27861;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#65292;&#33021;&#25104;&#21151;&#26816;&#27979;&#21040;&#23545;&#25239;&#24615;&#26041;&#21521;&#24182;&#20570;&#20986;&#40065;&#26834;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.05873</link><description>&lt;p&gt;
&#26816;&#27979;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#24615;&#26041;&#21521;&#20197;&#20570;&#20986;&#40065;&#26834;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions. (arXiv:2306.05873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05873
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#38750;&#40065;&#26834;&#26041;&#21521;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23616;&#22495;&#20108;&#27425;&#36924;&#36817;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#20026;&#23433;&#20840;&#35266;&#27979;&#21644;&#23545;&#25239;&#24615;&#35266;&#27979;&#20043;&#38388;&#30340;&#22522;&#26412;&#25130;&#27490;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;&#24182;&#19988;&#35813;&#26041;&#27861;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#65292;&#33021;&#25104;&#21151;&#26816;&#27979;&#21040;&#23545;&#25239;&#24615;&#26041;&#21521;&#24182;&#20570;&#20986;&#40065;&#26834;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#29616;&#22312;&#21487;&#20197;&#22312;&#20855;&#26377;&#39640;&#24230;&#22797;&#26434;&#29366;&#24577;&#34920;&#31034;&#30340;MDPs&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20197;&#21450;&#35266;&#27979;&#31354;&#38388;&#32500;&#24230;&#30340;&#22686;&#21152;&#37117;&#24102;&#26469;&#20102;&#26131;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#27874;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#31574;&#30053;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#28145;&#24230;&#31070;&#32463;&#31574;&#30053;&#25439;&#22833;&#30340;&#23616;&#22495;&#20108;&#27425;&#36924;&#36817;&#26469;&#26816;&#27979;&#36825;&#20123;&#38750;&#40065;&#26834;&#26041;&#21521;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#23433;&#20840;&#35266;&#27979;&#21644;&#23545;&#25239;&#24615;&#35266;&#27979;&#20043;&#38388;&#30340;&#22522;&#26412;&#25130;&#27490;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#26041;&#21521;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Arcade Learning Environment&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#25216;&#26415;&#12290;&#26368;&#26174;&#30528;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#65292;&#33021;&#22815;&#25104;&#21151;&#26816;&#27979;&#21040;&#23545;&#25239;&#24615;&#26041;&#21521;&#24182;&#20570;&#20986;&#30456;&#24212;&#30340;&#40065;&#26834;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning in MDPs with highly complex state representations is currently possible due to multiple advancements in reinforcement learning algorithm design. However, this incline in complexity, and furthermore the increase in the dimensions of the observation came at the cost of volatility that can be taken advantage of via adversarial attacks (i.e. moving along worst-case directions in the observation space). To solve this policy instability problem we propose a novel method to detect the presence of these non-robust directions via local quadratic approximation of the deep neural policy loss. Our method provides a theoretical basis for the fundamental cut-off between safe observations and adversarial observations. Furthermore, our technique is computationally efficient, and does not depend on the methods used to produce the worst-case directions. We conduct extensive experiments in the Arcade Learning Environment with several different adversarial attack techniques. Most significantly, w
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;Motion-DVAE&#65292;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#20381;&#36182;&#20851;&#31995;&#30340;&#36816;&#21160;&#20808;&#39564;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#30340;&#20154;&#20307;&#36816;&#21160;&#21435;&#22122;&#12290;&#20854;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#21435;&#22122;&#26041;&#27861;&#32467;&#21512;&#20102;&#22238;&#24402;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05846</link><description>&lt;p&gt;
Motion-DVAE: &#38754;&#21521;&#24555;&#36895;&#20154;&#20307;&#36816;&#21160;&#21435;&#22122;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Motion-DVAE: Unsupervised learning for fast human motion denoising. (arXiv:2306.05846v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;Motion-DVAE&#65292;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#20381;&#36182;&#20851;&#31995;&#30340;&#36816;&#21160;&#20808;&#39564;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#30340;&#20154;&#20307;&#36816;&#21160;&#21435;&#22122;&#12290;&#20854;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#21435;&#22122;&#26041;&#27861;&#32467;&#21512;&#20102;&#22238;&#24402;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20174;&#22024;&#26434;&#30340;&#35266;&#27979;&#20013;&#24674;&#22797;&#30495;&#23454;&#20934;&#30830;&#30340;&#20154;&#20307;&#36816;&#21160;&#20013;&#65292;&#23039;&#24577;&#21644;&#36816;&#21160;&#20808;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290; &#22312;&#22270;&#20687;&#20013;&#36827;&#34892;&#23039;&#24577;&#21644;&#24418;&#29366;&#20272;&#35745;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20808;&#39564;&#26469;&#25552;&#28860;&#36880;&#24103;&#39044;&#27979;&#30340;&#32467;&#26524;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290; &#28982;&#32780;&#65292;&#35768;&#22810;&#36816;&#21160;&#20808;&#39564;&#20165;&#27169;&#25311;&#30456;&#37051;&#23039;&#24577;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#24182;&#22312;&#32791;&#26102;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#20351;&#29992;&#65292;&#36825;&#23545;&#35768;&#22810;&#38656;&#35201;&#23454;&#26102;&#36816;&#21160;&#25429;&#25417;&#30340;&#24212;&#29992;&#31243;&#24207;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;Motion-DVAE&#65292;&#36825;&#26159;&#19968;&#31181;&#36816;&#21160;&#20808;&#39564;&#65292;&#29992;&#20110;&#25429;&#33719;&#20154;&#31867;&#36816;&#21160;&#30340;&#30701;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290; &#20316;&#20026;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;DVAE&#65289;&#27169;&#22411;&#31995;&#21015;&#30340;&#19968;&#37096;&#20998;&#65292;Motion-DVAE&#32467;&#21512;&#20102;VAE&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#24490;&#29615;&#32467;&#26500;&#30340;&#26102;&#38388;&#24314;&#27169;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#21435;&#22122;&#26041;&#27861;&#65292;&#32467;&#21512;&#22238;&#24402;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#20110;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#23454;&#26102;&#30340;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Pose and motion priors are crucial for recovering realistic and accurate human motion from noisy observations. Substantial progress has been made on pose and shape estimation from images, and recent works showed impressive results using priors to refine frame-wise predictions. However, a lot of motion priors only model transitions between consecutive poses and are used in time-consuming optimization procedures, which is problematic for many applications requiring real-time motion capture. We introduce Motion-DVAE, a motion prior to capture the short-term dependencies of human motion. As part of the dynamical variational autoencoder (DVAE) models family, Motion-DVAE combines the generative capability of VAE models and the temporal modeling of recurrent architectures. Together with Motion-DVAE, we introduce an unsupervised learned denoising method unifying regression- and optimization-based approaches in a single framework for real-time 3D human pose estimation. Experiments show that the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22788;&#29702;&#22810;&#26679;&#21270;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#26410;&#30693;&#32422;&#26463;&#20197;&#21450;&#26597;&#35810;&#25298;&#32477;&#38382;&#39064;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05843</link><description>&lt;p&gt;
&#26080;&#39046;&#22495;&#20559;&#35265;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#31215;&#20998;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Batch Bayesian Optimization with Diverse Constraints via Bayesian Quadrature. (arXiv:2306.05843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22788;&#29702;&#22810;&#26679;&#21270;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#26410;&#30693;&#32422;&#26463;&#20197;&#21450;&#26597;&#35810;&#25298;&#32477;&#38382;&#39064;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#20855;&#26377;&#22810;&#26679;&#30340;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#39640;&#24230;&#21487;&#24182;&#34892;&#21270;&#31561;&#29305;&#28857;&#12290;&#21516;&#26102;&#65292;&#24403;&#23384;&#22312;&#26410;&#30693;&#32422;&#26463;&#26102;&#65292;&#20363;&#22914;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#21160;&#29289;&#23454;&#39564;&#23433;&#20840;&#24615;&#31561;&#39046;&#22495;&#65292;&#24517;&#39035;&#30830;&#31435;&#26410;&#30693;&#32422;&#26463;&#20043;&#21518;&#25165;&#33021;&#26597;&#35810;&#30446;&#26631;&#20989;&#25968;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#20165;&#38024;&#23545;&#19978;&#36848;&#26576;&#20123;&#29305;&#24449;&#32780;&#24182;&#38750;&#32508;&#21512;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22522;&#20110;SOBER&#31639;&#27861;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#35880;&#24910;&#24182;&#34892;&#20027;&#21160;&#37319;&#26679;&#22120;&#65292;&#32771;&#34385;&#21040;&#20102;&#26410;&#30693;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#38598;&#25104;&#35823;&#24046;&#30340;&#24433;&#21709;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#26041;&#27861;&#65292;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#26465;&#20214;&#21644;&#26410;&#30693;&#32422;&#26463;&#26597;&#35810;&#25298;&#32477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world optimisation problems often feature complex combinations of (1) diverse constraints, (2) discrete and mixed spaces, and are (3) highly parallelisable. (4) There are also cases where the objective function cannot be queried if unknown constraints are not satisfied, e.g. in drug discovery, safety on animal experiments (unknown constraints) must be established before human clinical trials (querying objective function) may proceed. However, most existing works target each of the above three problems in isolation and do not consider (4) unknown constraints with query rejection. For problems with diverse constraints and/or unconventional input spaces, it is difficult to apply these techniques as they are often mutually incompatible. We propose cSOBER, a domain-agnostic prudent parallel active sampler for Bayesian optimisation, based on SOBER of Adachi et al. (2023). We consider infeasibility under unknown constraints as a type of integration error that we can estimate. We propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.05836</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20174;&#30456;&#20851;&#24615;&#20013;&#25512;&#26029;&#20986;&#22240;&#26524;&#20851;&#31995;?
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#20154;&#31867;&#26234;&#24935;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#34429;&#28982;CausalNLP&#39046;&#22495;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;NLP&#20013;&#29616;&#26377;&#30340;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#32463;&#39564;&#30693;&#35782;&#65288;&#20363;&#22914;&#24120;&#35782;&#30693;&#35782;&#65289;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32431;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;Corr2Cause&#65292;&#23427;&#37319;&#29992;&#19968;&#32452;&#30456;&#20851;&#35821;&#21477;&#24182;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;400K&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#22312;&#20854;&#20013;&#35780;&#20272;&#20102;17&#20010;&#29616;&#26377;&#30340;LLMs&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;LLMs&#22312;&#22240;&#26524;&#25512;&#26029;&#25216;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20960;&#20046;&#25509;&#36817;&#38543;&#26426;&#12290;&#24403;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#24494;&#35843;&#23558;LLMs&#37325;&#26032;&#29992;&#20110;&#36825;&#31181;&#25216;&#33021;&#26102;&#65292;&#36825;&#31181;&#32570;&#38519;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.05817</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22914;&#20309;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21463;&#30410;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Can Recommender Systems Benefit from Large Language Models: A Survey. (arXiv:2306.05817v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#21305;&#37197;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#25351;&#20196;&#36319;&#36394;&#12289;&#25512;&#29702;&#65289;&#65292;&#20174;&#32780;&#20026;&#23558;LLM&#35843;&#25972;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#30740;&#31350;&#26041;&#21521;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24212;&#29992;&#23548;&#21521;&#30340;&#35282;&#24230;&#23545;&#27492;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20004;&#20010;&#27491;&#20132;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#23545;&#20110;&#8220;&#22312;&#21738;&#37324;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLM&#22312;&#25512;&#33616;&#27969;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#20013;&#21487;&#33021;&#21457;&#25381;&#30340;&#20316;&#29992;&#65292;&#21363;&#29305;&#24449;&#24037;&#31243;&#12289;&#29305;&#24449;&#32534;&#30721;&#22120;&#12289;&#35780;&#20998;/&#25490;&#21517;&#20989;&#25968;&#21644;&#27969;&#31243;&#25511;&#21046;&#22120;&#12290;&#23545;&#20110;&#8220;&#22914;&#20309;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#20174;&#32780;&#24471;&#20986;&#20004;&#20010;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#26631;&#20934;&#65292;&#21363;&#26159;&#21542;&#35843;&#25972;LLM&#21644;&#26159;&#21542;&#23558;LLM&#20316;&#20026;&#29420;&#31435;&#27169;&#22411;&#25110;&#28151;&#21512;&#27169;&#22411;&#32452;&#20214;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#23558;LLM&#35843;&#25972;&#21040;RS&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#28508;&#22312;&#26041;&#21521;&#65292;&#21253;&#25324;&#19982;&#29616;&#26377;&#31995;&#32479;&#30340;&#38598;&#25104;&#12289;&#29992;&#25143;&#21453;&#39304;&#12289;&#35780;&#20272;&#24230;&#37327;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems (RS) play important roles to match users' information needs for Internet applications. In natural language processing (NLP) domains, large language model (LLM) has shown astonishing emergent abilities (e.g., instruction following, reasoning), thus giving rise to the promising research direction of adapting LLM to RS for performance enhancements and user experience improvements. In this paper, we conduct a comprehensive survey on this research direction from an application-oriented view. We first summarize existing research works from two orthogonal perspectives: where and how to adapt LLM to RS. For the "WHERE" question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, and pipeline controller. For the "HOW" question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLMs or not, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#26694;&#26550;PAAE&#65292;&#23558;&#29983;&#29289;&#36890;&#36335;&#20449;&#24687;&#34701;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#25552;&#39640;&#23545;&#30142;&#30149;&#30340;&#39044;&#27979;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.05813</link><description>&lt;p&gt;
&#22522;&#20110;&#36890;&#36335;&#27963;&#24615;&#33258;&#32534;&#30721;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#34701;&#20837;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Incorporating Prior Knowledge in Deep Learning Models via Pathway Activity Autoencoders. (arXiv:2306.05813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#26694;&#26550;PAAE&#65292;&#23558;&#29983;&#29289;&#36890;&#36335;&#20449;&#24687;&#34701;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#25552;&#39640;&#23545;&#30142;&#30149;&#30340;&#39044;&#27979;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#39640;&#36890;&#37327;&#20998;&#23376;&#20998;&#26512;&#25216;&#26415;&#65288;&#20363;&#22914;&#36716;&#24405;&#32452;&#23398;&#65289;&#30340;&#35745;&#31639;&#20998;&#26512;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19982;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#20108;&#20998;&#27861;&#12290;&#27492;&#22806;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#35797;&#22270;&#23558;&#35299;&#37322;&#24615;&#36716;&#21270;&#20026;&#29983;&#29289;&#23398;&#30456;&#20851;&#26415;&#35821;&#65292;&#20363;&#22914;&#24050;&#30693;&#30340;&#36890;&#36335;&#32423;&#32852;&#12290;&#29983;&#29289;&#23398;&#36890;&#36335;&#21453;&#26144;&#20449;&#21495;&#20107;&#20214;&#25110;&#20195;&#35874;&#36716;&#21270;&#12290;&#36890;&#36807;&#30830;&#23450;&#21738;&#20123;&#36890;&#36335;&#28041;&#21450;&#30142;&#30149;&#24182;&#23558;&#27492;&#31867;&#36890;&#36335;&#25968;&#25454;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#21487;&#33021;&#22686;&#24378;&#39044;&#27979;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#35786;&#26029;&#27835;&#30103;&#21644;&#39044;&#38450;&#30142;&#30149;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Despite advances in the computational analysis of high-throughput molecular profiling assays (e.g. transcriptomics), a dichotomy exists between methods that are simple and interpretable, and ones that are complex but with lower degree of interpretability. Furthermore, very few methods deal with trying to translate interpretability in biologically relevant terms, such as known pathway cascades. Biological pathways reflecting signalling events or metabolic conversions are Small improvements or modifications of existing algorithms will generally not be suitable, unless novel biological results have been predicted and verified. Determining which pathways are implicated in disease and incorporating such pathway data as prior knowledge may enhance predictive modelling and personalised strategies for diagnosis, treatment and prevention of disease.  Results: We propose a novel prior-knowledge-based deep auto-encoding framework, PAAE, together with its accompanying generative varian
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#36171;&#20104;&#29992;&#25143;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05809</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#31185;&#23398;&#25991;&#29486;&#25512;&#33616;&#31995;&#32479;&#20013;&#37319;&#29992;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System. (arXiv:2306.05809v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#36171;&#20104;&#29992;&#25143;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#37319;&#29992;&#19968;&#31181;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#26041;&#27861;&#65292;&#21521;&#27599;&#20010;&#29992;&#25143;&#25552;&#20379;&#30456;&#21516;&#31243;&#24230;&#30340;&#35299;&#37322;&#65292;&#32780;&#19981;&#32771;&#34385;&#20182;&#20204;&#30340;&#20010;&#20307;&#38656;&#27714;&#21644;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35299;&#37322;&#22823;&#22810;&#20197;&#38745;&#24577;&#21644;&#38750;&#20132;&#20114;&#26041;&#24335;&#21576;&#29616;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#22522;&#20110;&#20854;&#38656;&#27714;&#21644;&#20559;&#22909;&#36827;&#34892;&#20132;&#20114;&#12289;&#25511;&#21046;&#21644;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#65288;&#22522;&#26412;&#12289;&#20013;&#32423;&#21644;&#39640;&#32423;&#65289;&#65292;&#24182;&#22312;&#36879;&#26126;&#30340;&#25512;&#33616;&#21644;&#20852;&#36259;&#24314;&#27169;&#24212;&#29992;&#65288;RIMA&#65289;&#20013;&#23454;&#29616;&#20102;&#23427;&#20204;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23450;&#24615;&#29992;&#25143;&#30740;&#31350;&#65288;N=14&#65289;&#65292;&#20197;&#35843;&#26597;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#23545;&#29992;&#25143;&#23545;&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#30340;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable recommender systems (RS) have traditionally followed a one-size-fits-all approach, delivering the same explanation level of detail to each user, without considering their individual needs and goals. Further, explanations in RS have so far been presented mostly in a static and non-interactive manner. To fill these research gaps, we aim in this paper to adopt a user-centered, interactive explanation model that provides explanations with different levels of detail and empowers users to interact with, control, and personalize the explanations based on their needs and preferences. We followed a user-centered approach to design interactive explanations with three levels of detail (basic, intermediate, and advanced) and implemented them in the transparent Recommendation and Interest Modeling Application (RIMA). We conducted a qualitative user study (N=14) to investigate the impact of providing interactive explanations with varying level of details on the users' perception of the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;XAI&#26041;&#27861;&#25913;&#36827;&#20998;&#31867;&#31995;&#32479;&#30340;&#31574;&#30053;&#65292;&#25506;&#31350;&#20102;&#20004;&#31181;&#20351;&#29992;&#35299;&#37322;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;Integrated Gradients&#26041;&#27861;&#21019;&#24314;&#30340;&#35299;&#37322;&#21487;&#20197;&#31361;&#20986;&#36755;&#20837;&#29305;&#24449;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05801</link><description>&lt;p&gt;
&#21033;&#29992;XAI&#25913;&#36827;&#20998;&#31867;&#31995;&#32479;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Strategies to exploit XAI to improve classification systems. (arXiv:2306.05801v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;XAI&#26041;&#27861;&#25913;&#36827;&#20998;&#31867;&#31995;&#32479;&#30340;&#31574;&#30053;&#65292;&#25506;&#31350;&#20102;&#20004;&#31181;&#20351;&#29992;&#35299;&#37322;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;Integrated Gradients&#26041;&#27861;&#21019;&#24314;&#30340;&#35299;&#37322;&#21487;&#20197;&#31361;&#20986;&#36755;&#20837;&#29305;&#24449;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#21521;&#29992;&#25143;&#21576;&#29616;AI&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20102;&#35299;AI&#27169;&#22411;&#30340;&#32467;&#26524;&#24182;&#36229;&#36234;&#20854;&#20915;&#31574;&#12290; XAI&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#25552;&#20379;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#26469;&#25913;&#36827;AI&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;XAI&#25991;&#29486;&#37117;&#38598;&#20013;&#22312;&#22914;&#20309;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#32780;&#36739;&#23569;&#20851;&#27880;&#22914;&#20309;&#21033;&#29992;XAI&#26041;&#27861;&#25913;&#36827;AI&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19968;&#32452;&#36890;&#24120;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#31867;&#20219;&#21153;&#30340;&#30693;&#21517;XAI&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#34987;&#21033;&#29992;&#65292;&#19981;&#20165;&#20026;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#25913;&#36827;&#20854;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;Fashion-MNIST&#12289;CIFAR10&#21644;STL10&#19978;&#25253;&#21578;&#20102;&#20004;&#31181;&#20351;&#29992;&#35299;&#37322;&#26469;&#25913;&#36827;&#20998;&#31867;&#31995;&#32479;&#30340;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#32508;&#21512;&#28176;&#21464;&#21019;&#24314;&#30340;&#35299;&#37322;&#31361;&#20986;&#20102;&#36755;&#20837;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) aims to provide insights into the decision-making process of AI models, allowing users to understand their results beyond their decisions. A significant goal of XAI is to improve the performance of AI models by providing explanations for their decision-making processes. However, most XAI literature focuses on how to explain an AI system, while less attention has been given to how XAI methods can be exploited to improve an AI system. In this work, a set of well-known XAI methods typically used with Machine Learning (ML) classification tasks are investigated to verify if they can be exploited, not just to provide explanations but also to improve the performance of the model itself. To this aim, two strategies to use the explanation to improve a classification system are reported and empirically evaluated on three datasets: Fashion-MNIST, CIFAR10, and STL10. Results suggest that explanations built by Integrated Gradients highlight input features t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22240;&#26524;&#22270;&#21457;&#29616;&#30340;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;r-adaptive&#31639;&#27861;&#22312;&#26368;&#23567;&#21270;&#24635;&#24178;&#39044;&#27425;&#25968;&#30340;&#21516;&#26102;&#27491;&#30830;&#23398;&#20064;&#20986;&#22240;&#26524;&#22270;&#12290;</title><link>http://arxiv.org/abs/2306.05781</link><description>&lt;p&gt;
&#22240;&#26524;&#22270;&#21457;&#29616;&#30340;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Adaptivity Complexity for Causal Graph Discovery. (arXiv:2306.05781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05781
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22240;&#26524;&#22270;&#21457;&#29616;&#30340;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;r-adaptive&#31639;&#27861;&#22312;&#26368;&#23567;&#21270;&#24635;&#24178;&#39044;&#27425;&#25968;&#30340;&#21516;&#26102;&#27491;&#30830;&#23398;&#20064;&#20986;&#22240;&#26524;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20219;&#21153;&#26159;&#35774;&#35745;&#19968;&#20010;&#24178;&#39044;&#31574;&#30053;&#65292;&#22312;&#26368;&#23567;&#21270;&#25191;&#34892;&#24178;&#39044;&#30340;&#25968;&#37327;&#30340;&#21516;&#26102;&#23398;&#20064;&#21253;&#21547;n&#20010;&#33410;&#28857;&#30340;&#22240;&#26524;&#22270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#24635;&#20849;$r$&#20010;&#39034;&#24207;&#22238;&#21512;&#65292;&#31639;&#27861;&#35774;&#35745;&#24072;&#22914;&#20309;&#22312;&#26368;&#23567;&#21270;&#24635;&#24178;&#39044;&#27425;&#25968;&#30340;&#21516;&#26102;&#24674;&#22797;&#22240;&#26524;&#22270;&#30340;$r$&#36866;&#24212;&#24615;&#38382;&#39064;&#12290;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;$r$-adaptive&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;$O(n^2 2^r)$&#30340;&#24635;&#24178;&#39044;&#27425;&#25968;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#27491;&#30830;&#22320;&#23398;&#20064;$n$&#20010;&#33410;&#28857;&#30340;&#22240;&#26524;&#22270;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#30028;&#38480;&#26159;&#26368;&#20339;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery from interventional data is an important problem, where the task is to design an interventional strategy that learns the hidden ground truth causal graph $G(V,E)$ on $|V| = n$ nodes while minimizing the number of performed interventions. Most prior interventional strategies broadly fall into two categories: non-adaptive and adaptive. Non-adaptive strategies decide on a single fixed set of interventions to be performed while adaptive strategies can decide on which nodes to intervene on sequentially based on past interventions. While adaptive algorithms may use exponentially fewer interventions than their non-adaptive counterparts, there are practical concerns that constrain the amount of adaptivity allowed. Motivated by this trade-off, we study the problem of $r$-adaptivity, where the algorithm designer recovers the causal graph under a total of $r$ sequential rounds whilst trying to minimize the total number of interventions. For this problem, we provide a $r$-adaptive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#21327;&#20316;&#23398;&#20064;&#28608;&#21169;&#35774;&#35745;&#65292;&#36991;&#20813;&#20102;&#8220;&#23500;&#32773;&#36234;&#23500;&#8221;&#30340;&#29616;&#35937;&#65292;&#24182;&#20026;&#36739;&#23569;&#36164;&#28304;&#30340;&#33410;&#28857;&#25552;&#20379;&#20102;&#38271;&#26399;&#24179;&#31561;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2306.05764</link><description>&lt;p&gt;
&#20844;&#24179;&#20294;&#28176;&#36817;&#30456;&#31561;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fair yet Asymptotically Equal Collaborative Learning. (arXiv:2306.05764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#21327;&#20316;&#23398;&#20064;&#28608;&#21169;&#35774;&#35745;&#65292;&#36991;&#20813;&#20102;&#8220;&#23500;&#32773;&#36234;&#23500;&#8221;&#30340;&#29616;&#35937;&#65292;&#24182;&#20026;&#36739;&#23569;&#36164;&#28304;&#30340;&#33410;&#28857;&#25552;&#20379;&#20102;&#38271;&#26399;&#24179;&#31561;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27969;&#25968;&#25454;&#30340;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#33410;&#28857;&#65288;&#20363;&#22914;&#32452;&#32455;&#65289;&#36890;&#36807;&#20849;&#20139;&#20174;&#20854;&#26368;&#26032;&#27969;&#25968;&#25454;&#35745;&#31639;&#20986;&#30340;&#26368;&#26032;&#27169;&#22411;&#26356;&#26032;&#26469;&#20849;&#21516;&#25345;&#32493;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#20026;&#20102;&#26356;&#26377;&#36164;&#28304;&#30340;&#33410;&#28857;&#24895;&#24847;&#20849;&#20139;&#20854;&#27169;&#22411;&#26356;&#26032;&#65292;&#20182;&#20204;&#38656;&#35201;&#24471;&#21040;&#20844;&#24179;&#30340;&#28608;&#21169;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#28608;&#21169;&#35774;&#35745;&#65292;&#20445;&#35777;&#20844;&#24179;&#65292;&#20351;&#33410;&#28857;&#33719;&#24471;&#19982;&#20854;&#36129;&#29486;&#30456;&#31216;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25506;&#32034;-&#21033;&#29992;&#30340;&#24418;&#24335;&#20272;&#35745;&#33410;&#28857;&#30340;&#36129;&#29486;&#65288;&#21363;&#25506;&#32034;&#65289;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#20844;&#27491;&#28608;&#21169;&#65288;&#21363;&#21033;&#29992;&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#20445;&#35777;&#20844;&#24179;&#30340;&#26041;&#27861;&#20013;&#20986;&#29616;&#20102;&#8220;&#23500;&#32773;&#36234;&#23500;&#8221;&#30340;&#29616;&#35937;&#65292;&#36825;&#38459;&#30861;&#20102;&#36164;&#28304;&#36739;&#23569;&#30340;&#33410;&#28857;&#30340;&#21442;&#19982;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21478;&#22806;&#20445;&#25345;&#28176;&#36817;&#24179;&#31561;&#65292;&#21363;&#36739;&#23569;&#36164;&#28304;&#30340;&#33410;&#28857;&#26368;&#32456;&#23454;&#29616;&#19982;&#36739;&#26377;&#36164;&#28304;&#30340;&#8220;&#23500;&#8221;&#33410;&#28857;&#30456;&#31561;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In collaborative learning with streaming data, nodes (e.g., organizations) jointly and continuously learn a machine learning (ML) model by sharing the latest model updates computed from their latest streaming data. For the more resourceful nodes to be willing to share their model updates, they need to be fairly incentivized. This paper explores an incentive design that guarantees fairness so that nodes receive rewards commensurate to their contributions. Our approach leverages an explore-then-exploit formulation to estimate the nodes' contributions (i.e., exploration) for realizing our theoretically guaranteed fair incentives (i.e., exploitation). However, we observe a "rich get richer" phenomenon arising from the existing approaches to guarantee fairness and it discourages the participation of the less resourceful nodes. To remedy this, we additionally preserve asymptotic equality, i.e., less resourceful nodes achieve equal performance eventually to the more resourceful/"rich" nodes. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26234;&#33021;&#23621;&#23478;&#29615;&#22659;&#20013;&#22522;&#20110;&#24515;&#29702;&#29305;&#24449;&#30340;&#23433;&#20840;&#35686;&#21578;&#24314;&#35758;&#65292;&#21457;&#29616;&#35748;&#30693;&#38656;&#27714;&#31561;&#29305;&#24449;&#24433;&#21709;&#24314;&#35758;&#20351;&#29992;&#26041;&#24335;&#65292;&#26131;&#25026;&#30340;&#23433;&#20840;&#35828;&#26126;&#27604;&#35686;&#21578;&#26356;&#20855;&#35828;&#26381;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05752</link><description>&lt;p&gt;
&#26234;&#33021;&#23621;&#23478;&#29615;&#22659;&#20013;&#22522;&#20110;&#24515;&#29702;&#29305;&#24449;&#30340;&#23433;&#20840;&#35686;&#21578;&#24314;&#35758;&#65306;&#19977;&#39033;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From psychological traits to safety warnings: three studies on recommendations in a smart home environment. (arXiv:2306.05752v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26234;&#33021;&#23621;&#23478;&#29615;&#22659;&#20013;&#22522;&#20110;&#24515;&#29702;&#29305;&#24449;&#30340;&#23433;&#20840;&#35686;&#21578;&#24314;&#35758;&#65292;&#21457;&#29616;&#35748;&#30693;&#38656;&#27714;&#31561;&#29305;&#24449;&#24433;&#21709;&#24314;&#35758;&#20351;&#29992;&#26041;&#24335;&#65292;&#26131;&#25026;&#30340;&#23433;&#20840;&#35828;&#26126;&#27604;&#35686;&#21578;&#26356;&#20855;&#35828;&#26381;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#21578;&#20102; EMPATHY &#39033;&#30446;&#20013;&#36827;&#34892;&#30340;&#19977;&#39033;&#23454;&#39564;&#65292;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#22312;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#20570;&#20986;&#26356;&#22909;&#30340;&#37197;&#32622;&#36873;&#25321;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23384;&#22312;&#24515;&#29702;&#29305;&#24449;&#65292;&#20363;&#22914;&#35748;&#30693;&#38656;&#27714;&#65292;&#24433;&#21709;&#20010;&#20307;&#20351;&#29992;&#24314;&#35758;&#30340;&#26041;&#24335;&#65307;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#65292;&#23545;&#24314;&#35758;&#30340;&#24863;&#30693;&#26377;&#30528;&#38750;&#26174;&#32780;&#26131;&#35265;&#30340;&#20851;&#31995;&#65292;&#20010;&#20307;&#21033;&#29992;&#24314;&#35758;&#36827;&#34892;&#37197;&#32622;&#36873;&#25321;&#30340;&#33021;&#21147;&#22240;&#20154;&#32780;&#24322;&#65307;&#23545;&#20110;&#21487;&#33021;&#24341;&#36215;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#30340;&#35268;&#21017;&#30340;&#36866;&#29992;&#24615;&#20915;&#31574;&#65292;&#35814;&#32454;&#26131;&#25026;&#30340;&#23433;&#20840;&#35828;&#26126;&#27604;&#31616;&#21333;&#30340;&#23433;&#20840;&#35686;&#21578;&#26356;&#20855;&#26377;&#35828;&#26381;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we report on three experiments we have carried out in the context of the EMPATHY project, with the aim of helping users make better configuration choices in a smart home environment, and discuss our results. We found that there are psychological traits, such as Need for Cognition, which influence the way individuals tend to use recommendations, that there are non obvious relationships between the perceived usefulness of recommendations in different domains and individuals' ability to exploit suggestions on configuration choices, and that detailed, easy-to-understand security explanations are more persuasive than simple security warnings, when it comes to make decisions on the applicability of rules which might cause privacy and security risks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;&#36890;&#29992;CP&#32534;&#30721;&#12289;&#32422;&#26463;&#26465;&#20214;&#21644;&#21453;&#26144;JSSP&#26368;&#20248;&#26631;&#20934;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#21487;&#20026;&#22823;&#22411;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20135;&#29983;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.05747</link><description>&lt;p&gt;
&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An End-to-End Reinforcement Learning Approach for Job-Shop Scheduling Problems Based on Constraint Programming. (arXiv:2306.05747v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;&#36890;&#29992;CP&#32534;&#30721;&#12289;&#32422;&#26463;&#26465;&#20214;&#21644;&#21453;&#26144;JSSP&#26368;&#20248;&#26631;&#20934;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#21487;&#20026;&#22823;&#22411;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20135;&#29983;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#32534;&#31243;(CP)&#26159;&#19968;&#31181;&#22768;&#26126;&#24335;&#32534;&#31243;&#33539;&#20363;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#21644;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#22914;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;(JSSP)&#12290;&#34429;&#28982;CP&#27714;&#35299;&#22120;&#33021;&#22815;&#25214;&#21040;&#23567;&#23454;&#20363;&#30340;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;&#20294;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#22823;&#23454;&#20363;&#65292;&#21363;&#38656;&#35201;&#38271;&#26102;&#38388;&#35745;&#31639;&#25110;&#20135;&#29983;&#20302;&#36136;&#37327;&#35299;&#12290;&#22240;&#27492;&#65292;&#23454;&#38469;&#35843;&#24230;&#24212;&#29992;&#32463;&#24120;&#37319;&#29992;&#24555;&#36895;&#25163;&#24037;&#21046;&#20316;&#30340;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#35843;&#24230;&#21551;&#21457;&#24335;&#26041;&#27861;&#20197;&#25214;&#21040;&#33391;&#22909;&#30340;&#21021;&#22987;&#35299;&#65292;&#24182;&#20351;&#29992;&#20248;&#21270;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;CP&#21644;&#24378;&#21270;&#23398;&#20064;(RL)&#26469;&#35299;&#20915;&#35843;&#24230;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#23450;&#21046;&#30340;RL&#26041;&#27861;&#19981;&#21516;&#65292;&#21253;&#25324;&#36807;&#31243;&#27169;&#25311;&#31639;&#27861;&#12289;&#22797;&#26434;&#30340;&#29305;&#24449;&#24037;&#31243;&#25110;&#25163;&#24037;&#21046;&#20316;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#20165;&#38656;&#35201;&#26576;&#20123;&#35843;&#24230;&#38382;&#39064;&#30340;&#36890;&#29992;CP&#32534;&#30721;&#12289;&#19968;&#23567;&#32452;&#38382;&#39064;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#21644;&#21453;&#26144;JSSP&#26368;&#20248;&#26631;&#20934;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23545;&#20960;&#20010;JSSP&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;RL&#21644;CP&#26041;&#27861;&#31454;&#20105;&#65292;&#24182;&#19988;&#36890;&#24120;&#21487;&#20197;&#22312;&#24456;&#30701;&#26102;&#38388;&#20869;&#20026;&#22823;&#22411;&#23454;&#20363;&#20135;&#29983;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constraint Programming (CP) is a declarative programming paradigm that allows for modeling and solving combinatorial optimization problems, such as the Job-Shop Scheduling Problem (JSSP). While CP solvers manage to find optimal or near-optimal solutions for small instances, they do not scale well to large ones, i.e., they require long computation times or yield low-quality solutions. Therefore, real-world scheduling applications often resort to fast, handcrafted, priority-based dispatching heuristics to find a good initial solution and then refine it using optimization methods.  This paper proposes a novel end-to-end approach to solving scheduling problems by means of CP and Reinforcement Learning (RL). In contrast to previous RL methods, tailored for a given problem by including procedural simulation algorithms, complex feature engineering, or handcrafted reward functions, our neural-network architecture and training algorithm merely require a generic CP encoding of some scheduling pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;35&#31687;&#35770;&#25991;&#30340;&#25991;&#29486;&#32508;&#36848;&#20197;&#21450;15&#21517;&#26234;&#33021;&#38899;&#31665;&#29992;&#25143;&#30340;&#35775;&#35848;&#65292;&#24635;&#32467;&#20986;&#20102;&#26234;&#33021;&#38899;&#31665;&#35774;&#35745;&#20013;&#30340;&#20116;&#20010;&#20027;&#39064;&#21644;&#22235;&#20010;&#35774;&#35745;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22235;&#20010;&#35774;&#35745;&#26426;&#36935;&#31354;&#38388;&#65292;&#22914;&#25552;&#20379;&#20449;&#24687;&#25903;&#25345;&#65292;&#22312;&#35774;&#35745;&#20013;&#34701;&#20837;&#29992;&#25143;&#30340;&#24515;&#29702;&#27169;&#22411;&#20197;&#21450;&#25972;&#21512;&#24179;&#38745;&#30340;&#35774;&#35745;&#21407;&#21017;&#31561;&#12290;</title><link>http://arxiv.org/abs/2306.05741</link><description>&lt;p&gt;
&#26234;&#33021;&#38899;&#31665;&#35774;&#35745;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Challenges and Opportunities for the Design of Smart Speakers. (arXiv:2306.05741v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;35&#31687;&#35770;&#25991;&#30340;&#25991;&#29486;&#32508;&#36848;&#20197;&#21450;15&#21517;&#26234;&#33021;&#38899;&#31665;&#29992;&#25143;&#30340;&#35775;&#35848;&#65292;&#24635;&#32467;&#20986;&#20102;&#26234;&#33021;&#38899;&#31665;&#35774;&#35745;&#20013;&#30340;&#20116;&#20010;&#20027;&#39064;&#21644;&#22235;&#20010;&#35774;&#35745;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22235;&#20010;&#35774;&#35745;&#26426;&#36935;&#31354;&#38388;&#65292;&#22914;&#25552;&#20379;&#20449;&#24687;&#25903;&#25345;&#65292;&#22312;&#35774;&#35745;&#20013;&#34701;&#20837;&#29992;&#25143;&#30340;&#24515;&#29702;&#27169;&#22411;&#20197;&#21450;&#25972;&#21512;&#24179;&#38745;&#30340;&#35774;&#35745;&#21407;&#21017;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#25216;&#26415;&#21644;&#35821;&#38899;&#29992;&#25143;&#30028;&#38754;&#65288;VUI&#65289;&#65292;&#22914;Alexa&#12289;Siri&#21644;Google Home&#31561;&#30340;&#36827;&#27493;&#65292;&#20026;&#35768;&#22810;&#26032;&#31867;&#22411;&#30340;&#20132;&#20114;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24066;&#22330;&#36234;&#26469;&#36234;&#22823;&#65292;VUI&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#20154;&#20204;&#20173;&#28982;&#24863;&#21040;&#36825;&#39033;&#25216;&#26415;&#34987;&#20302;&#20272;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;35&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#23558;127&#20010;VUI&#35774;&#35745;&#25351;&#21335;&#32508;&#21512;&#25104;&#20102;&#20116;&#20010;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;15&#21517;&#26234;&#33021;&#38899;&#31665;&#29992;&#25143;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#23545;&#25216;&#26415;&#30340;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#24773;&#20917;&#12290;&#36890;&#36807;&#35775;&#35848;&#65292;&#25105;&#20204;&#24635;&#32467;&#20986;&#20102;&#22235;&#20010;&#23545;&#19981;&#20351;&#29992;&#25216;&#26415;&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#35774;&#35745;&#25361;&#25112;&#12290;&#22522;&#20110;&#20182;&#20204;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#35774;&#35745;&#26426;&#36935;&#31354;&#38388;&#65292;&#20363;&#22914;&#19987;&#27880;&#20110;&#22312;&#22810;&#20219;&#21153;&#65288;&#28921;&#39274;&#12289;&#39550;&#39542;&#12289;&#32946;&#20799;&#31561;&#65289;&#20013;&#25552;&#20379;&#20449;&#24687;&#25903;&#25345;&#65292;&#23558;&#29992;&#25143;&#30340;&#26234;&#33021;&#38899;&#31665;&#24515;&#29702;&#27169;&#22411;&#34701;&#20837;&#35774;&#35745;&#24403;&#20013;&#65292;&#20197;&#21450;&#25972;&#21512;&#24179;&#38745;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in voice technology and voice user interfaces (VUIs) -- such as Alexa, Siri, and Google Home -- have opened up the potential for many new types of interaction. However, despite the potential of these devices reflected by the growing market and body of VUI research, there is a lingering sense that the technology is still underused. In this paper, we conducted a systematic literature review of 35 papers to identify and synthesize 127 VUI design guidelines into five themes. Additionally, we conducted semi-structured interviews with 15 smart speaker users to understand their use and non-use of the technology. From the interviews, we distill four design challenges that contribute the most to non-use. Based on their (non-)use, we identify four opportunity spaces for designers to explore such as focusing on information support while multitasking (cooking, driving, childcare, etc), incorporating users' mental models for smart speakers, and integrating calm design principles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#22810;&#27169;&#24577;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;MXAI&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20174;&#39044;&#27979;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#21040;MXAI&#26041;&#27861;&#21644;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#20102;&#20840;&#38754;&#20171;&#32461;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2306.05731</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65306;&#26041;&#27861;&#23398;&#36827;&#23637;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Multimodal Explainable Artificial Intelligence: A Comprehensive Review of Methodological Advances and Future Research Directions. (arXiv:2306.05731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#22810;&#27169;&#24577;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;MXAI&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20174;&#39044;&#27979;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#21040;MXAI&#26041;&#27861;&#21644;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#20102;&#20840;&#38754;&#20171;&#32461;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#22810;&#27169;&#24577;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;MXAI&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#39318;&#20808;&#25551;&#36848;&#20102;&#30456;&#20851;&#30340;&#20027;&#35201;&#39044;&#27979;&#20219;&#21153;&#21644;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#25552;&#20379;&#20102;&#25991;&#29486;&#20013;MXAI&#26041;&#27861;&#30340;&#32467;&#26500;&#21270;&#20171;&#32461;&#65292;&#32771;&#34385;&#21040;&#20197;&#19979;&#26631;&#20934;&#65306;a&#65289;&#28041;&#21450;&#30340;&#27169;&#24577;&#25968;&#37327;&#65292;b&#65289;&#20135;&#29983;&#35299;&#37322;&#30340;&#38454;&#27573;&#65292;&#20197;&#21450;c&#65289;&#37319;&#29992;&#30340;&#26041;&#27861;&#35770;&#31867;&#22411;&#65288;&#21363;&#25968;&#23398;&#24418;&#24335;&#21270;&#65289;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#29992;&#20110;MXAI&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#26368;&#21518;&#65292;&#25552;&#20379;&#20102;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current study focuses on systematically analyzing the recent advances in the field of Multimodal eXplainable Artificial Intelligence (MXAI). In particular, the relevant primary prediction tasks and publicly available datasets are initially described. Subsequently, a structured presentation of the MXAI methods of the literature is provided, taking into account the following criteria: a) The number of the involved modalities, b) The stage at which explanations are produced, and c) The type of the adopted methodology (i.e. mathematical formalism). Then, the metrics used for MXAI evaluation are discussed. Finally, a comprehensive analysis of current challenges and future research directions is provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#31639;&#27861;&#26469;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.05726</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#20869;&#25919;&#31574;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Sample Policy Iteration for Offline Reinforcement Learning. (arXiv:2306.05726v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#31639;&#27861;&#26469;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#20197;&#21069;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#26469;&#25512;&#23548;&#20986;&#26377;&#25928;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#25968;&#25454;&#35206;&#30422;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#38169;&#35823;&#65292;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20559;&#31163;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#24403;&#31163;&#32447;&#25968;&#25454;&#38598;&#30001;&#27425;&#20248;&#31574;&#30053;&#25910;&#38598;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#20339;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26174;&#33879;&#22686;&#24378;&#20102;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#12290;&#26680;&#24515;&#35265;&#35299;&#26159;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#29992;&#20110;&#34892;&#20026;&#35268;&#21017;&#30340;&#31574;&#30053;&#65292;&#26679;&#26412;&#20869;&#25919;&#31574;&#36845;&#20195;&#36880;&#28176;&#25913;&#36827;&#33258;&#36523;&#65292;&#21516;&#26102;&#38544;&#24335;&#36991;&#20813;&#26597;&#35810;&#26679;&#26412;&#22806;&#30340;&#34892;&#21160;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#30340;&#23398;&#20064;&#22833;&#36133;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;&#20854;&#23398;&#20064;&#20165;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#33391;&#22909;&#35206;&#30422;&#30340;&#34892;&#21160;&#23398;&#20064;&#26679;&#26412;&#20869;&#26368;&#20248;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) seeks to derive an effective control policy from previously collected data. To circumvent errors due to inadequate data coverage, behavior-regularized methods optimize the control policy while concurrently minimizing deviation from the data collection policy. Nevertheless, these methods often exhibit subpar practical performance, particularly when the offline dataset is collected by sub-optimal policies. In this paper, we propose a novel algorithm employing in-sample policy iteration that substantially enhances behavior-regularized methods in offline RL. The core insight is that by continuously refining the policy used for behavior regularization, in-sample policy iteration gradually improves itself while implicitly avoids querying out-of-sample actions to avert catastrophic learning failures. Our theoretical analysis verifies its ability to learn the in-sample optimal policy, exclusively utilizing actions well-covered by the dataset. Moreover, we pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20869;&#37096;&#26159;&#21542;&#21019;&#24314;&#21644;&#20351;&#29992;&#31616;&#21333;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#21457;&#29616;LDM&#20869;&#37096;&#28608;&#27963;&#25552;&#20379;&#20102;&#20851;&#20110;3D&#28145;&#24230;&#25968;&#25454;&#21644;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#20998;&#31163;&#30340;&#32447;&#24615;&#34920;&#31034;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#20855;&#26377;&#22240;&#26524;&#20316;&#29992;, &#21487;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2306.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#34920;&#38754;&#32479;&#35745;&#23398;&#65306;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model. (arXiv:2306.05720v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20869;&#37096;&#26159;&#21542;&#21019;&#24314;&#21644;&#20351;&#29992;&#31616;&#21333;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#21457;&#29616;LDM&#20869;&#37096;&#28608;&#27963;&#25552;&#20379;&#20102;&#20851;&#20110;3D&#28145;&#24230;&#25968;&#25454;&#21644;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#20998;&#31163;&#30340;&#32447;&#24615;&#34920;&#31034;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#20855;&#26377;&#22240;&#26524;&#20316;&#29992;, &#21487;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#23637;&#29616;&#20102;&#20135;&#29983;&#36924;&#30495;&#22270;&#20687;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#22312;&#26426;&#21046;&#20173;&#28982;&#31070;&#31192;&#12290;&#21363;&#20351;&#22312;&#27809;&#26377;&#26174;&#24335;&#28145;&#24230;&#20449;&#24687;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20204;&#36890;&#24120;&#20063;&#20250;&#36755;&#20986;&#19968;&#33268;&#30340;3D&#22330;&#26223;&#22270;&#29255;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#22522;&#26412;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65306;LDM&#26159;&#21542;&#21019;&#24314;&#24182;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65311;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#65292;&#25105;&#20204;&#21457;&#29616;LDM&#30340;&#20869;&#37096;&#28608;&#27963;&#32534;&#30721;&#20102;&#32447;&#24615;&#34920;&#31034;&#65292;&#26082;&#21253;&#25324;3D&#28145;&#24230;&#25968;&#25454;&#65292;&#21448;&#21253;&#25324;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#21306;&#21035;&#12290;&#36825;&#20123;&#34920;&#31034;&#20284;&#20046;&#22312;&#21435;&#22122;&#36807;&#31243;&#30340;&#26089;&#26399;&#23601;&#20986;&#29616;&#20102;&#8212;&#8212;&#22312;&#20154;&#31867;&#33021;&#36731;&#26131;&#29702;&#35299;&#22024;&#26434;&#30340;&#22270;&#20687;&#20043;&#21069;&#12290;&#24178;&#39044;&#24615;&#23454;&#39564;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#36825;&#20123;&#34920;&#31034;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#25198;&#28436;&#22240;&#26524;&#20316;&#29992;&#65292;&#24182;&#19988;&#21487;&#33021;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent diffusion models (LDMs) exhibit an impressive ability to produce realistic images, yet the inner workings of these models remain mysterious. Even when trained purely on images without explicit depth information, they typically output coherent pictures of 3D scenes. In this work, we investigate a basic interpretability question: does an LDM create and use an internal representation of simple scene geometry? Using linear probes, we find evidence that the internal activations of the LDM encode linear representations of both 3D depth data and a salient-object / background distinction. These representations appear surprisingly early in the denoising process$-$well before a human can easily make sense of the noisy images. Intervention experiments further indicate these representations play a causal role in image synthesis, and may be used for simple high-level editing of an LDM's output.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05716</link><description>&lt;p&gt;
&#20026;&#25235;&#20303;&#20219;&#20309;&#29289;&#21697;&#38138;&#24179;&#36947;&#36335;&#65306;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#36890;&#29992;&#25235;&#21462;&#25918;&#32622;&#26426;&#22120;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots. (arXiv:2306.05716v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#19968;&#30452;&#26159;&#30740;&#31350;&#31038;&#21306;&#38271;&#26399;&#36861;&#27714;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25910;&#38598;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25968;&#25454;&#65292;&#22914; RT-1 &#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#26032;&#23545;&#35937;&#21644;&#22810;&#26679;&#32972;&#26223;&#30340;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#20197;&#35299;&#20915;&#26085;&#24120;&#22330;&#26223;&#20013;&#24191;&#27867;&#30340;&#25342;&#25918;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#25513;&#27169;&#20256;&#36798;&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#20960;&#20309;&#24418;&#29366;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#31574;&#30053;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24863;&#30693;&#20934;&#30830;&#30340;&#29289;&#20307;&#23039;&#24577;&#24182;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#26377;&#25928;&#30340;&#26032;&#23545;&#35937;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#21487;&#20197;&#23454;&#29616;&#22312;&#35757;&#32451;&#26102;&#35266;&#23519;&#21040;&#30456;&#20284;&#24418;&#29366;&#30340;&#26032;&#29289;&#20307;&#30340;&#25235;&#21462;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the generalization capabilities of general-purpose robotic agents has long been a significant challenge actively pursued by research communities. Existing approaches often rely on collecting large-scale real-world robotic data, such as the RT-1 dataset. However, these approaches typically suffer from low efficiency, limiting their capability in open-domain scenarios with new objects, and diverse backgrounds. In this paper, we propose a novel paradigm that effectively leverages language-grounded segmentation masks generated by state-of-the-art foundation models, to address a wide range of pick-and-place robot manipulation tasks in everyday scenarios. By integrating precise semantics and geometries conveyed from masks into our multi-view policy model, our approach can perceive accurate object poses and enable sample-efficient learning. Besides, such design facilitates effective generalization for grasping new objects with similar shapes observed during training. Our approach co
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23398;&#29983;&#31243;&#24207;&#21592;&#27714;&#21161;&#35831;&#27714;&#30340;&#22238;&#24212;&#33021;&#21147;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#23398;&#29983;&#38382;&#39064;&#20195;&#30721;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;GPT-3.5&#22312;&#22823;&#22810;&#25968;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;Codex&#65292;&#20004;&#20010;LLM&#32463;&#24120;&#22312;&#27599;&#20010;&#23398;&#29983;&#31243;&#24207;&#20013;&#25214;&#21040;&#33267;&#23569;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#65292;&#20294;&#37117;&#26410;&#33021;&#25214;&#21040;&#20840;&#37096;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05715</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21021;&#23398;&#32773;&#31243;&#24207;&#21592;&#27714;&#21161;&#35831;&#27714;&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests. (arXiv:2306.05715v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05715
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23398;&#29983;&#31243;&#24207;&#21592;&#27714;&#21161;&#35831;&#27714;&#30340;&#22238;&#24212;&#33021;&#21147;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#23398;&#29983;&#38382;&#39064;&#20195;&#30721;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;GPT-3.5&#22312;&#22823;&#22810;&#25968;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;Codex&#65292;&#20004;&#20010;LLM&#32463;&#24120;&#22312;&#27599;&#20010;&#23398;&#29983;&#31243;&#24207;&#20013;&#25214;&#21040;&#33267;&#23569;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#65292;&#20294;&#37117;&#26410;&#33021;&#25214;&#21040;&#20840;&#37096;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#21644;&#32972;&#26223;&#65306;&#22312;&#36807;&#21435;&#30340;&#19968;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24109;&#21367;&#20840;&#29699;&#12290;&#22312;&#35745;&#31639;&#26426;&#25945;&#32946;&#20013;&#65292;&#23601;&#20687;&#22312;&#29983;&#27963;&#30340;&#20854;&#20182;&#26041;&#38754;&#19968;&#26679;&#65292;&#35768;&#22810;&#26426;&#36935;&#21644;&#23041;&#32961;&#20986;&#29616;&#20102;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#26426;&#36935;&#21644;&#23041;&#32961;&#65306;&#22238;&#24212;&#23398;&#29983;&#31243;&#24207;&#21592;&#30340;&#27714;&#21161;&#35831;&#27714;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLM&#22312;&#35782;&#21035;&#23398;&#29983;&#35831;&#27714;&#24110;&#21161;&#30340;&#38382;&#39064;&#20195;&#30721;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20174;&#22312;&#32447;&#32534;&#31243;&#35838;&#31243;&#20013;&#25910;&#38598;&#20102;&#27714;&#21161;&#35831;&#27714;&#21644;&#20195;&#30721;&#26679;&#26412;&#12290;&#28982;&#21518;&#20419;&#20351;&#20004;&#20010;&#19981;&#21516;&#30340;LLM&#65288;OpenAI Codex&#21644;GPT-3.5&#65289;&#35782;&#21035;&#21644;&#35299;&#37322;&#23398;&#29983;&#20195;&#30721;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#20102;LLM&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#21457;&#29616;&#65306;GPT-3.5&#22312;&#22823;&#22810;&#25968;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;Codex&#12290;&#20004;&#20010;LLM&#32463;&#24120;&#22312;&#27599;&#20010;&#23398;&#29983;&#31243;&#24207;&#20013;&#25214;&#21040;&#33267;&#23569;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#65288;GPT-3.5&#22312;90&#65285;&#30340;&#24773;&#20917;&#19979;&#65289; &#12290;
&lt;/p&gt;
&lt;p&gt;
Background and Context: Over the past year, large language models (LLMs) have taken the world by storm. In computing education, like in other walks of life, many opportunities and threats have emerged as a consequence.  Objectives: In this article, we explore such opportunities and threats in a specific area: responding to student programmers' help requests. More specifically, we assess how good LLMs are at identifying issues in problematic code that students request help on.  Method: We collected a sample of help requests and code from an online programming course. We then prompted two different LLMs (OpenAI Codex and GPT-3.5) to identify and explain the issues in the students' code and assessed the LLM-generated answers both quantitatively and qualitatively.  Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently find at least one actual issue in each student program (GPT-3.5 in 90% of the cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05685</link><description>&lt;p&gt;
&#29992;MT-Bench&#21644;Chatbot Arena&#35780;&#20272;&#20197;LLM&#20026;&#22522;&#30784;&#30340;&#32842;&#22825;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05685
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32842;&#22825;&#21161;&#25163;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#32780;&#29616;&#26377;&#30340;&#22522;&#20934;&#26080;&#27861;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#22312;&#26356;&#21152;&#24320;&#25918;&#30340;&#38382;&#39064;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#30340;&#20351;&#29992;&#21644;&#23616;&#38480;&#24615;&#65292;&#22914;&#20301;&#32622;&#21644;&#20887;&#20313;&#20559;&#35265;&#20197;&#21450;&#26377;&#38480;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#26469;&#36801;&#31227;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#65288;&#19968;&#20010;&#22810;&#36718;&#38382;&#31572;&#38598;&#21644;&#19968;&#20010;&#20247;&#21253;&#31454;&#25216;&#24179;&#21488;&#65289;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#24378;&#22823;LLM&#35780;&#21028;&#21592;&#21487;&#20197;&#24456;&#22909;&#22320;&#21305;&#37197;&#21463;&#25511;&#21644;&#20247;&#21253;&#20154;&#31867;&#20559;&#22909;&#65292;&#36798;&#21040;&#20102;80&#65285;&#20197;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#19982;&#20154;&#31867;&#19968;&#33268;&#24615;&#27700;&#24179;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#36924;&#36817;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#24335;&#65292;&#32780;&#36825;&#20123;&#20559;&#22909;&#26159;&#38750;&#24120;&#26114;&#36149;&#33719;&#21462;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#20840;&#23616;&#20196;&#29260;&#20849;&#20139;&#30340;&#36731;&#37327;&#32423;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;&#23884;&#20837;&#24335;&#35774;&#22791;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.05682</link><description>&lt;p&gt;
&#36890;&#36807;&#20196;&#29260;&#20849;&#20139;Transformer&#23454;&#29616;&#36731;&#37327;&#32423;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Lightweight Monocular Depth Estimation via Token-Sharing Transformer. (arXiv:2306.05682v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#20840;&#23616;&#20196;&#29260;&#20849;&#20139;&#30340;&#36731;&#37327;&#32423;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;&#23884;&#20837;&#24335;&#35774;&#22791;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20272;&#35745;&#26159;&#21508;&#31181;&#26426;&#22120;&#20154;&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#26159;&#19968;&#31181;&#29702;&#24819;&#30340;&#36873;&#25321;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#26041;&#27861;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Token-Sharing Transformer&#20307;&#31995;&#32467;&#26500;&#26469;&#20248;&#21270;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#20013;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth estimation is an important task in various robotics systems and applications. In mobile robotics systems, monocular depth estimation is desirable since a single RGB camera can be deployable at a low cost and compact size. Due to its significant and growing needs, many lightweight monocular depth estimation networks have been proposed for mobile robotics systems. While most lightweight monocular depth estimation methods have been developed using convolution neural networks, the Transformer has been gradually utilized in monocular depth estimation recently. However, massive parameters and large computational costs in the Transformer disturb the deployment to embedded devices. In this paper, we present a Token-Sharing Transformer (TST), an architecture using the Transformer for monocular depth estimation, optimized especially in embedded devices. The proposed TST utilizes global token sharing, which enables the model to obtain an accurate depth prediction with high throughput in emb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25351;&#23548;&#25351;&#23450;&#30340; 3D &#20869;&#23481;&#30340;&#21464;&#21270;&#65292;&#24182;&#33021;&#22815;&#25509;&#21463; RGB &#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#25913;&#21464;&#31070;&#32463;&#22330;&#26223;&#20013;&#30340; 3D &#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2306.05668</link><description>&lt;p&gt;
RePaint-NeRF&#65306;&#22522;&#20110;&#35821;&#20041;&#25513;&#30721;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340; NeRF &#32534;&#36753;.
&lt;/p&gt;
&lt;p&gt;
RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models. (arXiv:2306.05668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25351;&#23548;&#25351;&#23450;&#30340; 3D &#20869;&#23481;&#30340;&#21464;&#21270;&#65292;&#24182;&#33021;&#22815;&#25509;&#21463; RGB &#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#25913;&#21464;&#31070;&#32463;&#22330;&#26223;&#20013;&#30340; 3D &#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;&#20986;&#29616;&#20419;&#36827;&#20102;&#23545;&#22797;&#26434;&#30495;&#23454;&#19990;&#30028;&#30340;&#39640;&#20445;&#30495;&#35270;&#22270;&#30340;&#21512;&#25104;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312; NeRF &#20013;&#37325;&#26032;&#32472;&#21046;&#20869;&#23481;&#20173;&#28982;&#26159;&#19968;&#39033;&#38750;&#24120;&#33499;&#21051;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25509;&#21463; RGB &#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#25913;&#21464;&#31070;&#32463;&#22330;&#26223;&#20013;&#30340; 3D &#20869;&#23481;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21033;&#29992;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25351;&#23548;&#25351;&#23450;&#30340; 3D &#20869;&#23481;&#30340;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35821;&#20041;&#22320;&#36873;&#25321;&#30446;&#26631;&#23545;&#35937;&#65292;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#23558;&#25351;&#23548; NeRF &#27169;&#22411;&#29983;&#25104;&#26032;&#30340; 3D &#23545;&#35937;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640; NeRF &#30340;&#21487;&#32534;&#36753;&#24615;&#65292;&#22810;&#26679;&#24615;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#20110;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#19979;&#32534;&#36753; NeRF &#20013;&#30340; 3D &#23545;&#35937;&#26159;&#26377;&#25928;&#30340;&#65292;&#21253;&#25324;&#32534;&#36753;&#22806;&#35266;&#12289;&#24418;&#29366;&#31561;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#23436;&#25104;&#36825;&#20123;&#32534;&#36753;&#20219;&#21153;&#12290;&#35831;&#35775;&#38382; https://repaintnerf.github.io &#20197;&#26597;&#30475;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of Neural Radiance Fields (NeRF) has promoted the development of synthesized high-fidelity views of the intricate real world. However, it is still a very demanding task to repaint the content in NeRF. In this paper, we propose a novel framework that can take RGB images as input and alter the 3D content in neural scenes. Our work leverages existing diffusion models to guide changes in the designated 3D content. Specifically, we semantically select the target object and a pre-trained diffusion model will guide the NeRF model to generate new 3D objects, which can improve the editability, diversity, and application range of NeRF. Experiment results show that our algorithm is effective for editing 3D objects in NeRF under different text prompts, including editing appearance, shape, and more. We validate our method on both real-world datasets and synthetic-world datasets for these editing tasks. Please visit https://repaintnerf.github.io for a better view of our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05659</link><description>&lt;p&gt;
COVER&#65306;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#20687;&#23569;&#37327;&#26679;&#26412;&#22330;&#26223;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;PLMs&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#22312;&#22522;&#20110;&#27169;&#26495;&#30340;&#25552;&#31034;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#22312;&#30340;&#28431;&#27934;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#24341;&#36215;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#25552;&#20986;&#22522;&#20110;&#25552;&#31034;&#30340;&#23545;&#25239;&#25915;&#20987;&#25163;&#27573;&#65292;&#25581;&#31034;&#20102;PLMs&#30340;&#19968;&#20123;&#28431;&#27934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23383;&#31526;&#32423;&#21035;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#30772;&#22351;&#25163;&#21160;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#19978;&#36848;&#21551;&#21457;&#24335;&#30772;&#22351;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;BERT&#31995;&#21015;&#27169;&#22411;&#30340;&#19977;&#20010;&#21464;&#31181;&#21644;&#20843;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20219;&#21153;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#27861;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#35780;&#20272;&#30340;&#25216;&#26415;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#29992;&#20363;&#23588;&#20854;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.05652</link><description>&lt;p&gt;
&#22312;&#32447;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#35780;&#20272;&#30340;&#38544;&#31169;&#24863;&#30693;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Privacy Aware Question-Answering System for Online Mental Health Risk Assessment. (arXiv:2306.05652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#27861;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#35780;&#20272;&#30340;&#25216;&#26415;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#29992;&#20363;&#23588;&#20854;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20351;&#24739;&#26377;&#31934;&#31070;&#30142;&#30149;&#30340;&#20154;&#20998;&#20139;&#29983;&#27963;&#32463;&#21382;&#24182;&#25214;&#21040;&#22312;&#32447;&#25903;&#25345;&#20197;&#24212;&#23545;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29992;&#25143;&#26410;&#33021;&#33719;&#24471;&#30495;&#27491;&#30340;&#20020;&#24202;&#25903;&#25345;&#65292;&#20174;&#32780;&#21152;&#21095;&#20102;&#20182;&#20204;&#30340;&#30151;&#29366;&#12290;&#22522;&#20110;&#29992;&#25143;&#22312;&#32593;&#19978;&#21457;&#24067;&#30340;&#20869;&#23481;&#36827;&#34892;&#31579;&#36873;&#21487;&#20197;&#24110;&#21161;&#25552;&#20379;&#32773;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#21307;&#30103;&#20445;&#20581;&#65292;&#24182;&#23613;&#37327;&#20943;&#23569;&#34394;&#20551;&#38451;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#35780;&#20272;&#29992;&#25143;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20854;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#19968;&#38382;&#31572;&#65288;Unified-QA&#65289;&#27169;&#22411;&#30340;&#38382;&#31572;&#26041;&#27861;&#26469;&#35780;&#20272;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#65292;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;Unified-QA&#65292;&#24182;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#21311;&#21517;&#21270;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#39118;&#38505;&#35780;&#20272;&#24314;&#27169;&#20026;&#38382;&#31572;&#20219;&#21153;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#29992;&#20363;&#29305;&#21035;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#21253;&#21547;&#24046;&#20998;&#38544;&#31169;&#21518;&#65292;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#19981;&#21040;1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms have enabled individuals suffering from mental illnesses to share their lived experiences and find the online support necessary to cope. However, many users fail to receive genuine clinical support, thus exacerbating their symptoms. Screening users based on what they post online can aid providers in administering targeted healthcare and minimize false positives. Pre-trained Language Models (LMs) can assess users' social media data and classify them in terms of their mental health risk. We propose a Question-Answering (QA) approach to assess mental health risk using the Unified-QA model on two large mental health datasets. To protect user data, we extend Unified-QA by anonymizing the model training process using differential privacy. Our results demonstrate the effectiveness of modeling risk assessment as a QA task, specifically for mental health use cases. Furthermore, the model's performance decreases by less than 1% with the inclusion of differential privacy. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32531;&#35299;&#38544;&#31169;-&#20248;&#21270;&#21046;&#32422;&#20851;&#31995;&#30340;&#38160;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05651</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38160;&#21270;&#24863;&#30693;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Sharpness-Aware Training. (arXiv:2306.05651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32531;&#35299;&#38544;&#31169;-&#20248;&#21270;&#21046;&#32422;&#20851;&#31995;&#30340;&#38160;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31169;&#26377;&#23398;&#20064;&#30340;&#20960;&#20309;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38160;&#21270;&#24863;&#30693;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#38544;&#31169;-&#20248;&#21270;&#30340;&#21046;&#32422;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep learning models with differential privacy (DP) results in a degradation of performance. The training dynamics of models with DP show a significant difference from standard training, whereas understanding the geometric properties of private learning remains largely unexplored. In this paper, we investigate sharpness, a key factor in achieving better generalization, in private learning. We show that flat minima can help reduce the negative effects of per-example gradient clipping and the addition of Gaussian noise. We then verify the effectiveness of Sharpness-Aware Minimization (SAM) for seeking flat minima in private learning. However, we also discover that SAM is detrimental to the privacy budget and computational time due to its two-step optimization. Thus, we propose a new sharpness-aware training method that mitigates the privacy-optimization trade-off. Our experimental results demonstrate that the proposed method improves the performance of deep learning models with 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#20197;&#29992;&#20110;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#36731;&#37327;&#32423;&#26597;&#35810;Transformer&#36830;&#25509;&#20004;&#20010;FMs&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05642</link><description>&lt;p&gt;
&#38754;&#21521;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#33258;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
Customizing General-Purpose Foundation Models for Medical Report Generation. (arXiv:2306.05642v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#20197;&#29992;&#20110;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#36731;&#37327;&#32423;&#26597;&#35810;Transformer&#36830;&#25509;&#20004;&#20010;FMs&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#23383;&#24149;&#39044;&#27979;&#65292;&#20063;&#34987;&#35270;&#20026;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#65288;MRG&#65289;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20026;&#32473;&#23450;&#30340;&#21307;&#30103;&#22270;&#20687;&#33258;&#21160;&#29983;&#25104;&#36830;&#36143;&#20934;&#30830;&#30340;&#23383;&#24149;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#30340;&#21307;&#30103;&#22270;&#20687;-&#25253;&#21578;&#23545;&#30340;&#31232;&#32570;&#24615;&#22312;&#28145;&#24230;&#21644;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#21457;&#20013;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36825;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#36890;&#29992;&#30340;&#38754;&#21521;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#23450;&#21046;&#65292;&#29305;&#21035;&#20851;&#27880;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26681;&#25454;BLIP-2&#25552;&#20986;&#20102;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;MRG&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#36731;&#37327;&#32423;&#26597;&#35810;Transformer&#36830;&#25509;&#20004;&#20010;FMs&#65306;&#24040;&#22411;&#35270;&#35273;Transformer EVA-ViT-g&#21644;&#21452;&#35821;LLM&#65292;&#35813;LLM&#34987;&#35757;&#32451;&#29992;&#20110;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#65288;&#31216;&#20026;T5-base-CN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;&#20102;&#23558;&#22522;&#30784;&#27169;&#22411;&#36866;&#24212;&#20110;&#27492;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical caption prediction which can be regarded as a task of medical report generation (MRG), requires the automatic generation of coherent and accurate captions for the given medical images. However, the scarcity of labelled medical image-report pairs presents great challenges in the development of deep and large-scale neural networks capable of harnessing the potential artificial general intelligence power like large language models (LLMs). In this work, we propose customizing off-the-shelf general-purpose large-scale pre-trained models, i.e., foundation models (FMs), in computer vision and natural language processing with a specific focus on medical report generation. Specifically, following BLIP-2, a state-of-the-art vision-language pre-training approach, we introduce our encoder-decoder-based MRG model. This model utilizes a lightweight query Transformer to connect two FMs: the giant vision Transformer EVA-ViT-g and a bilingual LLM trained to align with human intentions (referred
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#21512;&#24182;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26356;&#20026;&#26174;&#33879;&#65292;&#22240;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#21512;&#24182;&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#21387;&#32553;&#21019;&#24314;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#21407;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#21697;&#12290;</title><link>http://arxiv.org/abs/2306.05641</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#32622;&#25442;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#38388;&#27169;&#22411;&#21512;&#24182;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Permutation Symmetry for Merging Models between Different Datasets. (arXiv:2306.05641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#21512;&#24182;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26356;&#20026;&#26174;&#33879;&#65292;&#22240;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#21512;&#24182;&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#21387;&#32553;&#21019;&#24314;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#21407;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21512;&#24182;&#26159;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#21019;&#24314;&#26032;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#22411;&#21512;&#24182;&#23545;&#20110;&#19981;&#21516;&#38543;&#26426;&#25968;&#35757;&#32451;&#27169;&#22411;&#30340;&#21333;&#19968;&#25968;&#25454;&#38598;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26159;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#21512;&#24182;&#21364;&#24456;&#22256;&#38590;&#12290;&#23558;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#21512;&#24182;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#20294;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#38388;&#21512;&#24182;&#27169;&#22411;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;&#25968;&#25454;&#38598;&#30340;&#24046;&#24322;&#36234;&#22823;&#65292;&#21512;&#24182;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#24471;&#26356;&#20026;&#26174;&#33879;&#65292;&#32780;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#27169;&#22411;&#21512;&#24182;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#21512;&#24182;&#30340;&#27169;&#22411;&#38656;&#35201;&#25968;&#25454;&#38598;&#25165;&#33021;&#23454;&#29616;&#39640;&#31934;&#24230;&#21512;&#24182;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#24403;&#21512;&#24182;&#27169;&#22411;&#26102;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#21387;&#32553;&#21019;&#24314;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#21407;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model merging is a new approach to creating a new model by combining the weights of different trained models. Previous studies report that model merging works well for models trained on a single dataset with different random seeds, while model merging between different datasets is difficult. Merging knowledge from different datasets has practical significance, but it has not been well investigated. In this paper, we investigate the properties of merging models between different datasets. Through theoretical and empirical analyses, we find that the accuracy of the merged model decreases more significantly as the datasets diverge more and that the different loss landscapes for each dataset make model merging between different datasets difficult. We also show that merged models require datasets for merging in order to achieve a high accuracy. Furthermore, we show that condensed datasets created by dataset condensation can be used as substitutes for the original datasets when merging model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#26041;&#27861;&#26469;&#32479;&#19968;&#22788;&#29702;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#20135;&#21697;&#23646;&#24615;&#20540;&#35782;&#21035;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05605</link><description>&lt;p&gt;
&#20135;&#21697;&#23646;&#24615;&#20540;&#35782;&#21035;&#30340;&#32479;&#19968;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Generative Approach to Product Attribute-Value Identification. (arXiv:2306.05605v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#26041;&#27861;&#26469;&#32479;&#19968;&#22788;&#29702;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#20135;&#21697;&#23646;&#24615;&#20540;&#35782;&#21035;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#23646;&#24615;&#20540;&#35782;&#21035;&#65288;PAVI&#65289;&#26088;&#22312;&#20351;&#29992;&#20135;&#21697;&#25991;&#26412;&#20316;&#20026;&#32447;&#32034;&#23558;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#20135;&#21697;&#19982;&#20854;&#23646;&#24615;&#20540;&#65288;&#20363;&#22914;&lt;Material&#65292;Cotton&gt;&#65289;&#38142;&#25509;&#36215;&#26469;&#12290;&#29616;&#23454;&#19990;&#30028;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#30340;&#25216;&#26415;&#38656;&#27714;&#35201;&#27714;PAVI&#26041;&#27861;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#20540;&#12289;&#22810;&#23646;&#24615;&#20540;&#21644;&#35268;&#33539;&#21270;&#30340;&#20540;&#65292;&#32780;&#36825;&#20123;&#20165;&#22312;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#20013;&#37096;&#20998;&#24471;&#21040;&#35299;&#20915;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#26041;&#27861;&#26469;&#22788;&#29702;PAVI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Product attribute-value identification (PAVI) has been studied to link products on e-commerce sites with their attribute values (e.g., &lt;Material, Cotton&gt;) using product text as clues. Technical demands from real-world e-commerce platforms require PAVI methods to handle unseen values, multi-attribute values, and canonicalized values, which are only partly addressed in existing extraction- and classification-based approaches. Motivated by this, we explore a generative approach to the PAVI task. We finetune a pre-trained generative model, T5, to decode a set of attribute-value pairs as a target sequence from the given product text. Since the attribute value pairs are unordered set elements, how to linearize them will matter; we, thus, explore methods of composing an attribute-value pair and ordering the pairs for the task. Experimental results confirm that our generation-based approach outperforms the existing extraction and classification-based methods on large-scale real-world datasets 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#39046;&#22495;&#32422;&#26463;&#19979;&#65292;&#35774;&#35745;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#38598;&#32676;&#30340;&#32852;&#30431;&#24418;&#25104;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#31639;&#27861;&#30340;&#28151;&#21512;&#25913;&#36827;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#20219;&#21153;&#20998;&#37197;&#25928;&#29575;&#65292;&#24182;&#26377;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.05590</link><description>&lt;p&gt;
&#26377;&#39046;&#22495;&#32422;&#26463;&#30340;&#32852;&#21512;&#32534;&#38431;&#22312;&#26426;&#22120;&#20154;&#38598;&#32676;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
The Viability of Domain Constrained Coalition Formation for Robotic Collectives. (arXiv:2306.05590v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05590
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#39046;&#22495;&#32422;&#26463;&#19979;&#65292;&#35774;&#35745;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#38598;&#32676;&#30340;&#32852;&#30431;&#24418;&#25104;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#31639;&#27861;&#30340;&#28151;&#21512;&#25913;&#36827;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#20219;&#21153;&#20998;&#37197;&#25928;&#29575;&#65292;&#24182;&#26377;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20891;&#20107;&#21644;&#28798;&#38590;&#21709;&#24212;&#31561;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#20154;&#38598;&#32676;&#30340;&#21327;&#21516;&#33021;&#21147;&#21487;&#20197;&#22312;&#22823;&#33539;&#22260;&#20869;&#39640;&#25928;&#25191;&#34892;&#22810;&#31181;&#21512;&#20316;&#20219;&#21153;&#65288;&#22914;&#30417;&#35270;&#12289;&#25439;&#20260;&#35780;&#20272;&#65289;&#12290;&#28982;&#32780;&#65292;&#32852;&#30431;&#24418;&#25104;&#31639;&#27861;&#30340;&#22823;&#22810;&#25968;&#35774;&#35745;&#37117;&#26159;&#38024;&#23545;&#23567;&#22411;&#26426;&#22120;&#20154;&#31995;&#32479;&#65288;&#21363;2-50&#20010;&#26426;&#22120;&#20154;&#65289;&#65292;&#32780;&#22823;&#35268;&#27169;&#38598;&#32676;&#21644;&#39046;&#22495;&#30456;&#20851;&#30340;&#32422;&#26463;&#26465;&#20214;&#65288;&#22914;&#39640;&#25928;&#20998;&#37197;&#12289;&#23454;&#26102;&#24615;&#12289;&#26368;&#23567;&#36890;&#20449;&#65289;&#20351;&#24471;&#32852;&#30431;&#24418;&#25104;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#25293;&#21334;&#21644;&#24841;&#24742;&#21338;&#24328;&#21487;&#33021;&#26159;&#26368;&#26377;&#21487;&#36716;&#31227;&#24615;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25913;&#36827;&#25293;&#21334;&#31639;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#38598;&#32676;&#21327;&#20316;&#25628;&#32034;&#21644;&#25910;&#38598;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;&#20998;&#24067;&#24335;&#25191;&#34892;&#30340;&#25293;&#21334;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#20998;&#37197;&#25928;&#29575;&#65292;&#26377;&#39046;&#22495;&#32422;&#26463;&#30340;&#32852;&#21512;&#32534;&#38431;&#31639;&#27861;&#26174;&#31034;&#20986;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#38598;&#32676;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications, such as military and disaster response, can benefit from robotic collectives' ability to perform multiple cooperative tasks (e.g., surveillance, damage assessments) efficiently across a large spatial area. Coalition formation algorithms can potentially facilitate collective robots' assignment to appropriate task teams; however, most coalition formation algorithms were designed for smaller multiple robot systems (i.e., 2-50 robots). Collectives' scale and domain-relevant constraints (i.e., distribution, near real-time, minimal communication) make coalition formation more challenging. This manuscript identifies the challenges inherent to designing coalition formation algorithms for very large collectives (e.g., 1000 robots). A survey of multiple robot coalition formation algorithms finds that most are unable to transfer directly to collectives, due to the identified system differences; however, auctions and hedonic games may be the most transferable. A simulation-based eval
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SE&#65288;3&#65289;&#31561;&#21464;&#32467;&#26500;&#21644;&#38750;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21018;&#24615;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#65292;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#27169;&#22411;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05584</link><description>&lt;p&gt;
&#22810;&#20307;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#21018;&#20307;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation. (arXiv:2306.05584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SE&#65288;3&#65289;&#31561;&#21464;&#32467;&#26500;&#21644;&#38750;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21018;&#24615;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#65292;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#27169;&#22411;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#21018;&#20307;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#30340;&#30495;&#27491;&#36890;&#29992;&#26041;&#27861;&#23545;&#20110;&#29702;&#35299;&#20851;&#33410;&#29289;&#20307;&#21644;&#31227;&#21160;&#22330;&#26223;&#30340;&#19977;&#32500;&#24433;&#20687;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#20998;&#21106;&#21644;&#36816;&#21160;&#20272;&#35745;&#20043;&#38388;&#23494;&#20999;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SE&#65288;3&#65289;&#31561;&#21464;&#20307;&#31995;&#32467;&#26500;&#21644;&#22521;&#35757;&#31574;&#30053;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#21253;&#25324;&#20004;&#20010;&#36731;&#37327;&#32423;&#21644;&#30456;&#20114;&#36830;&#25509;&#30340;&#22836;&#37096;&#65292;&#20351;&#29992;&#28857;&#32423;&#19981;&#21464;&#29305;&#24449;&#21644;&#26469;&#33258;SE&#65288;3&#65289;&#31561;&#21464;&#29305;&#24449;&#30340;&#36816;&#21160;&#20272;&#35745;&#26469;&#39044;&#27979;&#20998;&#21106;&#25513;&#27169;&#65292;&#32780;&#19981;&#38656;&#35201;&#31867;&#21035;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32479;&#19968;&#22521;&#35757;&#31574;&#30053;&#21487;&#20197;&#22312;&#32447;&#25191;&#34892;&#65292;&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#27969;&#65292;&#20998;&#21106;&#25513;&#27169;&#21644;&#21018;&#24615;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#26469;&#21516;&#26102;&#20248;&#21270;&#20004;&#20010;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21482;&#26377;0.25M&#21442;&#25968;&#21644;0.92G FLOPs&#12290;
&lt;/p&gt;
&lt;p&gt;
A truly generalizable approach to rigid segmentation and motion estimation is fundamental to 3D understanding of articulated objects and moving scenes. In view of the tightly coupled relationship between segmentation and motion estimates, we present an SE(3) equivariant architecture and a training strategy to tackle this task in an unsupervised manner. Our architecture comprises two lightweight and inter-connected heads that predict segmentation masks using point-level invariant features and motion estimates from SE(3) equivariant features without the prerequisites of category information. Our unified training strategy can be performed online while jointly optimizing the two predictions by exploiting the interrelations among scene flow, segmentation mask, and rigid transformations. We show experiments on four datasets as evidence of the superiority of our method both in terms of model performance and computational efficiency with only 0.25M parameters and 0.92G FLOPs. To the best of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26041;&#27861; - &#8220;&#26032;&#29983;&#20799;&#36523;&#20307;&#22270;&#28789;&#27979;&#35797;&#8221;&#65292;&#33021;&#22815;&#22312;&#30456;&#21516;&#29615;&#22659;&#20013;&#27604;&#36739;&#26032;&#29983;&#21160;&#29289;&#21644;&#26426;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23454;&#39564;&#20013;&#30340;&#26426;&#22120;&#33021;&#22815;&#33258;&#21457;&#22320;&#21457;&#23637;&#35270;&#35273;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05582</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#35270;&#35282;&#19981;&#21464;&#29289;&#20307;&#35782;&#21035;&#30340;&#26032;&#29983;&#20799;&#36523;&#20307;&#22270;&#28789;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A newborn embodied Turing test for view-invariant object recognition. (arXiv:2306.05582v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26041;&#27861; - &#8220;&#26032;&#29983;&#20799;&#36523;&#20307;&#22270;&#28789;&#27979;&#35797;&#8221;&#65292;&#33021;&#22815;&#22312;&#30456;&#21516;&#29615;&#22659;&#20013;&#27604;&#36739;&#26032;&#29983;&#21160;&#29289;&#21644;&#26426;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23454;&#39564;&#20013;&#30340;&#26426;&#22120;&#33021;&#22815;&#33258;&#21457;&#22320;&#21457;&#23637;&#35270;&#35273;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#20204;&#37325;&#26032;&#20851;&#27880;&#36215;&#20687;&#21160;&#29289;&#19968;&#26679;&#23398;&#20064;&#30340;&#26426;&#22120;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#27604;&#29983;&#29289;&#21644;&#20154;&#24037;&#31995;&#32479;&#23398;&#20064;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#22823;&#37096;&#20998;&#27604;&#36739;&#30340;&#26159;&#21160;&#29289;&#21644;&#26426;&#22120;&#25509;&#25910;&#19981;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#31181;&#23545;&#27604;&#25513;&#30422;&#20102;&#21160;&#29289;&#21644;&#26426;&#22120;&#30340;&#24046;&#24322;&#26159;&#21542;&#26469;&#33258;&#20110;&#23398;&#20064;&#26426;&#21046;&#30340;&#24322;&#21516;&#25110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#39564;&#26041;&#27861;-&#8220;&#26032;&#29983;&#20799;&#36523;&#20307;&#22270;&#28789;&#27979;&#35797;&#8221;&#65292;&#20801;&#35768;&#26032;&#29983;&#21160;&#29289;&#21644;&#26426;&#22120;&#22312;&#30456;&#21516;&#30340;&#29615;&#22659;&#20013;&#25509;&#21463;&#30456;&#21516;&#30340;&#20219;&#21153;&#27979;&#35797;&#65292;&#30452;&#25509;&#27604;&#36739;&#23427;&#20204;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#20102;&#26500;&#24314;&#36825;&#20010;&#24179;&#21488;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#26032;&#29983;&#23567;&#40479;&#30340;&#21463;&#25511;&#39282;&#20859;&#25968;&#25454;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#8220;&#25968;&#23383;&#21452;&#32990;&#32974;&#8221;&#23454;&#39564;&#65292;&#35753;&#26426;&#22120;&#22312;&#27169;&#25311;&#23567;&#40479;&#39282;&#20859;&#26465;&#20214;&#19979;&#30340;&#34394;&#25311;&#29615;&#22659;&#20013;&#25104;&#38271;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#65288;1&#65289;&#26426;&#22120;&#65288;&#20855;&#26377;&#22266;&#26377;&#21160;&#26426;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65289;&#21487;&#20197;&#33258;&#21457;&#22320;&#21457;&#23637;&#35270;&#35273;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Recent progress in artificial intelligence has renewed interest in building machines that learn like animals. Almost all of the work comparing learning across biological and artificial systems comes from studies where animals and machines received different training data, obscuring whether differences between animals and machines emerged from differences in learning mechanisms versus training data. We present an experimental approach-a "newborn embodied Turing Test"-that allows newborn animals and machines to be raised in the same environments and tested with the same tasks, permitting direct comparison of their learning abilities. To make this platform, we first collected controlled-rearing data from newborn chicks, then performed "digital twin" experiments in which machines were raised in virtual environments that mimicked the rearing conditions of the chicks. We found that (1) machines (deep reinforcement learning agents with intrinsic motivation) can spontaneously develop visually 
&lt;/p&gt;</description></item><item><title>AircraftVerse&#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#39134;&#34892;&#22120;&#35774;&#35745;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;27,714&#20010;&#19981;&#21516;&#30340;&#35774;&#35745;&#65292;&#26159;&#20855;&#26377;&#27492;&#22797;&#26434;&#31243;&#24230;&#30340;&#24037;&#31243;&#35774;&#35745;&#26368;&#22823;&#30340;&#35821;&#26009;&#24211;&#12290;&#27599;&#20010;&#35774;&#35745;&#37117;&#28085;&#30422;&#20102;&#39134;&#34892;&#22120;&#35774;&#35745;&#39046;&#22495;&#30340;&#22810;&#20010;&#29289;&#29702;&#39046;&#22495;&#21644;&#22810;&#31181;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.05562</link><description>&lt;p&gt;
&#39134;&#34892;&#22120;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65306;AircraftVerse
&lt;/p&gt;
&lt;p&gt;
AircraftVerse: A Large-Scale Multimodal Dataset of Aerial Vehicle Designs. (arXiv:2306.05562v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05562
&lt;/p&gt;
&lt;p&gt;
AircraftVerse&#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#39134;&#34892;&#22120;&#35774;&#35745;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;27,714&#20010;&#19981;&#21516;&#30340;&#35774;&#35745;&#65292;&#26159;&#20855;&#26377;&#27492;&#22797;&#26434;&#31243;&#24230;&#30340;&#24037;&#31243;&#35774;&#35745;&#26368;&#22823;&#30340;&#35821;&#26009;&#24211;&#12290;&#27599;&#20010;&#35774;&#35745;&#37117;&#28085;&#30422;&#20102;&#39134;&#34892;&#22120;&#35774;&#35745;&#39046;&#22495;&#30340;&#22810;&#20010;&#29289;&#29702;&#39046;&#22495;&#21644;&#22810;&#31181;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#39134;&#34892;&#22120;&#35774;&#35745;&#25968;&#25454;&#38598;&#65292;&#21363;AircraftVerse&#12290;&#39134;&#34892;&#22120;&#35774;&#35745;&#28085;&#30422;&#19981;&#21516;&#30340;&#29289;&#29702;&#39046;&#22495;&#21644;&#22810;&#31181;&#34920;&#31034;&#24418;&#24335;&#12290;&#35780;&#20272;&#36825;&#20123;&#26234;&#33021;&#29289;&#29702;&#31995;&#32479;&#35774;&#35745;&#38656;&#35201;&#20351;&#29992;&#31185;&#23398;&#20998;&#26512;&#21644;&#20223;&#30495;&#27169;&#22411;&#65292;&#20174;&#32467;&#26500;&#21644;&#21046;&#36896;&#20998;&#26512;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#24037;&#20855;&#65292;&#21040;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#24037;&#20855;&#30340;&#38459;&#21147;&#21644;&#21319;&#21147;&#35745;&#31639;&#65292;&#20877;&#21040;&#33021;&#28304;&#20272;&#35745;&#30340;&#30005;&#27744;&#27169;&#22411;&#21644;&#39134;&#34892;&#25511;&#21046;&#19982;&#21160;&#21147;&#23398;&#30340;&#20223;&#30495;&#27169;&#22411;&#12290;AircraftVerse&#21253;&#21547;27,714&#31181;&#19981;&#21516;&#30340;&#39134;&#34892;&#22120;&#35774;&#35745;&#65292;&#26159;&#20855;&#26377;&#27492;&#22797;&#26434;&#31243;&#24230;&#30340;&#24037;&#31243;&#35774;&#35745;&#26368;&#22823;&#30340;&#35821;&#26009;&#24211;&#12290;&#27599;&#20010;&#35774;&#35745;&#21253;&#25324;&#20197;&#19979;&#26500;&#20214;&#65306;&#25551;&#36848;&#25299;&#25169;&#12289;&#25512;&#36827;&#23376;&#31995;&#32479;&#12289;&#30005;&#27744;&#23376;&#31995;&#32479;&#21644;&#20854;&#20182;&#35774;&#35745;&#32454;&#33410;&#30340;&#31526;&#21495;&#35774;&#35745;&#26641;&#65307;&#20197;STandard for the Exchange of Product (STEP) &#27169;&#22411;&#25968;&#25454;&#20026;&#22522;&#30784;&#30340; 3D CAD&#35774;&#35745;&#65307;&#20197;&#31435;&#20307;&#20809;&#21051;&#65288;STL&#65289;&#25991;&#20214;&#26684;&#24335;&#20026;&#22522;&#30784;&#30340;3D&#28857;&#20113;&#65292;&#29992;&#20110;&#25551;&#36848;&#35774;&#35745;&#30340;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present AircraftVerse, a publicly available aerial vehicle design dataset. Aircraft design encompasses different physics domains and, hence, multiple modalities of representation. The evaluation of these cyber-physical system (CPS) designs requires the use of scientific analytical and simulation models ranging from computer-aided design tools for structural and manufacturing analysis, computational fluid dynamics tools for drag and lift computation, battery models for energy estimation, and simulation models for flight control and dynamics. AircraftVerse contains 27,714 diverse air vehicle designs - the largest corpus of engineering designs with this level of complexity. Each design comprises the following artifacts: a symbolic design tree describing topology, propulsion subsystem, battery subsystem, and other design details; a STandard for the Exchange of Product (STEP) model data; a 3D CAD design using a stereolithography (STL) file format; a 3D point cloud for the shape of the de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05554</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#20013;&#30340;&#24212;&#29992;&#21644;&#39044;&#27979;&#65306;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Simulation and Prediction of Countercurrent Spontaneous Imbibition at Early and Late Times Using Physics-Informed Neural Networks. (arXiv:2306.05554v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#65288;COUCSI&#65289;&#26159;&#19968;&#31181;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#28070;&#28287;&#30456;&#21462;&#20195;&#20102;&#38750;&#28070;&#28287;&#30456;&#30340;&#20301;&#32622;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#35299;&#20915;&#26089;&#26399;&#65288;ET&#65289;&#21644;&#26202;&#26399;&#65288;LT&#65289;COUCSI&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#20197;&#25913;&#36827;PINNs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#33258;&#21464;&#37327;&#23558;COUCSI&#38382;&#39064;&#20998;&#21035;&#29992;XT-&#65292;XY-&#21644;Z-&#19977;&#31181;&#31561;&#25928;&#24418;&#24335;&#36827;&#34892;&#25551;&#36848;&#65306;&#31532;&#19968;&#20010;&#25551;&#36848;&#20102;&#39281;&#21644;&#24230;&#20316;&#20026;&#35268;&#33539;&#21270;&#20301;&#32622;X&#21644;&#26102;&#38388;T&#30340;&#20989;&#25968;;&#31532;&#20108;&#20010;&#25551;&#36848;&#20102;X&#21644;Y=T^0.5&#20316;&#20026;&#20989;&#25968;&#30340;&#39281;&#21644;&#24230;;&#31532;&#19977;&#20010;&#20316;&#20026;Z=X/T^0.5&#30340;&#21807;&#19968;&#20989;&#25968;&#65288;&#20165;&#22312;ET&#19979;&#26377;&#25928;&#65289;&#12290;&#35813;PINN&#27169;&#22411;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#65292;&#24182;&#22522;&#20110;&#26368;&#23567;&#21270;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#29289;&#29702;&#20449;&#24687;&#20002;&#22833;&#39033;&#21644;&#19982;&#21021;&#22987;&#36793;&#30028;&#26465;&#20214;&#30456;&#23545;&#24212;&#30340;&#39033;&#12290;&#27809;&#26377;&#21512;&#25104;&#25110;&#23454;&#39564;&#25968;&#25454;&#34987;&#35843;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Countercurrent spontaneous imbibition (COUCSI) is a process in porous materials in which a wetting phase displaces non-wetting phase. In this work, we investigate for the first time the application of Physics-Informed Neural Networks (PINNs) in solving the 1D COUCSI problem in both early (ET) and late (LT) times. Also novel, we examine the Change-of-Variables technique for improving the performance of PINNs. We formulated the COUCSI problem in three equivalent forms by changing the independent variables: XT-, XY-, and Z-formulations. The first describes saturation as function of normalized position X and time T; the second as function of X and Y=T^0.5; and the third as a sole function of Z=X/T^0.5 (valid only at ET). The PINN model was generated using a feed-forward neural network and trained based on minimizing a weighted loss function, including the physics-informed loss term and terms corresponding to the initial and boundary conditions. No synthetical or experimental data were invo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33521;&#35821;&#39044;&#35757;&#32451;&#30340;Masked Language Models&#65288;MLMs&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#23545;&#32654;&#22269;93&#20010;&#31038;&#20250;&#27745;&#21517;&#21270;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#20026;&#20102;&#35780;&#20272;93&#20010;&#27745;&#21517;&#21270;&#26465;&#20214;&#30340;&#20559;&#35265;&#23384;&#22312;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#25214;&#21040;&#20102;&#20559;&#35265;&#23384;&#22312;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05550</link><description>&lt;p&gt;
&#23545;&#24102;&#8220;&#26631;&#35760;&#35821;&#35328;&#27169;&#22411;&#8221;&#20013;93&#20010;&#21463;&#27495;&#35270;&#32676;&#20307;&#30340;&#20559;&#35265;&#21450;&#20854;&#23545;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks. (arXiv:2306.05550v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33521;&#35821;&#39044;&#35757;&#32451;&#30340;Masked Language Models&#65288;MLMs&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#23545;&#32654;&#22269;93&#20010;&#31038;&#20250;&#27745;&#21517;&#21270;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#20026;&#20102;&#35780;&#20272;93&#20010;&#27745;&#21517;&#21270;&#26465;&#20214;&#30340;&#20559;&#35265;&#23384;&#22312;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#25214;&#21040;&#20102;&#20559;&#35265;&#23384;&#22312;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24555;&#36895;&#37096;&#32626;&#38656;&#35201;&#23545;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#35265;&#21644;&#39118;&#38505;&#36827;&#34892;&#24443;&#24213;&#30340;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#31038;&#20250;&#27745;&#21517;&#21270;&#30340;&#20559;&#35265;&#36827;&#34892;&#30740;&#31350;&#65292;&#25193;&#23637;&#20102;&#24050;&#26377;&#24037;&#20316;&#23545;&#20559;&#35265;&#35780;&#20272;&#30340;&#28966;&#28857;&#12290;&#23427;&#20851;&#27880;&#32654;&#22269;93&#20010;&#31038;&#20250;&#27745;&#21517;&#21270;&#32676;&#20307;&#65292;&#21253;&#25324;&#19982;&#30142;&#30149;&#12289;&#27531;&#30142;&#12289;&#33647;&#29289;&#20351;&#29992;&#12289;&#24515;&#29702;&#30142;&#30149;&#12289;&#23447;&#25945;&#12289;&#24615;&#21462;&#21521;&#12289;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#20854;&#20182;&#30456;&#20851;&#22240;&#32032;&#26377;&#20851;&#30340;&#19968;&#31995;&#21015;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#33521;&#35821;&#39044;&#35757;&#32451;&#30340;Masked Language Models&#65288;MLMs&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#23545;&#36825;&#20123;&#32676;&#20307;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#35780;&#20272;93&#20010;&#27745;&#21517;&#21270;&#26465;&#20214;&#30340;&#20559;&#35265;&#23384;&#22312;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;29&#20010;&#38750;&#27745;&#21517;&#21270;&#26465;&#20214;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#12290;&#22522;&#20110;&#31038;&#20250;&#25490;&#26021;&#30340;&#24515;&#29702;&#23398;&#23610;&#24230;-&#31038;&#20250;&#36317;&#31163;&#37327;&#34920;&#65292;&#25105;&#20204;&#23545;&#20845;&#20010;MLMs&#36827;&#34892;&#20102;&#25552;&#31034;&#65306;RoBERTa-base&#12289;RoBERTa-large&#12289;XLNet-large&#12289;BERTweet-base&#12289;BERTweet-la&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid deployment of artificial intelligence (AI) models demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-la
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;DetectLLM-LRR&#21644;DetectLLM-NPR&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26085;&#24535;&#25490;&#21517;&#20449;&#24687;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#25928;&#26524;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05540</link><description>&lt;p&gt;
DetectLLM&#65306;&#22522;&#20110;&#23545;&#25968;&#31209;&#20449;&#24687;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text. (arXiv:2306.05540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05540
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;DetectLLM-LRR&#21644;DetectLLM-NPR&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26085;&#24535;&#25490;&#21517;&#20449;&#24687;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#25928;&#26524;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#21644;&#25152;&#29983;&#25104;&#30340;&#25991;&#26412;&#25968;&#37327;&#30340;&#24040;&#22823;&#22686;&#21152;&#65292;&#20154;&#24037;&#21306;&#20998;&#25991;&#26412;&#26159;&#21542;&#20026;&#26426;&#22120;&#29983;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#20999;&#23454;&#38469;&#12290;&#32771;&#34385;&#21040;LLM&#22312;&#31038;&#20132;&#23186;&#20307;&#21644;&#25945;&#32946;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#38450;&#27490;&#24694;&#24847;&#20351;&#29992;&#65292;&#22914;&#25220;&#34989;&#12289;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#31181;&#38646;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#23545;&#25968;&#31209;&#20449;&#24687;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#12290;&#20854;&#20013;&#19968;&#20010;&#31216;&#20026;DetectLLM-LRR&#65292;&#24555;&#36895;&#39640;&#25928;&#65292;&#21478;&#19968;&#20010;&#31216;&#20026;DetectLLM-NPR&#65292;&#26356;&#31934;&#30830;&#65292;&#20294;&#38656;&#35201;&#25200;&#21160;&#65292;&#22240;&#27492;&#36895;&#24230;&#26356;&#24930;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid progress of large language models (LLMs) and the huge amount of text they generated, it becomes more and more impractical to manually distinguish whether a text is machine-generated. Given the growing use of LLMs in social media and education, it prompts us to develop methods to detect machine-generated text, preventing malicious usage such as plagiarism, misinformation, and propaganda. Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the
&lt;/p&gt;</description></item><item><title>&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05535</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#26816;&#27979;&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;
&lt;/p&gt;
&lt;p&gt;
Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05535
&lt;/p&gt;
&lt;p&gt;
&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#30340;&#19968;&#22823;&#37096;&#20998;&#22242;&#32467;&#22312;&#30456;&#21516;&#30340;&#24895;&#26223;&#21644;&#24605;&#24819;&#21608;&#22260;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#33021;&#37327;&#12290;&#36825;&#27491;&#26159;&#25919;&#27835;&#20154;&#29289;&#24076;&#26395;&#20026;&#20182;&#20204;&#30340;&#20107;&#19994;&#25152;&#32047;&#31215;&#30340;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#20182;&#20204;&#26377;&#26102;&#20250;&#20351;&#29992;&#25197;&#26354;&#25110;&#38544;&#34255;&#30495;&#30456;&#30340;&#25163;&#27573;&#65292;&#26080;&#35770;&#26159;&#26080;&#24847;&#30340;&#36824;&#26159;&#26377;&#24847;&#30340;&#65292;&#36825;&#20026;&#38169;&#35823;&#20449;&#24687;&#21644;&#35823;&#23548;&#24320;&#20102;&#22823;&#38376;&#12290;&#33258;&#21160;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20855;&#23558;&#23545;&#36777;&#35770;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26377;&#24456;&#22823;&#24110;&#21161;&#12290;&#34429;&#28982;&#20197;&#21069;&#20851;&#20110;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#25991;&#26412;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38899;&#39057;&#20449;&#21495;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#28304;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#65292;&#21253;&#21547;48&#23567;&#26102;&#30340;&#28436;&#35762;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#28436;&#35762;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#38899;&#39057;&#27169;&#24577;&#19982;&#25991;&#26412;&#32467;&#21512;&#20351;&#29992;&#27604;&#20165;&#20351;&#29992;&#25991;&#26412;&#20855;&#26377;&#25913;&#36827;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#21333;&#22768;&#36947;&#38899;&#39057;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#21333;&#22768;&#36947;&#25991;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large portion of society united around the same vision and ideas carries enormous energy. That is precisely what political figures would like to accumulate for their cause. With this goal in mind, they can sometimes resort to distorting or hiding the truth, unintentionally or on purpose, which opens the door for misinformation and disinformation. Tools for automatic detection of check-worthy claims would be of great help to moderators of debates, journalists, and fact-checking organizations. While previous work on detecting check-worthy claims has focused on text, here we explore the utility of the audio signal as an additional information source. We create a new multimodal dataset (text and audio in English) containing 48 hours of speech. Our evaluation results show that the audio modality together with text yields improvements over text alone in the case of multiple speakers. Moreover, an audio-only model could outperform a text-only one for a single speaker.
&lt;/p&gt;</description></item><item><title>FACTIFY3M&#26159;&#19968;&#20010;&#20197;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#39564;&#35777;&#20026;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#12290;&#34394;&#20551;&#20449;&#24687;&#22914;&#20170;&#24050;&#25104;&#20026;&#24403;&#19979;&#37325;&#22823;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#36825;&#19968;&#25968;&#25454;&#38598;&#26088;&#22312;&#36890;&#36807;&#22810;&#27169;&#24335;&#39564;&#35777;&#26469;&#21450;&#26102;&#35782;&#21035;&#21644;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05523</link><description>&lt;p&gt;
FACTIFY3M: &#36890;&#36807;5W&#38382;&#31572;&#35299;&#37322;&#30340;&#22810;&#27169;&#24335;&#20107;&#23454;&#39564;&#35777;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering. (arXiv:2306.05523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05523
&lt;/p&gt;
&lt;p&gt;
FACTIFY3M&#26159;&#19968;&#20010;&#20197;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#39564;&#35777;&#20026;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#12290;&#34394;&#20551;&#20449;&#24687;&#22914;&#20170;&#24050;&#25104;&#20026;&#24403;&#19979;&#37325;&#22823;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#36825;&#19968;&#25968;&#25454;&#38598;&#26088;&#22312;&#36890;&#36807;&#22810;&#27169;&#24335;&#39564;&#35777;&#26469;&#21450;&#26102;&#35782;&#21035;&#21644;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#26159;&#24403;&#21069;&#20127;&#24453;&#35299;&#20915;&#30340;&#31038;&#20250;&#21361;&#26426;&#20043;&#19968;&#8212;&#8212;&#22823;&#32422;67%&#30340;&#32654;&#22269;&#20154;&#35748;&#20026;&#34394;&#20551;&#20449;&#24687;&#20250;&#20135;&#29983;&#22823;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#26377;10%&#30340;&#20154;&#26377;&#24847;&#35782;&#22320;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#12290;&#35777;&#25454;&#34920;&#26126;&#65292;&#34394;&#20551;&#20449;&#24687;&#21487;&#20197;&#25805;&#32437;&#27665;&#20027;&#36827;&#31243;&#21644;&#20844;&#20247;&#33286;&#35770;&#65292;&#24182;&#22312;&#21361;&#26426;&#26399;&#38388;&#24341;&#36215;&#32929;&#24066;&#21160;&#33633;&#12289;&#31038;&#20250;&#24656;&#24908;&#29978;&#33267;&#27515;&#20129;&#12290;&#22240;&#27492;&#65292;&#24212;&#21450;&#26102;&#35782;&#21035;&#24182;&#23613;&#21487;&#33021;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27599;&#22825;&#20998;&#20139;&#22823;&#32422;32&#20159;&#24352;&#22270;&#20687;&#21644;720,000 &#23567;&#26102;&#30340;&#35270;&#39057;&#65292;&#22240;&#27492;&#23545;&#20110;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#30340;&#21487;&#25193;&#23637;&#24615;&#26816;&#27979;&#38656;&#35201;&#39640;&#25928;&#30340;&#20107;&#23454;&#39564;&#35777;&#12290;&#23613;&#31649;&#22312;&#25991;&#26412;&#27169;&#24335;&#19979;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#21462;&#24471;&#20102;&#36827;&#23637;(&#20363;&#22914;&#65292;FEVER, LIAR)&#65292;&#20294;&#23398;&#26415;&#30028;&#22312;&#22810;&#27169;&#24335;&#20107;&#23454;&#39564;&#35777;&#26041;&#38754;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#21162;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FACTIFY3M&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;300&#19975;&#20010;&#26679;&#26412;&#65292;&#36890;&#36807;&#22810;&#31181;&#27169;&#24335;&#21644;5W&#38382;&#31572;&#25552;&#39640;&#20102;&#20107;&#23454;&#39564;&#35777;&#39046;&#22495;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combating disinformation is one of the burning societal crises -- about 67% of the American population believes that disinformation produces a lot of uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows that disinformation can manipulate democratic processes and public opinion, causing disruption in the share market, panic and anxiety in society, and even death during crises. Therefore, disinformation should be identified promptly and, if possible, mitigated. With approximately 3.2 billion images and 720,000 hours of video shared online daily on social media platforms, scalable detection of multimodal disinformation requires efficient fact verification. Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR), the research community lacks substantial effort in multimodal fact verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3 million samples that pushes the boundaries of the domain of fact verification via a multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#23569;&#25968;&#26063;&#35028;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25552;&#31034;&#20013;&#21333;&#35789;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#35782;&#21035;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>http://arxiv.org/abs/2306.05500</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#20998;&#26512;&#20559;&#35265;&#30340;&#21333;&#35789;&#32423;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Word-Level Explanations for Analyzing Bias in Text-to-Image Models. (arXiv:2306.05500v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#23569;&#25968;&#26063;&#35028;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25552;&#31034;&#20013;&#21333;&#35789;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#35782;&#21035;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#25509;&#25910;&#19968;&#21477;&#35805;&#65288;&#21363;&#25552;&#31034;&#65289;&#24182;&#29983;&#25104;&#19982;&#35813;&#36755;&#20837;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#22522;&#20110;&#31181;&#26063;&#21644;&#24615;&#21035;&#32780;&#20559;&#34962;&#23569;&#25968;&#26063;&#35028;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36755;&#20837;&#25552;&#31034;&#20013;&#21738;&#20010;&#21333;&#35789;&#23548;&#33268;&#29983;&#25104;&#22270;&#20687;&#20986;&#29616;&#20559;&#35265;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35745;&#31639;&#25552;&#31034;&#20013;&#27599;&#20010;&#21333;&#35789;&#24471;&#20998;&#30340;&#26041;&#27861;&#65307;&#36825;&#20123;&#24471;&#20998;&#20195;&#34920;&#20854;&#22312;&#27169;&#22411;&#36755;&#20986;&#20559;&#24046;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36981;&#24490;&#8220;&#21024;&#38500;&#35299;&#37322;&#8221;&#30340;&#21407;&#21017;&#65292;&#21033;&#29992;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#24433;&#21709;&#24471;&#20998;&#12290;&#25105;&#20204;&#22312;&#31283;&#23450;&#25193;&#25955;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#22797;&#21046;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models take a sentence (i.e., prompt) and generate images associated with this input prompt. These models have created award wining-art, videos, and even synthetic datasets. However, text-to-image (T2I) models can generate images that underrepresent minorities based on race and sex. This paper investigates which word in the input prompt is responsible for bias in generated images. We introduce a method for computing scores for each word in the prompt; these scores represent its influence on biases in the model's output. Our method follows the principle of \emph{explaining by removing}, leveraging masked language models to calculate the influence scores. We perform experiments on Stable Diffusion to demonstrate that our method identifies the replication of societal stereotypes in generated images.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;HouYi&#65292;&#24182;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.05499</link><description>&lt;p&gt;
LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection attack against LLM-integrated Applications. (arXiv:2306.05499v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;HouYi&#65292;&#24182;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#22240;&#20854;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#32780;&#22312;&#23427;&#20204;&#21608;&#22260;&#21050;&#28608;&#20102;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#24212;&#29992;&#29983;&#24577;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#26381;&#21153;&#20013;&#30340;&#24191;&#27867;&#34701;&#21512;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#23558;&#35299;&#26500;&#23454;&#38469;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#23545;&#21313;&#20010;&#21830;&#19994;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#30446;&#21069;&#25915;&#20987;&#31574;&#30053;&#22312;&#23454;&#36341;&#20013;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#21463;&#36825;&#20123;&#38480;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#38543;&#21518;&#21046;&#23450;&#20102;HouYi&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;&#65292;&#23427;&#20511;&#37492;&#20102;&#20256;&#32479;&#30340;Web&#27880;&#20837;&#25915;&#20987;&#12290;HouYi&#20998;&#20026;&#19977;&#20010;&#20851;&#38190;&#20803;&#32032;: &#19968;&#20010;&#26080;&#32541;&#38598;&#25104;&#30340;&#39044;&#26500;&#24314;&#25552;&#31034;&#12289;&#19968;&#20010;&#27880;&#20837;&#25552;&#31034;&#35825;&#23548;&#19978;&#19979;&#25991;&#20998;&#21306;&#20197;&#21450;&#19968;&#20010;&#24694;&#24847;&#36733;&#33655;&#65292;&#26088;&#22312;&#23454;&#29616;&#25915;&#20987;&#30446;&#26631;&#12290;&#21033;&#29992;HouYi&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#65292;&#24182;&#28436;&#31034;&#20102;&#32469;&#36807;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and sev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#25216;&#26415;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.05497</link><description>&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#25439;&#22833;&#20989;&#25968;&#65306;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reevaluating Loss Functions: Enhancing Robustness to Label Noise in Deep Learning Models. (arXiv:2306.05497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#25216;&#26415;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20013;&#38590;&#20813;&#20250;&#20986;&#29616;&#38169;&#35823;&#30340;&#26631;&#31614;&#65292;&#36825;&#32473;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#23481;&#26131;&#36866;&#24212;&#36825;&#20123;&#38169;&#35823;&#30340;&#26631;&#31614;&#12290;&#21482;&#26377;&#20351;&#29992;&#19981;&#21463;&#22122;&#22768;&#24178;&#25200;&#30340;&#40065;&#26834;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#25165;&#33021;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#21019;&#24314;&#22122;&#22768;&#40065;&#26834;&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#26159;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#25968;&#37327;&#20247;&#22810;&#65292;&#23427;&#20204;&#36890;&#24120;&#20276;&#38543;&#30528;&#36229;&#21442;&#25968;&#65292;&#32780;&#19988;&#21487;&#33021;&#23398;&#20064;&#36895;&#24230;&#27604;&#24191;&#27867;&#20351;&#29992;&#20294;&#23545;&#22122;&#22768;&#25935;&#24863;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#35201;&#24930;&#12290;&#36890;&#36807;&#21551;&#21457;&#24335;&#32771;&#34385;&#21644;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#36866;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#25439;&#22833;&#30340;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#24102;&#26377;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#23398;&#20064;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#65292;&#21363;&#30053;&#24494;&#22686;&#21152;&#19982;&#27491;&#30830;&#26631;&#31614;&#30456;&#23545;&#24212;&#30340;&#31070;&#32463;&#20803;&#39044;&#28608;&#27963;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#25216;&#26415;&#22312;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large annotated datasets inevitably contain incorrect labels, which poses a major challenge for the training of deep neural networks as they easily fit the labels. Only when training with a robust model that is not easily distracted by the noise, a good generalization performance can be achieved. A simple yet effective way to create a noise robust model is to use a noise robust loss function. However, the number of proposed loss functions is large, they often come with hyperparameters, and may learn slower than the widely used but noise sensitive Cross Entropy loss. By heuristic considerations and extensive numerical experiments, we study in which situations the proposed loss functions are applicable and give suggestions on how to choose an appropriate loss. Additionally, we propose a novel technique to enhance learning with bounded loss functions: the inclusion of an output bias, i.e. a slight increase in the neuron pre-activation corresponding to the correct label. Surprisingly, we f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26816;&#27979;&#36229;&#20986;&#35757;&#32451;&#33539;&#30068;&#30340;&#30446;&#26631;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05493</link><description>&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Classifiers for Open-Vocabulary Object Detection. (arXiv:2306.05493v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26816;&#27979;&#36229;&#20986;&#35757;&#32451;&#33539;&#30068;&#30340;&#30446;&#26631;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#26631;&#22312;&#20110;&#23454;&#29616;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#65288;OVOD&#65289;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#35328;&#25551;&#36848;&#12289;&#22270;&#20687;&#23454;&#20363;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#65292;&#25351;&#23450;&#26032;&#31867;&#21035;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#26500;&#24314;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#22823;&#20998;&#31867;&#22120;&#65292;&#26368;&#32456;&#25552;&#20986;&#19968;&#31181;&#34701;&#21512;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#20998;&#31867;&#22120;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is open-vocabulary object detection (OVOD) $\unicode{x2013}$ building a model that can detect objects beyond the set of categories seen at training, thus enabling the user to specify categories of interest at inference without the need for model retraining. We adopt a standard two-stage object detector architecture, and explore three ways for specifying novel categories: via language descriptions, via image exemplars, or via a combination of the two. We make three contributions: first, we prompt a large language model (LLM) to generate informative language descriptions for object classes, and construct powerful text-based classifiers; second, we employ a visual aggregator on image exemplars that can ingest any number of images as input, forming vision-based classifiers; and third, we provide a simple method to fuse information from language descriptions and image exemplars, yielding a multi-modal classifier. When evaluating on the challenging LVIS open-vocabulary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#22522;&#30784;&#65292;&#20197;&#35777;&#26126;&#21487;&#20197;&#29992;PAC&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#22810;&#26234;&#33021;&#20307;&#35748;&#30693;&#36923;&#36753;&#65292;&#24182;&#23454;&#29616;&#20102;&#38544;&#24335;&#23398;&#20064;&#26469;&#24212;&#23545;PAC&#35821;&#20041;&#24378;&#20581;&#23398;&#20064;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2306.05490</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20449;&#24565;&#30340;PAC&#35821;&#20041;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learnability with PAC Semantics for Multi-agent Beliefs. (arXiv:2306.05490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#22522;&#30784;&#65292;&#20197;&#35777;&#26126;&#21487;&#20197;&#29992;PAC&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#22810;&#26234;&#33021;&#20307;&#35748;&#30693;&#36923;&#36753;&#65292;&#24182;&#23454;&#29616;&#20102;&#38544;&#24335;&#23398;&#20064;&#26469;&#24212;&#23545;PAC&#35821;&#20041;&#24378;&#20581;&#23398;&#20064;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#32462;&#21644;&#24402;&#32435;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#21487;&#33021;&#26159;&#21746;&#23398;&#12289;&#35748;&#30693;&#21644;&#20154;&#24037;&#26234;&#33021;&#31561;&#39046;&#22495;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#26469;&#25429;&#25417;PAC&#23398;&#20064;&#31639;&#27861;&#36755;&#20986;&#30340;&#24615;&#36136;&#65292;&#20351;&#20854;&#19982;&#28436;&#32462;&#34701;&#20026;&#19968;&#20307;&#65292;&#20197;&#20415;&#20026;&#22238;&#31572;&#26597;&#35810;&#25552;&#20379;&#24378;&#22823;&#30340;&#27169;&#22411;&#29702;&#35770;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25216;&#26415;&#22522;&#30784;&#65292;&#20197;&#35777;&#26126;&#21487;&#20197;&#29992;PAC&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#22810;&#26234;&#33021;&#20307;&#35748;&#30693;&#36923;&#36753;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#19968;&#31181;&#31216;&#20026;&#38544;&#24335;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25991;&#29486;&#20013;&#20851;&#20110;PAC&#35821;&#20041;&#24378;&#20581;&#23398;&#20064;&#22256;&#38590;&#30340;&#36127;&#38754;&#32467;&#26524;&#65292;&#21363;&#25105;&#20204;&#33021;&#22815;&#23558;&#35266;&#23519;&#32467;&#26524;&#25972;&#21512;&#21040;&#32972;&#26223;&#29702;&#35770;&#20013;&#65292;&#20197;&#24110;&#21161;&#20915;&#23450;&#35748;&#30693;&#26597;&#35810;&#30340;&#34164;&#21547;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tension between deduction and induction is perhaps the most fundamental issue in areas such as philosophy, cognition and artificial intelligence. In an influential paper, Valiant recognised that the challenge of learning should be integrated with deduction. In particular, he proposed a semantics to capture the quality possessed by the output of Probably Approximately Correct (PAC) learning algorithms when formulated in a logic. Although weaker than classical entailment, it allows for a powerful model-theoretic framework for answering queries. In this paper, we provide a new technical foundation to demonstrate PAC learning with multi-agent epistemic logics. To circumvent the negative results in the literature on the difficulty of robust learning with the PAC semantics, we consider so-called implicit learning where we are able to incorporate observations to the background theory in service of deciding the entailment of an epistemic query. We prove correctness of the learning procedur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#26469;&#24320;&#21457;&#21644;&#37096;&#32626;AGI&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05480</link><description>&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence for Medical Imaging. (arXiv:2306.05480v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#26469;&#24320;&#21457;&#21644;&#37096;&#32626;AGI&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#23558;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#25972;&#21512;&#21040;AGI&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;AGI&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#36335;&#32447;&#22270;&#12290;&#22312;&#25972;&#20010;&#32508;&#36848;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#21307;&#23398;&#39046;&#22495;&#37096;&#32626;&#22823;&#35268;&#27169;AGI&#27169;&#22411;&#21487;&#33021;&#38754;&#20020;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#32570;&#38519;&#30340;&#20851;&#38190;&#35266;&#28857;&#12290;&#36825;&#31687;&#32508;&#21512;&#24615;&#30340;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;AGI&#23545;&#21307;&#23398;&#25104;&#20687;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#20854;&#20182;&#39046;&#22495;&#26410;&#26469;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this review, we explore the potential applications of Artificial General Intelligence (AGI) models in healthcare, focusing on foundational Large Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We emphasize the importance of integrating clinical expertise, domain knowledge, and multimodal capabilities into AGI models. In addition, we lay out key roadmaps that guide the development and deployment of healthcare AGI models. Throughout the review, we provide critical perspectives on the potential challenges and pitfalls associated with deploying large-scale AGI models in the medical field. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare and beyond.
&lt;/p&gt;</description></item><item><title>&#24739;&#26377;&#35821;&#35328;&#38556;&#30861;&#30340;&#20154;&#22312;&#29616;&#26377;&#28040;&#36153;&#31867;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#30701;&#35821;&#35782;&#21035;&#31995;&#32479;&#29992;&#20110;&#36866;&#24212;&#20854;&#38750;&#20856;&#22411;&#30340;&#35821;&#38899;&#27169;&#24335;&#65292;&#20854;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05446</link><description>&lt;p&gt;
&#24739;&#35821;&#35328;&#38556;&#30861;&#32773;&#30340;&#28508;&#22312;&#30701;&#35821;&#21305;&#37197;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Latent Phrase Matching for Dysarthric Speech. (arXiv:2306.05446v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05446
&lt;/p&gt;
&lt;p&gt;
&#24739;&#26377;&#35821;&#35328;&#38556;&#30861;&#30340;&#20154;&#22312;&#29616;&#26377;&#28040;&#36153;&#31867;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#30701;&#35821;&#35782;&#21035;&#31995;&#32479;&#29992;&#20110;&#36866;&#24212;&#20854;&#38750;&#20856;&#22411;&#30340;&#35821;&#38899;&#27169;&#24335;&#65292;&#20854;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#28040;&#36153;&#31867;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#19981;&#33021;&#20026;&#35821;&#35328;&#38556;&#30861;&#24739;&#32773;&#25552;&#20379;&#33391;&#22909;&#30340;&#20307;&#39564;&#21644;&#35782;&#21035;&#25928;&#26524;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35821;&#35328;&#38556;&#30861;&#24773;&#20917;&#26356;&#20026;&#20005;&#37325;&#30340;&#20154;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20010;&#24615;&#21270;&#35821;&#38899;&#27169;&#22411;&#30340;&#25506;&#32034;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#36866;&#24212;&#38750;&#20856;&#22411;&#30340;&#35821;&#38899;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#20010;&#24615;&#21270;&#30701;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#23569;&#37327;&#30340;&#35821;&#38899;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#20381;&#36182;&#20110;&#20256;&#32479;&#21457;&#38899;&#35789;&#20856;&#65292;&#19981;&#21463;&#19981;&#21516;&#35821;&#35328;&#24433;&#21709;&#65292;&#19988;&#22312;&#21508;&#31181;&#35821;&#38899;&#38556;&#30861;&#24773;&#20917;&#19979;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;32&#21517;&#24739;&#26377;&#35328;&#35821;&#22256;&#38590;&#30340;&#20154;&#21592;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#19981;&#21463;&#20005;&#37325;&#31243;&#24230;&#24433;&#21709;&#65292;&#19982;&#21830;&#19994;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30456;&#27604;&#65292;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;60%&#12290;&#22312;&#20844;&#20849;EasyCall&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#31934;&#24230;&#25552;&#39640;&#20102;30.5%&#12290;&#24403;&#35757;&#32451;50&#20010;&#29420;&#29305;&#30701;&#35821;&#26102;&#65292;&#24615;&#33021;&#38543;&#30701;&#35821;&#25968;&#37327;&#22686;&#21152;&#32780;&#19979;&#38477;&#65292;&#20294;&#22987;&#32456;&#20248;&#20110;ASR&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many consumer speech recognition systems are not tuned for people with speech disabilities, resulting in poor recognition and user experience, especially for severe speech differences. Recent studies have emphasized interest in personalized speech models from people with atypical speech patterns. We propose a query-by-example-based personalized phrase recognition system that is trained using small amounts of speech, is language agnostic, does not assume a traditional pronunciation lexicon, and generalizes well across speech difference severities. On an internal dataset collected from 32 people with dysarthria, this approach works regardless of severity and shows a 60% improvement in recall relative to a commercial speech recognition system. On the public EasyCall dataset of dysarthric speech, our approach improves accuracy by 30.5%. Performance degrades as the number of phrases increases, but consistently outperforms ASR systems when trained with 50 unique phrases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PIXIU&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;Fine-tuning LLaMA&#30340;&#31532;&#19968;&#20010;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#12289;&#21253;&#21547;136K&#25968;&#25454;&#26679;&#26412;&#29992;&#20110;Fine-tuning&#30340;&#31532;&#19968;&#20010;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#21450;&#20855;&#26377;5&#20010;&#20219;&#21153;&#21644;9&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#12289;&#37329;&#34701;&#25991;&#26723;&#31867;&#22411;&#21644;&#37329;&#34701;&#25968;&#25454;&#27169;&#24577;&#32771;&#34385;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#20219;&#21153;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#21517;&#20026;FinMA&#30340;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#23545;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#36827;&#34892;&#25351;&#20196;&#36319;&#38543;&#12290;</title><link>http://arxiv.org/abs/2306.05443</link><description>&lt;p&gt;
PIXIU&#65306;&#38754;&#21521;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#25351;&#20196;&#25968;&#25454;&#21644;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance. (arXiv:2306.05443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PIXIU&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;Fine-tuning LLaMA&#30340;&#31532;&#19968;&#20010;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#12289;&#21253;&#21547;136K&#25968;&#25454;&#26679;&#26412;&#29992;&#20110;Fine-tuning&#30340;&#31532;&#19968;&#20010;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#21450;&#20855;&#26377;5&#20010;&#20219;&#21153;&#21644;9&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#12289;&#37329;&#34701;&#25991;&#26723;&#31867;&#22411;&#21644;&#37329;&#34701;&#25968;&#25454;&#27169;&#24577;&#32771;&#34385;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#20219;&#21153;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#21517;&#20026;FinMA&#30340;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#23545;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#36827;&#34892;&#25351;&#20196;&#36319;&#38543;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#20844;&#24320;&#30340;&#38754;&#21521;&#37329;&#34701;&#30340;&#35821;&#35328;&#27169;&#22411;&#12289;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#36825;&#23545;&#20110;&#19981;&#26029;&#25512;&#36827;&#37329;&#34701;&#20154;&#24037;&#26234;&#33021;&#24320;&#28304;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PIXIU&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;Fine-tuning LLaMA&#30340;&#31532;&#19968;&#20010;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#12289;&#21253;&#21547;136K&#25968;&#25454;&#26679;&#26412;&#29992;&#20110;Fine-tuning&#30340;&#31532;&#19968;&#20010;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#21450;&#20855;&#26377;5&#20010;&#20219;&#21153;&#21644;9&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#12289;&#37329;&#34701;&#25991;&#26723;&#31867;&#22411;&#21644;&#37329;&#34701;&#25968;&#25454;&#27169;&#24577;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#20219;&#21153;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;Fine-tuning LLaMA&#25552;&#20986;&#20102;&#21517;&#20026;FinMA&#30340;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#23545;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#36827;&#34892;&#25351;&#20196;&#36319;&#38543;&#12290;&#20026;&#20102;&#25903;&#25345;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) has shown great performance on natural language processing (NLP) in the financial domain, there are no publicly available financial tailtored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 136K data samples to support the fine-tuning, and an evaluation benchmark with 5 tasks and 9 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05439</link><description>&lt;p&gt;
CLC: &#22522;&#20110;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#30340;&#32858;&#31867;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLC: Cluster Assignment via Contrastive Representation Learning. (arXiv:2306.05439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#26679;&#26412;&#20998;&#32452;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21253;&#21547;&#22823;&#37327;&#32858;&#31867;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;ImageNet&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#23454;&#29616;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#12290;&#25105;&#20204;&#23558;&#34920;&#31034;&#20998;&#35299;&#20026;&#20004;&#37096;&#20998;&#65306;&#19968;&#37096;&#20998;&#23545;&#31867;&#21035;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#37319;&#29992;&#31561;&#20998;&#32422;&#26463;&#65292;&#21478;&#19968;&#37096;&#20998;&#25429;&#25417;&#23454;&#20363;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#25439;&#22833;&#65292;&#20351;&#29992;&#34920;&#31034;&#30340;&#20004;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#24182;&#25581;&#31034;&#20102;CLC&#22312;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#26102;&#20026;&#36127;&#26679;&#26412;&#35774;&#32622;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;&#36827;&#19968;&#27493;&#30340;&#26799;&#24230;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;CLC&#26102;&#65292;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering remains an important and challenging task of grouping samples into clusters without manual annotations. Recent works have achieved excellent results on small datasets by performing clustering on feature representations learned from self-supervised learning. However, for datasets with a large number of clusters, such as ImageNet, current methods still can not achieve high clustering performance. In this paper, we propose Contrastive Learning-based Clustering (CLC), which uses contrastive learning to directly learn cluster assignment. We decompose the representation into two parts: one encodes the categorical information under an equipartition constraint, and the other captures the instance-wise factors. We propose a contrastive loss using both parts of the representation. We theoretically analyze the proposed contrastive loss and reveal that CLC sets different weights for the negative samples while learning cluster assignments. Further gradient analysis shows that the larger 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05437</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65306;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
One-step Multi-view Clustering with Diverse Representation. (arXiv:2306.05437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#22240;&#20854;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#35270;&#35282;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#25928;&#26524;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#21516;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-view clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, which limits the expressiveness of the model. Moreover, a range of methods suffer from a two-step process, i.e., multimodal learning and the subsequent $k$-means, inevitably causing a sub-optimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation method, which incorporates multi-view learning and $k$-means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36808;&#21521;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#21644;&#36319;&#36394;&#27599;&#22825;&#19978;&#20256;&#22312;&#32447;&#30340;&#24191;&#25773;&#26032;&#38395;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#20016;&#23500;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26469;&#25552;&#39640;&#20854;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05432</link><description>&lt;p&gt;
&#36808;&#21521;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-end Speech-to-text Summarization. (arXiv:2306.05432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36808;&#21521;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#21644;&#36319;&#36394;&#27599;&#22825;&#19978;&#20256;&#22312;&#32447;&#30340;&#24191;&#25773;&#26032;&#38395;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#20016;&#23500;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26469;&#25552;&#39640;&#20854;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21040;&#25991;&#26412;&#65288;S2T&#65289;&#25688;&#35201;&#26159;&#19968;&#31181;&#33410;&#30465;&#26102;&#38388;&#30340;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#36807;&#28388;&#21644;&#36319;&#36394;&#27599;&#22825;&#19978;&#20256;&#22312;&#32447;&#30340;&#24191;&#25773;&#26032;&#38395;&#12290;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#20013;&#20986;&#29616;&#20102;&#20855;&#26377;&#20986;&#33394;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#36215;&#20102;&#23545;&#20135;&#29983;&#31616;&#27905;&#25991;&#26723;&#29256;&#26412;&#30340;&#25688;&#35201;&#31995;&#32479;&#30340;&#30740;&#31350;&#37325;&#28857;&#65292;&#20063;&#31216;&#20026;&#25277;&#35937;&#25688;&#35201;&#12290;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#24314;&#27169;&#30340;S2T&#25277;&#35937;&#25688;&#35201;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20379;&#20102;&#20135;&#29983;&#20016;&#23500;&#28508;&#22312;&#34920;&#31034;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#20123;&#34920;&#31034;&#21033;&#29992;&#20102;&#38750;&#35821;&#35328;&#21644;&#22768;&#23398;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20165;&#20351;&#29992;&#32423;&#32852;&#31995;&#32479;&#20013;&#33258;&#21160;&#29983;&#25104;&#30340;&#36716;&#24405;&#25991;&#26412;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#20851;&#20110;&#36825;&#39033;&#20219;&#21153;&#30340;E2E&#24314;&#27169;&#24456;&#23569;&#25506;&#32034;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#24191;&#25773;&#26032;&#38395;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#65292;&#27599;&#22825;&#21521;&#29992;&#25143;&#21576;&#29616;&#22823;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;&#32423;&#32852;&#32763;&#35793;&#26041;&#27861;&#21644;&#31471;&#21040;&#31471;&#26041;&#27861;&#24314;&#27169;S2T&#25688;&#35201;&#65292;&#24182;&#22312;&#24191;&#25773;&#26032;&#38395;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;E2E&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26469;&#25552;&#39640;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-to-text (S2T) summarization is a time-saving technique for filtering and keeping up with the broadcast news uploaded online on a daily basis. The rise of large language models from deep learning with impressive text generation capabilities has placed the research focus on summarization systems that produce paraphrased compact versions of the document content, also known as abstractive summaries. End-to-end (E2E) modelling of S2T abstractive summarization is a promising approach that offers the possibility of generating rich latent representations that leverage non-verbal and acoustic information, as opposed to the use of only linguistic information from automatically generated transcripts in cascade systems. However, the few literature on E2E modelling of this task fails on exploring different domains, namely broadcast news, which is challenging domain where large and diversified volumes of data are presented to the user every day. We model S2T summarization both with a cascade 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.05176</link><description>&lt;p&gt;
RRWKV&#65306;&#22312;RWKV&#20013;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
RRWKV: Capturing Long-range Dependencies in RWKV. (arXiv:2306.05176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Transformer&#24778;&#20154;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#26550;&#26500;&#12290;&#26368;&#36817;&#65292;Receptance Weighted Key Value&#65288;RWKV&#65289;&#26550;&#26500;&#36981;&#24490;&#38750;Transformer&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#32570;&#28857;&#65292;&#20854;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#21576;&#20108;&#27425;&#25193;&#23637;&#12290;&#23613;&#31649;RWKV&#21033;&#29992;&#20102;&#32447;&#24615;&#24352;&#37327;&#31215;&#27880;&#24847;&#26426;&#21046;&#24182;&#36890;&#36807;&#37096;&#32626;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#23454;&#29616;&#20102;&#24182;&#34892;&#35745;&#31639;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#20013;&#30452;&#25509;&#20132;&#20114;&#33719;&#24471;&#30340;&#23436;&#25972;&#20449;&#24687;&#30456;&#27604;&#65292;&#23427;&#26080;&#27861;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#20026;&#20854;&#21463;&#38480;&#20110;&#21521;&#21518;&#26597;&#30475;&#20808;&#21069;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#23558;&#22238;&#39038;&#33021;&#21147;&#32435;&#20837;RWKV&#20013;&#26469;&#35774;&#35745;Retrospected Receptance Weighted Key Value&#65288;RRWKV&#65289;&#26550;&#26500;&#65292;&#20197;&#26377;&#25928;&#22320;&#21560;&#25910;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to the impressive dot-product attention, the Transformers have been the dominant architectures in various natural language processing (NLP) tasks. Recently, the Receptance Weighted Key Value (RWKV) architecture follows a non-transformer architecture to eliminate the drawbacks of dot-product attention, where memory and computational complexity exhibits quadratic scaling with sequence length. Although RWKV has exploited a linearly tensor-product attention mechanism and achieved parallelized computations by deploying the time-sequential mode, it fails to capture long-range dependencies because of its limitation on looking back at previous information, compared with full information obtained by direct interactions in the standard transformer. Therefore, the paper devises the Retrospected Receptance Weighted Key Value (RRWKV) architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#20849;&#21516;&#24320;&#21457;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;AI&#32534;&#31243;&#21161;&#25163;&#65292;&#22686;&#24378;&#21452;&#26041;&#21512;&#20316;&#30340;&#29983;&#20135;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05153</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#26159;&#26356;&#22909;&#30340;&#32534;&#31243;&#20249;&#20276;&#65311;&#20154;&#20154;&#23545;&#32534;&#31243; vs &#20154;&#24037;&#26234;&#33021;&#23545;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Is AI the better programming partner? Human-Human Pair Programming vs. Human-AI pAIr Programming. (arXiv:2306.05153v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05153
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#20849;&#21516;&#24320;&#21457;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;AI&#32534;&#31243;&#21161;&#25163;&#65292;&#22686;&#24378;&#21452;&#26041;&#21512;&#20316;&#30340;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25484;&#25569;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#20197;&#21450;&#21830;&#19994;&#20135;&#21697;&#65292;&#22914;GitHub&#30340;Copilot&#65292;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#21512;&#20316;(pAIr&#32534;&#31243;)&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#20154;&#20154;&#23545;&#32534;&#31243;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20173;&#28982;&#19981;&#30830;&#23450;&#20854;&#30740;&#31350;&#32467;&#26524;&#26159;&#21542;&#33021;&#36866;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#23545;&#32534;&#31243;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20154;&#20154;&#23545;&#32534;&#31243;&#21644;&#20154;&#24037;&#26234;&#33021;&#23545;&#32534;&#31243;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#20132;&#20114;&#12289;&#34913;&#37327;&#12289;&#20248;&#28857;&#21644;&#25361;&#25112;&#26041;&#38754;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#24046;&#24322;&#20043;&#22788;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#25991;&#29486;&#20013;&#21508;&#19981;&#30456;&#21516;&#65288;&#23613;&#31649;&#29992;&#20110;pAIr&#32534;&#31243;&#30340;&#24230;&#37327;&#19981;&#22826;&#20840;&#38754;&#65289;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#24433;&#21709;&#20154;&#20154;&#23545;&#32534;&#31243;&#25104;&#21151;&#30340;&#35843;&#33410;&#22240;&#32032;&#65292;&#20026;pAIr&#32534;&#31243;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#20363;&#22914;&#65292;&#19981;&#21305;&#37197;&#30340;&#19987;&#19994;&#30693;&#35782;&#20250;&#20351;&#23545;&#32534;&#31243;&#32570;&#20047;&#29983;&#20135;&#21147;&#65292;&#22240;&#27492;&#35774;&#35745;&#33391;&#22909;&#30340;AI&#32534;&#31243;&#21161;&#25163;&#21487;&#20197;&#28508;&#22312;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-language models (LLMs) that excel at code generation and commercial products such as GitHub's Copilot has sparked interest in human-AI pair programming (referred to as "pAIr programming") where an AI system collaborates with a human programmer. While traditional pair programming between humans has been extensively studied, it remains uncertain whether its findings can be applied to human-AI pair programming. We compare human-human and human-AI pair programming, exploring their similarities and differences in interaction, measures, benefits, and challenges. We find that the effectiveness of both approaches is mixed in the literature (though the measures used for pAIr programming are not as comprehensive). We summarize moderating factors on the success of human-human pair programming, which provides opportunities for pAIr programming research. For example, mismatched expertise makes pair programming less productive, therefore well-designed AI programming assistants
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#32771;&#34385;&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04641</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#20302;&#36164;&#28304;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Generalizable Low-Resource Activity Recognition with Diverse and Discriminative Representation Learning. (arXiv:2306.04641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04641
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#32771;&#34385;&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26159;&#19968;&#39033;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#20174;&#20154;&#31867;&#20256;&#24863;&#22120;&#35835;&#25968;&#20013;&#35782;&#21035;&#21160;&#20316;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#20805;&#36275;&#30340;&#25968;&#25454;&#26159;&#35757;&#32451;&#36890;&#29992;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#30340;&#20851;&#38190;&#38590;&#28857;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#32447;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#30340;&#23450;&#21046;&#21644;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#32771;&#34385;&#24046;&#24322;&#24615;&#21644;&#27495;&#35270;&#24615;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) is a time series classification task that focuses on identifying the motion patterns from human sensor readings. Adequate data is essential but a major bottleneck for training a generalizable HAR model, which assists customization and optimization of online web applications. However, it is costly in time and economy to collect large-scale labeled data in reality, i.e., the low-resource challenge. Meanwhile, data collected from different persons have distribution shifts due to different living habits, body shapes, age groups, etc. The low-resource and distribution shift challenges are detrimental to HAR when applying the trained model to new unseen subjects. In this paper, we propose a novel approach called Diverse and Discriminative representation Learning (DDLearn) for generalizable low-resource HAR. DDLearn simultaneously considers diversity and discrimination learning. With the constructed self-supervised learning task, DDLearn enlarges the data dive
&lt;/p&gt;</description></item><item><title>GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04607</link><description>&lt;p&gt;
&#23558;&#20960;&#20309;&#25511;&#21046;&#38598;&#25104;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt. (arXiv:2306.04607v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04607
&lt;/p&gt;
&lt;p&gt;
GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#21019;&#24314;&#20869;&#23481;&#21644;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#26174;&#30528;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;&#19981;&#20165;&#22270;&#20687;&#27700;&#24179;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#32780;&#19988;&#36793;&#30028;&#26694;&#21644;&#30456;&#26426;&#35270;&#22270;&#31561;&#20960;&#20309;&#26465;&#20214;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#21069;&#26399;&#30740;&#31350;&#20351;&#29992;&#27169;&#22359;&#32534;&#30721;&#35821;&#20041;&#24067;&#23616;&#26469;&#23454;&#29616;&#22797;&#21046;&#31896;&#36148;&#21512;&#25104;&#25110;&#24067;&#23616;&#21040;&#22270;&#20687;(L2I)&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GeoDiffusion&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;(T2I)&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#12290;&#19982;&#20197;&#24448;&#30340;L2I&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;GeoDiffusion&#19981;&#20165;&#33021;&#22815;&#32534;&#30721;&#36793;&#30028;&#26694;&#65292;&#36824;&#33021;&#22815;&#32534;&#30721;&#33258;&#39550;&#22330;&#26223;&#20013;&#30340;&#39069;&#22806;&#20960;&#20309;&#26465;&#20214;&#65292;&#22914;&#25668;&#20687;&#22836;&#35270;&#22270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GeoDiffusion&#22312;&#29289;&#20307;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#29983;&#25104;&#20855;&#26377;&#26356;&#39640;&#24863;&#30693;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have attracted significant attention due to their remarkable ability to create content and generate data for tasks such as image classification. However, the usage of diffusion models to generate high-quality object detection data remains an underexplored area, where not only the image-level perceptual quality but also geometric conditions such as bounding boxes and camera views are essential. Previous studies have utilized either copy-paste synthesis or layout-to-image (L2I) generation with specifically designed modules to encode semantic layouts. In this paper, we propose GeoDiffusion, a simple framework that can flexibly translate various geometric conditions into text prompts and empower the pre-trained text-to-image (T2I) diffusion models for high-quality detection data generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not only bounding boxes but also extra geometric conditions such as camera views in self-driving scenes. Extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.04026</link><description>&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#21363;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65306;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#39640;&#24230;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#20294;&#39564;&#35777;&#31574;&#30053;&#34892;&#20026;&#30340;&#38590;&#24230;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20998;&#26512;&#23433;&#20840;&#32500;&#25252;&#30340;&#31616;&#21333;&#20219;&#21153;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23558;&#20540;&#20989;&#25968;&#19982;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30456;&#32852;&#31995;&#30340;&#21407;&#22987;&#23450;&#29702;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#23398;&#20064;&#30340;&#23454;&#38469;&#23454;&#26045;&#32454;&#33410;&#12290;&#38500;&#20102;&#25552;&#20986;&#35777;&#20070;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;RL&#31574;&#30053;&#35299;&#38145;&#20102;&#20016;&#23500;&#30340;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRS&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21360;&#35937;&#26041;&#24046;&#24863;&#30693;&#30340;&#26041;&#24335;&#23545;&#24191;&#21578;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20010;&#24615;&#21270;&#24191;&#21578;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03293</link><description>&lt;p&gt;
&#37319;&#29992;&#21360;&#35937;&#26041;&#24046;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20010;&#24615;&#21270;&#24191;&#21578;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Fairness in Personalized Ads Using Impression Variance Aware Reinforcement Learning. (arXiv:2306.03293v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRS&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21360;&#35937;&#26041;&#24046;&#24863;&#30693;&#30340;&#26041;&#24335;&#23545;&#24191;&#21578;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20010;&#24615;&#21270;&#24191;&#21578;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#24191;&#21578;&#31995;&#32479;&#20013;&#65292;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;&#24191;&#21578;&#21360;&#35937;&#32467;&#26524;&#24046;&#24322;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#35270;&#20026;&#31639;&#27861;&#20559;&#35265;&#30340;&#21487;&#33021;&#26631;&#24535;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31216;&#20026;VRS&#65288;Variance Reduction System&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;Meta&#24191;&#21578;&#31995;&#32479;&#32467;&#26524;&#65292;&#21516;&#26102;&#36890;&#36807;&#21360;&#35937;&#26041;&#24046;&#24863;&#30693;&#30340;&#26041;&#24335;&#23545;&#24191;&#21578;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variances in ad impression outcomes across demographic groups are increasingly considered to be potentially indicative of algorithmic bias in personalized ads systems. While there are many definitions of fairness that could be applicable in the context of personalized systems, we present a framework which we call the Variance Reduction System (VRS) for achieving more equitable outcomes in Meta's ads systems. VRS seeks to achieve a distribution of impressions with respect to selected protected class (PC) attributes that more closely aligns the demographics of an ad's eligible audience (a function of advertiser targeting criteria) with the audience who sees that ad, in a privacy-preserving manner. We first define metrics to quantify fairness gaps in terms of ad impression variances with respect to PC attributes including gender and estimated race. We then present the VRS for re-ranking ads in an impression variance-aware manner. We evaluate VRS via extensive simulations over different pa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#22810;&#21464;&#37327;&#19981;&#23545;&#40784;&#31232;&#30095;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;SERT&#65292;&#21478;&#19968;&#31181;&#26159;&#25552;&#20379;&#21487;&#35299;&#37322;&#32467;&#26524;&#30340;&#31616;&#21333;&#27169;&#22411;SST-ANN</title><link>http://arxiv.org/abs/2306.03042</link><description>&lt;p&gt;
SERT: &#22522;&#20110;Transformer&#30340;&#32570;&#22833;&#31354;&#38388;-&#26102;&#38388;&#20256;&#24863;&#22120;&#25968;&#25454;&#27169;&#22411;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
SERT: A Transfomer Based Model for Spatio-Temporal Sensor Data with Missing Values for Environmental Monitoring. (arXiv:2306.03042v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#22810;&#21464;&#37327;&#19981;&#23545;&#40784;&#31232;&#30095;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;SERT&#65292;&#21478;&#19968;&#31181;&#26159;&#25552;&#20379;&#21487;&#35299;&#37322;&#32467;&#26524;&#30340;&#31616;&#21333;&#27169;&#22411;SST-ANN
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#30417;&#27979;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#12289;&#29983;&#29289;&#22810;&#26679;&#24615;&#20007;&#22833;&#21644;&#27745;&#26579;&#31561;&#26041;&#38754;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#12290;&#26469;&#33258;&#20256;&#24863;&#22120;&#21644;&#21355;&#26143;&#31561;&#25968;&#25454;&#28304;&#30340;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#22797;&#26434;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20102;&#35299;&#20027;&#35201;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;&#20294;&#30001;&#20110;&#35774;&#22791;&#38382;&#39064;&#25110;&#32500;&#25252;&#38382;&#39064;&#65292;&#20174;&#20256;&#24863;&#22120;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#32570;&#22833;&#20540;&#12290;&#32570;&#22833;&#20540;&#24456;&#23569;&#21516;&#26102;&#21457;&#29983;&#65292;&#23548;&#33268;&#25968;&#25454;&#26159;&#22810;&#21464;&#37327;&#19981;&#23545;&#40784;&#30340;&#31232;&#30095;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#26102;&#33258;&#28982;&#22320;&#25191;&#34892;&#22810;&#21464;&#37327;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#25554;&#34917;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;SERT&#65288;&#26469;&#33258;Transformer&#30340;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#34920;&#31034;&#65289;&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#34987;&#21629;&#21517;&#20026;SST-ANN&#65288;&#31232;&#30095;&#31354;&#38388;-&#26102;&#38388;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#33021;&#22815;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Environmental monitoring is crucial to our understanding of climate change, biodiversity loss and pollution. The availability of large-scale spatio-temporal data from sources such as sensors and satellites allows us to develop sophisticated models for forecasting and understanding key drivers. However, the data collected from sensors often contain missing values due to faulty equipment or maintenance issues. The missing values rarely occur simultaneously leading to data that are multivariate misaligned sparse time series. We propose two models that are capable of performing multivariate spatio-temporal forecasting while handling missing data naturally without the need for imputation. The first model is a transformer-based model, which we name SERT (Spatio-temporal Encoder Representations from Transformers). The second is a simpler model named SST-ANN (Sparse Spatio-Temporal Artificial Neural Network) which is capable of providing interpretable results. We conduct extensive experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24314;&#27169;&#21644;&#35299;&#20915;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#26694;&#26550;&#8212;&#34892;&#21160;&#28436;&#21464;Petri&#32593;&#26684;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#21487;&#20197;&#34920;&#31034;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#25152;&#26377;&#35201;&#32032;&#12290;&#32780;&#19988;&#65292;&#35813;&#27169;&#22411;&#26159;&#21487;&#25191;&#34892;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#37197;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.02910</link><description>&lt;p&gt;
&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#35299;&#20915;&#26694;&#26550;&#65306;&#34892;&#21160;&#28436;&#21464;Petri&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Action-Evolution Petri Nets: a Framework for Modeling and Solving Dynamic Task Assignment Problems. (arXiv:2306.02910v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24314;&#27169;&#21644;&#35299;&#20915;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#26694;&#26550;&#8212;&#34892;&#21160;&#28436;&#21464;Petri&#32593;&#26684;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#21487;&#20197;&#34920;&#31034;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#25152;&#26377;&#35201;&#32032;&#12290;&#32780;&#19988;&#65292;&#35813;&#27169;&#22411;&#26159;&#21487;&#25191;&#34892;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#37197;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#28041;&#21450;&#23558;&#21040;&#36798;&#30340;&#20219;&#21153;&#20998;&#37197;&#32473;&#26377;&#38480;&#30340;&#36164;&#28304;&#65292;&#20197;&#26368;&#23567;&#21270;&#20998;&#37197;&#30340;&#24635;&#25104;&#26412;&#12290;&#20026;&#20102;&#36798;&#21040;&#26368;&#20248;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#24517;&#39035;&#20808;&#23545;&#20998;&#37197;&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#12290;&#34429;&#28982;&#23384;&#22312;&#29992;&#20110;&#27169;&#25311;&#12289;&#25191;&#34892;&#21644;&#35299;&#20915;&#38382;&#39064;&#19981;&#21516;&#26041;&#38754;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#22914;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;Petri&#32593;&#26684;&#65292;&#20294;&#19981;&#23384;&#22312;&#19968;&#31181;&#38598;&#25104;&#24314;&#27169;&#25216;&#26415;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Action-Evolution Petri&#32593;&#26684;(A-E PN)&#20316;&#20026;&#19968;&#31181;&#24314;&#27169;&#21644;&#35299;&#20915;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;A-E PN&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#21487;&#20197;&#34920;&#31034;&#21160;&#24577;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#30340;&#25152;&#26377;&#35201;&#32032;&#12290;&#27492;&#22806;&#65292;A-E PN&#27169;&#22411;&#26159;&#21487;&#25191;&#34892;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;(RL)&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#37197;&#31574;&#30053;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#24314;&#27169;&#24037;&#20316;&#12290;&#20026;&#20102;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#21407;&#22411;&#20998;&#37197;&#38382;&#39064;&#30340;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic task assignment involves assigning arriving tasks to a limited number of resources in order to minimize the overall cost of the assignments. To achieve optimal task assignment, it is necessary to model the assignment problem first. While there exist separate formalisms, specifically Markov Decision Processes and (Colored) Petri Nets, to model, execute, and solve different aspects of the problem, there is no integrated modeling technique. To address this gap, this paper proposes Action-Evolution Petri Nets (A-E PN) as a framework for modeling and solving dynamic task assignment problems. A-E PN provides a unified modeling technique that can represent all elements of dynamic task assignment problems. Moreover, A-E PN models are executable, which means they can be used to learn close-to-optimal assignment policies through Reinforcement Learning (RL) without additional modeling effort. To evaluate the framework, we define a taxonomy of archetypical assignment problems. We show for 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#39044;&#35757;&#32451;&#21644;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#21516;&#26102;&#21033;&#29992;&#22522;&#20110;&#30456;&#20851;&#24615;&#20248;&#21270;&#27169;&#22411;&#26500;&#24314;&#22810;&#23618;&#32593;&#32476;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#25552;&#39640;&#39118;&#36895;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24314;&#27169;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01986</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#20248;&#21270;&#30340;&#39118;&#36895;&#39044;&#27979;&#28145;&#24230;&#23398;&#20064;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Correlation-optimized Deep Learning Method for Wind Speed Forecast. (arXiv:2306.01986v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01986
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#39044;&#35757;&#32451;&#21644;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#21516;&#26102;&#21033;&#29992;&#22522;&#20110;&#30456;&#20851;&#24615;&#20248;&#21270;&#27169;&#22411;&#26500;&#24314;&#22810;&#23618;&#32593;&#32476;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#25552;&#39640;&#39118;&#36895;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24314;&#27169;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39118;&#21147;&#21457;&#30005;&#35013;&#26426;&#29575;&#30340;&#22686;&#21152;&#65292;&#32473;&#20840;&#29699;&#33021;&#28304;&#31995;&#32479;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#30830;&#20445;&#30005;&#21147;&#31995;&#32479;&#30340;&#21487;&#38752;&#36816;&#34892;&#65292;&#38656;&#35201;&#20934;&#30830;&#39044;&#27979;&#39118;&#21147;&#28065;&#36718;&#26426;&#30340;&#39118;&#36895;&#21644;&#21151;&#29575;&#12290;&#30446;&#21069;&#65292;&#28145;&#24230;&#23398;&#20064;&#34987;&#36880;&#28176;&#24212;&#29992;&#20110;&#39118;&#36895;&#39044;&#27979;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#30828;&#20214;&#38480;&#21046;&#31561;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#23604;&#23596;&#20043;&#22788;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28151;&#21512;&#20102;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#30693;&#35782;&#23398;&#20064;&#26694;&#26550;&#30340;&#25968;&#25454;&#34920;&#29616;&#21644;&#24314;&#27169;&#12290;&#20026;&#20102;&#24418;&#25104;&#30693;&#35782;&#21644;&#23545;&#24212;&#21560;&#25910;&#22120;&#65292;&#21407;&#22987;&#25968;&#25454;&#32463;&#30001;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#20248;&#21270;&#27169;&#22411;&#39044;&#22788;&#29702;&#65292;&#26500;&#24314;&#22810;&#23618;&#32593;&#32476;&#65288;&#30693;&#35782;&#65289;&#65292;&#34987;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;Seq2Seq&#65289;&#27169;&#22411;&#21560;&#25910;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing installation rate of wind power poses great challenges to the global power system. In order to ensure the reliable operation of the power system, it is necessary to accurately forecast the wind speed and power of the wind turbines. At present, deep learning is progressively applied to the wind speed prediction. Nevertheless, the recent deep learning methods still reflect the embarrassment for practical applications due to model interpretability and hardware limitation. To this end, a novel deep knowledge-based learning method is proposed in this paper. The proposed method hybridizes pre-training method and auto-encoder structure to improve data representation and modeling of the deep knowledge-based learning framework. In order to form knowledge and corresponding absorbers, the original data is preprocessed by an optimization model based on correlation to construct multi-layer networks (knowledge) which are absorbed by sequence to sequence (Seq2Seq) models. Specifically,
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19452</link><description>&lt;p&gt;
&#26356;&#22823;&#12289;&#26356;&#22909;&#12289;&#26356;&#24555;&#65306;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#30340;&#20154;&#31867;&#32423;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19452
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;BBF&#30340;&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#23454;&#29616;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;BBF&#20381;&#38752;&#31070;&#32463;&#32593;&#32476;&#30340;&#20215;&#20540;&#20272;&#35745;&#25193;&#23637;&#20197;&#21450;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#65292;&#22312;&#36981;&#24490;&#26679;&#26412;&#39640;&#25928;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#31181;&#25193;&#23637;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26356;&#26032;ALE&#19978;&#26679;&#26412;&#39640;&#25928;&#30340;RL&#30740;&#31350;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#22312;https://github.com/google-research/google-research/tree/master/bigger_better_faster&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;BigVideo&#65292; &#38598;&#25104;&#20102;4.5 million&#21477;&#23376;&#23545;&#21644;9981&#23567;&#26102;&#35270;&#39057;&#65292;&#35774;&#35745;&#20102;&#26377;&#27495;&#20041;&#21644;&#26126;&#30830;&#30340;&#27979;&#35797;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35270;&#35273;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;NMT&#27169;&#22411;&#30340;BLEU&#12289;BLEURT&#21644;COMET&#24471;&#20998;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27495;&#20041;&#65292;&#25968;&#25454;&#38598;&#21644;&#32763;&#35793;&#27169;&#22411;&#20844;&#24320;&#21487;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.18326</link><description>&lt;p&gt;
BigVideo:&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#30340;&#22823;&#35268;&#27169;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation. (arXiv:2305.18326v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;BigVideo&#65292; &#38598;&#25104;&#20102;4.5 million&#21477;&#23376;&#23545;&#21644;9981&#23567;&#26102;&#35270;&#39057;&#65292;&#35774;&#35745;&#20102;&#26377;&#27495;&#20041;&#21644;&#26126;&#30830;&#30340;&#27979;&#35797;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35270;&#35273;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;NMT&#27169;&#22411;&#30340;BLEU&#12289;BLEURT&#21644;COMET&#24471;&#20998;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27495;&#20041;&#65292;&#25968;&#25454;&#38598;&#21644;&#32763;&#35793;&#27169;&#22411;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;BigVideo&#65292;&#20197;&#20419;&#36827;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#30340;&#30740;&#31350;&#12290;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;How2&#21644;VaTeX&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;BigVideo&#36229;&#36807;10&#20493;&#65292;&#21253;&#25324;450&#19975;&#20010;&#21477;&#23376;&#23545;&#21644;9981&#23567;&#26102;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#26377;&#24847;&#35774;&#35745;&#30340;&#27979;&#35797;&#38598;&#26469;&#39564;&#35777;&#35270;&#35273;&#20449;&#24687;&#30340;&#24517;&#35201;&#24615;&#65306;&#19968;&#20010;&#26159;&#19968;&#20010;&#26377;&#27495;&#20041;&#35789;&#30340;&#19981;&#30830;&#23450;&#38598;&#21512;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#20854;&#20013;&#25991;&#26412;&#19978;&#19979;&#25991;&#23545;&#20110;&#32763;&#35793;&#26159;&#33258;&#21253;&#21547;&#30340;&#26126;&#30830;&#38598;&#21512;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#24314;&#27169;&#25991;&#26412;&#21644;&#35270;&#39057;&#20849;&#20139;&#30340;&#20849;&#21516;&#35821;&#20041;&#65292;&#25105;&#20204;&#22312;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;BigVideo&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65306;a&#65289;&#35270;&#35273;&#20449;&#24687;&#22312;&#27495;&#20041;&#21644;&#26126;&#30830;&#30340;&#27979;&#35797;&#38598;&#19978;&#22987;&#32456;&#25552;&#39640;NMT&#27169;&#22411;&#30340;BLEU&#12289;BLEURT&#21644;COMET&#24471;&#20998;&#12290;b&#65289;&#35270;&#35273;&#20449;&#24687;&#23545;&#20110;&#26415;&#35821;&#23450;&#20301;&#24471;&#20998;&#21644;&#20154;&#24037;&#35780;&#20272;&#32780;&#35328;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27495;&#20041;&#65292;&#19982;&#24378;&#25991;&#26412;&#22522;&#32447;&#30456;&#27604;&#12290;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#32763;&#35793;&#27169;&#22411;&#37117;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a large-scale video subtitle translation dataset, BigVideo, to facilitate the study of multi-modality machine translation. Compared with the widely used How2 and VaTeX datasets, BigVideo is more than 10 times larger, consisting of 4.5 million sentence pairs and 9,981 hours of videos. We also introduce two deliberately designed test sets to verify the necessity of visual information: Ambiguous with the presence of ambiguous words, and Unambiguous in which the text context is self-contained for translation. To better model the common semantics shared across texts and videos, we introduce a contrastive learning method in the cross-modal encoder. Extensive experiments on the BigVideo show that: a) Visual information consistently improves the NMT model in terms of BLEU, BLEURT, and COMET on both Ambiguous and Unambiguous test sets. b) Visual information helps disambiguation, compared to the strong text baseline on terminology-targeted scores and human evaluation. Dataset and our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; Vision Transformer &#21387;&#32553;&#26041;&#27861;&#65292;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#19978;&#36827;&#34892;&#20102;&#26032;&#30340;&#25506;&#31350;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17235</link><description>&lt;p&gt;
COMCAT&#65306;&#39640;&#25928;&#21387;&#32553;&#21644;&#33258;&#23450;&#20041;&#27880;&#24847;&#21147;&#35270;&#35273;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models. (arXiv:2305.17235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; Vision Transformer &#21387;&#32553;&#26041;&#27861;&#65292;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#19978;&#36827;&#34892;&#20102;&#26032;&#30340;&#25506;&#31350;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#20363;&#22914;Vision Transformer&#65288;ViT&#65289;&#21450;&#20854;&#21464;&#20307;&#65292;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#20852;&#30340;&#26550;&#26500;&#23384;&#22312;&#30528;&#27169;&#22411;&#23610;&#23544;&#22823;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#31350;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#20197;&#20016;&#23500;&#33719;&#21462;&#32039;&#20945;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#24037;&#20855;&#38598;&#12290;&#22522;&#20110;&#23545;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#30340;&#26032;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;ViT&#21387;&#32553;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#26041;&#27861;&#12290;&#22312;ImageNet&#19978;&#23545;DeiT-small&#21644;DeiT-base&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#21363;&#20351;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#27604;&#29616;&#26377;&#26041;&#27861;&#39640;0.45&#65285;&#21644;0.76&#65285;&#30340;top-1&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods. For compressing DeiT-small and DeiT-base models on ImageNet, our proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters. Our fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#28436;&#21270;&#31070;&#32463;&#20803;&#20013;&#24515;&#21442;&#25968;&#26469;&#33719;&#24471;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#25191;&#34892;&#22797;&#26434;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#28436;&#21270;&#31070;&#32463;&#21442;&#25968;&#23601;&#36275;&#20197;&#20351;&#20195;&#29702;&#22312;&#19981;&#20248;&#21270;&#20219;&#20309;&#31361;&#35302;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15945</link><description>&lt;p&gt;
&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#36890;&#36807;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#30340;&#28436;&#21270;&#23398;&#20064;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning to Act through Evolution of Neural Diversity in Random Neural Networks. (arXiv:2305.15945v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#28436;&#21270;&#31070;&#32463;&#20803;&#20013;&#24515;&#21442;&#25968;&#26469;&#33719;&#24471;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#25191;&#34892;&#22797;&#26434;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#28436;&#21270;&#31070;&#32463;&#21442;&#25968;&#23601;&#36275;&#20197;&#20351;&#20195;&#29702;&#22312;&#19981;&#20248;&#21270;&#20219;&#20309;&#31361;&#35302;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#31070;&#32463;&#31995;&#32479;&#30001;&#19981;&#21516;&#31867;&#21035;&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#32593;&#32476;&#26500;&#25104;&#65292;&#23427;&#20204;&#26159;&#22810;&#26679;&#21270;&#12289;&#22797;&#26434;&#30340;&#20449;&#24687;&#22788;&#29702;&#22120;&#12290;&#22312;&#22823;&#22810;&#25968;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#35745;&#31639;&#34987;&#25277;&#35937;&#20026;&#19968;&#20010;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#24120;&#26159;&#22312;&#25972;&#20010;&#32593;&#32476;&#25110;&#21516;&#19968;&#23618;&#20013;&#30340;&#25152;&#26377;&#31070;&#32463;&#20803;&#20043;&#38388;&#20849;&#20139;&#30340;&#65307;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20027;&#35201;&#20851;&#27880;&#31361;&#35302;&#26435;&#37325;&#30340;&#20248;&#21270;&#12290;&#26412;&#25991;&#25512;&#33616;&#36890;&#36807;&#20248;&#21270;&#31070;&#32463;&#20803;&#20013;&#24515;&#21442;&#25968;&#26469;&#33719;&#24471;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#25191;&#34892;&#22797;&#26434;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20165;&#36890;&#36807;&#28436;&#21270;&#31070;&#32463;&#21442;&#25968;&#21363;&#21487;&#20351;&#20195;&#29702;&#22312;&#19981;&#20248;&#21270;&#20219;&#20309;&#31361;&#35302;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#34429;&#28982;&#19981;&#33268;&#21147;&#20110;&#31934;&#30830;&#30340;&#29983;&#29289;&#27169;&#22411;&#65292;&#20294;&#36890;&#36807;&#27604;&#24403;&#21069;&#24120;&#35265;&#30340;&#20570;&#27861;&#26356;&#22823;&#31243;&#24230;&#22320;&#23545;&#31070;&#32463;&#20803;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#33021;&#22815;&#25506;&#35752;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#22810;&#26679;&#24615;&#25552;&#20379;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological nervous systems consist of networks of diverse, sophisticated information processors in the form of neurons of different classes. In most artificial neural networks (ANNs), neural computation is abstracted to an activation function that is usually shared between all neurons within a layer or even the whole network; training of ANNs focuses on synaptic optimization. In this paper, we propose the optimization of neuro-centric parameters to attain a set of diverse neurons that can perform complex computations. Demonstrating the promise of the approach, we show that evolving neural parameters alone allows agents to solve various reinforcement learning tasks without optimizing any synaptic weights. While not aiming to be an accurate biological model, parameterizing neurons to a larger degree than the current common practice, allows us to ask questions about the computational abilities afforded by neural diversity in random neural networks. The presented results open up interestin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LaDI-VTON&#65292;&#19968;&#31181;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#25991;&#26412;&#21453;&#28436;&#32452;&#20214;&#22686;&#24378;&#34394;&#25311;&#35797;&#31359;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#36136;&#37327;&#36824;&#21407;&#21830;&#24215;&#26381;&#35013;&#30340;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2305.13501</link><description>&lt;p&gt;
LaDI-VTON: &#28508;&#22312;&#25193;&#25955;&#25991;&#26412;&#21453;&#28436;&#22686;&#24378;&#34394;&#25311;&#35797;&#31359;
&lt;/p&gt;
&lt;p&gt;
LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On. (arXiv:2305.13501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LaDI-VTON&#65292;&#19968;&#31181;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#25991;&#26412;&#21453;&#28436;&#32452;&#20214;&#22686;&#24378;&#34394;&#25311;&#35797;&#31359;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#36136;&#37327;&#36824;&#21407;&#21830;&#24215;&#26381;&#35013;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#21644;&#20803;&#23431;&#23449;&#36825;&#20004;&#20010;&#19981;&#26029;&#21457;&#23637;&#21464;&#21270;&#30340;&#39046;&#22495;&#32487;&#32493;&#23547;&#27714;&#21019;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#28040;&#36153;&#32773;&#20307;&#39564;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#32593;&#32476;&#33021;&#22815;&#21019;&#36896;&#20986;&#38750;&#24120;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#34394;&#25311;&#35797;&#31359;&#65292;&#23601;&#26159;&#29983;&#25104;&#19968;&#20010;&#30446;&#26631;&#27169;&#22411;&#31359;&#30528;&#21830;&#24215;&#20013;&#30340;&#26576;&#20214;&#26381;&#35013;&#30340;&#26032;&#22270;&#29255;&#65292;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;LaDI-VTON&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#25991;&#26412;&#21453;&#28436;&#22686;&#24378;&#30340;&#34394;&#25311;&#35797;&#31359;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20381;&#36182;&#20110;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#19968;&#20010;&#26032;&#30340;&#38468;&#21152;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36339;&#36807;&#36830;&#25509;&#26469;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#65292;&#20445;&#25345;&#27169;&#22411;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20445;&#25345;&#21830;&#24215;&#26381;&#35013;&#30340;&#36136;&#22320;&#21644;&#32454;&#33410;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#21453;&#28436;&#32452;&#20214;&#65292;&#21487;&#20197;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;&#28508;&#22312;&#25193;&#25955;&#31354;&#38388;&#65292;&#23454;&#29616;&#26381;&#35013;&#30340;&#39640;&#36136;&#37327;&#32454;&#33410;&#36824;&#21407;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly evolving fields of e-commerce and metaverse continue to seek innovative approaches to enhance the consumer experience. At the same time, recent advancements in the development of diffusion models have enabled generative networks to create remarkably realistic images. In this context, image-based virtual try-on, which consists in generating a novel image of a target model wearing a given in-shop garment, has yet to capitalize on the potential of these powerful generative solutions. This work introduces LaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for the Virtual Try-ON task. The proposed architecture relies on a latent diffusion model extended with a novel additional autoencoder module that exploits learnable skip connections to enhance the generation process preserving the model's characteristics. To effectively maintain the texture and details of the in-shop garment, we propose a textual inversion component that can map the visual features of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#36951;&#20256;&#31639;&#27861;&#22312;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;&#32463;&#36807;&#39564;&#35777;&#30340;NSGA-II&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.13459</link><description>&lt;p&gt;
&#36951;&#20256;&#31639;&#27861;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The First Proven Performance Guarantees for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) on a Combinatorial Optimization Problem. (arXiv:2305.13459v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#36951;&#20256;&#31639;&#27861;&#22312;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;&#32463;&#36807;&#39564;&#35777;&#30340;NSGA-II&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#65288;NSGA-II&#65289;&#26159;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#31361;&#20986;&#31639;&#27861;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#35813;&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#24050;&#34987;&#35777;&#26126;&#65292;&#20294;&#20165;&#36866;&#29992;&#20110;&#21512;&#25104;&#22522;&#20934;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;NP&#23436;&#20840;&#30340;&#21452;&#30446;&#26368;&#23567;&#29983;&#25104;&#26641;&#38382;&#39064;&#25552;&#20379;&#20102;&#39318;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;NSGA-II&#24615;&#33021;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;NSGA-II&#20351;&#29992;&#31181;&#32676;&#22823;&#23567;$N \ge 4((n-1) w_\max + 1)$&#65292;&#33021;&#22815;&#22312;&#26395;&#22312;$O(m^2 n w_\max \log(n w_\max))$&#27425;&#36845;&#20195;&#20013;&#65292;&#35745;&#31639;&#20986;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25152;&#26377;&#26497;&#31471;&#28857;&#65292;&#20854;&#20013;$n$&#26159;&#39030;&#28857;&#25968;&#65292;$m$&#26159;&#36793;&#25968;&#65292;$w_\max$&#26159;&#38382;&#39064;&#23454;&#20363;&#20013;&#30340;&#26368;&#22823;&#36793;&#26435;&#12290;&#35813;&#32467;&#26524;&#36890;&#36807;&#25968;&#23398;&#25163;&#27573;&#35777;&#23454;&#20102;NSGA-II&#30340;&#33391;&#22909;&#25928;&#26524;&#24182;&#19988;&#34920;&#26126;&#21487;&#20197;&#23558;&#27492;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Non-dominated Sorting Genetic Algorithm-II (NSGA-II) is one of the most prominent algorithms to solve multi-objective optimization problems. Recently, the first mathematical runtime guarantees have been obtained for this algorithm, however only for synthetic benchmark problems.  In this work, we give the first proven performance guarantees for a classic optimization problem, the NP-complete bi-objective minimum spanning tree problem. More specifically, we show that the NSGA-II with population size $N \ge 4((n-1) w_{\max} + 1)$ computes all extremal points of the Pareto front in an expected number of $O(m^2 n w_{\max} \log(n w_{\max}))$ iterations, where $n$ is the number of vertices, $m$ the number of edges, and $w_{\max}$ is the maximum edge weight in the problem instance. This result confirms, via mathematical means, the good performance of the NSGA-II observed empirically. It also shows that mathematical analyses of this algorithm are not only possible for synthetic benchmark pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SPECTRA&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24212;&#29992;&#20102;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#65292;&#21516;&#26102;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#65292;&#29992;&#20110;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.11579</link><description>&lt;p&gt;
&#24102;&#26377;&#26174;&#24335;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#29992;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment. (arXiv:2305.11579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SPECTRA&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24212;&#29992;&#20102;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#65292;&#21516;&#26102;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#65292;&#29992;&#20110;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#35768;&#22810;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#38024;&#23545;&#19968;&#20010;&#25110;&#20004;&#20010;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20102;&#23450;&#21046;&#65292;&#20294;&#26410;&#33021;&#24449;&#26381;&#21508;&#31181;&#35821;&#38899;&#25991;&#26412;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#26410;&#33021;&#25506;&#32034;&#23545;&#35805;&#20013;&#30340;&#24773;&#22659;&#20449;&#24687;&#20197;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#26174;&#24335;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;SPECTRA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#32771;&#34385;&#35821;&#38899;&#27169;&#24577;&#30340;&#26102;&#38388;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#12290;&#36825;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#26088;&#22312;&#39044;&#27979;&#30456;&#24212;&#35821;&#38899;&#27874;&#24418;&#20013;&#27599;&#20010;&#25991;&#26412;&#21333;&#35789;&#30340;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23398;&#20064;&#21475;&#35821;&#23545;&#35805;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, speech-text pre-training methods have shown remarkable success in many speech and natural language processing tasks. However, most previous pre-trained models are usually tailored for one or two specific tasks, but fail to conquer a wide range of speech-text tasks. In addition, existing speech-text pre-training methods fail to explore the contextual information within a dialogue to enrich utterance representations. In this paper, we propose Speech-text dialog Pre-training for spoken dialog understanding with ExpliCiT cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog pre-training model. Concretely, to consider the temporality of speech modality, we design a novel temporal position prediction task to capture the speech-text alignment. This pre-training task aims to predict the start and end time of each textual word in the corresponding speech waveform. In addition, to learn the characteristics of spoken dialogs, we generalize a response selection task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;IBC&#65289;&#65292;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#21644;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#20808;&#21069;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#31232;&#30095;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#30340;&#29615;&#22659;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09943</link><description>&lt;p&gt;
&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#22686;&#24378;&#23398;&#20064;&#65306;&#38544;&#24335;&#21452;&#21521;&#35838;&#31243;&#27861;
&lt;/p&gt;
&lt;p&gt;
Demonstration-free Autonomous Reinforcement Learning via Implicit and Bidirectional Curriculum. (arXiv:2305.09943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;IBC&#65289;&#65292;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#21644;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#20808;&#21069;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#31232;&#30095;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#30340;&#29615;&#22659;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#20165;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#33719;&#24471;&#22797;&#26434;&#25216;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20551;&#35774;&#22312;&#27599;&#20010;&#21608;&#26399;&#32467;&#26463;&#26102;&#37117;&#21487;&#20197;&#36731;&#26131;&#22320;&#22238;&#21040;&#21021;&#22987;&#29366;&#24577;&#12290;&#36825;&#31181;&#20551;&#35774;&#22952;&#30861;&#20102;&#20855;&#36523;&#20195;&#29702;&#30340;&#33258;&#20027;&#23398;&#20064;&#65292;&#22240;&#20026;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#36827;&#34892;&#37325;&#32622;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#21644;&#32321;&#29712;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#33021;&#22815;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#65288;ARL&#65289;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ARL&#26041;&#27861;&#21463;&#21040;&#20854;&#23545;&#20808;&#21069;&#25968;&#25454;&#30340;&#20381;&#36182;&#30340;&#38480;&#21046;&#65292;&#26080;&#27861;&#22312;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#21644;&#21452;&#21521;&#35838;&#31243;&#30340;&#26080;&#28436;&#31034;ARL&#31639;&#27861;&#65288;IBC&#65289;&#12290;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#20197;&#21450;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#29978;&#33267;&#27604;&#21033;&#29992;&#28436;&#31034;&#30340;&#26041;&#27861;&#36824;&#35201;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) has achieved great success in acquiring complex skills solely from environmental interactions, it assumes that resets to the initial state are readily available at the end of each episode. Such an assumption hinders the autonomous learning of embodied agents due to the time-consuming and cumbersome workarounds for resetting in the physical world. Hence, there has been a growing interest in autonomous RL (ARL) methods that are capable of learning from non-episodic interactions. However, existing works on ARL are limited by their reliance on prior data and are unable to learn in environments where task-relevant interactions are sparse. In contrast, we propose a demonstration-free ARL algorithm via Implicit and Bi-directional Curriculum (IBC). With an auxiliary agent that is conditionally activated upon learning progress and a bidirectional goal curriculum based on optimal transport, our method outperforms previous methods, even the ones that leverage dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CoFe&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;</title><link>http://arxiv.org/abs/2305.04835</link><description>&lt;p&gt;
&#22914;&#20309;&#24433;&#21709;&#19978;&#19979;&#25991;&#33539;&#20363;&#22312;&#32452;&#21512;&#36890;&#29992;&#24615;&#20013;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Do In-Context Examples Affect Compositional Generalization?. (arXiv:2305.04835v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CoFe&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#29702;&#35299;&#30475;&#19981;&#35265;&#30340;&#24050;&#30693;&#21407;&#22987;&#32452;&#21512;&#8212;&#8212;&#26159;&#20154;&#31867;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25512;&#29702;&#33021;&#21147;&#12290;AI&#31038;&#21306;&#20027;&#35201;&#36890;&#36807;&#22312;&#35768;&#22810;&#35757;&#32451;&#26679;&#26412;&#19978;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#26469;&#30740;&#31350;&#36825;&#31181;&#33021;&#21147;&#65292;&#28982;&#32780;&#36824;&#19981;&#28165;&#26970;&#19978;&#19979;&#25991;&#23398;&#20064;&#8212;&#8212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#35201;&#23569;&#26679;&#26412;&#33539;&#24335;&#8212;&#8212;&#26159;&#21542;&#23637;&#31034;&#32452;&#21512;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoFe&#65292;&#19968;&#20010;&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32452;&#21512;&#27867;&#21270;&#24615;&#33021;&#24456;&#23481;&#26131;&#21463;&#21040;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#30740;&#31350;&#38382;&#39064;&#65306;&#20160;&#20040;&#26159;&#22312;&#32452;&#21512;&#27867;&#21270;&#20013;&#21046;&#20316;&#22909;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#30456;&#20284;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32452;&#21512;&#36890;&#29992;&#24615;&#20013;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional generalization--understanding unseen combinations of seen primitives--is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning--the prevailing few-shot paradigm based on large language models--exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEGA&#30340;&#38170;&#28857;&#35270;&#22270;&#29992;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65292;&#35813;&#35270;&#22270;&#36890;&#36807;&#32467;&#26500;&#29109;&#24341;&#23548;&#20197;&#20445;&#25345;&#36755;&#20837;&#22270;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.04501</link><description>&lt;p&gt;
SEGA&#65306;&#29992;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#32467;&#26500;&#29109;&#24341;&#23548;&#30340;&#38170;&#28857;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
SEGA: Structural Entropy Guided Anchor View for Graph Contrastive Learning. (arXiv:2305.04501v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEGA&#30340;&#38170;&#28857;&#35270;&#22270;&#29992;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65292;&#35813;&#35270;&#22270;&#36890;&#36807;&#32467;&#26500;&#29109;&#24341;&#23548;&#20197;&#20445;&#25345;&#36755;&#20837;&#22270;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#8220;&#35270;&#22270;&#8221;&#30340;&#36873;&#25321;&#25511;&#21046;&#30528;&#34920;&#31034;&#25429;&#33719;&#30340;&#20449;&#24687;&#24182;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#30340;&#22270;&#20687;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#38543;&#26426;&#25439;&#22351;&#25110;&#23398;&#20064;&#26469;&#20135;&#29983;&#35270;&#22270;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#20449;&#24687;&#30340;&#20002;&#22833;&#21644;&#35821;&#20041;&#20449;&#24687;&#30340;&#21464;&#21270;&#12290;&#23545;&#20110;&#23545;&#27604;&#23398;&#20064;&#26469;&#35828;&#65292;&#20445;&#25345;&#36755;&#20837;&#22270;&#24418;&#30340;&#22522;&#26412;&#20449;&#24687;&#30340;&#38170;&#28857;&#35270;&#22270;&#24456;&#23569;&#21463;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#36825;&#20010;&#38170;&#28857;&#35270;&#22270;&#30340;&#23450;&#20041;&#65307;&#25442;&#21477;&#35805;&#35828;&#65292;&#8220;&#20855;&#26377;&#36755;&#20837;&#22270;&#30340;&#22522;&#26412;&#20449;&#24687;&#30340;&#38170;&#28857;&#35270;&#22270;&#24212;&#35813;&#20855;&#26377;&#26368;&#23567;&#30340;&#32467;&#26500;&#19981;&#30830;&#23450;&#24615;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#26500;&#29109;&#23454;&#29616;&#20102;&#35813;&#38170;&#28857;&#35270;&#22270;&#65292;&#31216;&#20026;SEGA&#65292;&#29992;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39564;&#35777;&#65292;&#21253;&#25324;&#22312;&#26080;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#19979;&#30340;&#22270;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrastive learning, the choice of ``view'' controls the information that the representation captures and influences the performance of the model. However, leading graph contrastive learning methods generally produce views via random corruption or learning, which could lead to the loss of essential information and alteration of semantic information. An anchor view that maintains the essential information of input graphs for contrastive learning has been hardly investigated. In this paper, based on the theory of graph information bottleneck, we deduce the definition of this anchor view; put differently, \textit{the anchor view with essential information of input graph is supposed to have the minimal structural uncertainty}. Furthermore, guided by structural entropy, we implement the anchor view, termed \textbf{SEGA}, for graph contrastive learning. We extensively validate the proposed anchor view on various benchmarks regarding graph classification under unsupervised, semi-supervise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>Queer in AI&#26159;&#19968;&#20010;&#22522;&#20110;&#31038;&#21306;&#21442;&#19982;&#30340;AI&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#25298;&#32477;&#31561;&#32423;&#21046;&#24230;&#32780;&#36873;&#25321;&#21435;&#20013;&#24515;&#21270;&#65292;&#22312;&#37239;&#20799;&#31038;&#32676;&#20869;&#37096;&#24314;&#31435;&#20102;&#25588;&#21161;&#21644;&#39033;&#30446;&#65292;&#21516;&#26102;&#21162;&#21147;&#25913;&#21464;&#37239;&#20799;&#31038;&#32676;&#22806;&#30340;&#21442;&#19982;&#32773;&#21644;&#26426;&#26500;&#12290;&#36890;&#36807;&#22521;&#32946;AI&#21442;&#19982;&#25991;&#21270;&#65292;&#27426;&#36814;&#21644;&#36171;&#26435;&#36793;&#32536;&#21270;&#21442;&#19982;&#32773;&#65292;&#20026;AI&#30340;&#21442;&#19982;&#24335;&#35774;&#35745;&#20570;&#20986;&#20102;&#26356;&#24191;&#27867;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.16972</link><description>&lt;p&gt;
Queer In AI:&#22522;&#20110;&#31038;&#21306;&#21442;&#19982;&#30340;AI&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Queer In AI: A Case Study in Community-Led Participatory AI. (arXiv:2303.16972v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16972
&lt;/p&gt;
&lt;p&gt;
Queer in AI&#26159;&#19968;&#20010;&#22522;&#20110;&#31038;&#21306;&#21442;&#19982;&#30340;AI&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#25298;&#32477;&#31561;&#32423;&#21046;&#24230;&#32780;&#36873;&#25321;&#21435;&#20013;&#24515;&#21270;&#65292;&#22312;&#37239;&#20799;&#31038;&#32676;&#20869;&#37096;&#24314;&#31435;&#20102;&#25588;&#21161;&#21644;&#39033;&#30446;&#65292;&#21516;&#26102;&#21162;&#21147;&#25913;&#21464;&#37239;&#20799;&#31038;&#32676;&#22806;&#30340;&#21442;&#19982;&#32773;&#21644;&#26426;&#26500;&#12290;&#36890;&#36807;&#22521;&#32946;AI&#21442;&#19982;&#25991;&#21270;&#65292;&#27426;&#36814;&#21644;&#36171;&#26435;&#36793;&#32536;&#21270;&#21442;&#19982;&#32773;&#65292;&#20026;AI&#30340;&#21442;&#19982;&#24335;&#35774;&#35745;&#20570;&#20986;&#20102;&#26356;&#24191;&#27867;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;Queer in AI&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#31038;&#21306;&#21442;&#19982;&#24335;AI&#35774;&#35745;&#30340;&#23454;&#36341;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#31038;&#21306;&#21442;&#19982;&#35774;&#35745;&#21644;&#20132;&#21449;&#24615;&#21407;&#21017;&#22914;&#20309;&#22312;&#22810;&#24180;&#37324;&#22312;&#36825;&#20010;&#31038;&#32676;&#20013;&#33804;&#33469;&#21644;&#22609;&#36896;&#20102;&#20854;&#39033;&#30446;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35813;&#32452;&#32455;&#22312;&#27492;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#19981;&#21516;&#25361;&#25112;&#65292;&#23457;&#35270;&#20102;&#35813;&#32452;&#32455;&#22312;&#23454;&#29616;&#21442;&#19982;&#24615;&#19982;&#20132;&#21449;&#24615;&#21407;&#21017;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#35780;&#20272;&#20102;&#35813;&#32452;&#32455;&#30340;&#24433;&#21709;&#12290;Queer in AI&#36890;&#36807;&#25298;&#32477;&#31561;&#32423;&#21046;&#24230;&#32780;&#36873;&#25321;&#21435;&#20013;&#24515;&#21270;&#65292;&#36890;&#36807;&#23558;&#25588;&#21161;&#21644;&#39033;&#30446;&#24314;&#35774;&#24314;&#31435;&#22312;&#37239;&#20799;&#31038;&#32676;&#20869;&#37096;&#12289;&#30001;&#37239;&#20799;&#31038;&#32676;&#20869;&#25104;&#21592;&#26469;&#36127;&#36131;&#30340;&#26041;&#24335;&#65292;&#20197;&#21450;&#21162;&#21147;&#25913;&#21464;&#37239;&#20799;&#31038;&#32676;&#22806;&#30340;&#21442;&#19982;&#32773;&#21644;&#26426;&#26500;&#65292;&#20026;&#21442;&#19982;&#24335;&#26041;&#27861;&#30340;&#20174;&#19994;&#32773;&#21644;&#29702;&#35770;&#23478;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#21644;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25512;&#27979;&#20687;Queer in AI&#36825;&#26679;&#30340;&#31038;&#21306;&#22914;&#20309;&#36890;&#36807;&#22521;&#32946;AI&#30340;&#21442;&#19982;&#25991;&#21270;&#65292;&#27426;&#36814;&#21644;&#36171;&#26435;&#36793;&#32536;&#21270;&#30340;&#21442;&#19982;&#32773;&#65292;&#25209;&#35780;&#36139;&#30240;&#21644;&#21093;&#21066;&#24615;&#34920;&#36848;&#31561;&#26041;&#38754;&#65292;&#20026;AI&#30340;&#21442;&#19982;&#24335;&#35774;&#35745;&#20570;&#20986;&#20102;&#26356;&#24191;&#27867;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community's programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization's impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative parti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35745;&#31639;&#35770;&#35777;&#23454;&#29616;&#20195;&#29702;&#38388;&#20914;&#31361;&#35299;&#20915;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;XAI&#65292;&#33021;&#22815;&#21160;&#24577;&#20849;&#20139;&#20195;&#29702;&#30340;&#20449;&#24687;&#20197;&#35299;&#20915;&#20195;&#29702;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2303.15022</link><description>&lt;p&gt;
&#36890;&#36807;&#20105;&#35770;&#20132;&#25442;&#23454;&#29616;&#20132;&#20114;&#24335;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interactive Explanations by Conflict Resolution via Argumentative Exchanges. (arXiv:2303.15022v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35745;&#31639;&#35770;&#35777;&#23454;&#29616;&#20195;&#29702;&#38388;&#20914;&#31361;&#35299;&#20915;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;XAI&#65292;&#33021;&#22815;&#21160;&#24577;&#20849;&#20139;&#20195;&#29702;&#30340;&#20449;&#24687;&#20197;&#35299;&#20915;&#20195;&#29702;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#25104;&#29087;&#65292;&#35201;&#27714;&#20026;&#65288;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#65289;&#36755;&#20986;&#25552;&#20379;&#20132;&#20114;&#24335;&#35299;&#37322;&#30340;&#21628;&#22768;&#27491;&#22312;&#22686;&#21152;&#65292;&#20294;&#29616;&#26377;&#25216;&#26415;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;&#35299;&#37322;&#19978;&#12290;&#26412;&#25991;&#30528;&#37325;&#20171;&#32461;&#20102;&#23558;&#20132;&#20114;&#24335;&#35299;&#37322;&#26694;&#26550;&#21270;&#20026;&#20195;&#29702;&#20043;&#38388;&#30340;&#20914;&#31361;&#35299;&#20915;&#65292;&#24182;&#20511;&#21161;&#35745;&#31639;&#35770;&#35777;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;Argumentative eXchanges&#65288;AXs&#65289;&#65292;&#20197;&#22312;&#22810;&#20195;&#29702;&#31995;&#32479;&#20013;&#21160;&#24577;&#20849;&#20139;&#20010;&#20307;&#20195;&#29702;&#30340;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#20013;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#20195;&#29702;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;AXs&#37096;&#32626;&#22312;XAI&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#26426;&#22120;&#21644;&#20154;&#31867;&#20114;&#21160;&#35752;&#35770;&#26426;&#22120;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#35780;&#20272;&#20102;&#20960;&#20010;&#29702;&#35770;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#36866;&#29992;&#20110;XAI&#30340;AXs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#21508;&#31181;&#20195;&#29702;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#25429;&#33719;&#26426;&#22120;&#20013;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#27169;&#24335;&#24182;&#31361;&#20986;&#20854;&#24433;&#21709;&#65289;&#26469;&#23454;&#20363;&#21270;XAI&#30340;AXs&#12290;
&lt;/p&gt;
&lt;p&gt;
As the field of explainable AI (XAI) is maturing, calls for interactive explanations for (the outputs of) AI models are growing, but the state-of-the-art predominantly focuses on static explanations. In this paper, we focus instead on interactive explanations framed as conflict resolution between agents (i.e. AI models and/or humans) by leveraging on computational argumentation. Specifically, we define Argumentative eXchanges (AXs) for dynamically sharing, in multi-agent systems, information harboured in individual agents' quantitative bipolar argumentation frameworks towards resolving conflicts amongst the agents. We then deploy AXs in the XAI setting in which a machine and a human interact about the machine's predictions. We identify and assess several theoretical properties characterising AXs that are suitable for XAI. Finally, we instantiate AXs for XAI by defining various agent behaviours, e.g. capturing counterfactual patterns of reasoning in machines and highlighting the effects
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.04091</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23454;&#29616;&#35270;&#35273;&#25277;&#35937;&#21644;&#25512;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Visual Abstraction and Reasoning through Language. (arXiv:2303.04091v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#23616;&#38480;&#24212;&#29992;&#20013;&#24050;&#32463;&#36798;&#21040;&#20102;&#20154;&#31867;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#23637;&#29616;&#26356;&#24191;&#27867;&#21644;&#26356;&#28789;&#27963;&#30340;&#26234;&#33021;&#12290;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#26088;&#22312;&#35780;&#20272;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65288;DSL&#65289;&#65292;&#29992;&#20110;&#26292;&#21147;&#35299;&#20915;ARC&#20013;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;ARC&#38382;&#39064;&#12290;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Artificial Intelligence (AI) models have achieved human or even superhuman performance in narrowly defined applications, they still struggle to show signs of broader and more flexible intelligence. The Abstraction and Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how close AI systems are to human-like cognitive abilities. Most current approaches rely on carefully handcrafted domain-specific languages (DSLs), which are used to brute-force solutions to the tasks present in ARC. In this work, we propose a general framework for solving ARC based on natural language descriptions of the tasks. While not yet beating state-of-the-art DSL models on ARC, we demonstrate the immense potential of our approach hinted at by the ability to solve previously unsolved tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#26694;&#26550;ExplainableFold&#65292;&#29992;&#20110;&#23545;AlphaFold&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#36827;&#34892;&#35299;&#37322;&#12290;&#25552;&#20379;&#20102;&#25509;&#36817;&#23454;&#39564;&#30340;&#23545;&#27688;&#22522;&#37240;&#23545;3D&#34507;&#30333;&#36136;&#32467;&#26500;&#24433;&#21709;&#30340;&#29702;&#35299;&#65292;&#26377;&#28508;&#21147;&#20419;&#36827;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.11765</link><description>&lt;p&gt;
ExplainableFold&#65306;&#21033;&#29992;&#21487;&#35299;&#37322;AI&#29702;&#35299;AlphaFold&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ExplainableFold: Understanding AlphaFold Prediction with Explainable AI. (arXiv:2301.11765v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#26694;&#26550;ExplainableFold&#65292;&#29992;&#20110;&#23545;AlphaFold&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#36827;&#34892;&#35299;&#37322;&#12290;&#25552;&#20379;&#20102;&#25509;&#36817;&#23454;&#39564;&#30340;&#23545;&#27688;&#22522;&#37240;&#23545;3D&#34507;&#30333;&#36136;&#32467;&#26500;&#24433;&#21709;&#30340;&#29702;&#35299;&#65292;&#26377;&#28508;&#21147;&#20419;&#36827;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ExplainableFold&#65292;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;AI&#26694;&#26550;&#12290;&#23613;&#31649;&#20687;AlphaFold&#36825;&#26679;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#20854;&#39044;&#27979;&#30340;&#22522;&#26412;&#21407;&#22240;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#29983;&#29289;&#23398;&#21407;&#29702;&#21551;&#21457;&#30340;&#21453;&#20107;&#23454;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20174;&#32780;&#23454;&#29616;&#19968;&#20010;&#24178;&#23454;&#39564;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ExplainableFold&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;AlphaFold&#39044;&#27979;&#35299;&#37322;&#65292;&#25552;&#20379;&#25509;&#36817;&#23454;&#39564;&#30340;&#23545;&#27688;&#22522;&#37240;&#23545;3D&#34507;&#30333;&#36136;&#32467;&#26500;&#24433;&#21709;&#30340;&#29702;&#35299;&#12290;&#36825;&#19968;&#26694;&#26550;&#26377;&#28508;&#21147;&#20419;&#36827;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents ExplainableFold, an explainable AI framework for protein structure prediction. Despite the success of AI-based methods such as AlphaFold in this field, the underlying reasons for their predictions remain unclear due to the black-box nature of deep learning models. To address this, we propose a counterfactual learning framework inspired by biological principles to generate counterfactual explanations for protein structure prediction, enabling a dry-lab experimentation approach. Our experimental results demonstrate the ability of ExplainableFold to generate high-quality explanations for AlphaFold's predictions, providing near-experimental understanding of the effects of amino acids on 3D protein structure. This framework has the potential to facilitate a deeper understanding of protein structures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36873;&#39033;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#21160;&#20316;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;&#32467;&#26524;&#26469;&#25299;&#23637;&#33267;&#21487;&#20280;&#32553;&#24615;&#26356;&#22909;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2301.11181</link><description>&lt;p&gt;
&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#28145;&#24230;&#36873;&#39033;&#26041;&#27861;&#29992;&#20110;&#26102;&#38388;&#25193;&#23637;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Deep Laplacian-based Options for Temporally-Extended Exploration. (arXiv:2301.11181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36873;&#39033;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#21160;&#20316;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;&#32467;&#26524;&#26469;&#25299;&#23637;&#33267;&#21487;&#20280;&#32553;&#24615;&#26356;&#22909;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36873;&#25321;&#20855;&#26377;&#20016;&#23500;&#32463;&#39564;&#27969;&#30340;&#25506;&#32034;&#24615;&#21160;&#20316;&#20197;&#36827;&#34892;&#26356;&#22909;&#23398;&#20064;&#26159;&#19968;&#39033;&#22522;&#26412;&#25361;&#25112;&#12290;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#26681;&#25454;&#29305;&#23450;&#30340;&#31574;&#30053;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#36873;&#25321;&#21160;&#20316;&#65292;&#20063;&#31216;&#20026;&#36873;&#39033;&#12290;&#26368;&#36817;&#19968;&#31181;&#29992;&#20110;&#33719;&#21462;&#27492;&#31867;&#25506;&#32034;&#36873;&#39033;&#30340;&#24037;&#20316;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#26412;&#24449;&#20989;&#25968;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#30452;&#21040;&#29616;&#22312;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22270;&#34920;&#39046;&#22495;&#20013;&#22823;&#22810;&#34987;&#38480;&#21046;&#22312;&#20197;&#19979;&#26041;&#38754;&#65306;(1)&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#24050;&#32473;&#20986;&#25110;&#21487;&#20197;&#23436;&#20840;&#20272;&#35745;&#65292;(2)&#22312;&#35813;&#30697;&#38453;&#19978;&#36827;&#34892;&#29305;&#24449;&#20998;&#35299;&#26159;&#21487;&#35745;&#31639;&#30340;&#65292;(3)&#20540;&#20989;&#25968;&#21487;&#20197;&#34987;&#23436;&#20840;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#21333;&#29420;&#30340;&#36873;&#39033;&#21457;&#29616;&#38454;&#27573;&#12290;&#36825;&#20123;&#20551;&#35774;&#22312;&#26681;&#26412;&#19978;&#26159;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#36817;&#30452;&#25509;&#36924;&#36817;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#26412;&#24449;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#30495;&#27491;&#23454;&#29616;&#36873;&#39033;&#26041;&#27861;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting exploratory actions that generate a rich stream of experience for better learning is a fundamental challenge in reinforcement learning (RL). An approach to tackle this problem consists in selecting actions according to specific policies for an extended period of time, also known as options. A recent line of work to derive such exploratory options builds upon the eigenfunctions of the graph Laplacian. Importantly, until now these methods have been mostly limited to tabular domains where (1) the graph Laplacian matrix was either given or could be fully estimated, (2) performing eigendecomposition on this matrix was computationally tractable, and (3) value functions could be learned exactly. Additionally, these methods required a separate option discovery phase. These assumptions are fundamentally not scalable. In this paper we address these limitations and show how recent results for directly approximating the eigenfunctions of the Laplacian can be leveraged to truly scale up o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#27169;&#22411;&#12289;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#24033;&#36923;&#29615;&#22659;&#20013;&#22810;&#20010;&#33258;&#20027;&#36710;&#36742;&#21327;&#21516;&#25191;&#34892;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36824;&#32771;&#34385;&#20102;&#33021;&#37327;&#28040;&#32791;&#21644;&#23481;&#38169;&#31561;&#22240;&#32032;&#65292;&#20197;&#30830;&#20445;&#31995;&#32479;&#36830;&#32493;&#33258;&#36866;&#24212;&#22320;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2212.08230</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#24033;&#36923;&#38382;&#39064;&#30340;&#33021;&#37327;&#24863;&#30693;&#21644;&#23481;&#38169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Energy-aware and Fault-tolerant Deep Reinforcement Learning based approach for Multi-agent Patrolling Problems. (arXiv:2212.08230v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#27169;&#22411;&#12289;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#24033;&#36923;&#29615;&#22659;&#20013;&#22810;&#20010;&#33258;&#20027;&#36710;&#36742;&#21327;&#21516;&#25191;&#34892;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36824;&#32771;&#34385;&#20102;&#33021;&#37327;&#28040;&#32791;&#21644;&#23481;&#38169;&#31561;&#22240;&#32032;&#65292;&#20197;&#30830;&#20445;&#31995;&#32479;&#36830;&#32493;&#33258;&#36866;&#24212;&#22320;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#36866;&#29992;&#20110;&#36830;&#32493;&#21306;&#22495;&#24033;&#36923;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20247;&#22810;&#21407;&#22240;&#65292;&#23547;&#25214;&#26368;&#20248;&#24033;&#36923;&#31574;&#30053;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#39318;&#20808;&#65292;&#24033;&#36923;&#29615;&#22659;&#36890;&#24120;&#36739;&#20026;&#22797;&#26434;&#65292;&#21487;&#33021;&#20250;&#21253;&#21547;&#35832;&#22914;&#39118;&#25110;&#26223;&#35266;&#31561;&#26410;&#30693;&#29615;&#22659;&#22240;&#32032;&#12290;&#20854;&#27425;&#65292;&#33258;&#20027;&#36710;&#36742;&#21487;&#33021;&#20250;&#20986;&#29616;&#25925;&#38556;&#25110;&#30828;&#20214;&#38480;&#21046;&#65292;&#20363;&#22914;&#30005;&#27744;&#23551;&#21629;&#26377;&#38480;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#24033;&#36923;&#22823;&#21306;&#22495;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#26234;&#33021;&#20307;&#21327;&#21516;&#25191;&#34892;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#27169;&#22411;&#12289;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#26234;&#33021;&#20307;&#34987;&#35757;&#32451;&#22312;&#20855;&#26377;&#21508;&#31181;&#26410;&#30693;&#21160;&#24577;&#21644;&#22240;&#32032;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#24033;&#36923;&#12290;&#23427;&#20204;&#21487;&#20197;&#33258;&#21160;&#20805;&#30005;&#20197;&#25903;&#25345;&#36830;&#32493;&#38598;&#20307;&#24033;&#36923;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;&#21516;&#26500;&#22810;&#26234;&#33021;&#20307;&#26550;&#26500;&#65292;&#20854;&#20013;&#25152;&#26377;&#24033;&#36923;&#26234;&#33021;&#20307;&#22522;&#20110;&#20854;&#26412;&#22320;&#35266;&#23519;&#21644;&#20849;&#20139;&#36890;&#20449;&#22312;&#26412;&#22320;&#25191;&#34892;&#30456;&#21516;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#33021;&#37327;&#28040;&#32791;&#21644;&#23481;&#38169;&#31561;&#22240;&#32032;&#65292;&#20197;&#30830;&#20445;&#31995;&#32479;&#36830;&#32493;&#33258;&#36866;&#24212;&#22320;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles are suited for continuous area patrolling problems. However, finding an optimal patrolling strategy can be challenging for many reasons. Firstly, patrolling environments are often complex and can include unknown environmental factors, such as wind or landscape. Secondly, autonomous vehicles can have failures or hardware constraints, such as limited battery life. Importantly, patrolling large areas often requires multiple agents that need to collectively coordinate their actions. In this work, we consider these limitations and propose an approach based on model-free, deep multi-agent reinforcement learning. In this approach, the agents are trained to patrol an environment with various unknown dynamics and factors. They can automatically recharge themselves to support continuous collective patrolling. A distributed homogeneous multi-agent architecture is proposed, where all patrolling agents execute identical policies locally based on their local observations and shar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Huber&#33021;&#37327;&#37327;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#30340;&#26368;&#20339;&#36924;&#36817;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#27979;&#24230;&#19982;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#35813;&#31639;&#27861;&#24050;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2212.08162</link><description>&lt;p&gt;
Huber&#33021;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Huber-energy measure quantization. (arXiv:2212.08162v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Huber&#33021;&#37327;&#37327;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#30340;&#26368;&#20339;&#36924;&#36817;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#27979;&#24230;&#19982;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#35813;&#31639;&#27861;&#24050;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#27979;&#37327;&#37327;&#21270;&#36807;&#31243;&#65292;&#21363;&#19968;&#31181;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;$Q$&#20010;&#29380;&#25289;&#20811;&#20989;&#25968;&#30340;&#24635;&#21644;&#65288;$Q$&#20026;&#37327;&#21270;&#21442;&#25968;&#65289;&#65292;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20026;&#26377;&#38480;&#21464;&#24046;&#27979;&#24230;&#65289;&#30340;&#26368;&#20339;&#36924;&#36817;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#23558;&#21407;&#27979;&#24230;&#19982;&#20854;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26368;&#23567;&#21270;&#26469;&#23454;&#29616;&#65307;&#35813;&#36317;&#31163;&#22522;&#20110;&#36127;&#23450;&#26680;&#26500;&#24314;&#65292;&#24182;&#19988;&#22914;&#26524;&#24517;&#35201;&#65292;&#21487;&#20197;&#23454;&#26102;&#35745;&#31639;&#24182;&#36755;&#20837;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;SGD&#65292;Adam&#31561;&#65289;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#26368;&#20248;&#27979;&#37327;&#37327;&#21270;&#22120;&#30340;&#23384;&#22312;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#38656;&#35201;&#20445;&#35777;&#21512;&#36866;&#34892;&#20026;&#30340;&#26680;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26368;&#20339;&#32447;&#24615;&#26080;&#20559;&#65288;BLUE&#65289;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#24179;&#26041;&#32479;&#35745;&#36317;&#31163;&#65292;&#24182;&#23558;&#23427;&#20204;&#29992;&#20110;&#26080;&#20559;&#31243;&#24207;HEMQ&#20013;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#37327;&#21270;&#12290;&#25105;&#20204;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#27979;&#35797;&#20102;HEMQ
&lt;/p&gt;
&lt;p&gt;
We describe a measure quantization procedure i.e., an algorithm which finds the best approximation of a target probability law (and more generally signed finite variation measure) by a sum of $Q$ Dirac masses ($Q$ being the quantization parameter). The procedure is implemented by minimizing the statistical distance between the original measure and its quantized version; the distance is built from a negative definite kernel and, if necessary, can be computed on the fly and feed to a stochastic optimization algorithm (such as SGD, Adam, ...). We investigate theoretically the fundamental questions of existence of the optimal measure quantizer and identify what are the required kernel properties that guarantee suitable behavior. We propose two best linear unbiased (BLUE) estimators for the squared statistical distance and use them in an unbiased procedure, called HEMQ, to find the optimal quantization. We test HEMQ on several databases: multi-dimensional Gaussian mixtures, Wiener space cub
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ViT-CX&#65292;&#36890;&#36807;&#22522;&#20110;&#34917;&#19969;&#23884;&#20837;&#30340;&#22240;&#26524;&#24433;&#21709;&#26469;&#35299;&#37322;&#35270;&#35273;Transformer(ViTs)&#65292;&#24182;&#32771;&#34385;&#20102;ViTs&#30340;&#22240;&#26524;&#36807;&#20915;&#23450;&#24615;&#31561;&#29305;&#24615;&#65292;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#24847;&#20041;&#19988;&#26356;&#24544;&#23454;&#20110;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.03064</link><description>&lt;p&gt;
ViT-CX: &#35270;&#35273;Transformer&#30340;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
ViT-CX: Causal Explanation of Vision Transformers. (arXiv:2211.03064v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ViT-CX&#65292;&#36890;&#36807;&#22522;&#20110;&#34917;&#19969;&#23884;&#20837;&#30340;&#22240;&#26524;&#24433;&#21709;&#26469;&#35299;&#37322;&#35270;&#35273;Transformer(ViTs)&#65292;&#24182;&#32771;&#34385;&#20102;ViTs&#30340;&#22240;&#26524;&#36807;&#20915;&#23450;&#24615;&#31561;&#29305;&#24615;&#65292;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#24847;&#20041;&#19988;&#26356;&#24544;&#23454;&#20110;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35270;&#35273;Transformer(ViTs)&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#30446;&#21069;&#20026;&#27490;&#21482;&#26377;&#23569;&#25968;&#19987;&#38376;&#20026;ViTs&#35774;&#35745;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#23427;&#20204;&#20027;&#35201;&#20351;&#29992;&#23545;&#34917;&#19969;&#23884;&#20837;&#30340;[CLS]&#20196;&#29260;&#30340;&#27880;&#24847;&#26435;&#37325;&#65292;&#36890;&#24120;&#20250;&#20135;&#29983;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#26174;&#33879;&#24615;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ViT-CX&#30340;&#35299;&#37322;ViTs&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#22522;&#20110;&#34917;&#19969;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#27880;&#37325;&#23427;&#20204;&#30340;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;ViTs&#30340;&#20854;&#20182;&#29305;&#24615;&#65292;&#22914;&#22240;&#26524;&#36807;&#20915;&#23450;&#24615;&#65292;&#22312;ViT-CX&#30340;&#35774;&#35745;&#20013;&#20063;&#32771;&#34385;&#20102;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;ViT-CX&#20135;&#29983;&#30340;&#26174;&#33879;&#24615;&#22270;&#26356;&#26377;&#24847;&#20041;&#65292;&#33021;&#26356;&#22909;&#22320;&#25581;&#31034;&#39044;&#27979;&#30340;&#25152;&#26377;&#37325;&#35201;&#35777;&#25454;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;ViT-CX&#20135;&#29983;&#30340;&#35299;&#37322;&#20063;&#26174;&#31034;&#20986;&#19982;&#27169;&#22411;&#26174;&#33879;&#26356;&#22909;&#30340;&#24544;&#23454;&#24615;&#12290;&#20195;&#30721;&#21644;&#38468;&#24405;&#21487;&#22312;https://github.com/vaynexie/CausalX-ViT&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the popularity of Vision Transformers (ViTs) and eXplainable AI (XAI), only a few explanation methods have been designed specially for ViTs thus far. They mostly use attention weights of the [CLS] token on patch embeddings and often produce unsatisfactory saliency maps. This paper proposes a novel method for explaining ViTs called ViT-CX. It is based on patch embeddings, rather than attentions paid to them, and their causal impacts on the model output. Other characteristics of ViTs such as causal overdetermination are also considered in the design of ViT-CX. The empirical results show that ViT-CX produces more meaningful saliency maps and does a better job revealing all important evidence for the predictions than previous methods. The explanation generated by ViT-CX also shows significantly better faithfulness to the model. The codes and appendix are available at https://github.com/vaynexie/CausalX-ViT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#35299;&#32544;&#35270;&#35282;&#22788;&#29702;&#35270;&#39057;&#39046;&#22495;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#32544;&#38745;&#24577;&#21644;&#21160;&#24577;&#20449;&#24687;&#24182;&#20351;&#29992;&#22810;&#31181;&#32422;&#26463;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#31354;&#38388;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#21644;&#20943;&#23569;&#26102;&#38388;&#39046;&#22495;&#24046;&#24322;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.07365</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#21160;&#20316;&#35782;&#21035;&#65306;&#19968;&#20010;&#35299;&#32544;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective. (arXiv:2208.07365v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#35299;&#32544;&#35270;&#35282;&#22788;&#29702;&#35270;&#39057;&#39046;&#22495;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#32544;&#38745;&#24577;&#21644;&#21160;&#24577;&#20449;&#24687;&#24182;&#20351;&#29992;&#22810;&#31181;&#32422;&#26463;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#31354;&#38388;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#21644;&#20943;&#23569;&#26102;&#38388;&#39046;&#22495;&#24046;&#24322;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#19968;&#39033;&#23454;&#36341;&#24615;&#32780;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#35299;&#32544;&#35270;&#35282;&#20837;&#25163;&#22788;&#29702;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#36890;&#36807;&#35299;&#32544;&#26469;&#20998;&#21035;&#22788;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#39046;&#22495;&#30340;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#21253;&#21547;&#38745;&#24577;&#20449;&#24687;&#30340;&#19968;&#32452;&#28508;&#22312;&#22240;&#32032;&#21644;&#21253;&#21547;&#21160;&#24577;&#20449;&#24687;&#30340;&#21478;&#19968;&#32452;&#28508;&#22312;&#22240;&#32032;&#20013;&#29983;&#25104;&#36328;&#39046;&#22495;&#35270;&#39057;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36716;&#31227;&#26102;&#24207;VAE&#65288;TranSVAE&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;&#36825;&#31181;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#32422;&#26463;&#28508;&#22312;&#22240;&#32032;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#20123;&#32422;&#26463;&#65292;&#38745;&#24577;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#35299;&#32544;&#21487;&#20197;&#36731;&#26494;&#31227;&#38500;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#20174;&#24103;&#21644;&#35270;&#39057;&#23618;&#38754;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#26102;&#38388;&#24046;&#24322;&#12290;&#22312;UCF-HMDB&#12289;Jester&#21644;Epic-Kitchens&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;TranSVAE&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised video domain adaptation is a practical yet challenging task. In this work, for the first time, we tackle it from a disentanglement view. Our key idea is to handle the spatial and temporal domain divergence separately through disentanglement. Specifically, we consider the generation of cross-domain videos from two sets of latent factors, one encoding the static information and another encoding the dynamic information. A Transfer Sequential VAE (TranSVAE) framework is then developed to model such generation. To better serve for adaptation, we propose several objectives to constrain the latent factors. With these constraints, the spatial divergence can be readily removed by disentangling the static domain-specific information out, and the temporal divergence is further reduced from both frame- and video-levels through adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE compared wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35745;&#31639;&#22270;&#36716;&#25442;&#22120;&#65288;CGT&#65289;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#36890;&#36807;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#24335;&#23398;&#20064;&#21644;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#29983;&#25104;&#26377;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22270;&#65292;&#35753;GNNs&#22312;&#20854;&#19978;&#23637;&#31034;&#19982;&#28304;&#22270;&#30456;&#20284;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.04396</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Generative Model for Benchmarking Graph Neural Networks. (arXiv:2207.04396v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35745;&#31639;&#22270;&#36716;&#25442;&#22120;&#65288;CGT&#65289;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#36890;&#36807;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#24335;&#23398;&#20064;&#21644;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#29983;&#25104;&#26377;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22270;&#65292;&#35753;GNNs&#22312;&#20854;&#19978;&#23637;&#31034;&#19982;&#28304;&#22270;&#30456;&#20284;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23545;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#26032;&#30340;GNN&#27169;&#22411;&#20197;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#30340;&#30495;&#23454;&#19990;&#30028;&#22270;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26469;&#33258;&#20110;&#22312;&#32447;&#12289;&#39640;&#24230;&#38480;&#21046;&#38544;&#31169;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#36825;&#20351;&#24471;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30740;&#31350;&#21644;&#24320;&#21457;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#35745;&#31639;&#22270;&#36716;&#25442;&#22120;&#65288;CGT&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#36890;&#36807;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#24335;&#23398;&#20064;&#21644;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#29983;&#25104;&#26377;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22270;&#65292;&#35753;GNNs&#22312;&#20854;&#19978;&#23637;&#31034;&#19982;&#28304;&#22270;&#30456;&#20284;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the field of Graph Neural Networks (GNN) continues to grow, it experiences a corresponding increase in the need for large, real-world datasets to train and test new GNN models on challenging, realistic problems. Unfortunately, such graph datasets are often generated from online, highly privacy-restricted ecosystems, which makes research and development on these datasets hard, if not impossible. This greatly reduces the amount of benchmark graphs available to researchers, causing the field to rely only on a handful of publicly-available datasets. To address this problem, we introduce a novel graph generative model, Computation Graph Transformer (CGT) that learns and reproduces the distribution of real-world graphs in a privacy-controlled way. More specifically, CGT (1) generates effective benchmark graphs on which GNNs show similar task performance as on the source graphs, (2) scales to process large-scale graphs, (3) incorporates off-the-shelf privacy modules to guarantee end-user p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DGraph&#30340;&#22823;&#35268;&#27169;&#37329;&#34701;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#35266;&#23519;&#21644;&#24191;&#27867;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#24322;&#24120;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#19981;&#21516;&#30340;&#29305;&#24449;&#21644;&#32467;&#26500;&#65292;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#33410;&#28857;&#37117;&#26159;&#26816;&#27979;&#27450;&#35784;&#32773;&#30340;&#24517;&#19981;&#21487;&#23569;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2207.03579</link><description>&lt;p&gt;
DGraph: &#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#37329;&#34701;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection. (arXiv:2207.03579v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DGraph&#30340;&#22823;&#35268;&#27169;&#37329;&#34701;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#35266;&#23519;&#21644;&#24191;&#27867;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#24322;&#24120;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#19981;&#21516;&#30340;&#29305;&#24449;&#21644;&#32467;&#26500;&#65292;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#33410;&#28857;&#37117;&#26159;&#26816;&#27979;&#27450;&#35784;&#32773;&#30340;&#24517;&#19981;&#21487;&#23569;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#30001;&#20110;&#20854;&#23454;&#29992;&#24615;&#21644;&#29702;&#35770;&#20215;&#20540;&#32780;&#25104;&#20026;&#36817;&#26399;&#28909;&#38376;&#30740;&#31350;&#39046;&#22495;&#12290;&#30001;&#20110;GAD&#24378;&#35843;&#24212;&#29992;&#21644;&#24322;&#24120;&#26679;&#26412;&#30340;&#32597;&#35265;&#24615;&#65292;&#20016;&#23500;&#20854;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26159;&#22522;&#30784;&#24615;&#30340;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DGraph&#65292;&#23427;&#26159;&#37329;&#34701;&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#21160;&#24577;&#22270;&#12290;DGraph&#20811;&#26381;&#20102;&#24403;&#21069;GAD&#25968;&#25454;&#38598;&#30340;&#35768;&#22810;&#38480;&#21046;&#12290;&#23427;&#21253;&#21547;&#32422;3&#30334;&#19975;&#20010;&#33410;&#28857;&#12289;4&#30334;&#19975;&#20010;&#21160;&#24577;&#36793;&#32536;&#21644;1&#30334;&#19975;&#20010;&#22320;&#38754;&#30495;&#23454;&#33410;&#28857;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;DGraph&#30340;&#20840;&#38754;&#35266;&#23519;&#65292;&#25581;&#31034;&#20102;&#24322;&#24120;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26500;&#12289;&#37051;&#23621;&#20998;&#24067;&#21644;&#26102;&#38388;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#23427;&#34920;&#26126;&#26631;&#35760;&#33410;&#28857;&#23545;&#20110;&#26816;&#27979;&#27450;&#35784;&#32773;&#20063;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;DGraph&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#35266;&#23519;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;DGraph&#26159;&#25512;&#21160;GAD&#30740;&#31350;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#21487;&#20197;&#28145;&#20837;&#25506;&#32034;&#24322;&#24120;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Anomaly Detection (GAD) has recently become a hot research spot due to its practicability and theoretical value. Since GAD emphasizes the application and the rarity of anomalous samples, enriching the varieties of its datasets is fundamental work. Thus, this paper present DGraph, a real-world dynamic graph in the finance domain. DGraph overcomes many limitations of current GAD datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth nodes. We provide a comprehensive observation of DGraph, revealing that anomalous nodes and normal nodes generally have different structures, neighbor distribution, and temporal dynamics. Moreover, it suggests that unlabeled nodes are also essential for detecting fraudsters. Furthermore, we conduct extensive experiments on DGraph. Observation and experiments demonstrate that DGraph is propulsive to advance GAD research and enable in-depth exploration of anomalous nodes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#26469;&#20998;&#26512;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.05359</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#25506;&#32034;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Local Explanations of Nonlinear Models Using Animated Linear Projections. (arXiv:2205.05359v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#26469;&#20998;&#26512;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#23616;&#37096;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#26085;&#30410;&#22686;&#24378;&#65292;&#20294;&#19982;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#30456;&#27604;&#65292;&#20854;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19979;&#38477;&#12290;&#36825;&#31181;&#25240;&#34935;&#23548;&#33268;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#20986;&#29616;&#65292;&#25552;&#20379;&#20102;&#35832;&#22914;&#23616;&#37096;&#35299;&#37322;&#65288;LE&#65289;&#21644;&#23616;&#37096;&#21464;&#37327;&#24402;&#22240;&#65288;LVA&#65289;&#20043;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#39044;&#27979;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;LVA&#36890;&#24120;&#19981;&#33021;&#26377;&#25928;&#22788;&#29702;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#29702;&#35299;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#65292;&#21487;&#20197;&#23558;LVA&#36716;&#25442;&#20026;&#32447;&#24615;&#25237;&#24433;&#65292;&#24182;&#20351;&#29992;&#24452;&#21521;&#28216;&#35272;&#12290;&#36825;&#23545;&#20110;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#29359;&#38169;&#65292;&#25110;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#25110;&#35266;&#27979;&#20540;&#30340;&#32858;&#31867;&#20063;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#20351;&#29992;&#21508;&#31181;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#65288;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#31034;&#20363;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased predictive power of machine learning models comes at the cost of increased complexity and loss of interpretability, particularly in comparison to parametric statistical models. This trade-off has led to the emergence of eXplainable AI (XAI) which provides methods, such as local explanations (LEs) and local variable attributions (LVAs), to shed light on how a model use predictors to arrive at a prediction. These provide a point estimate of the linear variable importance in the vicinity of a single observation. However, LVAs tend not to effectively handle association between predictors. To understand how the interaction between predictors affects the variable importance estimate, we can convert LVAs into linear projections and use the radial tour. This is also useful for learning how a model has made a mistake, or the effect of outliers, or the clustering of observations. The approach is illustrated with examples from categorical (penguin species, chocolate types) and quant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;,&#35774;&#35745;&#20102;BoBW-lil'UCB $(\gamma)$&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#27809;&#26377;&#31639;&#27861;&#33021;&#21516;&#26102;&#20026;RM&#21644;BAI&#30446;&#26631;&#34920;&#29616;&#26368;&#20339;&#65292;BoBW-lil'UCB $(\gamma)$&#21487;&#22312;&#19981;&#21516;&#30340;$\gamma$&#20540;&#19979;&#23454;&#29616;RM&#25110;BAI&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2110.08627</link><description>&lt;p&gt;
&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#23454;&#29616;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Achieving the Pareto Frontier of Regret Minimization and Best Arm Identification in Multi-Armed Bandits. (arXiv:2110.08627v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;,&#35774;&#35745;&#20102;BoBW-lil'UCB $(\gamma)$&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#27809;&#26377;&#31639;&#27861;&#33021;&#21516;&#26102;&#20026;RM&#21644;BAI&#30446;&#26631;&#34920;&#29616;&#26368;&#20339;&#65292;BoBW-lil'UCB $(\gamma)$&#21487;&#22312;&#19981;&#21516;&#30340;$\gamma$&#20540;&#19979;&#23454;&#29616;RM&#25110;BAI&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#20004;&#20010;&#20856;&#22411;&#30446;&#26631;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#21363;&#22266;&#23450;&#26102;&#38388;&#20869;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#12290;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#23545;&#20110;&#21518;&#24724;&#26368;&#23567;&#21270;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#37117;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#21518;&#32773;&#26469;&#35828;&#65292;&#25506;&#32034;&#26356;&#20851;&#38190;&#12290;&#26412;&#25991;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;BoBW-lil'UCB $(\gamma)$&#31639;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#22522;&#20110;BAI&#22833;&#36133;&#27010;&#29575;&#30340;&#21487;&#36798;&#21040;&#36951;&#25022;&#19979;&#38480;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;(i)&#27809;&#26377;&#31639;&#27861;&#33021;&#21516;&#26102;&#20026;RM&#21644;BAI&#30446;&#26631;&#34920;&#29616;&#26368;&#20339;&#65292;(ii)BoBW-lil'UCB $(\gamma)$&#21487;&#22312;&#19981;&#21516;&#30340;$\gamma$&#20540;&#19979;&#23454;&#29616;RM&#25110;BAI&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#23637;&#31034;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#24120;&#25968;&#22914;&#20309;&#20381;&#36182;&#26576;&#20123;&#38590;&#24230;&#21442;&#25968;&#65292;&#26356;&#31934;&#30830;&#22320;&#38416;&#26126;&#20102;&#27492;&#31867;&#31639;&#27861;&#20013;&#30340;&#26435;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;BoBW-lil'UCB&#20248;&#20110;&#20854;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#32773;UCB$_\alpha$&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the Pareto frontier of two archetypal objectives in multi-armed bandits, namely, regret minimization (RM) and best arm identification (BAI) with a fixed horizon. It is folklore that the balance between exploitation and exploration is crucial for both RM and BAI, but exploration is more critical in achieving the optimal performance for the latter objective. To this end, we design and analyze the BoBW-lil'UCB$(\gamma)$ algorithm. Complementarily, by establishing lower bounds on the regret achievable by any algorithm with a given BAI failure probability, we show that (i) no algorithm can simultaneously perform optimally for both the RM and BAI objectives, and (ii) BoBW-lil'UCB$(\gamma)$ achieves order-wise optimal performance for RM or BAI under different values of $\gamma$. Our work elucidates the trade-off more precisely by showing how the constants in previous works depend on certain hardness parameters. Finally, we show that BoBW-lil'UCB outperforms a close competitor UCB$_\a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#26426;&#22120;&#31526;&#21495;&#23398;&#30340;&#24605;&#24819;&#65292;&#21363;&#26426;&#22120;&#19981;&#38656;&#35201;&#29702;&#35299;&#35805;&#35821;&#30340;&#20256;&#32479;&#24847;&#20041;&#65292;&#32780;&#26159;&#33021;&#22815;&#20197;&#26684;&#37324;&#26031;&#35821;&#29992;&#23398;&#30340;&#26041;&#24335;&#32472;&#21046;&#20250;&#35805;&#34164;&#28085;&#35828;&#26126;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#24418;&#24335;&#21270;&#20026;&#35805;&#35821;-&#24847;&#20041;&#23545;&#65288;UMP&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2008.10522</link><description>&lt;p&gt;
&#26426;&#22120;&#31526;&#21495;&#23398;
&lt;/p&gt;
&lt;p&gt;
Machine Semiotics. (arXiv:2008.10522v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.10522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#26426;&#22120;&#31526;&#21495;&#23398;&#30340;&#24605;&#24819;&#65292;&#21363;&#26426;&#22120;&#19981;&#38656;&#35201;&#29702;&#35299;&#35805;&#35821;&#30340;&#20256;&#32479;&#24847;&#20041;&#65292;&#32780;&#26159;&#33021;&#22815;&#20197;&#26684;&#37324;&#26031;&#35821;&#29992;&#23398;&#30340;&#26041;&#24335;&#32472;&#21046;&#20250;&#35805;&#34164;&#28085;&#35828;&#26126;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#24418;&#24335;&#21270;&#20026;&#35805;&#35821;-&#24847;&#20041;&#23545;&#65288;UMP&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#35782;&#21040;&#20154;&#31867;&#21644;&#26426;&#22120;&#31526;&#21495;&#23398;&#30340;&#22522;&#26412;&#24046;&#24322;&#20026;&#20811;&#26381;&#24403;&#21069;&#35821;&#38899;&#36741;&#21161;&#35774;&#22791;&#30340;&#32570;&#28857;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#23545;&#20110;&#26426;&#22120;&#32780;&#35328;&#65292;&#65288;&#20154;&#31867;&#65289;&#35805;&#35821;&#30340;&#24847;&#20041;&#30001;&#20854;&#33258;&#36523;&#30340;&#34892;&#21160;&#33539;&#22260;&#23450;&#20041;&#12290;&#22240;&#27492;&#65292;&#26426;&#22120;&#19981;&#38656;&#35201;&#29702;&#35299;&#35805;&#35821;&#30340;&#20256;&#32479;&#24847;&#20041;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#20197;&#65288;&#26032;&#65289;&#26684;&#37324;&#26031;&#35821;&#29992;&#23398;&#30340;&#24847;&#20041;&#32472;&#21046;&#20250;&#35805;&#34164;&#28085;&#35828;&#26126;&#12290;&#23545;&#20110;&#35821;&#38899;&#36741;&#21161;&#35774;&#22791;&#32780;&#35328;&#65292;&#23398;&#20064;&#20154;&#31867;&#35805;&#35821;&#30340;&#26426;&#22120;&#29305;&#23450;&#21547;&#20041;&#65292;&#21363;&#36890;&#36807;&#35789;&#27719;&#21270;&#30340;&#23581;&#35797;&#21644;&#35823;&#24046;&#23558;&#20250;&#35805;&#34164;&#28085;&#35828;&#26126;&#21464;&#25104;&#20256;&#32479;&#21270;&#30340;&#21547;&#20041;&#65292;&#20284;&#20046;&#24050;&#32463;&#36275;&#22815;&#20102;&#12290;&#36890;&#36807;&#19968;&#20010;&#30456;&#24403;&#29712;&#30862;&#30340;&#35748;&#30693;&#21152;&#28909;&#35774;&#22791;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#21160;&#24577;&#35821;&#20041;&#23398;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#24418;&#24335;&#21270;&#20026;&#35805;&#35821;-&#24847;&#20041;&#23545;&#65288;UMP&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing a basic difference between the semiotics of humans and machines presents a possibility to overcome the shortcomings of current speech assistive devices. For the machine, the meaning of a (human) utterance is defined by its own scope of actions. Machines, thus, do not need to understand the conventional meaning of an utterance. Rather, they draw conversational implicatures in the sense of (neo-)Gricean pragmatics. For speech assistive devices, the learning of machine-specific meanings of human utterances, i.e. the fossilization of conversational implicatures into conventionalized ones by trial and error through lexicalization appears to be sufficient. Using the quite trivial example of a cognitive heating device, we show that - based on dynamic semantics - this process can be formalized as the reinforcement learning of utterance-meaning pairs (UMP).
&lt;/p&gt;</description></item></channel></rss>