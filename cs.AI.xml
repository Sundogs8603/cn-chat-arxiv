<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DELTA&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28151;&#21512;&#30340;&#26174;&#24335;-&#38544;&#24335;3D&#34920;&#31034;&#23545;&#20154;&#20307;&#36827;&#34892;&#35299;&#32806;&#21512;&#24314;&#27169;&#12290;&#36890;&#36807;&#23558;&#20154;&#20307;&#25110;&#38754;&#37096;&#34920;&#31034;&#20026;&#20998;&#24320;&#30340;&#23618;&#65292;DELTA&#33021;&#22815;&#23454;&#29616;&#20154;&#20307;&#21644;&#26381;&#35013;/&#22836;&#21457;&#30340;&#35299;&#32806;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23398;&#20064;&#21487;&#21160;&#30011;&#21644;&#36924;&#30495;&#30340;&#20154;&#20307;&#35282;&#33394;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06441</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;3D&#34920;&#31034;&#23398;&#20064;&#35299;&#32806;&#21512;&#30340;&#34394;&#25311;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Learning Disentangled Avatars with Hybrid 3D Representations. (arXiv:2309.06441v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DELTA&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28151;&#21512;&#30340;&#26174;&#24335;-&#38544;&#24335;3D&#34920;&#31034;&#23545;&#20154;&#20307;&#36827;&#34892;&#35299;&#32806;&#21512;&#24314;&#27169;&#12290;&#36890;&#36807;&#23558;&#20154;&#20307;&#25110;&#38754;&#37096;&#34920;&#31034;&#20026;&#20998;&#24320;&#30340;&#23618;&#65292;DELTA&#33021;&#22815;&#23454;&#29616;&#20154;&#20307;&#21644;&#26381;&#35013;/&#22836;&#21457;&#30340;&#35299;&#32806;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23398;&#20064;&#21487;&#21160;&#30011;&#21644;&#36924;&#30495;&#30340;&#20154;&#20307;&#35282;&#33394;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23398;&#20064;&#21160;&#30011;&#21270;&#21644;&#36924;&#30495;&#30340;&#20154;&#20307;&#35282;&#33394;&#65292;&#20154;&#20204;&#20570;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#12290;&#20026;&#20102;&#23545;&#25972;&#20010;&#20154;&#20307;&#65288;&#20363;&#22914;&#36523;&#20307;&#12289;&#26381;&#35013;&#12289;&#38754;&#37096;&#21644;&#22836;&#21457;&#65289;&#36827;&#34892;&#25972;&#20307;&#24314;&#27169;&#21644;&#25429;&#25417;&#65292;&#24050;&#32463;&#23545;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;3D&#34920;&#31034;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#26159;&#36825;&#20123;&#34920;&#31034;&#37117;&#19981;&#26159;&#22312;&#34920;&#31034;&#25928;&#26524;&#26041;&#38754;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#22240;&#20026;&#20154;&#20307;&#35282;&#33394;&#30340;&#19981;&#21516;&#37096;&#20998;&#20855;&#26377;&#19981;&#21516;&#30340;&#24314;&#27169;&#35201;&#27714;&#12290;&#20363;&#22914;&#65292;&#32593;&#26684;&#36890;&#24120;&#19981;&#36866;&#21512;&#27169;&#25311;&#26381;&#35013;&#21644;&#22836;&#21457;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Disentangled Avatars&#65288;DELTA&#65289;&#65292;&#23427;&#20351;&#29992;&#28151;&#21512;&#30340;&#26174;&#24335;-&#38544;&#24335;3D&#34920;&#31034;&#23545;&#20154;&#20307;&#36827;&#34892;&#24314;&#27169;&#12290;DELTA&#20197;&#21333;&#30446;RGB&#35270;&#39057;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#20998;&#24320;&#36523;&#20307;&#21644;&#26381;&#35013;/&#22836;&#21457;&#23618;&#30340;&#34394;&#25311;&#20154;&#20307;&#35282;&#33394;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DELTA&#30340;&#20004;&#20010;&#37325;&#35201;&#24212;&#29992;&#12290;&#31532;&#19968;&#20010;&#24212;&#29992;&#32771;&#34385;&#20102;&#20154;&#20307;&#21644;&#26381;&#35013;&#30340;&#35299;&#32806;&#65292;&#31532;&#20108;&#20010;&#24212;&#29992;&#32771;&#34385;&#20102;&#38754;&#37096;&#21644;&#22836;&#21457;&#30340;&#35299;&#32806;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;DELTA&#23545;&#36523;&#20307;&#25110;&#38754;&#37096;&#36827;&#34892;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tremendous efforts have been made to learn animatable and photorealistic human avatars. Towards this end, both explicit and implicit 3D representations are heavily studied for a holistic modeling and capture of the whole human (e.g., body, clothing, face and hair), but neither representation is an optimal choice in terms of representation efficacy since different parts of the human avatar have different modeling desiderata. For example, meshes are generally not suitable for modeling clothing and hair. Motivated by this, we present Disentangled Avatars~(DELTA), which models humans with hybrid explicit-implicit 3D representations. DELTA takes a monocular RGB video as input, and produces a human avatar with separate body and clothing/hair layers. Specifically, we demonstrate two important applications for DELTA. For the first one, we consider the disentanglement of the human body and clothing and in the second, we disentangle the face and hair. To do so, DELTA represents the body or face 
&lt;/p&gt;</description></item><item><title>LEAP Hand&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#25928;&#19988;&#25311;&#20154;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25163;&#12290;&#20854;&#26032;&#39062;&#30340;&#36816;&#21160;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#20854;&#25104;&#26412;&#20026;&#20854;&#20182;&#31454;&#20105;&#23545;&#25163;&#30340;1/8&#12290;LEAP Hand&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#22810;&#20010;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.06440</link><description>&lt;p&gt;
LEAP Hand: &#20302;&#25104;&#26412;&#12289;&#39640;&#25928;&#21644;&#25311;&#20154;&#21270;&#26426;&#22120;&#23398;&#20064;&#25163;
&lt;/p&gt;
&lt;p&gt;
LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning. (arXiv:2309.06440v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06440
&lt;/p&gt;
&lt;p&gt;
LEAP Hand&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#25928;&#19988;&#25311;&#20154;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25163;&#12290;&#20854;&#26032;&#39062;&#30340;&#36816;&#21160;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#20854;&#25104;&#26412;&#20026;&#20854;&#20182;&#31454;&#20105;&#23545;&#25163;&#30340;1/8&#12290;LEAP Hand&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#22810;&#20010;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#65292;&#28789;&#24039;&#25805;&#25511;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#36825;&#26041;&#38754;&#26174;&#31034;&#20986;&#19968;&#20123;&#24076;&#26395;&#65292;&#20294;&#32467;&#26524;&#20027;&#35201;&#38480;&#20110;&#27169;&#25311;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#30828;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEAP Hand&#65292;&#36825;&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#28789;&#24039;&#21644;&#25311;&#20154;&#21270;&#26426;&#22120;&#23398;&#20064;&#25163;&#12290;&#19982;&#20197;&#24448;&#30340;&#25163;&#30456;&#27604;&#65292;LEAP Hand&#20855;&#26377;&#19968;&#31181;&#26032;&#39062;&#30340;&#36816;&#21160;&#32467;&#26500;&#65292;&#26080;&#35770;&#25163;&#25351;&#30340;&#23039;&#21183;&#22914;&#20309;&#37117;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;LEAP Hand&#25104;&#26412;&#20302;&#24265;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;4&#20010;&#23567;&#26102;&#20869;&#20351;&#29992;&#29616;&#26377;&#38646;&#20214;&#32452;&#35013;&#32780;&#25104;&#65292;&#25104;&#26412;&#20026;2000&#32654;&#20803;&#12290;&#23427;&#33021;&#22815;&#25345;&#32493;&#38271;&#26102;&#38388;&#22320;&#26045;&#21152;&#22823;&#25197;&#30697;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LEAP Hand&#21487;&#20197;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#22810;&#20010;&#25805;&#20316;&#20219;&#21153;&#65292;&#20174;&#35270;&#35273;&#36828;&#31243;&#25805;&#20316;&#21040;&#20174;&#34987;&#21160;&#35270;&#39057;&#25968;&#25454;&#21644;&#27169;&#25311;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#23398;&#20064;&#12290;LEAP Hand&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#37117;&#26126;&#26174;&#20248;&#20110;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#23545;&#25163;Allegro Hand&#65292;&#21516;&#26102;&#25104;&#26412;&#21482;&#26377;&#20854;1/8&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#35814;&#32454;&#30340;&#35774;&#35745;&#21644;&#32452;&#35013;&#35828;&#26126;&#65292;&#20197;&#20415;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#37325;&#22797;&#25105;&#20204;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dexterous manipulation has been a long-standing challenge in robotics. While machine learning techniques have shown some promise, results have largely been currently limited to simulation. This can be mostly attributed to the lack of suitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous and anthropomorphic hand for machine learning research. In contrast to previous hands, LEAP Hand has a novel kinematic structure that allows maximal dexterity regardless of finger pose. LEAP Hand is low-cost and can be assembled in 4 hours at a cost of 2000 USD from readily available parts. It is capable of consistently exerting large torques over long durations of time. We show that LEAP Hand can be used to perform several manipulation tasks in the real world -- from visual teleoperation to learning from passive video data and sim2real. LEAP Hand significantly outperforms its closest competitor Allegro Hand in all our experiments while being 1/8th of the cost. We release detailed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;GPT-3&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#29983;&#25104;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#35780;&#20272;&#12289;&#20998;&#26512;&#21644;&#39564;&#35777;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#32473;&#23450;&#20195;&#30721;&#29255;&#27573;&#30340;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#21464;&#20307;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.06424</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#26041;&#38754;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the potential of large language models in generating semantic and cross-language clones. (arXiv:2309.06424v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;GPT-3&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#29983;&#25104;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#35780;&#20272;&#12289;&#20998;&#26512;&#21644;&#39564;&#35777;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#32473;&#23450;&#20195;&#30721;&#29255;&#27573;&#30340;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#21464;&#20307;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20195;&#30721;&#20811;&#38534;&#29983;&#25104;&#23545;&#20110;&#20195;&#30721;&#37325;&#29992;&#12289;&#20195;&#30721;&#29702;&#35299;&#12289;&#37325;&#26500;&#21644;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#38750;&#24120;&#26377;&#29992;&#12290;OpenAI&#30340;GPT&#27169;&#22411;&#22312;&#36825;&#31181;&#20811;&#38534;&#29983;&#25104;&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;GPT&#34987;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#12290;&#24403;&#24320;&#21457;&#20154;&#21592;&#20174;Stack Overflow&#65288;SO&#65289;&#25110;&#31995;&#32479;&#20869;&#37096;&#22797;&#21046;/&#31896;&#36148;&#20195;&#30721;&#26102;&#65292;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#19968;&#33268;&#30340;&#26356;&#25913;&#23548;&#33268;&#24847;&#22806;&#34892;&#20026;&#12290;&#21516;&#26679;&#65292;&#22914;&#26524;&#26576;&#20154;&#25317;&#26377;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#20294;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#23547;&#25214;&#31561;&#25928;&#21151;&#33021;&#65292;&#21017;&#35821;&#20041;&#36328;&#35821;&#35328;&#20195;&#30721;&#20811;&#38534;&#29983;&#25104;&#26041;&#27861;&#21487;&#33021;&#20250;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#24110;&#21161;&#12290; &#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;SemanticCloneBench&#20316;&#20026;&#24037;&#20855;&#65292;&#35780;&#20272;&#20102;GPT-3&#27169;&#22411;&#22312;&#29983;&#25104;&#32473;&#23450;&#20195;&#30721;&#29255;&#27573;&#30340;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#21464;&#20307;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#24182;&#35780;&#20272;&#20102;GPT-3&#22312;&#29983;&#25104;&#20195;&#30721;&#21464;&#20307;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#36890;&#36807;9&#20301;&#27861;&#23448;&#33457;&#36153;&#20102;158&#20010;&#23567;&#26102;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic and Cross-language code clone generation may be useful for code reuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model has potential in such clone generation as GPT is used for text generation. When developers copy/paste codes from Stack Overflow (SO) or within a system, there might be inconsistent changes leading to unexpected behaviours. Similarly, if someone possesses a code snippet in a particular programming language but seeks equivalent functionality in a different language, a semantic cross-language code clone generation approach could provide valuable assistance.In this study, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3 model could help generate semantic and cross-language clone variants for a given fragment.We have comprised a diverse set of code fragments and assessed GPT-3s performance in generating code variants.Through extensive experimentation and analysis, where 9 judges spent 158 hours to validate, we investigate 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#39564;&#35777;&#21644;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#23376;&#31995;&#32479;&#32452;&#21512;&#36215;&#26469;&#23454;&#29616;&#25972;&#20307;&#20219;&#21153;&#12290;&#36890;&#36807;&#23450;&#20041;&#23376;&#31995;&#32479;&#20043;&#38388;&#30340;&#25509;&#21475;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#35268;&#33539;&#30340;&#33258;&#21160;&#20998;&#35299;&#65292;&#24182;&#20801;&#35768;&#23376;&#31995;&#32479;&#30340;&#29420;&#31435;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2309.06420</link><description>&lt;p&gt;
&#21487;&#39564;&#35777;&#30340;&#32452;&#21512;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Verifiable Reinforcement Learning Systems via Compositionality. (arXiv:2309.06420v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#39564;&#35777;&#21644;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#23376;&#31995;&#32479;&#32452;&#21512;&#36215;&#26469;&#23454;&#29616;&#25972;&#20307;&#20219;&#21153;&#12290;&#36890;&#36807;&#23450;&#20041;&#23376;&#31995;&#32479;&#20043;&#38388;&#30340;&#25509;&#21475;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#35268;&#33539;&#30340;&#33258;&#21160;&#20998;&#35299;&#65292;&#24182;&#20801;&#35768;&#23376;&#31995;&#32479;&#30340;&#29420;&#31435;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#39564;&#35777;&#21644;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#19968;&#32452;RL&#23376;&#31995;&#32479;&#34987;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#23436;&#25104;&#19968;&#20010;&#25972;&#20307;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#39640;&#32423;&#27169;&#22411;&#21644;&#19968;&#32452;&#20302;&#32423;&#23376;&#31995;&#32479;&#32452;&#25104;&#12290;&#39640;&#32423;&#27169;&#22411;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#29992;&#20110;&#35268;&#21010;&#21644;&#20998;&#26512;&#23376;&#31995;&#32479;&#30340;&#32452;&#21512;&#65292;&#32780;&#20302;&#32423;&#23376;&#31995;&#32479;&#21017;&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#25805;&#20316;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#23454;&#29616;&#12290;&#36890;&#36807;&#23450;&#20041;&#23376;&#31995;&#32479;&#20043;&#38388;&#30340;&#25509;&#21475;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#20998;&#35299;&#20219;&#21153;&#35268;&#33539;&#25104;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#20801;&#35768;&#23376;&#31995;&#32479;&#30340;&#29420;&#31435;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35770;&#32467;&#26524;&#20445;&#35777;&#20102;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for verifiable and compositional reinforcement learning (RL) in which a collection of RL subsystems, each of which learns to accomplish a separate subtask, are composed to achieve an overall task. The framework consists of a high-level model, represented as a parametric Markov decision process, which is used to plan and analyze compositions of subsystems, and of the collection of low-level subsystems themselves. The subsystems are implemented as deep RL agents operating under partial observability. By defining interfaces between the subsystems, the framework enables automatic decompositions of task specifications, e.g., reach a target set of states with a probability of at least 0.95, into individual subtask specifications, i.e. achieve the subsystem's exit conditions with at least some minimum probability, given that its entry conditions are met. This in turn allows for the independent training and testing of the subsystems. We present theoretical results guaran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#36890;&#36807;&#21024;&#38500;&#32593;&#32476;&#36793;&#32536;&#26469;&#35843;&#33410;&#20851;&#38190;&#33410;&#28857;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#36817;&#20284;&#36138;&#23146;&#31639;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#33410;&#28857;&#30340;&#20449;&#24687;&#20013;&#24515;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#32593;&#32476;&#36830;&#36890;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06392</link><description>&lt;p&gt;
&#36890;&#36807;&#21024;&#38500;&#36793;&#32536;&#23454;&#29616;&#20851;&#38190;&#33410;&#28857;&#30340;&#24555;&#36895;&#35843;&#33410;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast Algorithm for Moderating Critical Nodes via Edge Removal. (arXiv:2309.06392v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#36890;&#36807;&#21024;&#38500;&#32593;&#32476;&#36793;&#32536;&#26469;&#35843;&#33410;&#20851;&#38190;&#33410;&#28857;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#36817;&#20284;&#36138;&#23146;&#31639;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#33410;&#28857;&#30340;&#20449;&#24687;&#20013;&#24515;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#32593;&#32476;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#33410;&#28857;&#26497;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#65292;&#35302;&#21457;&#36127;&#38754;&#32423;&#32852;&#20107;&#20214;&#65292;&#22914;&#38169;&#35823;&#20449;&#24687;&#21644;&#30142;&#30149;&#20256;&#25773;&#12290;&#22240;&#27492;&#65292;&#23545;&#20851;&#38190;&#33410;&#28857;&#30340;&#26377;&#25928;&#35843;&#33410;&#23545;&#20110;&#20943;&#36731;&#30001;&#27492;&#31867;&#24694;&#24847;&#25193;&#25955;&#24341;&#36215;&#30340;&#28508;&#22312;&#25439;&#23475;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#35843;&#33410;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24573;&#35270;&#20102;&#20449;&#24687;&#20013;&#24515;&#24230;&#36825;&#19968;&#22522;&#26412;&#25351;&#26631;&#65292;&#35813;&#25351;&#26631;&#34913;&#37327;&#33410;&#28857;&#30340;&#20256;&#25773;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;$k$&#20010;&#36793;&#32536;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#33410;&#28857;&#30340;&#20449;&#24687;&#20013;&#24515;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#32593;&#32476;&#36830;&#36890;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#35745;&#31639;&#22797;&#26434;&#24615;&#65306;&#23427;&#26159;NP&#23436;&#20840;&#30340;&#65292;&#20854;&#30446;&#26631;&#20989;&#25968;&#19981;&#26159;&#36229;&#27169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#36817;&#20284;&#36138;&#23146;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#22914;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#33298;&#23572;&#34917;&#36924;&#36817;&#21644;&#24555;&#36895;&#27714;&#21644;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#20854;&#20013;&#19968;&#31181;&#31639;&#27861;&#22312;&#36817;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical nodes in networks are extremely vulnerable to malicious attacks to trigger negative cascading events such as the spread of misinformation and diseases. Therefore, effective moderation of critical nodes is very vital for mitigating the potential damages caused by such malicious diffusions. The current moderation methods are computationally expensive. Furthermore, they disregard the fundamental metric of information centrality, which measures the dissemination power of nodes.  We investigate the problem of removing $k$ edges from a network to minimize the information centrality of a target node $\lea$ while preserving the network's connectivity. We prove that this problem is computationally challenging: it is NP-complete and its objective function is not supermodular. However, we propose three approximation greedy algorithms using novel techniques such as random walk-based Schur complement approximation and fast sum estimation. One of our algorithms runs in nearly linear time in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#35780;&#35770;&#27169;&#22411;&#21644;&#24341;&#20837;&#21453;&#39304;&#23398;&#20064;&#24490;&#29615;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#24341;&#29992;&#38169;&#35823;&#12289;&#29983;&#25104;&#34394;&#26500;&#20449;&#24687;&#21644;&#32570;&#23569;&#20851;&#38190;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06384</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#38752;&#27969;&#21033;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36808;&#36827;&#65306;&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#24341;&#20837;&#21453;&#39304;&#23398;&#20064;&#24490;&#29615;
&lt;/p&gt;
&lt;p&gt;
Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems. (arXiv:2309.06384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#35780;&#35770;&#27169;&#22411;&#21644;&#24341;&#20837;&#21453;&#39304;&#23398;&#20064;&#24490;&#29615;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#24341;&#29992;&#38169;&#35823;&#12289;&#29983;&#25104;&#34394;&#26500;&#20449;&#24687;&#21644;&#32570;&#23569;&#20851;&#38190;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#26085;&#24120;&#24212;&#29992;&#20013;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#24341;&#29992;&#38169;&#35823;&#65288;&#24341;&#25991;&#65289;&#12289;&#29983;&#25104;&#34394;&#26500;&#20449;&#24687;&#65288;&#27491;&#30830;&#24615;&#65289;&#20197;&#21450;&#21253;&#21547;&#22810;&#20313;&#25110;&#36951;&#28431;&#20851;&#38190;&#32454;&#33410;&#65288;&#27969;&#30021;&#24615;&#65289;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#20570;&#20986;&#20102;&#20960;&#20010;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#35780;&#35770;&#27169;&#22411;&#65292;&#35780;&#20272;LLMs&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#29983;&#25104;&#30340;&#22238;&#31572;&#30340;&#24341;&#25991;&#12289;&#27491;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21453;&#39304;&#26426;&#21046;&#65292;&#21033;&#29992;&#35780;&#35770;&#27169;&#22411;&#23545;&#29983;&#25104;&#25991;&#26412;&#30340;&#24322;&#26500;&#26041;&#38754;&#36827;&#34892;&#23454;&#26102;&#21453;&#39304;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21453;&#39304;&#23398;&#20064;&#24490;&#29615;&#65292;&#20351;&#29992;&#35780;&#35770;&#27169;&#22411;&#26469;&#36845;&#20195;&#25913;&#36827;&#36127;&#36131;&#22238;&#31572;&#29983;&#25104;&#30340;LLM&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as versatile tools in various daily applications. However, they are fraught with issues that undermine their utility and trustworthiness. These include the incorporation of erroneous references (citation), the generation of hallucinated information (correctness), and the inclusion of superfluous or omission of crucial details (fluency). To ameliorate these concerns, this study makes several key contributions. First, we build a dataset to train a critic model capable of evaluating the citation, correctness, and fluency of responses generated by LLMs in QA systems. Second, we propose an automated feedback mechanism that leverages the critic model to offer real-time feedback on heterogeneous aspects of generated text. Third, we introduce a feedback learning loop that uses this critic model to iteratively improve the performance of the LLM responsible for response generation. Experimental results demonstrate the efficacy of our approach, showing su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65292;&#28789;&#27963;&#30340;&#25513;&#27169;&#21644;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#65292;&#20351;&#24471;&#19968;&#20010;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#24418;&#27169;&#22411;&#20013;&#21487;&#20197;&#29992;&#26469;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.06382</link><description>&lt;p&gt;
&#38598;&#25104;&#25513;&#27169;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Ensemble Mask Networks. (arXiv:2309.06382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65292;&#28789;&#27963;&#30340;&#25513;&#27169;&#21644;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#65292;&#20351;&#24471;&#19968;&#20010;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#24418;&#27169;&#22411;&#20013;&#21487;&#20197;&#29992;&#26469;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;$\mathbb{R}^n\rightarrow \mathbb{R}^n$&#30340;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#21527;&#65311;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65306;&#28789;&#27963;&#30340;&#25513;&#27169;&#29992;&#20110;&#25509;&#25910;&#30697;&#38453;&#36755;&#20837;&#65292;&#20197;&#21450;&#19968;&#31181;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#20197;&#23562;&#37325;&#25513;&#27169;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#32593;&#32476;&#21487;&#20197;&#36817;&#20284;&#22266;&#23450;&#25805;&#20316;&#65292;&#22914;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;$\phi(A,x) \rightarrow Ax$&#65292;&#36825;&#28608;&#21457;&#20102;&#24341;&#20837;&#30340;&#26426;&#21046;&#22312;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20013;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can an $\mathbb{R}^n\rightarrow \mathbb{R}^n$ feedforward network learn matrix-vector multiplication? This study introduces two mechanisms - flexible masking to take matrix inputs, and a unique network pruning to respect the mask's dependency structure. Networks can approximate fixed operations such as matrix-vector multiplication $\phi(A,x) \rightarrow Ax$, motivating the mechanisms introduced with applications towards litmus-testing dependencies or interaction order in graph-based models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;AI&#36827;&#34892;&#21151;&#33021;&#24863;&#30693;&#20998;&#21106;&#65292;&#22312;&#21046;&#36896;3D&#27169;&#22411;&#26102;&#20445;&#30041;&#20102;&#21151;&#33021;&#30340;&#21516;&#26102;&#65292;&#20801;&#35768;&#29992;&#25143;&#26377;&#36873;&#25321;&#24615;&#22320;&#20462;&#25913;&#27169;&#22411;&#30340;&#23457;&#32654;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.06379</link><description>&lt;p&gt;
Style2Fab: &#20351;&#29992;&#29983;&#25104;AI&#30340;&#21151;&#33021;&#24863;&#30693;&#20998;&#21106;&#26469;&#21046;&#36896;&#20010;&#24615;&#21270;3D&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Style2Fab: Functionality-Aware Segmentation for Fabricating Personalized 3D Models with Generative AI. (arXiv:2309.06379v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06379
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;AI&#36827;&#34892;&#21151;&#33021;&#24863;&#30693;&#20998;&#21106;&#65292;&#22312;&#21046;&#36896;3D&#27169;&#22411;&#26102;&#20445;&#30041;&#20102;&#21151;&#33021;&#30340;&#21516;&#26102;&#65292;&#20801;&#35768;&#29992;&#25143;&#26377;&#36873;&#25321;&#24615;&#22320;&#20462;&#25913;&#27169;&#22411;&#30340;&#23457;&#32654;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;AI&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#33258;&#21160;&#25805;&#20316;3D&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#20250;&#20840;&#23616;&#24212;&#29992;&#32534;&#36753;&#65292;&#36825;&#21487;&#33021;&#20250;&#21361;&#21450;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#21046;&#36896;&#30340;3D&#27169;&#22411;&#30340;&#39044;&#26399;&#21151;&#33021;&#12290;&#20363;&#22914;&#65292;&#20462;&#25913;3D&#27169;&#22411;&#20013;&#30340;&#21151;&#33021;&#27573;&#65292;&#22914;&#33457;&#29942;&#30340;&#24213;&#24231;&#65292;&#21487;&#33021;&#20250;&#30772;&#22351;&#27169;&#22411;&#30340;&#21407;&#22987;&#21151;&#33021;&#65292;&#23548;&#33268;&#33457;&#29942;&#20542;&#20498;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;3D&#27169;&#22411;&#33258;&#21160;&#20998;&#21106;&#25104;&#21151;&#33021;&#21644;&#23457;&#32654;&#20803;&#32032;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#26377;&#36873;&#25321;&#24615;&#22320;&#20462;&#25913;3D&#27169;&#22411;&#30340;&#23457;&#32654;&#37096;&#20998;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#21151;&#33021;&#37096;&#20998;&#12290;&#20026;&#20102;&#24320;&#21457;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#26469;&#33258;&#30693;&#21517;3D&#25171;&#21360;&#20179;&#24211;Thingiverse&#30340;1000&#20010;&#27169;&#22411;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21151;&#33021;&#20998;&#31867;&#30340;&#20998;&#31867;&#27861;&#12290;&#20511;&#21161;&#36825;&#20010;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;3D&#27169;&#22411;&#20998;&#35299;&#20026;&#21151;&#33021;&#21644;&#23457;&#32654;&#20803;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;...
&lt;/p&gt;
&lt;p&gt;
With recent advances in Generative AI, it is becoming easier to automatically manipulate 3D models. However, current methods tend to apply edits to models globally, which risks compromising the intended functionality of the 3D model when fabricated in the physical world. For example, modifying functional segments in 3D models, such as the base of a vase, could break the original functionality of the model, thus causing the vase to fall over. We introduce a method for automatically segmenting 3D models into functional and aesthetic elements. This method allows users to selectively modify aesthetic segments of 3D models, without affecting the functional segments. To develop this method we first create a taxonomy of functionality in 3D models by qualitatively analyzing 1000 models sourced from a popular 3D printing repository, Thingiverse. With this taxonomy, we develop a semi-automatic classification method to decompose 3D models into functional and aesthetic elements. We propose a syste
&lt;/p&gt;</description></item><item><title>&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#38656;&#35201;&#32771;&#34385;&#21442;&#19982;&#32773;&#28608;&#21169;&#12289;&#34892;&#20026;&#20197;&#21450;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65292;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#26435;&#34913;&#65292;&#24182;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;</title><link>http://arxiv.org/abs/2309.06375</link><description>&lt;p&gt;
&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#65306;&#26426;&#21046;&#35774;&#35745;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20132;&#21449;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models. (arXiv:2309.06375v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06375
&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#38656;&#35201;&#32771;&#34385;&#21442;&#19982;&#32773;&#28608;&#21169;&#12289;&#34892;&#20026;&#20197;&#21450;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65292;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#26435;&#34913;&#65292;&#24182;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20301;&#20110;&#28085;&#30422;&#29992;&#25143;&#12289;&#20869;&#23481;&#25552;&#20379;&#21830;&#12289;&#24191;&#21578;&#21830;&#21644;&#20854;&#20182;&#21442;&#19982;&#32773;&#34892;&#20026;&#30340;&#22797;&#26434;&#29983;&#24577;&#31995;&#32479;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#30340;&#37325;&#28857;&#65292;&#20197;&#21450;&#22823;&#22810;&#25968;&#37325;&#35201;&#23454;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#20165;&#38480;&#20110;&#20010;&#21035;&#29992;&#25143;&#25512;&#33616;&#30340;&#23616;&#37096;&#12289;&#30701;&#35270;&#20248;&#21270;&#12290;&#36825;&#32473;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#20026;&#29992;&#25143;&#24102;&#26469;&#30340;&#38271;&#26399;&#25928;&#29992;&#24102;&#26469;&#20102;&#37325;&#22823;&#25104;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#35201;&#26368;&#22823;&#21270;&#31995;&#32479;&#23545;&#36825;&#20123;&#21442;&#19982;&#32773;&#30340;&#20215;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#29983;&#24577;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#29366;&#20917;&#65292;&#26377;&#24517;&#35201;&#26126;&#30830;&#22320;&#23545;&#31995;&#32479;&#20013;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#28608;&#21169;&#21644;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23545;&#20854;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#27492;&#38656;&#35201;&#65306;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31561;&#25216;&#26415;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65307;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#20026;&#19981;&#21516;&#21442;&#19982;&#32773;&#30340;&#25928;&#29992;&#36827;&#34892;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#65307;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems lie at the heart of complex ecosystems that couple the behavior of users, content providers, advertisers, and other actors. Despite this, the focus of the majority of recommender research -- and most practical recommenders of any import -- is on the local, myopic optimization of the recommendations made to individual users. This comes at a significant cost to the long-term utility that recommenders could generate for its users. We argue that explicitly modeling the incentives and behaviors of all actors in the system -- and the interactions among them induced by the recommender's policy -- is strictly necessary if one is to maximize the value the system brings to these actors and improve overall ecosystem "health". Doing so requires: optimization over long horizons using techniques such as reinforcement learning; making inevitable tradeoffs in the utility that can be generated for different actors using the methods of social choice; reducing information asymm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Chebyshev&#31890;&#23376;&#30340;&#39034;&#24207;MCMC&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21152;&#26435;Riesz&#26497;&#21270;&#37327;&#65292;&#22312;&#26080;&#31351;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#31163;&#25955;&#21270;&#21487;&#30697;&#24418;&#23376;&#27969;&#24418;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#25512;&#26029;&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06373</link><description>&lt;p&gt;
Chebyshev&#31890;&#23376;
&lt;/p&gt;
&lt;p&gt;
Chebyshev Particles. (arXiv:2309.06373v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Chebyshev&#31890;&#23376;&#30340;&#39034;&#24207;MCMC&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21152;&#26435;Riesz&#26497;&#21270;&#37327;&#65292;&#22312;&#26080;&#31351;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#31163;&#25955;&#21270;&#21487;&#30697;&#24418;&#23376;&#27969;&#24418;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#25512;&#26029;&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;(MCMC)&#20026;&#25512;&#26029;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#22120;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#38543;&#26426;&#37319;&#21462;&#23567;&#27493;&#39588;&#22312;&#19981;&#30830;&#23450;&#21306;&#22495;&#20013;&#31359;&#36234;&#65292;&#36890;&#24120;&#35745;&#31639;&#37327;&#24456;&#22823;&#65292;&#23588;&#20854;&#21463;&#21040;&#32500;&#25968;&#28798;&#38590;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#39318;&#27425;&#23558;&#30446;&#26631;&#30340;&#21518;&#39564;&#20998;&#24067;&#35270;&#20026;&#26679;&#26412;&#22312;&#26080;&#31351;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#26144;&#23556;&#65292;&#30830;&#23450;&#24615;&#23376;&#27969;&#24418;&#23884;&#20837;&#20854;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#21152;&#26435;Riesz&#26497;&#21270;&#37327;&#26469;&#31163;&#25955;&#21270;&#21487;&#30697;&#24418;&#23376;&#27969;&#24418;&#30340;&#26032;&#26631;&#20934;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;Chebyshev&#31890;&#23376;&#30340;&#29305;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#23884;&#20837;&#39034;&#24207;MCMC&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#39640;&#25509;&#21463;&#29575;&#19988;&#21482;&#25552;&#20986;&#23569;&#37327;&#35780;&#20272;&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#12290;&#25105;&#20204;&#22312;&#32447;&#24615;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19982;&#21512;&#25104;&#25968;&#25454;&#21644;&#38750;&#32447;&#24615;&#38543;&#26426;&#27874;&#21160;&#29575;&#27169;&#22411;&#30340;&#21442;&#25968;&#25512;&#26029;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov chain Monte Carlo (MCMC) provides a feasible method for inferring Hidden Markov models, however, it is often computationally prohibitive, especially constrained by the curse of dimensionality, as the Monte Carlo sampler traverses randomly taking small steps within uncertain regions in the parameter space. We are the first to consider the posterior distribution of the objective as a mapping of samples in an infinite-dimensional Euclidean space where deterministic submanifolds are embedded and propose a new criterion by maximizing the weighted Riesz polarization quantity, to discretize rectifiable submanifolds via pairwise interaction. We study the characteristics of Chebyshev particles and embed them into sequential MCMC, a novel sampler with a high acceptance ratio that proposes only a few evaluations. We have achieved high performance from the experiments for parameter inference in a linear Gaussian state-space model with synthetic data and a non-linear stochastic volatility mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06364</link><description>&lt;p&gt;
&#22522;&#20110;&#26694;&#26550;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#30001;&#22238;&#31572;&#30340;&#23450;&#24615;&#20998;&#26512;&#65306;&#31639;&#27861;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21487;&#20197;&#27169;&#25311;&#33258;&#30001;&#22238;&#31572;&#38754;&#35797;&#38382;&#39064;&#65292;&#23601;&#20687;&#20256;&#32479;&#19978;&#20351;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#20998;&#26512;&#30340;&#37027;&#26679;&#12290;&#23450;&#24615;&#26041;&#27861;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#28041;&#21450;&#23545;&#24320;&#25918;&#24335;&#35775;&#35848;&#25110;&#33258;&#30001;&#36827;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#30340;&#25163;&#21160;&#20998;&#26512;&#12290;&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#23450;&#24615;&#26041;&#27861;&#23545;LLMs&#29983;&#25104;&#30340;"&#30789;&#21442;&#19982;&#32773;"&#36827;&#34892;&#30740;&#31350;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#32676;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#27010;&#24565;&#26159;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#36825;&#26159;&#30001;Argyle&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24341;&#20837;&#30340;&#19968;&#20010;&#26415;&#35821;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20122;&#32676;&#20307;&#30340;&#20449;&#24565;&#21644;&#24577;&#24230;&#30340;&#31243;&#24230;&#30456;&#21563;&#21512;&#12290;&#26681;&#25454;&#23450;&#20041;&#65292;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#34920;&#26126;&#20174;LLMs&#20013;&#25552;&#21462;&#30340;&#28508;&#22312;&#20449;&#24565;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#65292;&#32780;&#20302;&#31639;&#27861;&#20445;&#30495;&#24230;&#21017;&#20351;&#24471;&#36825;&#26679;&#30340;&#30740;&#31350;&#26080;&#25928;&#12290;&#26412;&#25991;&#20351;&#29992;LLM&#29983;&#25104;&#38754;&#35797;&#38382;&#31572;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Today, using Large-scale generative Language Models (LLMs) it is possible to simulate free responses to interview questions like those traditionally analyzed using qualitative research methods. Qualitative methodology encompasses a broad family of techniques involving manual analysis of open-ended interviews or conversations conducted freely in natural language. Here we consider whether artificial "silicon participants" generated by LLMs may be productively studied using qualitative methods aiming to produce insights that could generalize to real human populations. The key concept in our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023) capturing the degree to which LLM-generated outputs mirror human sub-populations' beliefs and attitudes. By definition, high algorithmic fidelity suggests latent beliefs elicited from LLMs may generalize to real humans, whereas low algorithmic fidelity renders such research invalid. Here we used an LLM to generate interviews wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06358</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#38382;&#31572;&#20013;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering. (arXiv:2309.06358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#38382;&#31572;&#29615;&#22659;&#20013;&#65292;&#23545;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#30740;&#31350;&#24037;&#20316;&#20173;&#22312;&#19981;&#26029;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#22495;&#27867;&#21270;&#27010;&#24565;&#21364;&#21463;&#21040;&#24456;&#23569;&#20851;&#27880;&#65292;&#22240;&#20026;&#30446;&#26631;&#22495;&#26159;&#26410;&#30693;&#30340;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#21644;&#33719;&#21462;&#26041;&#24335;&#30340;&#22823;&#24133;&#25552;&#39640;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#22914;&#20309;&#24433;&#21709;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#8220;&#37326;&#22806;&#29983;&#25104;&#8221;&#22914;&#20309;&#24110;&#21161;&#23454;&#29616;&#22495;&#27867;&#21270;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#20004;&#27493;&#29983;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#19978;&#19979;&#25991;&#21644;&#38382;&#31572;&#23545;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#26469;&#25552;&#21319;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness in Natural Language Processing continues to be a pertinent issue, where state of the art models under-perform under naturally shifted distributions. In the context of Question Answering, work on domain adaptation methods continues to be a growing body of research. However, very little attention has been given to the notion of domain generalization under natural distribution shifts, where the target domain is unknown. With drastic improvements in the quality and access to generative models, we answer the question: How do generated datasets influence the performance of QA models under natural distribution shifts? We perform experiments on 4 different datasets under varying amounts of distribution shift, and analyze how "in-the-wild" generation can help achieve domain generalization. We take a two-step generation approach, generating both contexts and QA pairs to augment existing datasets. Through our experiments, we demonstrate how augmenting reading comprehension datasets wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#35757;&#32451;&#26032;&#20852;&#35821;&#35328;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#26469;&#21457;&#23637;&#22522;&#20110;&#23454;&#36341;&#30340;&#35270;&#35273;&#25968;&#25454;&#34920;&#36848;&#30340;&#31169;&#20154;&#35821;&#35328;&#12290;&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#36716;&#25442;&#31526;&#21495;&#34920;&#31034;&#21040;&#31867;&#21035;&#26631;&#31614;&#65292;&#24182;&#24212;&#29992;&#20110;&#29289;&#20307;&#35782;&#21035;&#21644;&#21160;&#20316;&#35782;&#21035;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.06335</link><description>&lt;p&gt;
&#20174;&#29289;&#20307;&#21644;&#21160;&#20316;&#24847;&#35937;&#20013;&#33719;&#24471;&#22522;&#20110;&#23454;&#36341;&#30340;&#35821;&#35328;&#20064;&#24471;
&lt;/p&gt;
&lt;p&gt;
Grounded Language Acquisition From Object and Action Imagery. (arXiv:2309.06335v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#35757;&#32451;&#26032;&#20852;&#35821;&#35328;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#26469;&#21457;&#23637;&#22522;&#20110;&#23454;&#36341;&#30340;&#35270;&#35273;&#25968;&#25454;&#34920;&#36848;&#30340;&#31169;&#20154;&#35821;&#35328;&#12290;&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#36716;&#25442;&#31526;&#21495;&#34920;&#31034;&#21040;&#31867;&#21035;&#26631;&#31614;&#65292;&#24182;&#24212;&#29992;&#20110;&#29289;&#20307;&#35782;&#21035;&#21644;&#21160;&#20316;&#35782;&#21035;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#20102;&#20256;&#36798;&#20102;&#22823;&#37327;&#22810;&#26679;&#21270;&#30693;&#35782;&#30340;&#31526;&#21495;&#65292;&#20294;&#22914;&#20309;&#23558;&#36825;&#20123;&#31526;&#21495;&#19982;&#26469;&#33258;&#19990;&#30028;&#30340;&#25968;&#25454;&#32852;&#31995;&#36215;&#26469;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#20256;&#32479;&#30340;&#25351;&#20195;&#28216;&#25103;&#29615;&#22659;&#21644;&#21033;&#29992;&#31867;&#20869;&#21305;&#37197;&#35757;&#32451;&#33539;&#24335;&#30340;&#23545;&#27604;&#23398;&#20064;&#29615;&#22659;&#20013;&#35757;&#32451;&#26032;&#20852;&#35821;&#35328;&#65288;EL&#65289;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#26469;&#24320;&#21457;&#35270;&#35273;&#25968;&#25454;&#34920;&#36848;&#30340;&#31169;&#20154;&#35821;&#35328;&#30340;&#21457;&#23637;&#12290;&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#26469;&#36716;&#25442;&#31526;&#21495;&#34920;&#31034;&#65288;&#25972;&#25968;&#31526;&#21495;&#24207;&#21015;&#65289;&#21040;&#31867;&#21035;&#26631;&#31614;&#30340;&#39069;&#22806;&#20998;&#31867;&#23618;&#12290;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#23454;&#39564;&#65292;&#37325;&#28857;&#26159;&#29289;&#20307;&#35782;&#21035;&#21644;&#21160;&#20316;&#35782;&#21035;&#12290;&#23545;&#20110;&#29289;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#20102;&#30001;&#30495;&#23454;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#32472;&#21046;&#30340;&#19968;&#32452;&#32032;&#25551;&#65288;Sketchy&#25968;&#25454;&#38598;&#65289;&#65292;&#23545;&#20110;&#21160;&#20316;&#35782;&#21035;&#65292;&#20351;&#29992;&#20102;2D&#36712;&#36857;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning approaches to natural language processing have made great strides in recent years. While these models produce symbols that convey vast amounts of diverse knowledge, it is unclear how such symbols are grounded in data from the world. In this paper, we explore the development of a private language for visual data representation by training emergent language (EL) encoders/decoders in both i) a traditional referential game environment and ii) a contrastive learning environment utilizing a within-class matching training paradigm. An additional classification layer utilizing neural machine translation and random forest classification was used to transform symbolic representations (sequences of integer symbols) to class labels. These methods were applied in two experiments focusing on object recognition and action recognition. For object recognition, a set of sketches produced by human participants from real imagery was used (Sketchy dataset) and for action recognition, 2D traje
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#21453;&#39304;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#26469;&#23398;&#20064;&#26368;&#31616;&#21270;&#30340;Tsetlin&#26426;&#22120;&#23376;&#21477;&#65292;&#20197;&#25552;&#20379;&#26368;&#20339;&#30340;&#29305;&#24449;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.06315</link><description>&lt;p&gt;
&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#24341;&#23548;&#20462;&#21098;&#23398;&#20064;&#26368;&#31616;&#21270;Tsetlin&#26426;&#22120;&#23376;&#21477;
&lt;/p&gt;
&lt;p&gt;
Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning. (arXiv:2309.06315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#21453;&#39304;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#26469;&#23398;&#20064;&#26368;&#31616;&#21270;&#30340;Tsetlin&#26426;&#22120;&#23376;&#21477;&#65292;&#20197;&#25552;&#20379;&#26368;&#20339;&#30340;&#29305;&#24449;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#21253;&#21547;&#20102;&#39044;&#27979;&#21464;&#37327;&#25152;&#38656;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#37027;&#20040;&#19968;&#32452;&#21464;&#37327;&#23601;&#26159;&#38543;&#26426;&#21464;&#37327;&#30340;&#39532;&#23572;&#31185;&#22827;&#30422;&#34987;&#12290;&#22914;&#26524;&#30422;&#34987;&#26080;&#27861;&#20943;&#23569;&#32780;&#19981;&#20002;&#22833;&#26377;&#29992;&#20449;&#24687;&#65292;&#21017;&#34987;&#31216;&#20026;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#12290;&#35782;&#21035;&#38543;&#26426;&#21464;&#37327;&#30340;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#26159;&#26377;&#20248;&#21183;&#30340;&#65292;&#22240;&#20026;&#36793;&#30028;&#22806;&#30340;&#25152;&#26377;&#21464;&#37327;&#37117;&#26159;&#22810;&#20313;&#30340;&#12290;&#22240;&#27492;&#65292;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#25552;&#20379;&#20102;&#26368;&#20339;&#30340;&#29305;&#24449;&#38598;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#20855;&#26377;&#20004;&#20010;&#25361;&#25112;&#12290;&#22914;&#26524;&#20174;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#20013;&#31227;&#38500;&#19968;&#20010;&#25110;&#22810;&#20010;&#21464;&#37327;&#65292;&#36793;&#30028;&#22806;&#30340;&#21464;&#37327;&#21487;&#33021;&#24320;&#22987;&#25552;&#20379;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#36793;&#30028;&#20869;&#30340;&#21464;&#37327;&#21487;&#33021;&#20572;&#27490;&#25552;&#20379;&#20449;&#24687;&#12290;&#27599;&#20010;&#20505;&#36873;&#21464;&#37327;&#30340;&#30495;&#27491;&#20316;&#29992;&#21482;&#26377;&#22312;&#35782;&#21035;&#20102;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#21518;&#25165;&#20250;&#26174;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#21453;&#39304;&#26041;&#26696;&#65292;&#20197;&#34917;&#20805;&#31867;&#22411;I&#21644;&#31867;&#22411;II&#30340;&#21453;&#39304;&#12290;&#35813;&#26041;&#26696;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426; - &#19968;&#31181;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#26426;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
A set of variables is the Markov blanket of a random variable if it contains all the information needed for predicting the variable. If the blanket cannot be reduced without losing useful information, it is called a Markov boundary. Identifying the Markov boundary of a random variable is advantageous because all variables outside the boundary are superfluous. Hence, the Markov boundary provides an optimal feature set. However, learning the Markov boundary from data is challenging for two reasons. If one or more variables are removed from the Markov boundary, variables outside the boundary may start providing information. Conversely, variables within the boundary may stop providing information. The true role of each candidate variable is only manifesting when the Markov boundary has been identified. In this paper, we propose a new Tsetlin Machine (TM) feedback scheme that supplements Type I and Type II feedback. The scheme introduces a novel Finite State Automaton - a Context-Specific I
&lt;/p&gt;</description></item><item><title>AI4Food-NutritionFW&#26159;&#19968;&#20010;&#26032;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20010;&#20307;&#30340;&#20581;&#24247;&#24773;&#20917;&#20197;&#21450;&#26681;&#25454;&#21487;&#37197;&#32622;&#30340;&#39278;&#39135;&#34892;&#20026;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#65292;&#26159;&#30740;&#31350;&#39278;&#39135;&#34892;&#20026;&#21644;&#25913;&#21892;&#33829;&#20859;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.06308</link><description>&lt;p&gt;
AI4Food-NutritionFW&#65306;&#29992;&#20110;&#33258;&#21160;&#21512;&#25104;&#21644;&#20998;&#26512;&#39278;&#39135;&#34892;&#20026;&#30340;&#26032;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AI4Food-NutritionFW: A Novel Framework for the Automatic Synthesis and Analysis of Eating Behaviours. (arXiv:2309.06308v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06308
&lt;/p&gt;
&lt;p&gt;
AI4Food-NutritionFW&#26159;&#19968;&#20010;&#26032;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20010;&#20307;&#30340;&#20581;&#24247;&#24773;&#20917;&#20197;&#21450;&#26681;&#25454;&#21487;&#37197;&#32622;&#30340;&#39278;&#39135;&#34892;&#20026;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#65292;&#26159;&#30740;&#31350;&#39278;&#39135;&#34892;&#20026;&#21644;&#25913;&#21892;&#33829;&#20859;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#21644;&#32593;&#32476;&#24179;&#21488;&#19978;&#20998;&#20139;&#20102;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#22270;&#29255;&#12290;&#20854;&#20013;&#35768;&#22810;&#26159;&#20174;&#26234;&#33021;&#25163;&#26426;&#19978;&#25293;&#25668;&#30340;&#39135;&#29289;&#22270;&#29255;&#65292;&#38543;&#26102;&#38388;&#25552;&#20379;&#20102;&#19982;&#20010;&#20307;&#39278;&#39135;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#39278;&#39135;&#34892;&#20026;&#19982;&#19990;&#30028;&#19978;&#19968;&#20123;&#26368;&#24120;&#35265;&#30340;&#30142;&#30149;&#30452;&#25509;&#30456;&#20851;&#12290;&#21033;&#29992;&#22270;&#20687;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36825;&#31181;&#24773;&#20917;&#20026;&#65306;i&#65289;&#21019;&#24314;&#20998;&#26512;&#20010;&#20307;&#20581;&#24247;&#30340;&#26032;&#26041;&#27861;&#65292;&#26469;&#28304;&#20110;&#20182;&#20204;&#25152;&#21507;&#30340;&#19996;&#35199;&#65292;&#20197;&#21450; ii&#65289;&#22312;&#29305;&#23450;&#24773;&#20917;&#65288;&#22914;&#32933;&#32982;&#30151;&#25110;COVID&#65289;&#19979;&#21046;&#23450;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#25913;&#21892;&#33829;&#20859;&#21644;&#39278;&#39135;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26426;&#20250;&#12290;&#24456;&#26377;&#24517;&#35201;&#25317;&#26377;&#21487;&#35843;&#33410;&#30340;&#24037;&#20855;&#26469;&#21019;&#24314;&#39135;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#36825;&#20004;&#20010;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AI4Food-NutritionFW&#65292;&#19968;&#20010;&#26681;&#25454;&#21487;&#37197;&#32622;&#30340;&#39278;&#39135;&#34892;&#20026;&#21019;&#24314;&#39135;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;AI4Food-NutritionFW&#27169;&#25311;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays millions of images are shared on social media and web platforms. In particular, many of them are food images taken from a smartphone over time, providing information related to the individual's diet. On the other hand, eating behaviours are directly related to some of the most prevalent diseases in the world. Exploiting recent advances in image processing and Artificial Intelligence (AI), this scenario represents an excellent opportunity to: i) create new methods that analyse the individuals' health from what they eat, and ii) develop personalised recommendations to improve nutrition and diet under specific circumstances (e.g., obesity or COVID). Having tunable tools for creating food image datasets that facilitate research in both lines is very much needed.  This paper proposes AI4Food-NutritionFW, a framework for the creation of food image datasets according to configurable eating behaviours. AI4Food-NutritionFW simulates a user-friendly and widespread scenario where images 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#30693;&#35782;&#36801;&#31227;&#24615;&#20998;&#26512;&#26694;&#26550;&#26469;&#25903;&#25345;&#25968;&#25454;&#39537;&#21160;&#30340;&#22686;&#26448;&#21046;&#36896;&#30693;&#35782;&#30340;&#36801;&#31227;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#21508;&#31181;&#22686;&#26448;&#21046;&#36896;&#25216;&#26415;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20174;&#19968;&#20010;&#24037;&#33402;&#25110;&#38382;&#39064;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#24037;&#33402;&#25110;&#38382;&#39064;&#20013;&#12290;</title><link>http://arxiv.org/abs/2309.06286</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#22686;&#26448;&#21046;&#36896;&#30693;&#35782;&#30340;&#21487;&#36801;&#31227;&#24615;&#20998;&#26512;&#65306;&#31881;&#24202;&#29076;&#21270;&#21644;&#23450;&#21521;&#33021;&#37327;&#27785;&#31215;&#20043;&#38388;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition. (arXiv:2309.06286v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#30693;&#35782;&#36801;&#31227;&#24615;&#20998;&#26512;&#26694;&#26550;&#26469;&#25903;&#25345;&#25968;&#25454;&#39537;&#21160;&#30340;&#22686;&#26448;&#21046;&#36896;&#30693;&#35782;&#30340;&#36801;&#31227;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#21508;&#31181;&#22686;&#26448;&#21046;&#36896;&#25216;&#26415;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20174;&#19968;&#20010;&#24037;&#33402;&#25110;&#38382;&#39064;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#24037;&#33402;&#25110;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22686;&#26448;&#21046;&#36896;&#65288;AM&#65289;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#31185;&#23398;&#25991;&#29486;&#12290;&#36825;&#20123;&#30740;&#31350;&#20013;&#30340;&#30693;&#35782;&#28041;&#21450;&#21040;AM&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#65292;&#20294;&#27809;&#26377;&#20197;&#19968;&#31181;&#25972;&#21512;&#30340;&#26041;&#24335;&#36827;&#34892;&#25366;&#25496;&#21644;&#24418;&#24335;&#21270;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#27809;&#26377;&#25903;&#25345;&#25968;&#25454;&#39537;&#21160;&#30693;&#35782;&#20174;&#19968;&#20010;&#19978;&#19979;&#25991;&#36801;&#31227;&#21040;&#21478;&#19968;&#20010;&#19978;&#19979;&#25991;&#30340;&#24037;&#20855;&#25110;&#25351;&#21335;&#12290;&#22240;&#27492;&#65292;&#20165;&#38024;&#23545;&#29305;&#23450;&#30340;AM&#24037;&#33402;&#25216;&#26415;&#24320;&#21457;&#21644;&#39564;&#35777;&#20102;&#29305;&#23450;&#30340;AI&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#12290;&#26377;&#28508;&#21147;&#21033;&#29992;&#21508;&#31181;AM&#25216;&#26415;&#20043;&#38388;&#30340;&#20869;&#22312;&#30456;&#20284;&#24615;&#65292;&#21033;&#29992;AI&#65288;&#22914;&#36801;&#31227;&#23398;&#20064;&#65289;&#20174;&#19968;&#20010;&#24037;&#33402;&#25110;&#38382;&#39064;&#20013;&#36866;&#24212;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#30693;&#35782;&#36801;&#31227;&#24615;&#20998;&#26512;&#26694;&#26550;&#26469;&#25903;&#25345;&#25968;&#25454;&#39537;&#21160;&#30340;AM&#30693;&#35782;&#36801;&#31227;&#12290;&#20316;&#20026;&#36801;&#31227;&#24615;&#20998;&#26512;&#30340;&#20808;&#20915;&#26465;&#20214;&#65292;AM&#30693;&#35782;&#34987;&#36716;&#21270;&#20026;&#35782;&#21035;&#20986;&#30340;&#30693;&#35782;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven research in Additive Manufacturing (AM) has gained significant success in recent years. This has led to a plethora of scientific literature to emerge. The knowledge in these works consists of AM and Artificial Intelligence (AI) contexts that have not been mined and formalized in an integrated way. Moreover, no tools or guidelines exist to support data-driven knowledge transfer from one context to another. As a result, data-driven solutions using specific AI techniques are being developed and validated only for specific AM process technologies. There is a potential to exploit the inherent similarities across various AM technologies and adapt the existing solutions from one process or problem to another using AI, such as Transfer Learning. We propose a three-step knowledge transferability analysis framework in AM to support data-driven AM knowledge transfer. As a prerequisite to transferability analysis, AM knowledge is featurized into identified knowledge components. The fra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#20998;&#36776;&#29575;&#24191;&#25773;&#35270;&#39057;&#20013;&#35782;&#21035;&#29699;&#34915;&#21495;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#38190;&#24103;&#35782;&#21035;&#25552;&#21462;&#21253;&#21547;&#29699;&#34915;&#21495;&#30721;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#26102;&#31354;&#32593;&#32476;&#39044;&#27979;&#29699;&#34915;&#21495;&#30721;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.06285</link><description>&lt;p&gt;
&#20302;&#20998;&#36776;&#29575;&#24191;&#25773;&#35270;&#39057;&#20013;&#30340;&#29699;&#34915;&#21495;&#30721;&#35782;&#21035;&#65306;&#22522;&#20110;&#20851;&#38190;&#24103;&#35782;&#21035;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Jersey Number Recognition using Keyframe Identification from Low-Resolution Broadcast Videos. (arXiv:2309.06285v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#20998;&#36776;&#29575;&#24191;&#25773;&#35270;&#39057;&#20013;&#35782;&#21035;&#29699;&#34915;&#21495;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#38190;&#24103;&#35782;&#21035;&#25552;&#21462;&#21253;&#21547;&#29699;&#34915;&#21495;&#30721;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#26102;&#31354;&#32593;&#32476;&#39044;&#27979;&#29699;&#34915;&#21495;&#30721;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29699;&#21592;&#35782;&#21035;&#26159;&#36816;&#29992;&#35270;&#35273;&#20998;&#26512;&#30340;&#36275;&#29699;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#29699;&#21592;&#35780;&#20272;&#12289;&#27604;&#36187;&#20998;&#26512;&#21644;&#24191;&#25773;&#21046;&#20316;&#31561;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#39057;&#20013;&#33258;&#21160;&#26816;&#27979;&#29699;&#21592;&#36712;&#36857;&#20013;&#30340;&#29699;&#34915;&#21495;&#30721;&#23384;&#22312;&#30528;&#22240;&#36816;&#21160;&#27169;&#31946;&#12289;&#20302;&#20998;&#36776;&#29575;&#12289;&#30072;&#21464;&#21644;&#36974;&#25377;&#31561;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#31354;&#38388;&#21464;&#25442;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#22270;&#20687;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#39057;&#25968;&#25454;&#19978;&#20173;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#24103;&#20013;&#29699;&#34915;&#21495;&#30721;&#26159;&#19981;&#21487;&#35265;&#30340;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#21253;&#21547;&#29699;&#34915;&#21495;&#30721;&#30340;&#20851;&#38190;&#24103;&#25104;&#20026;&#35299;&#20915;&#38382;&#39064;&#30340;&#19968;&#20010;&#20851;&#38190;&#23376;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#20851;&#38190;&#24103;&#35782;&#21035;&#27169;&#22359;&#65292;&#25552;&#21462;&#21253;&#21547;&#29699;&#34915;&#21495;&#30721;&#30340;&#37325;&#35201;&#39640;&#32423;&#20449;&#24687;&#30340;&#24103;&#12290;&#28982;&#21518;&#65292;&#37319;&#29992;&#26102;&#31354;&#32593;&#32476;&#26469;&#24314;&#27169;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#24182;&#39044;&#27979;&#35270;&#39057;&#20013;&#29699;&#34915;&#21495;&#30721;&#30340;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#31181;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#29699;&#34915;&#21495;&#30721;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Player identification is a crucial component in vision-driven soccer analytics, enabling various downstream tasks such as player assessment, in-game analysis, and broadcast production. However, automatically detecting jersey numbers from player tracklets in videos presents challenges due to motion blur, low resolution, distortions, and occlusions. Existing methods, utilizing Spatial Transformer Networks, CNNs, and Vision Transformers, have shown success in image data but struggle with real-world video data, where jersey numbers are not visible in most of the frames. Hence, identifying frames that contain the jersey number is a key sub-problem to tackle. To address these issues, we propose a robust keyframe identification module that extracts frames containing essential high-level information about the jersey number. A spatio-temporal network is then employed to model spatial and temporal context and predict the probabilities of jersey numbers in the video. Additionally, we adopt a mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.06255</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#22686;&#24378;&#22810;&#27169;&#24577;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24322;&#36136;&#20449;&#24687;&#20849;&#21516;&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21327;&#20316;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#23613;&#20154;&#24847;&#30340;&#38382;&#39064;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#20849;&#21516;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#12290;&#19968;&#20123;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#35782;&#21035;&#21644;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#36739;&#24046;&#30340;&#27169;&#24577;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#23545;&#26679;&#26412;&#32423;&#21035;&#22810;&#27169;&#24577;&#21327;&#20316;&#30340;&#32454;&#31890;&#24230;&#35266;&#23519;&#21644;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#35266;&#23519;&#21644;&#25913;&#36827;&#27169;&#24577;&#20043;&#38388;&#32454;&#31890;&#24230;&#30340;&#21327;&#20316;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#27169;&#24577;&#24046;&#24322;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#21487;&#33021;&#21464;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#26102;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#27169;&#24577;&#35780;&#20272;&#65292;&#25105;&#20204;&#36951;&#25022;&#22320;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this iss
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#33402;&#26415;&#20013;&#30340;&#31105;&#20196;&#65288;XAIxArt&#65289;&#24341;&#21457;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;XAIxArt&#26088;&#22312;&#35299;&#20915;&#20154;&#31867;&#20013;&#24515;&#30340;&#33402;&#26415;&#35266;&#24565;&#30340;&#19981;&#23433;&#21644;&#23545;&#36807;&#26102;&#30340;&#20316;&#32773;&#27010;&#24565;&#30340;&#24576;&#26087;&#24895;&#26395;&#12290;</title><link>http://arxiv.org/abs/2309.06227</link><description>&lt;p&gt;
&#20851;&#20110;XAIxArt&#30340;&#34987;&#31105;&#20196;&#65288;arXiv:2309.06227v1 [cs.HC]&#65289;
&lt;/p&gt;
&lt;p&gt;
On the Injunction of XAIxArt. (arXiv:2309.06227v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06227
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#33402;&#26415;&#20013;&#30340;&#31105;&#20196;&#65288;XAIxArt&#65289;&#24341;&#21457;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;XAIxArt&#26088;&#22312;&#35299;&#20915;&#20154;&#31867;&#20013;&#24515;&#30340;&#33402;&#26415;&#35266;&#24565;&#30340;&#19981;&#23433;&#21644;&#23545;&#36807;&#26102;&#30340;&#20316;&#32773;&#27010;&#24565;&#30340;&#24576;&#26087;&#24895;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24378;&#35843;&#20102;&#22312;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#33402;&#26415;&#20013;&#30340;&#31105;&#20196;&#65288;XAIxArt&#65289;&#20013;&#28041;&#21450;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#31616;&#30701;&#30340;&#23376;&#38382;&#39064;&#65292;&#23427;&#25351;&#20986;&#20102;&#19982;&#8220;&#35299;&#37322;&#8221;&#21644;&#8220;&#30456;&#20851;&#35299;&#37322;&#8221;&#30340;&#21547;&#31946;&#20043;&#22788;&#12290;&#26412;&#25991;&#25298;&#32477;&#20102;&#8220;&#35299;&#37322;&#8221;&#21644;&#8220;&#30456;&#20851;&#35299;&#37322;&#8221;&#65292;&#35748;&#20026;XAIxArt&#26159;&#23545;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#33402;&#26415;&#35266;&#24565;&#30340;&#19981;&#23433;&#21644;&#23545;&#36807;&#26102;&#30340;&#20316;&#32773;&#27010;&#24565;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#24576;&#26087;&#24895;&#26395;&#30340;&#30151;&#29366;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#31435;&#22330;&#65292;&#26412;&#25991;&#23558;&#35299;&#37322;&#27169;&#22411;&#20174;&#35013;&#39280;&#27169;&#22411;&#36716;&#21464;&#20026;&#24847;&#20041;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The position paper highlights the range of concerns that are engulfed in the injunction of explainable artificial intelligence in art (XAIxArt). Through a series of quick sub-questions, it points towards the ambiguities concerning 'explanation' and the postpositivist tradition of 'relevant explanation'. Rejecting both 'explanation' and 'relevant explanation', the paper takes a stance that XAIxArt is a symptom of insecurity of the anthropocentric notion of art and a nostalgic desire to return to outmoded notions of authorship and human agency. To justify this stance, the paper makes a distinction between an ornamentation model of explanation to a model of explanation as sense-making.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#30001;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#20197;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#20301;&#65292;&#24182;&#30830;&#23450;&#20102;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65292;&#25581;&#31034;&#20102;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#25915;&#20987;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.06223</link><description>&lt;p&gt;
&#25581;&#31034;&#23545;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Unveiling Signle-Bit-Flip Attacks on DNN Executables. (arXiv:2309.06223v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06223
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30001;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#20197;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#20301;&#65292;&#24182;&#30830;&#23450;&#20102;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65292;&#25581;&#31034;&#20102;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#25915;&#20987;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20301;&#32763;&#36716;&#25915;&#20987;(BFA)&#21487;&#20197;&#36890;&#36807;DRAM Rowhammer&#21033;&#29992;&#26469;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#20027;&#35201;&#38024;&#23545;&#39640;&#32423;DNN&#26694;&#26550;&#65288;&#22914;PyTorch&#65289;&#20013;&#30340;&#27169;&#22411;&#26435;&#37325;&#25991;&#20214;&#36827;&#34892;&#20301;&#32763;&#36716;&#12290;&#28982;&#32780;&#65292;DNN&#32463;&#24120;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#25104;&#20302;&#32423;&#21487;&#25191;&#34892;&#25991;&#20214;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20302;&#32423;&#30828;&#20214;&#21407;&#35821;&#12290;&#32534;&#35793;&#21518;&#30340;&#20195;&#30721;&#36890;&#24120;&#36895;&#24230;&#24456;&#24555;&#65292;&#24182;&#19988;&#19982;&#39640;&#32423;DNN&#26694;&#26550;&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#30340;&#25191;&#34892;&#33539;&#24335;&#12290;&#26412;&#25991;&#38024;&#23545;&#30001;DL&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;BFA&#25915;&#20987;&#38754;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#65292;&#29992;&#20110;&#35782;&#21035;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20013;&#30340;&#26131;&#21463;&#25915;&#20987;&#20301;&#65292;&#24182;&#30830;&#23450;&#21033;&#29992;BFAs&#25915;&#20987;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20013;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65288;&#32780;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#23545;&#25915;&#20987;&#27169;&#22411;&#26435;&#37325;&#20570;&#20986;&#20102;&#24378;&#20551;&#35774;&#65289;&#12290;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20284;&#20046;&#27604;&#39640;&#32423;DNN&#20013;&#30340;&#27169;&#22411;&#26356;&#21152;&#8220;&#19981;&#36879;&#26126;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that bit-flip attacks (BFAs) can manipulate deep neural networks (DNNs) via DRAM Rowhammer exploitations. Existing attacks are primarily launched over high-level DNN frameworks like PyTorch and flip bits in model weight files. Nevertheless, DNNs are frequently compiled into low-level executables by deep learning (DL) compilers to fully leverage low-level hardware primitives. The compiled code is usually high-speed and manifests dramatically distinct execution paradigms from high-level DNN frameworks.  In this paper, we launch the first systematic study on the attack surface of BFA specifically for DNN executables compiled by DL compilers. We design an automated search tool to identify vulnerable bits in DNN executables and identify practical attack vectors that exploit the model structure in DNN executables with BFAs (whereas prior works make likely strong assumptions to attack model weights). DNN executables appear more "opaque" than models in high-level DNN 
&lt;/p&gt;</description></item><item><title>SCP&#25552;&#20986;&#20102;&#19968;&#31181;&#22330;&#26223;&#23436;&#25972;&#24615;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25913;&#21892;&#28857;&#20113;&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#12289;&#28040;&#38500;&#23545;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#21644;&#20943;&#23569;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.06199</link><description>&lt;p&gt;
SCP: &#22330;&#26223;&#23436;&#25972;&#24615;&#39044;&#35757;&#32451;&#29992;&#20110;3D&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SCP: Scene Completion Pre-training for 3D Object Detection. (arXiv:2309.06199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06199
&lt;/p&gt;
&lt;p&gt;
SCP&#25552;&#20986;&#20102;&#19968;&#31181;&#22330;&#26223;&#23436;&#25972;&#24615;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25913;&#21892;&#28857;&#20113;&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#12289;&#28040;&#38500;&#23545;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#21644;&#20943;&#23569;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#39046;&#22495;&#65292;&#20351;&#29992;LiDAR&#28857;&#20113;&#36827;&#34892;3D&#30446;&#26631;&#26816;&#27979;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;3D&#26816;&#27979;&#22120;&#20005;&#37325;&#20381;&#36182;&#20110;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#36825;&#22312;&#26631;&#35760;3D&#36793;&#30028;&#26694;&#30340;&#36807;&#31243;&#20013;&#26082;&#32791;&#26102;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22330;&#26223;&#23436;&#25972;&#24615;&#39044;&#35757;&#32451;&#65288;SCP&#65289;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20351;&#29992;&#36739;&#23569;&#26631;&#35760;&#25968;&#25454;&#30340;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;SCP&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;&#25913;&#21892;&#28857;&#20113;&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#12290;&#36890;&#36807;&#23436;&#21892;&#22330;&#26223;&#28857;&#20113;&#65292;SCP&#26377;&#25928;&#25429;&#25417;&#22478;&#24066;&#29615;&#22659;&#20013;&#29289;&#20307;&#38388;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20851;&#31995;&#12290;&#65288;2&#65289;&#28040;&#38500;&#23545;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;SCP&#20316;&#20026;&#26377;&#20215;&#20540;&#30340;&#36741;&#21161;&#32593;&#32476;&#65292;&#19981;&#20250;&#23545;3D&#26816;&#27979;&#22120;&#26045;&#21152;&#20219;&#20309;&#39069;&#22806;&#30340;&#24037;&#20316;&#25110;&#25968;&#25454;&#35201;&#27714;&#12290;&#65288;3&#65289;&#20943;&#23569;&#30446;&#26631;&#26816;&#27979;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;&#20511;&#21161;SCP&#30340;&#24110;&#21161;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;3D&#26816;&#27979;&#22120;&#21487;&#20197;&#23454;&#29616;&#19982;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D object detection using LiDAR point clouds is a fundamental task in the fields of computer vision, robotics, and autonomous driving. However, existing 3D detectors heavily rely on annotated datasets, which are both time-consuming and prone to errors during the process of labeling 3D bounding boxes. In this paper, we propose a Scene Completion Pre-training (SCP) method to enhance the performance of 3D object detectors with less labeled data. SCP offers three key advantages: (1) Improved initialization of the point cloud model. By completing the scene point clouds, SCP effectively captures the spatial and semantic relationships among objects within urban environments. (2) Elimination of the need for additional datasets. SCP serves as a valuable auxiliary network that does not impose any additional efforts or data requirements on the 3D detectors. (3) Reduction of the amount of labeled data for detection. With the help of SCP, the existing state-of-the-art 3D detectors can achieve compa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;LiDAR&#20998;&#21106;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#25945;&#24072;&#32593;&#32476;&#22312;&#21333;&#20010;&#25668;&#20687;&#26426;&#35270;&#35282;&#19979;&#29983;&#25104;&#35821;&#20041;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#26631;&#24230;&#39640;&#25928;&#30340;LiDAR&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2309.06197</link><description>&lt;p&gt;
&#19968;&#21488;&#25668;&#20687;&#26426;&#30340;360&#176;&#35270;&#35282;&#65306;&#29992;&#20110;LiDAR&#20998;&#21106;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
360$^\circ$ from a Single Camera: A Few-Shot Approach for LiDAR Segmentation. (arXiv:2309.06197v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;LiDAR&#20998;&#21106;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#25945;&#24072;&#32593;&#32476;&#22312;&#21333;&#20010;&#25668;&#20687;&#26426;&#35270;&#35282;&#19979;&#29983;&#25104;&#35821;&#20041;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#26631;&#24230;&#39640;&#25928;&#30340;LiDAR&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LiDAR&#25968;&#25454;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#25110;&#20219;&#21153;&#26102;&#65292;&#20250;&#36973;&#21463;&#21040;&#24378;&#28872;&#30340;&#22495;&#38388;&#24046;&#36317;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#33719;&#24471;&#19982;&#20844;&#20849;&#22522;&#20934;&#25253;&#21578;&#20540;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#38656;&#35201;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24102;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26631;&#27880;&#25968;&#25454;&#30340;&#33719;&#21462;&#25104;&#26412;&#21644;&#26102;&#38388;&#25104;&#26412;&#24456;&#39640;&#12290;&#36825;&#20123;&#22240;&#32032;&#24341;&#21457;&#20102;&#23545;&#26631;&#24230;&#25928;&#29575;&#26041;&#27861;&#30340;&#21508;&#31181;&#30740;&#31350;&#65292;&#20294;&#19982;&#23436;&#20840;&#30417;&#30563;&#30340;&#23545;&#29031;&#26041;&#27861;&#30456;&#27604;&#65292;&#20173;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ImageTo360&#65292;&#19968;&#31181;&#26377;&#25928;&#19988;&#31616;&#27905;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#65292;&#29992;&#20110;&#26631;&#24230;&#39640;&#25928;&#30340;LiDAR&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#20687;&#25945;&#24072;&#32593;&#32476;&#22312;&#21333;&#20010;&#25668;&#20687;&#26426;&#35270;&#35282;&#19979;&#20026;LiDAR&#25968;&#25454;&#29983;&#25104;&#35821;&#20041;&#39044;&#27979;&#12290;&#22312;&#23545;360&#176;&#25968;&#25454;&#36827;&#34892;&#21487;&#36873;&#24494;&#35843;&#20043;&#21069;&#65292;&#20351;&#29992;&#25945;&#24072;&#32593;&#32476;&#23545;LiDAR&#20998;&#21106;&#23398;&#29983;&#32593;&#32476;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#22312;&#28857;&#32423;&#21035;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning applications on LiDAR data suffer from a strong domain gap when applied to different sensors or tasks. In order for these methods to obtain similar accuracy on different data in comparison to values reported on public benchmarks, a large scale annotated dataset is necessary. However, in practical applications labeled data is costly and time consuming to obtain. Such factors have triggered various research in label-efficient methods, but a large gap remains to their fully-supervised counterparts. Thus, we propose ImageTo360, an effective and streamlined few-shot approach to label-efficient LiDAR segmentation. Our method utilizes an image teacher network to generate semantic predictions for LiDAR data within a single camera view. The teacher is used to pretrain the LiDAR segmentation student network, prior to optional fine-tuning on 360$^\circ$ data. Our method is implemented in a modular manner on the point level and as such is generalizable to different architectures. We 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;3M&#28151;&#21512;&#27169;&#22411;&#26469;&#24674;&#22797;&#27704;&#20048;&#23467;&#29420;&#29305;&#24040;&#24133;&#22721;&#30011;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#20462;&#22797;&#26041;&#27861;&#20013;&#30340;&#39046;&#22495;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#33021;&#24212;&#23545;&#22721;&#30011;&#24040;&#22823;&#23610;&#23544;&#24102;&#26469;&#30340;&#26356;&#24191;&#27867;&#32570;&#38519;&#31867;&#22411;&#21644;&#23610;&#23544;&#33539;&#22260;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.06194</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#24674;&#22797;&#29420;&#29305;&#24040;&#24133;&#22721;&#30011;&#30340;3M&#28151;&#21512;&#27169;&#22411;&#65306;&#20197;&#27704;&#20048;&#23467;&#22721;&#30011;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
A 3M-Hybrid Model for the Restoration of Unique Giant Murals: A Case Study on the Murals of Yongle Palace. (arXiv:2309.06194v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06194
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;3M&#28151;&#21512;&#27169;&#22411;&#26469;&#24674;&#22797;&#27704;&#20048;&#23467;&#29420;&#29305;&#24040;&#24133;&#22721;&#30011;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#20462;&#22797;&#26041;&#27861;&#20013;&#30340;&#39046;&#22495;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#33021;&#24212;&#23545;&#22721;&#30011;&#24040;&#22823;&#23610;&#23544;&#24102;&#26469;&#30340;&#26356;&#24191;&#27867;&#32570;&#38519;&#31867;&#22411;&#21644;&#23610;&#23544;&#33539;&#22260;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26377;&#20215;&#20540;&#30340;&#25991;&#21270;&#36951;&#20135;&#65292;&#27704;&#20048;&#23467;&#22721;&#30011;&#36973;&#21463;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#25439;&#22351;&#65292;&#22240;&#27492;&#23545;&#20854;&#36827;&#34892;&#20462;&#22797;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#27704;&#20048;&#23467;&#22721;&#30011;&#30340;&#24040;&#22823;&#23610;&#23544;&#21644;&#29420;&#29305;&#25968;&#25454;&#32473;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#22797;&#26041;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65306;1&#65289;&#29420;&#29305;&#30340;&#39118;&#26684;&#22312;&#20256;&#32479;&#30340;&#36801;&#31227;&#23398;&#20064;&#20462;&#22797;&#26041;&#27861;&#20013;&#24341;&#20837;&#20102;&#39046;&#22495;&#20559;&#24046;&#65292;&#32780;&#22721;&#30011;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;2&#65289;&#27492;&#22806;&#65292;&#36825;&#20123;&#22721;&#30011;&#30340;&#24040;&#22823;&#23610;&#23544;&#23548;&#33268;&#20102;&#26356;&#24191;&#27867;&#30340;&#32570;&#38519;&#31867;&#22411;&#21644;&#23610;&#23544;&#33539;&#22260;&#65292;&#38656;&#35201;&#20855;&#26377;&#26356;&#22823;&#36866;&#24212;&#24615;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#32570;&#20047;&#38024;&#23545;&#27704;&#20048;&#23467;&#29420;&#29305;&#24040;&#24133;&#22721;&#30011;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#22797;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#37324;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;3M&#28151;&#21512;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#22721;&#30011;&#25968;&#25454;&#39057;&#29575;&#22312;&#20302;&#39057;&#21644;&#39640;&#39057;&#29305;&#24449;&#20998;&#24067;&#20013;&#26174;&#33879;&#65292;&#39640;&#39057;&#21644;&#20302;&#39057;&#29305;&#24449;&#34987;&#20998;&#24320;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Yongle Palace murals, as valuable cultural heritage, have suffered varying degrees of damage, making their restoration of significant importance. However, the giant size and unique data of Yongle Palace murals present challenges for existing deep-learning based restoration methods: 1) The distinctive style introduces domain bias in traditional transfer learning-based restoration methods, while the scarcity of mural data further limits the applicability of these methods. 2) Additionally, the giant size of these murals results in a wider range of defect types and sizes, necessitating models with greater adaptability. Consequently, there is a lack of focus on deep learning-based restoration methods for the unique giant murals of Yongle Palace. Here, a 3M-Hybrid model is proposed to address these challenges. Firstly, based on the characteristic that the mural data frequency is prominent in the distribution of low and high frequency features, high and low frequency features are separate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#20013;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#30340;&#36880;&#27493;&#20943;&#23569;&#21487;&#29992;&#28304;&#20449;&#24687;&#65292;&#20174;&#25972;&#20010;&#21477;&#23376;&#21040;&#19982;&#24310;&#36831;&#23545;&#24212;&#30340;&#21069;&#32512;&#65292;&#20197;&#23454;&#29616;&#20174;&#24207;&#21015;&#21040;&#24207;&#21015;&#35757;&#32451;&#21040;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#30340;&#36807;&#28193;&#65292;&#20174;&#32780;&#22686;&#24378;SiMT&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06179</link><description>&lt;p&gt;
&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#20013;&#23637;&#26395;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Glancing Future for Simultaneous Machine Translation. (arXiv:2309.06179v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#20013;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#30340;&#36880;&#27493;&#20943;&#23569;&#21487;&#29992;&#28304;&#20449;&#24687;&#65292;&#20174;&#25972;&#20010;&#21477;&#23376;&#21040;&#19982;&#24310;&#36831;&#23545;&#24212;&#30340;&#21069;&#32512;&#65292;&#20197;&#23454;&#29616;&#20174;&#24207;&#21015;&#21040;&#24207;&#21015;&#35757;&#32451;&#21040;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#30340;&#36807;&#28193;&#65292;&#20174;&#32780;&#22686;&#24378;SiMT&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#22312;&#38405;&#35835;&#28304;&#35821;&#21477;&#30340;&#21516;&#26102;&#36755;&#20986;&#32763;&#35793;&#12290;&#19982;&#20256;&#32479;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#35757;&#32451;&#19981;&#21516;&#65292;&#29616;&#26377;&#30340;SiMT&#26041;&#27861;&#37319;&#29992;&#21069;&#32512;&#21040;&#21069;&#32512;&#65288;prefix2prefix&#65289;&#35757;&#32451;&#65292;&#21363;&#27169;&#22411;&#22522;&#20110;&#37096;&#20998;&#28304;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#38477;&#20302;&#20102;&#27169;&#22411;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#24517;&#35201;&#30340;&#28304;&#20449;&#24687;&#32780;&#24341;&#20837;&#20102;&#24378;&#21046;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#24357;&#21512;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#35757;&#32451;&#20043;&#38388;&#24046;&#36317;&#20197;&#22686;&#24378;SiMT&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#35838;&#31243;&#23398;&#20064;&#20013;&#23637;&#26395;&#26410;&#26469;&#65292;&#23454;&#29616;&#20174;&#24207;&#21015;&#21040;&#24207;&#21015;&#35757;&#32451;&#36807;&#28193;&#21040;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36880;&#28176;&#20943;&#23569;&#21487;&#29992;&#30340;&#28304;&#20449;&#24687;&#65292;&#20174;&#25972;&#20010;&#21477;&#23376;&#21040;&#19982;&#24310;&#36831;&#23545;&#24212;&#30340;&#21069;&#32512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;SiMT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous machine translation (SiMT) outputs translation while reading the source sentence. Unlike conventional sequence-to-sequence (seq2seq) training, existing SiMT methods adopt the prefix-to-prefix (prefix2prefix) training, where the model predicts target tokens based on partial source tokens. However, the prefix2prefix training diminishes the ability of the model to capture global information and introduces forced predictions due to the absence of essential source information. Consequently, it is crucial to bridge the gap between the prefix2prefix training and seq2seq training to enhance the translation capability of the SiMT model. In this paper, we propose a novel method that glances future in curriculum learning to achieve the transition from the seq2seq training to prefix2prefix training. Specifically, we gradually reduce the available source information from the whole sentence to the prefix corresponding to that latency. Our method is applicable to a wide range of SiMT met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#31561;&#32452;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#23427;&#22312;&#36724;&#25215;&#26426;&#22120;&#24212;&#29992;&#20013;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06157</link><description>&lt;p&gt;
Robust-MBDL:&#19968;&#31181;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#37492;&#21035;&#30340;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines. (arXiv:2309.06157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#31561;&#32452;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#23427;&#22312;&#36724;&#25215;&#26426;&#22120;&#24212;&#29992;&#20013;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#21097;&#20313;&#23551;&#21629;(RUL)&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;(CO)&#37492;&#21035;&#30340;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#21253;&#25324;&#20027;&#35201;&#32452;&#20214;&#65306;(1)&#37319;&#29992;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#65307;(2)&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#20174;&#21435;&#22122;&#25968;&#25454;&#20013;&#29983;&#25104;&#26102;&#22495;&#12289;&#39057;&#22495;&#21644;&#26102;&#39057;&#22495;&#29305;&#24449;&#65307;(3)&#37319;&#29992;&#26032;&#39062;&#32780;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#26469;&#21033;&#29992;&#22810;&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;XJTU-SY&#21644;PRONOSTIA&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#25105;&#20204;&#30340;&#31995;&#32479;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#20855;&#26377;&#22312;&#36724;&#25215;&#26426;&#22120;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a Robust Multi-branch Deep learning-based system for remaining useful life (RUL) prediction and condition operations (CO) identification of rotating machines is proposed. In particular, the proposed system comprises main components: (1) an LSTM-Autoencoder to denoise the vibration data; (2) a feature extraction to generate time-domain, frequency-domain, and time-frequency based features from the denoised data; (3) a novel and robust multi-branch deep learning network architecture to exploit the multiple features. The performance of our proposed system was evaluated and compared to the state-of-the-art systems on two benchmark datasets of XJTU-SY and PRONOSTIA. The experimental results prove that our proposed system outperforms the state-of-the-art systems and presents potential for real-life applications on bearing machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#20197;&#21450;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26041;&#27861;&#22312;&#22266;&#23450;&#35821;&#26009;&#24211;&#21644;&#22810;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06132</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20013;&#27979;&#37327;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#65306;&#20174;&#31526;&#21495;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;VAGO
&lt;/p&gt;
&lt;p&gt;
Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO. (arXiv:2309.06132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#20197;&#21450;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26041;&#27861;&#22312;&#22266;&#23450;&#35821;&#26009;&#24211;&#21644;&#22810;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#24182;&#22312;&#19968;&#23567;&#32452;&#20107;&#23454;&#19982;&#35266;&#28857;&#21477;&#23376;&#30340;&#22522;&#20934;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#24182;&#22312;&#26356;&#22823;&#30340;&#27861;&#35821;&#26032;&#38395;&#35821;&#26009;&#24211;FreSaDa&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#30830;&#35748;&#35773;&#21050;&#24615;&#25991;&#26412;&#20013;&#20027;&#35266;&#26631;&#35760;&#30340;&#26356;&#39640;&#27969;&#34892;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;VAGO&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26550;&#26500;&#22522;&#20110;&#22312;FreSaDa&#19978;&#33719;&#24471;&#30340;&#31526;&#21495;VAGO&#20998;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65288;LIME&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31070;&#32463;&#29256;&#26412;&#22312;&#20016;&#23500;&#31526;&#21495;&#29256;&#26412;&#30340;&#35789;&#20856;&#21644;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#29256;&#26412;&#26041;&#38754;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a hybrid approach to the automated measurement of vagueness and subjectivity in texts. We first introduce the expert system VAGO, we illustrate it on a small benchmark of fact vs. opinion sentences, and then test it on the larger French press corpus FreSaDa to confirm the higher prevalence of subjective markers in satirical vs. regular texts. We then build a neural clone of VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores obtained on FreSaDa. Using explainability tools (LIME), we show the interest of this neural version for the enrichment of the lexicons of the symbolic version, and for the production of versions in other languages.
&lt;/p&gt;</description></item><item><title>JOADAA&#27169;&#22411;&#23558;&#21160;&#20316;&#39044;&#27979;&#21644;&#22312;&#32447;&#21160;&#20316;&#26816;&#27979;&#20004;&#20010;&#20219;&#21153;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26550;&#26500;&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#25512;&#26029;&#21160;&#20316;&#20381;&#36182;&#20851;&#31995;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06130</link><description>&lt;p&gt;
JOADAA:&#32852;&#21512;&#22312;&#32447;&#21160;&#20316;&#26816;&#27979;&#21644;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
JOADAA: joint online action detection and action anticipation. (arXiv:2309.06130v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06130
&lt;/p&gt;
&lt;p&gt;
JOADAA&#27169;&#22411;&#23558;&#21160;&#20316;&#39044;&#27979;&#21644;&#22312;&#32447;&#21160;&#20316;&#26816;&#27979;&#20004;&#20010;&#20219;&#21153;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26550;&#26500;&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#25512;&#26029;&#21160;&#20316;&#20381;&#36182;&#20851;&#31995;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#20316;&#39044;&#27979;&#28041;&#21450;&#36890;&#36807;&#36830;&#25509;&#36807;&#21435;&#20107;&#20214;&#19982;&#26410;&#26469;&#20107;&#20214;&#26469;&#39044;&#27979;&#26410;&#26469;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25512;&#29702;&#24573;&#35270;&#20102;&#30495;&#23454;&#29983;&#27963;&#20013;&#20107;&#20214;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#23427;&#34987;&#35748;&#20026;&#21253;&#25324;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#12290;&#25105;&#20204;&#35748;&#20026;&#32771;&#34385;&#36825;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#21450;&#20854;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#32447;&#21160;&#20316;&#26816;&#27979;&#26159;&#22312;&#27969;&#24335;&#26041;&#24335;&#19979;&#39044;&#27979;&#21160;&#20316;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21482;&#33021;&#35775;&#38382;&#36807;&#21435;&#21644;&#29616;&#22312;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#22312;&#32447;&#21160;&#20316;&#26816;&#27979;&#20013;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#35821;&#20041;&#25110;&#26410;&#26469;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#23545;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#32570;&#22833;&#20102;&#23436;&#25972;&#30340;&#30693;&#35782;&#38598;&#21512;&#65288;&#36807;&#21435;-&#29616;&#22312;-&#26410;&#26469;&#65289;&#65292;&#36825;&#20351;&#24471;&#25512;&#26029;&#21160;&#20316;&#20381;&#36182;&#20851;&#31995;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#24615;&#33021;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20004;&#20010;&#20219;&#21153;&#34701;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#26550;&#26500;&#12290;&#36890;&#36807;&#32467;&#21512;&#21160;&#20316;&#39044;&#27979;&#21644;&#22312;&#32447;&#21160;&#20316;&#26816;&#27979;&#65292;
&lt;/p&gt;
&lt;p&gt;
Action anticipation involves forecasting future actions by connecting past events to future ones. However, this reasoning ignores the real-life hierarchy of events which is considered to be composed of three main parts: past, present, and future. We argue that considering these three main parts and their dependencies could improve performance. On the other hand, online action detection is the task of predicting actions in a streaming manner. In this case, one has access only to the past and present information. Therefore, in online action detection (OAD) the existing approaches miss semantics or future information which limits their performance. To sum up, for both of these tasks, the complete set of knowledge (past-present-future) is missing, which makes it challenging to infer action dependencies, therefore having low performances. To address this limitation, we propose to fuse both tasks into a single uniform architecture. By combining action anticipation and online action detection
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEyes&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#30524;&#37096;&#22270;&#20687;&#21464;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06129</link><description>&lt;p&gt;
LEyes&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images. (arXiv:2309.06129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEyes&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#30524;&#37096;&#22270;&#20687;&#21464;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21152;&#24378;&#20102;&#20957;&#35270;&#20272;&#35745;&#25216;&#26415;&#65292;&#20294;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#19981;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#30524;&#37096;&#22270;&#20687;&#30340;&#30828;&#20214;&#24341;&#36215;&#30340;&#21464;&#24322;&#20197;&#21450;&#35760;&#24405;&#30340;&#21442;&#19982;&#32773;&#20043;&#38388;&#22266;&#26377;&#30340;&#29983;&#29289;&#24046;&#24322;&#20250;&#23548;&#33268;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#24046;&#24322;&#65292;&#38459;&#30861;&#20102;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#34394;&#25311;&#25968;&#25454;&#38598;&#21487;&#20197;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21019;&#24314;&#34394;&#25311;&#25968;&#25454;&#38598;&#26082;&#38656;&#35201;&#26102;&#38388;&#21448;&#38656;&#35201;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Light Eyes or "LEyes"&#30340;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#30340;&#36924;&#30495;&#26041;&#27861;&#19981;&#21516;&#65292;LEyes&#20165;&#27169;&#25311;&#35270;&#39057;&#30524;&#21160;&#36319;&#36394;&#25152;&#38656;&#30340;&#20851;&#38190;&#22270;&#20687;&#29305;&#24449;&#12290;LEyes&#20415;&#20110;&#22312;&#22810;&#26679;&#21270;&#30340;&#20957;&#35270;&#20272;&#35745;&#20219;&#21153;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30524;&#30555;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has bolstered gaze estimation techniques, but real-world deployment has been impeded by inadequate training datasets. This problem is exacerbated by both hardware-induced variations in eye images and inherent biological differences across the recorded participants, leading to both feature and pixel-level variance that hinders the generalizability of models trained on specific datasets. While synthetic datasets can be a solution, their creation is both time and resource-intensive. To address this problem, we present a framework called Light Eyes or "LEyes" which, unlike conventional photorealistic methods, only models key image features required for video-based eye tracking using simple light distributions. LEyes facilitates easy configuration for training neural networks across diverse gaze-estimation tasks. We demonstrate that models trained using LEyes outperform other state-of-the-art algorithms in terms of pupil and CR localization across well-known datasets. In addit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20445;&#30495;&#24230;&#35825;&#23548;&#30340;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#65288;FIPE&#65289;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#19981;&#36879;&#26126;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26143;&#38469;&#20105;&#38712; II &#36825;&#26679;&#30340;&#22797;&#26434;&#25511;&#21046;&#29615;&#22659;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06097</link><description>&lt;p&gt;
&#22522;&#20110;&#20445;&#30495;&#24230;&#35825;&#23548;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning. (arXiv:2309.06097v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20445;&#30495;&#24230;&#35825;&#23548;&#30340;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#65288;FIPE&#65289;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#19981;&#36879;&#26126;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26143;&#38469;&#20105;&#38712; II &#36825;&#26679;&#30340;&#22797;&#26434;&#25511;&#21046;&#29615;&#22659;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20197;&#19981;&#36879;&#26126;&#30340;&#26041;&#24335;&#36827;&#34892;&#20915;&#31574;&#65292;&#38459;&#30861;&#20102;&#29992;&#25143;&#24314;&#31435;&#20449;&#20219;&#21644;&#23457;&#35270;&#20195;&#29702;&#30340;&#24369;&#28857;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20123;&#21487;&#35299;&#37322;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#26469;&#35299;&#37322;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20294;&#23427;&#20204;&#30340;&#35299;&#37322;&#24120;&#24120;&#19982;&#20195;&#29702;&#30340;&#34892;&#20026;&#19981;&#19968;&#33268;&#65292;&#22240;&#27492;&#32463;&#24120;&#26080;&#27861;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#20445;&#30495;&#24230;&#35825;&#23548;&#30340;&#31574;&#30053;&#25552;&#21462;&#65288;FIPE&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#29616;&#26377;&#21487;&#35299;&#37322;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#30340;&#20248;&#21270;&#26426;&#21046;&#65292;&#38416;&#36848;&#20102;&#22312;&#22686;&#21152;&#32047;&#31215;&#22870;&#21169;&#26102;&#24573;&#35270;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#20445;&#30495;&#24230;&#37327;&#38598;&#25104;&#21040;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20445;&#30495;&#24230;&#35825;&#23548;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;&#26143;&#38469;&#20105;&#38712; II &#30340;&#22797;&#26434;&#25511;&#21046;&#29615;&#22659;&#19979;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#26159;&#24403;&#21069;&#21487;&#35299;&#37322;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#36890;&#24120;&#36991;&#20813;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making problems. However, existing DRL agents make decisions in an opaque fashion, hindering the user from establishing trust and scrutinizing weaknesses of the agents. While recent research has developed Interpretable Policy Extraction (IPE) methods for explaining how an agent takes actions, their explanations are often inconsistent with the agent's behavior and thus, frequently fail to explain. To tackle this issue, we propose a novel method, Fidelity-Induced Policy Extraction (FIPE). Specifically, we start by analyzing the optimization mechanism of existing IPE methods, elaborating on the issue of ignoring consistency while increasing cumulative rewards. We then design a fidelity-induced mechanism by integrate a fidelity measurement into the reinforcement learning feedback. We conduct experiments in the complex control environment of StarCraft II, an arena typically avoided by current IPE method
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#26512;&#29616;&#20195;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#21147;&#24066;&#22330;&#20013;&#20215;&#26684;&#39129;&#21319;&#20107;&#20214;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.06082</link><description>&lt;p&gt;
&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35299;&#26512;&#30005;&#21147;&#24066;&#22330;&#20215;&#26684;&#20107;&#20214;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Framework to Deconstruct the Primary Drivers for Electricity Market Price Events. (arXiv:2309.06082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#26512;&#29616;&#20195;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#21147;&#24066;&#22330;&#20013;&#20215;&#26684;&#39129;&#21319;&#20107;&#20214;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#32593;&#32476;&#27491;&#22312;&#21521;100%&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#22823;&#23481;&#37327;&#30005;&#21147;&#32593;&#32476;&#36716;&#21464;&#65292;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#21644;&#30005;&#21147;&#24066;&#22330;&#30340;&#25972;&#20307;&#21160;&#24577;&#20063;&#22312;&#21457;&#29983;&#21464;&#21270;&#12290;&#30005;&#21147;&#24066;&#22330;&#19981;&#20165;&#22312;&#32463;&#27982;&#19978;&#35843;&#24230;&#36164;&#28304;&#65292;&#36824;&#32771;&#34385;&#20102;&#21508;&#31181;&#21487;&#25511;&#34892;&#21160;&#65292;&#22914;&#21487;&#20877;&#29983;&#33021;&#28304;&#38480;&#21046;&#12289;&#36755;&#30005;&#38459;&#22622;&#32531;&#35299;&#21644;&#33021;&#37327;&#20648;&#23384;&#20248;&#21270;&#31561;&#65292;&#20197;&#30830;&#20445;&#30005;&#32593;&#21487;&#38752;&#24615;&#12290;&#22240;&#27492;&#65292;&#30005;&#21147;&#24066;&#22330;&#30340;&#20215;&#26684;&#24418;&#25104;&#21464;&#24471;&#38750;&#24120;&#22797;&#26434;&#12290;&#20256;&#32479;&#30340;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#21644;&#32479;&#35745;&#26041;&#27861;&#26080;&#27861;&#20998;&#26512;&#21644;&#25512;&#26029;&#29616;&#20195;&#30005;&#32593;&#21644;&#20855;&#26377;&#21487;&#21464;&#21487;&#20877;&#29983;&#33021;&#28304;&#65288;VRE&#65289;&#30340;&#24066;&#22330;&#20215;&#26684;&#24418;&#25104;&#32972;&#21518;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#26512;&#29616;&#20195;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#21147;&#24066;&#22330;&#20013;&#20215;&#26684;&#39129;&#21319;&#20107;&#20214;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#24212;&#29992;&#20110;&#24066;&#22330;&#35774;&#35745;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#35843;&#24230;&#31561;&#21508;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Power grids are moving towards 100% renewable energy source bulk power grids, and the overall dynamics of power system operations and electricity markets are changing. The electricity markets are not only dispatching resources economically but also taking into account various controllable actions like renewable curtailment, transmission congestion mitigation, and energy storage optimization to ensure grid reliability. As a result, price formations in electricity markets have become quite complex. Traditional root cause analysis and statistical approaches are rendered inapplicable to analyze and infer the main drivers behind price formation in the modern grid and markets with variable renewable energy (VRE). In this paper, we propose a machine learning-based analysis framework to deconstruct the primary drivers for price spike events in modern electricity markets with high renewable energy. The outcomes can be utilized for various critical aspects of market design, renewable dispatch an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#22120;&#21463;&#26631;&#31614;&#22122;&#22768;&#24433;&#21709;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#21463;&#21040;&#26631;&#31614;&#22122;&#22768;&#24433;&#21709;&#30340;&#20803;&#35757;&#32451;&#20013;&#65292;Reptile&#12289;iMAML&#21644;foMAML&#22312;Omniglot&#21644;CifarFS&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#20102;&#26368;&#39640;&#36798;42%&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Man&#21644;BatMan&#20004;&#31181;&#37319;&#26679;&#25216;&#26415;&#65292;&#23558;&#26377;&#22122;&#22768;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#36716;&#21464;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.06046</link><description>&lt;p&gt;
BatMan-CLR: &#20351;&#24471;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#22120;&#23545;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise. (arXiv:2309.06046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#22120;&#21463;&#26631;&#31614;&#22122;&#22768;&#24433;&#21709;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#21463;&#21040;&#26631;&#31614;&#22122;&#22768;&#24433;&#21709;&#30340;&#20803;&#35757;&#32451;&#20013;&#65292;Reptile&#12289;iMAML&#21644;foMAML&#22312;Omniglot&#21644;CifarFS&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#20102;&#26368;&#39640;&#36798;42%&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Man&#21644;BatMan&#20004;&#31181;&#37319;&#26679;&#25216;&#26415;&#65292;&#23558;&#26377;&#22122;&#22768;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#36716;&#21464;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#23545;&#20110;&#32463;&#20856;&#30340;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#26377;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#26159;&#22312;&#20803;&#23398;&#20064;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20803;&#23398;&#20064;&#22120;&#26088;&#22312;&#36890;&#36807;&#22312;&#20803;&#35757;&#32451;&#20013;&#23398;&#20064;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#22987;&#27169;&#22411;&#65292;&#24182;&#22312;&#20803;&#27979;&#35797;&#26399;&#38388;&#26681;&#25454;&#26032;&#20219;&#21153;&#36827;&#34892;&#36830;&#32493;&#24494;&#35843;&#65292;&#20197;&#36866;&#24212;&#26410;&#30693;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#26368;&#20808;&#36827;&#30340;&#20803;&#23398;&#20064;&#22120;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;&#26799;&#24230;&#30340;N-way K-shot&#23398;&#20064;&#22120;&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20803;&#35757;&#32451;&#21463;&#21040;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#26102;&#65292;Reptile&#12289;iMAML&#21644;foMAML&#22312;Omniglot&#21644;CifarFS&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#20102;&#26368;&#39640;&#36798;42%&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#37319;&#26679;&#25216;&#26415;&#65292;&#21363;&#27969;&#24418;&#65288;Man&#65289;&#21644;&#25209;&#27425;&#27969;&#24418;&#65288;BatMan&#65289;&#65292;&#23558;&#26377;&#22122;&#22768;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#36716;&#21464;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#65292;&#20197;&#22686;&#21152;&#22122;&#22768;&#26631;&#31614;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;N-way 2-contrastiv&#30340;&#27969;&#24418;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
The negative impact of label noise is well studied in classical supervised learning yet remains an open research question in meta-learning. Meta-learners aim to adapt to unseen learning tasks by learning a good initial model in meta-training and consecutively fine-tuning it according to new tasks during meta-testing. In this paper, we present the first extensive analysis of the impact of varying levels of label noise on the performance of state-of-the-art meta-learners, specifically gradient-based $N$-way $K$-shot learners. We show that the accuracy of Reptile, iMAML, and foMAML drops by up to 42% on the Omniglot and CifarFS datasets when meta-training is affected by label noise. To strengthen the resilience against label noise, we propose two sampling techniques, namely manifold (Man) and batch manifold (BatMan), which transform the noisy supervised learners into semi-supervised ones to increase the utility of noisy labels. We first construct manifold samples of $N$-way $2$-contrastiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#20840;&#23616;&#25628;&#32034;&#30340;&#31639;&#27861;&#65288;UMCTS&#65289;&#29992;&#20110;&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#26356;&#26032;&#36807;&#31243;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#20197;&#21450;&#20351;&#29992;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#26469;&#33719;&#24471;&#21512;&#36866;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.06045</link><description>&lt;p&gt;
&#22522;&#20110;&#21551;&#21457;&#24335;&#20840;&#23616;&#25628;&#32034;&#30340;&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#38382;&#39064;&#30340;&#25913;&#36827;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;UMCTS&#65289;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Update Monte Carlo tree search (UMCTS) algorithm for heuristic global search of sizing optimization problems for truss structures. (arXiv:2309.06045v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#20840;&#23616;&#25628;&#32034;&#30340;&#31639;&#27861;&#65288;UMCTS&#65289;&#29992;&#20110;&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#26356;&#26032;&#36807;&#31243;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#20197;&#21450;&#20351;&#29992;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#26469;&#33719;&#24471;&#21512;&#36866;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26689;&#26550;&#32467;&#26500;&#23610;&#23544;&#20248;&#21270;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36866;&#29992;&#20110;&#22788;&#29702;&#26080;&#26799;&#24230;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#8212;&#8212;&#26356;&#26032;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;UMCTS&#65289;&#65292;&#29992;&#20110;&#33719;&#24471;&#21512;&#36866;&#30340;&#26689;&#26550;&#32467;&#26500;&#35774;&#35745;&#12290;UMCTS&#26159;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#23558;&#26032;&#39062;&#30340;&#26356;&#26032;&#36807;&#31243;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#19982;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#30456;&#32467;&#21512;&#12290;&#26356;&#26032;&#36807;&#31243;&#24847;&#21619;&#30528;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#27599;&#20010;&#26500;&#20214;&#30340;&#26368;&#20339;&#25130;&#38754;&#31215;&#36890;&#36807;&#25628;&#32034;&#26641;&#30830;&#23450;&#65292;&#20854;&#21021;&#22987;&#29366;&#24577;&#26159;&#19978;&#19968;&#36718;&#30340;&#26368;&#32456;&#29366;&#24577;&#12290;&#22312;UMCTS&#31639;&#27861;&#20013;&#65292;&#24341;&#20837;&#20102;&#21152;&#36895;&#36873;&#25321;&#25104;&#21592;&#38754;&#31215;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#21152;&#36895;&#22120;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#20010;&#29366;&#24577;&#65292;&#24179;&#22343;&#22870;&#21169;&#34987;&#26368;&#20339;&#22870;&#21169;&#30340;&#27169;&#25311;&#36807;&#31243;&#20013;&#25910;&#38598;&#26469;&#30340;&#22870;&#21169;&#26367;&#20195;&#65292;&#30830;&#23450;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26356;&#26032;&#36807;&#31243;&#21644;MCTS&#65292;&#20197;&#21450;&#20351;&#29992;UCB&#26469;&#20248;&#21270;&#26689;&#26550;&#32467;&#26500;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sizing optimization of truss structures is a complex computational problem, and the reinforcement learning (RL) is suitable for dealing with multimodal problems without gradient computations. In this paper, a new efficient optimization algorithm called update Monte Carlo tree search (UMCTS) is developed to obtain the appropriate design for truss structures. UMCTS is an RL-based method that combines the novel update process and Monte Carlo tree search (MCTS) with the upper confidence bound (UCB). Update process means that in each round, the optimal cross-sectional area of each member is determined by search tree, and its initial state is the final state in the previous round. In the UMCTS algorithm, an accelerator for the number of selections for member area and iteration number is introduced to reduce the computation time. Moreover, for each state, the average reward is replaced by the best reward collected on the simulation process to determine the optimal solution. The proposed optim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;Grasping Gradient Field&#21644;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#65292;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#30340;&#28789;&#24039;&#25235;&#21462;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.06038</link><description>&lt;p&gt;
&#20026;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#23398;&#20064;&#22522;&#20110;&#24471;&#20998;&#30340;&#25235;&#21462;&#21407;&#35821;
&lt;/p&gt;
&lt;p&gt;
Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping. (arXiv:2309.06038v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;Grasping Gradient Field&#21644;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#65292;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#30340;&#28789;&#24039;&#25235;&#21462;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#26088;&#22312;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#24110;&#21161;&#29992;&#25143;&#25235;&#21462;&#29289;&#20307;&#30340;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#28789;&#24039;&#25235;&#21462;&#19981;&#21516;&#65292;&#36825;&#20010;&#20219;&#21153;&#38754;&#20020;&#30528;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#31574;&#30053;&#38656;&#35201;&#36866;&#24212;&#19981;&#21516;&#30340;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#30001;&#20004;&#20010;&#23376;&#27169;&#22359;&#32452;&#25104;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#19968;&#31181;&#25163;-&#29289;&#20307;&#26465;&#20214;&#25235;&#21462;&#21407;&#35821;&#31216;&#20026;Grasping Gradient Field&#65288;GraspGF&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#12290;GraspGF&#36890;&#36807;&#20272;&#35745;&#26469;&#33258;&#25104;&#21151;&#25235;&#21462;&#31034;&#20363;&#38598;&#30340;&#26799;&#24230;&#26469;&#23398;&#20064;&#8220;&#22914;&#20309;&#8221;&#25235;&#21462;&#65292;&#32780;&#27531;&#24046;&#31574;&#30053;&#26681;&#25454;&#36712;&#36857;&#21382;&#21490;&#30830;&#23450;&#8220;&#20309;&#26102;&#8221;&#21644;&#20197;&#20309;&#31181;&#36895;&#24230;&#25191;&#34892;&#25235;&#21462;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of anthropomorphic robotic hands for assisting individuals in situations where human hands may be unavailable or unsuitable has gained significant importance. In this paper, we propose a novel task called human-assisting dexterous grasping that aims to train a policy for controlling a robotic hand's fingers to assist users in grasping objects. Unlike conventional dexterous grasping, this task presents a more complex challenge as the policy needs to adapt to diverse user intentions, in addition to the object's geometry. We address this challenge by proposing an approach consisting of two sub-modules: a hand-object-conditional grasping primitive called Grasping Gradient Field~(GraspGF), and a history-conditional residual policy. GraspGF learns `how' to grasp by estimating the gradient from a success grasping example set, while the residual policy determines `when' and at what speed the grasping action should be executed based on the trajectory history. Experimental results demons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#33258;&#21160;&#20272;&#31639;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;&#30340;&#36824;&#27454;&#24037;&#20316;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#36824;&#27454;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.06020</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#20607;&#36824;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;
&lt;/p&gt;
&lt;p&gt;
Automatically Estimating the Effort Required to Repay Self-Admitted Technical Debt. (arXiv:2309.06020v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#33258;&#21160;&#20272;&#31639;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;&#30340;&#36824;&#27454;&#24037;&#20316;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#36824;&#27454;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#20538;&#21153;&#26159;&#25351;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#20026;&#20102;&#30701;&#26399;&#21033;&#30410;&#32780;&#20570;&#20986;&#30340;&#27425;&#20248;&#20915;&#31574;&#25152;&#24102;&#26469;&#30340;&#21518;&#26524;&#12290;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;(SATD)&#26159;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#25216;&#26415;&#20538;&#21153;&#65292;&#24320;&#21457;&#20154;&#21592;&#26126;&#30830;&#22320;&#22312;&#36719;&#20214;&#30340;&#28304;&#20195;&#30721;&#27880;&#37322;&#21644;&#25552;&#20132;&#28040;&#24687;&#20013;&#35760;&#24405;&#19979;&#26469;&#12290;&#30001;&#20110;SATD&#21487;&#33021;&#38459;&#30861;&#36719;&#20214;&#30340;&#24320;&#21457;&#21644;&#32500;&#25252;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#35299;&#20915;&#21644;&#20248;&#20808;&#22788;&#29702;&#23427;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#26681;&#25454;SATD&#30340;&#25991;&#26412;&#25551;&#36848;&#33258;&#21160;&#35780;&#20272;&#20854;&#36824;&#27454;&#24037;&#20316;&#37327;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#20010;&#21253;&#25324;1,060&#20010;Apache&#20195;&#30721;&#24211;&#20013;&#20849;2,568,728&#20010;&#25552;&#20132;&#30340;341,740&#20010;SATD&#39033;&#30446;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#26469;&#33258;&#21160;&#20272;&#31639;SATD&#36824;&#27454;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;SATD&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#36824;&#27454;&#24037;&#20316;&#37327;&#65292;&#20854;&#20013;&#20195;&#30721;/&#35774;&#35745;&#12289;&#38656;&#27714;&#21644;&#27979;&#35797;&#20538;&#21153;&#38656;&#35201;&#26356;&#22810;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Technical debt refers to the consequences of sub-optimal decisions made during software development that prioritize short-term benefits over long-term maintainability. Self-Admitted Technical Debt (SATD) is a specific form of technical debt, explicitly documented by developers within software artifacts such as source code comments and commit messages. As SATD can hinder software development and maintenance, it is crucial to address and prioritize it effectively. However, current methodologies lack the ability to automatically estimate the repayment effort of SATD based on its textual descriptions. To address this limitation, we propose a novel approach for automatically estimating SATD repayment effort, utilizing a comprehensive dataset comprising 341,740 SATD items from 2,568,728 commits across 1,060 Apache repositories. Our findings show that different types of SATD require varying levels of repayment effort, with code/design, requirement, and test debt demanding greater effort compa
&lt;/p&gt;</description></item><item><title>DSLOT-NN&#26159;&#19968;&#31181;&#25968;&#23383;&#20018;&#34892;&#20174;&#24038;&#21040;&#21491;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#36816;&#31639;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#21644;&#32456;&#27490;&#26080;&#25928;&#30340;&#21367;&#31215;&#25805;&#20316;&#23454;&#29616;&#21151;&#32791;&#21644;&#33021;&#37327;&#30340;&#22823;&#35268;&#27169;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2309.06019</link><description>&lt;p&gt;
DSLOT-NN: &#25968;&#23383;&#20018;&#34892;&#20174;&#24038;&#21040;&#21491;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
DSLOT-NN: Digit-Serial Left-to-Right Neural Network Accelerator. (arXiv:2309.06019v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06019
&lt;/p&gt;
&lt;p&gt;
DSLOT-NN&#26159;&#19968;&#31181;&#25968;&#23383;&#20018;&#34892;&#20174;&#24038;&#21040;&#21491;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#36816;&#31639;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#21644;&#32456;&#27490;&#26080;&#25928;&#30340;&#21367;&#31215;&#25805;&#20316;&#23454;&#29616;&#21151;&#32791;&#21644;&#33021;&#37327;&#30340;&#22823;&#35268;&#27169;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSLOT-NN&#30340;&#22522;&#20110;&#25968;&#23383;&#20018;&#34892;&#20174;&#24038;&#21040;&#21491;&#65288;DSLOT&#65289;&#31639;&#26415;&#22788;&#29702;&#25216;&#26415;&#65292;&#26088;&#22312;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#30340;&#21367;&#31215;&#36816;&#31639;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#35780;&#20272;&#21644;&#32456;&#27490;&#26080;&#25928;&#30340;&#21367;&#31215;&#25805;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#35268;&#27169;&#30340;&#21151;&#32791;&#21644;&#33021;&#37327;&#33410;&#30465;&#12290;&#22788;&#29702;&#24341;&#25806;&#30001;&#20302;&#24310;&#36831;&#30340;&#26368;&#39640;&#26377;&#25928;&#25968;&#23383;&#20248;&#20808;&#65288;MSDF&#65289;&#65288;&#20063;&#31216;&#20026;&#22312;&#32447;&#65289;&#20056;&#27861;&#22120;&#21644;&#21152;&#27861;&#22120;&#32452;&#25104;&#65292;&#20174;&#24038;&#21040;&#21491;&#22788;&#29702;&#25968;&#25454;&#65292;&#20801;&#35768;&#21518;&#32493;&#25805;&#20316;&#20197;&#25968;&#23383;&#27969;&#27700;&#32447;&#26041;&#24335;&#25191;&#34892;&#12290;&#20351;&#29992;&#22312;&#32447;&#36816;&#31639;&#22120;&#28040;&#38500;&#20102;&#22797;&#26434;&#30340;&#36127;&#28608;&#27963;&#35782;&#21035;&#26426;&#21046;&#30340;&#24320;&#21457;&#38656;&#27714;&#65292;&#22240;&#20026;&#39318;&#20808;&#29983;&#25104;&#20855;&#26377;&#26368;&#39640;&#26435;&#37325;&#20540;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#19968;&#26086;&#29983;&#25104;&#31532;&#19968;&#20010;&#38750;&#38646;&#25968;&#23383;&#65292;&#21363;&#21487;&#35782;&#21035;&#32467;&#26524;&#30340;&#31526;&#21495;&#12290;&#22312;&#32447;&#36816;&#31639;&#22120;&#30340;&#31934;&#24230;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#36827;&#34892;&#35843;&#25972;&#65292;&#20351;&#20854;&#22312;&#21487;&#20197;&#29306;&#29298;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#26497;&#20854;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Digit-Serial Left-tO-righT (DSLOT) arithmetic based processing technique called DSLOT-NN with aim to accelerate inference of the convolution operation in the deep neural networks (DNNs). The proposed work has the ability to assess and terminate the ineffective convolutions which results in massive power and energy savings. The processing engine is comprised of low-latency most-significant-digit-first (MSDF) (also called online) multipliers and adders that processes data from left-to-right, allowing the execution of subsequent operations in digit-pipelined manner. Use of online operators eliminates the need for the development of complex mechanism of identifying the negative activation, as the output with highest weight value is generated first, and the sign of the result can be identified as soon as first non-zero digit is generated. The precision of the online operators can be tuned at run-time, making them extremely useful in situations where accuracy can be compromised 
&lt;/p&gt;</description></item><item><title>SoccerNet 2023&#25361;&#25112;&#26159;&#19968;&#27425;&#35270;&#39057;&#29702;&#35299;&#30340;&#27604;&#36187;&#65292;&#21253;&#21547;&#24191;&#25773;&#35270;&#39057;&#29702;&#35299;&#12289;&#22330;&#22320;&#29702;&#35299;&#21644;&#29699;&#21592;&#29702;&#35299;&#19977;&#20010;&#20027;&#39064;&#19979;&#30340;&#22810;&#20010;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.06006</link><description>&lt;p&gt;
SoccerNet 2023&#25361;&#25112;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
SoccerNet 2023 Challenges Results. (arXiv:2309.06006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06006
&lt;/p&gt;
&lt;p&gt;
SoccerNet 2023&#25361;&#25112;&#26159;&#19968;&#27425;&#35270;&#39057;&#29702;&#35299;&#30340;&#27604;&#36187;&#65292;&#21253;&#21547;&#24191;&#25773;&#35270;&#39057;&#29702;&#35299;&#12289;&#22330;&#22320;&#29702;&#35299;&#21644;&#29699;&#21592;&#29702;&#35299;&#19977;&#20010;&#20027;&#39064;&#19979;&#30340;&#22810;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SoccerNet 2023&#25361;&#25112;&#26159;SoccerNet&#22242;&#38431;&#32452;&#32455;&#30340;&#31532;&#19977;&#23626;&#24180;&#24230;&#35270;&#39057;&#29702;&#35299;&#25361;&#25112;&#12290;&#22312;&#31532;&#19977;&#23626;&#20013;&#65292;&#25361;&#25112;&#20998;&#20026;&#19971;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#20219;&#21153;&#65292;&#20998;&#20026;&#19977;&#20010;&#20027;&#39064;&#12290;&#31532;&#19968;&#20010;&#20027;&#39064;&#26159;&#24191;&#25773;&#35270;&#39057;&#29702;&#35299;&#65292;&#21253;&#25324;&#19977;&#20010;&#19982;&#35270;&#39057;&#24191;&#25773;&#20013;&#21457;&#29983;&#20107;&#20214;&#25551;&#36848;&#30456;&#20851;&#30340;&#39640;&#32423;&#20219;&#21153;&#65306;(1) &#21160;&#20316;&#23450;&#20301;&#65292;&#37325;&#28857;&#26159;&#26816;&#32034;&#19982;&#36275;&#29699;&#20840;&#23616;&#21160;&#20316;&#30456;&#20851;&#30340;&#20840;&#37096;&#26102;&#38388;&#25139;&#65292;(2) &#36275;&#29699;&#21160;&#20316;&#23450;&#20301;&#65292;&#37325;&#28857;&#26159;&#26816;&#32034;&#19982;&#36275;&#29699;&#29366;&#24577;&#25913;&#21464;&#30456;&#20851;&#30340;&#20840;&#37096;&#26102;&#38388;&#25139;&#65292;&#20197;&#21450;(3) &#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#22266;&#23450;&#26102;&#38388;&#25139;&#25551;&#36848;&#24191;&#25773;&#20869;&#23481;&#12290;&#31532;&#20108;&#20010;&#20027;&#39064;&#26159;&#22330;&#22320;&#29702;&#35299;&#65292;&#19982;(4) &#30456;&#26426;&#26657;&#20934;&#30340;&#21333;&#19968;&#20219;&#21153;&#30456;&#20851;&#65292;&#37325;&#28857;&#26159;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#20869;&#37096;&#21644;&#22806;&#37096;&#30456;&#26426;&#21442;&#25968;&#12290;&#31532;&#19977;&#20010;&#20063;&#26159;&#26368;&#21518;&#19968;&#20010;&#20027;&#39064;&#65292;&#26159;&#29699;&#21592;&#29702;&#35299;&#65292;&#21253;&#25324;&#19977;&#20010;&#19982;&#25552;&#21462;&#20851;&#20110;&#29699;&#21592;&#20449;&#24687;&#30456;&#20851;&#30340;&#20302;&#32423;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The SoccerNet 2023 challenges were the third annual video understanding challenges organized by the SoccerNet team. For this third edition, the challenges were composed of seven vision-based tasks split into three main themes. The first theme, broadcast video understanding, is composed of three high-level tasks related to describing events occurring in the video broadcasts: (1) action spotting, focusing on retrieving all timestamps related to global actions in soccer, (2) ball action spotting, focusing on retrieving all timestamps related to the soccer ball change of state, and (3) dense video captioning, focusing on describing the broadcast with natural language and anchored timestamps. The second theme, field understanding, relates to the single task of (4) camera calibration, focusing on retrieving the intrinsic and extrinsic camera parameters from images. The third and last theme, player understanding, is composed of three low-level tasks related to extracting information about the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#21629;&#29702;&#35770;&#21644;&#25511;&#21046;&#35770;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#23558;&#20869;&#24863;&#30693;&#24212;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.05999</link><description>&lt;p&gt;
&#29983;&#21629;&#21551;&#21457;&#30340;&#33258;&#20027;&#21644;&#36866;&#24212;&#26234;&#33021;&#20026;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#20195;&#29702;&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#33021;&#21147;&#21644;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#20195;&#29702;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#32456;&#26497;&#30446;&#26631;&#12290;&#29983;&#29289;&#20307;&#26159;&#36825;&#26679;&#19968;&#20010;&#20195;&#29702;&#30340;&#26368;&#22909;&#20363;&#35777;&#65292;&#23427;&#20026;&#33258;&#36866;&#24212;&#33258;&#20027;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20851;&#27880;&#20869;&#24863;&#30693;&#65292;&#36825;&#26159;&#19968;&#20010;&#30417;&#25511;&#33258;&#36523;&#20869;&#37096;&#29615;&#22659;&#26469;&#20445;&#25345;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#36807;&#31243;&#65292;&#23427;&#20026;&#29983;&#29289;&#20307;&#30340;&#29983;&#23384;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20026;&#20102;&#24320;&#21457;&#20855;&#26377;&#20869;&#24863;&#30693;&#30340;AI&#65292;&#25105;&#20204;&#38656;&#35201;&#23558;&#34920;&#31034;&#20869;&#37096;&#29615;&#22659;&#30340;&#29366;&#24577;&#21464;&#37327;&#19982;&#22806;&#37096;&#29615;&#22659;&#30456;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#29983;&#21629;&#21551;&#21457;&#30340;&#20869;&#37096;&#29615;&#22659;&#29366;&#24577;&#30340;&#25968;&#23398;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#21363;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#20869;&#24863;&#30693;&#22914;&#20309;&#24110;&#21161;&#26500;&#24314;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Life-inspired Interoceptive Artificial Intelligence for Autonomous and Adaptive Agents. (arXiv:2309.05999v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#21629;&#29702;&#35770;&#21644;&#25511;&#21046;&#35770;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#23558;&#20869;&#24863;&#30693;&#24212;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#33021;&#21147;&#21644;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#20195;&#29702;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#32456;&#26497;&#30446;&#26631;&#12290;&#29983;&#29289;&#20307;&#26159;&#36825;&#26679;&#19968;&#20010;&#20195;&#29702;&#30340;&#26368;&#22909;&#20363;&#35777;&#65292;&#23427;&#20026;&#33258;&#36866;&#24212;&#33258;&#20027;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;&#26412;&#25991;&#20851;&#27880;&#20869;&#24863;&#30693;&#65292;&#36825;&#26159;&#19968;&#20010;&#30417;&#25511;&#33258;&#36523;&#20869;&#37096;&#29615;&#22659;&#26469;&#20445;&#25345;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#36807;&#31243;&#65292;&#23427;&#20026;&#29983;&#29289;&#20307;&#30340;&#29983;&#23384;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20026;&#20102;&#24320;&#21457;&#20855;&#26377;&#20869;&#24863;&#30693;&#30340;AI&#65292;&#25105;&#20204;&#38656;&#35201;&#23558;&#34920;&#31034;&#20869;&#37096;&#29615;&#22659;&#30340;&#29366;&#24577;&#21464;&#37327;&#19982;&#22806;&#37096;&#29615;&#22659;&#30456;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#29983;&#21629;&#21551;&#21457;&#30340;&#20869;&#37096;&#29615;&#22659;&#29366;&#24577;&#30340;&#25968;&#23398;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#21363;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#20869;&#24863;&#30693;&#22914;&#20309;&#24110;&#21161;&#26500;&#24314;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building autonomous --- i.e., choosing goals based on one's needs -- and adaptive -- i.e., surviving in ever-changing environments -- agents has been a holy grail of artificial intelligence (AI). A living organism is a prime example of such an agent, offering important lessons about adaptive autonomy. Here, we focus on interoception, a process of monitoring one's internal environment to keep it within certain bounds, which underwrites the survival of an organism. To develop AI with interoception, we need to factorize the state variables representing internal environments from external environments and adopt life-inspired mathematical properties of internal environment states. This paper offers a new perspective on how interoception can help build autonomous and adaptive agents by integrating the legacy of cybernetics with recent advances in theories of life, reinforcement learning, and neuroscience.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#30701;&#19978;&#19979;&#25991;&#34892;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#20154;&#31867;&#20013;&#24515;&#30340;&#35270;&#39057;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#30701;&#35270;&#39057;&#29255;&#27573;&#26469;&#39044;&#27979;&#38271;&#26399;&#30340;&#20154;&#31867;&#34892;&#21160;&#65292;&#25552;&#39640;&#20102;&#32534;&#36753;&#24037;&#20316;&#27969;&#31243;&#30340;&#36895;&#24230;&#21644;&#21019;&#20316;&#24615;&#65292;&#36890;&#36807;&#22686;&#24378;&#21464;&#25442;&#22120;&#32593;&#32476;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#34892;&#21160;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;9%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05943</link><description>&lt;p&gt;
&#22312;&#20154;&#31867;&#20013;&#24515;&#30340;&#35270;&#39057;&#20013;&#65292;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#30701;&#19978;&#19979;&#25991;&#34892;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Guided Short-Context Action Anticipation in Human-Centric Videos. (arXiv:2309.05943v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05943
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#30701;&#19978;&#19979;&#25991;&#34892;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#20154;&#31867;&#20013;&#24515;&#30340;&#35270;&#39057;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#30701;&#35270;&#39057;&#29255;&#27573;&#26469;&#39044;&#27979;&#38271;&#26399;&#30340;&#20154;&#31867;&#34892;&#21160;&#65292;&#25552;&#39640;&#20102;&#32534;&#36753;&#24037;&#20316;&#27969;&#31243;&#30340;&#36895;&#24230;&#21644;&#21019;&#20316;&#24615;&#65292;&#36890;&#36807;&#22686;&#24378;&#21464;&#25442;&#22120;&#32593;&#32476;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#34892;&#21160;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;9%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#30701;&#35270;&#39057;&#29255;&#27573;&#26469;&#39044;&#27979;&#38271;&#26399;&#30340;&#20154;&#31867;&#34892;&#21160;&#65292;&#20174;&#32780;&#36890;&#36807;&#25552;&#20379;&#26356;&#22909;&#30340;&#24314;&#35758;&#26469;&#21152;&#24555;&#32534;&#36753;&#24037;&#20316;&#27969;&#31243;&#65292;&#21516;&#26102;&#36890;&#36807;&#25552;&#20379;&#21465;&#20107;&#26469;&#22521;&#20859;&#21019;&#36896;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36816;&#29992;&#31526;&#21495;&#21270;&#30693;&#35782;&#22270;&#35889;&#26469;&#22686;&#24378;&#21464;&#25442;&#22120;&#32593;&#32476;&#22312;&#35270;&#39057;&#29255;&#27573;&#20013;&#30340;&#34892;&#21160;&#39044;&#27979;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#25552;&#21319;&#21464;&#25442;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;Breakfast&#21644;50Salads&#19978;&#30340;&#28436;&#31034;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#30701;&#35270;&#39057;&#19978;&#19979;&#25991;&#36827;&#34892;&#38271;&#26399;&#34892;&#21160;&#39044;&#27979;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on anticipating long-term human actions, particularly using short video segments, which can speed up editing workflows through improved suggestions while fostering creativity by suggesting narratives. To this end, we imbue a transformer network with a symbolic knowledge graph for action anticipation in video segments by boosting certain aspects of the transformer's attention mechanism at run-time. Demonstrated on two benchmark datasets, Breakfast and 50Salads, our approach outperforms current state-of-the-art methods for long-term action anticipation using short video context by up to 9%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#19981;&#21516;&#65292;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20174;&#22810;&#20010;&#35282;&#24230;&#24635;&#32467;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#21644;&#23458;&#35266;&#30693;&#35782;&#26469;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#12289;&#30456;&#20851;&#24615;&#25429;&#25417;&#21644;&#25688;&#35201;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.05938</link><description>&lt;p&gt;
&#36890;&#36807;&#24635;&#32467;&#22810;&#28304;&#22810;&#35270;&#35282;&#30693;&#35782;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge. (arXiv:2309.05938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#19981;&#21516;&#65292;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20174;&#22810;&#20010;&#35282;&#24230;&#24635;&#32467;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#21644;&#23458;&#35266;&#30693;&#35782;&#26469;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#12289;&#30456;&#20851;&#24615;&#25429;&#25417;&#21644;&#25688;&#35201;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#30340;&#39046;&#22495;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#12290;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#20294;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#26469;&#35299;&#37322;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#8220;&#25163;&#26426;&#26159;&#21542;&#37325;&#8221;&#30340;&#31572;&#26696;&#26377;&#22810;&#31181;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#19968;&#20010;&#28385;&#24847;&#30340;&#31572;&#26696;&#24212;&#35813;&#33021;&#22815;&#24635;&#32467;&#36825;&#20123;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#65292;&#24182;&#25552;&#20379;&#23458;&#35266;&#30693;&#35782;&#65292;&#27604;&#22914;&#25163;&#26426;&#30340;&#37325;&#37327;&#12290;&#36825;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#65292;&#20256;&#32479;QA&#20219;&#21153;&#20013;&#23545;&#20110;&#20107;&#23454;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#21333;&#20010;&#25968;&#25454;&#28304;&#20013;&#25214;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#30693;&#35782;&#28304;&#20013;&#26816;&#32034;&#25152;&#26377;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#32447;&#32034;&#65292;&#21253;&#25324;&#20107;&#23454;&#21644;&#35266;&#28857;&#12290;&#36824;&#25910;&#38598;&#20102;&#38544;&#21547;&#30340;&#24120;&#35782;&#20107;&#23454;&#26469;&#34917;&#20805;&#24517;&#35201;&#20294;&#32570;&#22833;&#30340;&#32972;&#26223;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20132;&#20114;&#24335;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#23427;&#20204;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25688;&#35201;&#29983;&#25104;&#22120;&#26469;&#32858;&#21512;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new task in the field of Answering Subjective Induction Question on Products (SUBJPQA). The answer to this kind of question is non-unique, but can be interpreted from many perspectives. For example, the answer to 'whether the phone is heavy' has a variety of different viewpoints. A satisfied answer should be able to summarize these subjective opinions from multiple sources and provide objective knowledge, such as the weight of a phone. That is quite different from the traditional QA task, in which the answer to a factoid question is unique and can be found from a single data source. To address this new task, we propose a three-steps method. We first retrieve all answer-related clues from multiple knowledge sources on facts and opinions. The implicit commonsense facts are also collected to supplement the necessary but missing contexts. We then capture their relevance with the questions by interactive attention. Next, we design a reinforcement-based summarizer to ag
&lt;/p&gt;</description></item><item><title>MatSciML&#26159;&#19968;&#20010;&#29992;&#20110;&#22266;&#24577;&#26448;&#26009;&#24314;&#27169;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#26448;&#26009;&#31995;&#32479;&#21644;&#23646;&#24615;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#35813;&#39046;&#22495;&#20013;&#26041;&#27861;&#24615;&#33021;&#27604;&#36739;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05934</link><description>&lt;p&gt;
MatSciML: &#29992;&#20110;&#22266;&#24577;&#26448;&#26009;&#24314;&#27169;&#30340;&#24191;&#27867;&#12289;&#22810;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MatSciML: A Broad, Multi-Task Benchmark for Solid-State Materials Modeling. (arXiv:2309.05934v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05934
&lt;/p&gt;
&lt;p&gt;
MatSciML&#26159;&#19968;&#20010;&#29992;&#20110;&#22266;&#24577;&#26448;&#26009;&#24314;&#27169;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#26448;&#26009;&#31995;&#32479;&#21644;&#23646;&#24615;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#35813;&#39046;&#22495;&#20013;&#26041;&#27861;&#24615;&#33021;&#27604;&#36739;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MatSciML&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20855;&#26377;&#21608;&#26399;&#24615;&#26230;&#20307;&#32467;&#26500;&#30340;&#22266;&#24577;&#26448;&#26009;&#24314;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#12290;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#22266;&#24577;&#26448;&#26009;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#30001;&#20110;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#30340;&#31181;&#31867;&#32321;&#22810;&#65292;&#23548;&#33268;&#39046;&#22495;&#20869;&#23384;&#22312;&#30528;&#36739;&#22823;&#30340;&#20998;&#25955;&#24615;&#12290;&#36825;&#31181;&#20998;&#25955;&#24615;&#20351;&#24471;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#21464;&#24471;&#22256;&#38590;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#25972;&#20307;&#30740;&#31350;&#36827;&#23637;&#12290;&#22312;&#24320;&#28304;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#65292;&#21253;&#25324;OpenCatalyst&#12289;OQMD&#12289;NOMAD&#12289;Carolina&#26448;&#26009;&#25968;&#25454;&#24211;&#21644;Materials Project&#31561;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;MatSciML&#22522;&#20934;&#20026;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#26448;&#26009;&#31995;&#32479;&#21644;&#23646;&#24615;&#25968;&#25454;&#65292;&#21253;&#25324;&#27169;&#25311;&#33021;&#37327;&#12289;&#21407;&#23376;&#21147;&#12289;&#26448;&#26009;&#33021;&#38553;&#20197;&#21450;&#36890;&#36807;&#31354;&#38388;&#32676;&#36827;&#34892;&#26230;&#20307;&#23545;&#31216;&#24615;&#30340;&#20998;&#31867;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MatSci ML, a novel benchmark for modeling MATerials SCIence using Machine Learning (MatSci ML) methods focused on solid-state materials with periodic crystal structures. Applying machine learning methods to solid-state materials is a nascent field with substantial fragmentation largely driven by the great variety of datasets used to develop machine learning models. This fragmentation makes comparing the performance and generalizability of different methods difficult, thereby hindering overall research progress in the field. Building on top of open-source datasets, including large-scale datasets like the OpenCatalyst, OQMD, NOMAD, the Carolina Materials Database, and Materials Project, the MatSci ML benchmark provides a diverse set of materials systems and properties data for model training and evaluation, including simulated energies, atomic forces, material bandgaps, as well as classification data for crystal symmetries via space groups. The diversity of properties in MatSc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#34903;&#26223;&#22270;&#20687;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#38754;&#21442;&#32771;&#65292;&#35299;&#20915;&#20102;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#21019;&#24314;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05930</link><description>&lt;p&gt;
&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#34903;&#26223;&#22270;&#20687;&#26469;&#32472;&#21046;&#23567;&#20892;&#25143;&#20892;&#20316;&#29289;&#31867;&#22411;&#30340;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Combining deep learning and street view imagery to map smallholder crop types. (arXiv:2309.05930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#34903;&#26223;&#22270;&#20687;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#38754;&#21442;&#32771;&#65292;&#35299;&#20915;&#20102;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#21019;&#24314;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#22270;&#23545;&#20110;&#30417;&#27979;&#35268;&#27169;&#30340;&#20135;&#37327;&#36827;&#23637;&#12289;&#39044;&#27979;&#20840;&#29699;&#20892;&#20316;&#29289;&#29983;&#20135;&#21644;&#21046;&#23450;&#26377;&#25928;&#25919;&#31574;&#26159;&#19968;&#31181;&#24517;&#35201;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#32570;&#20047;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#30340;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#22270;&#21046;&#20316;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30000;&#38388;&#35843;&#26597;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#26159;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#12289;&#37329;&#38065;&#21644;&#32479;&#35745;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#25552;&#20379;&#20102;&#34903;&#26223;&#22270;&#20687;&#65288;&#22914;Google Street View&#65292;KartaView&#21644;Mapillary&#65289;&#12290;&#36825;&#20123;&#22270;&#20687;&#21253;&#21547;&#20102;&#29305;&#23450;&#20301;&#32622;&#21644;&#26102;&#38388;&#30340;&#20892;&#20316;&#29289;&#31181;&#31867;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;Google Street View&#22270;&#20687;&#29983;&#25104;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#38754;&#21442;&#32771;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#31579;&#36873;&#20102;&#19968;&#32452;&#21253;&#21547;&#20892;&#30000;&#30340;&#34903;&#26223;&#22270;&#20687;&#65292;&#36890;&#36807;&#21033;&#29992;&#24369;&#26631;&#31614;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#20892;&#20316;&#29289;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate crop type maps are an essential source of information for monitoring yield progress at scale, projecting global crop production, and planning effective policies. To date, however, crop type maps remain challenging to create in low and middle-income countries due to a lack of ground truth labels for training machine learning models. Field surveys are the gold standard in terms of accuracy but require an often-prohibitively large amount of time, money, and statistical capacity. In recent years, street-level imagery, such as Google Street View, KartaView, and Mapillary, has become available around the world. Such imagery contains rich information about crop types grown at particular locations and times. In this work, we develop an automated system to generate crop type ground references using deep learning and Google Street View imagery. The method efficiently curates a set of street view images containing crop fields, trains a model to predict crop type by utilizing weakly-label
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{bio}$FAME&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#30340;&#39044;&#35757;&#32451;&#12290;&#20854;&#36890;&#36807;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#36827;&#34892;&#34920;&#31034;&#21442;&#25968;&#21270;&#65292;&#21033;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.05927</link><description>&lt;p&gt;
&#38024;&#23545;&#29983;&#29289;&#20449;&#21495;&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals. (arXiv:2309.05927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{bio}$FAME&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#30340;&#39044;&#35757;&#32451;&#12290;&#20854;&#36890;&#36807;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#36827;&#34892;&#34920;&#31034;&#21442;&#25968;&#21270;&#65292;&#21033;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26469;&#33258;&#29983;&#29289;&#20449;&#21495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#23545;&#20154;&#20204;&#30340;&#36523;&#24515;&#29366;&#24577;&#36827;&#34892;&#32508;&#21512;&#24314;&#27169;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#36890;&#24120;&#22312;&#39044;&#35757;&#32451;&#21644;&#25512;&#26029;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36825;&#28304;&#20110;&#20219;&#21153;&#35268;&#33539;&#30340;&#21464;&#21270;&#25110;&#32773;&#27169;&#24577;&#32452;&#21512;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#22312;&#28508;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39057;&#29575;&#24863;&#30693;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;$\texttt{bio}$FAME&#65289;&#65292;&#35813;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#30340;&#34920;&#31034;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;$\texttt{bio}$FAME&#21253;&#21547;&#19968;&#20010;&#39057;&#29575;&#24863;&#30693;&#21464;&#21387;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#22266;&#23450;&#22823;&#23567;&#30340;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#19982;&#36755;&#20837;&#30340;&#38271;&#24230;&#21644;&#37319;&#26679;&#29575;&#26080;&#20851;&#12290;&#20026;&#20102;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#28508;&#31354;&#38388;&#20013;&#25191;&#34892;&#25513;&#30721;&#33258;&#32534;&#30721;&#12290;&#26368;&#32456;&#30340;&#26550;&#26500;&#26377;&#25928;&#22320;&#25429;&#33719;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#39057;&#29575;&#29305;&#24449;&#21644;&#27169;&#24577;&#32452;&#21512;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder ($\texttt{bio}$FAME) that learns to parameterize the representation of biosignals in the frequency space. $\texttt{bio}$FAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. The resulting architecture effectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#12290;&#32463;&#39564;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.05925</link><description>&lt;p&gt;
&#20851;&#20110;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Regularized Sparse Logistic Regression. (arXiv:2309.05925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#12290;&#32463;&#39564;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#26088;&#22312;&#21516;&#26102;&#36827;&#34892;&#39640;&#32500;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#30740;&#31350;&#35299;&#20915;&#20102;$\ell_1$&#27491;&#21017;&#21270;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#19982;&#38750;&#20984;&#24809;&#32602;&#30456;&#20851;&#30340;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#24182;&#27809;&#26377;&#31561;&#37327;&#30340;&#25991;&#29486;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#19968;&#23450;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#31867;&#20284;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#22312;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19981;&#21516;&#30340;&#32447;&#25628;&#32034;&#20934;&#21017;&#26469;&#20445;&#35777;&#19981;&#21516;&#27491;&#21017;&#21270;&#39033;&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse logistic regression aims to perform classification and feature selection simultaneously for high-dimensional data. Although many studies have been done to solve $\ell_1$-regularized logistic regression, there is no equivalently abundant literature about solving sparse logistic regression associated with nonconvex penalties. In this paper, we propose to solve $\ell_1$-regularized sparse logistic regression and some nonconvex penalties-regularized sparse logistic regression, when the nonconvex penalties satisfy some prerequisites, with similar optimization frameworks. In the proposed optimization frameworks, we utilize different line search criteria to guarantee good convergence performance for different regularization terms. Empirical experiments on binary classification tasks with real-world datasets demonstrate our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#21253;&#25324;&#24187;&#35273;&#29616;&#35937;&#30340;&#20998;&#31867;&#12289;&#35780;&#20272;&#26631;&#20934;&#21644;&#20943;&#36731;&#24187;&#35273;&#30340;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.05922</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Hallucination in Large Foundation Models. (arXiv:2309.05922v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#21253;&#25324;&#24187;&#35273;&#29616;&#35937;&#30340;&#20998;&#31867;&#12289;&#35780;&#20272;&#26631;&#20934;&#21644;&#20943;&#36731;&#24187;&#35273;&#30340;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#25351;&#30340;&#26159;&#29983;&#25104;&#20559;&#31163;&#20107;&#23454;&#30340;&#20869;&#23481;&#25110;&#21253;&#21547;&#34394;&#26500;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#36817;&#26399;&#21162;&#21147;&#30340;&#24191;&#27867;&#27010;&#36848;&#65292;&#36825;&#20123;&#21162;&#21147;&#26088;&#22312;&#35782;&#21035;&#12289;&#38416;&#26126;&#21644;&#35299;&#20915;&#24187;&#35273;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#8220;&#22823;&#8221;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#12290;&#26412;&#25991;&#23545;&#29305;&#23450;&#20110;LFMs&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#24314;&#31435;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;&#23427;&#36824;&#26816;&#26597;&#20102;&#20943;&#36731;LFM&#20013;&#24187;&#35273;&#30340;&#29616;&#26377;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;LFMs&#20013;&#24187;&#35273;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucination in a foundation model (FM) refers to the generation of content that strays from factual reality or includes fabricated information. This survey paper provides an extensive overview of recent efforts that aim to identify, elucidate, and tackle the problem of hallucination, with a particular focus on ``Large'' Foundation Models (LFMs). The paper classifies various types of hallucination phenomena that are specific to LFMs and establishes evaluation criteria for assessing the extent of hallucination. It also examines existing strategies for mitigating hallucination in LFMs and discusses potential directions for future research in this area. Essentially, the paper offers a comprehensive examination of the challenges and solutions related to hallucination in LFMs.
&lt;/p&gt;</description></item><item><title>SAGE&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#21313;&#20159;&#32423;&#20135;&#21697;&#30446;&#24405;&#20013;&#29983;&#25104;&#23646;&#24615;&#20540;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#36328;&#35821;&#35328;&#12289;&#20135;&#21697;&#31867;&#22411;&#21644;&#30446;&#26631;&#23646;&#24615;&#30340;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25512;&#26029;&#38544;&#24335;&#20351;&#29992;&#36802;&#22238;&#35821;&#35328;&#25552;&#21040;&#30340;&#23646;&#24615;&#20540;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#23646;&#24615;&#30340;&#19981;&#36866;&#29992;&#24615;&#21644;&#26080;&#27861;&#20174;&#21487;&#29992;&#20449;&#24687;&#20013;&#33719;&#21462;&#23646;&#24615;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.05920</link><description>&lt;p&gt;
SAGE: &#38024;&#23545;&#21313;&#20159;&#32423;&#20135;&#21697;&#30446;&#24405;&#30340;&#32467;&#26500;&#21270;&#23646;&#24615;&#20540;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SAGE: Structured Attribute Value Generation for Billion-Scale Product Catalogs. (arXiv:2309.05920v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05920
&lt;/p&gt;
&lt;p&gt;
SAGE&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#21313;&#20159;&#32423;&#20135;&#21697;&#30446;&#24405;&#20013;&#29983;&#25104;&#23646;&#24615;&#20540;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#36328;&#35821;&#35328;&#12289;&#20135;&#21697;&#31867;&#22411;&#21644;&#30446;&#26631;&#23646;&#24615;&#30340;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25512;&#26029;&#38544;&#24335;&#20351;&#29992;&#36802;&#22238;&#35821;&#35328;&#25552;&#21040;&#30340;&#23646;&#24615;&#20540;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#23646;&#24615;&#30340;&#19981;&#36866;&#29992;&#24615;&#21644;&#26080;&#27861;&#20174;&#21487;&#29992;&#20449;&#24687;&#20013;&#33719;&#21462;&#23646;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SAGE&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#20840;&#29699;&#30005;&#23376;&#21830;&#21153;&#30446;&#24405;&#20013;&#25512;&#26029;&#20135;&#21697;&#23646;&#24615;&#20540;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#23646;&#24615;&#20540;&#39044;&#27979;&#38382;&#39064;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#36716;&#21270;&#20026;&#36328;&#35821;&#35328;&#12289;&#20135;&#21697;&#31867;&#22411;&#21644;&#30446;&#26631;&#23646;&#24615;&#30340;Seq2Seq&#25688;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#26041;&#27861;&#19981;&#20877;&#23616;&#38480;&#20110;&#22312;&#39044;&#20808;&#25351;&#23450;&#30340;&#36873;&#39033;&#38598;&#20869;&#39044;&#27979;&#23646;&#24615;&#20540;&#65292;&#24182;&#19988;&#20063;&#19981;&#35201;&#27714;&#25152;&#23547;&#25214;&#30340;&#23646;&#24615;&#20540;&#22312;&#25991;&#26412;&#20013;&#26126;&#30830;&#25552;&#21450;&#12290;SAGE&#21487;&#20197;&#25512;&#26029;&#38544;&#24335;&#20351;&#29992;&#36802;&#22238;&#35821;&#35328;&#25552;&#21040;&#30340;&#23646;&#24615;&#20540;&#65292;&#25110;&#32773;&#26681;&#26412;&#19981;&#25552;&#21450;&#30340;&#24120;&#35782;&#40664;&#35748;&#24773;&#20917;&#19979;&#30340;&#23646;&#24615;&#20540;&#12290;&#27492;&#22806;&#65292;SAGE&#33021;&#22815;&#39044;&#27979;&#19968;&#20010;&#23646;&#24615;&#23545;&#20110;&#24403;&#21069;&#20135;&#21697;&#26159;&#21542;&#19981;&#36866;&#29992;&#65292;&#25110;&#32773;&#26159;&#21542;&#26080;&#27861;&#20174;&#21487;&#29992;&#20449;&#24687;&#20013;&#33719;&#21462;&#12290;SAGE&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#22312;&#23454;&#38469;&#30005;&#23376;&#21830;&#21153;&#30446;&#24405;&#35774;&#32622;&#20013;&#22788;&#29702;&#23646;&#24615;&#20540;&#39044;&#27979;&#20219;&#21153;&#25152;&#26377;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;&#19968;&#22871;&#32508;&#21512;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We introduce SAGE; a Generative LLM for inferring attribute values for products across world-wide e-Commerce catalogs. We introduce a novel formulation of the attribute-value prediction problem as a Seq2Seq summarization task, across languages, product types and target attributes. Our novel modeling approach lifts the restriction of predicting attribute values within a pre-specified set of choices, as well as, the requirement that the sought attribute values need to be explicitly mentioned in the text. SAGE can infer attribute values even when such values are mentioned implicitly using periphrastic language, or not-at-all-as is the case for common-sense defaults. Additionally, SAGE is capable of predicting whether an attribute is inapplicable for the product at hand, or non-obtainable from the available information. SAGE is the first method able to tackle all aspects of the attribute-value-prediction task as they arise in practical settings in e-Commerce catalogs. A comprehensive set o
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;</title><link>http://arxiv.org/abs/2309.05918</link><description>&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#65306;&#36208;&#21521;&#31526;&#21495;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#26412;&#20307;&#35770;&#22522;&#20110;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05918
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;LLMs&#26080;&#27861;&#29702;&#35299;&#35821;&#35328;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#20197;&#20381;&#36182;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#23427;&#20204;&#23384;&#20648;&#30340;&#35821;&#35328;&#30693;&#35782;&#22475;&#34255;&#22312;&#26080;&#24847;&#20041;&#30340;&#24494;&#29305;&#24449;&#20013;&#65292;&#24182;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#12290;&#26412;&#25991;&#24314;&#35758;&#22312;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#22260;&#32469;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#23545;&#25104;&#21151;&#30340;&#29378;&#28909;&#26159;&#26377;&#20123;&#35823;&#23548;&#30340;&#65292;&#21407;&#22240;&#22914;&#19979;&#65306;&#65288;i&#65289;LLMs&#19981;&#33021;&#20381;&#36182;&#20110;&#20107;&#23454;&#20449;&#24687;&#65292;&#22240;&#20026;&#23545;&#20110;LLMs&#26469;&#35828;&#65292;&#25668;&#20837;&#30340;&#25152;&#26377;&#25991;&#26412;&#65288;&#20107;&#23454;&#25110;&#38750;&#20107;&#23454;&#65289;&#37117;&#26159;&#24179;&#31561;&#30340;&#65307;&#65288;ii&#65289;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#35821;&#35328;&#30340;&#20219;&#20309;&#8220;&#30693;&#35782;&#8221;&#37117;&#23558;&#27704;&#36828;&#22475;&#34255;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#26412;&#36523;&#26159;&#26377;&#24847;&#20041;&#30340;&#65307;&#20197;&#21450;&#65288;iii&#65289;LLMs&#22312;&#20960;&#31181;&#35821;&#35328;&#19978;&#19979;&#25991;&#20013;&#24120;&#24120;&#26080;&#27861;&#36827;&#34892;&#27491;&#30830;&#25512;&#29702;&#65288;&#22914;&#21517;&#35789;&#22797;&#21512;&#35789;&#12289;&#20849;&#35859;&#35789;&#12289;&#37327;&#35789;&#33539;&#22260;&#27169;&#31946;&#21644;&#24847;&#21521;&#24615;&#19978;&#19979;&#25991;&#65289;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30456;&#23545;&#25104;&#21151;&#19981;&#26159;&#31526;&#21495;&#19982;&#20122;&#31526;&#21495;&#20043;&#36777;&#30340;&#21453;&#26144;&#65292;&#32780;&#26159;&#22312;&#35268;&#27169;&#19978;&#24212;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#30340;&#25104;&#21151;&#31574;&#30053;&#30340;&#21453;&#26144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26377;&#25928;&#30340;&#33258;&#19979;&#32780;&#19978;&#31574;&#30053;&#24212;&#29992;&#20110;&#31526;&#21495;&#21270;&#26041;&#27861;&#20013;
&lt;/p&gt;
&lt;p&gt;
In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic na-ture, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambi-guities, intensional contexts. Since we believe the relative success of data-driven large language models (LLMs) is not a reflection on the symbolic vs. subsymbol-ic debate but a reflection on applying the successful strategy of a bottom-up reverse engineering of language at scale, we suggest in this paper applying the effective bottom-up strategy in a symbol
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#21160;&#24577;&#35268;&#21010;&#24212;&#29992;&#20110;&#20915;&#31574;Transformer&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#20010;&#27493;&#39588;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#35780;&#20272;&#21160;&#20316;&#36136;&#37327;&#65292;&#24182;&#35757;&#32451;ACT&#29983;&#25104;&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#21160;&#20316;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05915</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21183;&#35843;&#33410;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#22686;&#24378;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning. (arXiv:2309.05915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#21160;&#24577;&#35268;&#21010;&#24212;&#29992;&#20110;&#20915;&#31574;Transformer&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#20010;&#27493;&#39588;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#35780;&#20272;&#21160;&#20316;&#36136;&#37327;&#65292;&#24182;&#35757;&#32451;ACT&#29983;&#25104;&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#21160;&#20316;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;Transformer (DT) &#21033;&#29992;&#34920;&#36798;&#20016;&#23500;&#30340;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#26469;&#25191;&#34892;&#21160;&#20316;&#29983;&#25104;&#65292;&#24050;&#25104;&#20026;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;DT &#29983;&#25104;&#30340;&#21160;&#20316;&#26159;&#22522;&#20110;&#26399;&#26395;&#26410;&#26469;&#22238;&#25253;&#30340;&#26465;&#20214;&#65292;&#24050;&#30693;&#20855;&#26377;&#26576;&#20123;&#24369;&#28857;&#65292;&#27604;&#22914;&#26131;&#21463;&#29615;&#22659;&#38543;&#26426;&#24615;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;DT&#30340;&#24369;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;DT&#20013;&#22686;&#21152;&#21160;&#24577;&#35268;&#21010;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#26469;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#36825;&#28041;&#21450;&#21040;MDP&#32467;&#26500;&#19978;&#30340;&#21160;&#24577;&#35268;&#21010;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#26469;&#35780;&#20272;&#21160;&#20316;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#20248;&#21183;&#20272;&#35745;&#22120;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20197;&#20272;&#35745;&#30340;&#20248;&#21183;&#20026;&#26465;&#20214;&#29983;&#25104;&#21160;&#20316;&#30340;&#20248;&#21183;&#26465;&#20214;Transformer (ACT)&#12290;&#26368;&#21518;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;ACT&#26681;&#25454;&#25152;&#38656;&#30340;&#20248;&#21183;&#29983;&#25104;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Decision Transformer (DT), which employs expressive sequence modeling techniques to perform action generation, has emerged as a promising approach to offline policy optimization. However, DT generates actions conditioned on a desired future return, which is known to bear some weaknesses such as the susceptibility to environmental stochasticity. To overcome DT's weaknesses, we propose to empower DT with dynamic programming. Our method comprises three steps. First, we employ in-sample value iteration to obtain approximated value functions, which involves dynamic programming over the MDP structure. Second, we evaluate action quality in context with estimated advantages. We introduce two types of advantage estimators, IAE and GAE, which are suitable for different tasks. Third, we train an Advantage-Conditioned Transformer (ACT) to generate actions conditioned on the estimated advantages. Finally, during testing, ACT generates actions conditioned on a desired advantage. Our evaluation resul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20851;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;(QAD)&#65292;&#36890;&#36807;&#27169;&#22411;&#20869;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#21516;&#26102;&#26816;&#27979;&#19981;&#21516;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#12290;&#26159;&#19968;&#31181;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05911</link><description>&lt;p&gt;
&#19982;&#27169;&#22411;&#20869;&#21327;&#20316;&#23398;&#20064;&#26080;&#20851;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quality-Agnostic Deepfake Detection with Intra-model Collaborative Learning. (arXiv:2309.05911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20851;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;(QAD)&#65292;&#36890;&#36807;&#27169;&#22411;&#20869;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#21516;&#26102;&#26816;&#27979;&#19981;&#21516;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#12290;&#26159;&#19968;&#31181;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#20266;&#36896;&#24341;&#21457;&#20102;&#35832;&#22810;&#31038;&#20250;&#20851;&#27880;&#65292;&#21487;&#33021;&#23545;&#23433;&#20840;&#36896;&#25104;&#23041;&#32961;&#24182;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#12290;&#24050;&#32463;&#26377;&#24456;&#22810;&#20851;&#20110;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;&#21516;&#26102;&#26816;&#27979;&#20302;&#36136;&#37327;&#21644;&#19981;&#21516;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#20808;&#36827;&#26041;&#27861;&#20165;&#20351;&#29992;&#21333;&#19968;&#29305;&#23450;&#27169;&#22411;&#26469;&#26816;&#27979;&#29305;&#23450;&#30340;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#36136;&#37327;&#31867;&#22411;&#65292;&#36825;&#31181;&#31574;&#30053;&#22312;&#26500;&#24314;&#22810;&#20010;&#27169;&#22411;&#26102;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#24320;&#38144;&#65292;&#32780;&#19988;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26080;&#27861;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#22411;&#20869;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#36136;&#37327;&#28145;&#24230;&#20266;&#36896;&#30340;&#26377;&#25928;&#21516;&#26102;&#26816;&#27979;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#26080;&#20851;&#36136;&#37327;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;QAD&#12290;&#36890;&#36807;&#35266;&#23519;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Deepfake has recently raised a plethora of societal concerns over its possible security threats and dissemination of fake information. Much research on deepfake detection has been undertaken. However, detecting low quality as well as simultaneously detecting different qualities of deepfakes still remains a grave challenge. Most SOTA approaches are limited by using a single specific model for detecting certain deepfake video quality type. When constructing multiple models with prior information about video quality, this kind of strategy incurs significant computational cost, as well as model and training data overhead. Further, it cannot be scalable and practical to deploy in real-world settings. In this work, we propose a universal intra-model collaborative learning framework to enable the effective and simultaneous detection of different quality of deepfakes. That is, our approach is the quality-agnostic deepfake detection method, dubbed QAD . In particular, by observing the upper bou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21338;&#24328;&#35770;&#26694;&#26550;&#19979;&#30340;&#25112;&#30053;&#20915;&#31574;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#23545;&#24773;&#22659;&#26694;&#26550;&#25935;&#24863;&#20294;&#25277;&#35937;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21644;LLaMa-2&#22312;&#28216;&#25103;&#32467;&#26500;&#21644;&#24773;&#22659;&#19979;&#33021;&#35843;&#25972;&#31574;&#30053;&#65292;&#20294;LLaMa-2&#22312;&#28216;&#25103;&#26426;&#21046;&#30340;&#29702;&#35299;&#19978;&#26356;&#21152;&#24494;&#22937;&#12290;</title><link>http://arxiv.org/abs/2309.05898</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25112;&#30053;&#34892;&#20026;&#65306;&#28216;&#25103;&#32467;&#26500;&#19982;&#24773;&#22659;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Strategic Behavior of Large Language Models: Game Structure vs. Contextual Framing. (arXiv:2309.05898v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21338;&#24328;&#35770;&#26694;&#26550;&#19979;&#30340;&#25112;&#30053;&#20915;&#31574;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#23545;&#24773;&#22659;&#26694;&#26550;&#25935;&#24863;&#20294;&#25277;&#35937;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21644;LLaMa-2&#22312;&#28216;&#25103;&#32467;&#26500;&#21644;&#24773;&#22659;&#19979;&#33021;&#35843;&#25972;&#31574;&#30053;&#65292;&#20294;LLaMa-2&#22312;&#28216;&#25103;&#26426;&#21046;&#30340;&#29702;&#35299;&#19978;&#26356;&#21152;&#24494;&#22937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65306;GPT-3.5&#12289;GPT-4&#21644;LLaMa-2&#22312;&#21338;&#24328;&#35770;&#26694;&#26550;&#19979;&#30340;&#25112;&#30053;&#20915;&#31574;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#22235;&#31181;&#20856;&#22411;&#30340;&#21452;&#20154;&#21338;&#24328;&#28216;&#25103;&#8212;&#8212;&#22234;&#24466;&#22256;&#22659;&#12289;&#29454;&#20820;&#12289;&#38634;&#23849;&#21644;&#22234;&#24466;&#30340;&#21916;&#24742;&#8212;&#8212;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#23548;&#33322;&#26041;&#24335;&#65292;&#21363;&#29609;&#23478;&#21487;&#20197;&#21512;&#20316;&#33719;&#24471;&#38598;&#20307;&#21033;&#30410;&#65292;&#20063;&#21487;&#20197;&#20026;&#20102;&#20010;&#20154;&#21033;&#30410;&#32780;&#32972;&#21467;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20998;&#26512;&#65292;&#30740;&#31350;&#24773;&#22659;&#26694;&#26550;&#65288;&#22914;&#22806;&#20132;&#20851;&#31995;&#25110;&#38750;&#27491;&#24335;&#21451;&#35850;&#65289;&#22312;&#22609;&#36896;&#27169;&#22411;&#20915;&#31574;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#26223;&#35266;&#65306;&#34429;&#28982;GPT-3.5&#23545;&#24773;&#22659;&#26694;&#26550;&#38750;&#24120;&#25935;&#24863;&#65292;&#20294;&#23427;&#22312;&#25277;&#35937;&#25112;&#30053;&#25512;&#29702;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#32780;GPT-4&#21644;LLaMa-2&#26681;&#25454;&#28216;&#25103;&#32467;&#26500;&#21644;&#24773;&#22659;&#35843;&#25972;&#31574;&#30053;&#65292;&#20294;LLaMa-2&#22312;&#23545;&#28216;&#25103;&#28508;&#22312;&#26426;&#21046;&#30340;&#29702;&#35299;&#19978;&#26356;&#21152;&#24494;&#22937;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#21644;&#22810;&#26679;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the strategic decision-making capabilities of three Large Language Models (LLMs): GPT-3.5, GPT-4, and LLaMa-2, within the framework of game theory. Utilizing four canonical two-player games -- Prisoner's Dilemma, Stag Hunt, Snowdrift, and Prisoner's Delight -- we explore how these models navigate social dilemmas, situations where players can either cooperate for a collective benefit or defect for individual gain. Crucially, we extend our analysis to examine the role of contextual framing, such as diplomatic relations or casual friendships, in shaping the models' decisions. Our findings reveal a complex landscape: while GPT-3.5 is highly sensitive to contextual framing, it shows limited ability to engage in abstract strategic reasoning. Both GPT-4 and LLaMa-2 adjust their strategies based on game structure and context, but LLaMa-2 exhibits a more nuanced understanding of the games' underlying mechanics. These results highlight the current limitations and varied p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32908;&#32905;&#39592;&#39612;&#27169;&#25311;&#21644;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#21644;&#32908;&#32905;&#21147;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.05863</link><description>&lt;p&gt;
&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22806;&#37096;&#27169;&#25311;&#20154;&#31867;&#36816;&#21160;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The bionic neural network for external simulation of human locomotor system. (arXiv:2309.05863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32908;&#32905;&#39592;&#39612;&#27169;&#25311;&#21644;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#21644;&#32908;&#32905;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#32908;&#32905;&#39592;&#39612;&#27169;&#25311;&#25216;&#26415;&#20272;&#35745;&#30340;&#32908;&#32905;&#21147;&#37327;&#21644;&#20851;&#33410;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#25551;&#36848;&#36816;&#21160;&#36136;&#37327;&#30340;&#26377;&#29992;&#25351;&#26631;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#35745;&#31639;&#26426;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#39537;&#21160;&#32908;&#32905;&#12289;&#32908;&#32905;&#21160;&#21147;&#23398;&#12289;&#36523;&#20307;&#21644;&#20851;&#33410;&#21160;&#21147;&#23398;&#20197;&#21450;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#23384;&#22312;&#35745;&#31639;&#26102;&#38388;&#38271;&#21644;&#32908;&#32905;&#25307;&#21215;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#24182;&#19981;&#23481;&#26131;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32908;&#32905;&#39592;&#39612;&#27169;&#25311;&#30340;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#21644;&#32908;&#32905;&#21147;&#37327;&#12290;&#23558;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#20316;&#20026;&#19968;&#20010;&#24102;&#26377;&#29983;&#29702;&#21442;&#25968;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35782;&#21035;&#32908;&#32905;&#28608;&#27963;&#21160;&#21147;&#23398;&#21644;&#32908;&#32905;&#25910;&#32553;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Muscle forces and joint kinematics estimated with musculoskeletal (MSK) modeling techniques offer useful metrics describing movement quality. Model-based computational MSK models can interpret the dynamic interaction between the neural drive to muscles, muscle dynamics, body and joint kinematics, and kinetics. Still, such a set of solutions suffers from high computational time and muscle recruitment problems, especially in complex modeling. In recent years, data-driven methods have emerged as a promising alternative due to the benefits of flexibility and adaptability. However, a large amount of labeled training data is not easy to be acquired. This paper proposes a physics-informed deep learning method based on MSK modeling to predict joint motion and muscle forces. The MSK model is embedded into the neural network as an ordinary differential equation (ODE) loss function with physiological parameters of muscle activation dynamics and muscle contraction dynamics to be identified. These 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;mesa-optimization&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.05858</link><description>&lt;p&gt;
&#25581;&#31034;Transformer&#20013;&#30340;mesa-optimization&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Uncovering mesa-optimization algorithms in Transformers. (arXiv:2309.05858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;mesa-optimization&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#20027;&#23548;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#21331;&#36234;&#24615;&#33021;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20551;&#35774;Transformer&#30340;&#24378;&#22823;&#24615;&#33021;&#28304;&#20110;&#20854;&#26550;&#26500;&#20013;&#23545;mesa-optimization&#30340;&#20559;&#22909;&#65292;&#21363;&#19968;&#20010;&#23398;&#20064;&#36807;&#31243;&#22312;&#27169;&#22411;&#30340;&#21069;&#21521;&#20256;&#36882;&#20013;&#36816;&#34892;&#65292;&#30001;&#20197;&#19979;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65306;&#65288;i&#65289;&#26500;&#24314;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#65292;&#21644;&#65288;ii&#65289;&#36890;&#36807;&#20248;&#21270;&#25214;&#21040;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#22312;&#31616;&#21333;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;Transformer&#36827;&#34892;&#20102;&#36870;&#21521;&#24037;&#31243;&#65292;&#25581;&#31034;&#20102;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24213;&#23618;mesa-optimization&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#21069;&#21521;&#20256;&#36882;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#31435;&#21363;&#34987;&#37325;&#26032;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#36825;&#34920;&#26126;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention 
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24322;&#24120;&#27963;&#21160;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;F1&#20998;&#25968;&#20026;0.839&#12290;</title><link>http://arxiv.org/abs/2309.05845</link><description>&lt;p&gt;
&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#25968;&#25454;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#24322;&#24120;&#27963;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data. (arXiv:2309.05845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05845
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24322;&#24120;&#27963;&#21160;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;F1&#20998;&#25968;&#20026;0.839&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#25968;&#25454;&#36890;&#36807;&#22810;&#20010;&#20256;&#24863;&#22120;&#25910;&#38598;&#65292;&#20026;&#26234;&#33021;&#21307;&#30103;&#22330;&#26223;&#20013;&#20934;&#30830;&#30340;&#24322;&#24120;&#27963;&#21160;&#26816;&#27979;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24322;&#24120;&#34920;&#29616;&#20986;&#22810;&#26679;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#22312;MTS&#25968;&#25454;&#20013;&#21464;&#24471;&#19981;&#23481;&#26131;&#23519;&#35273;&#12290;&#22240;&#27492;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;Rs-AD&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24322;&#24120;&#27963;&#21160;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#27493;&#24577;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;F1&#20998;&#25968;&#20026;0.839&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series (MTS) data collected from multiple sensors provide the potential for accurate abnormal activity detection in smart healthcare scenarios. However, anomalies exhibit diverse patterns and become unnoticeable in MTS data. Consequently, achieving accurate anomaly detection is challenging since we have to capture both temporal dependencies of time series and inter-relationships among variables. To address this problem, we propose a Residual-based Anomaly Detection approach, Rs-AD, for effective representation learning and abnormal activity detection. We evaluate our scheme on a real-world gait dataset and the experimental results demonstrate an F1 score of 0.839.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05833</link><description>&lt;p&gt;
PACE: &#20351;&#29992;GPT-4&#36827;&#34892;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#20013;&#30340;&#25552;&#31034;&#21644;&#22686;&#21152;&#20197;&#36827;&#34892;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;IT&#34892;&#19994;&#21521;&#22522;&#20110;&#20113;&#30340;&#24179;&#21488;&#30340;&#36716;&#21464;&#24378;&#35843;&#20102;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#32500;&#25252;&#23458;&#25143;&#20449;&#20219;&#12290;&#26680;&#24515;&#38382;&#39064;&#26159;&#26377;&#25928;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#30001;&#20110;&#24403;&#20195;&#20113;&#22522;&#30784;&#35774;&#26045;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#35782;&#21035;&#30340;&#22522;&#20110;AI&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20173;&#21463;&#21040;&#20854;&#36755;&#20986;&#36136;&#37327;&#19981;&#19968;&#33268;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#26681;&#25454;&#21382;&#21490;&#20107;&#20214;&#25968;&#25454;&#35780;&#20272;&#33258;&#36523;&#30340;&#32622;&#20449;&#24230;&#65292;&#32771;&#34385;&#20854;&#23545;&#35777;&#25454;&#30340;&#35780;&#20272;&#24378;&#24230;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#23457;&#26680;&#30001;&#39044;&#27979;&#22120;&#29983;&#25104;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#20248;&#21270;&#27493;&#39588;&#23558;&#36825;&#20123;&#35780;&#20272;&#32467;&#21512;&#36215;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#23454;&#39564;&#23460;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#26102;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.05831</link><description>&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#35299;&#20915;&#24037;&#20316;&#22330;&#25152;&#23433;&#20840;&#38382;&#39064;&#20013;&#65292;&#22522;&#20110;&#23454;&#39564;&#23460;&#25215;&#36733;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Studying Accuracy of Machine Learning Models Trained on Lab Lifting Data in Solving Real-World Problems Using Wearable Sensors for Workplace Safety. (arXiv:2309.05831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#23454;&#39564;&#23460;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#26102;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23454;&#39564;&#23460;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#23454;&#39564;&#23460;&#35757;&#32451;&#30340;&#20030;&#37325;&#35782;&#21035;&#27169;&#22411;&#31227;&#26893;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#12290;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#27604;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#35201;&#20302;&#24471;&#22810;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22833;&#36133;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Porting ML models trained on lab data to real-world situations has long been a challenge. This paper discusses porting a lab-trained lifting identification model to the real-world. With performance much lower than on training data, we explored causes of the failure and proposed four potential solutions to increase model performance
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#26102;&#38388;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22320;&#29702;&#32593;&#26684;&#20043;&#38388;&#30340;&#21160;&#24577;&#31354;&#38388;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.05828</link><description>&lt;p&gt;
&#25506;&#32034;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Geometric Deep Learning For Precipitation Nowcasting. (arXiv:2309.05828v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#26102;&#38388;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22320;&#29702;&#32593;&#26684;&#20043;&#38388;&#30340;&#21160;&#24577;&#31354;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#27700;&#39044;&#27979;&#65288;&#20960;&#23567;&#26102;&#20869;&#65289;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#38656;&#35201;&#20934;&#30830;&#25429;&#25417;&#22797;&#26434;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#20110;&#21367;&#31215;&#26680;&#19982;&#32593;&#26684;&#25968;&#25454;&#36827;&#34892;&#21367;&#31215;&#65292;&#24182;&#19988;&#25552;&#21462;&#30340;&#29305;&#24449;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#24863;&#30693;&#22495;&#65292;&#36890;&#24120;&#34920;&#29616;&#20026;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#27604;&#36807;&#24230;&#24179;&#28369;&#30340;&#36755;&#20986;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#32570;&#20047;&#23545;&#32593;&#26684;&#20043;&#38388;&#22797;&#26434;&#31354;&#38388;&#20851;&#31995;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26088;&#22312;&#23558;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25512;&#24191;&#21040;&#38750;&#27431;&#20960;&#37324;&#24503;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#23450;&#20041;&#33410;&#28857;&#21644;&#36793;&#32536;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22320;&#29702;&#32593;&#26684;&#20043;&#38388;&#30340;&#21160;&#24577;&#31354;&#38388;&#20851;&#31995;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24212;&#29992;&#20110;&#38477;&#27700;&#39044;&#27979;&#12290;&#33258;&#21160;&#23398;&#20064;&#25551;&#36848;&#32593;&#26684;&#21333;&#20803;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#20540;&#19982;&#30495;&#23454;&#20687;&#32032;&#20540;&#20043;&#38388;&#30340;L1&#25439;&#22833;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precipitation nowcasting (up to a few hours) remains a challenge due to the highly complex local interactions that need to be captured accurately. Convolutional Neural Networks rely on convolutional kernels convolving with grid data and the extracted features are trapped by limited receptive field, typically expressed in excessively smooth output compared to ground truth. Thus they lack the capacity to model complex spatial relationships among the grids. Geometric deep learning aims to generalize neural network models to non-Euclidean domains. Such models are more flexible in defining nodes and edges and can effectively capture dynamic spatial relationship among geographical grids. Motivated by this, we explore a geometric deep learning-based temporal Graph Convolutional Network (GCN) for precipitation nowcasting. The adjacency matrix that simulates the interactions among grid cells is learned automatically by minimizing the L1 loss between prediction and ground truth pixel value durin
&lt;/p&gt;</description></item><item><title>PhotoVerse&#26159;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#20998;&#25903;&#35843;&#33410;&#26426;&#21046;&#21644;&#38754;&#37096;&#36523;&#20221;&#25439;&#22833;&#22686;&#24378;&#20102;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25511;&#21046;&#33021;&#21147;&#21644;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#30446;&#26631;&#36523;&#20221;&#30340;&#19968;&#24352;&#38754;&#37096;&#29031;&#29255;&#26469;&#26174;&#33879;&#38477;&#20302;&#22270;&#20687;&#29983;&#25104;&#30340;&#36164;&#28304;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.05793</link><description>&lt;p&gt;
PhotoVerse: &#26080;&#38656;&#35843;&#21442;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models. (arXiv:2309.05793v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05793
&lt;/p&gt;
&lt;p&gt;
PhotoVerse&#26159;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#20998;&#25903;&#35843;&#33410;&#26426;&#21046;&#21644;&#38754;&#37096;&#36523;&#20221;&#25439;&#22833;&#22686;&#24378;&#20102;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25511;&#21046;&#33021;&#21147;&#21644;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#30446;&#26631;&#36523;&#20221;&#30340;&#19968;&#24352;&#38754;&#37096;&#29031;&#29255;&#26469;&#26174;&#33879;&#38477;&#20302;&#22270;&#20687;&#29983;&#25104;&#30340;&#36164;&#28304;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#19988;&#21463;&#36861;&#25447;&#30340;&#24037;&#20855;&#65292;&#36171;&#20104;&#29992;&#25143;&#26681;&#25454;&#29305;&#23450;&#27010;&#24565;&#21644;&#25552;&#31034;&#21019;&#24314;&#23450;&#21046;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#38271;&#26102;&#38388;&#35843;&#21442;&#12289;&#22823;&#37327;&#23384;&#20648;&#38656;&#27714;&#12289;&#23545;&#27599;&#20010;&#36523;&#20221;&#38656;&#35201;&#22810;&#20010;&#36755;&#20837;&#22270;&#20687;&#30340;&#24517;&#35201;&#24615;&#20197;&#21450;&#22312;&#20445;&#30041;&#36523;&#20221;&#21644;&#21487;&#32534;&#36753;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;PhotoVerse&#65292;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#39046;&#22495;&#20013;&#24341;&#20837;&#20102;&#21452;&#20998;&#25903;&#35843;&#33410;&#26426;&#21046;&#65292;&#20174;&#32780;&#26377;&#25928;&#25511;&#21046;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#38754;&#37096;&#36523;&#20221;&#25439;&#22833;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#36523;&#20221;&#20445;&#30041;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;PhotoVerse&#26041;&#27861;&#26080;&#38656;&#27979;&#35797;&#26102;&#38388;&#35843;&#21442;&#65292;&#20165;&#20381;&#36182;&#30446;&#26631;&#36523;&#20221;&#30340;&#19968;&#24352;&#38754;&#37096;&#29031;&#29255;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#22270;&#20687;&#29983;&#25104;&#25152;&#38656;&#30340;&#36164;&#28304;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized text-to-image generation has emerged as a powerful and sought-after tool, empowering users to create customized images based on their specific concepts and prompts. However, existing approaches to personalization encounter multiple challenges, including long tuning times, large storage requirements, the necessity for multiple input images per identity, and limitations in preserving identity and editability. To address these obstacles, we present PhotoVerse, an innovative methodology that incorporates a dual-branch conditioning mechanism in both text and image domains, providing effective control over the image generation process. Furthermore, we introduce facial identity loss as a novel component to enhance the preservation of identity during training. Remarkably, our proposed PhotoVerse eliminates the need for test time tuning and relies solely on a single facial photo of the target identity, significantly reducing the resource cost associated with image generation. After
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29992;&#25143;&#20013;&#24515;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#22810;&#27169;&#24577;&#20132;&#20114;&#20013;&#33258;&#20027;&#31995;&#32479;&#23545;&#29289;&#20307;&#21644;&#29615;&#22659;&#30340;&#31526;&#21495;&#21270;&#29702;&#35299;&#33021;&#21147;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.05787</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#29992;&#25143;&#20013;&#24515;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#20132;&#20114;&#20013;&#30340;&#24212;&#29992;&#20110;&#33258;&#20027;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems. (arXiv:2309.05787v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29992;&#25143;&#20013;&#24515;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#22810;&#27169;&#24577;&#20132;&#20114;&#20013;&#33258;&#20027;&#31995;&#32479;&#23545;&#29289;&#20307;&#21644;&#29615;&#22659;&#30340;&#31526;&#21495;&#21270;&#29702;&#35299;&#33021;&#21147;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#20197;&#24863;&#30693;&#30340;&#38750;&#31526;&#21495;&#21270;&#26041;&#24335;&#35782;&#21035;&#21644;&#29702;&#35299;&#23545;&#35937;&#21450;&#20854;&#29615;&#22659;&#12290;&#36825;&#20123;&#31995;&#32479;&#29616;&#22312;&#21487;&#20197;&#25191;&#34892;&#29289;&#20307;&#26816;&#27979;&#12289;&#20256;&#24863;&#22120;&#25968;&#25454;&#34701;&#21512;&#21644;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#35201;&#25552;&#21319;&#36825;&#20123;&#31995;&#32479;&#23545;&#23545;&#35937;&#21450;&#20854;&#29615;&#22659;&#30340;&#27010;&#24565;&#21644;&#31526;&#21495;&#29702;&#35299;&#33021;&#21147;&#65292;&#38656;&#35201;&#32508;&#21512;&#32771;&#34385;&#20154;&#31867;&#25552;&#20379;&#30340;&#26174;&#24615;&#25945;&#23548;&#65288;&#20363;&#22914;&#25551;&#36848;&#24773;&#20917;&#25110;&#35299;&#37322;&#22914;&#20309;&#34892;&#21160;&#65289;&#21644;&#36890;&#36807;&#35266;&#23519;&#20154;&#31867;&#34892;&#20026;&#65288;&#36890;&#36807;&#31995;&#32479;&#30340;&#20256;&#24863;&#22120;&#65289;&#33719;&#24471;&#30340;&#38544;&#24615;&#25945;&#23548;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#24517;&#39035;&#35774;&#35745;&#20855;&#26377;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#33021;&#21147;&#65292;&#20197;&#25903;&#25345;&#38544;&#24615;&#21644;&#26174;&#24615;&#20132;&#20114;&#27169;&#22411;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20027;&#24352;&#21516;&#26102;&#32771;&#34385;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65292;&#20197;&#21450;&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#21442;&#19982;&#21644;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#25512;&#36827;&#36825;&#19968;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#27700;&#24179;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning, particularly deep learning, have enabled autonomous systems to perceive and comprehend objects and their environments in a perceptual subsymbolic manner. These systems can now perform object detection, sensor data fusion, and language understanding tasks. However, there is a growing need to enhance these systems to understand objects and their environments more conceptually and symbolically. It is essential to consider both the explicit teaching provided by humans (e.g., describing a situation or explaining how to act) and the implicit teaching obtained by observing human behavior (e.g., through the system's sensors) to achieve this level of powerful artificial intelligence. Thus, the system must be designed with multimodal input and output capabilities to support implicit and explicit interaction models. In this position paper, we argue for considering both types of inputs, as well as human-in-the-loop and incremental learning techniques, for advan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28784;&#30418;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20223;&#30495;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20851;&#20110;&#27963;&#21160;&#30340;&#31354;&#38388;&#20998;&#24067;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#65292;&#24182;&#22312;&#36300;&#20498;&#26816;&#27979;&#12289;&#23460;&#20869;&#23450;&#20301;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#20110;&#40657;&#30418;&#20248;&#21270;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05784</link><description>&lt;p&gt;
&#36741;&#21161;&#29983;&#27963;&#29615;&#22659;&#20013;&#20256;&#24863;&#22120;&#24067;&#32622;&#30340;&#28784;&#30418;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments. (arXiv:2309.05784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28784;&#30418;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20223;&#30495;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20851;&#20110;&#27963;&#21160;&#30340;&#31354;&#38388;&#20998;&#24067;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#65292;&#24182;&#22312;&#36300;&#20498;&#26816;&#27979;&#12289;&#23460;&#20869;&#23450;&#20301;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#20110;&#40657;&#30418;&#20248;&#21270;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#20256;&#24863;&#22120;&#30340;&#37197;&#32622;&#21644;&#24067;&#32622;&#23545;&#20110;&#21487;&#38752;&#30340;&#36300;&#20498;&#26816;&#27979;&#12289;&#23460;&#20869;&#23450;&#20301;&#21644;&#27963;&#21160;&#35782;&#21035;&#22312;&#36741;&#21161;&#29983;&#27963;&#31354;&#38388;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28784;&#30418;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#22522;&#20110;&#20223;&#30495;&#30340;&#35780;&#20272;&#65292;&#22312;&#20219;&#24847;&#23460;&#20869;&#31354;&#38388;&#20013;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#22312;&#20110;&#25429;&#25417;&#20851;&#20110;&#27963;&#21160;&#30340;&#31354;&#38388;&#20998;&#24067;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36845;&#20195;&#26597;&#35810;&#28857;&#36873;&#25321;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20004;&#20010;&#27169;&#25311;&#23460;&#20869;&#29615;&#22659;&#21644;&#21253;&#21547;&#20154;&#31867;&#27963;&#21160;&#21644;&#20256;&#24863;&#22120;&#35302;&#21457;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#39640;&#36136;&#37327;&#20256;&#24863;&#22120;&#24067;&#32622;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#40657;&#30418;&#20248;&#21270;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#65292;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#27963;&#21160;&#35782;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#22823;&#24133;&#24230;&#20943;&#23569;(&#24179;&#22343;&#20943;&#23569;51.3%)&#26114;&#36149;&#30340;&#20989;&#25968;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing the configuration and placement of sensors is crucial for reliable fall detection, indoor localization, and activity recognition in assisted living spaces. We propose a novel, sample-efficient approach to find a high-quality sensor placement in an arbitrary indoor space based on grey-box Bayesian optimization and simulation-based evaluation. Our key technical contribution lies in capturing domain-specific knowledge about the spatial distribution of activities and incorporating it into the iterative selection of query points in Bayesian optimization. Considering two simulated indoor environments and a real-world dataset containing human activities and sensor triggers, we show that our proposed method performs better compared to state-of-the-art black-box optimization techniques in identifying high-quality sensor placements, leading to accurate activity recognition in terms of F1-score, while also requiring a significantly lower (51.3% on average) number of expensive function 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#24378;&#21644;&#21152;&#36895;&#23545;P vs. NP&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#20837;&#24605;&#32771;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;P vs. NP&#38382;&#39064;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;GPT-4&#25104;&#21151;&#20135;&#29983;&#20102;&#35777;&#26126;&#26550;&#26500;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25512;&#29702;&#65292;&#24471;&#20986;&#20102;"P &#8800; NP"&#30340;&#32467;&#35770;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05689</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31185;&#23398;&#30740;&#31350;&#65306;&#23545;P vs. NP&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Science: A Study on P vs. NP. (arXiv:2309.05689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#24378;&#21644;&#21152;&#36895;&#23545;P vs. NP&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#20837;&#24605;&#32771;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;P vs. NP&#38382;&#39064;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;GPT-4&#25104;&#21151;&#20135;&#29983;&#20102;&#35777;&#26126;&#26550;&#26500;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25512;&#29702;&#65292;&#24471;&#20986;&#20102;"P &#8800; NP"&#30340;&#32467;&#35770;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#21644;&#21152;&#36895;&#23545;P vs. NP&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25968;&#23398;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#65288;Socratic reasoning&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;LLMs&#36827;&#34892;&#28145;&#20837;&#24605;&#32771;&#26469;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#12290;&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#40723;&#21169;LLMs&#36882;&#24402;&#22320;&#21457;&#29616;&#12289;&#35299;&#20915;&#21644;&#25972;&#21512;&#38382;&#39064;&#65292;&#21516;&#26102;&#20419;&#36827;&#33258;&#25105;&#35780;&#20272;&#21644;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;P vs. NP&#38382;&#39064;&#30340;&#35797;&#28857;&#30740;&#31350;&#34920;&#26126;&#65292;GPT-4&#22312;97&#20010;&#23545;&#35805;&#22238;&#21512;&#20013;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#19968;&#20010;&#35777;&#26126;&#26550;&#26500;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25512;&#29702;&#65292;&#24471;&#20986;&#20102;&#8220;P &#8800; NP&#8221;&#30340;&#32467;&#35770;&#65292;&#36825;&#19982;&#65288;Xu and Zhou, 2023&#65289;&#30340;&#32467;&#35770;&#19968;&#33268;&#12290;&#35813;&#35843;&#26597;&#22312;LLMs&#30340;&#24191;&#27867;&#35299;&#31354;&#38388;&#20013;&#25581;&#31034;&#20102;&#26032;&#30340;&#27934;&#23519;&#21147;&#65292;&#20026;LLM&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we use large language models (LLMs) to augment and accelerate research on the P versus NP problem, one of the most important open problems in theoretical computer science and mathematics. Specifically, we propose Socratic reasoning, a general framework that promotes in-depth thinking with LLMs for complex problem-solving. Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. Our pilot study on the P vs. NP problem shows that GPT-4 successfully produces a proof schema and engages in rigorous reasoning throughout 97 dialogue turns, concluding "P $\neq$ NP", which is in alignment with (Xu and Zhou, 2023). The investigation uncovers novel insights within the extensive solution space of LLMs, shedding light on LLM for Science.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#23478;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#22312;&#32447;&#36712;&#36857;&#39044;&#27979;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35843;&#25972;&#32593;&#32476;&#23618;&#30340;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#38382;&#39064;&#65292;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26032;&#22330;&#26223;&#30340;&#30693;&#35782;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.05683</link><description>&lt;p&gt;
EANet: &#19987;&#23478;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#22312;&#32447;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EANet: Expert Attention Network for Online Trajectory Prediction. (arXiv:2309.05683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#23478;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#22312;&#32447;&#36712;&#36857;&#39044;&#27979;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35843;&#25972;&#32593;&#32476;&#23618;&#30340;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#38382;&#39064;&#65292;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26032;&#22330;&#26223;&#30340;&#30693;&#35782;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#20027;&#27969;&#30740;&#31350;&#21644;&#22522;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#22312;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#23548;&#33268;&#22312;&#22330;&#26223;&#31361;&#28982;&#21464;&#21270;&#26102;&#39044;&#27979;&#20934;&#30830;&#24230;&#36739;&#20302;&#65292;&#26080;&#27861;&#21450;&#26102;&#21709;&#24212;&#21644;&#26356;&#26032;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#23454;&#26102;&#39044;&#27979;&#24182;&#20351;&#29992;&#25968;&#25454;&#23454;&#20363;&#31435;&#21363;&#26356;&#26032;&#27169;&#22411;&#65288;&#21363;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#65289;&#20173;&#28982;&#26159;&#20010;&#38382;&#39064;&#12290;&#36824;&#38656;&#35201;&#35299;&#20915;&#30001;&#25968;&#25454;&#23454;&#20363;&#27969;&#24341;&#36215;&#30340;&#26799;&#24230;&#29190;&#28856;&#25110;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;&#21463;&#21040;Hedge Propagation&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#19987;&#23478;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#19987;&#23478;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35843;&#25972;&#32593;&#32476;&#23618;&#30340;&#19981;&#21516;&#28145;&#24230;&#30340;&#26435;&#37325;&#65292;&#36991;&#20813;&#30001;&#20110;&#26799;&#24230;&#38382;&#39064;&#23548;&#33268;&#27169;&#22411;&#26356;&#26032;&#32531;&#24930;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26032;&#22330;&#26223;&#30340;&#30693;&#35782;&#20197;&#24674;&#22797;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#30701;&#26399;&#36816;&#21160;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trajectory prediction plays a crucial role in autonomous driving. Existing mainstream research and continuoual learning-based methods all require training on complete datasets, leading to poor prediction accuracy when sudden changes in scenarios occur and failing to promptly respond and update the model. Whether these methods can make a prediction in real-time and use data instances to update the model immediately(i.e., online learning settings) remains a question. The problem of gradient explosion or vanishing caused by data instance streams also needs to be addressed. Inspired by Hedge Propagation algorithm, we propose Expert Attention Network, a complete online learning framework for trajectory prediction. We introduce expert attention, which adjusts the weights of different depths of network layers, avoiding the model updated slowly due to gradient problem and enabling fast learning of new scenario's knowledge to restore prediction accuracy. Furthermore, we propose a short-term mot
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#28304;&#22823;&#20840;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#27861;&#24459;&#12289;&#29983;&#21629;&#31185;&#23398;&#12289;&#26032;&#38395;&#31038;&#20132;&#31561;&#65292;&#20197;&#28385;&#36275;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.05682</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#28304;&#22823;&#20840;
&lt;/p&gt;
&lt;p&gt;
A compendium of data sources for data science, machine learning, and artificial intelligence. (arXiv:2309.05682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05682
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#28304;&#22823;&#20840;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#27861;&#24459;&#12289;&#29983;&#21629;&#31185;&#23398;&#12289;&#26032;&#38395;&#31038;&#20132;&#31561;&#65292;&#20197;&#28385;&#36275;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23548;&#33268;&#23545;&#21487;&#20379;&#36825;&#20123;&#27169;&#22411;&#22788;&#29702;&#30340;&#25968;&#25454;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#34429;&#28982;&#25968;&#25454;&#28304;&#26159;&#24212;&#29992;&#29305;&#23450;&#30340;&#65292;&#26080;&#27861;&#21015;&#20986;&#35814;&#23613;&#26080;&#36951;&#30340;&#25968;&#25454;&#28304;&#21015;&#34920;&#65292;&#20294;&#19968;&#20010;&#20840;&#38754;&#32780;&#19981;&#23436;&#25972;&#30340;&#21015;&#34920;&#20173;&#28982;&#26377;&#21033;&#20110;&#21508;&#32423;&#21035;&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#36825;&#26679;&#19968;&#20010;&#65288;&#24517;&#28982;&#19981;&#23436;&#25972;&#30340;&#65289;&#21015;&#34920;&#65292;&#21363;&#36328;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#25968;&#25454;&#28304;&#22823;&#20840;&#25110;&#27010;&#35272;&#65292;&#21253;&#25324;&#37329;&#34701;&#21644;&#32463;&#27982;&#12289;&#27861;&#24459;&#65288;&#27861;&#24459;&#21644;&#27861;&#35268;&#65289;&#12289;&#29983;&#21629;&#31185;&#23398;&#65288;&#21307;&#23398;&#21644;&#33647;&#29289;&#21457;&#29616;&#65289;&#12289;&#26032;&#38395;&#24773;&#32490;&#21644;&#31038;&#20132;&#23186;&#20307;&#12289;&#38646;&#21806;&#21644;&#30005;&#23376;&#21830;&#21153;&#12289;&#21355;&#26143;&#22270;&#20687;&#20197;&#21450;&#33322;&#36816;&#21644;&#29289;&#27969;&#65292;&#20197;&#28385;&#36275;&#21508;&#31181;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in data science, machine learning, and artificial intelligence, such as the emergence of large language models, are leading to an increasing demand for data that can be processed by such models. While data sources are application-specific, and it is impossible to produce an exhaustive list of such data sources, it seems that a comprehensive, rather than complete, list would still benefit data scientists and machine learning experts of all levels of seniority. The goal of this publication is to provide just such an (inevitably incomplete) list -- or compendium -- of data sources across multiple areas of applications, including finance and economics, legal (laws and regulations), life sciences (medicine and drug discovery), news sentiment and social media, retail and ecommerce, satellite imagery, and shipping and logistics, and sports.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#26469;&#32454;&#21270;&#31185;&#23398;&#20986;&#29256;&#29289;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#23398;&#20064;&#27010;&#29575;&#36923;&#36753;&#27169;&#22411;&#24182;&#20351;&#29992;&#21151;&#33021;&#26799;&#24230;&#25552;&#21319;&#21644;&#20154;&#31867;&#30693;&#35782;&#24341;&#23548;&#26469;&#35782;&#21035;&#20316;&#32773;&#36523;&#20221;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20154;&#31867;&#30693;&#35782;&#22312;&#20316;&#32773;&#36523;&#20221;&#39046;&#22495;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.05681</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#31185;&#23398;&#20986;&#29256;&#29289;&#30693;&#35782;&#22270;&#35889;&#30340;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based Refinement of Scientific Publication Knowledge Graphs. (arXiv:2309.05681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#26469;&#32454;&#21270;&#31185;&#23398;&#20986;&#29256;&#29289;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#23398;&#20064;&#27010;&#29575;&#36923;&#36753;&#27169;&#22411;&#24182;&#20351;&#29992;&#21151;&#33021;&#26799;&#24230;&#25552;&#21319;&#21644;&#20154;&#31867;&#30693;&#35782;&#24341;&#23548;&#26469;&#35782;&#21035;&#20316;&#32773;&#36523;&#20221;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20154;&#31867;&#30693;&#35782;&#22312;&#20316;&#32773;&#36523;&#20221;&#39046;&#22495;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#35782;&#21035;&#20316;&#32773;&#36523;&#20221;&#30340;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#30693;&#35782;&#22270;&#26500;&#24314;&#21644;&#32454;&#21270;&#30340;&#38382;&#39064;&#26469;&#32771;&#34385;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#22312;&#20154;&#31867;&#25351;&#23548;&#65288;&#22522;&#20110;&#30693;&#35782;&#30340;&#23398;&#20064;&#65289;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27010;&#29575;&#36923;&#36753;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21151;&#33021;&#26799;&#24230;&#25552;&#21319;&#23398;&#20064;&#20851;&#31995;&#22238;&#24402;&#26641;&#65292;&#24182;&#36755;&#20986;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#12290;&#20026;&#20102;&#24341;&#20837;&#20154;&#31867;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#19968;&#38454;&#23376;&#21477;&#30340;&#24418;&#24335;&#30340;&#24314;&#35758;&#27880;&#20837;&#21040;&#26641;&#20013;&#36827;&#34892;&#32454;&#21270;&#12290;&#25105;&#20204;&#22312;&#19971;&#20010;&#20316;&#32773;&#36523;&#20221;&#39046;&#22495;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#20154;&#31867;&#30693;&#35782;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of identifying authorship by posing it as a knowledge graph construction and refinement. To this effect, we model this problem as learning a probabilistic logic model in the presence of human guidance (knowledge-based learning). Specifically, we learn relational regression trees using functional gradient boosting that outputs explainable rules. To incorporate human knowledge, advice in the form of first-order clauses is injected to refine the trees. We demonstrate the usefulness of human knowledge both quantitatively and qualitatively in seven authorship domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#24403;&#21069;&#32842;&#22825;&#26426;&#22120;&#20154;&#27979;&#35797;&#30340;&#23454;&#36341;&#21644;&#24320;&#25918;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#21644;&#20943;&#36731;&#19982;&#29992;&#25143;&#20449;&#20219;&#30456;&#20851;&#30340;&#26381;&#21153;&#25110;&#20135;&#21697;&#24615;&#33021;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#20197;&#21450;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24847;&#22806;&#21518;&#26524;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05680</link><description>&lt;p&gt;
&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#20419;&#36827;&#29992;&#25143;&#30340;&#20449;&#20219; - &#23454;&#36341;&#21644;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Evaluating Chatbots to Promote Users' Trust -- Practices and Open Problems. (arXiv:2309.05680v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#24403;&#21069;&#32842;&#22825;&#26426;&#22120;&#20154;&#27979;&#35797;&#30340;&#23454;&#36341;&#21644;&#24320;&#25918;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#21644;&#20943;&#36731;&#19982;&#29992;&#25143;&#20449;&#20219;&#30456;&#20851;&#30340;&#26381;&#21153;&#25110;&#20135;&#21697;&#24615;&#33021;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#20197;&#21450;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24847;&#22806;&#21518;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36719;&#20214;&#65292;&#36890;&#36807;&#19982;&#23427;&#20204;&#33258;&#28982;&#22320;&#20114;&#21160;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#23613;&#31649;&#32842;&#22825;&#26426;&#22120;&#20154;&#20174;&#20154;&#24037;&#26234;&#33021;&#35806;&#29983;&#20043;&#21021;&#23601;&#24050;&#32463;&#34987;&#30740;&#31350;&#65292;&#20294;&#33258;&#20174;&#26131;&#20110;&#20351;&#29992;&#19988;&#36890;&#29992;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22914;ChatGPT&#25512;&#20986;&#20197;&#26469;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#29305;&#21035;&#21560;&#24341;&#20102;&#20844;&#20247;&#21644;&#20225;&#19994;&#30340;&#20851;&#27880;&#12290;&#20225;&#19994;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#19968;&#31181;&#28508;&#22312;&#25216;&#26415;&#26469;&#21560;&#24341;&#29992;&#25143;&#65292;&#36825;&#20123;&#29992;&#25143;&#21487;&#33021;&#26159;&#26368;&#32456;&#28040;&#36153;&#32773;&#12289;&#20379;&#24212;&#21830;&#65292;&#29978;&#33267;&#26159;&#33258;&#24049;&#30340;&#21592;&#24037;&#65292;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#36866;&#24403;&#30340;&#27979;&#35797;&#20197;&#35299;&#20915;&#21644;&#20943;&#36731;&#19982;&#26381;&#21153;&#25110;&#20135;&#21697;&#24615;&#33021;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#20197;&#21450;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24847;&#22806;&#21518;&#26524;&#30456;&#20851;&#30340;&#20449;&#20219;&#38382;&#39064;&#26159;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#24403;&#21069;&#32842;&#22825;&#26426;&#22120;&#20154;&#27979;&#35797;&#30340;&#23454;&#36341;&#65292;&#30830;&#23450;&#20102;&#36861;&#27714;&#29992;&#25143;&#20449;&#20219;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21069;&#36827;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots, the common moniker for collaborative assistants, are Artificial Intelligence (AI) software that enables people to naturally interact with them to get tasks done. Although chatbots have been studied since the dawn of AI, they have particularly caught the imagination of the public and businesses since the launch of easy-to-use and general-purpose Large Language Model-based chatbots like ChatGPT. As businesses look towards chatbots as a potential technology to engage users, who may be end customers, suppliers, or even their own employees, proper testing of chatbots is important to address and mitigate issues of trust related to service or product performance, user satisfaction and long-term unintended consequences for society. This paper reviews current practices for chatbot testing, identifies gaps as open problems in pursuit of user trust, and outlines a path forward.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#36235;&#21183;&#30340;&#27979;&#35797;&#26041;&#27861;&#35780;&#20272;&#20102;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#30340;&#24544;&#35802;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#36235;&#21183;&#27979;&#35797;&#26041;&#27861;&#65292;&#20174;&#23454;&#35777;&#32467;&#26524;&#26469;&#30475;&#65292;&#36825;&#20123;&#26032;&#27979;&#35797;&#26041;&#27861;&#22312;&#22270;&#20687;&#12289;&#33258;&#28982;&#35821;&#35328;&#21644;&#23433;&#20840;&#20219;&#21153;&#20013;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#35299;&#37322;&#27169;&#22411;&#30340;&#24544;&#35802;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.05679</link><description>&lt;p&gt;
&#22806;&#34920;&#20248;&#32654;&#20294;&#32570;&#20047;&#24544;&#35802;&#24230;&#65306;&#36890;&#36807;&#22522;&#20110;&#36235;&#21183;&#30340;&#27979;&#35797;&#29702;&#35299;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing. (arXiv:2309.05679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#36235;&#21183;&#30340;&#27979;&#35797;&#26041;&#27861;&#35780;&#20272;&#20102;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#30340;&#24544;&#35802;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#36235;&#21183;&#27979;&#35797;&#26041;&#27861;&#65292;&#20174;&#23454;&#35777;&#32467;&#26524;&#26469;&#30475;&#65292;&#36825;&#20123;&#26032;&#27979;&#35797;&#26041;&#27861;&#22312;&#22270;&#20687;&#12289;&#33258;&#28982;&#35821;&#35328;&#21644;&#23433;&#20840;&#20219;&#21153;&#20013;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#35299;&#37322;&#27169;&#22411;&#30340;&#24544;&#35802;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25104;&#23601;&#65292;&#20294;&#20154;&#20204;&#20063;&#23545;DL&#27169;&#22411;&#30340;&#20915;&#31574;&#24863;&#21040;&#25285;&#24551;&#65292;&#22240;&#20026;DL&#27169;&#22411;&#30340;&#39640;&#24230;&#38750;&#32447;&#24615;&#20351;&#24471;&#20915;&#31574;&#26497;&#20854;&#38590;&#20197;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#25915;&#20987;&#65288;&#22914;&#23545;&#25239;&#25915;&#20987;&#65289;&#24456;&#23481;&#26131;&#36827;&#34892;&#65292;&#20294;&#24456;&#38590;&#26816;&#27979;&#21644;&#35299;&#37322;&#65292;&#36825;&#23548;&#33268;&#20102;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#30340;&#30740;&#31350;&#28608;&#22686;&#65292;&#20197;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35299;&#37322;&#26041;&#27861;&#30340;&#24544;&#35802;&#24230;&#65292;&#24182;&#21457;&#29616;&#20256;&#32479;&#30340;&#24544;&#35802;&#24230;&#27979;&#35797;&#36935;&#21040;&#20102;&#38543;&#26426;&#20248;&#21183;&#38382;&#39064;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#25928;&#26524;&#26368;&#22909;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22797;&#26434;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#24544;&#35802;&#24230;&#27979;&#35797;&#65292;&#24182;&#20174;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#26032;&#30340;&#36235;&#21183;&#27979;&#35797;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#22270;&#20687;&#12289;&#33258;&#28982;&#35821;&#35328;&#21644;&#23433;&#20840;&#20219;&#21153;&#30340;&#27979;&#35797;&#26356;&#22909;&#22320;&#35780;&#20272;&#24544;&#35802;&#24230;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#35780;&#20272;&#31995;&#32479;&#65292;&#24182;&#35780;&#20272;&#20102;&#21313;&#31181;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While enjoying the great achievements brought by deep learning (DL), people are also worried about the decision made by DL models, since the high degree of non-linearity of DL models makes the decision extremely difficult to understand. Consequently, attacks such as adversarial attacks are easy to carry out, but difficult to detect and explain, which has led to a boom in the research on local explanation methods for explaining model decisions. In this paper, we evaluate the faithfulness of explanation methods and find that traditional tests on faithfulness encounter the random dominance problem, \ie, the random selection performs the best, especially for complex data. To further solve this problem, we propose three trend-based faithfulness tests and empirically demonstrate that the new trend tests can better assess faithfulness than traditional tests on image, natural language and security tasks. We implement the assessment system and evaluate ten popular explanation methods. Benefitin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAPE&#30340;&#36866;&#24212;&#26679;&#26412;&#30340;&#23618;&#27425;&#33647;&#29289;&#39044;&#27979;&#32593;&#32476;&#65292;&#26088;&#22312;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#20013;&#22797;&#26434;&#22810;&#21457;&#30149;&#26465;&#20214;&#19979;&#30340;&#33647;&#29289;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#32039;&#20945;&#30340;&#20869;&#37096;&#30149;&#24739;&#23601;&#35786;&#20107;&#20214;&#20851;&#31995;&#32534;&#30721;&#22120;&#21644;&#32437;&#21521;&#30149;&#21382;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#20934;&#30830;&#30340;&#30149;&#24739;&#34920;&#31034;&#21644;&#32437;&#21521;&#24207;&#21015;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.05675</link><description>&lt;p&gt;
SHAPE&#65306;&#19968;&#31181;&#36866;&#24212;&#26679;&#26412;&#30340;&#23618;&#27425;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
SHAPE: A Sample-adaptive Hierarchical Prediction Network for Medication Recommendation. (arXiv:2309.05675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAPE&#30340;&#36866;&#24212;&#26679;&#26412;&#30340;&#23618;&#27425;&#33647;&#29289;&#39044;&#27979;&#32593;&#32476;&#65292;&#26088;&#22312;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#20013;&#22797;&#26434;&#22810;&#21457;&#30149;&#26465;&#20214;&#19979;&#30340;&#33647;&#29289;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#32039;&#20945;&#30340;&#20869;&#37096;&#30149;&#24739;&#23601;&#35786;&#20107;&#20214;&#20851;&#31995;&#32534;&#30721;&#22120;&#21644;&#32437;&#21521;&#30149;&#21382;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#20934;&#30830;&#30340;&#30149;&#24739;&#34920;&#31034;&#21644;&#32437;&#21521;&#24207;&#21015;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#38024;&#23545;&#22797;&#26434;&#30340;&#22810;&#21457;&#30149;&#26465;&#20214;&#36827;&#34892;&#26377;&#25928;&#30340;&#33647;&#29289;&#25512;&#33616;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#22522;&#20110;&#32437;&#21521;&#35760;&#24405;&#39044;&#27979;&#33647;&#29289;&#65292;&#36825;&#31181;&#26041;&#27861;&#20551;&#35774;&#23398;&#20064;&#32437;&#21521;&#24207;&#21015;&#25968;&#25454;&#30340;&#20449;&#24687;&#20256;&#36755;&#27169;&#24335;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#30149;&#24739;&#22312;&#23601;&#35786;&#26399;&#38388;&#30340;&#21307;&#30103;&#20107;&#20214;&#26159;&#26377;&#24207;&#30340;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#35270;&#20102;&#20197;&#19979;&#26465;&#20214;&#65306;1&#65289;&#24613;&#38656;&#19968;&#31181;&#26356;&#32039;&#20945;&#30340;&#20869;&#37096;&#30149;&#24739;&#23601;&#35786;&#20107;&#20214;&#20851;&#31995;&#32534;&#30721;&#22120;&#65307;2&#65289;&#23398;&#20064;&#30149;&#20154;&#21487;&#21464;&#32437;&#21521;&#24207;&#21015;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#31574;&#30053;&#26159;&#19981;&#21516;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#26679;&#26412;&#30340;&#23618;&#27425;&#33647;&#29289;&#39044;&#27979;&#32593;&#32476;&#65292;&#31216;&#20026;SHAPE&#65292;&#26469;&#35299;&#20915;&#33647;&#29289;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#19978;&#36848;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#20869;&#37096;&#30149;&#24739;&#23601;&#35786;&#20107;&#20214;&#20851;&#31995;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#33719;&#24471;&#23601;&#35786;&#32423;&#21035;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21457;&#23637;&#20102;&#19968;&#20010;&#32437;&#21521;&#30149;&#21382;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;&#24739;&#32773;&#30340;&#32437;&#21521;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively medication recommendation with complex multimorbidity conditions is a critical task in healthcare. Most existing works predicted medications based on longitudinal records, which assumed the information transmitted patterns of learning longitudinal sequence data are stable and intra-visit medical events are serialized. However, the following conditions may have been ignored: 1) A more compact encoder for intra-relationship in the intra-visit medical event is urgent; 2) Strategies for learning accurate representations of the variable longitudinal sequences of patients are different. In this paper, we proposed a novel Sample-adaptive Hierarchical medicAtion Prediction nEtwork, termed SHAPE, to tackle the above challenges in the medication recommendation task. Specifically, we design a compact intra-visit set encoder to encode the relationship in the medical event for obtaining visit-level representation and then develop an inter-visit longitudinal encoder to learn the patient-
&lt;/p&gt;</description></item><item><title>tSPM+&#31639;&#27861;&#26159;&#19968;&#31181;&#39640;&#24615;&#33021;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#27169;&#24335;&#20013;&#21152;&#20837;&#25345;&#32493;&#26102;&#38388;&#32500;&#24230;&#65292;&#25552;&#20379;&#20102;&#39640;&#36895;&#36816;&#34892;&#21644;&#20869;&#23384;&#28040;&#32791;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.05671</link><description>&lt;p&gt;
tSPM+&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20020;&#24202;&#25968;&#25454;&#20013;&#25366;&#25496;&#20256;&#36882;&#39034;&#24207;&#27169;&#24335;&#30340;&#39640;&#24615;&#33021;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
tSPM+; a high-performance algorithm for mining transitive sequential patterns from clinical data. (arXiv:2309.05671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05671
&lt;/p&gt;
&lt;p&gt;
tSPM+&#31639;&#27861;&#26159;&#19968;&#31181;&#39640;&#24615;&#33021;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#27169;&#24335;&#20013;&#21152;&#20837;&#25345;&#32493;&#26102;&#38388;&#32500;&#24230;&#65292;&#25552;&#20379;&#20102;&#39640;&#36895;&#36816;&#34892;&#21644;&#20869;&#23384;&#28040;&#32791;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#20174;&#24739;&#32773;&#37027;&#37324;&#25910;&#38598;&#21040;&#30340;&#22823;&#22411;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#20351;&#24471;&#20351;&#29992;&#19981;&#21516;&#30340;&#20998;&#26512;&#31639;&#27861;&#23545;&#22797;&#26434;&#30142;&#30149;&#36827;&#34892;&#35745;&#31639;&#21270;&#29305;&#24449;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#19968;&#31181;&#20174;&#22823;&#22411;&#20020;&#24202;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#27861;&#26159;&#23558;&#26102;&#38388;&#27169;&#24335;&#25366;&#25496;&#19982;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#25366;&#25496;&#36825;&#20123;&#26102;&#38388;&#27169;&#24335;&#26159;&#19968;&#39033;&#35745;&#31639;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#24182;&#19988;&#20250;&#23545;&#23384;&#20648;&#22120;&#20135;&#29983;&#24433;&#21709;&#12290;&#30446;&#21069;&#30340;&#31639;&#27861;&#65292;&#22914;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#65288;tSPM&#65289;&#31639;&#27861;&#65292;&#24050;&#32463;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#26377;&#20248;&#21270;&#30340;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;tSPM+&#31639;&#27861;&#65292;&#36825;&#26159;tSPM&#31639;&#27861;&#30340;&#19968;&#31181;&#39640;&#24615;&#33021;&#23454;&#29616;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#27169;&#24335;&#20013;&#28155;&#21152;&#25345;&#32493;&#26102;&#38388;&#32500;&#24230;&#65292;&#22686;&#21152;&#20102;&#19968;&#31181;&#26032;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;tSPM+&#31639;&#27861;&#25552;&#20379;&#20102;&#39640;&#36798;980&#20493;&#30340;&#21152;&#36895;&#21644;&#39640;&#36798;48&#20493;&#30340;&#20869;&#23384;&#20351;&#29992;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;R&#21253;&#30340;docker&#23481;&#22120;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#23454;&#39564;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing availability of large clinical datasets collected from patients can enable new avenues for computational characterization of complex diseases using different analytic algorithms. One of the promising new methods for extracting knowledge from large clinical datasets involves temporal pattern mining integrated with machine learning workflows. However, mining these temporal patterns is a computational intensive task and has memory repercussions. Current algorithms, such as the temporal sequence pattern mining (tSPM) algorithm, are already providing promising outcomes, but still leave room for optimization. In this paper, we present the tSPM+ algorithm, a high-performance implementation of the tSPM algorithm, which adds a new dimension by adding the duration to the temporal patterns. We show that the tSPM+ algorithm provides a speed up to factor 980 and a up to 48 fold improvement in memory consumption. Moreover, we present a docker container with an R-package, We also provi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#20013;&#20351;&#29992;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#36896;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#20351;&#29992;CNN/DailyMail&#26032;&#38395;&#25991;&#31456;&#39044;&#35757;&#32451;&#30340;RoBERTa&#21644;&#20351;&#29992;&#30456;&#21516;&#25991;&#31456;&#39044;&#35757;&#32451;&#30340;ChatGPT&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05668</link><description>&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;ChatGPT&#29983;&#25104;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks. (arXiv:2309.05668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#20013;&#20351;&#29992;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#36896;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#20351;&#29992;CNN/DailyMail&#26032;&#38395;&#25991;&#31456;&#39044;&#35757;&#32451;&#30340;RoBERTa&#21644;&#20351;&#29992;&#30456;&#21516;&#25991;&#31456;&#39044;&#35757;&#32451;&#30340;ChatGPT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#20174;&#20114;&#32852;&#32593;&#26723;&#26696;&#20013;&#25552;&#21462;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;LLM&#65292;&#20363;&#22914;ChatGPT&#65292;&#24050;&#24191;&#27867;&#21487;&#29992;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#20026;&#21508;&#31181;&#30446;&#30340;&#29983;&#25104;&#25991;&#26412;&#65292;&#21253;&#25324;&#25991;&#31456;&#12289;&#35770;&#25991;&#12289;&#31505;&#35805;&#21644;&#35799;&#27468;&#12290;&#30001;&#20110;LLM&#26159;&#22312;&#28085;&#30422;Reddit&#21644;Twitter&#31561;&#24179;&#21488;&#30340;&#21508;&#31181;&#25991;&#26412;&#26469;&#28304;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#21487;&#20197;&#39044;&#35265;&#26410;&#26469;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#36824;&#23558;&#21253;&#21547;&#21069;&#20960;&#20010;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#37492;&#20110;&#27492;&#21457;&#23637;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#20013;&#20154;&#36896;&#25991;&#26412;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#20351;&#29992;CNN/DailyMail&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;RoBERTa&#21644;&#19968;&#20010;&#20351;&#29992;&#30456;&#21516;&#25991;&#31456;&#36827;&#34892;&#35757;&#32451;&#30340;ChatGPT&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, significant advancements have been witnessed in the field of language models, particularly with the emergence of Large Language Models (LLMs) that are trained on vast amounts of data extracted from internet archives. These LLMs, such as ChatGPT, have become widely accessible, allowing users to generate text for various purposes including articles, essays, jokes, and poetry. Given that LLMs are trained on a diverse range of text sources, encompassing platforms like Reddit and Twitter, it is foreseeable that future training datasets will also incorporate text generated by previous iterations of the models themselves. In light of this development, our research aims to investigate the influence of artificial text in the pre-training phase of language models. Specifically, we conducted a comparative analysis between a language model, RoBERTa, pre-trained using CNN/DailyMail news articles, and ChatGPT, which employed the same articles for its training and evaluated their per
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#22522;&#20110;&#35270;&#35273;&#30340;&#22810;&#26679;&#21270;&#20844;&#22253;our&#25216;&#33021;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#31616;&#21333;&#30340;&#22870;&#21169;&#32780;&#19981;&#20351;&#29992;&#21442;&#32771;&#21160;&#20316;&#25968;&#25454;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#19981;&#21516;&#30340;&#20844;&#22253;our&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#22235;&#36275;&#26426;&#22120;&#20154;&#20013;&#12290;</title><link>http://arxiv.org/abs/2309.05665</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20844;&#22253;our&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robot Parkour Learning. (arXiv:2309.05665v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#22522;&#20110;&#35270;&#35273;&#30340;&#22810;&#26679;&#21270;&#20844;&#22253;our&#25216;&#33021;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#31616;&#21333;&#30340;&#22870;&#21169;&#32780;&#19981;&#20351;&#29992;&#21442;&#32771;&#21160;&#20316;&#25968;&#25454;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#19981;&#21516;&#30340;&#20844;&#22253;our&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#22235;&#36275;&#26426;&#22120;&#20154;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#22253;our&#26159;&#19968;&#20010;&#23545;&#20110;&#26426;&#22120;&#20154;&#26469;&#35828;&#38656;&#35201;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36805;&#36895;&#20811;&#26381;&#21508;&#31181;&#38556;&#30861;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#21160;&#29289;&#25968;&#25454;&#25110;&#22797;&#26434;&#30340;&#22870;&#21169;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#20294;&#30450;&#30446;&#30340;&#36816;&#21160;&#25216;&#33021;&#25110;&#22522;&#20110;&#35270;&#35273;&#30340;&#29305;&#27530;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#33258;&#20027;&#20844;&#22253;our&#38656;&#35201;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20110;&#35270;&#35273;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21487;&#25512;&#24191;&#25216;&#33021;&#65292;&#20197;&#24863;&#30693;&#21644;&#24212;&#23545;&#21508;&#31181;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#22870;&#21169;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#21442;&#32771;&#21160;&#20316;&#25968;&#25454;&#65292;&#26469;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#12289;&#22522;&#20110;&#35270;&#35273;&#30340;&#20844;&#22253;our&#25216;&#33021;&#30340;&#21333;&#19968;&#31471;&#21040;&#31471;&#31574;&#30053;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21463;&#21040;&#30452;&#25509;&#21327;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20844;&#22253;our&#25216;&#33021;&#65292;&#21253;&#25324;&#25856;&#29228;&#39640;&#38556;&#30861;&#29289;&#12289;&#36328;&#36234;&#22823;&#36317;&#31163;&#38388;&#38553;&#12289;&#29228;&#34892;&#20302;&#22721;&#22418;&#12289;&#31359;&#36234;&#29421;&#31364;&#32541;&#38553;&#21644;&#22868;&#36305;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25216;&#33021;&#25552;&#28860;&#25104;&#19968;&#20010;&#21333;&#19968;&#22522;&#20110;&#35270;&#35273;&#30340;&#20844;&#22253;our&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#65292;&#21033;&#29992;&#20854;&#33258;&#25105;&#20013;&#24515;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#20855;&#26377;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#65292;&#33021;&#22815;&#25552;&#21319;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05557</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32593;&#32476;&#36816;&#32500;&#33021;&#21147;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of NetOps Capability of Pre-Trained Large Language Models. (arXiv:2309.05557v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#20855;&#26377;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#65292;&#33021;&#22815;&#25552;&#21319;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22238;&#31572;&#20154;&#31867;&#35821;&#35328;&#26597;&#35810;&#65292;&#24182;&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#12290;&#30001;&#20110;&#20855;&#22791;&#22823;&#37327;&#24120;&#35782;&#30693;&#35782;&#65292;LLMs&#22312;&#25512;&#29702;&#20934;&#30830;&#24615;&#19978;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#25512;&#29702;&#33021;&#21147;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#20123;&#33021;&#21147;&#21487;&#33021;&#23545;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#26377;&#24040;&#22823;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#21508;&#31181;NetOps&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#36873;&#25321;&#30340;&#20960;&#31181;LLMs&#22312;NetOps&#39046;&#22495;&#30340;&#33021;&#21147;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;&#35780;&#20272;&#38024;&#23545;5732&#20010;&#20851;&#20110;NetOps&#30340;&#38382;&#39064;&#36827;&#34892;&#65292;&#28085;&#30422;&#20102;26&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#36890;&#29992;&#39046;&#22495;LLMs&#65292;&#21253;&#25324;ChatGPT&#12289;LLaMA&#12289;Falcon&#31561;&#12290;&#25105;&#20204;&#36824;&#23545;&#20854;&#20013;&#19968;&#20123;LLMs&#36827;&#34892;&#20102;NetOps&#35821;&#26009;&#24211;&#30340;&#24494;&#35843;&#65292;&#24182;&#35780;&#20272;&#20102;&#32467;&#26524;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#26041;&#27861;&#36981;&#24490;&#24191;&#27867;&#37319;&#29992;&#30340;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can respond to human language queries and have shown powerful potential applications in network operations (NetOps). Thanks to the large amount of commonsense knowledge inherent, LLMs achieve much better inference accuracy than traditional models and emerge with strong abilities in generalization, reasoning, and code generation. These abilities may have a crucial boost to automated and intelligent NetOps. However, it remains under-explored how well LLMs perform in various NetOps tasks. In this work, we make a systematic assessment of the capabilities, strengths, and limitations of selected LLMs in the field of NetOps. The evaluation is conducted on a collection of 5,732 questions about NetOps, encompassing 26 publicly available general-domain LLMs, including ChatGPT, LLaMA, Falcon, etc. We also finetune some of these LLMs with our collected NetOps corpus and evaluate the resulting models. The evaluation method follows the widely adopted benchmarks for gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#21333;&#20010;&#21644;&#32806;&#21512;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#35760;&#24518;&#38459;&#24615;&#65292;&#24182;&#21457;&#29616;&#26368;&#22823;&#21270;&#35760;&#24518;&#38459;&#24615;&#21487;&#20197;&#25552;&#39640;&#20004;&#20010;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#32416;&#32544;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#37327;&#23376;&#30456;&#20851;&#24615;&#19982;&#35760;&#24518;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#19968;&#21457;&#29616;&#22686;&#24378;&#20102;&#23558;&#37327;&#23376;&#24518;&#38459;&#22120;&#24212;&#29992;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#37327;&#23376;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05062</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#26368;&#22823;&#21270;&#21333;&#20010;&#21644;&#32806;&#21512;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#35760;&#24518;&#38459;&#24615;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for maximizing the memristivity of single and coupled quantum memristors. (arXiv:2309.05062v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#21333;&#20010;&#21644;&#32806;&#21512;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#35760;&#24518;&#38459;&#24615;&#65292;&#24182;&#21457;&#29616;&#26368;&#22823;&#21270;&#35760;&#24518;&#38459;&#24615;&#21487;&#20197;&#25552;&#39640;&#20004;&#20010;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#32416;&#32544;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#37327;&#23376;&#30456;&#20851;&#24615;&#19982;&#35760;&#24518;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#19968;&#21457;&#29616;&#22686;&#24378;&#20102;&#23558;&#37327;&#23376;&#24518;&#38459;&#22120;&#24212;&#29992;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#37327;&#23376;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#34920;&#24449;&#21333;&#20010;&#21644;&#32806;&#21512;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#35760;&#24518;&#38459;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#21270;&#35760;&#24518;&#38459;&#24615;&#20250;&#23548;&#33268;&#20004;&#20010;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#32416;&#32544;&#31243;&#24230;&#36739;&#22823;&#65292;&#25581;&#31034;&#20102;&#37327;&#23376;&#30456;&#20851;&#24615;&#19982;&#35760;&#24518;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22686;&#24378;&#20102;&#23558;&#37327;&#23376;&#24518;&#38459;&#22120;&#20316;&#20026;&#31070;&#32463;&#24418;&#24577;&#30340;&#37327;&#23376;&#35745;&#31639;&#30340;&#20851;&#38190;&#32452;&#20214;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose machine learning (ML) methods to characterize the memristive properties of single and coupled quantum memristors. We show that maximizing the memristivity leads to large values in the degree of entanglement of two quantum memristors, unveiling the close relationship between quantum correlations and memory. Our results strengthen the possibility of using quantum memristors as key components of neuromorphic quantum computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#25991;&#26723;&#25688;&#35201;&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20026;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21442;&#32771;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04951</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;&#65306;&#19968;&#39033;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-document Summarization: A Comparative Evaluation. (arXiv:2309.04951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#25991;&#26723;&#25688;&#35201;&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20026;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21442;&#32771;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#35780;&#20272;&#65292;&#20197;&#30830;&#23450;&#26368;&#26032;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;BigSurvey-MDS&#21644;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;PRIMERA&#21644;PEGASUS&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#39046;&#22495;&#30340;&#19981;&#21516;&#32780;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;PRIMERA&#21644;PEGASUS&#12290;&#25105;&#20204;&#20351;&#29992;ROUGE&#20998;&#25968;&#20316;&#20026;&#24615;&#33021;&#24230;&#37327;&#25351;&#26631;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20102;&#35299;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#39046;&#22495;&#20013;&#20934;&#30830;&#12289;&#40065;&#26834;&#30340;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is aimed at evaluating state-of-the-art models for Multi-document Summarization (MDS) on different types of datasets in various domains and investigating the limitations of existing models to determine future research directions. To address this gap, we conducted an extensive literature review to identify state-of-the-art models and datasets. We analyzed the performance of PRIMERA and PEGASUS models on BigSurvey-MDS and MS$^2$ datasets, which posed unique challenges due to their varied domains. Our findings show that the General-Purpose Pre-trained Model LED outperforms PRIMERA and PEGASUS on the MS$^2$ dataset. We used the ROUGE score as a performance metric to evaluate the identified models on different datasets. Our study provides valuable insights into the models' strengths and weaknesses, as well as their applicability in different domains. This work serves as a reference for future MDS research and contributes to the development of accurate and robust models which can 
&lt;/p&gt;</description></item><item><title>MFPNet&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#20998;&#21106;&#26550;&#26500;&#65292;&#21033;&#29992;&#29305;&#27530;&#30340;&#27531;&#24046;&#22359;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#20256;&#25773;&#65292;&#33719;&#24471;&#20102;&#20248;&#36234;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04914</link><description>&lt;p&gt;
MFPNet: &#36731;&#37327;&#32423;&#35821;&#20041;&#20998;&#21106;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#20256;&#25773;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MFPNet: Multi-scale Feature Propagation Nwtwork For Lightweight Semantic Segmentation. (arXiv:2309.04914v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04914
&lt;/p&gt;
&lt;p&gt;
MFPNet&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#20998;&#21106;&#26550;&#26500;&#65292;&#21033;&#29992;&#29305;&#27530;&#30340;&#27531;&#24046;&#22359;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#20256;&#25773;&#65292;&#33719;&#24471;&#20102;&#20248;&#36234;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#20016;&#23500;&#30740;&#31350;&#30456;&#27604;&#65292;&#36731;&#37327;&#32423;&#35821;&#20041;&#20998;&#21106;&#30340;&#36827;&#23637;&#20284;&#20046;&#36827;&#23637;&#36739;&#24930;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32039;&#20945;&#26041;&#27861;&#30001;&#20110;&#32593;&#32476;&#30340;&#27973;&#23618;&#32780;&#23548;&#33268;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#20998;&#21106;&#26550;&#26500;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;&#29305;&#24449;&#20256;&#25773;&#32593;&#32476;&#65288;MFPNet&#65289;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#37319;&#29992;&#23545;&#31216;&#30340;&#27531;&#24046;&#22359;&#65292;&#20854;&#20013;&#21253;&#21547;&#28789;&#27963;&#30340;&#29942;&#39048;&#27531;&#24046;&#27169;&#22359;&#65288;BRM&#65289;&#65292;&#20197;&#25506;&#32034;&#28145;&#24230;&#21644;&#20016;&#23500;&#30340;&#22810;&#23610;&#24230;&#35821;&#20041;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#24314;&#27169;&#28508;&#22312;&#30340;&#38271;&#31243;&#19978;&#19979;&#25991;&#20851;&#31995;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;BRM&#22359;&#20043;&#38388;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#20256;&#25773;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to the abundant research focusing on large-scale models, the progress in lightweight semantic segmentation appears to be advancing at a comparatively slower pace. However, existing compact methods often suffer from limited feature representation capability due to the shallowness of their networks. In this paper, we propose a novel lightweight segmentation architecture, called Multi-scale Feature Propagation Network (MFPNet), to address the dilemma. Specifically, we design a robust Encoder-Decoder structure featuring symmetrical residual blocks that consist of flexible bottleneck residual modules (BRMs) to explore deep and rich muti-scale semantic context. Furthermore, taking benefit from their capacity to model latent long-range contextual relationships, we leverage Graph Convolutional Networks (GCNs) to facilitate multi-scale feature propagation between the BRM blocks. When evaluated on benchmark datasets, our proposed approach shows superior segmentation results.
&lt;/p&gt;</description></item><item><title>TMComposites&#26159;&#19968;&#31181;&#25554;&#25300;&#24335;&#21327;&#20316;&#26041;&#24335;&#65292;&#36890;&#36807;&#29305;&#21270;&#21644;&#35780;&#20272;&#25104;&#21592;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#21487;&#20197;&#22312;&#36739;&#22797;&#26434;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04801</link><description>&lt;p&gt;
TMComposites: &#19987;&#29992;Tsetlin&#26426;&#22120;&#20043;&#38388;&#30340;&#21363;&#25554;&#21363;&#29992;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
TMComposites: Plug-and-Play Collaboration Between Specialized Tsetlin Machines. (arXiv:2309.04801v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04801
&lt;/p&gt;
&lt;p&gt;
TMComposites&#26159;&#19968;&#31181;&#25554;&#25300;&#24335;&#21327;&#20316;&#26041;&#24335;&#65292;&#36890;&#36807;&#29305;&#21270;&#21644;&#35780;&#20272;&#25104;&#21592;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#21487;&#20197;&#22312;&#36739;&#22797;&#26434;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#20174;&#22522;&#20110;&#31639;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#36716;&#21464;&#20026;&#22522;&#20110;&#36923;&#36753;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26681;&#26412;&#24615;&#30340;&#25913;&#21464;&#12290;&#25903;&#25345;&#21367;&#31215;&#65292;TM&#25104;&#21151;&#22320;&#22788;&#29702;&#20687;MNIST&#65292;Fashion-MNIST&#21644;CIFAR-2&#36825;&#26679;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;TM&#22312;&#34920;&#31034;&#26356;&#22797;&#26434;&#20219;&#21153;&#30340;CIFAR-10&#21644;CIFAR-100&#19978;&#38590;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19987;&#29992;TM&#20043;&#38388;&#30340;&#21363;&#25554;&#21363;&#29992;&#21327;&#20316;&#65292;&#31216;&#20026;TM Composites&#12290;&#21327;&#20316;&#20381;&#36182;&#20110;TM&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#29305;&#21270;&#30340;&#33021;&#21147;&#21644;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#35780;&#20272;&#33258;&#36523;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;&#24403;&#22242;&#38431;&#32452;&#21512;&#26102;&#65292;&#26368;&#33258;&#20449;&#30340;TM&#20570;&#20986;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#19981;&#30830;&#23450;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;TM&#32452;&#21512;&#27604;&#20854;&#25104;&#21592;&#26356;&#26377;&#33021;&#21147;&#65292;&#20174;&#20182;&#20204;&#30340;&#19987;&#19994;&#21270;&#20013;&#21463;&#30410;&#12290;&#21327;&#20316;&#30340;&#29305;&#28857;&#26159;&#21363;&#25554;&#21363;&#29992;&#65292;&#21363;&#25104;&#21592;&#21487;&#20197;&#22312;&#20219;&#20309;&#26102;&#38388;&#12289;&#20219;&#20309;&#26041;&#24335;&#19979;&#32452;&#21512;&#65292;&#26080;&#38656;&#24494;&#35843;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19977;&#20010;TM&#19987;&#19994;&#21270;&#65306;&#26799;&#24230;&#30452;&#26041;&#22270;&#65292;&#33258;&#36866;&#24212;&#39640;&#26031;
&lt;/p&gt;
&lt;p&gt;
Tsetlin Machines (TMs) provide a fundamental shift from arithmetic-based to logic-based machine learning. Supporting convolution, they deal successfully with image classification datasets like MNIST, Fashion-MNIST, and CIFAR-2. However, the TM struggles with getting state-of-the-art performance on CIFAR-10 and CIFAR-100, representing more complex tasks. This paper introduces plug-and-play collaboration between specialized TMs, referred to as TM Composites. The collaboration relies on a TM's ability to specialize during learning and to assess its competence during inference. When teaming up, the most confident TMs make the decisions, relieving the uncertain ones. In this manner, a TM Composite becomes more competent than its members, benefiting from their specializations. The collaboration is plug-and-play in that members can be combined in any way, at any time, without fine-tuning. We implement three TM specializations in our empirical evaluation: Histogram of Gradients, Adaptive Gauss
&lt;/p&gt;</description></item><item><title>FIAT&#26159;&#19968;&#31181;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#23436;&#20840;&#24494;&#35843;&#33539;&#24335;&#34701;&#21512;&#30340;&#26032;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#36827;&#34892;&#25351;&#20196;&#21644;&#25512;&#29702;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#27169;&#22411;&#19978;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#65292;&#32463;&#36807;&#22810;&#35821;&#35328;&#20219;&#21153;&#27979;&#35797;&#65292;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.04663</link><description>&lt;p&gt;
FIAT: &#23558;&#23398;&#20064;&#33539;&#24335;&#19982;&#25351;&#20196;&#21152;&#36895;&#35843;&#20248;&#30456;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning. (arXiv:2309.04663v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04663
&lt;/p&gt;
&lt;p&gt;
FIAT&#26159;&#19968;&#31181;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#23436;&#20840;&#24494;&#35843;&#33539;&#24335;&#34701;&#21512;&#30340;&#26032;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#36827;&#34892;&#25351;&#20196;&#21644;&#25512;&#29702;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#27169;&#22411;&#19978;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#65292;&#32463;&#36807;&#22810;&#35821;&#35328;&#20219;&#21153;&#27979;&#35797;&#65292;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23398;&#20064;&#33539;&#24335;&#36890;&#24120;&#20998;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#12290;&#27599;&#31181;&#33539;&#24335;&#37117;&#26377;&#20854;&#33258;&#36523;&#30340;&#21462;&#33293;&#65292;&#36825;&#21462;&#20915;&#20110;&#21487;&#29992;&#25968;&#25454;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35745;&#31639;&#25104;&#26412;&#12289;&#26131;&#29992;&#24615;&#21644;&#26368;&#32456;&#36136;&#37327;&#65292;&#20294;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20197;&#24378;&#35843;&#23427;&#20204;&#20043;&#38388;&#33258;&#28982;&#32852;&#31995;&#30340;&#26041;&#24335;&#25551;&#36848;&#20102;ICL&#21644;&#24494;&#35843;&#33539;&#24335;&#12290;&#22522;&#20110;&#36825;&#20123;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FIAT&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#36825;&#20123;&#33539;&#24335;&#30340;&#20248;&#28857;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20351;&#24471;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#24037;&#31243;&#25351;&#20196;&#21644;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#21516;&#26102;&#22312;&#21442;&#25968;&#25928;&#29575;&#35843;&#20248;&#30340;&#36739;&#23567;&#27169;&#22411;&#19978;&#20351;&#29992;&#31867;&#20284;&#30340;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;FIAT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;FIAT&#22312;100-10,000&#20010;&#35757;&#32451;&#26679;&#26412;&#35268;&#27169;&#19979;&#22343;&#27604;ICL&#21644;&#24494;&#35843;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24076;&#26395;FIAT&#33021;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning paradigms for large language models (LLMs) currently tend to fall within either in-context learning (ICL) or full fine-tuning. Each of these comes with their own trade-offs based on available data, model size, compute cost, ease-of-use, and final quality with neither solution performing well across-the-board. In this article, we first describe ICL and fine-tuning paradigms in a way that highlights their natural connections. Based on these connections, we propose a new learning paradigm called FIAT that fuses the best of these paradigms together, enabling prompt-engineered instructions and chain-of-thought reasoning with the very largest models while also using similar methods to perform parameter updates on a modestly-sized LLM with parameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of multilingual tasks and observe that FIAT performs better than both ICL and fine-tuning at scales ranging from 100-10,000 training examples. We hope that FIAT provides a pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#29289;&#29702;&#20449;&#24687;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#21644;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#26469;&#33719;&#21462;&#26368;&#36866;&#24403;&#30340;&#21453;&#23545;&#35282;&#39033;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2309.04434</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26368;&#20248;&#21453;&#23545;&#35282;&#37327;&#23376;&#35745;&#31639;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation. (arXiv:2309.04434v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#29289;&#29702;&#20449;&#24687;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#21644;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#26469;&#33719;&#21462;&#26368;&#36866;&#24403;&#30340;&#21453;&#23545;&#35282;&#39033;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#20248;&#21183;&#26469;&#35299;&#20915;&#30001;$N_{Q}$&#27604;&#29305;&#31995;&#32479;&#32452;&#25104;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#30340;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#31934;&#30830;&#22320;&#35299;&#20915;&#37327;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#29289;&#29702;&#21487;&#35266;&#27979;&#37327;&#30340;&#26102;&#38388;&#28436;&#21270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#24517;&#35201;&#30340;&#29289;&#29702;&#20449;&#24687;&#23884;&#20837;&#21040;&#24213;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23545;&#25152;&#26377;&#29289;&#29702;&#21487;&#35266;&#27979;&#37327;&#26045;&#21152;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#65292;&#20445;&#35777;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26368;&#36866;&#24403;&#21453;&#23545;&#35282;&#39033;&#30340;&#33719;&#21462;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#36873;&#25321;&#26469;&#35299;&#20915;CD&#39537;&#21160;&#38382;&#39064;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel methodology that leverages the strength of Physics-Informed Neural Networks (PINNs) to address the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits. The primary objective is to utilize physics-inspired deep learning techniques to accurately solve the time evolution of the different physical observables within the quantum system. To accomplish this objective, we embed the necessary physical information into an underlying neural network to effectively tackle the problem. In particular, we impose the hermiticity condition on all physical observables and make use of the principle of least action, guaranteeing the acquisition of the most appropriate counterdiabatic terms based on the underlying physics. The proposed approach offers a dependable alternative to address the CD driving problem, free from the constraints typically encountered in previous methodologies relying on classical numerical approximations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04296</link><description>&lt;p&gt;
&#22312;COVID-19&#26399;&#38388;&#23548;&#33322;&#19981;&#22312;&#20998;&#24067;&#33539;&#22260;&#20869;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65306;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility. (arXiv:2309.04296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#25968;&#25454;&#20998;&#24067;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#38750;&#20998;&#24067;&#26399;&#38388;&#26102;&#65292;&#22914;COVID-19&#30340;&#23553;&#38145;&#26399;&#65292;&#25968;&#25454;&#20998;&#24067;&#19982;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#35265;&#30340;&#26126;&#26174;&#20559;&#31163;&#12290;&#26412;&#25991;&#37319;&#29992;&#21452;&#37325;&#31574;&#30053;&#65306;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#26356;&#26032;&#27169;&#22411;&#30340;&#26032;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#22312;&#24314;&#31569;&#29289;&#22806;&#37096;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#34892;&#20154;&#35745;&#25968;&#22120;&#25910;&#38598;&#30340;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#12290;&#19982;&#22312;&#32447;&#23398;&#20064;&#30456;&#27604;&#65292;&#21518;&#32773;&#24120;&#24120;&#20250;&#36973;&#21463;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#26032;&#33719;&#24471;&#30340;&#30693;&#35782;&#24120;&#24120;&#20250;&#25273;&#21435;&#20808;&#21069;&#30340;&#20449;&#24687;&#65292;&#25345;&#32493;&#23398;&#20064;&#21017;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#20307;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23558;FSNet&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#22696;&#23572;&#26412;&#24066;13&#20010;&#24314;&#31569;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEX&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04213</link><description>&lt;p&gt;
UQ&#22312;#SMM4H 2023&#19978;&#30340;&#35770;&#25991;&#65306;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#36827;&#34892;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#30340;ALEX&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media. (arXiv:2309.04213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEX&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#19982;&#20844;&#20849;&#21355;&#29983;&#30456;&#20851;&#30340;&#27963;&#21160;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#30446;&#21069;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#25216;&#26415;&#28041;&#21450;&#21040;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#22914;BERT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#20026;&#20844;&#20849;&#21355;&#29983;&#22495;&#35757;&#32451;LLMs&#30340;&#25104;&#26412;&#23588;&#20854;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#20013;&#36825;&#31181;&#22495;&#20869;&#25968;&#25454;&#38598;&#24448;&#24448;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#35774;&#32622;&#27169;&#22411;&#30340;&#24341;&#23548;&#26041;&#24335;&#26377;&#25928;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ALEX&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;ALEX&#27169;&#22411;&#22312;Social Media Mining for Health 2023 &#65288;SMM4H&#65289;&#30340;&#20219;&#21153;2&#21644;&#20219;&#21153;4&#20013;&#33719;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;&#20219;&#21153;1&#20013;&#24471;&#21040;&#20102;&#36739;&#39640;&#30340;&#35780;&#20998;[1]&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312; https:/ /github &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media becomes increasingly popular, more and more activities related to public health emerge. Current techniques for public health analysis involve popular models such as BERT and large language models (LLMs). However, the costs of training in-domain LLMs for public health are especially expensive. Furthermore, such kinds of in-domain datasets from social media are generally imbalanced. To tackle these challenges, the data imbalance issue can be overcome by data augmentation and balanced training. Moreover, the ability of the LLMs can be effectively utilized by prompting the model properly. In this paper, a novel ALEX framework is proposed to improve the performance of public health analysis on social media by adopting an LLMs explanation mechanism. Results show that our ALEX model got the best performance among all submissions in both Task 2 and Task 4 with a high score in Task 1 in Social Media Mining for Health 2023 (SMM4H)[1]. Our code has been released at https:// github
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03241</link><description>&lt;p&gt;
GPT&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25191;&#34892;&#31639;&#26415;&#36816;&#31639;&#65292;&#29305;&#21035;&#26159;&#36229;&#36807;8&#20301;&#25968;&#23383;&#30340;&#20056;&#27861;&#65292;&#20197;&#21450;&#28041;&#21450;&#23567;&#25968;&#21644;&#20998;&#25968;&#30340;&#36816;&#31639;&#12290;&#26412;&#25991;&#26088;&#22312;&#25361;&#25112;&#36825;&#31181;&#35823;&#35299;&#12290;&#36890;&#36807;&#20805;&#20998;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19968;&#20010;&#25317;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20197;&#36817;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#32780;&#19988;&#27809;&#26377;&#25968;&#25454;&#27844;&#38706;&#65292;&#26174;&#33879;&#36229;&#36807;&#20102;GPT-4&#65288;&#20854;&#22810;&#20301;&#25968;&#20056;&#27861;&#20934;&#30830;&#29575;&#20165;&#20026;4.3%&#65289;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;MathGLM&#65292;&#23427;&#26159;&#36890;&#36807;&#22312;&#21253;&#21547;&#20102;&#25991;&#26412;&#25551;&#36848;&#30340;&#38468;&#21152;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#20174;GLM-10B&#24494;&#35843;&#32780;&#25104;&#30340;&#65292;&#23427;&#22312;&#19968;&#20010;&#21253;&#21547;5000&#20010;&#26679;&#26412;&#30340;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#27979;&#35797;&#38598;&#19978;&#30340;&#34920;&#29616;&#19982;GPT-4&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of &gt;8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#26631;&#35760;&#19981;&#36275;&#12289;POI&#38388;&#26102;&#31354;&#20381;&#36182;&#24615;&#22797;&#26434;&#21644;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30456;&#20851;&#24615;&#22810;&#26679;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03239</link><description>&lt;p&gt;
POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference. (arXiv:2309.03239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#26631;&#35760;&#19981;&#36275;&#12289;POI&#38388;&#26102;&#31354;&#20381;&#36182;&#24615;&#22797;&#26434;&#21644;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30456;&#20851;&#24615;&#22810;&#26679;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#33719;&#21462;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#30340;&#20154;&#32676;&#27969;&#37327;&#23545;&#20110;&#26377;&#25928;&#30340;&#20132;&#36890;&#31649;&#29702;&#12289;&#20844;&#20849;&#26381;&#21153;&#21644;&#22478;&#24066;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22914;&#27492;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#22478;&#24066;&#24863;&#30693;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#20197;&#30417;&#27979;&#27599;&#20010;POI&#30340;&#20154;&#32676;&#27969;&#21160;&#12290;&#36825;&#20351;&#24471;&#20174;&#20302;&#36136;&#37327;&#25968;&#25454;&#20013;&#25512;&#26029;&#20934;&#30830;&#30340;&#20154;&#32676;&#27969;&#37327;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#19968;&#22797;&#26434;&#24615;&#20027;&#35201;&#30001;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#24341;&#36215;&#65306;1&#65289;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#32597;&#35265;&#24615;&#65307;2&#65289;POI&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65307;3&#65289;&#31934;&#30830;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30340;&#20247;&#22810;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#20154;&#32676;&#27969;&#25512;&#26029;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#25968;&#25454;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;model&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#26500;&#24314;&#19968;&#20010;&#31354;&#38388;&#22270;&#24320;&#22987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad correlations between precise crowd flow and GPS reports}.  To address these challenges, we recast the crowd flow inference problem as a self-supervised attributed graph representation learning task and introduce a novel \underline{C}ontrastive \underline{S}elf-learning framework for \underline{S}patio-\underline{T}emporal data (\model). Our approach initiates with the construction of a spati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.03224</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#20381;&#28982;&#33021;&#33719;&#30410;&#65306;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03224
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32972;&#26223;&#23398;&#20064;&#33021;&#21147;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36807;&#31243;&#30417;&#30563;&#65292;&#23558;PLMs&#24212;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#36890;&#24120;&#26080;&#27861;&#29983;&#25104;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#21644;&#26368;&#32456;&#31572;&#26696;&#65292;&#21363;&#20351;&#35299;&#20915;&#26041;&#26696;&#27010;&#29575;&#24456;&#39640;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#24494;&#35843;&#30340;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#21644;&#36731;&#37327;&#32423;&#33021;&#37327;&#20989;&#25968;&#20026;LLMs&#36171;&#20104;&#21363;&#26102;&#21453;&#24212;&#21644;&#31934;&#32454;&#25512;&#29702;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#24494;&#35843;&#30340;LLMs&#37325;&#26032;&#23450;&#20041;&#20026;&#22522;&#20110;&#27531;&#24046;&#30340;&#33021;&#37327;&#27169;&#22411;&#65288;Residual-EBM&#65289;&#65292;&#24182;&#24212;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#26469;&#20272;&#35745;&#33021;&#37327;&#20989;&#25968;&#30340;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#33021;&#37327;&#20989;&#25968;&#30340;MCTS&#20316;&#20026;&#36335;&#24452;&#39564;&#35777;&#22120;&#26469;&#25628;&#32034;&#36755;&#20986;&#31354;&#38388;&#24182;&#35780;&#20272;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit impressive language understanding and in-context learning abilities including natural language processing (NLP) tasks and challenging mathematical reasoning. However, due to the lack of process-supervision, applying PLMs to mathematical reasoning tasks often fail to generate correct reasoning steps and final answer even though solutions have high probabilities. To unleash the mathematical reasoning of finetuned-LLMs without any further fineutuning steps, we propose a method to endow LLMs with immediate reaction and delicate reasoning system via Monte Carlo Tree Search(MCTS) and a light energy function to rank the decision steps. In particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy Model~(Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function . Then we use MCTS with energy function as path verifier to search the output space and evaluating the reasoning path. Through extensive 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363; Stylebook&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20174;&#30446;&#26631;&#35821;&#38899;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#39640;&#25928;&#22320;&#36716;&#31227;&#21040;&#28304;&#35821;&#38899;&#20869;&#23481;&#19978;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#25110;&#35828;&#35805;&#32773;&#26631;&#35760;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#26679;&#24335;&#25163;&#20876;&#65292;&#21487;&#20197;&#23454;&#29616;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#24544;&#23454;&#22797;&#21046;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2309.02730</link><description>&lt;p&gt;
Stylebook: &#22312;&#21482;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#30340;&#20219;&#24847;-&#20219;&#24847;&#35821;&#38899;&#36716;&#25442;&#20013;&#36827;&#34892;&#20381;&#36182;&#20869;&#23481;&#30340;&#35828;&#35805;&#39118;&#26684;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Stylebook: Content-Dependent Speaking Style Modeling for Any-to-Any Voice Conversion using Only Speech Data. (arXiv:2309.02730v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363; Stylebook&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20174;&#30446;&#26631;&#35821;&#38899;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#39640;&#25928;&#22320;&#36716;&#31227;&#21040;&#28304;&#35821;&#38899;&#20869;&#23481;&#19978;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#25110;&#35828;&#35805;&#32773;&#26631;&#35760;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#26679;&#24335;&#25163;&#20876;&#65292;&#21487;&#20197;&#23454;&#29616;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#24544;&#23454;&#22797;&#21046;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#20219;&#24847;-&#20219;&#24847;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#25104;&#21151;&#22320;&#23558;&#19968;&#20123;&#30446;&#26631;&#35821;&#38899;&#30340;&#39118;&#26684;&#20449;&#24687;&#36716;&#31227;&#21040;&#36716;&#25442;&#30340;&#35821;&#38899;&#20013;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#24544;&#23454;&#22320;&#22797;&#21046;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#35828;&#35805;&#39118;&#26684;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30446;&#26631;&#35821;&#38899;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#39640;&#25928;&#22320;&#36716;&#31227;&#21040;&#28304;&#35821;&#38899;&#20869;&#23481;&#19978;&#65292;&#32780;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#25110;&#35828;&#35805;&#32773;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27169;&#22411;&#25910;&#38598;&#19982;&#19981;&#21516;&#38899;&#32032;&#20869;&#23481;&#30456;&#23545;&#24212;&#30340;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#35828;&#35805;&#39118;&#26684;&#12290;&#36825;&#20123;&#39118;&#26684;&#29992;&#19968;&#32452;&#31216;&#20026;&#26679;&#24335;&#25163;&#20876;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#19979;&#19968;&#27493;&#20013;&#65292;&#26679;&#24335;&#25163;&#20876;&#19982;&#28304;&#35821;&#38899;&#30340;&#38899;&#32032;&#20869;&#23481;&#19968;&#36215;&#21442;&#19982;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#28304;&#20869;&#23481;&#30340;&#26368;&#32456;&#30446;&#26631;&#39118;&#26684;&#12290;&#26368;&#21518;&#65292;&#20174;&#28304;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20869;&#23481;&#20449;&#24687;&#21644;&#20381;&#36182;&#20869;&#23481;&#30340;&#30446;&#26631;&#39118;&#26684;&#23884;&#20837;&#34987;&#36755;&#20837;&#21040;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
While many recent any-to-any voice conversion models succeed in transferring some target speech's style information to the converted speech, they still lack the ability to faithfully reproduce the speaking style of the target speaker. In this work, we propose a novel method to extract rich style information from target utterances and to efficiently transfer it to source speech content without requiring text transcriptions or speaker labeling. Our proposed approach introduces an attention mechanism utilizing a self-supervised learning (SSL) model to collect the speaking styles of a target speaker each corresponding to the different phonetic content. The styles are represented with a set of embeddings called stylebook. In the next step, the stylebook is attended with the source speech's phonetic content to determine the final target style for each source content. Finally, content information extracted from the source speech and content-dependent target style embeddings are fed into a dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BEVTrack&#30340;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#22522;&#32447;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#65292;BEVTrack&#33021;&#22815;&#32534;&#30721;&#31354;&#38388;&#37051;&#36817;&#24615;&#21644;&#25429;&#25417;&#36816;&#21160;&#32447;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36319;&#36394;&#12290;</title><link>http://arxiv.org/abs/2309.02185</link><description>&lt;p&gt;
BEVTrack&#65306;&#19968;&#31181;&#38024;&#23545;&#40479;&#30640;&#22270;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#31616;&#21333;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
BEVTrack: A Simple Baseline for 3D Single Object Tracking in Birds's-Eye-View. (arXiv:2309.02185v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BEVTrack&#30340;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#22522;&#32447;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#65292;BEVTrack&#33021;&#22815;&#32534;&#30721;&#31354;&#38388;&#37051;&#36817;&#24615;&#21644;&#25429;&#25417;&#36816;&#21160;&#32447;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22806;&#35266;&#21464;&#21270;&#12289;&#24178;&#25200;&#29289;&#21644;&#28857;&#20113;&#30340;&#39640;&#31232;&#30095;&#24615;&#65292;&#28857;&#20113;&#20013;&#30340;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#29289;&#20307;&#36890;&#24120;&#22312;&#36830;&#32493;&#24103;&#20013;&#20445;&#25345;&#31354;&#38388;&#37051;&#36817;&#24615;&#65292;&#20027;&#35201;&#27700;&#24179;&#31227;&#21160;&#12290;&#36825;&#31181;&#31354;&#38388;&#36830;&#32493;&#24615;&#20026;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36319;&#36394;&#22120;&#36890;&#24120;&#37319;&#29992;&#28857;&#32423;&#34920;&#31034;&#65292;&#38590;&#20197;&#26377;&#25928;&#21033;&#29992;&#36825;&#31181;&#30693;&#35782;&#65292;&#22240;&#20026;&#36825;&#31181;&#34920;&#31034;&#30340;&#19981;&#35268;&#21017;&#26684;&#24335;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#24182;&#35299;&#20915;&#22810;&#20010;&#23376;&#20219;&#21153;&#26469;&#24314;&#31435;&#31354;&#38388;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;BEVTrack&#65292;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#29992;&#20110;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#22522;&#32447;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#36830;&#32493;&#30340;&#28857;&#20113;&#36716;&#25442;&#20026;&#24120;&#35265;&#30340;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;BEVTrack&#36890;&#36807;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#31354;&#38388;&#37051;&#36817;&#24615;&#65292;&#24182;&#28789;&#27963;&#22320;&#25429;&#25417;&#20102;&#36319;&#36394;&#30340;&#36816;&#21160;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D single object tracking (SOT) in point clouds is still a challenging problem due to appearance variation, distractors, and high sparsity of point clouds. Notably, in autonomous driving scenarios, the target object typically maintains spatial adjacency across consecutive frames, predominantly moving horizontally. This spatial continuity offers valuable prior knowledge for target localization. However, existing trackers, which often employ point-wise representations, struggle to efficiently utilize this knowledge owing to the irregular format of such representations. Consequently, they require elaborate designs and solving multiple subtasks to establish spatial correspondence. In this paper, we introduce BEVTrack, a simple yet strong baseline framework for 3D SOT. After converting consecutive point clouds into the common Bird's-Eye-View representation, BEVTrack inherently encodes spatial proximity and adeptly captures motion cues for tracking via a simple element-wise operation and con
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#36807;&#31243;&#27169;&#22411;&#65288;LPM&#65289;&#32467;&#21512;&#20102;&#22823;&#35268;&#27169;&#20449;&#24687;&#35821;&#26009;&#24211;&#21644;&#22522;&#20110;&#30693;&#35782;&#31995;&#32479;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#26088;&#22312;&#20026;&#32452;&#32455;&#25552;&#20379;&#36807;&#31243;&#24314;&#35758;&#21644;&#20248;&#21270;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.00900</link><description>&lt;p&gt;
&#22823;&#22411;&#36807;&#31243;&#27169;&#22411;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Process Models: Business Process Management in the Age of Generative AI. (arXiv:2309.00900v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00900
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#36807;&#31243;&#27169;&#22411;&#65288;LPM&#65289;&#32467;&#21512;&#20102;&#22823;&#35268;&#27169;&#20449;&#24687;&#35821;&#26009;&#24211;&#21644;&#22522;&#20110;&#30693;&#35782;&#31995;&#32479;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#26088;&#22312;&#20026;&#32452;&#32455;&#25552;&#20379;&#36807;&#31243;&#24314;&#35758;&#21644;&#20248;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20854;&#20182;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#25345;&#32493;&#25104;&#21151;&#31361;&#26174;&#20102;&#22823;&#35268;&#27169;&#20449;&#24687;&#35821;&#26009;&#24211;&#30456;&#23545;&#20110;&#20005;&#26684;&#23450;&#20041;&#30340;&#31526;&#21495;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20294;&#20063;&#35777;&#26126;&#20102;&#32431;&#32479;&#35745;&#26041;&#27861;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#23545;LLMs&#21644;&#20854;&#20182;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#25216;&#26415;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#36827;&#34892;&#26694;&#26550;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#22411;&#36807;&#31243;&#27169;&#22411;&#65288;LPM&#65289;&#30340;&#27010;&#24565;&#65292;&#23427;&#23558;LLMs&#30340;&#30456;&#20851;&#24615;&#33021;&#21147;&#19982;&#22522;&#20110;&#30693;&#35782;&#31995;&#32479;&#21644;&#33258;&#21160;&#25512;&#29702;&#26041;&#27861;&#30340;&#20998;&#26512;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#30456;&#32467;&#21512;&#12290;LPMs&#34987;&#35774;&#24819;&#20026;&#30452;&#25509;&#21033;&#29992;&#19987;&#23478;&#31215;&#32047;&#30340;&#36807;&#31243;&#31649;&#29702;&#32463;&#39564;&#21644;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#65288;&#20363;&#22914;&#35268;&#27169;&#12289;&#22320;&#21306;&#25110;&#34892;&#19994;&#65289;&#30340;&#32452;&#32455;&#30340;&#36807;&#31243;&#32489;&#25928;&#25968;&#25454;&#12290;&#22312;&#36825;&#20010;&#24895;&#26223;&#20013;&#65292;&#25552;&#20986;&#30340;LPM&#23558;&#20801;&#35768;&#32452;&#32455;&#25509;&#25910;&#21040;&#23545;&#20854;&#29305;&#24449;&#36827;&#34892;&#20102;&#24314;&#27169;&#30340;&#36807;&#31243;&#24314;&#35758;&#21644;&#20248;&#21270;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
The continued success of Large Language Models (LLMs) and other generative artificial intelligence approaches highlights the advantages that large information corpora can have over rigidly defined symbolic models, but also serves as a proof-point of the challenges that purely statistics-based approaches have in terms of safety and trustworthiness. As a framework for contextualizing the potential, as well as the limitations of LLMs and other foundation model-based technologies, we propose the concept of a Large Process Model (LPM) that combines the correlation power of LLMs with the analytical precision and reliability of knowledge-based systems and automated reasoning approaches. LPMs are envisioned to directly utilize the wealth of process management experience that experts have accumulated, as well as process performance data of organizations with diverse characteristics, e.g., regarding size, region, or industry. In this vision, the proposed LPM would allow organizations to receive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38754;&#21521;Fon&#35821;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24615;&#26631;&#27880;&#20219;&#21153;&#19978;&#20849;&#20139;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#22312;Fon&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14280</link><description>&lt;p&gt;
FonMTL:&#38754;&#21521;Fon&#35821;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FonMTL: Towards Multitask Learning for the Fon Language. (arXiv:2308.14280v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38754;&#21521;Fon&#35821;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24615;&#26631;&#27880;&#20219;&#21153;&#19978;&#20849;&#20139;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#22312;Fon&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fon&#35821;&#26159;&#19968;&#31181;&#30495;&#27491;&#30340;&#20302;&#36164;&#28304;&#38750;&#27954;&#35821;&#35328;&#65292;&#22823;&#32422;&#26377;200&#19975;&#20154;&#21475;&#65292;&#20854;&#22312;&#32447;&#23384;&#22312;&#26377;&#38480;&#65292;&#24182;&#19988;&#29616;&#26377;&#25968;&#25454;&#38598;&#20063;&#24456;&#26377;&#38480;&#12290;&#22810;&#20219;&#21153;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#38388;&#20849;&#20139;&#30693;&#35782;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#22330;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#25506;&#32034;&#24615;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;Fon&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#27169;&#22411;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;Fon&#35821;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#26469;&#26500;&#24314;&#36755;&#20837;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#23618;&#22359;&#36827;&#34892;&#27599;&#20010;&#20219;&#21153;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;Fon&#35821;&#30340;NER&#21644;POS&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#19982;&#20960;&#20010;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24615;&#33021;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65288;&#25110;&#26356;&#22909;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fon language, spoken by an average 2 million of people, is a truly low-resourced African language, with a limited online presence, and existing datasets (just to name but a few). Multitask learning is a learning paradigm that aims to improve the generalization capacity of a model by sharing knowledge across different but related tasks: this could be prevalent in very data-scarce scenarios. In this paper, we present the first explorative approach to multitask learning, for model capabilities enhancement in Natural Language Processing for the Fon language. Specifically, we explore the tasks of Named Entity Recognition (NER) and Part of Speech Tagging (POS) for Fon. We leverage two language model heads as encoders to build shared representations for the inputs, and we use linear layers blocks for classification relative to each task. Our results on the NER and POS tasks for Fon, show competitive (or better) performances compared to several multilingual pretrained language models finet
&lt;/p&gt;</description></item><item><title>Infinitia&#26159;&#19968;&#20010;&#27169;&#25311;&#28216;&#25103;&#31995;&#32479;&#65292;&#20351;&#29992;&#29983;&#25104;&#22270;&#20687;&#21644;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#29609;&#23478;&#30340;&#25551;&#36848;&#22609;&#36896;&#28216;&#25103;&#22330;&#26223;&#21644;NPC&#65292;&#31867;&#20284;&#20110;&#20840;&#24687;&#33329;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38480;&#29983;&#25104;&#30340;&#24187;&#24819;&#19990;&#30028;&#12289;&#21487;&#25511;&#30340;NPC&#34892;&#20026;&#12289;&#24189;&#40664;&#23545;&#35805;&#12289;&#25104;&#26412;&#21644;&#26102;&#38388;&#25928;&#29575;&#12289;&#29609;&#23478;&#21512;&#20316;&#20197;&#21450;&#28216;&#25103;&#20869;&#20107;&#20214;&#30340;&#38750;&#30830;&#23450;&#24615;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.13548</link><description>&lt;p&gt;
&#26397;&#30528;&#20840;&#24687;&#33329;&#24335;&#27169;&#25311;&#28216;&#25103;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards a Holodeck-style Simulation Game. (arXiv:2308.13548v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13548
&lt;/p&gt;
&lt;p&gt;
Infinitia&#26159;&#19968;&#20010;&#27169;&#25311;&#28216;&#25103;&#31995;&#32479;&#65292;&#20351;&#29992;&#29983;&#25104;&#22270;&#20687;&#21644;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#29609;&#23478;&#30340;&#25551;&#36848;&#22609;&#36896;&#28216;&#25103;&#22330;&#26223;&#21644;NPC&#65292;&#31867;&#20284;&#20110;&#20840;&#24687;&#33329;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38480;&#29983;&#25104;&#30340;&#24187;&#24819;&#19990;&#30028;&#12289;&#21487;&#25511;&#30340;NPC&#34892;&#20026;&#12289;&#24189;&#40664;&#23545;&#35805;&#12289;&#25104;&#26412;&#21644;&#26102;&#38388;&#25928;&#29575;&#12289;&#29609;&#23478;&#21512;&#20316;&#20197;&#21450;&#28216;&#25103;&#20869;&#20107;&#20214;&#30340;&#38750;&#30830;&#23450;&#24615;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Infinitia&#65292;&#19968;&#20010;&#27169;&#25311;&#28216;&#25103;&#31995;&#32479;&#65292;&#22312;&#28216;&#25103;&#26102;&#38388;&#20869;&#20351;&#29992;&#29983;&#25104;&#22270;&#20687;&#21644;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#29609;&#23478;&#30340;&#31616;&#30701;&#25551;&#36848;&#37325;&#26032;&#22609;&#36896;&#28216;&#25103;&#22330;&#26223;&#21644;NPC&#65292;&#31867;&#20284;&#20110;&#34394;&#26500;&#30340;&#20840;&#24687;&#33329;&#20013;&#21019;&#24314;&#35774;&#32622;&#30340;&#26041;&#24335;&#12290;&#22522;&#20110;&#12298;&#29983;&#25104;&#20195;&#29702;&#12299;&#35770;&#25991;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#24341;&#20837;&#20102;&#28216;&#25103;&#24615;&#20803;&#32032;&#65292;&#22914;&#26080;&#38480;&#29983;&#25104;&#30340;&#24187;&#24819;&#19990;&#30028;&#65292;NPC&#34892;&#20026;&#30340;&#21487;&#25511;&#24615;&#65292;&#24189;&#40664;&#23545;&#35805;&#65292;&#25104;&#26412;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#29609;&#23478;&#20043;&#38388;&#30340;&#21512;&#20316;&#20197;&#21450;&#28216;&#25103;&#20869;&#20107;&#20214;&#30340;&#38750;&#30830;&#23450;&#24615;&#20803;&#32032;&#12290;Infinitia&#20351;&#29992;Unity&#24341;&#25806;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#26550;&#26500;&#65292;&#26041;&#20415;&#26410;&#26469;&#31038;&#21306;&#24320;&#21457;&#32773;&#21152;&#20837;&#20196;&#20154;&#20852;&#22859;&#30340;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#29992;&#20102;&#22810;&#20154;&#26694;&#26550;&#65292;&#20801;&#35768;&#29609;&#23478;&#22312;&#27169;&#25311;&#20013;&#23384;&#22312;&#24182;&#36827;&#34892;&#20132;&#20114;&#12290;&#27169;&#25311;&#23558;&#24456;&#24555;&#22312;https://infinitia.ai/&#19978;&#25552;&#20379;&#24320;&#25918;&#24335;&#27979;&#35797;&#29256;&#65292;&#24182;&#19988;&#25105;&#20204;&#26399;&#24453;&#19982;&#31038;&#21306;&#20849;&#21516;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Infinitia, a simulation game system that uses generative image and language models at play time to reshape all aspects of the setting and NPCs based on a short description from the player, in a way similar to how settings are created on the fictional Holodeck. Building off the ideas of the Generative Agents paper, our system introduces gameplay elements, such as infinite generated fantasy worlds, controllability of NPC behavior, humorous dialogue, cost &amp; time efficiency, collaboration between players and elements of non-determinism among in-game events. Infinitia is implemented in the Unity engine with a server-client architecture, facilitating the addition of exciting features by community developers in the future. Furthermore, it uses a multiplayer framework to allow humans to be present and interact in the simulation. The simulation will be available in open-alpha shortly at https://infinitia.ai/ and we are looking forward to building upon it with the community.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#38598;&#20013;&#20110;&#20102;&#35299;&#22914;&#20309;&#21033;&#29992;LLM&#26469;&#20248;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#25991;&#31456;&#24635;&#32467;&#20102;&#19981;&#21516;LLM&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#20197;&#21450;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10620</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Software Engineering: A Systematic Literature Review. (arXiv:2308.10620v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10620
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#38598;&#20013;&#20110;&#20102;&#35299;&#22914;&#20309;&#21033;&#29992;LLM&#26469;&#20248;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#25991;&#31456;&#24635;&#32467;&#20102;&#19981;&#21516;LLM&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#20197;&#21450;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21253;&#25324;&#36719;&#20214;&#24037;&#31243;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#25991;&#29486;&#25506;&#35752;&#20102;LLM&#22312;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;LLM&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#12289;&#24433;&#21709;&#21644;&#21487;&#33021;&#30340;&#38480;&#21046;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;LLM&#19982;&#36719;&#20214;&#24037;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#29305;&#21035;&#20851;&#27880;LLM&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#22914;&#20309;&#34987;&#21033;&#29992;&#26469;&#20248;&#21270;&#36807;&#31243;&#21644;&#32467;&#26524;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#20998;&#26512;&#20102;2017&#24180;&#33267;2023&#24180;&#30340;229&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#20197;&#22238;&#31572;&#22235;&#20010;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65288;RQs&#65289;&#12290;&#22312;RQ1&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;LLM&#36827;&#34892;&#20998;&#31867;&#21644;&#27604;&#36739;&#20998;&#26512;&#65292;&#25551;&#32472;&#20854;&#29420;&#29305;&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#12290;&#22312;RQ2&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#24378;&#22823;&#12289;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#25104;&#21151;&#21033;&#29992;LLM&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks and applications. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review on the intersection of LLMs and SE, with a particular focus on understanding how LLMs can be exploited in SE to optimize processes and outcomes. We collect and analyze a total of 229 research papers from 2017 to 2023 to answer four key research questions (RQs). In RQ1, we categorize and provide a comparative analysis of different LLMs that have been employed in SE tasks, characterising their distinctive features and uses. In RQ2, we analyse the methods used in data collection, preprocessing, and application highlighting the role of robust, well-curated datasets for successful LLM for S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#25945;&#24072;&#30340;&#26041;&#27861;&#65288;TA&#65289;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09544</link><description>&lt;p&gt;
&#36866;&#24212;&#24744;&#30340;&#25945;&#24072;: &#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning. (arXiv:2308.09544v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#25945;&#24072;&#30340;&#26041;&#27861;&#65288;TA&#65289;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20316;&#20026;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#26088;&#22312;&#38450;&#27490;&#36951;&#24536;&#12290;KD&#26041;&#27861;&#22312;CIL&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#22312;&#27809;&#26377;&#35775;&#38382;&#20808;&#21069;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24448;&#24448;&#24456;&#38590;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#36825;&#20010;&#38382;&#39064;&#28304;&#20110;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#26102;&#25945;&#24072;&#32593;&#32476;&#20013;&#30340;&#26174;&#33879;&#34920;&#31034;&#36716;&#25442;&#12290;&#36825;&#23548;&#33268;KD&#25439;&#22833;&#25104;&#20998;&#20013;&#20986;&#29616;&#36739;&#22823;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#23548;&#33268;CIL&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#21463;&#26368;&#36817;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25945;&#24072;&#36866;&#24212;&#65288;TA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#26356;&#26032;&#25945;&#24072;&#21644;&#20027;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;KD&#30340;CIL&#26041;&#27861;&#26080;&#32541;&#38598;&#25104;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#26080;&#26679;&#26412;CIL&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL models. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main models during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#21407;&#22240;&#65292;&#26377;&#25928;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#34987;&#39564;&#35777;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.09437</link><description>&lt;p&gt;
&#20174;&#26399;&#26395;&#21040;&#23433;&#20840;&#65306;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#27491;&#30830;&#30340;&#21407;&#22240;&#26469;&#28040;&#38500;&#28145;&#24230;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space. (arXiv:2308.09437v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#21407;&#22240;&#65292;&#26377;&#25928;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#34987;&#39564;&#35777;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20013;&#28508;&#34255;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#21487;&#33021;&#26377;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#36825;&#22312;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#20110;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#65288;&#22914;&#21307;&#23398;&#24212;&#29992;&#65289;&#26102;&#23384;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#30340;&#21518;&#22788;&#29702;&#27169;&#22411;&#26657;&#27491;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#36755;&#20837;&#32423;&#21035;&#30340;&#27880;&#37322;&#65292;&#36825;&#21482;&#36866;&#29992;&#20110;&#23616;&#37096;&#21270;&#20559;&#35265;&#65292;&#35201;&#20040;&#36890;&#36807;&#25193;&#20805;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#65292;&#24076;&#26395;&#33021;&#23454;&#29616;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;&#24403;&#36890;&#36807;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#26469;&#24314;&#27169;&#20559;&#35265;&#26102;&#65292;&#25105;&#20204;&#24378;&#35843;&#36873;&#25321;&#31283;&#20581;&#30340;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#65288;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#24448;&#24448;&#20250;&#23548;&#33268;&#21457;&#25955;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#20351;&#29992;VGG&#12289;ResNet&#21644;EfficientN&#22312;ISIC&#12289;&#39592;&#40836;&#12289;ImageNet&#21644;CelebA&#25968;&#25454;&#38598;&#19978;&#22312;&#21463;&#25511;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#26377;&#25928;&#20943;&#36731;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#25512;&#29702;&#26102;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#25552;&#20379;&#36229;&#36234;&#25991;&#23383;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#24378;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.01936</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#25105;&#20204;&#38656;&#35201;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26469;&#24314;&#27169;&#23454;&#29992;&#30340;&#31867;&#27604;?
&lt;/p&gt;
&lt;p&gt;
Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?. (arXiv:2308.01936v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#25512;&#29702;&#26102;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#25552;&#20379;&#36229;&#36234;&#25991;&#23383;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#24378;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#20010;&#29305;&#28857;&#26159;&#33021;&#22815;&#21033;&#29992;&#29087;&#24713;&#30340;&#39046;&#22495;&#23545;&#19981;&#37027;&#20040;&#29087;&#24713;&#30340;&#39046;&#22495;&#36827;&#34892;&#25512;&#29702;&#65292;&#21363;&#31867;&#27604;&#25512;&#29702;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22235;&#20010;&#19981;&#21516;&#22797;&#26434;&#32423;&#21035;&#30340;&#31867;&#27604;&#65306;&#35789;&#27719;&#31867;&#27604;&#12289;&#21477;&#27861;&#31867;&#27604;&#12289;&#35821;&#20041;&#31867;&#27604;&#21644;&#23454;&#29992;&#31867;&#27604;&#12290;&#38543;&#30528;&#31867;&#27604;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#38656;&#35201;&#36229;&#20986;&#25991;&#26412;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#36825;&#22312;&#25903;&#25345;LLMs&#30340;&#35789;&#27719;&#20849;&#29616;&#32479;&#35745;&#20013;&#19981;&#22826;&#21487;&#33021;&#25214;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#37319;&#29992;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24517;&#35201;&#24615;&#65292;&#36825;&#20123;&#25216;&#26415;&#32467;&#21512;&#20102;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65292;&#26681;&#25454;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#20379;&#20449;&#24687;&#20197;&#31361;&#20986;&#21644;&#22686;&#24378;&#30456;&#20851;&#20869;&#23481;&#65292;&#25552;&#20379;&#25277;&#35937;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#22312;&#20445;&#25345;LLMs&#30340;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hallmark of intelligence is the ability to use a familiar domain to make inferences about a less familiar domain, known as analogical reasoning. In this article, we delve into the performance of Large Language Models (LLMs) in dealing with progressively complex analogies expressed in unstructured text. We discuss analogies at four distinct levels of complexity: lexical analogies, syntactic analogies, semantic analogies, and pragmatic analogies. As the analogies become more complex, they require increasingly extensive, diverse knowledge beyond the textual content, unlikely to be found in the lexical co-occurrence statistics that power LLMs. To address this, we discuss the necessity of employing Neuro-symbolic AI techniques that combine statistical and symbolic AI, informing the representation of unstructured text to highlight and augment relevant content, provide abstraction and guide the mapping process. Our knowledge-informed approach maintains the efficiency of LLMs while preservin
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#20195;&#30721;&#26356;&#25913;&#20174;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#32763;&#35793;&#21040;&#21478;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#23454;&#29616;&#31532;&#19968;&#20010;LLM&#65292;Codeditor&#65292;&#20197;&#23558;&#20195;&#30721;&#26356;&#25913;&#24314;&#27169;&#20026;&#32534;&#36753;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#25105;&#20204;&#20026;&#22810;&#35821;&#35328;&#20195;&#30721;&#20849;&#21516;&#28436;&#36827;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.14991</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#20849;&#21516;&#28436;&#36827;
&lt;/p&gt;
&lt;p&gt;
Multilingual Code Co-Evolution Using Large Language Models. (arXiv:2307.14991v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#20195;&#30721;&#26356;&#25913;&#20174;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#32763;&#35793;&#21040;&#21478;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#23454;&#29616;&#31532;&#19968;&#20010;LLM&#65292;Codeditor&#65292;&#20197;&#23558;&#20195;&#30721;&#26356;&#25913;&#24314;&#27169;&#20026;&#32534;&#36753;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#25105;&#20204;&#20026;&#22810;&#35821;&#35328;&#20195;&#30721;&#20849;&#21516;&#28436;&#36827;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#36719;&#20214;&#39033;&#30446;&#22312;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#23454;&#29616;API&#21644;&#31639;&#27861;&#12290;&#32500;&#25252;&#36825;&#26679;&#30340;&#39033;&#30446;&#24456;&#32321;&#29712;&#65292;&#22240;&#20026;&#24320;&#21457;&#20154;&#21592;&#24517;&#39035;&#30830;&#20445;&#20219;&#20309;&#21464;&#21270;&#65288;&#20363;&#22914;&#38169;&#35823;&#20462;&#22797;&#25110;&#26032;&#21151;&#33021;&#65289;&#33021;&#22815;&#21450;&#26102;&#19988;&#26080;&#35823;&#22320;&#20256;&#25773;&#21040;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#30340;&#23454;&#29616;&#20013;&#12290;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#36719;&#20214;&#19990;&#30028;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#32763;&#35793;&#24037;&#20855;&#65288;&#20363;&#22914;&#36716;&#35793;&#22120;&#65289;&#25110;&#29992;&#20110;&#23558;&#20195;&#30721;&#20174;&#19968;&#31181;&#35821;&#35328;&#32763;&#35793;&#25104;&#21478;&#19968;&#31181;&#35821;&#35328;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#26377;&#38480;&#30340;&#20215;&#20540;&#12290;&#27599;&#27425;&#23558;&#25972;&#20010;&#20195;&#30721;&#24211;&#20174;&#19968;&#31181;&#35821;&#35328;&#32763;&#35793;&#21040;&#21478;&#19968;&#31181;&#35821;&#35328;&#30340;&#26041;&#24335;&#24182;&#19981;&#31526;&#21512;&#24320;&#21457;&#20154;&#21592;&#30340;&#24037;&#20316;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#20195;&#30721;&#26356;&#25913;&#20174;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#32763;&#35793;&#21040;&#21478;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;LLM&#65292;&#21517;&#20026;Codeditor&#65292;&#26469;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#12290;Codeditor&#26126;&#30830;&#22320;&#23558;&#20195;&#30721;&#26356;&#25913;&#24314;&#27169;&#20026;&#32534;&#36753;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#22312;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#24314;&#31435;&#26356;&#25913;&#30340;&#20851;&#32852;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;Codeditor&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;6,613&#20010;&#31034;&#20363;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 ali
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.13704</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review. (arXiv:2307.13704v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#29616;&#22312;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#33021;&#22815;&#35299;&#37322;&#22797;&#26434;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;XAI&#29305;&#21035;&#36866;&#29992;&#20110;&#21361;&#38505;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#20154;&#31867;&#30340;&#29983;&#21629;&#20381;&#36182;&#20110;AI&#31995;&#32479;&#30340;&#20915;&#31574;&#12290;&#21307;&#30103;&#30740;&#31350;&#30340;&#19968;&#20010;&#39046;&#22495;&#26159;&#24180;&#40836;&#39044;&#27979;&#21644;&#34928;&#32769;&#21450;&#19982;&#24180;&#40836;&#30456;&#20851;&#30142;&#30149;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#37492;&#23450;&#12290;&#28982;&#32780;&#65292;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;XAI&#30340;&#20316;&#29992;&#23578;&#26410;&#30452;&#25509;&#25506;&#35752;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#22120;&#23448;&#31995;&#32479;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;XAI&#22312;&#21307;&#30103;&#24212;&#29992;&#20197;&#21450;&#29305;&#21035;&#26159;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
&lt;/p&gt;</description></item><item><title>ProbVLM&#26159;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22266;&#26377;&#30340;&#23884;&#20837;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.00398</link><description>&lt;p&gt;
ProbVLM: &#20923;&#32467;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models. (arXiv:2307.00398v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00398
&lt;/p&gt;
&lt;p&gt;
ProbVLM&#26159;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22266;&#26377;&#30340;&#23884;&#20837;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22914;CLIP&#25104;&#21151;&#22320;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#12290;&#36890;&#36807;&#26631;&#20934;&#30340;&#30830;&#23450;&#24615;&#26144;&#23556;&#36807;&#31243;&#65292;&#23558;&#22270;&#20687;&#25110;&#25991;&#26412;&#26679;&#26412;&#26144;&#23556;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#21521;&#37327;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65306;&#30001;&#20110;&#22810;&#20010;&#26679;&#26412;&#65288;&#22270;&#20687;&#25110;&#25991;&#26412;&#65289;&#21487;&#20197;&#25277;&#35937;&#20986;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#30456;&#21516;&#27010;&#24565;&#65292;&#30830;&#23450;&#24615;&#23884;&#20837;&#19981;&#21453;&#26144;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProbVLM&#65292;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20107;&#21518;&#26041;&#24335;&#22312;&#39044;&#35757;&#32451;&#30340;VLM&#20013;&#36890;&#36807;&#20869;&#37096;/&#22806;&#37096;&#27169;&#24577;&#23545;&#40784;&#20272;&#35745;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#25110;&#35745;&#31639;&#12290;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#21363;COCO&#12289;Flickr&#12289;CUB&#21644;Oxford-flowers&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#20004;&#20010;VLM&#65288;CLIP&#21644;BLIP&#65289;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#37327;&#21270;&#20102;&#23884;&#20837;&#19981;&#30830;&#23450;&#24615;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#26657;&#20934;&#65292;&#24182;&#34920;&#26126;ProbVLM&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20027;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model 
&lt;/p&gt;</description></item><item><title>PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.10403</link><description>&lt;p&gt;
PaLM 2 &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
PaLM 2 Technical Report. (arXiv:2305.10403v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10403
&lt;/p&gt;
&lt;p&gt;
PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; PaLM 2&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#27604;&#20854;&#21069;&#36523; PaLM &#22312;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#26356;&#21152;&#20986;&#33394;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;PaLM 2 &#26159;&#19968;&#31181;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#20197;&#21450;&#25512;&#29702;&#20219;&#21153;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; PaLM 2 &#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#65292;&#21516;&#26102;&#23637;&#29616;&#20102;&#27604; PaLM &#26356;&#24555;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#30340;&#25928;&#29575;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#21516;&#26102;&#20063;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24555;&#22320;&#21709;&#24212;&#65292;&#20197;&#33719;&#24471;&#26356;&#33258;&#28982;&#30340;&#20132;&#20114;&#33410;&#22863;&#12290;PaLM 2 &#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312; BIG-Bench &#21644;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110; PaLM &#26377;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;PaLM 2 &#22312;&#19968;&#22871;&#36127;&#36131;&#20154;&#30340; AI &#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#38468;&#21152;&#36816;&#34892;&#24320;&#38144;&#25110;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23545;&#27602;&#24615;&#36827;&#34892;&#25512;&#29702;&#26102;&#38388;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Over
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;PyTorch&#30340;Fully Sharded Data Parallel&#65288;FSDP&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21487;&#25193;&#23637;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20248;&#21270;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11277</link><description>&lt;p&gt;
PyTorch FSDP&#65306;&#20840;&#38754;&#20998;&#29255;&#25968;&#25454;&#24182;&#34892;&#35268;&#27169;&#21270;&#30340;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. (arXiv:2304.11277v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;PyTorch&#30340;Fully Sharded Data Parallel&#65288;FSDP&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21487;&#25193;&#23637;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20248;&#21270;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#24191;&#27867;&#39046;&#22495;&#20869;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#28508;&#21147;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#24320;&#21457;&#21644;&#25506;&#32034;&#22823;&#22411;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#20173;&#21463;&#38480;&#20110;&#23569;&#25968;&#39640;&#32423;&#29992;&#25143;&#21644;&#34892;&#19994;&#39046;&#34966;&#65292;&#23548;&#33268;&#25216;&#26415;&#19978;&#30340;&#38544;&#21547;&#22721;&#22418;&#38459;&#30861;&#24191;&#27867;&#31038;&#21306;&#35775;&#38382;&#21644;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PyTorch Fully Sharded Data Parallel&#65288;FSDP&#65289;&#20316;&#20026;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#30340;&#20135;&#19994;&#32423;&#35299;&#20915;&#26041;&#26696;&#12290;FSDP&#24050;&#19982;&#20960;&#20010;&#20851;&#38190;PyTorch&#26680;&#24515;&#32452;&#20214;&#65288;&#21253;&#25324;&#24352;&#37327;&#23454;&#29616;&#12289;&#20998;&#21457;&#22120;&#31995;&#32479;&#21644;CUDA&#20869;&#23384;&#32531;&#23384;&#20998;&#37197;&#22120;&#65289;&#23494;&#20999;&#21327;&#20316;&#65292;&#20197;&#25552;&#20379;&#38750;&#20405;&#20837;&#24335;&#29992;&#25143;&#20307;&#39564;&#21644;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;FSDP&#26412;&#22320;&#38598;&#25104;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#21644;&#35774;&#32622;&#65292;&#20248;&#21270;&#20102;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.05257</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#26102;&#38388;&#21464;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Multi-granulariy Time-based Transformer for Knowledge Tracing. (arXiv:2304.05257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#23398;&#29983;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#20026;&#27599;&#20010;&#23398;&#29983;&#21019;&#24314;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#39044;&#27979;&#23398;&#29983;&#22312;&#32473;&#23450;&#27979;&#35797;&#20013;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;RIIID&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20316;&#20026;&#35299;&#30721;&#22120;&#36755;&#20837;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;LightGBM&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a transformer architecture for predicting student performance on standardized tests. Specifically, we leverage students historical data, including their past test scores, study habits, and other relevant information, to create a personalized model for each student. We then use these models to predict their future performance on a given test. Applying this model to the RIIID dataset, we demonstrate that using multiple granularities for temporal features as the decoder input significantly improve model performance. Our results also show the effectiveness of our approach, with substantial improvements over the LightGBM method. Our work contributes to the growing field of AI in education, providing a scalable and accurate tool for predicting student outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.13592</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#25552;&#31034;&#65306;&#19996;&#21335;&#20122;&#35821;&#35328;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28151;&#21512;&#20195;&#30721;&#22312;&#19990;&#30028;&#35768;&#22810;&#22320;&#21306;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#23454;&#36341;&#65292;&#20294;&#25910;&#38598;&#39640;&#36136;&#37327;&#19988;&#20302;&#25104;&#26412;&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#36843;&#20351;&#20154;&#20204;&#38382;&#65306;&#36825;&#20123;&#31995;&#32479;&#33021;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#38646;-shot&#30340;&#26041;&#24335;&#19979;&#22914;&#20309;&#25552;&#31034;LLMs&#20026;&#19996;&#21335;&#20122;&#65288;SEA&#65289;&#30340;&#20116;&#31181;&#35821;&#35328;&#65288;&#21360;&#23612;&#35821;&#65292;&#39532;&#26469;&#35821;&#65292;&#20013;&#25991;&#65292;&#22612;&#21152;&#36335;&#35821;&#65292;&#36234;&#21335;&#35821;&#65289;&#21450;&#20811;&#37324;&#22885;&#23572;&#35821;S ingl ish&#21019;&#36896;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#26174;&#31034;&#20986;&#26368;&#22823;&#30340;&#28508;&#21147;&#65292;&#24403;&#26126;&#30830;&#23450;&#20041;&#8220;&#28151;&#21512;&#20195;&#30721;&#8221;&#26415;&#35821;&#26102;&#65292;&#33021;&#22815;68%&#30340;&#26102;&#38388;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;ChatGPT&#21644;InstructGPT&#65288;davinci-003&#65289;&#29983;&#25104;S ingl ish&#25991;&#26412;&#30340;&#34920;&#29616;&#20063;&#20540;&#24471;&#27880;&#24847;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#25552;&#31034;&#19979;&#30340;&#25104;&#21151;&#29575;&#24179;&#22343;&#20026;96%&#12290;&#20294;&#26159;&#65292;ChatGPT&#21644;InstructGPT&#30340;&#28151;&#21512;&#20195;&#30721;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA) -Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish. We find that ChatGPT shows the most potential, capable of producing code-mixed text 68% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.04634</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#24320;&#25918;&#19990;&#30028;&#25925;&#20107;&#29983;&#25104;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey. (arXiv:2212.04634v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35762;&#25925;&#20107;&#21644;&#21465;&#20107;&#26159;&#20154;&#31867;&#20307;&#39564;&#30340;&#22522;&#30784;&#65292;&#19982;&#25105;&#20204;&#30340;&#31038;&#20250;&#21644;&#25991;&#21270;&#21442;&#19982;&#23494;&#19981;&#21487;&#20998;&#12290;&#22240;&#27492;&#65292;&#38271;&#26399;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#23581;&#35797;&#21019;&#24314;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#25925;&#20107;&#30340;&#31995;&#32479;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#36164;&#28304;&#30340;&#25512;&#21160;&#65292;&#33258;&#21160;&#29983;&#25104;&#25925;&#20107;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#25925;&#20107;&#20013;&#23454;&#29616;&#20840;&#23616;&#19968;&#33268;&#24615;&#65292;&#36825;&#20351;&#24471;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#36798;&#21040;&#19982;&#20154;&#31867;&#21465;&#36848;&#32773;&#30456;&#21516;&#30340;&#21465;&#20107;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35768;&#22810;&#30740;&#31350;&#35797;&#22270;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#36825;&#34987;&#31216;&#20026;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#12290;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#20854;&#20013;&#21487;&#20197;&#22686;&#24378;&#25925;&#20107;&#20107;&#20214;&#20043;&#38388;&#30340;&#36923;&#36753;&#36830;&#36143;&#24615;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#30693;&#35782;&#22522;&#30784;&#65292;&#24182;&#20943;&#36731;&#25925;&#20107;&#20013;&#36807;&#24230;&#27010;&#25324;&#21644;&#37325;&#22797;&#38382;&#39064;&#12290;&#26412;&#27425;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#21644;&#20840;&#38754;&#30340;&#22238;&#39038;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#25925;&#20107;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#65288;iii&#65289;&#25105;&#20204;&#25351;&#20986;&#20102;&#23578;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#19982;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Storytelling and narrative are fundamental to human experience, intertwined with our social and cultural engagement. As such, researchers have long attempted to create systems that can generate stories automatically. In recent years, powered by deep learning and massive data resources, automatic story generation has shown significant advances. However, considerable challenges, like the need for global coherence in generated stories, still hamper generative models from reaching the same storytelling ability as human narrators. To tackle these challenges, many studies seek to inject structured knowledge into the generation process, which is referred to as structured knowledge-enhanced story generation. Incorporating external knowledge can enhance the logical coherence among story events, achieve better knowledge grounding, and alleviate over-generalization and repetition problems in stories. This survey provides the latest and comprehensive review of this research field: (i) we present a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#26469;&#26356;&#24555;&#22320;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2211.16691</link><description>&lt;p&gt;
&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules. (arXiv:2211.16691v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#26469;&#26356;&#24555;&#22320;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#31351;&#20030;&#25506;&#32034;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20197;&#25214;&#21040;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#32780;&#23548;&#33268;&#26679;&#26412;&#22797;&#26434;&#24230;&#19981;&#22826;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#31995;&#32479;&#30340;&#19987;&#23478;&#30693;&#35782;&#36890;&#24120;&#20801;&#35768;&#25105;&#20204;&#35774;&#35745;&#31616;&#21333;&#35268;&#21017;&#65292;&#25105;&#20204;&#26399;&#26395;&#33391;&#22909;&#30340;&#31574;&#30053;&#22987;&#32456;&#36981;&#24490;&#36825;&#20123;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36830;&#32493;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#20197;&#32435;&#20837;&#36825;&#20123;&#35268;&#21017;&#24182;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#65292;&#20174;&#32780;&#26174;&#30528;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25913;&#36827;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#20195;&#29702;&#31243;&#24207;&#36873;&#25321;&#30340;&#21160;&#20316;&#19981;&#31526;&#21512;&#25105;&#20204;&#30340;&#30452;&#35273;&#65292;&#25105;&#20204;&#20250;&#39281;&#21644;&#36825;&#20123;&#21160;&#20316;&#65292;&#20851;&#38190;&#26159;&#20462;&#25913;&#31574;&#30053;&#30340;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#27969;&#31243;&#19981;&#21463;&#39281;&#21644;&#27493;&#39588;&#30340;&#24433;&#21709;&#12290;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#23427;&#20351;&#20195;&#29702;&#31243;&#24207;&#20197;&#27604;&#20256;&#32479;&#20195;&#29702;&#31243;&#24207;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#21040;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#28040;&#32791;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) generally suffers from poor sample complexity, mostly due to the need to exhaustively explore the state-action space to find well-performing policies. On the other hand, we postulate that expert knowledge of the system often allows us to design simple rules we expect good policies to follow at all times. In this work, we hence propose a simple yet effective modification of continuous actor-critic frameworks to incorporate such rules and avoid regions of the state-action space that are known to be suboptimal, thereby significantly accelerating the convergence of RL agents. Concretely, we saturate the actions chosen by the agent if they do not comply with our intuition and, critically, modify the gradient update step of the policy to ensure the learning process is not affected by the saturation step. On a room temperature control case study, it allows agents to converge to well-performing policies up to 6-7x faster than classical agents without computational o
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23545;&#28145;&#24230;&#22270;&#32858;&#31867;&#36827;&#34892;&#20102;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#65292;&#24182;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2211.12875</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#32858;&#31867;&#32508;&#36848;&#65306;&#20998;&#31867;&#12289;&#25361;&#25112;&#12289;&#24212;&#29992;&#21644;&#24320;&#25918;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Graph Clustering: Taxonomy, Challenge, Application, and Open Resource. (arXiv:2211.12875v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12875
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23545;&#28145;&#24230;&#22270;&#32858;&#31867;&#36827;&#34892;&#20102;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#65292;&#24182;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26088;&#22312;&#23558;&#22270;&#20013;&#30340;&#33410;&#28857;&#21010;&#20998;&#20026;&#33509;&#24178;&#19981;&#21516;&#30340;&#31751;&#65292;&#26159;&#19968;&#39033;&#22522;&#30784;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#20511;&#21161;&#28145;&#24230;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#32508;&#36848;&#35770;&#25991;&#30456;&#23545;&#36739;&#23569;&#65292;&#32508;&#36848;&#35813;&#39046;&#22495;&#30340;&#24037;&#20316;&#21183;&#22312;&#24517;&#34892;&#12290;&#22522;&#20110;&#27492;&#21160;&#26426;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#30340;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#20844;&#24335;&#21270;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#22270;&#31867;&#22411;&#12289;&#32593;&#32476;&#26550;&#26500;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#32858;&#31867;&#26041;&#27861;&#31561;&#22235;&#20010;&#19981;&#21516;&#30340;&#26631;&#20934;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#20174;&#22270;&#25968;&#25454;&#36136;&#37327;&#12289;&#31283;&#23450;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#36776;&#21035;&#33021;&#21147;&#21644;&#26410;&#30693;&#31751;&#25968;&#31561;&#20116;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering, which aims to divide nodes in the graph into several distinct clusters, is a fundamental yet challenging task. Benefiting from the powerful representation capability of deep learning, deep graph clustering methods have achieved great success in recent years. However, the corresponding survey paper is relatively scarce, and it is imminent to make a summary of this field. From this motivation, we conduct a comprehensive survey of deep graph clustering. Firstly, we introduce formulaic definition, evaluation, and development in this field. Secondly, the taxonomy of deep graph clustering methods is presented based on four different criteria, including graph type, network architecture, learning paradigm, and clustering method. Thirdly, we carefully analyze the existing methods via extensive experiments and summarize the challenges and opportunities from five perspectives, including graph data quality, stability, scalability, discriminative capability, and unknown cluster nu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.00979</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs. (arXiv:2206.00979v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00979
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65288;SP&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#26680;&#24515;&#20043;&#19968;&#12290;&#23427;&#23558;&#22270;&#20998;&#35299;&#20026;&#26368;&#30701;&#36335;&#24452;&#65292;&#24182;&#35745;&#31639;&#27599;&#20010;&#22270;&#20013;&#26368;&#30701;&#36335;&#24452;&#30340;&#39057;&#29575;&#12290;&#28982;&#32780;&#65292;SP&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#26368;&#30701;&#36335;&#24452;&#30340;&#19977;&#20803;&#34920;&#31034;&#22833;&#21435;&#20102;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;SP&#27604;&#36739;&#22270;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#22270;&#32467;&#26500;&#30340;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#24456;&#24120;&#35265;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38142;&#29366;&#32467;&#26500;&#12289;&#29615;&#29366;&#32467;&#26500;&#21644;&#26143;&#29366;&#32467;&#26500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#12290;&#23427;&#20351;&#29992;&#20197;&#27599;&#20010;&#39030;&#28857;&#20026;&#26681;&#30340;&#26576;&#20010;&#28145;&#24230;&#30340;BFS&#26641;&#26469;&#38480;&#21046;&#32771;&#34385;&#26368;&#30701;&#36335;&#24452;&#30340;&#26368;&#22823;&#38271;&#24230;&#65292;&#32771;&#34385;&#21040;&#23567;&#19990;&#30028;&#29305;&#24615;&#12290;&#23427;&#32771;&#34385;&#20102;&#26368;&#30701;&#36335;&#24452;&#20013;&#25152;&#26377;&#39030;&#28857;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#26041;&#20415;&#22312;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#19978;&#27604;&#36739;&#22270;&#65292;&#23427;&#20174;&#39030;&#28857;&#21644;
&lt;/p&gt;
&lt;p&gt;
The traditional shortest-path graph kernel (SP) is one of the most popular graph kernels. It decomposes graphs into shortest paths and computes their frequencies in each graph. However, SP has two main challenges: Firstly, the triplet representation of the shortest path loses information. Secondly, SP compares graphs without considering the multiple different scales of the graph structure which is common in real-world graphs, e.g., the chain-, ring-, and star-structures in social networks. To overcome these two challenges, we develop a novel shortest-path graph kernel called the Multi-scale Wasserstein Shortest-Path Filtration graph kernel (MWSPF). It uses a BFS tree of a certain depth rooted at each vertex to restrict the maximum length of the shortest path considering the small world property. It considers the labels of all the vertices in the shortest path. To facilitate the comparison of graphs at multiple different scales, it augments graphs from both the aspects of the vertex and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#20105;&#35758;&#21477;&#23545;&#36827;&#34892;&#23454;&#39564;&#27604;&#36739;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#20013;GPT-2&#19982;&#20154;&#31867;&#21028;&#26029;&#26368;&#20026;&#19968;&#33268;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#22833;&#36133;&#20197;&#21450;&#25214;&#20986;&#26368;&#31526;&#21512;&#20154;&#31867;&#21028;&#26029;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2204.03592</link><description>&lt;p&gt;
&#26816;&#39564;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#23545;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#39044;&#27979;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Testing the limits of natural language models for predicting human language judgments. (arXiv:2204.03592v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#20105;&#35758;&#21477;&#23545;&#36827;&#34892;&#23454;&#39564;&#27604;&#36739;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#20013;GPT-2&#19982;&#20154;&#31867;&#21028;&#26029;&#26368;&#20026;&#19968;&#33268;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#22833;&#36133;&#20197;&#21450;&#25214;&#20986;&#26368;&#31526;&#21512;&#20154;&#31867;&#21028;&#26029;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#20851;&#20110;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#26041;&#24335;&#30340;&#35745;&#31639;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#39564;&#26041;&#27861;&#23545;&#22810;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#19968;&#33268;&#24615;&#30340;&#27604;&#36739;&#65306;&#20105;&#35758;&#21477;&#23545;&#12290;&#23545;&#20110;&#27599;&#20010;&#20105;&#35758;&#21477;&#23545;&#65292;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#21738;&#20010;&#21477;&#23376;&#26356;&#21487;&#33021;&#20986;&#29616;&#22312;&#33258;&#28982;&#25991;&#26412;&#20013;&#19978;&#23384;&#22312;&#19981;&#21516;&#24847;&#35265;&#12290;&#32771;&#34385;&#21040;&#20061;&#20010;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;n-gram&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#25442;&#22120;&#27169;&#22411;&#65289;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#21477;&#23376;&#25110;&#32773;&#21512;&#25104;&#20248;&#21270;&#21477;&#23545;&#26469;&#21019;&#24314;&#20102;&#25968;&#30334;&#20010;&#36825;&#26679;&#30340;&#20105;&#35758;&#21477;&#23545;&#12290;&#28982;&#21518;&#65292;&#20154;&#31867;&#21463;&#35797;&#32773;&#25552;&#20379;&#20102;&#21028;&#26029;&#65292;&#25351;&#31034;&#22312;&#27599;&#20010;&#21477;&#23545;&#20013;&#65292;&#21738;&#20010;&#21477;&#23376;&#26356;&#21487;&#33021;&#21457;&#29983;&#12290;&#20105;&#35758;&#21477;&#23545;&#34987;&#35777;&#26126;&#26497;&#20026;&#26377;&#25928;&#65292;&#33021;&#22815;&#25581;&#31034;&#27169;&#22411;&#30340;&#22833;&#36133;&#21644;&#35782;&#21035;&#19982;&#20154;&#31867;&#21028;&#26029;&#26368;&#20026;&#19968;&#33268;&#30340;&#27169;&#22411;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;&#26368;&#31526;&#21512;&#20154;&#31867;&#21028;&#26029;&#30340;&#27169;&#22411;&#26159;GPT-2&#65292;&#23613;&#31649;&#23454;&#39564;&#36824;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network language models can serve as computational hypotheses about how humans process language. We compared the model-human consistency of diverse language models using a novel experimental approach: controversial sentence pairs. For each controversial sentence pair, two language models disagree about which sentence is more likely to occur in natural text. Considering nine language models (including n-gram, recurrent neural networks, and transformer models), we created hundreds of such controversial sentence pairs by either selecting sentences from a corpus or synthetically optimizing sentence pairs to be highly controversial. Human subjects then provided judgments indicating for each pair which of the two sentences is more likely. Controversial sentence pairs proved highly effective at revealing model failures and identifying models that aligned most closely with human judgments. The most human-consistent model tested was GPT-2, although experiments also revealed significant s
&lt;/p&gt;</description></item><item><title>&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2203.01881</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#20998;&#29305;&#24449;&#24230;&#37327;&#19979;&#28216;&#20998;&#31867;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features. (arXiv:2203.01881v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01881
&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#22833;&#36133;&#27169;&#24335;&#21644;&#23398;&#20064;&#34920;&#31034;&#30340;&#35299;&#37322;&#65292;&#23384;&#22312;&#30528;&#26377;&#38480;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102; SimCLR&#12289;SwaV&#12289;MoCo&#12289;BYOL&#12289;DINO&#12289;SimSiam&#12289;VICReg &#21644; Barlow Twins &#31561;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#22312;&#19981;&#20351;&#29992;&#31867;&#26631;&#31614;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23545;&#24212;&#20110;&#22270;&#20687;&#20013;&#29420;&#29305;&#29289;&#29702;&#23646;&#24615;&#30340;&#21306;&#20998;&#29305;&#24449;&#65292;&#36825;&#20123;&#21306;&#20998;&#29305;&#24449;&#20027;&#35201;&#23384;&#22312;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#34920;&#31034;&#20013;&#12290;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#34920;&#31034;&#31354;&#38388;&#21387;&#32553;&#22810;&#36798; 40%&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#32447;&#24615;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65288;&#25110; Q-Score&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#12289;&#26080;&#30417;&#30563;&#30340;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#19968;&#20010;&#32473;&#23450;&#26679;&#26412;&#22312;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#26159;&#21542;&#21487;&#33021;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#22312; ImageNet-100 &#21644; ImageNet-1K &#19978;&#23454;&#29616;&#20102; AUPRC &#20998;&#21035;&#20026; 91.45 &#21644; 78.78&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#36896;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#21487;&#35266;&#27979;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#30340;&#36991;&#35753;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#24471;&#21040;&#26410;&#27979;&#37327;&#29366;&#24577;&#30340;&#39640;&#26031;&#32622;&#20449;&#24230;&#30340;&#26377;&#38480;&#29366;&#24577;&#25277;&#35937;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#36825;&#31181;&#25511;&#21046;&#22120;&#21487;&#20197;&#20197;&#19968;&#31181;&#40065;&#26834;&#30340;&#26041;&#24335;&#36991;&#20813;&#19981;&#23433;&#20840;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2103.02398</link><description>&lt;p&gt;
&#22522;&#20110;&#26500;&#36896;&#30340;&#21487;&#35266;&#27979;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#30340;&#36991;&#35753;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Correct-by-construction reach-avoid control of partially observable linear stochastic systems. (arXiv:2103.02398v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.02398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#36896;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#21487;&#35266;&#27979;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#30340;&#36991;&#35753;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#24471;&#21040;&#26410;&#27979;&#37327;&#29366;&#24577;&#30340;&#39640;&#26031;&#32622;&#20449;&#24230;&#30340;&#26377;&#38480;&#29366;&#24577;&#25277;&#35937;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#36825;&#31181;&#25511;&#21046;&#22120;&#21487;&#20197;&#20197;&#19968;&#31181;&#40065;&#26834;&#30340;&#26041;&#24335;&#36991;&#20813;&#19981;&#23433;&#20840;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#39640;&#26031;&#36807;&#31243;&#21644;&#27979;&#37327;&#22122;&#22768;&#30340;&#31163;&#25955;&#26102;&#38388;&#12289;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#30340;&#21487;&#35266;&#27979;&#25511;&#21046;&#22120;&#21512;&#25104;&#38382;&#39064;&#12290;&#38382;&#39064;&#26159;&#35745;&#31639;&#19968;&#20010;&#25511;&#21046;&#22120;&#65292;&#20351;&#24471;&#31995;&#32479;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#20197;&#33267;&#23569;&#19968;&#23450;&#30340;&#27010;&#29575;&#36798;&#21040;&#26399;&#26395;&#30340;&#30446;&#26631;&#29366;&#24577;&#65292;&#21516;&#26102;&#36991;&#20813;&#19981;&#23433;&#20840;&#29366;&#24577;&#12290;&#30001;&#20110;&#38543;&#26426;&#24615;&#21644;&#38750;&#20984;&#24615;&#65292;&#35813;&#38382;&#39064;&#36890;&#24120;&#19981;&#20855;&#22791;&#31934;&#30830;&#30340;&#31639;&#27861;&#25110;&#38381;&#24335;&#35299;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#20351;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#33719;&#24471;&#30340;&#23545;&#26410;&#27979;&#37327;&#29366;&#24577;&#30340;&#39640;&#26031;&#32622;&#20449;&#24230;&#30340;&#26377;&#38480;&#29366;&#24577;&#25277;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26500;&#36896;&#30340;&#25511;&#21046;&#22120;&#21512;&#25104;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#25277;&#35937;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#20026;&#20102;&#23545;&#36817;&#20284;&#36716;&#31227;&#27010;&#29575;&#30340;&#25968;&#20540;&#19981;&#31934;&#30830;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#36716;&#31227;&#27010;&#29575;&#21306;&#38388;&#30340;MDP&#12290;&#36890;&#36807;&#26500;&#36896;&#65292;&#25277;&#35937;&#19978;&#30340;&#20219;&#20309;&#31574;&#30053;&#37117;&#21487;&#20197;&#32454;&#21270;&#20026;LTI&#31995;&#32479;&#30340;&#20998;&#27573;&#32447;&#24615;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
We study feedback controller synthesis for reach-avoid control of discrete-time, linear time-invariant (LTI) systems with Gaussian process and measurement noise. The problem is to compute a controller such that, with at least some required probability, the system reaches a desired goal state in finite time while avoiding unsafe states. Due to stochasticity and nonconvexity, this problem does not admit exact algorithmic or closed-form solutions in general. Our key contribution is a correct-by-construction controller synthesis scheme based on a finite-state abstraction of a Gaussian belief over the unmeasured state, obtained using a Kalman filter. We formalize this abstraction as a Markov decision process (MDP). To be robust against numerical imprecision in approximating transition probabilities, we use MDPs with intervals of transition probabilities. By construction, any policy on the abstraction can be refined into a piecewise linear feedback controller for the LTI system. We prove tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;&#36890;&#36807;&#23398;&#20064;&#22810;&#26679;&#30340;&#39044;&#27979;&#22120;&#21487;&#20197;&#20272;&#35745;&#26032;&#35266;&#27979;&#23454;&#20363;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#24182;&#19981;&#19968;&#23450;&#33021;&#36716;&#21270;&#20026;&#39044;&#27979;&#22120;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2010.12995</link><description>&lt;p&gt;
&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#65306;&#21442;&#25968;&#19982;&#39044;&#27979;&#22120;&#29109;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution detection for regression tasks: parameter versus predictor entropy. (arXiv:2010.12995v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.12995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;&#36890;&#36807;&#23398;&#20064;&#22810;&#26679;&#30340;&#39044;&#27979;&#22120;&#21487;&#20197;&#20272;&#35745;&#26032;&#35266;&#27979;&#23454;&#20363;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#24182;&#19981;&#19968;&#23450;&#33021;&#36716;&#21270;&#20026;&#39044;&#27979;&#22120;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#26816;&#27979;&#26679;&#26412;&#19982;&#35757;&#32451;&#26679;&#26412;&#30456;&#36317;&#22826;&#36828;&#26102;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#34987;&#31216;&#20026;&#31163;&#32676;&#26679;&#26412;&#65288;OOD&#65289;&#26816;&#27979;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#32780;&#35328;&#65292;&#19968;&#31181;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#30340;&#26041;&#27861;&#26159;&#23398;&#20064;&#22810;&#26679;&#30340;&#39044;&#27979;&#22120;&#65292;&#36825;&#20123;&#39044;&#27979;&#22120;&#37117;&#33021;&#35299;&#37322;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#20272;&#35745;&#26032;&#35266;&#27979;&#23454;&#20363;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#39044;&#27979;&#32467;&#26524;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#34913;&#37327;&#12290;&#35780;&#20272;&#21644;&#35748;&#35777;&#26041;&#27861;&#26816;&#27979;OOD&#30340;&#33021;&#21147;&#38656;&#35201;&#25351;&#23450;&#22312;&#37096;&#32626;&#20013;&#21487;&#33021;&#21457;&#29983;&#20294;&#27809;&#26377;&#21487;&#29992;&#39044;&#27979;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#36873;&#25321;&#22238;&#24402;&#20219;&#21153;&#20316;&#20026;&#30740;&#31350;&#37325;&#28857;&#65292;&#22312;&#27492;&#20219;&#21153;&#20013;&#36873;&#25321;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#27934;&#23519;&#21147;&#30340;&#27169;&#22411;&#26469;&#34920;&#31034;OOD&#20998;&#24067;&#65292;&#24182;&#23545;&#21508;&#31181;&#26041;&#27861;&#22312;&#21306;&#20998;OOD&#26679;&#26412;&#21644;&#25968;&#25454;&#20013;&#30340;&#33021;&#21147;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#21487;&#33021;&#26080;&#27861;&#36716;&#21270;&#20026;&#39044;&#27979;&#22120;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is crucial to detect when an instance lies downright too far from the training samples for the machine learning model to be trusted, a challenge known as out-of-distribution (OOD) detection. For neural networks, one approach to this task consists of learning a diversity of predictors that all can explain the training data. This information can be used to estimate the epistemic uncertainty at a given newly observed instance in terms of a measure of the disagreement of the predictions. Evaluation and certification of the ability of a method to detect OOD require specifying instances which are likely to occur in deployment yet on which no prediction is available. Focusing on regression tasks, we choose a simple yet insightful model for this OOD distribution and conduct an empirical evaluation of the ability of various methods to discriminate OOD samples from the data. Moreover, we exhibit evidence that a diversity of parameters may fail to translate to a diversity of predictors. Based 
&lt;/p&gt;</description></item></channel></rss>