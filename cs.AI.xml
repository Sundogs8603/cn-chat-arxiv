<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.09270</link><description>&lt;p&gt;
Retro-fallback: &#38754;&#21521;&#19981;&#30830;&#23450;&#19990;&#30028;&#30340;&#36870;&#21512;&#25104;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Retro-fallback: retrosynthetic planning in an uncertain world. (arXiv:2310.09270v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21512;&#25104;&#26159;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#21270;&#23398;&#21453;&#24212;&#20174;&#26356;&#31616;&#21333;&#12289;&#21487;&#36141;&#20080;&#30340;&#20998;&#23376;&#21019;&#24314;&#25152;&#38656;&#20998;&#23376;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#31639;&#27861;&#26469;&#23547;&#25214;&#19968;&#31995;&#21015;&#24230;&#37327;&#25351;&#26631;&#65288;&#20363;&#22914;&#26368;&#30701;&#36335;&#24452;&#12289;&#26368;&#20302;&#25104;&#26412;&#65289;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#24573;&#35270;&#20102;&#25105;&#20204;&#23545;&#21487;&#33021;&#21453;&#24212;&#31354;&#38388;&#30340;&#19981;&#23436;&#20840;&#20102;&#35299;&#65292;&#36825;&#24847;&#21619;&#30528;&#31639;&#27861;&#29983;&#25104;&#30340;&#35745;&#21010;&#21487;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#26080;&#27861;&#23454;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#36807;&#31243;&#30340;&#36870;&#21512;&#25104;&#26032;&#39062;&#34920;&#36848;&#65292;&#20197;&#32771;&#34385;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36138;&#23146;&#31639;&#27861;&#31216;&#20026; Retro-fallback&#65292;&#26368;&#22823;&#21270;&#33267;&#23569;&#26377;&#19968;&#31181;&#21512;&#25104;&#35745;&#21010;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#25191;&#34892;&#30340;&#27010;&#29575;&#12290;&#20351;&#29992;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126; Retro-fallback &#36890;&#24120;&#29983;&#25104;&#27604;&#27969;&#34892;&#30340; MCTS &#21644; retro* &#31639;&#27861;&#26356;&#22909;&#30340;&#19968;&#32452;&#21512;&#25104;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#34920;&#26684;&#35843;&#20248;"&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#30495;&#23454;&#34920;&#26684;&#20013;&#21512;&#25104;&#30340;&#22810;&#26679;&#21270;&#34920;&#26684;&#20219;&#21153;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;/&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#20854;&#29702;&#35299;&#34920;&#26684;&#21644;&#25191;&#34892;&#34920;&#26684;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09263</link><description>&lt;p&gt;
Table-GPT: &#38024;&#23545;&#22810;&#26679;&#34920;&#26684;&#20219;&#21153;&#30340;&#34920;&#26684;&#35843;&#20248;GPT
&lt;/p&gt;
&lt;p&gt;
Table-GPT: Table-tuned GPT for Diverse Table Tasks. (arXiv:2310.09263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#34920;&#26684;&#35843;&#20248;"&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#30495;&#23454;&#34920;&#26684;&#20013;&#21512;&#25104;&#30340;&#22810;&#26679;&#21270;&#34920;&#26684;&#20219;&#21153;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;/&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#20854;&#29702;&#35299;&#34920;&#26684;&#21644;&#25191;&#34892;&#34920;&#26684;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#21644;ChatGPT&#65292;&#23637;&#29616;&#20986;&#20102;&#36981;&#24490;&#22810;&#31181;&#20154;&#31867;&#25351;&#20196;&#21644;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#30340;&#38750;&#20961;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#19968;&#31995;&#21015;&#22522;&#26412;&#30340;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#26469;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#20170;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#28041;&#21450;&#34920;&#26684;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#19981;&#22815;&#20248;&#31168;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#22312;\emph{&#19968;&#32500;}&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32780;&#20851;&#31995;&#34920;&#26159;\emph{&#20108;&#32500;}&#23545;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;\emph{&#34920;&#26684;&#35843;&#20248;}&#8221;&#33539;&#24335;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#20174;&#30495;&#23454;&#34920;&#26684;&#20013;&#21512;&#25104;&#30340;&#22810;&#26679;&#21270;&#34920;&#26684;&#20219;&#21153;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#32487;&#32493;&#23545;GPT-3.5&#21644;ChatGPT&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;/&#24494;&#35843;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#34920;&#26684;&#21644;&#25191;&#34892;&#34920;&#26684;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;Table-GPT&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;\emph{&#34920;&#26684;&#29702;&#35299;}&#33021;&#21147;&#65292;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110;&#26222;&#36890;&#30340;GPT-3.5&#21644;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However, when probing language models using a range of basic table-understanding tasks, we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on \emph{one-dimensional} natural-language texts, whereas relational tables are \emph{two-dimensional} objects.  In this work, we propose a new "\emph{table-tuning}" paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, with the goal of enhancing language models' ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better \emph{table-understanding} capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of tabl
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#27169;&#22411;&#38598;&#21512;&#20013;&#65292;&#23545;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#26679;&#26412;&#28857;&#65292;&#20559;&#24046;&#21644;&#26041;&#24046;&#22312;&#26679;&#26412;&#32423;&#21035;&#19978;&#26159;&#23545;&#40784;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.09250</link><description>&lt;p&gt;
&#23427;&#26159;&#19968;&#31181;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#26435;&#34913;&#65306;&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
It's an Alignment, Not a Trade-off: Revisiting Bias and Variance in Deep Models. (arXiv:2310.09250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09250
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#27169;&#22411;&#38598;&#21512;&#20013;&#65292;&#23545;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#26679;&#26412;&#28857;&#65292;&#20559;&#24046;&#21644;&#26041;&#24046;&#22312;&#26679;&#26412;&#32423;&#21035;&#19978;&#26159;&#23545;&#40784;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26234;&#24935;&#35748;&#20026;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#20998;&#35299;&#20026;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#24182;&#19988;&#36825;&#20004;&#20010;&#26415;&#35821;&#20043;&#38388;&#23384;&#22312;&#30528;"&#26435;&#34913;"&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#27169;&#22411;&#38598;&#21512;&#20013;&#65292;&#20559;&#24046;&#21644;&#26041;&#24046;&#22312;&#26679;&#26412;&#32423;&#21035;&#19978;&#26159;"&#23545;&#40784;"&#30340;&#65292;&#20854;&#20013;&#23545;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#26679;&#26412;&#28857;&#65292;&#22343;&#26041;&#20559;&#24046;&#22823;&#32422;&#31561;&#20110;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#26469;&#35777;&#23454;&#36825;&#19968;&#29616;&#35937;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#26657;&#20934;&#21644;&#31070;&#32463;&#23849;&#28291;&#30340;&#20004;&#20010;&#29702;&#35770;&#35270;&#35282;&#30740;&#31350;&#20102;&#35813;&#29616;&#35937;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#22312;&#27169;&#22411;&#33391;&#22909;&#26657;&#20934;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35266;&#23519;&#21040;&#20559;&#24046;-&#26041;&#24046;&#30340;&#23545;&#40784;&#12290;&#20854;&#27425;&#65292;&#22312;&#31070;&#32463;&#23849;&#28291;&#29702;&#35770;&#25552;&#20379;&#30340;&#22270;&#26223;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20559;&#24046;&#21644;&#26041;&#24046;&#20043;&#38388;&#30340;&#36817;&#20284;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical wisdom in machine learning holds that the generalization error can be decomposed into bias and variance, and these two terms exhibit a \emph{trade-off}. However, in this paper, we show that for an ensemble of deep learning based classification models, bias and variance are \emph{aligned} at a sample level, where squared bias is approximately \emph{equal} to variance for correctly classified sample points. We present empirical evidence confirming this phenomenon in a variety of deep learning models and datasets. Moreover, we study this phenomenon from two theoretical perspectives: calibration and neural collapse. We first show theoretically that under the assumption that the models are well calibrated, we can observe the bias-variance alignment. Second, starting from the picture provided by the neural collapse theory, we show an approximate correlation between bias and variance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#29983;&#25104;&#35774;&#35745;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#35770;&#21453;&#24605;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#26469;&#22686;&#24378;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#22797;&#26434;&#35774;&#35745;&#31354;&#38388;&#30340;&#26144;&#23556;&#21644;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2310.09243</link><description>&lt;p&gt;
&#22686;&#24378;&#35745;&#31639;&#35774;&#35745;&#65306;&#22312;&#29983;&#25104;&#35774;&#35745;&#20013;&#31995;&#32479;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Augmented Computational Design: Methodical Application of Artificial Intelligence in Generative Design. (arXiv:2310.09243v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#29983;&#25104;&#35774;&#35745;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#35770;&#21453;&#24605;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#26469;&#22686;&#24378;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#22797;&#26434;&#35774;&#35745;&#31354;&#38388;&#30340;&#26144;&#23556;&#21644;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#33410;&#20171;&#32461;&#20102;&#20851;&#20110;&#22312;&#29983;&#25104;&#35774;&#35745;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#26041;&#27861;&#35770;&#21453;&#24605;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#31456;&#33410;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#29983;&#25104;&#35774;&#35745;&#36807;&#31243;&#65292;&#20197;&#22312;&#22788;&#29702;&#25968;&#30334;&#25110;&#25968;&#21315;&#20010;&#23567;&#20915;&#31574;&#30340;&#21516;&#26102;&#23454;&#29616;&#19968;&#20123;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#25110;&#24615;&#33021;&#25351;&#26631;&#12290;&#24615;&#33021;&#20026;&#22522;&#30784;&#30340;&#29983;&#25104;&#35774;&#35745;&#33539;&#24335;&#30340;&#26680;&#24515;&#22312;&#20110;&#22312;&#36825;&#20123;&#36873;&#25321;&#21644;&#21518;&#26524;&#20043;&#38388;&#24314;&#31435;&#32479;&#35745;&#25110;&#20223;&#30495;&#39537;&#21160;&#30340;&#20851;&#32852;&#65292;&#20197;&#22312;&#22797;&#26434;&#20915;&#31574;&#31354;&#38388;&#20013;&#36827;&#34892;&#26144;&#23556;&#21644;&#23548;&#33322;&#12290;&#26412;&#31456;&#33410;&#23558;&#35752;&#35770;&#20154;&#24037;&#26234;&#33021;&#22312;&#22686;&#24378;&#24314;&#31569;&#35774;&#35745;&#20013;&#20915;&#31574;&#36807;&#31243;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#35774;&#35745;&#31354;&#38388;&#30340;&#26144;&#23556;&#21644;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter presents methodological reflections on the necessity and utility of artificial intelligence in generative design. Specifically, the chapter discusses how generative design processes can be augmented by AI to deliver in terms of a few outcomes of interest or performance indicators while dealing with hundreds or thousands of small decisions. The core of the performance-based generative design paradigm is about making statistical or simulation-driven associations between these choices and consequences for mapping and navigating such a complex decision space. This chapter will discuss promising directions in Artificial Intelligence for augmenting decision-making processes in architectural design for mapping and navigating complex design spaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#22303;&#33879;&#20154;&#22312;&#19981;&#21516;&#35282;&#33394;&#19979;&#34920;&#29616;&#30340;&#24773;&#22659;&#26102;&#30340;&#33258;&#35748;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#25216;&#26415;&#23545;&#22303;&#33879;&#20154;&#30456;&#20851;&#31038;&#20250;&#20559;&#35265;&#30340;&#24863;&#30693;&#21644;&#28508;&#22312;&#25918;&#22823;&#65292;&#23545;&#22303;&#33879;&#20154;&#22312;&#37325;&#35201;&#35745;&#31639;&#20013;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.09237</link><description>&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#23545;&#22303;&#33879;&#20154;&#35748;&#30693;&#30340;&#30740;&#31350;: &#23545;ChatGPT&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#23545;&#22303;&#33879;&#35282;&#33394;&#35748;&#30693;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating Machine Perception of Indigeneity: An Analysis of ChatGPT's Perceptions of Indigenous Roles in Diverse Scenarios. (arXiv:2310.09237v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#22303;&#33879;&#20154;&#22312;&#19981;&#21516;&#35282;&#33394;&#19979;&#34920;&#29616;&#30340;&#24773;&#22659;&#26102;&#30340;&#33258;&#35748;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#25216;&#26415;&#23545;&#22303;&#33879;&#20154;&#30456;&#20851;&#31038;&#20250;&#20559;&#35265;&#30340;&#24863;&#30693;&#21644;&#28508;&#22312;&#25918;&#22823;&#65292;&#23545;&#22303;&#33879;&#20154;&#22312;&#37325;&#35201;&#35745;&#31639;&#20013;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#26159;&#22522;&#20110;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#24037;&#20855;&#65292;&#21453;&#26144;&#20102;&#22810;&#26679;&#21270;&#31038;&#20250;&#21360;&#35937;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;LLMs&#22312;&#27169;&#25311;&#22303;&#33879;&#20154;&#22312;&#19981;&#21516;&#35282;&#33394;&#19979;&#34920;&#29616;&#30340;&#24773;&#22659;&#26102;&#30340;&#33258;&#35748;&#20559;&#35265;&#12290;&#36890;&#36807;&#29983;&#25104;&#21644;&#20998;&#26512;&#22810;&#20010;&#24773;&#22659;&#65292;&#26412;&#30740;&#31350;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#20102;&#35299;&#25216;&#26415;&#22914;&#20309;&#24863;&#30693;&#21644;&#28508;&#22312;&#25918;&#22823;&#19982;&#22303;&#33879;&#20154;&#30456;&#20851;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#22303;&#33879;&#20154;&#22312;&#37325;&#35201;&#35745;&#31639;&#20013;&#30340;&#24191;&#27867;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), like ChatGPT, are fundamentally tools trained on vast data, reflecting diverse societal impressions. This paper aims to investigate LLMs' self-perceived bias concerning indigeneity when simulating scenarios of indigenous people performing various roles. Through generating and analyzing multiple scenarios, this work offers a unique perspective on how technology perceives and potentially amplifies societal biases related to indigeneity in social computing. The findings offer insights into the broader implications of indigeneity in critical computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;FSBN&#21644;SSBN&#65292;&#23427;&#20204;&#21033;&#29992;&#23616;&#37096;&#25628;&#32034;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#24402;&#32435;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09222</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#24555;&#36895;&#21644;&#39640;&#25928;&#22320;&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;&#65306;&#30693;&#35782;&#21457;&#29616;&#19982;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Fast &amp; Efficient Learning of Bayesian Networks from Data: Knowledge Discovery and Causality. (arXiv:2310.09222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;FSBN&#21644;SSBN&#65292;&#23427;&#20204;&#21033;&#29992;&#23616;&#37096;&#25628;&#32034;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#24402;&#32435;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#23398;&#20064;&#23545;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#21487;&#20197;&#25581;&#31034;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#33021;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#30693;&#35782;&#21457;&#29616;&#12289;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#20915;&#31574;&#12290;&#22522;&#20110;PC&#31639;&#27861;&#30340;&#20004;&#20010;&#26032;&#31639;&#27861;FSBN&#21644;SSBN&#65292;&#37319;&#29992;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32593;&#32476;&#32467;&#26500;&#12290;&#23427;&#20204;&#21033;&#29992;d-&#20998;&#31163;&#26469;&#25512;&#26029;&#39069;&#22806;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#20248;&#20808;&#32771;&#34385;&#26465;&#20214;&#38598;&#65292;&#24182;&#39640;&#25928;&#22320;&#32456;&#27490;&#25628;&#32034;&#12290;FSBN&#23454;&#29616;&#20102;&#39640;&#36798;52%&#30340;&#35745;&#31639;&#25104;&#26412;&#20943;&#23569;&#65292;&#32780;SSBN&#22312;200&#20010;&#33410;&#28857;&#30340;&#32593;&#32476;&#20013;&#36229;&#36807;&#20102;&#23427;&#65292;&#20943;&#23569;&#20102;72%&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#30001;&#20110;&#26234;&#33021;&#31574;&#30053;&#65292;SSBN&#23637;&#31034;&#20102;&#36827;&#19968;&#27493;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#19982;PC&#31639;&#27861;&#30340;&#24402;&#32435;&#36136;&#37327;&#30456;&#21305;&#37197;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20351;&#23427;&#20204;&#20855;&#26377;&#24456;&#39640;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure learning is essential for Bayesian networks (BNs) as it uncovers causal relationships, and enables knowledge discovery, predictions, inferences, and decision-making under uncertainty. Two novel algorithms, FSBN and SSBN, based on the PC algorithm, employ local search strategy and conditional independence tests to learn the causal network structure from data. They incorporate d-separation to infer additional topology information, prioritize conditioning sets, and terminate the search immediately and efficiently. FSBN achieves up to 52% computation cost reduction, while SSBN surpasses it with a remarkable 72% reduction for a 200-node network. SSBN demonstrates further efficiency gains due to its intelligent strategy. Experimental studies show that both algorithms match the induction quality of the PC algorithm while significantly reducing computation costs. This enables them to offer interpretability and adaptability while reducing the computational burden, making them valuable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.09219</link><description>&lt;p&gt;
"&#20975;&#21033;&#26159;&#19968;&#20010;&#28201;&#26262;&#30340;&#20154;&#65292;&#32422;&#29791;&#22827;&#26159;&#19968;&#20010;&#27036;&#26679;": LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#29992;&#25143;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21327;&#21161;&#25776;&#20889;&#21508;&#31181;&#31867;&#22411;&#30340;&#20869;&#23481;&#65292;&#21253;&#25324;&#25512;&#33616;&#20449;&#31561;&#32844;&#19994;&#25991;&#20214;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#26041;&#20415;&#24615;&#65292;&#20294;&#36825;&#20123;&#24212;&#29992;&#24341;&#20837;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#30001;&#20110;&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#21487;&#33021;&#34987;&#29992;&#25143;&#30452;&#25509;&#22312;&#32844;&#19994;&#25110;&#23398;&#26415;&#22330;&#26223;&#20013;&#20351;&#29992;&#65292;&#23427;&#20204;&#26377;&#21487;&#33021;&#36896;&#25104;&#30452;&#25509;&#30340;&#31038;&#20250;&#20260;&#23475;&#65292;&#22914;&#38477;&#20302;&#22899;&#24615;&#30003;&#35831;&#32773;&#30340;&#25104;&#21151;&#29575;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#26469;&#30340;&#32531;&#35299;&#21644;&#30417;&#25511;&#65292;&#20840;&#38754;&#30740;&#31350;&#27492;&#31867;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#21644;&#30456;&#20851;&#20260;&#23475;&#21183;&#22312;&#24517;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#30740;&#31350;&#12290;&#21463;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#32500;&#24230;&#26469;&#23637;&#29616;LLM&#29983;&#25104;&#30340;&#20449;&#20214;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#35821;&#35328;&#39118;&#26684;&#30340;&#20559;&#35265;&#21644;&#35789;&#27719;&#20869;&#23481;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25512;&#33616;&#20449;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative language models advance, users have started to utilize Large Language Models (LLMs) to assist in writing various types of content, including professional documents such as recommendation letters. Despite their convenience, these applications introduce unprecedented fairness concerns. As generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. Therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. In this paper, we critically examine gender bias in LLM-generated reference letters. Inspired by findings in social science, we design evaluation methods to manifest gender biases in LLM-generated letters through 2 dimensions: biases in language style and biases in lexical content. Furthermore, we investigate the ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#22269;&#20154;&#24037;&#26234;&#33021;&#21327;&#20250;&#65288;MAGIC&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#20840;&#29699;&#31105;&#20196;&#20943;&#36731;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#20154;&#31867;&#23384;&#22312;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#20801;&#35768;&#29421;&#20041;AI&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.09217</link><description>&lt;p&gt;
&#36328;&#22269;&#20154;&#24037;&#26234;&#33021;&#21327;&#20250;&#65288;MAGIC&#65289;&#65306;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#22269;&#38469;&#21327;&#35843;&#30340;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
Multinational AGI Consortium (MAGIC): A Proposal for International Coordination on AI. (arXiv:2310.09217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#22269;&#20154;&#24037;&#26234;&#33021;&#21327;&#20250;&#65288;MAGIC&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#20840;&#29699;&#31105;&#20196;&#20943;&#36731;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#20154;&#31867;&#23384;&#22312;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#20801;&#35768;&#29421;&#20041;AI&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#22269;&#20154;&#24037;&#26234;&#33021;&#21327;&#20250;&#65288;MAGIC&#65289;&#65292;&#26088;&#22312;&#20943;&#36731;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#20154;&#31867;&#23384;&#22312;&#30340;&#39118;&#38505;&#12290;MAGIC&#23558;&#25104;&#20026;&#20840;&#29699;&#21807;&#19968;&#34987;&#20801;&#35768;&#24320;&#21457;&#20808;&#36827;AI&#30340;&#26426;&#26500;&#65292;&#24182;&#36890;&#36807;&#20854;&#31614;&#32422;&#25104;&#21592;&#23545;&#20854;&#20182;&#25152;&#26377;&#20808;&#36827;AI&#24320;&#21457;&#23454;&#26045;&#20840;&#29699;&#31105;&#20196;&#36827;&#34892;&#24378;&#21046;&#25191;&#34892;&#12290;MAGIC&#23558;&#26159;&#19968;&#20010;&#29420;&#23478;&#30340;&#12289;&#27880;&#37325;&#23433;&#20840;&#30340;&#12289;&#39640;&#24230;&#23433;&#20840;&#30340;&#26426;&#26500;&#65292;&#30001;&#25104;&#21592;&#22269;&#20849;&#21516;&#25903;&#25345;&#65292;&#24182;&#20197;&#20844;&#24179;&#20998;&#37197;&#30340;&#21407;&#21017;&#26469;&#33719;&#24471;&#21033;&#30410;&#12290;MAGIC&#23558;&#20801;&#35768;&#29421;&#20041;AI&#27169;&#22411;&#36805;&#36895;&#21457;&#23637;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#36890;&#29992;&#31995;&#32479;&#20013;&#30340;&#19981;&#21305;&#37197;&#12289;&#27969;&#31388;&#12289;&#31361;&#30772;&#25110;&#22833;&#25511;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#19981;&#35752;&#35770;&#23454;&#26045;&#31105;&#20196;&#30340;&#25919;&#27835;&#21487;&#34892;&#24615;&#65292;&#20063;&#19981;&#35752;&#35770;&#23454;&#26045;&#31105;&#27490;&#39640;&#23481;&#37327;AGI&#35757;&#32451;&#30340;&#20855;&#20307;&#31435;&#27861;&#31574;&#30053;&#21644;&#35268;&#21017;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31215;&#26497;&#30340;&#26410;&#26469;&#24895;&#26223;&#65292;&#21363;MAGIC&#20316;&#20026;&#20840;&#29699;&#27835;&#29702;&#26426;&#21046;&#65292;&#21487;&#20197;&#20026;&#38271;&#26399;&#30340;&#21457;&#23637;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Multinational Artificial General Intelligence Consortium (MAGIC) to mitigate existential risks from advanced artificial intelligence (AI). MAGIC would be the only institution in the world permitted to develop advanced AI, enforced through a global moratorium by its signatory members on all other advanced AI development. MAGIC would be exclusive, safety-focused, highly secure, and collectively supported by member states, with benefits distributed equitably among signatories. MAGIC would allow narrow AI models to flourish while significantly reducing the possibility of misaligned, rogue, breakout, or runaway outcomes of general-purpose systems. We do not address the political feasibility of implementing a moratorium or address the specific legislative strategies and rules needed to enforce a ban on high-capacity AGI training runs. Instead, we propose one positive vision of the future, where MAGIC, as a global governance regime, can lay the groundwork for long-term, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SiamAF&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#36890;&#36807;Siamese&#32593;&#32476;&#21644;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.09203</link><description>&lt;p&gt;
SiamAF: &#23398;&#20064;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#29992;&#20110;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SiamAF: Learning Shared Information from ECG and PPG Signals for Robust Atrial Fibrillation Detection. (arXiv:2310.09203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09203
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SiamAF&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#36890;&#36807;Siamese&#32593;&#32476;&#21644;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#33039;&#24515;&#24459;&#22833;&#24120;&#31867;&#22411;&#65292;&#19982;&#20013;&#39118;&#12289;&#24515;&#21147;&#34928;&#31469;&#21644;&#20854;&#20182;&#24515;&#34880;&#31649;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#22686;&#21152;&#26377;&#20851;&#65292;&#20294;&#21487;&#20197;&#20020;&#24202;&#19978;&#26080;&#22768;&#12290;&#20329;&#25140;&#24335;&#35774;&#22791;&#36827;&#34892;&#34987;&#21160;&#24615;&#30340;AF&#30417;&#27979;&#21487;&#33021;&#26377;&#21161;&#20110;&#20943;&#23569;&#19982;AF&#30456;&#20851;&#30340;&#19981;&#33391;&#20020;&#24202;&#32467;&#26524;&#12290;&#22312;&#22024;&#26434;&#30340;&#20329;&#25140;&#24335;&#25968;&#25454;&#20013;&#26816;&#27979;AF&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#24341;&#21457;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#20808;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21333;&#19968;&#24418;&#24577;&#23398;&#20064;&#65292;&#35201;&#20040;&#26159;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#65292;&#35201;&#20040;&#26159;&#20809;&#30005;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#29305;&#24449;&#65292;&#24182;&#20381;&#36182;&#20110;&#26356;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#25439;&#22351;&#30340;&#29305;&#24449;&#65292;&#22312;&#26576;&#20123;&#22330;&#26223;&#20013;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36136;&#37327;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#12290;&#37492;&#20110;&#20329;&#25140;&#24335;&#35774;&#22791;&#21644;&#24202;&#36793;&#30417;&#25252;&#20202;&#19978;ECG&#21644;PPG&#20449;&#21495;&#37197;&#23545;&#30340;&#26085;&#30410;&#20016;&#23500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;SiamAF&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;Siamese&#32593;&#32476;&#32467;&#26500;&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Atrial fibrillation (AF) is the most common type of cardiac arrhythmia. It is associated with an increased risk of stroke, heart failure, and other cardiovascular complications, but can be clinically silent. Passive AF monitoring with wearables may help reduce adverse clinical outcomes related to AF. Detecting AF in noisy wearable data poses a significant challenge, leading to the emergence of various deep learning techniques. Previous deep learning models learn from a single modality, either electrocardiogram (ECG) or photoplethysmography (PPG) signals. However, deep learning models often struggle to learn generalizable features and rely on features that are more susceptible to corruption from noise, leading to sub-optimal performances in certain scenarios, especially with low-quality signals. Given the increasing availability of ECG and PPG signal pairs from wearables and bedside monitors, we propose a new approach, SiamAF, leveraging a novel Siamese network architecture and joint le
&lt;/p&gt;</description></item><item><title>Tikuna&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#65292;&#29992;&#20110;&#30417;&#27979;&#21644;&#26816;&#27979;&#20197;&#22826;&#22346;&#21306;&#22359;&#38142;P2P&#32593;&#32476;&#19978;&#30340;&#28508;&#22312;&#25915;&#20987;&#12290;&#23427;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;LSTM&#26041;&#27861;&#65292;&#36890;&#36807;&#25910;&#38598;&#25968;&#25454;&#21644;&#20998;&#26512;&#32593;&#32476;&#34892;&#20026;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#25915;&#20987;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.09193</link><description>&lt;p&gt;
Tikuna: &#19968;&#31181;&#20197;&#22826;&#22346;&#21306;&#22359;&#38142;&#32593;&#32476;&#23433;&#20840;&#30417;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Tikuna: An Ethereum Blockchain Network Security Monitoring System. (arXiv:2310.09193v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09193
&lt;/p&gt;
&lt;p&gt;
Tikuna&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#65292;&#29992;&#20110;&#30417;&#27979;&#21644;&#26816;&#27979;&#20197;&#22826;&#22346;&#21306;&#22359;&#38142;P2P&#32593;&#32476;&#19978;&#30340;&#28508;&#22312;&#25915;&#20987;&#12290;&#23427;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;LSTM&#26041;&#27861;&#65292;&#36890;&#36807;&#25910;&#38598;&#25968;&#25454;&#21644;&#20998;&#26512;&#32593;&#32476;&#34892;&#20026;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#25915;&#20987;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21306;&#22359;&#38142;&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#30340;&#24433;&#21709;&#36880;&#28176;&#25193;&#22823;&#65292;&#21306;&#22359;&#38142;&#23433;&#20840;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#20445;&#25252;&#21306;&#22359;&#38142;&#30340;&#26368;&#24213;&#23618;&#23618;&#32423;&#65292;&#29305;&#21035;&#26159;&#20801;&#35768;&#33410;&#28857;&#36827;&#34892;&#36890;&#20449;&#21644;&#20849;&#20139;&#20449;&#24687;&#30340;P2P&#32593;&#32476;&#12290;P2P&#32593;&#32476;&#23618;&#21487;&#33021;&#23481;&#26131;&#36973;&#21463;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#12289;&#26085;&#39135;&#25915;&#20987;&#25110;Sybil&#25915;&#20987;&#31561;&#22810;&#31181;&#25915;&#20987;&#12290;&#36825;&#19968;&#23618;&#38754;&#26131;&#21463;&#20256;&#32479;P2P&#32593;&#32476;&#20013;&#30340;&#23041;&#32961;&#65292;&#22240;&#27492;&#38656;&#35201;&#36890;&#36807;&#25910;&#38598;&#25968;&#25454;&#24182;&#20174;&#32593;&#32476;&#34892;&#20026;&#20013;&#25552;&#21462;&#27934;&#23519;&#26469;&#38477;&#20302;&#36825;&#20123;&#39118;&#38505;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Tikuna&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#20197;&#22826;&#22346;&#21306;&#22359;&#38142;P2P&#32593;&#32476;&#36827;&#34892;&#26089;&#26399;&#30417;&#27979;&#21644;&#26816;&#27979;&#28508;&#22312;&#25915;&#20987;&#30340;&#24320;&#28304;&#24037;&#20855;&#12290;Tikuna&#37319;&#29992;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#26080;&#30417;&#30563;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26041;&#27861;&#26469;&#26816;&#27979;&#25915;&#20987;&#24182;&#35686;&#25253;&#29992;&#25143;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#25915;&#20987;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain security is becoming increasingly relevant in today's cyberspace as it extends its influence in many industries. This paper focuses on protecting the lowest level layer in the blockchain, particularly the P2P network that allows the nodes to communicate and share information. The P2P network layer may be vulnerable to several families of attacks, such as Distributed Denial of Service (DDoS), eclipse attacks, or Sybil attacks. This layer is prone to threats inherited from traditional P2P networks, and it must be analyzed and understood by collecting data and extracting insights from the network behavior to reduce those risks. We introduce Tikuna, an open-source tool for monitoring and detecting potential attacks on the Ethereum blockchain P2P network, at an early stage. Tikuna employs an unsupervised Long Short-Term Memory (LSTM) method based on Recurrent Neural Network (RNN) to detect attacks and alert users. Empirical results indicate that the proposed approach significantl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#21387;&#32553;&#26041;&#27861;&#20013;&#23545;&#32467;&#26500;&#20449;&#24687;&#30340;&#24573;&#35270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#24191;&#25773;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#26696;&#65288;SGDD&#65289;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20445;&#30041;&#21407;&#22987;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;&#36328;&#26550;&#26500;&#27867;&#21270;&#21644;&#29305;&#23450;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09192</link><description>&lt;p&gt;
&#22270;&#29366;&#21387;&#32553;&#65306;&#26159;&#21542;&#30475;&#36215;&#26469;&#20687;&#35270;&#35273;&#25968;&#25454;&#38598;&#65311;(arXiv:2310.09192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Does Graph Distillation See Like Vision Dataset Counterpart?. (arXiv:2310.09192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#21387;&#32553;&#26041;&#27861;&#20013;&#23545;&#32467;&#26500;&#20449;&#24687;&#30340;&#24573;&#35270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#24191;&#25773;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#26696;&#65288;SGDD&#65289;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20445;&#30041;&#21407;&#22987;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;&#36328;&#26550;&#26500;&#27867;&#21270;&#21644;&#29305;&#23450;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#23545;&#22823;&#35268;&#27169;&#22270;&#36827;&#34892;&#35757;&#32451;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#25104;&#26412;&#21644;&#23384;&#20648;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#22270;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20248;&#21270;&#21387;&#32553;&#22270;&#30340;&#29305;&#24449;&#30697;&#38453;&#65292;&#32780;&#24573;&#35270;&#20102;&#21407;&#22987;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35843;&#26597;&#32467;&#26500;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20174;&#35889;&#22495;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#30830;&#23450;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#37325;&#35201;&#30340;&#25289;&#26222;&#25289;&#26031;&#33021;&#37327;&#20998;&#24067;&#65288;LED&#65289;&#30340;&#20559;&#31227;&#12290;&#36825;&#31181;&#20559;&#31227;&#23548;&#33268;&#22312;&#36328;&#26550;&#26500;&#27867;&#21270;&#21644;&#29305;&#23450;&#20219;&#21153;&#65288;&#21253;&#25324;&#24322;&#24120;&#26816;&#27979;&#21644;&#38142;&#25509;&#39044;&#27979;&#65289;&#20013;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24191;&#25773;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#65288;SGDD&#65289;&#26041;&#26696;&#65292;&#23558;&#21407;&#22987;&#32467;&#26500;&#20449;&#24687;&#24191;&#25773;&#21040;&#21512;&#25104;&#22270;&#30340;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#26174;&#24335;&#22320;&#38450;&#27490;&#24573;&#35270;&#21407;&#22987;&#32467;&#26500;&#20449;&#24687;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;SGDD&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20855;&#26377;&#20445;&#30041;&#21407;&#22987;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have attracted increasing concerns. Existing graph condensation methods primarily focus on optimizing the feature matrices of condensed graphs while overlooking the impact of the structure information from the original graphs. To investigate the impact of the structure information, we conduct analysis from the spectral domain and empirically identify substantial Laplacian Energy Distribution (LED) shifts in previous works. Such shifts lead to poor performance in cross-architecture generalization and specific tasks, including anomaly detection and link prediction. In this paper, we propose a novel Structure-broadcasting Graph Dataset Distillation (SGDD) scheme for broadcasting the original structure information to the generation of the synthetic one, which explicitly prevents overlooking the original structure information. Theoretically, the synthetic graphs by SGDD 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRIOR&#30340;&#20010;&#24615;&#21270;&#20808;&#39564;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20840;&#23616;&#27169;&#22411;&#24573;&#35270;&#29305;&#23450;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#20351;&#29992;&#24102;Bregman&#25955;&#24230;&#30340;PFL&#26694;&#26550;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#35299;&#32806;&#65292;&#20197;&#25552;&#39640;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09183</link><description>&lt;p&gt;
PRIOR: &#20010;&#24615;&#21270;&#20808;&#39564;&#29992;&#20110;&#37325;&#26032;&#28608;&#27963;&#32852;&#37030;&#23398;&#20064;&#20013;&#34987;&#24573;&#35270;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning. (arXiv:2310.09183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRIOR&#30340;&#20010;&#24615;&#21270;&#20808;&#39564;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20840;&#23616;&#27169;&#22411;&#24573;&#35270;&#29305;&#23450;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#20351;&#29992;&#24102;Bregman&#25955;&#24230;&#30340;PFL&#26694;&#26550;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#35299;&#32806;&#65292;&#20197;&#25552;&#39640;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#24322;&#36136;&#25968;&#25454;&#29305;&#24615;&#38477;&#20302;&#20102;&#23616;&#37096;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#36890;&#36807;&#20174;&#20840;&#23616;&#27169;&#22411;&#20013;&#21512;&#25104;&#20010;&#24615;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20840;&#23616;&#27169;&#22411;&#21487;&#33021;&#24573;&#35270;&#20102;&#23458;&#25143;&#31471;&#34987;&#37319;&#26679;&#30340;&#29305;&#23450;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20840;&#23616;&#27169;&#22411;&#20013;&#65292;&#35797;&#22270;&#20943;&#36731;PFL&#20013;&#24341;&#20837;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21363;&#24102;Bregman&#25955;&#24230;&#65288;pFedBreD&#65289;&#30340;PFL&#65292;&#36890;&#36807;&#22312;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#20351;&#29992;Bregman&#25955;&#24230;&#27491;&#21017;&#21270;&#30340;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#20043;&#35299;&#32806;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#36824;&#25918;&#26494;&#20102;&#38236;&#20687;&#19979;&#38477;&#65288;RMD&#65289;&#65292;&#20197;&#26174;&#24335;&#22320;&#25552;&#21462;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#20379;&#21487;&#36873;&#25321;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical federated learning (FL) enables training machine learning models without sharing data for privacy preservation, but heterogeneous data characteristic degrades the performance of the localized model. Personalized FL (PFL) addresses this by synthesizing personalized models from a global model via training on local data. Such a global model may overlook the specific information that the clients have been sampled. In this paper, we propose a novel scheme to inject personalized prior knowledge into the global model in each client, which attempts to mitigate the introduced incomplete information problem in PFL. At the heart of our proposed approach is a framework, the PFL with Bregman Divergence (pFedBreD), decoupling the personalized prior from the local objective function regularized by Bregman divergence for greater adaptability in personalized scenarios. We also relax the mirror descent (RMD) to extract the prior explicitly to provide optional strategies. Additionally, our pFed
&lt;/p&gt;</description></item><item><title>mnmDTW&#26159;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#25668;&#20687;&#22836;&#30340;&#36816;&#21160;&#35823;&#24046;&#23450;&#20301;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#25299;&#23637;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#27979;&#37327;&#36816;&#21160;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#28165;&#26970;&#22320;&#26174;&#31034;&#12289;&#35782;&#21035;&#21644;&#23450;&#20301;&#36816;&#21160;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.09170</link><description>&lt;p&gt;
mnmDTW:&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#25668;&#20687;&#22836;&#30340;&#36816;&#21160;&#35823;&#24046;&#23450;&#20301;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
mnmDTW: An extension to Dynamic Time Warping for Camera-based Movement Error Localization. (arXiv:2310.09170v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09170
&lt;/p&gt;
&lt;p&gt;
mnmDTW&#26159;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#25668;&#20687;&#22836;&#30340;&#36816;&#21160;&#35823;&#24046;&#23450;&#20301;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#25299;&#23637;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#27979;&#37327;&#36816;&#21160;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#28165;&#26970;&#22320;&#26174;&#31034;&#12289;&#35782;&#21035;&#21644;&#23450;&#20301;&#36816;&#21160;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#27010;&#24565;&#39564;&#35777;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#20174;&#36816;&#21160;&#35270;&#39057;&#20013;&#25552;&#21462;&#23039;&#21183;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;(DTW)&#26469;&#35745;&#31639;&#19982;&#40644;&#37329;&#26631;&#20934;&#36816;&#21160;&#25191;&#34892;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35745;&#31639;&#27599;&#20010;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20197;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#36816;&#21160;&#20934;&#30830;&#24615;&#27979;&#37327;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36825;&#20010;&#24230;&#37327;&#25351;&#26631;&#28165;&#26970;&#22320;&#30475;&#21040;&#12289;&#35782;&#21035;&#21644;&#23450;&#20301;&#36816;&#21160;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this proof of concept, we use Computer Vision (CV) methods to extract pose information out of exercise videos. We then employ a modified version of Dynamic Time Warping (DTW) to calculate the deviation from a gold standard execution of the exercise. Specifically, we calculate the distance between each body part individually to get a more precise measure for exercise accuracy. We can show that exercise mistakes are clearly visible, identifiable and localizable through this metric.
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35299;&#20915;&#27668;&#20505;&#21464;&#21270;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#33021;&#28304;&#31995;&#32479;&#12289;&#27668;&#20505;&#25968;&#25454;&#39044;&#27979;&#12289;&#27668;&#20505;&#30417;&#27979;&#21644;&#21361;&#38505;&#20107;&#20214;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#30446;&#21069;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12289;&#23616;&#38480;&#20197;&#21450;&#28508;&#22312;&#26426;&#20250;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.09162</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#27668;&#20505;&#21464;&#21270;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning in Climate Change and Sustainability: a Review. (arXiv:2310.09162v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09162
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35299;&#20915;&#27668;&#20505;&#21464;&#21270;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#33021;&#28304;&#31995;&#32479;&#12289;&#27668;&#20505;&#25968;&#25454;&#39044;&#27979;&#12289;&#27668;&#20505;&#30417;&#27979;&#21644;&#21361;&#38505;&#20107;&#20214;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#30446;&#21069;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12289;&#23616;&#38480;&#20197;&#21450;&#28508;&#22312;&#26426;&#20250;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#21450;&#20854;&#23545;&#20840;&#29699;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#24433;&#21709;&#26159;&#20851;&#38190;&#25361;&#25112;&#65292;&#38656;&#35201;&#32467;&#21512;&#23574;&#31471;&#25216;&#26415;&#21644;&#31185;&#23398;&#35265;&#35299;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#21147;&#37327;&#26469;&#35299;&#20915;&#21253;&#25324;&#27668;&#20505;&#21464;&#21270;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#22312;&#20869;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#35299;&#20915;&#27668;&#20505;&#21464;&#21270;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#38382;&#39064;&#30340;&#29616;&#26377;&#25991;&#29486;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#26377;&#28508;&#21147;&#21152;&#36895;&#20943;&#30899;&#30340;QML&#26041;&#27861;&#65292;&#21253;&#25324;&#33021;&#28304;&#31995;&#32479;&#12289;&#27668;&#20505;&#25968;&#25454;&#39044;&#27979;&#12289;&#27668;&#20505;&#30417;&#27979;&#21644;&#21361;&#38505;&#20107;&#20214;&#39044;&#27979;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#30446;&#21069;&#30340;&#23616;&#38480;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#27668;&#20505;&#21464;&#21270;&#30740;&#31350;&#36825;&#19968;&#37325;&#35201;&#39046;&#22495;&#21033;&#29992;&#22522;&#20110;QML&#30340;&#26041;&#27861;&#30340;&#28508;&#22312;&#26426;&#20250;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change and its impact on global sustainability are critical challenges, demanding innovative solutions that combine cutting-edge technologies and scientific insights. Quantum machine learning (QML) has emerged as a promising paradigm that harnesses the power of quantum computing to address complex problems in various domains including climate change and sustainability. In this work, we survey existing literature that applies quantum machine learning to solve climate change and sustainability-related problems. We review promising QML methodologies that have the potential to accelerate decarbonization including energy systems, climate data forecasting, climate monitoring, and hazardous events predictions. We discuss the challenges and current limitations of quantum machine learning approaches and provide an overview of potential opportunities and future work to leverage QML-based methods in the important area of climate change research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#35843;&#26597;&#21644;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#31574;&#30053;&#26469;&#36171;&#20104;LLMs&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26356;&#20855;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.09158</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#25945;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning To Teach Large Language Models Logical Reasoning. (arXiv:2310.09158v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#35843;&#26597;&#21644;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#31574;&#30053;&#26469;&#36171;&#20104;LLMs&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26356;&#20855;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#35821;&#35328;&#29983;&#25104;&#21644;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#38382;&#39064;&#65288;&#22914;&#24187;&#35273;&#65289;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#23454;&#38469;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#28982;&#36755;&#20986;&#19981;&#21487;&#38752;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;LLMs&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;LLMs&#22312;&#19981;&#21516;&#20219;&#21153;&#65288;&#21253;&#25324;&#20107;&#20214;&#20851;&#31995;&#25552;&#21462;&#21644;&#28436;&#32462;&#25512;&#29702;&#65289;&#20013;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#35299;&#20915;&#38656;&#35201;&#20005;&#26684;&#25512;&#29702;&#30340;&#20219;&#21153;&#26102;&#24182;&#19981;&#26159;&#24456;&#22909;&#30340;&#25512;&#29702;&#32773;&#65292;&#20250;&#20135;&#29983;&#21453;&#20107;&#23454;&#30340;&#31572;&#26696;&#65292;&#36825;&#35201;&#27714;&#25105;&#20204;&#19981;&#26029;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20840;&#38754;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#36171;&#20104;LLMs&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26356;&#20855;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have gained enormous attention from both academia and industry, due to their exceptional ability in language generation and extremely powerful generalization. However, current LLMs still output unreliable content in practical reasoning tasks due to their inherent issues (e.g., hallucination). To better disentangle this problem, in this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in logical reasoning. More in detail, we first investigate the deficiency of LLMs in logical reasoning on different tasks, including event relation extraction and deductive reasoning. Our study demonstrates that LLMs are not good reasoners in solving tasks with rigorous reasoning and will produce counterfactual answers, which require us to iteratively refine. Therefore, we comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent answers across
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;AI&#21152;&#36895;&#22120;&#21644;&#22788;&#29702;&#22120;&#35843;&#26597;&#30340;&#26356;&#26032;&#65292;&#28085;&#30422;&#36807;&#21435;&#22235;&#24180;&#30340;&#20869;&#23481;&#12290;&#36890;&#36807;&#25910;&#38598;&#24182;&#24635;&#32467;&#30446;&#21069;&#24050;&#20844;&#24320;&#23459;&#24067;&#30340;&#21830;&#29992;&#21152;&#36895;&#22120;&#30340;&#24615;&#33021;&#21644;&#21151;&#32791;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#32472;&#21046;&#22312;&#25955;&#28857;&#22270;&#19978;&#65292;&#20174;&#32780;&#20998;&#26512;&#20102;&#36235;&#21183;&#21644;&#24066;&#22330;&#32454;&#20998;&#65292;&#24182;&#21253;&#25324;&#20102;&#26032;&#22686;&#21152;&#30340;&#21152;&#36895;&#22120;&#30340;&#31616;&#35201;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.09145</link><description>&lt;p&gt;
Lincoln AI&#35745;&#31639;&#35843;&#26597;&#65288;LAICS&#65289;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lincoln AI Computing Survey (LAICS) Update. (arXiv:2310.09145v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09145
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;AI&#21152;&#36895;&#22120;&#21644;&#22788;&#29702;&#22120;&#35843;&#26597;&#30340;&#26356;&#26032;&#65292;&#28085;&#30422;&#36807;&#21435;&#22235;&#24180;&#30340;&#20869;&#23481;&#12290;&#36890;&#36807;&#25910;&#38598;&#24182;&#24635;&#32467;&#30446;&#21069;&#24050;&#20844;&#24320;&#23459;&#24067;&#30340;&#21830;&#29992;&#21152;&#36895;&#22120;&#30340;&#24615;&#33021;&#21644;&#21151;&#32791;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#32472;&#21046;&#22312;&#25955;&#28857;&#22270;&#19978;&#65292;&#20174;&#32780;&#20998;&#26512;&#20102;&#36235;&#21183;&#21644;&#24066;&#22330;&#32454;&#20998;&#65292;&#24182;&#21253;&#25324;&#20102;&#26032;&#22686;&#21152;&#30340;&#21152;&#36895;&#22120;&#30340;&#31616;&#35201;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;AI&#21152;&#36895;&#22120;&#21644;&#22788;&#29702;&#22120;&#35843;&#26597;&#30340;&#26356;&#26032;&#65292;&#28085;&#30422;&#36807;&#21435;&#22235;&#24180;&#30340;&#20869;&#23481;&#65292;&#29616;&#22312;&#31216;&#20026;Lincoln AI&#35745;&#31639;&#35843;&#26597; - LAICS&#65288;&#21457;&#38899;&#20026;&#8220;lace&#8221;&#65289;&#12290;&#19982;&#24448;&#24180;&#19968;&#26679;&#65292;&#26412;&#25991;&#25910;&#38598;&#24182;&#24635;&#32467;&#20102;&#30446;&#21069;&#24050;&#20844;&#24320;&#23459;&#24067;&#30340;&#21830;&#29992;&#21152;&#36895;&#22120;&#30340;&#23792;&#20540;&#24615;&#33021;&#21644;&#23792;&#20540;&#21151;&#32791;&#25968;&#25454;&#12290;&#23558;&#24615;&#33021;&#21644;&#21151;&#32791;&#25968;&#20540;&#32472;&#21046;&#22312;&#25955;&#28857;&#22270;&#19978;&#65292;&#24182;&#20877;&#27425;&#35752;&#35770;&#21644;&#20998;&#26512;&#20102;&#35813;&#22270;&#19978;&#30340;&#36235;&#21183;&#30340;&#22810;&#20010;&#32500;&#24230;&#21644;&#35266;&#23519;&#32467;&#26524;&#12290;&#22312;&#25955;&#28857;&#22270;&#19978;&#31361;&#20986;&#26174;&#31034;&#20102;&#24066;&#22330;&#32454;&#20998;&#65292;&#24182;&#21253;&#25324;&#27599;&#20010;&#32454;&#20998;&#30340;&#32553;&#25918;&#22270;&#12290;&#26368;&#21518;&#65292;&#36824;&#21253;&#25324;&#20102;&#23545;&#26412;&#24180;&#24230;&#35843;&#26597;&#20013;&#26032;&#22686;&#21152;&#30340;&#27599;&#20010;&#21152;&#36895;&#22120;&#30340;&#31616;&#35201;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is an update of the survey of AI accelerators and processors from past four years, which is now called the Lincoln AI Computing Survey - LAICS (pronounced "lace"). As in past years, this paper collects and summarizes the current commercial accelerators that have been publicly announced with peak performance and peak power consumption numbers. The performance and power values are plotted on a scatter graph, and a number of dimensions and observations from the trends on this plot are again discussed and analyzed. Market segments are highlighted on the scatter plot, and zoomed plots of each segment are also included. Finally, a brief description of each of the new accelerators that have been added in the survey this year is included.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#26041;&#27861;&#65292;&#23558;&#20854;&#35270;&#20026;&#35268;&#33539;&#21270;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#24207;&#21015;&#20449;&#21495;&#21338;&#24328;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#28857;&#24471;&#21040;&#20102;&#19968;&#20010;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38382;&#31572;&#21644;&#20854;&#20182;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.09139</link><description>&lt;p&gt;
&#20849;&#35782;&#28216;&#25103;&#65306;&#36890;&#36807;&#22343;&#34913;&#25628;&#32034;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Consensus Game: Language Model Generation via Equilibrium Search. (arXiv:2310.09139v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09139
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#26041;&#27861;&#65292;&#23558;&#20854;&#35270;&#20026;&#35268;&#33539;&#21270;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#24207;&#21015;&#20449;&#21495;&#21338;&#24328;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#28857;&#24471;&#21040;&#20102;&#19968;&#20010;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38382;&#31572;&#21644;&#20854;&#20182;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#20110;&#38382;&#31572;&#21644;&#20854;&#20182;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#24335;&#26597;&#35810;&#65288;&#36890;&#36807;&#20174;&#20854;&#36755;&#20986;&#20998;&#24067;&#20013;&#25277;&#26679;&#31572;&#26696;&#65289;&#25110;&#21028;&#21035;&#24335;&#26597;&#35810;&#65288;&#36890;&#36807;&#20351;&#29992;&#23427;&#20204;&#23545;&#19968;&#32452;&#20505;&#36873;&#36755;&#20986;&#36827;&#34892;&#35780;&#20998;&#25110;&#25490;&#24207;&#65289;&#36827;&#34892;&#26597;&#35810;&#12290;&#36825;&#20123;&#36807;&#31243;&#26377;&#26102;&#20250;&#20135;&#29983;&#38750;&#24120;&#19981;&#21516;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#22914;&#20309;&#35843;&#21644;&#20114;&#19981;&#30456;&#23481;&#30340;&#35780;&#20998;&#36807;&#31243;&#20197;&#33719;&#24471;&#36830;&#36143;&#30340;LM&#39044;&#27979;&#21602;&#65311;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#12289;&#21338;&#24328;&#35770;&#36807;&#31243;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#35270;&#20026;&#19968;&#31181;&#35268;&#33539;&#21270;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#24207;&#21015;&#20449;&#21495;&#21338;&#24328; - &#31216;&#20026;&#20849;&#35782;&#28216;&#25103; - &#22312;&#35813;&#21338;&#24328;&#20013;&#65292;&#19968;&#20010;&#29983;&#25104;&#22120;&#35797;&#22270;&#29992;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20256;&#36798;&#19968;&#20010;&#25277;&#35937;&#30340;&#27491;&#30830;&#24615;&#21442;&#25968;&#32473;&#19968;&#20010;&#21028;&#21035;&#22120;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#31243;&#24207;&#26469;&#25214;&#21040;&#36825;&#20010;&#21338;&#24328;&#30340;&#36817;&#20284;&#22343;&#34913;&#28857;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;EQUILIBRIUM-RANKING&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#24212;&#29992;&#20110;&#22823;&#37327;&#20219;&#21153;&#65288;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#65292;&#24120;&#35782;&#65289;
&lt;/p&gt;
&lt;p&gt;
When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate outputs). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game - which we term the CONSENSUS GAME - in which a GENERATOR seeks to communicate an abstract correctness parameter using natural language sentences to a DISCRIMINATOR. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call EQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading comprehension, commonsen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#65292;&#36890;&#36807;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#22312;&#26410;&#35265;&#25554;&#27133;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09135</link><description>&lt;p&gt;
HierarchicalContrast: &#19968;&#31181;&#29992;&#20110;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling. (arXiv:2310.09135v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#65292;&#36890;&#36807;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#22312;&#26410;&#35265;&#25554;&#27133;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#22330;&#26223;&#20013;&#65292;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#22312;&#21033;&#29992;&#28304;&#39046;&#22495;&#30693;&#35782;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#27867;&#21270;&#33021;&#21147;&#30340;&#27169;&#22411;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22312;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#32570;&#23569;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#20854;&#24615;&#33021;&#24448;&#24448;&#19981;&#29702;&#24819;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#26041;&#27861;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#23427;&#20204;&#21482;&#33021;&#22312;&#24050;&#35265;&#25554;&#27133;&#19978;&#26377;&#25928;&#22320;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#65292;&#23545;&#26410;&#35265;&#25554;&#27133;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#23884;&#20837;&#30340;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#38388;&#38548;&#21644;&#20869;&#37096;&#26631;&#35760;&#20998;&#24067;&#30340;&#36317;&#31163;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#12290;&#36825;&#40723;&#21169;HiCL&#22312;&#35757;&#32451;&#38454;&#27573;&#27867;&#21270;&#21040;&#26410;&#35265;&#30340;&#25554;&#27133;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#23545;&#26631;&#31614;&#36827;&#34892;&#20844;&#27491;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
In task-oriented dialogue scenarios, cross-domain zero-shot slot filling plays a vital role in leveraging source domain knowledge to learn a model with high generalization ability in unknown target domain where annotated data is unavailable. However, the existing state-of-the-art zero-shot slot filling methods have limited generalization ability in target domain, they only show effective knowledge transfer on seen slots and perform poorly on unseen slots. To alleviate this issue, we present a novel Hierarchical Contrastive Learning Framework (HiCL) for zero-shot slot filling. Specifically, we propose a coarseto fine-grained contrastive learning based on Gaussian-distributed embedding to learn the generalized deep semantic relations between utterance-tokens, by optimizing inter- and intra-token distribution distance. This encourages HiCL to generalize to the slot types unseen at training phase. Furthermore, we present a new iterative label set semantics inference method to unbiasedly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SnD&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#38454;&#27573;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#21644;&#24341;&#20837;&#22122;&#22768;&#26469;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09130</link><description>&lt;p&gt;
&#20351;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#65306;&#25286;&#20998;&#19982;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Split-and-Denoise: Protect large language model inference with local differential privacy. (arXiv:2310.09130v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SnD&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#38454;&#27573;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#21644;&#24341;&#20837;&#22122;&#22768;&#26469;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#25429;&#25417;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#38544;&#34255;&#35821;&#20041;&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#36825;&#19968;&#36807;&#31243;&#20016;&#23500;&#20102;&#25991;&#26412;&#23884;&#20837;&#30340;&#20215;&#20540;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20316;&#20026;&#26381;&#21153;&#65288;EaaS&#65289;&#30340;&#23884;&#20837;&#27169;&#22411;&#21830;&#19994;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#25991;&#26412;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#38754;&#20020;&#30528;&#36739;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#65292;&#36825;&#26159;&#19968;&#20010;&#23578;&#26410;&#24471;&#21040;&#26377;&#25928;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Split-N-Denoise&#65288;SnD&#65289;&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#26469;&#25286;&#20998;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#23558;&#23884;&#20837;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#20043;&#21069;&#24341;&#20837;&#22122;&#22768;&#65292;&#24182;&#38543;&#21518;&#25509;&#25910;&#21644;&#21435;&#22122;&#21518;&#30340;&#25200;&#21160;&#36755;&#20986;&#23884;&#20837;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#20026;LLMs&#30340;&#25512;&#26029;&#38454;&#27573;&#35774;&#35745;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;SnD&#22312;&#21508;&#31181;LLM&#20013;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) shows powerful capability in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of the text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the direct transmission of text to servers poses a largely unaddressed risk of privacy leakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an innovative framework that split the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD's effectiveness in optimizing the privacy-utility tradeoff across various LLM a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#25139;&#30417;&#30563;&#30340;&#21487;&#31359;&#25140;&#27963;&#21160;&#20998;&#21106;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#35201;&#27599;&#20010;&#27963;&#21160;&#27573;&#20013;&#30340;&#21333;&#20010;&#27880;&#37322;&#26679;&#26412;&#26469;&#35299;&#20915;&#30001;&#20110;&#31232;&#30095;&#26631;&#27880;&#23548;&#33268;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#38382;&#39064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.09114</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#25139;&#30417;&#30563;&#30340;&#21487;&#31359;&#25140;&#27963;&#21160;&#20998;&#21106;&#21644;&#35782;&#21035;&#26041;&#27861;&#65306;&#23545;&#27604;&#23398;&#20064;&#21644;&#20445;&#24207;&#26368;&#20248;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Timestamp-supervised Wearable-based Activity Segmentation and Recognition with Contrastive Learning and Order-Preserving Optimal Transport. (arXiv:2310.09114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#25139;&#30417;&#30563;&#30340;&#21487;&#31359;&#25140;&#27963;&#21160;&#20998;&#21106;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#35201;&#27599;&#20010;&#27963;&#21160;&#27573;&#20013;&#30340;&#21333;&#20010;&#27880;&#37322;&#26679;&#26412;&#26469;&#35299;&#20915;&#30001;&#20110;&#31232;&#30095;&#26631;&#27880;&#23548;&#33268;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#38382;&#39064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#27963;&#21160;&#35782;&#21035;&#26159;&#26222;&#36941;&#24212;&#29992;&#20110;&#26080;&#22788;&#19981;&#22312;&#21644;&#31227;&#21160;&#35745;&#31639;&#24212;&#29992;&#20013;&#30340;&#26377;&#29992;&#25216;&#26415;&#20043;&#19968;&#12290;&#28369;&#21160;&#31383;&#21475;&#26041;&#26696;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23384;&#22312;&#22810;&#31867;&#31383;&#21475;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26102;&#38388;&#25139;&#30417;&#30563;&#30340;&#32852;&#21512;&#27963;&#21160;&#20998;&#21106;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#21482;&#38656;&#27599;&#20010;&#27963;&#21160;&#27573;&#20013;&#30340;&#21333;&#20010;&#27880;&#37322;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#31232;&#30095;&#26631;&#27880;&#30340;&#26377;&#38480;&#20449;&#24687;&#21152;&#21095;&#20102;&#35782;&#21035;&#21644;&#20998;&#21106;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) with wearables is one of the serviceable technologies in ubiquitous and mobile computing applications. The sliding-window scheme is widely adopted while suffering from the multi-class windows problem. As a result, there is a growing focus on joint segmentation and recognition with deep-learning methods, aiming at simultaneously dealing with HAR and time-series segmentation issues. However, obtaining the full activity annotations of wearable data sequences is resource-intensive or time-consuming, while unsupervised methods yield poor performance. To address these challenges, we propose a novel method for joint activity segmentation and recognition with timestamp supervision, in which only a single annotated sample is needed in each activity segment. However, the limited information of sparse annotations exacerbates the gap between recognition and segmentation tasks, leading to sub-optimal model performance. Therefore, the prototypes are estimated by clas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GLoRE&#65292;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#25552;&#39640;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#21644;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09107</link><description>&lt;p&gt;
GLoRE: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GLoRE: Evaluating Logical Reasoning of Large Language Models. (arXiv:2310.09107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GLoRE&#65292;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#25552;&#39640;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#21644;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21253;&#25324;GPT-4&#21644;&#26032;&#20852;&#31038;&#21306;&#27169;&#22411;&#22312;&#20869;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;LLMs&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#30340;&#23581;&#35797;&#36824;&#24456;&#23569;&#65292;&#32780;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GLoRE&#65292;&#19968;&#20010;&#31934;&#24515;&#32452;&#32455;&#30340;&#36890;&#29992;&#36923;&#36753;&#25512;&#29702;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;12&#20010;&#35206;&#30422;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20154;&#31867;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;&#24615;&#33021;&#30456;&#27604;&#65292;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#65307;ChatGPT&#21644;GPT-4&#23637;&#31034;&#20102;&#36739;&#24378;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;GPT-4&#22823;&#24133;&#36229;&#36807;&#20102;ChatGPT&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;ChatGPT&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#21450;&#19968;&#31181;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs), including notable models such as GPT-4 and burgeoning community models, have showcased significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks. Our experimental results show that compared to the performance of human and supervised fine-tuning, the logical reasoning capabilities of open LLM models necessitate additional improvement; ChatGPT and GPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing ChatGPT by a large margin. We propose a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21152;&#23494;&#20302;&#21058;&#37327;CT&#21435;&#22122;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#23545;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#21152;&#23494;&#65292;&#36991;&#20813;&#20102;&#23558;&#25968;&#25454;&#26292;&#38706;&#32473;&#26381;&#21153;&#22120;&#30340;&#39118;&#38505;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#20113;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.09101</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#21152;&#23494;&#20302;&#21058;&#37327;CT&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Encrypted Low-Dose CT Denoising. (arXiv:2310.09101v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21152;&#23494;&#20302;&#21058;&#37327;CT&#21435;&#22122;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#23545;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#21152;&#23494;&#65292;&#36991;&#20813;&#20102;&#23558;&#25968;&#25454;&#26292;&#38706;&#32473;&#26381;&#21153;&#22120;&#30340;&#39118;&#38505;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#20113;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26029;&#23618;&#25104;&#20687;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#21058;&#37327;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;LDCT&#65289;&#21435;&#22122;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#36235;&#21183;&#26159;&#20351;&#29992;&#26381;&#21153;&#22120;&#35757;&#32451;&#20855;&#26377;&#22823;&#37327;&#33258;&#25105;&#25910;&#38598;&#30340;&#31169;&#26377;&#25968;&#25454;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#24182;&#20026;&#29992;&#25143;&#25552;&#20379;&#24212;&#29992;&#32534;&#31243;&#25509;&#21475;&#65288;API&#65289;&#65292;&#22914;Chat-GPT&#12290;&#20026;&#20102;&#36991;&#20813;&#27169;&#22411;&#27844;&#28431;&#65292;&#29992;&#25143;&#38656;&#35201;&#23558;&#20854;&#25968;&#25454;&#19978;&#20256;&#21040;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#20294;&#36825;&#31181;&#26041;&#24335;&#24341;&#36215;&#20102;&#20844;&#20247;&#23545;&#20110;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#21307;&#30103;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20943;&#36731;&#30456;&#20851;&#25285;&#24551;&#65292;&#26412;&#25991;&#25552;&#20986;&#22312;&#21152;&#23494;&#39046;&#22495;&#30452;&#25509;&#23545;LDCT&#36827;&#34892;&#21435;&#22122;&#65292;&#20197;&#23454;&#29616;&#23545;&#20113;&#26381;&#21153;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#32780;&#19981;&#23558;&#31169;&#26377;&#25968;&#25454;&#26292;&#38706;&#32473;&#26381;&#21153;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#21516;&#24577;&#21152;&#23494;&#26469;&#21152;&#23494;&#31169;&#26377;&#30340;LDCT&#25968;&#25454;&#65292;&#28982;&#21518;&#23558;&#20854;&#20256;&#36755;&#32473;&#20351;&#29992;&#26126;&#25991;LDCT&#35757;&#32451;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#21435;&#22122;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#20256;&#32479;&#30340;&#25805;&#20316;&#65288;&#22914;&#21367;&#31215;&#21644;&#32447;&#24615;&#21464;&#25442;&#65289;&#38656;&#35201;&#26126;&#25991;&#25968;&#25454;&#65292;&#22240;&#27492;&#22312;&#21152;&#23494;&#39046;&#22495;&#36827;&#34892;&#20302;&#21058;&#37327;CT&#21435;&#22122;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has made significant advancements in tomographic imaging, particularly in low-dose computed tomography (LDCT) denoising. A recent trend involves servers training powerful models with large amounts of self-collected private data and providing application programming interfaces (APIs) for users, such as Chat-GPT. To avoid model leakage, users are required to upload their data to the server model, but this way raises public concerns about the potential risk of privacy disclosure, especially for medical data. Hence, to alleviate related concerns, in this paper, we propose to directly denoise LDCT in the encrypted domain to achieve privacy-preserving cloud services without exposing private data to the server. To this end, we employ homomorphic encryption to encrypt private LDCT data, which is then transferred to the server model trained with plaintext LDCT for further denoising. However, since traditional operations, such as convolution and linear transformation, in DL me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21019;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#22823;&#35268;&#27169;&#21382;&#21490;&#25991;&#29486;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#37325;&#28857;&#30740;&#31350;&#20102;&#8220;Sacrobosco Collection&#8221;&#20013;&#30693;&#35782;&#30340;&#28436;&#21464;&#12290;&#36890;&#36807;&#36825;&#19968;&#30740;&#31350;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#21382;&#21490;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.09091</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;XAI&#22312;&#36229;&#36234;&#20154;&#31867;&#33021;&#21147;&#30340;&#35268;&#27169;&#19978;&#27934;&#23519;&#21382;&#21490;&#26469;&#28304;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Insightful analysis of historical sources at scales beyond human capabilities using unsupervised Machine Learning and XAI. (arXiv:2310.09091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21019;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#22823;&#35268;&#27169;&#21382;&#21490;&#25991;&#29486;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#37325;&#28857;&#30740;&#31350;&#20102;&#8220;Sacrobosco Collection&#8221;&#20013;&#30693;&#35782;&#30340;&#28436;&#21464;&#12290;&#36890;&#36807;&#36825;&#19968;&#30740;&#31350;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#21382;&#21490;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#36164;&#26009;&#20016;&#23500;&#65292;&#20294;&#22914;&#20309;&#23558;&#20154;&#31867;&#30693;&#35782;&#30340;&#28436;&#21464;&#21644;&#20256;&#25773;&#36827;&#34892;&#25972;&#21512;&#65292;&#26080;&#35770;&#26159;&#26102;&#38388;&#19978;&#36824;&#26159;&#31354;&#38388;&#19978;&#30340;&#65292;&#37117;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#21482;&#33021;&#36827;&#34892;&#26377;&#38480;&#30340;&#36873;&#25321;&#24615;&#30740;&#31350;&#12290;&#24040;&#22823;&#30340;&#21382;&#21490;&#36164;&#26009;&#37327;&#20351;&#24471;&#20840;&#38754;&#30740;&#31350;&#25104;&#20026;&#19981;&#21487;&#33021;&#65292;&#22240;&#20026;&#20154;&#31867;&#19987;&#23478;&#30340;&#25968;&#37327;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#37327;&#21382;&#21490;&#36164;&#26009;&#20197;&#25968;&#23383;&#24418;&#24335;&#21487;&#33719;&#24471;&#65292;AI&#36741;&#21161;&#21382;&#21490;&#20998;&#26512;&#26377;&#20102;&#24076;&#26395;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#22823;&#35268;&#27169;&#21382;&#21490;&#25991;&#29486;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#28145;&#20837;&#30340;&#21382;&#21490;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#8220;Sacrobosco Collection&#8221;&#20013;&#30340;&#30693;&#35782;&#28436;&#21464;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;359&#20010;&#26089;&#26399;&#29616;&#20195;&#21360;&#21047;&#29256;&#22825;&#25991;&#23398;&#25945;&#31185;&#20070;&#30340;&#25968;&#23383;&#21270;&#25910;&#34255;&#21697;&#65292;&#36825;&#20123;&#25945;&#31185;&#20070;&#22312;1472&#24180;&#33267;1650&#24180;&#38388;&#22312;&#27431;&#27954;&#22823;&#23398;&#20351;&#29992;&#65292;&#22823;&#32422;&#26377;76,000&#39029;&#65292;&#20854;&#20013;&#35768;&#22810;&#21253;&#21547;&#22825;&#25991;&#21644;&#35745;&#31639;&#34920;&#26684;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#36825;&#19968;&#25910;&#34255;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historical materials are abundant. Yet, piecing together how human knowledge has evolved and spread both diachronically and synchronically remains a challenge that can so far only be very selectively addressed. The vast volume of materials precludes comprehensive studies, given the restricted number of human specialists. However, as large amounts of historical materials are now available in digital form there is a promising opportunity for AI-assisted historical analysis. In this work, we take a pivotal step towards analyzing vast historical corpora by employing innovative machine learning (ML) techniques, enabling in-depth historical insights on a grand scale. Our study centers on the evolution of knowledge within the `Sacrobosco Collection' -- a digitized collection of 359 early modern printed editions of textbooks on astronomy used at European universities between 1472 and 1650 -- roughly 76,000 pages, many of which contain astronomic, computational tables. An ML based analysis of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29790;&#22763;&#24503;&#35821;&#35328;&#32763;&#35793;&#31995;&#32479;&#24314;&#35774;&#20013;&#30340;&#25361;&#25112;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#26041;&#35328;&#22810;&#26679;&#24615;&#21644;&#29790;&#22763;&#24503;&#35821;&#19982;&#26631;&#20934;&#24503;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.09088</link><description>&lt;p&gt;
&#29790;&#22763;&#24503;&#35821;&#35328;&#20256;&#36755;&#30340;&#26041;&#35328;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Dialect Transfer for Swiss German Speech Translation. (arXiv:2310.09088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29790;&#22763;&#24503;&#35821;&#35328;&#32763;&#35793;&#31995;&#32479;&#24314;&#35774;&#20013;&#30340;&#25361;&#25112;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#26041;&#35328;&#22810;&#26679;&#24615;&#21644;&#29790;&#22763;&#24503;&#35821;&#19982;&#26631;&#20934;&#24503;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24314;&#31435;&#29790;&#22763;&#24503;&#35821;&#35328;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#26041;&#35328;&#22810;&#26679;&#24615;&#20197;&#21450;&#29790;&#22763;&#24503;&#35821;&#21644;&#26631;&#20934;&#24503;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;&#29790;&#22763;&#24503;&#35821;&#26159;&#19968;&#31181;&#21475;&#35821;&#35821;&#35328;&#65292;&#27809;&#26377;&#27491;&#24335;&#30340;&#20070;&#20889;&#31995;&#32479;&#65292;&#21253;&#25324;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#35328;&#65292;&#26159;&#19968;&#31181;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65292;&#21482;&#26377;&#22823;&#32422;500&#19975;&#20351;&#29992;&#32773;&#12290;&#36825;&#39033;&#30740;&#31350;&#22260;&#32469;&#20004;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#36827;&#34892;&#65306;&#22312;&#35757;&#32451;&#29790;&#22763;&#24503;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#26102;&#65292;&#26041;&#35328;&#30340;&#21253;&#21547;&#21644;&#25490;&#38500;&#22914;&#20309;&#24433;&#21709;&#29305;&#23450;&#26041;&#35328;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#29790;&#22763;&#24503;&#35821;&#21644;&#26631;&#20934;&#24503;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#22914;&#20309;&#24433;&#21709;&#31995;&#32479;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#34920;&#26126;&#26041;&#35328;&#22810;&#26679;&#24615;&#21644;&#35821;&#35328;&#24046;&#24322;&#32473;&#29790;&#22763;&#24503;&#35821;&#35328;&#32763;&#35793;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#19982;&#32463;&#39564;&#30740;&#31350;&#24471;&#20986;&#30340;&#35821;&#35328;&#23398;&#20551;&#35774;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the challenges in building Swiss German speech translation systems, specifically focusing on the impact of dialect diversity and differences between Swiss German and Standard German. Swiss German is a spoken language with no formal writing system, it comprises many diverse dialects and is a low-resource language with only around 5 million speakers. The study is guided by two key research questions: how does the inclusion and exclusion of dialects during the training of speech translation models for Swiss German impact the performance on specific dialects, and how do the differences between Swiss German and Standard German impact the performance of the systems? We show that dialect diversity and linguistic differences pose significant challenges to Swiss German speech translation, which is in line with linguistic hypotheses derived from empirical investigations.
&lt;/p&gt;</description></item><item><title>ImageManip&#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#20687;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#30446;&#26631;&#29289;&#20307;&#22810;&#20010;&#35270;&#35282;&#21644;&#25512;&#26029;&#28145;&#24230;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#20351;&#29992;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#36827;&#34892;&#25805;&#25511;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.09069</link><description>&lt;p&gt;
ImageManip: &#22522;&#20110;&#22270;&#20687;&#30340;&#24102;&#26377;&#21487;&#20379;&#35782;&#21035;&#24341;&#23548;&#30340;&#19979;&#19968;&#20010;&#35270;&#22270;&#36873;&#25321;&#30340;&#26426;&#22120;&#20154;&#25805;&#25511;
&lt;/p&gt;
&lt;p&gt;
ImageManip: Image-based Robotic Manipulation with Affordance-guided Next View Selection. (arXiv:2310.09069v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09069
&lt;/p&gt;
&lt;p&gt;
ImageManip&#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#20687;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#30446;&#26631;&#29289;&#20307;&#22810;&#20010;&#35270;&#35282;&#21644;&#25512;&#26029;&#28145;&#24230;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#20351;&#29992;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#36827;&#34892;&#25805;&#25511;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#23478;&#24237;&#21161;&#25163;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#25511;&#23545;&#20110;&#20351;&#26426;&#22120;&#20154;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#30740;&#31350;&#20351;&#29992;&#19977;&#32500;&#28857;&#20113;&#20316;&#20026;&#25805;&#32437;&#31574;&#30053;&#30340;&#20027;&#35201;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#33719;&#21462;&#28857;&#20113;&#25968;&#25454;&#30340;&#26174;&#33879;&#25104;&#26412;&#32780;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#23454;&#38469;&#24212;&#29992;&#24615;&#12290;&#30456;&#21453;&#65292;RGB&#22270;&#20687;&#21033;&#29992;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35774;&#22791;&#25552;&#20379;&#39640;&#20998;&#36776;&#29575;&#30340;&#35266;&#23519;&#65292;&#20294;&#32570;&#20047;&#19977;&#32500;&#31354;&#38388;&#20960;&#20309;&#20449;&#24687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#25429;&#25417;&#30446;&#26631;&#29289;&#20307;&#30340;&#22810;&#20010;&#35270;&#35282;&#65292;&#24182;&#25512;&#26029;&#28145;&#24230;&#20449;&#24687;&#20197;&#34917;&#20805;&#20854;&#20960;&#20309;&#24418;&#29366;&#12290;&#21021;&#22987;&#38454;&#27573;&#65292;&#31995;&#32479;&#20351;&#29992;&#19968;&#20010;&#25163;&#19978;&#26377;&#30524;&#30340;RGB&#25668;&#20687;&#22836;&#25429;&#25417;&#30446;&#26631;&#29289;&#20307;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;&#23427;&#39044;&#27979;&#21021;&#22987;&#28145;&#24230;&#22270;&#21644;&#31895;&#30053;&#21487;&#20379;&#35782;&#21035;&#22270;&#12290;&#21487;&#20379;&#35782;&#21035;&#22270;&#25351;&#31034;&#20102;&#21487;&#34892;&#21160;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of future home-assistant robots, 3D articulated object manipulation is essential for enabling robots to interact with their environment. Many existing studies make use of 3D point clouds as the primary input for manipulation policies. However, this approach encounters challenges due to data sparsity and the significant cost associated with acquiring point cloud data, which can limit its practicality. In contrast, RGB images offer high-resolution observations using cost effective devices but lack spatial 3D geometric information. To overcome these limitations, we present a novel image-based robotic manipulation framework. This framework is designed to capture multiple perspectives of the target object and infer depth information to complement its geometry. Initially, the system employs an eye-on-hand RGB camera to capture an overall view of the target object. It predicts the initial depth map and a coarse affordance map. The affordance map indicates actionable areas on the 
&lt;/p&gt;</description></item><item><title>DATT&#26159;&#19968;&#31181;&#29992;&#20110;&#22235;&#26059;&#32764;&#25511;&#21046;&#30340;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#31934;&#30830;&#36319;&#36394;&#20219;&#24847;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#24182;&#33021;&#22815;&#22312;&#23384;&#22312;&#22823;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;L1&#33258;&#36866;&#24212;&#25511;&#21046;&#36827;&#34892;&#22686;&#24378;&#65292;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09053</link><description>&lt;p&gt;
DATT&#65306;&#29992;&#20110;&#22235;&#26059;&#32764;&#25511;&#21046;&#30340;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
DATT: Deep Adaptive Trajectory Tracking for Quadrotor Control. (arXiv:2310.09053v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09053
&lt;/p&gt;
&lt;p&gt;
DATT&#26159;&#19968;&#31181;&#29992;&#20110;&#22235;&#26059;&#32764;&#25511;&#21046;&#30340;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#31934;&#30830;&#36319;&#36394;&#20219;&#24847;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#24182;&#33021;&#22815;&#22312;&#23384;&#22312;&#22823;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;L1&#33258;&#36866;&#24212;&#25511;&#21046;&#36827;&#34892;&#22686;&#24378;&#65292;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26410;&#30693;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#12289;&#36712;&#36857;&#19981;&#21487;&#34892;&#24615;&#21644;&#25191;&#34892;&#38480;&#21046;&#65292;&#23545;&#20110;&#22235;&#26059;&#32764;&#30340;&#31934;&#30830;&#20219;&#24847;&#36712;&#36857;&#36319;&#36394;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;&#65288;DATT&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#31934;&#30830;&#22320;&#36319;&#36394;&#20219;&#24847;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#24182;&#22312;&#23384;&#22312;&#22823;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25511;&#21046;&#12290;DATT&#22522;&#20110;&#19968;&#31181;&#22312;&#20223;&#30495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#26032;&#39062;&#21069;&#39304;-&#21453;&#39304;&#33258;&#36866;&#24212;&#25511;&#21046;&#32467;&#26500;&#12290;&#24403;&#22312;&#30495;&#23454;&#30828;&#20214;&#19978;&#37096;&#32626;&#26102;&#65292;DATT&#36890;&#36807;&#22312;&#38381;&#29615;&#20013;&#20351;&#29992;L1&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#24178;&#25200;&#20272;&#35745;&#22120;&#36827;&#34892;&#22686;&#24378;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;DATT&#22312;&#20855;&#26377;&#19981;&#31283;&#23450;&#39118;&#22330;&#30340;&#21487;&#34892;&#24179;&#28369;&#21644;&#19981;&#21487;&#34892;&#36712;&#36857;&#19978;&#65292;&#26126;&#26174;&#20248;&#20110;&#31454;&#20105;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#21253;&#25324;&#22522;&#32447;&#23436;&#20840;&#22833;&#25928;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;DATT&#21487;&#20197;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#39640;&#25928;&#36816;&#34892;&#65292;&#25512;&#29702;&#26102;&#38388;&#19981;&#21040;3.2&#27627;&#31186;&#65292;&#20165;&#20026;&#19968;/&#22235;&#20998;&#20043;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise arbitrary trajectory tracking for quadrotors is challenging due to unknown nonlinear dynamics, trajectory infeasibility, and actuation limits. To tackle these challenges, we present Deep Adaptive Trajectory Tracking (DATT), a learning-based approach that can precisely track arbitrary, potentially infeasible trajectories in the presence of large disturbances in the real world. DATT builds on a novel feedforward-feedback-adaptive control structure trained in simulation using reinforcement learning. When deployed on real hardware, DATT is augmented with a disturbance estimator using L1 adaptive control in closed-loop, without any fine-tuning. DATT significantly outperforms competitive adaptive nonlinear and model predictive controllers for both feasible smooth and infeasible trajectories in unsteady wind fields, including challenging scenarios where baselines completely fail. Moreover, DATT can efficiently run online with an inference time less than 3.2 ms, less than 1/4 of the ad
&lt;/p&gt;</description></item><item><title>SAI&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;JSON&#26684;&#24335;&#30340;&#24847;&#22270;&#36755;&#20837;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#65292;&#22312;&#26234;&#33021;&#31227;&#21160;&#32593;&#32476;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09049</link><description>&lt;p&gt;
SAI: &#22312;&#36890;&#20449;&#32593;&#32476;&#20013;&#21033;&#29992;&#31995;&#32479;&#20154;&#24037;&#26234;&#33021;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SAI: Solving AI Tasks with Systematic Artificial Intelligence in Communication Network. (arXiv:2310.09049v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09049
&lt;/p&gt;
&lt;p&gt;
SAI&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;JSON&#26684;&#24335;&#30340;&#24847;&#22270;&#36755;&#20837;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#65292;&#22312;&#26234;&#33021;&#31227;&#21160;&#32593;&#32476;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#20013;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#26159;&#26234;&#33021;&#31227;&#21160;&#32593;&#32476;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#22312;&#26234;&#33021;&#31227;&#21160;&#32593;&#32476;&#20013;&#19987;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31995;&#32479;&#20154;&#24037;&#26234;&#33021;&#65288;SAI&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20197;JSON&#26684;&#24335;&#30340;&#24847;&#22270;&#20026;&#22522;&#30784;&#30340;&#36755;&#20837;&#26469;&#36830;&#25509;&#33258;&#35774;&#35745;&#30340;&#27169;&#22411;&#24211;&#21644;&#25968;&#25454;&#24211;&#65292;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#36755;&#20837;&#32452;&#20214;&#65292;&#21516;&#26102;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20197;JSON&#26684;&#24335;&#30340;&#24847;&#22270;&#20026;&#22522;&#30784;&#30340;&#36755;&#20837;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#29992;&#25143;&#30340;&#22810;&#26679;&#24847;&#22270;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#21345;&#29255;&#30340;&#27169;&#22411;&#24211;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#21033;&#29992;&#27169;&#22411;&#21345;&#29255;&#22312;&#19981;&#21516;&#27169;&#22359;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#32452;&#21512;&#30340;&#37197;&#23545;&#21305;&#37197;&#12290;&#27169;&#22411;&#21345;&#29255;&#21253;&#21547;&#30456;&#24212;&#27169;&#22411;&#30340;&#21517;&#31216;&#21644;&#25152;&#38656;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapid development of artificial intelligence, solving complex AI tasks is a crucial technology in intelligent mobile networks. Despite the good performance of specialized AI models in intelligent mobile networks, they are unable to handle complicated AI tasks. To address this challenge, we propose Systematic Artificial Intelligence (SAI), which is a framework designed to solve AI tasks by leveraging Large Language Models (LLMs) and JSON-format intent-based input to connect self-designed model library and database. Specifically, we first design a multi-input component, which simultaneously integrates Large Language Models (LLMs) and JSON-format intent-based inputs to fulfill the diverse intent requirements of different users. In addition, we introduce a model library module based on model cards which employ model cards to pairwise match between different modules for model composition. Model cards contain the corresponding model's name and the required performance metrics. Then wh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KCTS&#30340;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#35299;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#20998;&#31867;&#22120;&#21644;MCTS&#25351;&#23548;&#20923;&#32467;&#30340;LM&#29983;&#25104;&#19982;&#21442;&#32771;&#30693;&#35782;&#23545;&#40784;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;RIPA&#12290;</title><link>http://arxiv.org/abs/2310.09044</link><description>&lt;p&gt;
KCTS&#65306;&#24102;&#26377;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#30340;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection. (arXiv:2310.09044v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09044
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KCTS&#30340;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#35299;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#20998;&#31867;&#22120;&#21644;MCTS&#25351;&#23548;&#20923;&#32467;&#30340;LM&#29983;&#25104;&#19982;&#21442;&#32771;&#30693;&#35782;&#23545;&#40784;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;RIPA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#20154;&#31867;&#32423;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#21363;&#25152;&#35859;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20854;&#37096;&#32626;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#36755;&#20837;&#20013;&#30340;&#30693;&#35782;&#23545;LLM&#36827;&#34892;&#31934;&#32454;&#35843;&#33410;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#24341;&#36215;&#39640;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#21487;&#33021;&#23545;&#22810;&#20219;&#21153;&#27169;&#22411;&#36896;&#25104;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;KCTS&#65288;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#65289;&#30340;&#30693;&#35782;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#20998;&#31867;&#22120;&#24471;&#20998;&#21644;MCTS&#65288;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65289;&#26469;&#25351;&#23548;&#20923;&#32467;&#30340;LM&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#29983;&#25104;&#19982;&#21442;&#32771;&#30693;&#35782;&#23545;&#40784;&#30340;&#25991;&#26412;&#12290;&#20026;&#20102;&#23558;&#24207;&#21015;&#32423;&#30693;&#35782;&#20998;&#31867;&#22120;&#36866;&#24212;&#20196;&#29260;&#32423;&#25351;&#23548;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;RIPA&#65288;&#22870;&#21169;&#25296;&#28857;&#36817;&#20284;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment. A common approach to address this issue is to retrieve relevant knowledge and fine-tune the LLM with the knowledge in its input. Unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking models. To overcome these limitations, we propose a knowledge-constrained decoding method called KCTS (Knowledge-Constrained Tree Search), which guides a frozen LM to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and MCTS (Monte-Carlo Tree Search). To adapt the sequence-level knowledge classifier to token-level guidance, we also propose a novel token-level hallucination detection method called RIPA (Reward Inflection Point Approximation). Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#26102;&#38388;&#27573;&#30005;&#20215;&#35745;&#21010;&#65292;&#36890;&#36807;&#20248;&#21270;&#35843;&#24230;&#30005;&#21160;&#36710;&#20805;&#30005;&#36807;&#31243;&#26469;&#38477;&#20302;&#29992;&#25143;&#23478;&#24237;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.09040</link><description>&lt;p&gt;
&#32771;&#34385;&#32456;&#31471;&#29992;&#25143;&#28789;&#27963;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30005;&#21160;&#36710;&#20805;&#30005;&#20248;&#21270;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimal Scheduling of Electric Vehicle Charging with Deep Reinforcement Learning considering End Users Flexibility. (arXiv:2310.09040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#26102;&#38388;&#27573;&#30005;&#20215;&#35745;&#21010;&#65292;&#36890;&#36807;&#20248;&#21270;&#35843;&#24230;&#30005;&#21160;&#36710;&#20805;&#30005;&#36807;&#31243;&#26469;&#38477;&#20302;&#29992;&#25143;&#23478;&#24237;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#65288;&#29305;&#21035;&#26159;&#30005;&#21160;&#27773;&#36710;&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#39044;&#35745;&#22312;&#26410;&#26469;&#21313;&#24180;&#20869;&#23558;&#22823;&#24133;&#22686;&#21152;&#23545;&#29616;&#26377;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#30340;&#21387;&#21147;&#65292;&#22686;&#21152;&#20102;&#23545;&#26356;&#39640;&#31995;&#32479;&#21487;&#38752;&#24615;&#21644;&#28789;&#27963;&#24615;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#32593;&#32476;&#25237;&#36164;&#24182;&#22686;&#21152;&#23545;&#20998;&#37197;&#32593;&#32476;&#30340;&#21487;&#25511;&#24615;&#65292;&#32593;&#32476;&#36816;&#33829;&#21830;&#24320;&#23637;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#65292;&#20197;&#40723;&#21169;&#32456;&#31471;&#29992;&#25143;&#22312;&#22238;&#25253;&#32463;&#27982;&#25110;&#20854;&#20182;&#21033;&#30410;&#30340;&#21069;&#25552;&#19979;&#36716;&#31227;&#20854;&#29992;&#30005;&#28040;&#32791;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#22312;&#20303;&#23429;&#36127;&#33655;&#35843;&#24230;&#24212;&#29992;&#39046;&#22495;&#22788;&#20110;&#30740;&#31350;&#21069;&#27839;&#65292;&#20027;&#35201;&#30001;&#20110;&#20854;&#39640;&#31934;&#24230;&#24615;&#12289;&#39640;&#35745;&#31639;&#36895;&#24230;&#21644;&#36739;&#20302;&#23545;&#20110;&#27491;&#22312;&#24320;&#21457;&#30340;&#27169;&#22411;&#30340;&#29289;&#29702;&#29305;&#24615;&#30340;&#20381;&#36182;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22312;&#26102;&#38388;&#27573;&#30005;&#20215;&#35745;&#21010;&#19979;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30830;&#23450;&#29992;&#25143;&#23478;&#24237;&#30005;&#21160;&#36710;&#20943;&#23569;&#25104;&#26412;&#30340;&#20805;&#30005;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of decentralized energy resources and especially Electric Vehicles (EV), that are expected to increase sharply over the next decade, will put further stress on existing power distribution networks, increasing the need for higher system reliability and flexibility. In an attempt to avoid unnecessary network investments and to increase the controllability over distribution networks, network operators develop demand response (DR) programs that incentivize end users to shift their consumption in return for financial or other benefits. Artificial intelligence (AI) methods are in the research forefront for residential load scheduling applications, mainly due to their high accuracy, high computational speed and lower dependence on the physical characteristics of the models under development. The aim of this work is to identify households' EV cost-reducing charging policy under a Time-of-Use tariff scheme, with the use of Deep Reinforcement Learning, and more specifically Deep
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23376;&#31354;&#38388;&#36866;&#24212;&#20808;&#39564;&#31639;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#21021;&#22987;&#21270;&#21442;&#25968;&#21644;&#21442;&#25968;&#23376;&#31354;&#38388;&#65292;&#21487;&#20197;&#22522;&#20110;&#20219;&#21153;&#20998;&#24067;&#20915;&#23450;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35843;&#25972;&#21738;&#20123;&#25805;&#20316;&#23376;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#24182;&#38477;&#20302;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.09028</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23376;&#31354;&#38388;&#36866;&#24212;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Subspace Adaptation Prior for Few-Shot Learning. (arXiv:2310.09028v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23376;&#31354;&#38388;&#36866;&#24212;&#20808;&#39564;&#31639;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#21021;&#22987;&#21270;&#21442;&#25968;&#21644;&#21442;&#25968;&#23376;&#31354;&#38388;&#65292;&#21487;&#20197;&#22522;&#20110;&#20219;&#21153;&#20998;&#24067;&#20915;&#23450;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35843;&#25972;&#21738;&#20123;&#25805;&#20316;&#23376;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#24182;&#38477;&#20302;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#22522;&#20110;&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;&#26088;&#22312;&#20174;&#19968;&#31995;&#21015;&#35757;&#32451;&#20219;&#21153;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#20415;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#36866;&#24212;&#21487;&#35757;&#32451;&#23618;&#30340;&#25152;&#26377;&#21442;&#25968;&#12290;&#36825;&#24573;&#30053;&#20102;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#26469;&#35828;&#21487;&#33021;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#33021;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#20854;&#20013;&#24517;&#39035;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23376;&#31354;&#38388;&#36866;&#24212;&#20808;&#39564;(SAP)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#21516;&#26102;&#23398;&#20064;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#21442;&#25968;(&#20808;&#39564;&#30693;&#35782;)&#21644;&#21442;&#25968;&#23376;&#31354;&#38388;&#65292;&#20197;&#25805;&#20316;&#23376;&#38598;&#30340;&#24418;&#24335;&#34920;&#31034;&#24212;&#35813;&#26159;&#21487;&#36866;&#24212;&#30340;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;SAP&#21487;&#20197;&#26681;&#25454;&#28508;&#22312;&#30340;&#20219;&#21153;&#20998;&#24067;&#23398;&#20064;&#24212;&#35813;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35843;&#25972;&#30340;&#25805;&#20316;&#23376;&#38598;&#65292;&#21516;&#26102;&#38477;&#20302;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based meta-learning techniques aim to distill useful prior knowledge from a set of training tasks such that new tasks can be learned more efficiently with gradient descent. While these methods have achieved successes in various scenarios, they commonly adapt all parameters of trainable layers when learning new tasks. This neglects potentially more efficient learning strategies for a given task distribution and may be susceptible to overfitting, especially in few-shot learning where tasks must be learned from a limited number of examples. To address these issues, we propose Subspace Adaptation Prior (SAP), a novel gradient-based meta-learning algorithm that jointly learns good initialization parameters (prior knowledge) and layer-wise parameter subspaces in the form of operation subsets that should be adaptable. In this way, SAP can learn which operation subsets to adjust with gradient descent based on the underlying task distribution, simultaneously decreasing the risk of over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STDMMF-Net&#30340;&#26102;&#31354;&#21452;&#27169;&#28151;&#21512;&#27969;&#32593;&#32476;&#65292;&#29992;&#20110;&#20840;&#26223;&#35270;&#39057;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#19982;&#24212;&#29992;&#20132;&#21449;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#20132;&#21449;&#23618;&#26435;&#37325;&#27169;&#22359;&#21644;&#21452;&#27169;&#24577;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;2D&#35270;&#39057;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#22312;&#20840;&#26223;&#35270;&#39057;&#20013;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#34701;&#21512;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.09016</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20840;&#26223;&#35270;&#39057;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#30340;&#26102;&#31354;&#21452;&#27169;&#28151;&#21512;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Spatial-Temporal Dual-Mode Mixed Flow Network for Panoramic Video Salient Object Detection. (arXiv:2310.09016v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STDMMF-Net&#30340;&#26102;&#31354;&#21452;&#27169;&#28151;&#21512;&#27969;&#32593;&#32476;&#65292;&#29992;&#20110;&#20840;&#26223;&#35270;&#39057;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#19982;&#24212;&#29992;&#20132;&#21449;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#20132;&#21449;&#23618;&#26435;&#37325;&#27169;&#22359;&#21644;&#21452;&#27169;&#24577;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;2D&#35270;&#39057;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#22312;&#20840;&#26223;&#35270;&#39057;&#20013;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#34701;&#21512;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#35270;&#39057;&#20013;&#30340;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#20173;&#22788;&#20110;&#21021;&#27493;&#25506;&#32034;&#38454;&#27573;&#12290;&#23558;2D&#35270;&#39057;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#38388;&#25509;&#24212;&#29992;&#20110;&#20840;&#26223;&#35270;&#39057;&#20013;&#30340;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#26816;&#27979;&#31934;&#24230;&#20302;&#12289;&#27169;&#22411;&#22797;&#26434;&#24230;&#39640;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#31561;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20132;&#21449;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;ILA&#65289;&#12289;&#20132;&#21449;&#23618;&#26435;&#37325;&#27169;&#22359;&#65288;ILW&#65289;&#21644;&#21452;&#27169;&#24577;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;BMA&#65289;&#12290;&#22522;&#20110;&#36825;&#20123;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20840;&#26223;&#35270;&#39057;&#30340;&#31354;&#38388;&#27969;&#21644;&#30456;&#24212;&#20809;&#27969;&#36827;&#34892;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#30340;&#26102;&#31354;&#21452;&#27169;&#28151;&#21512;&#27969;&#32593;&#32476;&#65288;STDMMF-Net&#65289;&#12290;&#39318;&#20808;&#65292;ILA&#27169;&#22359;&#35745;&#31639;&#20102;&#30456;&#37051;&#23618;&#32423;&#29305;&#24449;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#65292;&#20197;&#25552;&#39640;&#20174;&#31354;&#38388;&#27969;&#20013;&#25552;&#21462;&#26174;&#33879;&#30446;&#26631;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;ILW&#27169;&#22359;&#37327;&#21270;&#20102;&#27599;&#20010;&#23618;&#32423;&#29305;&#24449;&#20013;&#21253;&#21547;&#30340;&#26174;&#33879;&#30446;&#26631;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#34701;&#21512;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Salient object detection (SOD) in panoramic video is still in the initial exploration stage. The indirect application of 2D video SOD method to the detection of salient objects in panoramic video has many unmet challenges, such as low detection accuracy, high model complexity, and poor generalization performance. To overcome these hurdles, we design an Inter-Layer Attention (ILA) module, an Inter-Layer weight (ILW) module, and a Bi-Modal Attention (BMA) module. Based on these modules, we propose a Spatial-Temporal Dual-Mode Mixed Flow Network (STDMMF-Net) that exploits the spatial flow of panoramic video and the corresponding optical flow for SOD. First, the ILA module calculates the attention between adjacent level features of consecutive frames of panoramic video to improve the accuracy of extracting salient object features from the spatial flow. Then, the ILW module quantifies the salient object information contained in the features of each level to improve the fusion efficiency of 
&lt;/p&gt;</description></item><item><title>CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08992</link><description>&lt;p&gt;
CodeChain: &#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#23454;&#29616;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules. (arXiv:2310.08992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08992
&lt;/p&gt;
&lt;p&gt;
CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#35299;&#20915;&#31616;&#21333;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#29087;&#32451;&#65292;&#27604;&#22914;&#22312;HumanEval&#25110;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#32534;&#31243;&#20219;&#21153;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#20316;&#20026;&#25972;&#20307;&#20195;&#30721;&#22359;&#32780;&#19981;&#26159;&#23558;&#20854;&#20998;&#35299;&#20026;&#36923;&#36753;&#23376;&#20219;&#21153;&#21644;&#23376;&#27169;&#22359;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26377;&#32463;&#39564;&#30340;&#31243;&#24207;&#21592;&#26412;&#33021;&#22320;&#32534;&#20889;&#20855;&#26377;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22359;&#21270;&#20195;&#30721;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#36890;&#24120;&#20250;&#37325;&#22797;&#20351;&#29992;&#20043;&#21069;&#24320;&#21457;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeChain&#65292;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CodeChain&#39318;&#20808;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25552;&#31034;&#25351;&#23548;LLM&#29983;&#25104;&#27169;&#22359;&#21270;&#20195;&#30721;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#20004;&#20010;&#27493;&#39588;&#23454;&#26045;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#65306;1&#65289;&#39069;&#22806;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#31215;&#26497;&#25903;&#25345;&#37325;&#26032;&#35268;&#21010;&#20915;&#31574;&#26469;&#20943;&#23569;&#33322;&#29677;&#24310;&#35823;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#21382;&#21490;&#37325;&#26032;&#35268;&#21010;&#25968;&#25454;&#21644;&#22825;&#27668;&#25968;&#25454;&#39044;&#27979;&#26410;&#26469;&#20960;&#22825;&#26159;&#21542;&#20250;&#21457;&#24067;&#37325;&#26032;&#35268;&#21010;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#24433;&#21709;&#33322;&#32447;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.08988</link><description>&lt;p&gt;
&#36335;&#32447;&#37325;&#26032;&#35268;&#21010;&#39044;&#27979;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Reroute Prediction Service. (arXiv:2310.08988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08988
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#31215;&#26497;&#25903;&#25345;&#37325;&#26032;&#35268;&#21010;&#20915;&#31574;&#26469;&#20943;&#23569;&#33322;&#29677;&#24310;&#35823;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#21382;&#21490;&#37325;&#26032;&#35268;&#21010;&#25968;&#25454;&#21644;&#22825;&#27668;&#25968;&#25454;&#39044;&#27979;&#26410;&#26469;&#20960;&#22825;&#26159;&#21542;&#20250;&#21457;&#24067;&#37325;&#26032;&#35268;&#21010;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#24433;&#21709;&#33322;&#32447;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#22312;2019&#24180;&#65292;&#32654;&#22269;&#22269;&#23478;&#33322;&#31354;&#31354;&#38388;&#31995;&#32479;&#30340;&#24310;&#35823;&#25104;&#26412;&#23601;&#20272;&#35745;&#20026;330&#20159;&#32654;&#20803;&#65292;&#36825;&#26159;&#36807;&#21435;&#20960;&#24180;&#22686;&#38271;&#36235;&#21183;&#30340;&#23792;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#24040;&#22823;&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#31215;&#26497;&#25903;&#25345;&#37325;&#26032;&#35268;&#21010;&#20915;&#31574;&#26469;&#20943;&#23569;&#24310;&#35823;&#12290;&#35813;&#31995;&#32479;&#22312;&#26410;&#26469;&#20960;&#22825;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#39044;&#27979;&#26576;&#20010;&#31354;&#20013;&#31649;&#21046;&#21306;&#22495;&#25110;&#26576;&#20010;&#25351;&#23450;&#30340;&#21672;&#35810;&#26631;&#35782;&#26159;&#21542;&#20250;&#21457;&#24067;&#37325;&#26032;&#35268;&#21010;&#24314;&#35758;&#65292;&#20174;&#32780;&#21487;&#33021;&#24433;&#21709;&#30456;&#20851;&#36335;&#32447;&#12290;&#20026;&#20102;&#25552;&#20379;&#36825;&#20123;&#39044;&#27979;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#20174;FAA&#25552;&#20379;&#30340;&#31995;&#32479;&#33539;&#22260;&#20449;&#24687;&#31649;&#29702;&#65288;SWIM&#65289;&#25968;&#25454;&#26381;&#21153;&#21644;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#39044;&#27979;&#20013;&#24515;&#65288;NCEP&#65289;&#25552;&#20379;&#30340;&#22825;&#27668;&#25968;&#25454;&#25910;&#38598;&#30340;&#21382;&#21490;&#37325;&#26032;&#35268;&#21010;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#37327;&#24222;&#22823;&#65292;&#21253;&#21547;&#35768;&#22810;&#20197;&#39640;&#36895;&#29575;&#27969;&#24335;&#20256;&#36755;&#30340;&#19981;&#30456;&#20851;&#21644;&#22122;&#22768;&#25968;&#25454;&#12290;&#31995;&#32479;&#25345;&#32493;&#22788;&#29702;&#36827;&#20837;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cost of delays was estimated as 33 billion US dollars only in 2019 for the US National Airspace System, a peak value following a growth trend in past years. Aiming to address this huge inefficiency, we designed and developed a novel Data Analytics and Machine Learning system, which aims at reducing delays by proactively supporting re-routing decisions.  Given a time interval up to a few days in the future, the system predicts if a reroute advisory for a certain Air Route Traffic Control Center or for a certain advisory identifier will be issued, which may impact the pertinent routes. To deliver such predictions, the system uses historical reroute data, collected from the System Wide Information Management (SWIM) data services provided by the FAA, and weather data, provided by the US National Centers for Environmental Prediction (NCEP). The data is huge in volume, and has many items streamed at high velocity, uncorrelated and noisy. The system continuously processes the incoming raw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#33322;&#31354;&#39046;&#22495;&#25317;&#22581;&#39044;&#27979;&#31995;&#32479;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#22269;&#23478;&#31354;&#22495;&#31995;&#32479;&#65288;NAS&#65289;&#20869;&#29305;&#23450;&#31354;&#22495;&#37096;&#38376;&#30340;&#39134;&#26426;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.08982</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#33322;&#31354;&#39046;&#22495;&#25317;&#22581;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Big data-driven prediction of airspace congestion. (arXiv:2310.08982v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#33322;&#31354;&#39046;&#22495;&#25317;&#22581;&#39044;&#27979;&#31995;&#32479;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#22269;&#23478;&#31354;&#22495;&#31995;&#32479;&#65288;NAS&#65289;&#20869;&#29305;&#23450;&#31354;&#22495;&#37096;&#38376;&#30340;&#39134;&#26426;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22269;&#38469;&#19978;&#30340;&#33322;&#31354;&#23548;&#33322;&#26381;&#21153;&#25552;&#20379;&#21830;&#19968;&#30452;&#22312;&#21162;&#21147;&#24320;&#21457;&#19968;&#31181;&#26356;&#22909;&#30340;&#26041;&#27861;&#26469;&#27979;&#37327;&#21644;&#39044;&#27979;&#29305;&#23450;&#31354;&#22495;&#20869;&#30340;&#39134;&#26426;&#25968;&#37327;&#65292;&#20063;&#31216;&#20026;&#31354;&#22495;&#23494;&#24230;&#12290;&#31934;&#30830;&#27979;&#37327;&#21644;&#39044;&#27979;&#31354;&#22495;&#23494;&#24230;&#23545;&#20110;&#26356;&#22909;&#22320;&#31649;&#29702;&#31354;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#26080;&#35770;&#26159;&#25112;&#30053;&#19978;&#36824;&#26159;&#25112;&#26415;&#19978;&#65292;&#37117;&#33021;&#23454;&#29616;&#26356;&#39640;&#32423;&#21035;&#30340;&#33258;&#21160;&#21270;&#65292;&#20174;&#32780;&#20943;&#36731;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#26041;&#27861;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24050;&#32463;&#33021;&#22815;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#22788;&#29702;&#19981;&#26029;&#22686;&#38271;&#30340;&#24222;&#22823;&#30340;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#26102;&#65292;&#25968;&#25454;&#31649;&#29702;&#21644;&#26597;&#35810;&#22788;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#32447;&#24615;&#39044;&#27979;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#39044;&#27979;&#31995;&#32479;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#22269;&#23478;&#31354;&#22495;&#31995;&#32479;&#65288;NAS&#65289;&#20869;&#29305;&#23450;&#31354;&#22495;&#37096;&#38376;&#30340;&#39134;&#26426;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air Navigation Service Providers (ANSP) worldwide have been making a considerable effort for the development of a better method to measure and predict aircraft counts within a particular airspace, also referred to as airspace density. An accurate measurement and prediction of airspace density is crucial for a better managed airspace, both strategically and tactically, yielding a higher level of automation and thereby reducing the air traffic controller's workload. Although the prior approaches have been able to address the problem to some extent, data management and query processing of ever-increasing vast volume of air traffic data at high rates, for various analytics purposes such as predicting aircraft counts, still remains a challenge especially when only linear prediction models are used.  In this paper, we present a novel data management and prediction system that accurately predicts aircraft counts for a particular airspace sector within the National Airspace System (NAS). The i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35770;&#25991;&#20027;&#35201;&#20998;&#26512;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#21382;&#21490;&#12289;&#22256;&#38590;&#21644;&#21069;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25913;&#21892;&#29992;&#25143;&#20132;&#20114;&#21644;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#30830;&#23450;&#29992;&#25143;&#24773;&#32490;&#12290;</title><link>http://arxiv.org/abs/2310.08977</link><description>&lt;p&gt;
&#22810;&#21151;&#33021;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32842;&#22825;&#26426;&#22120;&#20154;&#65306;&#35774;&#35745;&#65292;&#26041;&#27861;&#21644;&#32467;&#35770;
&lt;/p&gt;
&lt;p&gt;
Multi-Purpose NLP Chatbot : Design, Methodology &amp; Conclusion. (arXiv:2310.08977v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35770;&#25991;&#20027;&#35201;&#20998;&#26512;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#21382;&#21490;&#12289;&#22256;&#38590;&#21644;&#21069;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25913;&#21892;&#29992;&#25143;&#20132;&#20114;&#21644;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#30830;&#23450;&#29992;&#25143;&#24773;&#32490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20027;&#35201;&#20851;&#27880;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#21382;&#21490;&#12289;&#22256;&#38590;&#21644;&#21069;&#26223;&#65292;&#24182;&#23545;&#24403;&#21069;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#29615;&#22659;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#28789;&#27963;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25913;&#21892;&#29992;&#25143;&#20132;&#20114;&#21644;&#23545;&#35805;&#20307;&#39564;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#36824;&#21033;&#29992;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#30830;&#23450;&#29992;&#25143;&#24773;&#32490;&#12290;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#29305;&#28857;&#65292;&#21253;&#25324;&#35821;&#38899;&#23545;&#35805;&#12289;&#22810;&#35821;&#35328;&#25903;&#25345;[12]&#12289;&#21672;&#35810;&#25216;&#33021;&#12289;&#31163;&#32447;&#21151;&#33021;&#21644;&#24555;&#36895;&#24110;&#21161;&#21151;&#33021;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#24320;&#21457;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#25512;&#21160;&#36825;&#20123;&#21457;&#23637;&#30340;&#21407;&#22240;&#21450;&#20854;&#23545;&#21508;&#20010;&#34892;&#19994;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;&#26681;&#25454;&#30740;&#31350;&#65292;&#26377;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#38750;&#24120;&#37325;&#35201;&#65306;1&#65289;&#21363;&#20351;&#27809;&#26377;&#26126;&#30830;&#30340;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#20063;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a major focus on its history, difficulties, and promise, this research paper provides a thorough analysis of the chatbot technology environment as it exists today. It provides a very flexible chatbot system that makes use of reinforcement learning strategies to improve user interactions and conversational experiences. Additionally, this system makes use of sentiment analysis and natural language processing to determine user moods. The chatbot is a valuable tool across many fields thanks to its amazing characteristics, which include voice-to-voice conversation, multilingual support [12], advising skills, offline functioning, and quick help features. The complexity of chatbot technology development is also explored in this study, along with the causes that have propelled these developments and their far-reaching effects on a range of sectors. According to the study, three crucial elements are crucial: 1) Even without explicit profile information, the chatbot system is built to adept
&lt;/p&gt;</description></item><item><title>ChatKBQA&#26159;&#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08975</link><description>&lt;p&gt;
ChatKBQA: &#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#29992;&#20110;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models. (arXiv:2310.08975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08975
&lt;/p&gt;
&lt;p&gt;
ChatKBQA&#26159;&#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#33719;&#21462;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#30740;&#31350;&#32452;&#25104;&#37096;&#20998;&#65306;&#30693;&#35782;&#26816;&#32034;&#21644;&#35821;&#20041;&#35299;&#26512;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19977;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#21253;&#25324;&#20302;&#25928;&#30340;&#30693;&#35782;&#26816;&#32034;&#12289;&#26816;&#32034;&#38169;&#35823;&#23545;&#35821;&#20041;&#35299;&#26512;&#30340;&#19981;&#21033;&#24433;&#21709;&#20197;&#21450;&#20043;&#21069;&#30340;KBQA&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatKBQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#31934;&#35843;&#24320;&#28304;LLMs&#65288;&#22914;Llama-2&#12289;ChatGLM2&#21644;Baichuan2&#65289;&#26500;&#24314;&#30340;&#29983;&#25104;-&#26816;&#32034;KBQA&#26694;&#26550;&#12290;ChatKBQA&#25552;&#35758;&#39318;&#20808;&#20351;&#29992;&#31934;&#35843;&#30340;LLMs&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#28982;&#21518;&#36890;&#36807;&#26080;&#30417;&#30563;&#26816;&#32034;&#26041;&#27861;&#26816;&#32034;&#21644;&#26367;&#25442;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#30452;&#35266;&#22320;&#25913;&#36827;&#20102;&#29983;&#25104;&#21644;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatKBQA&#22312;&#26631;&#20934;KBQA&#25968;&#25454;&#38598;WebQSP&#21644;ComplexWebQuestions (CWQ)&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Base Question Answering (KBQA) aims to derive answers to natural language questions over large-scale knowledge bases (KBs), which are generally divided into two research components: knowledge retrieval and semantic parsing. However, three core challenges remain, including inefficient knowledge retrieval, retrieval errors adversely affecting semantic parsing, and the complexity of previous KBQA methods. In the era of large language models (LLMs), we introduce ChatKBQA, a novel generate-then-retrieve KBQA framework built on fine-tuning open-source LLMs such as Llama-2, ChatGLM2 and Baichuan2. ChatKBQA proposes generating the logical form with fine-tuned LLMs first, then retrieving and replacing entities and relations through an unsupervised retrieval method, which improves both generation and retrieval more straightforwardly. Experimental results reveal that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and ComplexWebQuestions (CWQ). This
&lt;/p&gt;</description></item><item><title>EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.08949</link><description>&lt;p&gt;
Easier Multimodal Generation: Diffusion Models Meet LLMs
&lt;/p&gt;
&lt;p&gt;
Making Multimodal Generation Easier: When Diffusion Models Meet LLMs. (arXiv:2310.08949v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08949
&lt;/p&gt;
&lt;p&gt;
EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EasyGen&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20027;&#35201;&#20381;&#36182;&#20110;&#32534;&#30721;&#22120;&#22914;CLIP&#25110;ImageBind&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#26469;&#26725;&#25509;&#27169;&#24577;&#20043;&#38388;&#24046;&#36317;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;EasyGen&#22522;&#20110;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#65292;&#20419;&#36827;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#12290;EasyGen&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#23618;&#23558;BiDiffuser&#21644;LLM&#36827;&#34892;&#38598;&#25104;&#65292;&#22788;&#29702;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38480;&#20110;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#19981;&#21516;&#65292;EasyGen&#36824;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;LLM&#21019;&#24314;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#30001;BiDiffuser&#35299;&#37322;&#29983;&#25104;&#36866;&#24403;&#30340;&#35270;&#35273;&#22238;&#22797;&#26469;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#20102;EasyGen&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#35757;&#32451;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities. EasyGen handles image-to-text generation by integrating BiDiffuser and an LLM via a simple projection layer. Unlike most existing multimodal models that are limited to generating text responses, EasyGen can also facilitate text-to-image generation by leveraging the LLM to create textual descriptions, which can be interpreted by BiDiffuser to generate appropriate visual responses. Extensive quantitative and qualitative experiments demonstrate the effectiveness of EasyGen, whose training can b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FCILPT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#25968;&#25454;&#20998;&#24067;&#24773;&#20917;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.08948</link><description>&lt;p&gt;
&#20855;&#26377;&#25552;&#31034;&#30340;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Class-Incremental Learning with Prompting. (arXiv:2310.08948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FCILPT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#25968;&#25454;&#20998;&#24067;&#24773;&#20917;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Web&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20351;&#29992;&#23384;&#20648;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#22312;&#35753;&#27169;&#22411;&#20174;&#20998;&#24067;&#22312;&#21508;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#26102;&#33021;&#22815;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#32852;&#37030;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#37117;&#20551;&#35774;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#26159;&#22266;&#23450;&#30340;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#24456;&#21487;&#33021;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#25968;&#25454;&#21487;&#33021;&#19981;&#26029;&#29983;&#25104;&#65292;&#26032;&#30340;&#31867;&#21035;&#20063;&#21487;&#33021;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23454;&#38469;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#65288;FCIL&#65289;&#38382;&#39064;&#12290;&#23545;&#20110;FCIL&#65292;&#30001;&#20110;&#26032;&#31867;&#21035;&#30340;&#20986;&#29616;&#21644;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#30340;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#24615;&#36136;&#65288;non-iid&#65289;&#65292;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#22411;&#21487;&#33021;&#20250;&#23545;&#26087;&#31867;&#21035;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20855;&#26377;&#25552;&#31034;&#30340;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#65288;FCILPT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Web technology continues to develop, it has become increasingly common to use data stored on different clients. At the same time, federated learning has received widespread attention due to its ability to protect data privacy when let models learn from data which is distributed across various clients. However, most existing works assume that the client's data are fixed. In real-world scenarios, such an assumption is most likely not true as data may be continuously generated and new classes may also appear. To this end, we focus on the practical and challenging federated class-incremental learning (FCIL) problem. For FCIL, the local and global models may suffer from catastrophic forgetting on old classes caused by the arrival of new classes and the data distributions of clients are non-independent and identically distributed (non-iid).  In this paper, we propose a novel method called Federated Class-Incremental Learning with PrompTing (FCILPT). Given the privacy and limited memory, F
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#25991;&#26412;&#27700;&#21360;&#26041;&#27861;Easymark&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#21547;&#20041;&#30340;&#24773;&#20917;&#19979;&#27880;&#20837;&#27700;&#21360;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#24230;&#21487;&#20449;&#22320;&#26816;&#27979;&#20986;&#25991;&#26412;&#26159;&#21542;&#30001;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#31995;&#32479;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;BLEU&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.08920</link><description>&lt;p&gt;
&#32670;&#20154;&#31616;&#21333;&#30340;&#25991;&#23383;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Embarrassingly Simple Text Watermarks. (arXiv:2310.08920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08920
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#25991;&#26412;&#27700;&#21360;&#26041;&#27861;Easymark&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#21547;&#20041;&#30340;&#24773;&#20917;&#19979;&#27880;&#20837;&#27700;&#21360;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#24230;&#21487;&#20449;&#22320;&#26816;&#27979;&#20986;&#25991;&#26412;&#26159;&#21542;&#30001;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#31995;&#32479;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Easymark&#65292;&#36825;&#26159;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#25991;&#26412;&#27700;&#21360;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;LLM&#21487;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#26080;&#27861;&#21306;&#20998;&#30340;&#25991;&#26412;&#65292;&#36825;&#23545;&#25991;&#26412;&#30340;&#21487;&#20449;&#24230;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290;Easymark&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;Easymark&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#21547;&#20041;&#30340;&#24773;&#20917;&#19979;&#27880;&#20837;&#27700;&#21360;&#65292;&#32780;&#39564;&#35777;&#22120;&#21487;&#20197;&#39640;&#24230;&#21487;&#20449;&#22320;&#26816;&#27979;&#20986;&#25991;&#26412;&#26159;&#21542;&#30001;&#37319;&#29992;Easymark&#30340;&#31995;&#32479;&#29983;&#25104;&#12290;Easymark&#38750;&#24120;&#23481;&#26131;&#23454;&#29616;&#65292;&#21482;&#38656;&#35201;&#20960;&#34892;&#20195;&#30721;&#12290;Easymark&#19981;&#38656;&#35201;&#35775;&#38382;LLMs&#65292;&#22240;&#27492;&#24403;LLM&#25552;&#20379;&#32773;&#19981;&#25552;&#20379;&#24102;&#27700;&#21360;&#30340;LLM&#26102;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#31471;&#23454;&#26045;&#12290;&#23613;&#31649;&#23427;&#24456;&#31616;&#21333;&#65292;&#20294;&#23427;&#33021;&#22815;&#23454;&#29616;&#27604;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#27700;&#21360;&#26041;&#27861;&#26356;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;BLEU&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23436;&#32654;&#27700;&#21360;&#30340;&#19981;&#21487;&#33021;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Easymark, a family of embarrassingly simple yet effective watermarks. Text watermarking is becoming increasingly important with the advent of Large Language Models (LLM). LLMs can generate texts that cannot be distinguished from human-written texts. This is a serious problem for the credibility of the text. Easymark is a simple yet effective solution to this problem. Easymark can inject a watermark without changing the meaning of the text at all while a validator can detect if a text was generated from a system that adopted Easymark or not with high credibility. Easymark is extremely easy to implement so that it only requires a few lines of code. Easymark does not require access to LLMs, so it can be implemented on the user-side when the LLM providers do not offer watermarked LLMs. In spite of its simplicity, it achieves higher detection accuracy and BLEU scores than the state-of-the-art text watermarking methods. We also prove the impossibility theorem of perfect watermarki
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20998;&#21106;&#25628;&#32034;&#21512;&#24182;&#30340;&#31639;&#27861;&#22312;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#26435;&#37325;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.08917</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Relation-aware Ensemble Learning for Knowledge Graph Embedding. (arXiv:2310.08917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20998;&#21106;&#25628;&#32034;&#21512;&#24182;&#30340;&#31639;&#27861;&#22312;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#26435;&#37325;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#25506;&#32034;&#19981;&#21516;&#26041;&#24335;&#30340;&#35821;&#20041;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#26469;&#23398;&#20064;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#25506;&#32034;&#36825;&#20123;&#35821;&#20041;&#20250;&#23548;&#33268;&#27604;&#19968;&#33324;&#38598;&#25104;&#26041;&#27861;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#21106;&#25628;&#32034;&#21512;&#24182;&#30340;&#31639;&#27861;RelEns-DSC&#65292;&#23427;&#29420;&#31435;&#22320;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#30340;&#26435;&#37325;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#19982;&#19968;&#33324;&#38598;&#25104;&#26041;&#27861;&#30456;&#21516;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#24615;&#33021;&#26356;&#22909;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#39640;&#25928;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#26435;&#37325;&#21644;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/LARS-research/RelEns&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) embedding is a fundamental task in natural language processing, and various methods have been proposed to explore semantic patterns in distinctive ways. In this paper, we propose to learn an ensemble by leveraging existing methods in a relation-aware manner. However, exploring these semantics using relation-aware ensemble leads to a much larger search space than general ensemble methods. To address this issue, we propose a divide-search-combine algorithm RelEns-DSC that searches the relation-wise ensemble weights independently. This algorithm has the same computation cost as general ensemble methods but with much better performance. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed method in efficiently searching relation-aware ensemble weights and achieving state-of-the-art embedding performance. The code is public at https://github.com/LARS-research/RelEns.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26114;&#36149;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#26356;&#26032;&#31232;&#30095;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#27492;&#26469;&#20943;&#23567;&#23558;&#20854;&#37096;&#32626;&#21040;&#35774;&#22791;&#19978;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08915</link><description>&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;&#65306;&#38024;&#23545;&#31232;&#30095;LLMs&#30340;&#26080;&#35757;&#32451;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs. (arXiv:2310.08915v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26114;&#36149;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#26356;&#26032;&#31232;&#30095;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#27492;&#26469;&#20943;&#23567;&#23558;&#20854;&#37096;&#32626;&#21040;&#35774;&#22791;&#19978;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#24222;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34429;&#28982;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#24320;&#36767;&#20102;&#28508;&#22312;&#36335;&#24452;&#65292;&#20294;&#24456;&#36951;&#25022;&#65292;&#22312;&#20854;&#22312;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#36947;&#36335;&#19978;&#23384;&#22312;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#38556;&#30861;&#12290;&#20316;&#20026;&#22312;&#20943;&#23569;&#27169;&#22411;&#22797;&#26434;&#24615;&#26041;&#38754;&#26368;&#25104;&#29087;&#30340;&#39044;-LLMs&#26041;&#27861;&#20043;&#19968;&#65292;&#32593;&#32476;&#20462;&#21098;&#20284;&#20046;&#22312;LLMs&#26102;&#20195;&#33853;&#21518;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22312;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#20013;&#38656;&#35201;&#26114;&#36149;&#30340;&#24494;&#35843;(&#25110;&#37325;&#26032;&#35757;&#32451;)&#12290;&#20026;&#20102;&#24357;&#21512;&#20135;&#19994;&#19982;&#23398;&#26415;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;(DSnoT)&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#35757;&#32451;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#22312;&#19981;&#36827;&#34892;&#26114;&#36149;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#20219;&#20309;&#26435;&#37325;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#30053;&#24494;&#26356;&#26032;&#31232;&#30095;LLMs&#12290;&#21463;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#30340;&#21551;&#21457;&#65292;DSnoT&#36890;&#36807;&#22312;&#31232;&#30095;LLMs&#20043;&#19978;&#25191;&#34892;&#36845;&#20195;&#30340;&#26435;&#37325;&#20462;&#21098;&#21644;&#29983;&#38271;&#30340;&#26041;&#24335;&#65292;&#26368;&#23567;&#21270;&#20102;&#31264;&#23494;&#21644;&#31232;&#30095;LLMs&#20043;&#38388;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#65292;DSnoT&#29305;&#21035;&#32771;&#34385;&#20102;&#39044;&#26399;&#30340;&#20943;&#23569;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25112;&#30053;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#38450;&#27490;&#33410;&#28857;&#34987;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08909</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#20316;&#20026;&#21453;&#20107;&#23454;&#22270;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Community Membership Hiding as Counterfactual Graph Search via Deep Reinforcement Learning. (arXiv:2310.08909v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25112;&#30053;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#38450;&#27490;&#33410;&#28857;&#34987;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21457;&#29616;&#24444;&#27492;&#32039;&#23494;&#32852;&#31995;&#30340;&#29992;&#25143;&#32676;&#20307;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#20182;&#20204;&#20849;&#20139;&#20849;&#21516;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21151;&#33021;&#24448;&#24448;&#20250;&#20197;&#21487;&#33021;&#26292;&#38706;&#20010;&#20154;&#38544;&#31169;&#20026;&#20195;&#20215;&#65292;&#26080;&#24847;&#20013;&#36879;&#38706;&#20182;&#20204;&#30340;&#21697;&#21619;&#25110;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#29992;&#25143;&#21487;&#33021;&#24076;&#26395;&#20445;&#25252;&#20182;&#20204;&#30340;&#21311;&#21517;&#24615;&#65292;&#24182;&#20986;&#20110;&#21508;&#31181;&#21407;&#22240;&#36873;&#25321;&#36864;&#20986;&#31038;&#21306;&#26816;&#27979;&#65292;&#20363;&#22914;&#19982;&#25919;&#27835;&#25110;&#23447;&#25945;&#32452;&#32455;&#30340;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#25112;&#30053;&#24615;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#20197;&#38450;&#27490;&#19968;&#20010;&#25110;&#22810;&#20010;&#33410;&#28857;&#34987;&#32473;&#23450;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#21046;&#23450;&#19968;&#20010;&#21463;&#38480;&#30340;&#21453;&#20107;&#23454;&#22270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#26469;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#33410;&#28857;&#21644;&#31038;&#21306;&#27450;&#39575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection techniques are useful tools for social media platforms to discover tightly connected groups of users who share common interests. However, this functionality often comes at the expense of potentially exposing individuals to privacy breaches by inadvertently revealing their tastes or preferences. Therefore, some users may wish to safeguard their anonymity and opt out of community detection for various reasons, such as affiliation with political or religious organizations.  In this study, we address the challenge of community membership hiding, which involves strategically altering the structural properties of a network graph to prevent one or more nodes from being identified by a given community detection algorithm. We tackle this problem by formulating it as a constrained counterfactual graph objective, and we solve it via deep reinforcement learning. We validate the effectiveness of our method through two distinct tasks: node and community deception. Extensive exper
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31119;&#21033;&#22806;&#20132;&#36825;&#19968;&#36890;&#29992;&#21644;&#21464;&#31181;&#30340;&#38646;&#21644;&#28216;&#25103;&#65292;&#30446;&#30340;&#26159;&#34913;&#37327;&#21644;&#24378;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#22522;&#20934;&#26234;&#33021;&#20307;&#22312;&#36798;&#21040;&#39640;&#31038;&#20250;&#31119;&#21033;&#26102;&#23384;&#22312;&#21487;&#21033;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08901</link><description>&lt;p&gt;
&#31119;&#21033;&#22806;&#20132;&#65306;&#22522;&#20934;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Welfare Diplomacy: Benchmarking Language Model Cooperation. (arXiv:2310.08901v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08901
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31119;&#21033;&#22806;&#20132;&#36825;&#19968;&#36890;&#29992;&#21644;&#21464;&#31181;&#30340;&#38646;&#21644;&#28216;&#25103;&#65292;&#30446;&#30340;&#26159;&#34913;&#37327;&#21644;&#24378;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#22522;&#20934;&#26234;&#33021;&#20307;&#22312;&#36798;&#21040;&#39640;&#31038;&#20250;&#31119;&#21033;&#26102;&#23384;&#22312;&#21487;&#21033;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#38271;&#21644;&#24191;&#27867;&#37096;&#32626;&#65292;&#23545;&#20110;&#34913;&#37327;&#23427;&#20204;&#21512;&#20316;&#33021;&#21147;&#30340;&#24378;&#22823;&#22522;&#20934;&#26159;&#24517;&#35201;&#30340;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#22810;&#26234;&#33021;&#20307;&#22522;&#20934;&#35201;&#20040;&#26159;&#38646;&#21644;&#28216;&#25103;&#65292;&#35201;&#20040;&#26159;&#32431;&#31929;&#21512;&#20316;&#30340;&#65292;&#32473;&#20104;&#20102;&#38750;&#24120;&#26377;&#38480;&#30340;&#26426;&#20250;&#36827;&#34892;&#36825;&#31181;&#27979;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38646;&#21644;&#28216;&#25103;Diplomacy&#30340;&#19968;&#20010;&#36890;&#29992;&#21644;&#21464;&#31181;&#8212;&#8212;&#31119;&#21033;&#22806;&#20132;&#65292;&#22312;&#20854;&#20013;&#29609;&#23478;&#24517;&#39035;&#24179;&#34913;&#20891;&#20107;&#24449;&#26381;&#21644;&#22269;&#20869;&#31119;&#21033;&#30340;&#25237;&#36164;&#12290;&#25105;&#20204;&#35748;&#20026;&#31119;&#21033;&#22806;&#20132;&#21487;&#20197;&#26356;&#28165;&#26224;&#22320;&#35780;&#20272;&#24182;&#25552;&#20379;&#26356;&#24378;&#22823;&#30340;&#21512;&#20316;&#33021;&#21147;&#30340;&#22521;&#35757;&#28608;&#21169;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#65306;&#65288;1&#65289;&#25552;&#20986;&#31119;&#21033;&#22806;&#20132;&#30340;&#35268;&#21017;&#24182;&#36890;&#36807;&#24320;&#28304;Diplomacy&#24341;&#25806;&#23454;&#29616;&#23427;&#20204;&#65307;&#65288;2&#65289;&#20351;&#29992;&#38646;&#26679;&#26412;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#22522;&#20934;&#26234;&#33021;&#20307;&#65307;&#20197;&#21450;&#65288;3&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#22522;&#20934;&#26234;&#33021;&#20307;&#21487;&#20197;&#36798;&#21040;&#24456;&#39640;&#30340;&#31038;&#20250;&#31119;&#21033;&#65292;&#20294;&#26159;&#23384;&#22312;&#34987;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#30340;&#31119;&#21033;&#22806;&#20132;&#20419;&#36827;&#31038;&#20250;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing capabilities and increasingly widespread deployment of AI systems necessitate robust benchmarks for measuring their cooperative capabilities. Unfortunately, most multi-agent benchmarks are either zero-sum or purely cooperative, providing limited opportunities for such measurements. We introduce a general-sum variant of the zero-sum board game Diplomacy -- called Welfare Diplomacy -- in which players must balance investing in military conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both a clearer assessment of and stronger training incentives for cooperative capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules and implementing them via an open-source Diplomacy engine; (2) constructing baseline agents using zero-shot prompted language models; and (3) conducting experiments where we find that baselines using state-of-the-art models attain high social welfare but are exploitable. Our work aims to promote societal safety by ai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#36741;&#21161;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#35782;&#21035;&#22235;&#20010;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#19981;&#21516;&#31867;&#21035;&#65292;&#23454;&#29616;&#20102;&#23545;&#35813;&#30142;&#30149;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21152;&#26435;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;98.91%&#12290;</title><link>http://arxiv.org/abs/2310.08888</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#36741;&#21161;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Transfer Learning Assisted Decision Support System for Accurate Prediction of Alzheimer Disease. (arXiv:2310.08888v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#36741;&#21161;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#35782;&#21035;&#22235;&#20010;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#19981;&#21516;&#31867;&#21035;&#65292;&#23454;&#29616;&#20102;&#23545;&#35813;&#30142;&#30149;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21152;&#26435;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;98.91%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#26159;&#32769;&#24180;&#20154;&#26368;&#24120;&#35265;&#30340;&#38271;&#26399;&#30142;&#30149;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#21464;&#24471;&#27969;&#34892;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22810;&#25104;&#21151;&#12290;&#23427;&#24050;&#25104;&#20026;&#26597;&#30475;&#21307;&#23398;&#24433;&#20687;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#26816;&#27979;AD&#26041;&#38754;&#65292;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#27604;&#19968;&#33324;&#30340;&#26426;&#22120;&#23398;&#20064;&#26356;&#20934;&#30830;&#21644;&#26377;&#25928;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35782;&#21035;&#39044;&#27979;AD&#30340;&#22235;&#20010;&#19981;&#21516;&#31867;&#21035;&#65292;&#20026;&#35813;&#30142;&#30149;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#21644;&#26816;&#27979;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20854;&#20013;&#21152;&#26435;&#20934;&#30830;&#29575;&#39640;&#36798;98.91%&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#38598;&#25104;&#24179;&#22343;&#27169;&#22411;&#21644;&#20116;&#31181;&#19981;&#21516;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#25913;&#21892;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#20998;&#31867;&#38382;&#39064;&#30340;&#20934;&#30830;&#29575;&#12290;EfficientNetB0+Resnet152(effnet+res152)&#21644;InceptionV3+EfficientNetB0+Resnet50(incep+effnet+res50)&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#36798;&#21040;&#20102;&#22810;&#31867;&#21035;A&#30340;&#26368;&#39640;&#21152;&#26435;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is the most common long-term illness in elderly people. In recent years, deep learning has become popular in the area of medical imaging and has had a lot of success there. It has become the most effective way to look at medical images. When it comes to detecting AD, the deep neural model is more accurate and effective than general machine learning. Our research contributes to the development of a more comprehensive understanding and detection of the disease by identifying four distinct classes that are predictive of AD with a high weighted accuracy of 98.91%. A unique strategy has been proposed to improve the accuracy of the imbalance dataset classification problem via the combination of ensemble averaging models and five different transfer learning models in this study. EfficientNetB0+Resnet152(effnet+res152) and InceptionV3+EfficientNetB0+Resnet50(incep+effnet+res50) models have been fine-tuned and have reached the highest weighted accuracy for multi-class A
&lt;/p&gt;</description></item><item><title>METRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#65292;&#26088;&#22312;&#20351;&#20854;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#12290;&#36825;&#20010;&#30446;&#26631;&#35299;&#20915;&#20102;&#32431;&#25506;&#32034;&#26041;&#27861;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20013;&#30340;&#22256;&#38590;&#20197;&#21450;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#20013;&#32570;&#20047;&#28608;&#21169;&#32780;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08887</link><description>&lt;p&gt;
METRA:&#20855;&#26377;&#24230;&#37327;&#24863;&#30693;&#25277;&#35937;&#30340;&#21487;&#25193;&#23637;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
METRA: Scalable Unsupervised RL with Metric-Aware Abstraction. (arXiv:2310.08887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08887
&lt;/p&gt;
&lt;p&gt;
METRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#65292;&#26088;&#22312;&#20351;&#20854;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#12290;&#36825;&#20010;&#30446;&#26631;&#35299;&#20915;&#20102;&#32431;&#25506;&#32034;&#26041;&#27861;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20013;&#30340;&#22256;&#38590;&#20197;&#21450;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#20013;&#32570;&#20047;&#28608;&#21169;&#32780;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#21516;&#26679;&#65292;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26377;&#26395;&#21457;&#29616;&#21508;&#31181;&#28508;&#22312;&#26377;&#29992;&#30340;&#34892;&#20026;&#65292;&#21487;&#20197;&#21152;&#36895;&#23398;&#20064;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20043;&#21069;&#30340;&#23581;&#35797;&#65292;&#20351;&#26080;&#30417;&#30563;RL&#30495;&#27491;&#21487;&#25193;&#23637;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25361;&#25112;&#65306;&#22312;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#32431;&#25506;&#32034;&#26041;&#27861;&#21487;&#33021;&#20250;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#35206;&#30422;&#27599;&#20010;&#21487;&#33021;&#30340;&#36716;&#25442;&#26159;&#19981;&#21487;&#34892;&#30340;&#65307;&#32780;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#28608;&#21169;&#32780;&#23436;&#20840;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#12290;&#20026;&#20102;&#20351;&#26080;&#30417;&#30563;RL&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;RL&#30446;&#26631;&#65292;&#31216;&#20026;&#24230;&#37327;&#24863;&#30693;&#25277;&#35937;&#65288;METRA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks. Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning. However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge: pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible, and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives. To make unsupervised RL scalable to complex, high-dimensional environments, we propose a novel unsupervised RL objective, which we call Metric-Aware Abstraction (METRA). Ou
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#26469;&#35780;&#20272;&#21464;&#21387;&#22120;&#22312;&#22788;&#29702;&#19981;&#21516;&#38590;&#24230;&#31034;&#20363;&#30340;&#38382;&#39064;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#33258;&#36866;&#24212;&#21644;&#27169;&#22359;&#21270;&#35745;&#31639;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#27867;&#21270;&#21040;&#26356;&#39640;&#25968;&#37327;&#30340;&#35745;&#31639;&#26102;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20844;&#24179;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2310.08866</link><description>&lt;p&gt;
&#24378;&#36866;&#24212;&#24615;&#21644;&#27169;&#22359;&#21270;&#29992;&#20110;&#26377;&#25928;&#22320;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#19978;&#36827;&#34892;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptivity and Modularity for Efficient Generalization Over Task Complexity. (arXiv:2310.08866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08866
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#26469;&#35780;&#20272;&#21464;&#21387;&#22120;&#22312;&#22788;&#29702;&#19981;&#21516;&#38590;&#24230;&#31034;&#20363;&#30340;&#38382;&#39064;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#33258;&#36866;&#24212;&#21644;&#27169;&#22359;&#21270;&#35745;&#31639;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#27867;&#21270;&#21040;&#26356;&#39640;&#25968;&#37327;&#30340;&#35745;&#31639;&#26102;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20844;&#24179;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26159;&#21542;&#33021;&#22815;&#22312;&#38656;&#35201;&#22788;&#29702;&#19981;&#21516;&#38590;&#24230;&#31034;&#20363;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#27867;&#21270;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#23545;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#27867;&#21270;&#65292;&#24182;&#25552;&#20986;&#30340;&#32467;&#26524;&#34920;&#26126;&#26631;&#20934;&#30340;&#21464;&#21387;&#22120;&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#20219;&#21153;&#26159;&#30001;Zhang&#31561;&#20154;&#22312;2021&#24180;&#25552;&#20986;&#30340;&#25351;&#38024;&#20540;&#26816;&#32034;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#20013;&#33258;&#36866;&#24212;&#21644;&#27169;&#22359;&#21270;&#35745;&#31639;&#26426;&#21046;&#30340;&#20351;&#29992;&#22914;&#20309;&#20419;&#36827;&#23398;&#20064;&#38656;&#35201;&#22312;&#39034;&#24207;&#35745;&#31639;&#27493;&#39588;&#25968;&#37327;&#65288;&#21363;&#35745;&#31639;&#22270;&#30340;&#28145;&#24230;&#65289;&#19978;&#36827;&#34892;&#27867;&#21270;&#30340;&#20219;&#21153;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#31216;&#20026;Hyper-UT&#65292;&#23427;&#32467;&#21512;&#20102;&#26469;&#33258;&#36229;&#32593;&#32476;&#30340;&#21160;&#24577;&#20989;&#25968;&#29983;&#25104;&#21644;&#26469;&#33258;&#36890;&#29992;&#21464;&#21387;&#22120;&#30340;&#33258;&#36866;&#24212;&#28145;&#24230;&#12290;&#35813;&#27169;&#22411;&#22312;&#27867;&#21270;&#21040;&#26356;&#39640;&#25968;&#37327;&#30340;&#35745;&#31639;&#26102;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20844;&#24179;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can transformers generalize efficiently on problems that require dealing with examples with different levels of difficulty? We introduce a new task tailored to assess generalization over different complexities and present results that indicate that standard transformers face challenges in solving these tasks. These tasks are variations of pointer value retrieval previously introduced by Zhang et al. (2021). We investigate how the use of a mechanism for adaptive and modular computation in transformers facilitates the learning of tasks that demand generalization over the number of sequential computation steps (i.e., the depth of the computation graph). Based on our observations, we propose a transformer-based architecture called Hyper-UT, which combines dynamic function generation from hyper networks with adaptive depth from Universal Transformers. This model demonstrates higher accuracy and a fairer allocation of computational resources when generalizing to higher numbers of computation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;Adam-family&#26041;&#27861;&#22312;&#35757;&#32451;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#31163;&#26435;&#37325;&#34928;&#20943;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#35768;&#22810;&#24050;&#30693;&#30340;Adam-family&#26041;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.08858</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#31163;&#26435;&#37325;&#34928;&#20943;&#30340;Adam-family&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Adam-family Methods with Decoupled Weight Decay in Deep Learning. (arXiv:2310.08858v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;Adam-family&#26041;&#27861;&#22312;&#35757;&#32451;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#31163;&#26435;&#37325;&#34928;&#20943;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#35768;&#22810;&#24050;&#30693;&#30340;Adam-family&#26041;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;Adam-family&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#20108;&#27425;&#27491;&#21017;&#21270;&#38750;&#20809;&#28369;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#20855;&#26377;&#26435;&#37325;&#34928;&#20943;&#30340;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#12290;&#21463;&#21040;AdamW&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#31163;&#26435;&#37325;&#34928;&#20943;&#30340;Adam-family&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#65292;&#38543;&#26426;&#23376;&#26799;&#24230;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#20998;&#21035;&#29420;&#31435;&#20110;&#26435;&#37325;&#34928;&#20943;&#39033;&#36827;&#34892;&#26356;&#26032;&#12290;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#65292;&#24182;&#19988;&#22312;&#26356;&#26032;&#20027;&#35201;&#20248;&#21270;&#21464;&#37327;&#26102;&#37319;&#29992;&#38750;&#36882;&#20943;&#27493;&#38271;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#21547;&#20102;&#35768;&#22810;&#20247;&#25152;&#21608;&#30693;&#30340;Adam-family&#26041;&#27861;&#65292;&#20174;&#32780;&#20026;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#28176;&#36817;&#36817;&#20284;&#20102;&#19968;&#31867;&#27425;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the convergence properties of a wide class of Adam-family methods for minimizing quadratically regularized nonsmooth nonconvex optimization problems, especially in the context of training nonsmooth neural networks with weight decay. Motivated by the AdamW method, we propose a novel framework for Adam-family methods with decoupled weight decay. Within our framework, the estimators for the first-order and second-order moments of stochastic subgradients are updated independently of the weight decay term. Under mild assumptions and with non-diminishing stepsizes for updating the primary optimization variables, we establish the convergence properties of our proposed framework. In addition, we show that our proposed framework encompasses a wide variety of well-known Adam-family methods, hence offering convergence guarantees for these methods in the training of nonsmooth neural networks. More importantly, we show that our proposed framework asymptotically approxi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#29992;&#25143;&#30340;&#21512;&#35268;&#35774;&#35745;&#65292;&#29992;&#20110;&#23454;&#29616;&#36879;&#26126;&#31995;&#32479;&#20013;&#30340;&#21151;&#33021;&#36879;&#26126;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36328;&#23398;&#31185;&#21512;&#20316;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08849</link><description>&lt;p&gt;
&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21151;&#33021;&#36879;&#26126;&#24615;&#30340;&#36335;&#24452;&#19982;&#26377;&#24847;&#20041;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Path To Gain Functional Transparency In Artificial Intelligence With Meaningful Explainability. (arXiv:2310.08849v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#29992;&#25143;&#30340;&#21512;&#35268;&#35774;&#35745;&#65292;&#29992;&#20110;&#23454;&#29616;&#36879;&#26126;&#31995;&#32479;&#20013;&#30340;&#21151;&#33021;&#36879;&#26126;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36328;&#23398;&#31185;&#21512;&#20316;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#24555;&#36895;&#34701;&#20837;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#24433;&#21709;&#30528;&#35832;&#22914;&#23450;&#21521;&#24191;&#21578;&#21644;&#37197;&#23545;&#31639;&#27861;&#31561;&#39046;&#22495;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#38543;&#30528;AI&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#30830;&#20445;&#20854;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#21151;&#33021;&#36879;&#26126;&#24615;&#26159;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#30340;&#19968;&#20010;&#22522;&#26412;&#26041;&#38754;&#65292;&#23427;&#20351;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#29702;&#35299;&#36825;&#20123;&#31995;&#32479;&#30340;&#20869;&#22312;&#36816;&#20316;&#65292;&#24182;&#33021;&#22815;&#35780;&#20272;&#20854;&#20844;&#27491;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#21151;&#33021;&#36879;&#26126;&#24615;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#21152;&#20197;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29992;&#25143;&#30340;&#21512;&#35268;&#35774;&#35745;&#65292;&#29992;&#20110;&#36879;&#26126;&#31995;&#32479;&#20013;&#30340;&#36879;&#26126;&#21151;&#33021;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#24320;&#21457;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#19968;&#39033;&#22797;&#26434;&#32780;&#36328;&#23398;&#31185;&#30340;&#21162;&#21147;&#65292;&#38656;&#35201;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#20262;&#29702;&#23398;&#12289;&#27861;&#24459;&#21644;&#31038;&#20250;&#30340;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is rapidly integrating into various aspects of our daily lives, influencing decision-making processes in areas such as targeted advertising and matchmaking algorithms. As AI systems become increasingly sophisticated, ensuring their transparency and explainability becomes crucial. Functional transparency is a fundamental aspect of algorithmic decision-making systems, allowing stakeholders to comprehend the inner workings of these systems and enabling them to evaluate their fairness and accuracy. However, achieving functional transparency poses significant challenges that need to be addressed. In this paper, we propose a design for user-centered compliant-by-design transparency in transparent systems. We emphasize that the development of transparent and explainable AI systems is a complex and multidisciplinary endeavor, necessitating collaboration among researchers from diverse fields such as computer science, artificial intelligence, ethics, law, and social 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#26696;&#20363;&#25512;&#29702;&#30740;&#31350;&#32773;&#23545;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24573;&#35270;&#65292;&#20197;&#21450;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#20037;&#24615;&#35760;&#24518;&#20013;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.08842</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#26696;&#20363;&#30340;&#25345;&#20037;&#24615;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
A Case-Based Persistent Memory for a Large Language Model. (arXiv:2310.08842v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#26696;&#20363;&#25512;&#29702;&#30740;&#31350;&#32773;&#23545;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24573;&#35270;&#65292;&#20197;&#21450;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#20037;&#24615;&#35760;&#24518;&#20013;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26696;&#20363;&#25512;&#29702;&#20316;&#20026;&#19968;&#31181;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#21512;&#36866;&#30340;&#35745;&#31639;&#25216;&#26415;&#12290;&#26412;&#35770;&#25991;&#25351;&#20986;&#65292;&#26696;&#20363;&#25512;&#29702;&#30340;&#30740;&#31350;&#32773;&#22312;&#36817;&#26399;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#26377;&#25152;&#24573;&#35270;&#12290;&#36817;&#26399;&#20154;&#24037;&#26234;&#33021;&#31361;&#30772;&#30340;&#28508;&#22312;&#25216;&#26415;&#21457;&#23637;&#19982;&#26696;&#20363;&#25512;&#29702;&#26377;&#30528;&#24378;&#28872;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#21487;&#20197;&#29992;&#20110;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25345;&#20037;&#24615;&#35760;&#24518;&#65292;&#23545;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26377;&#25152;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Case-based reasoning (CBR) as a methodology for problem-solving can use any appropriate computational technique. This position paper argues that CBR researchers have somewhat overlooked recent developments in deep learning and large language models (LLMs). The underlying technical developments that have enabled the recent breakthroughs in AI have strong synergies with CBR and could be used to provide a persistent memory for LLMs to make progress towards Artificial General Intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#22870;&#21169;&#26631;&#31614;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#24773;&#20917;&#19979;&#20998;&#37197;&#22870;&#21169;&#32473;&#36712;&#36857;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#36164;&#28304;&#23494;&#38598;&#22411;&#23454;&#26102;&#20132;&#20114;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.08841</link><description>&lt;p&gt;
&#22312;&#22806;&#31185;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Optimal Transport for Enhanced Offline Reinforcement Learning in Surgical Robotic Environments. (arXiv:2310.08841v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#22870;&#21169;&#26631;&#31614;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#24773;&#20917;&#19979;&#20998;&#37197;&#22870;&#21169;&#32473;&#36712;&#36857;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#36164;&#28304;&#23494;&#38598;&#22411;&#23454;&#26102;&#20132;&#20114;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;&#20195;&#29702;&#30452;&#25509;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#35266;&#23519;&#34892;&#21160;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#35797;&#38169;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20801;&#35768;&#37096;&#20998;&#35757;&#32451;&#20195;&#29702;&#19982;&#30495;&#23454;&#29289;&#29702;&#31995;&#32479;&#20132;&#20114;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#39640;&#25104;&#26412;&#12289;&#23433;&#20840;&#39118;&#38505;&#21644;&#38656;&#35201;&#25345;&#32493;&#30417;&#30563;&#12290;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#20943;&#23569;&#23545;&#36164;&#28304;&#23494;&#38598;&#22411;&#23454;&#26102;&#20132;&#20114;&#30340;&#38656;&#27714;&#26469;&#35299;&#20915;&#36825;&#20123;&#25104;&#26412;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#38656;&#35201;&#31934;&#24515;&#27880;&#37322;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#22870;&#21169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31639;&#27861;&#65292;&#21363;&#26368;&#20248;&#36755;&#36816;&#22870;&#21169;&#65288;OTR&#65289;&#26631;&#31614;&#65292;&#29992;&#20110;&#23545;&#31163;&#32447;&#36712;&#36857;&#20998;&#37197;&#22870;&#21169;&#65292;&#20351;&#29992;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#19987;&#23478;&#28436;&#31034;&#12290;OTR&#30340;&#26680;&#24515;&#21407;&#29702;&#26159;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#35745;&#31639;&#26080;&#26631;&#31614;&#36712;&#36857;&#19982;&#19987;&#23478;&#28436;&#31034;&#20043;&#38388;&#30340;&#26368;&#20248;&#23545;&#40784;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most Reinforcement Learning (RL) methods are traditionally studied in an active learning setting, where agents directly interact with their environments, observe action outcomes, and learn through trial and error. However, allowing partially trained agents to interact with real physical systems poses significant challenges, including high costs, safety risks, and the need for constant supervision. Offline RL addresses these cost and safety concerns by leveraging existing datasets and reducing the need for resource-intensive real-time interactions. Nevertheless, a substantial challenge lies in the demand for these datasets to be meticulously annotated with rewards. In this paper, we introduce Optimal Transport Reward (OTR) labelling, an innovative algorithm designed to assign rewards to offline trajectories, using a small number of high-quality expert demonstrations. The core principle of OTR involves employing Optimal Transport (OT) to calculate an optimal alignment between an unlabele
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SAFARI&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20010;&#24615;&#21270;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#28304;&#35745;&#21010;&#22120;&#65292;&#20351;&#24471;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20381;&#36182;&#20851;&#31995;&#33021;&#22815;&#34987;&#25972;&#21512;&#65292;&#24182;&#33021;&#29983;&#25104;&#19968;&#33268;&#30340;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.08840</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20010;&#24615;&#21270;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#28304;&#35745;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue. (arXiv:2310.08840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SAFARI&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20010;&#24615;&#21270;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#28304;&#35745;&#21010;&#22120;&#65292;&#20351;&#24471;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20381;&#36182;&#20851;&#31995;&#33021;&#22815;&#34987;&#25972;&#21512;&#65292;&#24182;&#33021;&#29983;&#25104;&#19968;&#33268;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#19981;&#21516;&#30340;&#30693;&#35782;&#28304;&#26469;&#29983;&#25104;&#26356;&#20855;&#20449;&#24687;&#24615;&#21644;&#35777;&#25454;&#24615;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#35201;&#20040;&#19987;&#27880;&#20110;&#21333;&#19968;&#30693;&#35782;&#28304;&#65292;&#35201;&#20040;&#24573;&#35270;&#20102;&#22810;&#20010;&#30693;&#35782;&#28304;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#19981;&#19968;&#33268;&#29978;&#33267;&#30683;&#30462;&#30340;&#22238;&#24212;&#12290;&#20026;&#20102;&#25972;&#21512;&#22810;&#20010;&#30693;&#35782;&#28304;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAFARI&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#20986;&#33394;&#33021;&#21147;&#26469;&#35268;&#21010;&#12289;&#29702;&#35299;&#21644;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SAFARI&#23558;&#30693;&#35782;&#30340;&#22522;&#30784;&#20998;&#35299;&#20026;&#22810;&#20010;&#28304;&#21644;&#22238;&#24212;&#29983;&#25104;&#65292;&#20174;&#32780;&#26041;&#20415;&#22320;&#25193;&#23637;&#21040;&#21508;&#31181;&#30693;&#35782;&#28304;&#65292;&#21253;&#25324;&#19981;&#20351;&#29992;&#20219;&#20309;&#28304;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#12289;&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain dialogue system usually requires different sources of knowledge to generate more informative and evidential responses. However, existing knowledge-grounded dialogue systems either focus on a single knowledge source or overlook the dependency between multiple sources of knowledge, which may result in generating inconsistent or even paradoxical responses. To incorporate multiple knowledge sources and dependencies between them, we propose SAFARI, a novel framework that leverages the exceptional capabilities of large language models (LLMs) in planning, understanding, and incorporating under both supervised and unsupervised settings. Specifically, SAFARI decouples the knowledge grounding into multiple sources and response generation, which allows easy extension to various knowledge sources including the possibility of not using any sources. To study the problem, we construct a personalized knowledge-grounded dialogue dataset \textit{\textbf{K}nowledge \textbf{B}ehind \textbf{P}e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35266;&#23519;&#26144;&#23556;&#21644;&#34892;&#20026;&#20811;&#38534;&#36827;&#34892;&#23569;&#26679;&#26412;&#31574;&#30053;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#26144;&#23556;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#35266;&#23519;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#26144;&#23556;&#26469;&#20811;&#38534;&#28304;&#20219;&#21153;&#30340;&#25104;&#21151;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.08836</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#26144;&#23556;&#21644;&#34892;&#20026;&#20811;&#38534;&#36827;&#34892;&#23569;&#26679;&#26412;&#31574;&#30053;&#36716;&#31227;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Few-Shot Policy Transfer through Observation Mapping and Behavior Cloning. (arXiv:2310.08836v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35266;&#23519;&#26144;&#23556;&#21644;&#34892;&#20026;&#20811;&#38534;&#36827;&#34892;&#23569;&#26679;&#26412;&#31574;&#30053;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#26144;&#23556;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#35266;&#23519;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#26144;&#23556;&#26469;&#20811;&#38534;&#28304;&#20219;&#21153;&#30340;&#25104;&#21151;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#20132;&#20114;&#25104;&#26412;&#65292;&#35768;&#22810;&#20219;&#21153;&#20173;&#28982;&#38590;&#20197;&#35299;&#20915;&#12290;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#23558;&#22312;&#28304;&#39046;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#30446;&#26631;&#39046;&#22495;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;Sim2Real&#36801;&#31227;&#26377;&#21161;&#20110;&#23558;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#39046;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#29289;&#29702;&#30446;&#26631;&#39046;&#22495;&#12290;&#30693;&#35782;&#36801;&#31227;&#20943;&#23569;&#20102;&#22312;&#20132;&#20114;&#25104;&#26412;&#39640;&#30340;&#29289;&#29702;&#19990;&#30028;&#20013;&#35757;&#32451;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#20004;&#20010;&#39046;&#22495;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#29289;&#29702;&#23646;&#24615;&#23436;&#20840;&#23545;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35266;&#23519;&#26144;&#23556;&#21644;&#34892;&#20026;&#20811;&#38534;&#36827;&#34892;&#23569;&#26679;&#26412;&#31574;&#30053;&#36716;&#31227;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#26144;&#23556;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#35266;&#23519;&#65292;&#24182;&#19988;&#21518;&#32493;&#20351;&#29992;&#36825;&#20010;&#23398;&#21040;&#30340;&#26144;&#23556;&#26469;&#20811;&#38534;&#28304;&#20219;&#21153;&#30340;&#25104;&#21151;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in Reinforcement Learning for robotics applications, many tasks remain prohibitively difficult to solve because of the expensive interaction cost. Transfer learning helps reduce the training time in the target domain by transferring knowledge learned in a source domain. Sim2Real transfer helps transfer knowledge from a simulated robotic domain to a physical target domain. Knowledge transfer reduces the time required to train a task in the physical world, where the cost of interactions is high. However, most existing approaches assume exact correspondence in the task structure and the physical properties of the two domains. This work proposes a framework for Few-Shot Policy Transfer between two domains through Observation Mapping and Behavior Cloning. We use Generative Adversarial Networks (GANs) along with a cycle-consistency loss to map the observations between the source and target domains and later use this learned mapping to clone the successful source task 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#19982;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22478;&#24066;&#26080;&#20154;&#26426;&#22312;&#32039;&#24613;&#25628;&#32034;&#21644;&#25937;&#25588;&#20013;&#30340;&#23548;&#33322;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22478;&#24066;&#24067;&#23616;&#22270;&#20687;&#25968;&#25454;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#22815;&#33258;&#20027;&#20915;&#31574;&#23548;&#33322;&#12289;&#20248;&#21270;&#36335;&#24452;&#24182;&#25269;&#25239;&#39118;&#25928;&#24212;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#22478;&#24066;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#25937;&#25588;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.08830</link><description>&lt;p&gt;
&#22478;&#24066;&#26080;&#20154;&#26426;&#23548;&#33322;&#65306;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#19982;&#31354;&#27668;&#21160;&#21147;&#23398;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Urban Drone Navigation: Autoencoder Learning Fusion for Aerodynamics. (arXiv:2310.08830v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#19982;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22478;&#24066;&#26080;&#20154;&#26426;&#22312;&#32039;&#24613;&#25628;&#32034;&#21644;&#25937;&#25588;&#20013;&#30340;&#23548;&#33322;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22478;&#24066;&#24067;&#23616;&#22270;&#20687;&#25968;&#25454;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#22815;&#33258;&#20027;&#20915;&#31574;&#23548;&#33322;&#12289;&#20248;&#21270;&#36335;&#24452;&#24182;&#25269;&#25239;&#39118;&#25928;&#24212;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#22478;&#24066;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#25937;&#25588;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20197;&#21450;&#23384;&#22312;&#24314;&#31569;&#29289;&#21644;&#39118;&#31561;&#38556;&#30861;&#29289;&#30340;&#25361;&#25112;&#65292;&#26080;&#20154;&#26426;&#23545;&#20110;&#22478;&#24066;&#32039;&#24613;&#25628;&#32034;&#21644;&#25937;&#25588;&#65288;SAR&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#19982;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#22478;&#24066;SAR&#20013;&#30340;&#26080;&#20154;&#26426;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;MORL&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#65292;&#24182;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#30340;&#39118;&#27169;&#25311;&#12290;&#36890;&#36807;&#21033;&#29992;&#22478;&#24066;&#24067;&#23616;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#26080;&#20154;&#26426;&#21487;&#20197;&#33258;&#20027;&#20570;&#20986;&#23548;&#33322;&#20915;&#31574;&#65292;&#20248;&#21270;&#36335;&#24452;&#65292;&#24182;&#25269;&#28040;&#20256;&#32479;&#20256;&#24863;&#22120;&#26080;&#27861;&#35299;&#20915;&#30340;&#39118;&#25928;&#24212;&#12290;&#22312;&#32445;&#32422;&#24066;&#27169;&#22411;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#26080;&#20154;&#26426;&#22312;&#22797;&#26434;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;SAR&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drones are vital for urban emergency search and rescue (SAR) due to the challenges of navigating dynamic environments with obstacles like buildings and wind. This paper presents a method that combines multi-objective reinforcement learning (MORL) with a convolutional autoencoder to improve drone navigation in urban SAR. The approach uses MORL to achieve multiple goals and the autoencoder for cost-effective wind simulations. By utilizing imagery data of urban layouts, the drone can autonomously make navigation decisions, optimize paths, and counteract wind effects without traditional sensors. Tested on a New York City model, this method enhances drone SAR operations in complex urban settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Distance-rank&#24863;&#30693;&#39034;&#24207;&#22870;&#21169;&#23398;&#20064;&#65288;DRASRL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#36712;&#36857;&#25490;&#21517;&#27169;&#31946;&#21644;&#22870;&#21169;&#27169;&#31946;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08823</link><description>&lt;p&gt;
Distance-rank&#24863;&#30693;&#39034;&#24207;&#22870;&#21169;&#23398;&#20064;&#29992;&#20110;&#20855;&#26377;&#27425;&#20248;&#28436;&#31034;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distance-rank Aware Sequential Reward Learning for Inverse Reinforcement Learning with Sub-optimal Demonstrations. (arXiv:2310.08823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08823
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Distance-rank&#24863;&#30693;&#39034;&#24207;&#22870;&#21169;&#23398;&#20064;&#65288;DRASRL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#36712;&#36857;&#25490;&#21517;&#27169;&#31946;&#21644;&#22870;&#21169;&#27169;&#31946;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26088;&#22312;&#22522;&#20110;&#25910;&#38598;&#21040;&#30340;&#19987;&#23478;&#28436;&#31034;&#26126;&#30830;&#25512;&#26029;&#20986;&#28508;&#22312;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#32771;&#34385;&#21040;&#33719;&#21462;&#19987;&#23478;&#28436;&#31034;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#65292;&#24403;&#21069;IRL&#25216;&#26415;&#30340;&#37325;&#28857;&#26159;&#20351;&#29992;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23548;&#20986;&#30340;&#22870;&#21169;&#20989;&#25968;&#23398;&#20064;&#19968;&#20010;&#20248;&#20110;&#28436;&#31034;&#32773;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;IRL&#31639;&#27861;&#20027;&#35201;&#35299;&#20915;&#20102;&#22312;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#26102;&#30340;&#36712;&#36857;&#25490;&#21517;&#27169;&#31946;&#30340;&#25361;&#25112;&#65292;&#21364;&#24573;&#35270;&#20102;&#22312;&#36827;&#19968;&#27493;&#28040;&#38500;&#22870;&#21169;&#27169;&#31946;&#24615;&#26041;&#38754;&#65292;&#32771;&#34385;&#36712;&#36857;&#20043;&#38388;&#30340;&#22238;&#25253;&#24046;&#24322;&#31243;&#24230;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#21333;&#20010;&#36716;&#25442;&#30340;&#22870;&#21169;&#21463;&#21040;&#36712;&#36857;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Distance-rank&#24863;&#30693;&#39034;&#24207;&#22870;&#21169;&#23398;&#20064;&#65288;DRASRL&#65289;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;DRASRL&#21516;&#26102;&#32771;&#34385;&#20102;&#36712;&#36857;&#30340;&#25490;&#21517;&#21644;&#22238;&#25253;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning (IRL) aims to explicitly infer an underlying reward function based on collected expert demonstrations. Considering that obtaining expert demonstrations can be costly, the focus of current IRL techniques is on learning a better-than-demonstrator policy using a reward function derived from sub-optimal demonstrations. However, existing IRL algorithms primarily tackle the challenge of trajectory ranking ambiguity when learning the reward function. They overlook the crucial role of considering the degree of difference between trajectories in terms of their returns, which is essential for further removing reward ambiguity. Additionally, it is important to note that the reward of a single transition is heavily influenced by the context information within the trajectory. To address these issues, we introduce the Distance-rank Aware Sequential Reward Learning (DRASRL) framework. Unlike existing approaches, DRASRL takes into account both the ranking of trajectories
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#37327;&#34920;&#31572;&#39064;&#36807;&#31243;&#20013;&#30340;&#21453;&#24212;&#26102;&#38388;&#19982;&#22833;&#30496;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#39044;&#27979;&#21442;&#19982;&#32773;&#26159;&#21542;&#23384;&#22312;&#22833;&#30496;&#12290;</title><link>http://arxiv.org/abs/2310.08817</link><description>&lt;p&gt;
&#25506;&#32034;&#37327;&#34920;&#31572;&#39064;&#36807;&#31243;&#20013;&#21453;&#24212;&#26102;&#38388;&#24207;&#21015;&#19982;&#22833;&#30496;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring the relationship between response time sequence in scale answering process and severity of insomnia: a machine learning approach. (arXiv:2310.08817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08817
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#37327;&#34920;&#31572;&#39064;&#36807;&#31243;&#20013;&#30340;&#21453;&#24212;&#26102;&#38388;&#19982;&#22833;&#30496;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#39044;&#27979;&#21442;&#19982;&#32773;&#26159;&#21542;&#23384;&#22312;&#22833;&#30496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#22833;&#30496;&#19982;&#21453;&#24212;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#24320;&#21457;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#21442;&#19982;&#32773;&#26159;&#21542;&#23384;&#22312;&#22833;&#30496;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#20174;2729&#21517;&#21442;&#19982;&#32773;&#37027;&#37324;&#25910;&#38598;&#21040;&#37327;&#34920;&#27979;&#35797;&#21644;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#12290;&#30740;&#31350;&#21457;&#29616;&#22833;&#30496;&#30151;&#29366;&#30340;&#21442;&#19982;&#32773;&#19982;&#38750;&#22833;&#30496;&#30151;&#29366;&#30340;&#21442;&#19982;&#32773;&#22312;&#24635;&#21453;&#24212;&#26102;&#38388;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65288;p &lt;0.001&#65289;&#12290;&#22312;&#20010;&#20307;&#38382;&#39064;&#27700;&#24179;&#19978;&#35266;&#23519;&#21040;&#29305;&#23450;&#22833;&#30496;&#26041;&#38754;&#30340;&#20005;&#37325;&#31243;&#24230;&#21644;&#21453;&#24212;&#26102;&#38388;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22522;&#20110;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#39044;&#27979;&#22833;&#30496;&#30151;&#29366;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#65288;0.743&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objectives: The study aims to investigate the relationship between insomnia and response time. Additionally, it aims to develop a machine learning model to predict the presence of insomnia in participants using response time data. Methods: A mobile application was designed to administer scale tests and collect response time data from 2729 participants. The relationship between symptom severity and response time was explored, and a machine learning model was developed to predict the presence of insomnia. Results: The result revealed a statistically significant difference (p&lt;.001) in the total response time between participants with or without insomnia symptoms. A correlation was observed between the severity of specific insomnia aspects and response times at the individual questions level. The machine learning model demonstrated a high predictive accuracy of 0.743 in predicting insomnia symptoms based on response time data. Conclusions: These findings highlight the potential utility of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;SCRL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#12290;&#35813;&#31639;&#27861;&#22312;&#22522;&#32447;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08809</link><description>&lt;p&gt;
DexCatch: &#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#20219;&#24847;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands. (arXiv:2310.08809v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;SCRL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#12290;&#35813;&#31639;&#27861;&#22312;&#22522;&#32447;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#23454;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#28789;&#24039;&#25805;&#32437;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#25343;&#21462;&#21644;&#25918;&#32622;&#20219;&#21153;&#30340;&#25104;&#21151;&#29575;&#19978;&#12290;&#19982;&#25343;&#21462;&#21644;&#25918;&#32622;&#30456;&#27604;&#65292;&#25243;&#25509;&#34892;&#20026;&#26377;&#28508;&#21147;&#22312;&#26080;&#38656;&#23558;&#29289;&#20307;&#36816;&#36865;&#21040;&#30446;&#30340;&#22320;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#25343;&#21462;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#21160;&#24577;&#30340;&#28789;&#24039;&#25805;&#32437;&#30001;&#20110;&#22823;&#37327;&#30340;&#21160;&#24577;&#25509;&#35302;&#32780;&#38754;&#20020;&#30528;&#31283;&#23450;&#25511;&#21046;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#65288;SCRL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#12290;&#35813;&#31639;&#27861;&#22312;&#22522;&#32447;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#22312;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#36801;&#31227;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#25163;&#20013;&#30340;&#29289;&#20307;&#38754;&#21521;&#20391;&#38754;&#38750;&#24120;&#19981;&#31283;&#23450;&#65292;&#30001;&#20110;&#32570;&#20047;&#26469;&#33258;&#25163;&#25484;&#30340;&#25903;&#25745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#28982;&#21487;&#20197;&#22312;&#26368;&#20855;&#25361;&#25112;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#34892;&#20026;&#30340;&#35270;&#39057;&#28436;&#31034;&#21644;&#21512;&#20316;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving human-like dexterous manipulation remains a crucial area of research in robotics. Current research focuses on improving the success rate of pick-and-place tasks. Compared with pick-and-place, throw-catching behavior has the potential to increase picking speed without transporting objects to their destination. However, dynamic dexterous manipulation poses a major challenge for stable control due to a large number of dynamic contacts. In this paper, we propose a Stability-Constrained Reinforcement Learning (SCRL) algorithm to learn to catch diverse objects with dexterous hands. The SCRL algorithm outperforms baselines by a large margin, and the learned policies show strong zero-shot transfer performance on unseen objects. Remarkably, even though the object in a hand facing sideward is extremely unstable due to the lack of support from the palm, our method can still achieve a high level of success in the most challenging task. Video demonstrations of learned behaviors and the co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#35748;&#30693;&#31185;&#23398;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#26469;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#26412;&#31687;&#32508;&#36848;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24863;&#30693;&#30340;&#35748;&#30693;&#21151;&#33021;&#65292;&#36890;&#36807;&#27604;&#36739;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#36807;&#31243;&#65292;&#22238;&#39038;&#20102;&#21508;&#20010;&#23376;&#23398;&#31185;&#30340;&#20027;&#35201;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.08803</link><description>&lt;p&gt;
&#36890;&#36807;&#35748;&#30693;&#31185;&#23398;&#21407;&#29702;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24863;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Advancing Perception in Artificial Intelligence through Principles of Cognitive Science. (arXiv:2310.08803v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08803
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#35748;&#30693;&#31185;&#23398;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#26469;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#26412;&#31687;&#32508;&#36848;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24863;&#30693;&#30340;&#35748;&#30693;&#21151;&#33021;&#65292;&#36890;&#36807;&#27604;&#36739;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#36807;&#31243;&#65292;&#22238;&#39038;&#20102;&#21508;&#20010;&#23376;&#23398;&#31185;&#30340;&#20027;&#35201;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#22312;&#36805;&#29467;&#21457;&#23637;&#20013;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#23601;&#65292;&#20294;&#22312;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#26041;&#38754;&#20173;&#23384;&#22312;&#38382;&#39064;&#21644;&#23616;&#38480;&#12290;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#23558;&#24456;&#22823;&#19968;&#37096;&#20998;&#24615;&#33021;&#26631;&#20934;&#22522;&#20934;&#23450;&#20026;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#35748;&#30693;&#31185;&#23398;&#20026;&#28789;&#24863;&#30340;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#30740;&#31350;&#35748;&#30693;&#31185;&#23398;&#21487;&#20197;&#20026;&#26500;&#24314;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#22522;&#26412;&#27169;&#22359;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#65292;&#20174;&#32780;&#25913;&#21892;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#26412;&#32508;&#36848;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24863;&#30693;&#30340;&#35748;&#30693;&#21151;&#33021;&#65292;&#21363;&#25509;&#25910;&#26469;&#33258;&#21608;&#22260;&#29615;&#22659;&#30340;&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#22788;&#29702;&#20197;&#29702;&#35299;&#29615;&#22659;&#30340;&#36807;&#31243;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#35270;&#35282;&#30740;&#31350;&#21644;&#27604;&#36739;&#20854;&#21508;&#20010;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24403;&#21069;&#35748;&#30693;&#31185;&#23398;&#65288;&#29305;&#21035;&#26159;&#31070;&#32463;&#31185;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#35821;&#35328;&#23398;&#65289;&#21508;&#20010;&#23376;&#23398;&#31185;&#30340;&#25152;&#26377;&#20027;&#35201;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although artificial intelligence (AI) has achieved many feats at a rapid pace, there still exist open problems and fundamental shortcomings related to performance and resource efficiency. Since AI researchers benchmark a significant proportion of performance standards through human intelligence, cognitive sciences-inspired AI is a promising domain of research. Studying cognitive science can provide a fresh perspective to building fundamental blocks in AI research, which can lead to improved performance and efficiency. In this review paper, we focus on the cognitive functions of perception, which is the process of taking signals from one's surroundings as input, and processing them to understand the environment. Particularly, we study and compare its various processes through the lens of both cognitive sciences and AI. Through this study, we review all current major theories from various sub-disciplines of cognitive science (specifically neuroscience, psychology and linguistics), and dr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDMT&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#22122;&#22768;&#21644;&#24369;&#36523;&#20221;&#26144;&#23556;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#21160;&#24577;&#37051;&#23621;&#25513;&#33180;&#26426;&#21046;(ADNM)&#20197;&#21450;&#38598;&#25104;Transformer&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.08800</link><description>&lt;p&gt;
DDMT: &#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#21435;&#22122;&#25193;&#25955;&#25513;&#33180;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DDMT: Denoising Diffusion Mask Transformer Models for Multivariate Time Series Anomaly Detection. (arXiv:2310.08800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDMT&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#22122;&#22768;&#21644;&#24369;&#36523;&#20221;&#26144;&#23556;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#21160;&#24577;&#37051;&#23621;&#25513;&#33180;&#26426;&#21046;(ADNM)&#20197;&#21450;&#38598;&#25104;Transformer&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#24050;&#32463;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22312;&#27450;&#35784;&#26816;&#27979;&#12289;&#25925;&#38556;&#35786;&#26029;&#21644;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#31561;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#37325;&#24314;&#30340;&#27169;&#22411;&#22312;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#35268;&#27169;&#21644;&#32500;&#24230;&#30340;&#24555;&#36895;&#22686;&#21152;&#65292;&#26102;&#38388;&#24207;&#21015;&#37325;&#24314;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#21644;&#24369;&#36523;&#20221;&#26144;&#23556;&#38382;&#39064;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21160;&#24577;&#37051;&#23621;&#25513;&#33180;&#26426;&#21046;(ADNM)&#65292;&#23558;&#20854;&#19982;Transformer&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#21517;&#20026;Denoising Diffusion Mask Transformer (DDMT) &#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;ADNM&#27169;&#22359;&#29992;&#20110;&#20943;&#36731;&#25968;&#25454;&#37325;&#24314;&#36807;&#31243;&#20013;&#36755;&#20837;&#21644;&#36755;&#20986;&#29305;&#24449;&#20043;&#38388;&#30340;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#24369;&#36523;&#20221;&#26144;&#23556;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in multivariate time series has emerged as a crucial challenge in time series research, with significant research implications in various fields such as fraud detection, fault diagnosis, and system state estimation. Reconstruction-based models have shown promising potential in recent years for detecting anomalies in time series data. However, due to the rapid increase in data scale and dimensionality, the issues of noise and Weak Identity Mapping (WIM) during time series reconstruction have become increasingly pronounced. To address this, we introduce a novel Adaptive Dynamic Neighbor Mask (ADNM) mechanism and integrate it with the Transformer and Denoising Diffusion Model, creating a new framework for multivariate time series anomaly detection, named Denoising Diffusion Mask Transformer (DDMT). The ADNM module is introduced to mitigate information leakage between input and output features during data reconstruction, thereby alleviating the problem of WIM during recon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#21387;&#32553;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20960;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;MiniLMv2&#30340;MHA&#36716;&#31227;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.08797</link><description>&lt;p&gt;
&#23545;&#20110;&#21387;&#32553;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#33976;&#39311;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models. (arXiv:2310.08797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#21387;&#32553;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20960;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;MiniLMv2&#30340;MHA&#36716;&#31227;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#20214;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#25512;&#26029;&#25104;&#26412;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#24120;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#25552;&#39640;&#20854;&#25928;&#29575;&#24182;&#20445;&#25345;&#22823;&#37096;&#20998;&#25928;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#29616;&#12289;&#27604;&#36739;&#21644;&#20998;&#26512;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#12289;&#29992;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#19981;&#21487;&#30693;(&#36890;&#29992;)&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36755;&#20986;&#20998;&#24067;(OD)&#36716;&#31227;&#12289;&#38544;&#34255;&#29366;&#24577;(HS)&#36716;&#31227;&#20197;&#21450;&#22522;&#20110;MiniLMv2&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;(MHA)&#36716;&#31227;&#31561;&#22810;&#31181;&#33976;&#39311;&#26041;&#27861;&#22312;&#21508;&#31181;&#23398;&#29983;&#26550;&#26500;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#21333;&#35821;(&#33521;&#35821;)&#21644;&#22810;&#35821;&#35774;&#32622;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;MiniLMv2&#30340;MHA&#36716;&#31227;&#36890;&#24120;&#26159;&#33976;&#39311;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#24182;&#35299;&#37322;&#20102;&#28508;&#22312;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have become a vital component in modern NLP, achieving state of the art performance in a variety of tasks. However, they are often inefficient for real-world deployment due to their expensive inference costs. Knowledge distillation is a promising technique to improve their efficiency while retaining most of their effectiveness. In this paper, we reproduce, compare and analyze several representative methods for task-agnostic (general-purpose) distillation of Transformer language models. Our target of study includes Output Distribution (OD) transfer, Hidden State (HS) transfer with various layer mapping strategies, and Multi-Head Attention (MHA) transfer based on MiniLMv2. Through our extensive experiments, we study the effectiveness of each method for various student architectures in both monolingual (English) and multilingual settings. Overall, we show that MHA transfer based on MiniLMv2 is generally the best option for distillation and explain the potential reaso
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.08795</link><description>&lt;p&gt;
&#36890;&#36807;&#36861;&#36394;&#20559;&#35265;&#24433;&#21709;&#26469;&#20943;&#36731;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Bias for Question Answering Models by Tracking Bias Influence. (arXiv:2310.08795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#27169;&#22411;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#65292;&#32780;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23588;&#20854;&#26377;&#23475;&#65292;&#22240;&#20026;&#36755;&#20986;&#30340;&#31572;&#26696;&#21487;&#33021;&#30452;&#25509;&#34987;&#26368;&#32456;&#29992;&#25143;&#20351;&#29992;&#12290;&#24050;&#32463;&#26377;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;QA&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20294;&#26159;&#23545;&#20110;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#20173;&#22788;&#20110;&#25506;&#32034;&#38454;&#27573;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#22522;&#20110;&#19968;&#20010;&#30452;&#35273;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#20174;&#19968;&#20010;&#26377;&#20559;&#35265;&#30340;&#20363;&#23376;&#20013;&#23398;&#21040;&#20102;&#19996;&#35199;&#65292;&#23427;&#21487;&#33021;&#26356;&#23481;&#26131;&#20986;&#29616;&#20559;&#35265;&#65292;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#26469;&#34913;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;&#22914;&#26524;&#21463;&#21040;&#24433;&#21709;&#30340;&#23454;&#20363;&#26356;&#20559;&#35265;&#65292;&#25105;&#20204;&#35748;&#20026;&#26597;&#35810;&#23454;&#20363;&#26159;&#26377;&#20559;&#35265;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#27979;&#21040;&#30340;&#20559;&#35265;&#31243;&#24230;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#38500;&#20102;&#21407;&#26469;&#30340;QA&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#20840;&#38754;&#32780;&#25935;&#24863;&#30340;&#26041;&#24335;&#37327;&#21270;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20943;&#36731;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applie
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#30740;&#31350;&#20102;&#26631;&#31614;&#22122;&#22768;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#28216;&#25103;&#27169;&#22411;&#26469;&#29702;&#35299;&#23458;&#25143;&#31471;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22343;&#34913;&#32467;&#26524;&#30340;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24230;&#20302;&#20110;&#31038;&#20250;&#26368;&#20248;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#26469;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2310.08790</link><description>&lt;p&gt;
&#36136;&#37327;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Price of Stability in Quality-Aware Federated Learning. (arXiv:2310.08790v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#30740;&#31350;&#20102;&#26631;&#31614;&#22122;&#22768;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#28216;&#25103;&#27169;&#22411;&#26469;&#29702;&#35299;&#23458;&#25143;&#31471;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22343;&#34913;&#32467;&#26524;&#30340;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24230;&#20302;&#20110;&#31038;&#20250;&#26368;&#20248;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#26469;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#23458;&#25143;&#31471;&#22312;&#19981;&#20132;&#25442;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#26631;&#31614;&#22122;&#22768;&#30340;&#23384;&#22312;&#20250;&#20005;&#37325;&#24433;&#21709;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#19968;&#20123;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#20851;&#27880;&#20102;&#29992;&#20110;&#26631;&#31614;&#21435;&#22122;&#30340;&#31639;&#27861;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#35270;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#30001;&#20110;&#33258;&#21033;&#24615;&#21644;&#23545;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#30340;&#24322;&#36136;&#20272;&#20540;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#19981;&#20250;&#24212;&#29992;&#26114;&#36149;&#30340;&#26631;&#31614;&#21435;&#22122;&#31574;&#30053;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23558;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20114;&#21160;&#24314;&#27169;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#31614;&#21435;&#22122;&#21338;&#24328;&#65292;&#24182;&#30830;&#23450;&#20854;&#22343;&#34913;&#29366;&#24577;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#31283;&#23450;&#25104;&#26412;&#65292;&#35813;&#25104;&#26412;&#29992;&#26469;&#34913;&#37327;&#22343;&#34913;&#32467;&#26524;&#19982;&#31038;&#20250;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#31995;&#32479;&#24615;&#33021;&#24046;&#24322;&#65288;&#20363;&#22914;&#65292;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24230;&#12289;&#31038;&#20250;&#31119;&#21033;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22343;&#34913;&#32467;&#26524;&#30340;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24230;&#22987;&#32456;&#20302;&#20110;&#31038;&#20250;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#26469;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning scheme that enables clients to train a shared global model without exchanging local data. The presence of label noise can severely degrade the FL performance, and some existing studies have focused on algorithm design for label denoising. However, they ignored the important issue that clients may not apply costly label denoising strategies due to them being self-interested and having heterogeneous valuations on the FL performance. To fill this gap, we model the clients' interactions as a novel label denoising game and characterize its equilibrium. We also analyze the price of stability, which quantifies the difference in the system performance (e.g., global model accuracy, social welfare) between the equilibrium outcome and the socially optimal solution. We prove that the equilibrium outcome always leads to a lower global model accuracy than the socially optimal solution does. We further design an efficient algorithm to compute 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeltaSpace&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#12290;&#22312;DeltaSpace&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;DeltaEdit&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08785</link><description>&lt;p&gt;
DeltaSpace:&#19968;&#31181;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#30340;&#35821;&#20041;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing. (arXiv:2310.08785v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeltaSpace&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#12290;&#22312;DeltaSpace&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;DeltaEdit&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#38754;&#20020;&#30528;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#35768;&#22810;&#25991;&#29486;&#36890;&#36807;&#25910;&#38598;&#22823;&#37327;&#26631;&#27880;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#26469;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#26082;&#26114;&#36149;&#21448;&#20302;&#25928;&#12290;&#28982;&#21518;&#65292;&#19968;&#20123;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20986;&#29616;&#20102;&#65292;&#20197;&#36991;&#20813;&#25968;&#25454;&#25910;&#38598;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#22522;&#20110;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#30340;&#20248;&#21270;&#25110;&#25512;&#29702;&#26102;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#21644;&#30830;&#23450;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#31354;&#38388;&#65292;&#31216;&#20026;CLIP DeltaSpace&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#20004;&#20010;&#22270;&#20687;&#30340;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#19982;&#20854;&#23545;&#24212;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;CLIP&#25991;&#26412;&#29305;&#24449;&#24046;&#24322;&#22312;&#35821;&#20041;&#19978;&#26159;&#23545;&#40784;&#30340;&#12290;&#22522;&#20110;DeltaSpace&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DeltaEdit&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided image editing faces significant challenges to training and inference flexibility. Much literature collects large amounts of annotated image-text pairs to train text-conditioned generative models from scratch, which is expensive and not efficient. After that, some approaches that leverage pre-trained vision-language models are put forward to avoid data collection, but they are also limited by either per text-prompt optimization or inference-time hyper-parameters tuning. To address these issues, we investigate and identify a specific space, referred to as CLIP DeltaSpace, where the CLIP visual feature difference of two images is semantically aligned with the CLIP textual feature difference of their corresponding text descriptions. Based on DeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP visual feature differences to the latent space directions of a generative model during the training phase, and predicts the latent space directions from the CLIP
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08782</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#39537;&#21160;&#29983;&#20135;&#21147;&#65306;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning. (arXiv:2310.08782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#22522;&#30784;&#35774;&#26045;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20462;&#21098;&#65288;DP&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20986;&#29616;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#21024;&#38500;&#20887;&#20313;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;DP&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19979;&#28216;&#30446;&#26631;&#20219;&#21153;&#20013;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#21644;&#23436;&#25972;&#24494;&#35843;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20462;&#21098;&#28304;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;DP&#38382;&#39064;&#20173;&#28982;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;DP&#21644;&#36801;&#31227;&#23398;&#20064;&#35270;&#20026;&#29420;&#31435;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#23558;DP&#19982;&#36801;&#31227;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;DP&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;DP&#26041;&#27861;&#65292;&#21363;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#65292;&#29992;&#20110;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, i.e., how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32431;&#25552;&#31034;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#36825;&#20123;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#25512;&#36827;&#36879;&#26126;&#24230;&#21644;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.08780</link><description>&lt;p&gt;
&#8220;&#25105;&#19981;&#26159;&#31181;&#26063;&#20027;&#20041;&#32773;&#65292;&#20294;&#26159;&#8230;&#8230;&#8221;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"Im not Racist but...": Discovering Bias in the Internal Knowledge of Large Language Models. (arXiv:2310.08780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32431;&#25552;&#31034;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#36825;&#20123;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#25512;&#36827;&#36879;&#26126;&#24230;&#21644;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#22312;&#19981;&#26029;&#25193;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#35777;&#26126;&#23384;&#22312;&#20869;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#25110;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#23427;&#20204;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#32431;&#25552;&#31034;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;&#20219;&#24847;LLM&#20013;&#38544;&#34255;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#29983;&#25104;&#20102;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#20174;&#32780;&#33021;&#22815;&#35782;&#21035;LLM&#20869;&#37096;&#30693;&#35782;&#20013;&#32534;&#30721;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25581;&#31034;LLM&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#24182;&#25552;&#20379;&#19968;&#31181;&#31995;&#32479;&#24615;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#25512;&#36827;&#36879;&#26126;&#24230;&#21644;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have garnered significant attention for their remarkable performance in a continuously expanding set of natural language processing tasks. However, these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications. In this paper, we introduce a novel, purely prompt-based approach to uncover hidden stereotypes within any arbitrary LLM. Our approach dynamically generates a knowledge representation of internal stereotypes, enabling the identification of biases encoded within the LLM's internal knowledge. By illuminating the biases present in LLMs and offering a systematic methodology for their analysis, our work contributes to advancing transparency and promoting fairness in natural language processing systems.
&lt;/p&gt;</description></item><item><title>ChatGPT&#34987;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#35299;&#20915;&#26126;&#30830;&#35268;&#23450;&#30340;&#38382;&#39064;&#26102;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#20294;&#22312;&#35299;&#20915;&#26410;&#26126;&#30830;&#35268;&#23450;&#30340;&#38382;&#39064;&#26102;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#38169;&#35823;&#35299;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19977;&#31181;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;LLM&#22686;&#24378;&#30340;&#25945;&#26448;&#25552;&#21319;STEM&#25945;&#32946;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#23545;&#20110;AI&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#20063;&#26377;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.08773</link><description>&lt;p&gt;
&#25506;&#31350;ChatGPT&#22312;&#31185;&#23398;&#19982;&#24037;&#31243;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#28508;&#21147;&#19982;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Examining the Potential and Pitfalls of ChatGPT in Science and Engineering Problem-Solving. (arXiv:2310.08773v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08773
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#34987;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#35299;&#20915;&#26126;&#30830;&#35268;&#23450;&#30340;&#38382;&#39064;&#26102;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#20294;&#22312;&#35299;&#20915;&#26410;&#26126;&#30830;&#35268;&#23450;&#30340;&#38382;&#39064;&#26102;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#38169;&#35823;&#35299;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19977;&#31181;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;LLM&#22686;&#24378;&#30340;&#25945;&#26448;&#25552;&#21319;STEM&#25945;&#32946;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#23545;&#20110;AI&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#20063;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;OpenAI&#30340;ChatGPT&#22312;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#29289;&#29702;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;&#20351;&#29992;ChatGPT&#65288;&#20855;&#26377;GPT-4&#65289;&#26469;&#35299;&#20915;&#20102;&#26469;&#33258;&#22823;&#23398;&#32423;&#24037;&#31243;&#29289;&#29702;&#35838;&#31243;&#30340;40&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#33539;&#22260;&#20174;&#26377;&#26126;&#30830;&#35268;&#23450;&#30340;&#38382;&#39064;&#65288;&#25552;&#20379;&#20102;&#35299;&#20915;&#38382;&#39064;&#25152;&#38656;&#30340;&#25152;&#26377;&#25968;&#25454;&#65289;&#21040;&#26410;&#26126;&#30830;&#35268;&#23450;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65288;&#26410;&#25552;&#20379;&#25152;&#26377;&#24517;&#35201;&#25968;&#25454;&#65289;&#19981;&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;62.5&#65285;&#30340;&#26377;&#26126;&#30830;&#35268;&#23450;&#30340;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#26410;&#26126;&#30830;&#35268;&#23450;&#30340;&#38382;&#39064;&#65292;&#20854;&#20934;&#30830;&#29575;&#19979;&#38477;&#21040;8.3&#65285;&#12290;&#23545;&#27169;&#22411;&#30340;&#38169;&#35823;&#35299;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22833;&#36133;&#27169;&#24335;&#65306;1&#65289;&#26080;&#27861;&#26500;&#24314;&#20934;&#30830;&#30340;&#29289;&#29702;&#19990;&#30028;&#27169;&#22411;&#65292;2&#65289;&#26080;&#27861;&#23545;&#32570;&#22833;&#25968;&#25454;&#20570;&#20986;&#21512;&#29702;&#30340;&#20551;&#35774;&#65292;&#20197;&#21450;3&#65289;&#35745;&#31639;&#38169;&#35823;&#12290;&#35813;&#30740;&#31350;&#20026;&#22914;&#20309;&#21033;&#29992;LLM&#22686;&#24378;&#30340;&#25945;&#23398;&#26448;&#26009;&#26469;&#25552;&#21319;STEM&#25945;&#32946;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#36824;&#23545;AI&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#24191;&#27867;&#35752;&#35770;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study explores the capabilities of OpenAI's ChatGPT in solving different types of physics problems. ChatGPT (with GPT-4) was queried to solve a total of 40 problems from a college-level engineering physics course. These problems ranged from well-specified problems, where all data required for solving the problem was provided, to under-specified, real-world problems where not all necessary data were given. Our findings show that ChatGPT could successfully solve 62.5\% of the well-specified problems, but its accuracy drops to 8.3\% for under-specified problems. Analysis of the model's incorrect solutions revealed three distinct failure modes: 1) failure to construct accurate models of the physical world, 2) failure to make reasonable assumptions about missing data, and 3) calculation errors. The study offers implications for how to leverage LLM-augmented instructional materials to enhance STEM education. The insights also contribute to the broader discourse on AI's strengths and limi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;EEG&#20998;&#31867;&#20013;&#20351;&#29992;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20943;&#23569;&#20102;&#22312;&#26410;&#35265;&#27979;&#35797;&#23545;&#35937;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#35774;&#35745;&#27491;&#21017;&#21270;&#24809;&#32602;&#21644;&#21457;&#25955;&#20272;&#35745;&#31639;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#24378;&#21046;&#25191;&#34892;&#20102;&#32479;&#35745;&#20851;&#31995;&#65292;&#24182;&#22312;&#22823;&#37327;&#35745;&#31639;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08762</link><description>&lt;p&gt;
EEG&#20998;&#31867;&#20013;&#37319;&#29992;&#21457;&#25955;&#20272;&#35745;&#31283;&#23450;&#20027;&#20307;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Subject Transfer in EEG Classification with Divergence Estimation. (arXiv:2310.08762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;EEG&#20998;&#31867;&#20013;&#20351;&#29992;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20943;&#23569;&#20102;&#22312;&#26410;&#35265;&#27979;&#35797;&#23545;&#35937;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#35774;&#35745;&#27491;&#21017;&#21270;&#24809;&#32602;&#21644;&#21457;&#25955;&#20272;&#35745;&#31639;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#24378;&#21046;&#25191;&#34892;&#20102;&#32479;&#35745;&#20851;&#31995;&#65292;&#24182;&#22312;&#22823;&#37327;&#35745;&#31639;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;encephalogram(EEG)&#25968;&#25454;&#30340;&#20998;&#31867;&#27169;&#22411;&#22312;&#26410;&#35265;&#27979;&#35797;&#23545;&#35937;&#19978;&#30340;&#34920;&#29616;&#22823;&#24133;&#19979;&#38477;&#12290;&#25105;&#20204;&#36890;&#36807;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20943;&#23569;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#22270;&#27169;&#22411;&#26469;&#25551;&#36848;EEG&#20998;&#31867;&#20219;&#21153;&#12290;&#20174;&#27599;&#20010;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#29702;&#24819;&#30340;&#35757;&#32451;&#22330;&#26223;&#19979;&#65288;&#20855;&#26377;&#26080;&#38480;&#25968;&#25454;&#21644;&#20840;&#23616;&#26368;&#20248;&#27169;&#22411;&#65289;&#65292;&#24212;&#35813;&#25104;&#31435;&#20294;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#30340;&#32479;&#35745;&#20851;&#31995;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#27491;&#21017;&#21270;&#24809;&#32602;&#26469;&#24378;&#21046;&#25191;&#34892;&#36825;&#20123;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36866;&#29992;&#20316;&#20026;&#20195;&#29702;&#25968;&#37327;&#30340;&#21512;&#36866;&#30340;&#21457;&#25955;&#65288;&#22914;&#20114;&#20449;&#24687;&#21644;Wasserstein-1&#65289;&#65292;&#21487;&#20197;&#29992;&#26469;&#27979;&#37327;&#32479;&#35745;&#29420;&#31435;&#21644;&#30456;&#20851;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#20108;&#27425;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#25928;&#20272;&#35745;&#36825;&#20123;&#25968;&#37327;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#22522;&#20934;EEG&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification models for electroencephalogram (EEG) data show a large decrease in performance when evaluated on unseen test sub jects. We reduce this performance decrease using new regularization techniques during model training. We propose several graphical models to describe an EEG classification task. From each model, we identify statistical relationships that should hold true in an idealized training scenario (with infinite data and a globally-optimal model) but that may not hold in practice. We design regularization penalties to enforce these relationships in two stages. First, we identify suitable proxy quantities (divergences such as Mutual Information and Wasserstein-1) that can be used to measure statistical independence and dependence relationships. Second, we provide algorithms to efficiently estimate these quantities during training using secondary neural network models. We conduct extensive computational experiments using a large benchmark EEG dataset, comparing our propo
&lt;/p&gt;</description></item><item><title>CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08753</link><description>&lt;p&gt;
CompA: &#35299;&#20915;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08753
&lt;/p&gt;
&lt;p&gt;
CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#30340;&#22522;&#26412;&#29305;&#24615;&#26159;&#20854;&#32452;&#21512;&#24615;&#12290;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;CLAP&#65289;&#35757;&#32451;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#33021;&#22815;&#23398;&#20064;&#38899;&#39057;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#12289;&#38899;&#39057;&#26816;&#32034;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26377;&#25928;&#25191;&#34892;&#32452;&#21512;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#36824;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CompA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#30495;&#23454;&#19990;&#30028;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CompA-order&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#25110;&#21457;&#29983;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#32780;CompA-attribute&#35780;&#20272;&#22768;&#38899;&#20107;&#20214;&#30340;&#23646;&#24615;&#32465;&#23450;&#12290;&#27599;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#21253;&#21547;&#20004;&#20010;&#38899;&#39057;-&#26631;&#39064;&#23545;&#65292;&#20854;&#20013;&#20004;&#20010;&#38899;&#39057;&#20855;&#26377;&#30456;&#21516;&#30340;&#22768;&#38899;&#20107;&#20214;&#65292;&#20294;&#32452;&#21512;&#26041;&#24335;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#36866;&#24212;&#20027;&#21160;&#23398;&#20064;&#26410;&#30693;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#32422;&#26463;&#22330;&#26223;&#26102;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.08751</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#36866;&#24212;&#20027;&#21160;&#23398;&#20064;&#26410;&#30693;&#32422;&#26463;&#30340;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Bayesian Optimization with Adaptive Active Learning of Unknown Constraints. (arXiv:2310.08751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#36866;&#24212;&#20027;&#21160;&#23398;&#20064;&#26410;&#30693;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#32422;&#26463;&#22330;&#26223;&#26102;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22914;&#31185;&#23398;&#23454;&#39564;&#35774;&#35745;&#12289;&#21307;&#23398;&#27835;&#30103;&#35774;&#35745;&#21644;&#24037;&#19994;&#36807;&#31243;&#20248;&#21270;&#31561;&#39046;&#22495;&#65292;&#20248;&#21270;&#30446;&#26631;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#24773;&#20917;&#26159;&#24120;&#35265;&#30340;&#65292;&#20854;&#20013;&#30446;&#26631;&#21644;&#32422;&#26463;&#37117;&#26159;&#40657;&#30418;&#20989;&#25968;&#12290;&#22788;&#29702;&#36825;&#20123;&#22797;&#26434;&#22330;&#26223;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#22312;&#29702;&#35770;&#34892;&#20026;&#26041;&#38754;&#65292;BO&#22312;&#26080;&#32422;&#26463;&#35774;&#32622;&#19979;&#30456;&#23545;&#36739;&#20026;&#29702;&#35299;&#65292;&#20854;&#21407;&#21017;&#24050;&#32463;&#34987;&#24191;&#27867;&#25506;&#32034;&#21644;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CBO&#65289;&#26469;&#35828;&#65292;&#29616;&#26377;&#26694;&#26550;&#24448;&#24448;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#36817;&#20284;&#26041;&#27861;&#65292;&#27809;&#26377;&#21516;&#26679;&#31243;&#24230;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#21644;&#32422;&#26463;&#21487;&#20197;&#29420;&#31435;&#35780;&#20272;&#19988;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#35748;&#35782;&#21040;&#30446;&#26631;&#21644;&#32422;&#26463;&#37117;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#39640;&#32622;&#20449;&#24230;&#30340;&#20852;&#36259;&#21306;&#22495;&#65292;
&lt;/p&gt;
&lt;p&gt;
Optimizing objectives under constraints, where both the objectives and constraints are black box functions, is a common scenario in real-world applications such as scientific experimental design, design of medical therapies, and industrial process optimization. One popular approach to handling these complex scenarios is Bayesian Optimization (BO). In terms of theoretical behavior, BO is relatively well understood in the unconstrained setting, where its principles have been well explored and validated. However, when it comes to constrained Bayesian optimization (CBO), the existing framework often relies on heuristics or approximations without the same level of theoretical guarantees.  In this paper, we delve into the theoretical and practical aspects of constrained Bayesian optimization, where the objective and constraints can be independently evaluated and are subject to noise. By recognizing that both the objective and constraints can help identify high-confidence regions of interest 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#39564;&#35777;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#21355;&#26143;&#19981;&#31283;&#23450;&#24615;&#39044;&#27979;&#22120;&#65292;&#21487;&#20197;&#20174;&#21069;&#21015;&#33146;&#30284;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;MSI&#29366;&#24577;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#20110;&#20813;&#30123;&#27835;&#30103;&#30340;&#24739;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.08743</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#21355;&#26143;&#19981;&#31283;&#23450;&#24615;&#39044;&#27979;&#22120;&#30340;&#24320;&#21457;&#21644;&#39564;&#35777;&#65292;&#26469;&#33258;&#20110;&#21069;&#21015;&#33146;&#30284;&#20840;&#20999;&#29255;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Development and Validation of a Deep Learning-Based Microsatellite Instability Predictor from Prostate Cancer Whole-Slide Images. (arXiv:2310.08743v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#39564;&#35777;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#21355;&#26143;&#19981;&#31283;&#23450;&#24615;&#39044;&#27979;&#22120;&#65292;&#21487;&#20197;&#20174;&#21069;&#21015;&#33146;&#30284;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;MSI&#29366;&#24577;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#20110;&#20813;&#30123;&#27835;&#30103;&#30340;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#21355;&#26143;&#19981;&#31283;&#23450;&#24615;&#39640;&#65288;MSI-H&#65289;&#26159;&#20813;&#30123;&#26816;&#26597;&#28857;&#25233;&#21046;&#21058;&#30103;&#27861;&#30340;&#19968;&#31181;&#32959;&#30244;&#19981;&#21487;&#30693;&#26631;&#24535;&#29289;&#12290;&#28982;&#32780;&#65292;&#22312;&#21069;&#21015;&#33146;&#30284;&#20013;&#65292;&#30001;&#20110;&#20302;&#24739;&#30149;&#29575;&#21644;&#26816;&#27979;&#25104;&#26412;&#36739;&#39640;&#65292;MSI&#29366;&#24577;&#36890;&#24120;&#19981;&#36827;&#34892;&#24120;&#35268;&#27979;&#35797;&#12290;&#22240;&#27492;&#65292;&#20174;&#34880;&#28082;&#26579;&#33394;&#21058;&#38747;&#34013;&#21644;&#20234;&#32418;&#65288;H&amp;E&#65289;&#26579;&#33394;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#20013;&#39044;&#27979;MSI&#29366;&#24577;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#20110;&#30830;&#35748;&#27979;&#35797;&#24182;&#26377;&#36164;&#26684;&#25509;&#21463;&#20813;&#30123;&#27835;&#30103;&#30340;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#12290;&#20998;&#26512;&#20102;&#36830;&#32493;&#36716;&#35786;&#33267;&#25105;&#20204;&#26426;&#26500;&#30340;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#30340;&#21435;&#21311;&#21517;&#35760;&#24405;&#20013;&#30340;&#21069;&#21015;&#33146;&#27963;&#26816;&#21644;&#25163;&#26415;&#20999;&#38500;&#26631;&#26412;&#12290;&#36890;&#36807;&#19979;&#19968;&#20195;&#27979;&#24207;&#30830;&#23450;&#20102;&#20182;&#20204;&#30340;MSI&#29366;&#24577;&#12290;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21069;&#30340;&#24739;&#32773;&#20998;&#20026;&#31639;&#27861;&#24320;&#21457;&#38598;&#65288;n=4015, MSI-H 1.8%&#65289;&#21644;&#37197;&#23545;&#39564;&#35777;&#38598;&#65288;n=173, MSI-H 19.7%&#65289;&#65292;&#27599;&#20010;&#26679;&#26412;&#30001;&#20004;&#20010;&#36830;&#32493;&#30340;&#20999;&#29255;&#32452;&#25104;&#65292;&#19968;&#20010;&#20869;&#37096;&#26579;&#33394;&#21644;&#25195;&#25551;&#65292;&#21478;&#19968;&#20010;&#22312;&#22806;&#37096;&#26576;&#20010;&#22320;&#28857;&#12290;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#24739;&#32773;&#24418;&#25104;&#20102;&#26102;&#38388;&#39564;&#35777;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Microsatellite instability-high (MSI-H) is a tumor agnostic biomarker for immune checkpoint inhibitor therapy. However, MSI status is not routinely tested in prostate cancer, in part due to low prevalence and assay cost. As such, prediction of MSI status from hematoxylin and eosin (H&amp;E) stained whole-slide images (WSIs) could identify prostate cancer patients most likely to benefit from confirmatory testing and becoming eligible for immunotherapy. Prostate biopsies and surgical resections from de-identified records of consecutive prostate cancer patients referred to our institution were analyzed. Their MSI status was determined by next generation sequencing. Patients before a cutoff date were split into an algorithm development set (n=4015, MSI-H 1.8%) and a paired validation set (n=173, MSI-H 19.7%) that consisted of two serial sections from each sample, one stained and scanned internally and the other at an external site. Patients after the cutoff date formed the temporal validation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#26862;&#26519;&#21644;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#26816;&#27979;&#19981;&#24076;&#26395;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31867;&#20107;&#20214;&#31867;&#22411;&#24182;&#39044;&#27979;&#20854;&#20986;&#29616;&#30340;&#27010;&#29575;&#65292;&#20026;&#29983;&#20135;&#36807;&#31243;&#20013;&#30340;&#25925;&#38556;&#20107;&#20214;&#31649;&#29702;&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.08737</link><description>&lt;p&gt;
&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#21644;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#23454;&#29616;&#30707;&#27833;&#20135;&#19994;&#26356;&#21487;&#25345;&#32493;&#30340;&#23454;&#26102;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-Time Event Detection with Random Forests and Temporal Convolutional Networks for More Sustainable Petroleum Industry. (arXiv:2310.08737v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08737
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#26862;&#26519;&#21644;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#26816;&#27979;&#19981;&#24076;&#26395;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31867;&#20107;&#20214;&#31867;&#22411;&#24182;&#39044;&#27979;&#20854;&#20986;&#29616;&#30340;&#27010;&#29575;&#65292;&#20026;&#29983;&#20135;&#36807;&#31243;&#20013;&#30340;&#25925;&#38556;&#20107;&#20214;&#31649;&#29702;&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30707;&#27833;&#20135;&#19994;&#23545;&#29616;&#20195;&#31038;&#20250;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#29983;&#20135;&#36807;&#31243;&#22797;&#26434;&#19988;&#39118;&#38505;&#39640;&#12290;&#22312;&#29983;&#20135;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#19981;&#24076;&#26395;&#30340;&#29983;&#20135;&#20107;&#20214;&#23548;&#33268;&#30340;&#20107;&#25925;&#25110;&#25925;&#38556;&#21487;&#33021;&#20250;&#36896;&#25104;&#20005;&#37325;&#30340;&#29615;&#22659;&#21644;&#32463;&#27982;&#25439;&#22833;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#35843;&#30740;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#19981;&#24076;&#26395;&#20107;&#20214;&#30340;&#26816;&#27979;&#65292;&#20294;&#23545;&#20110;&#23454;&#26102;&#20107;&#20214;&#27010;&#29575;&#30340;&#39044;&#27979;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#65292;&#32780;&#36825;&#19968;&#28857;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#21450;&#26089;&#24178;&#39044;&#26159;&#39044;&#35745;&#20107;&#20214;&#21457;&#29983;&#26102;&#30340;&#37325;&#35201;&#25514;&#26045;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#19981;&#24076;&#26395;&#30340;&#20107;&#20214;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#20107;&#20214;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#24182;&#39044;&#27979;&#20854;&#20986;&#29616;&#30340;&#27010;&#29575;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#30740;&#31350;&#20013;&#21457;&#29616;&#30340;&#25361;&#25112;&#65292;&#20026;&#29983;&#20135;&#36807;&#31243;&#20013;&#30340;&#25925;&#38556;&#20107;&#20214;&#31649;&#29702;&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The petroleum industry is crucial for modern society, but the production process is complex and risky. During the production, accidents or failures, resulting from undesired production events, can cause severe environmental and economic damage. Previous studies have investigated machine learning (ML) methods for undesired event detection. However, the prediction of event probability in real-time was insufficiently addressed, which is essential since it is important to undertake early intervention when an event is expected to happen. This paper proposes two ML approaches, random forests and temporal convolutional networks, to detect undesired events in real-time. Results show that our approaches can effectively classify event types and predict the probability of their appearance, addressing the challenges uncovered in previous studies and providing a more effective solution for failure event management during the production.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#12290;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.08731</link><description>&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#23454;&#29616;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Simple Way to Incorporate Novelty Detection in World Models. (arXiv:2310.08731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#12290;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#19990;&#30028;&#26426;&#21046;&#25110;&#23646;&#24615;&#21457;&#29983;&#31361;&#28982;&#21464;&#21270;&#26102;&#65292;&#20195;&#29702;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#21487;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#35270;&#35273;&#23646;&#24615;&#25110;&#29366;&#24577;&#36716;&#25442;&#30340;&#31361;&#21464;&#31216;&#20026;&#8220;&#26032;&#39062;&#24615;&#8221;&#12290;&#22312;&#29983;&#25104;&#30340;&#19990;&#30028;&#27169;&#22411;&#26694;&#26550;&#20013;&#23454;&#26045;&#26032;&#39062;&#24615;&#26816;&#27979;&#26159;&#20445;&#25252;&#37096;&#32626;&#26102;&#20195;&#29702;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19982;&#24207;&#21015;&#20915;&#31574;&#30456;&#20851;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26412;&#20307;&#35770;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#20195;&#29702;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#36716;&#25442;&#20998;&#24067;&#20013;&#26816;&#27979;&#26032;&#39062;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as {\em novelties}. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents, by utilizing the misalignment of the world model's hallucinated states and the true observed states as an anomaly score. We first provide an ontology of novelty detection relevant to sequential decision making, then we provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#27979;&#22810;&#20010;&#36873;&#25321;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#32771;&#34385;&#39038;&#23458;&#21644;&#39033;&#30446;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#21487;&#36873;&#39033;&#30340;&#33539;&#22260;&#21644;&#23450;&#21046;&#38656;&#27714;&#65289;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#22312;&#39038;&#23458;&#36873;&#25321;&#22810;&#20010;&#39033;&#30446;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08716</link><description>&lt;p&gt;
Transformer Choice Net: &#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39044;&#27979;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Transformer Choice Net: A Transformer Neural Network for Choice Prediction. (arXiv:2310.08716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#27979;&#22810;&#20010;&#36873;&#25321;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#32771;&#34385;&#39038;&#23458;&#21644;&#39033;&#30446;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#21487;&#36873;&#39033;&#30340;&#33539;&#22260;&#21644;&#23450;&#21046;&#38656;&#27714;&#65289;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#22312;&#39038;&#23458;&#36873;&#25321;&#22810;&#20010;&#39033;&#30446;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24066;&#22330;&#33829;&#38144;&#12289;&#32463;&#27982;&#23398;&#21644;&#36816;&#31609;&#23398;&#20013;&#65292;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#65292;&#22914;&#22810;&#39033;&#24335;&#36923;&#36753;&#27169;&#22411;&#12289;Probit&#27169;&#22411;&#25110;&#28151;&#21512;&#36923;&#36753;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#65306;&#32473;&#23450;&#19968;&#32452;&#21487;&#36873;&#39033;&#65292;&#39038;&#23458;&#34987;&#27169;&#25311;&#20026;&#36873;&#25321;&#20854;&#20013;&#20043;&#19968;&#20197;&#26368;&#22823;&#21270;&#65288;&#28508;&#22312;&#30340;&#65289;&#25928;&#29992;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#25512;&#24191;&#21040;&#39038;&#23458;&#36873;&#25321;&#22810;&#20010;&#39033;&#30446;&#30340;&#24773;&#20917;&#65288;&#22914;&#30005;&#23376;&#21830;&#21153;&#36141;&#29289;&#65289;&#21364;&#23384;&#22312;&#38382;&#39064;&#12290;&#34429;&#28982;&#21487;&#20197;&#24314;&#31435;&#21512;&#29702;&#30340;&#39038;&#23458;&#34892;&#20026;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#39033;&#30446;&#23376;&#38598;&#30340;&#32452;&#21512;&#29190;&#28856;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#20272;&#35745;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#21161;&#20110;&#39044;&#27979;&#22810;&#20010;&#36873;&#25321;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21363;Transformer Choice Net&#12290;&#20107;&#23454;&#35777;&#26126;&#65292;Transformer&#32593;&#32476;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#29305;&#21035;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20165;&#32771;&#34385;&#20102;&#39038;&#23458;&#21644;&#39033;&#30446;&#30340;&#29305;&#24449;&#65292;&#36824;&#32771;&#34385;&#20102;&#19978;&#19979;&#25991;&#65292;&#21363;&#21487;&#36873;&#39033;&#30340;&#33539;&#22260;&#20197;&#21450;&#23450;&#21046;&#21270;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete-choice models, such as Multinomial Logit, Probit, or Mixed-Logit, are widely used in Marketing, Economics, and Operations Research: given a set of alternatives, the customer is modeled as choosing one of the alternatives to maximize a (latent) utility function. However, extending such models to situations where the customer chooses more than one item (such as in e-commerce shopping) has proven problematic. While one can construct reasonable models of the customer's behavior, estimating such models becomes very challenging because of the combinatorial explosion in the number of possible subsets of items. In this paper we develop a transformer neural network architecture, the Transformer Choice Net, that is suitable for predicting multiple choices. Transformer networks turn out to be especially suitable for this task as they take into account not only the features of the customer and the items but also the context, which in this case could be the assortment as well as the custom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#35821;&#35328;&#24314;&#27169;&#20013;&#35821;&#38899;&#21333;&#20803;&#21644;&#25991;&#26412;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#28151;&#21512;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20165;&#20351;&#29992;&#35821;&#38899;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.08715</link><description>&lt;p&gt;
&#36808;&#21521;&#35821;&#38899;&#21333;&#20803;&#21644;&#25991;&#26412;&#30340;&#32852;&#21512;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Toward Joint Language Modeling for Speech Units and Text. (arXiv:2310.08715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#35821;&#35328;&#24314;&#27169;&#20013;&#35821;&#38899;&#21333;&#20803;&#21644;&#25991;&#26412;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#28151;&#21512;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20165;&#20351;&#29992;&#35821;&#38899;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21644;&#25991;&#26412;&#26159;&#20154;&#31867;&#35821;&#35328;&#30340;&#20004;&#31181;&#20027;&#35201;&#24418;&#24335;&#12290;&#30740;&#31350;&#30028;&#22810;&#24180;&#26469;&#19968;&#30452;&#22312;&#20851;&#27880;&#23558;&#35821;&#38899;&#26144;&#23556;&#21040;&#25991;&#26412;&#25110;&#32773;&#21453;&#20043;&#20134;&#28982;&#12290;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#65292;&#24456;&#23569;&#26377;&#20154;&#23581;&#35797;&#32852;&#21512;&#24314;&#27169;&#36825;&#20004;&#32773;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35821;&#38899;&#21333;&#20803;&#21644;&#25991;&#26412;&#30340;&#32852;&#21512;&#35821;&#35328;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#35821;&#38899;&#20998;&#35789;&#24037;&#20855;&#65292;&#23558;&#36830;&#32493;&#30340;&#35821;&#38899;&#20449;&#21495;&#36716;&#21270;&#20026;&#31163;&#25955;&#30340;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#26500;&#24314;&#28151;&#21512;&#30340;&#35821;&#38899;-&#25991;&#26412;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#21160;&#25351;&#26631;&#26469;&#35780;&#20272;&#32852;&#21512;&#35821;&#35328;&#24314;&#27169;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#34701;&#21512;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#38899;&#25110;&#25991;&#26412;&#65289;&#22312;&#19979;&#28216;&#30340;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#23545;&#32852;&#21512;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#27979;&#35797;&#20854;&#24615;&#33021;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#20849;&#20139;&#34920;&#31034;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#28151;&#21512;&#25216;&#26415;&#23558;&#35821;&#38899;&#21333;&#20803;&#21644;&#25991;&#26412;&#28151;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#21512;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#20248;&#20110;&#20165;&#20351;&#29992;&#35821;&#38899;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#38646;-shot&#30340;&#36328;&#27169;&#24577;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model's learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferabil
&lt;/p&gt;</description></item><item><title>&#22312;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELDEN&#65292;&#19968;&#31181;&#26032;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#40723;&#21169;&#26377;&#25928;&#22320;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.08702</link><description>&lt;p&gt;
ELDEN: &#22522;&#20110;&#26412;&#22320;&#20381;&#36182;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
ELDEN: Exploration via Local Dependencies. (arXiv:2310.08702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08702
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELDEN&#65292;&#19968;&#31181;&#26032;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#40723;&#21169;&#26377;&#25928;&#22320;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#19968;&#30452;&#38754;&#20020;&#30528;&#22256;&#38590;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#38656;&#35201;&#26377;&#25928;&#22320;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#65292;&#30452;&#21040;&#25214;&#21040;&#22870;&#21169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#31038;&#21306;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#22312;&#22870;&#21169;&#26469;&#22686;&#24378;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20869;&#22312;&#22870;&#21169;&#26159;&#19968;&#31181;&#40723;&#21169;&#20195;&#29702;&#35775;&#38382;&#26377;&#36259;&#29366;&#24577;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#23450;&#20041;&#20855;&#26377;&#22240;&#24335;&#29366;&#24577;&#31354;&#38388;&#21644;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#29615;&#22659;&#20013;&#30340;&#26377;&#36259;&#29366;&#24577;&#65292;&#20854;&#20013;&#20195;&#29702;&#30340;&#21160;&#20316;&#21487;&#33021;&#20250;&#25913;&#21464;&#19968;&#20010;&#23454;&#20307;&#30340;&#20540;&#65292;&#36827;&#32780;&#21487;&#33021;&#24433;&#21709;&#21478;&#19968;&#20010;&#23454;&#20307;&#30340;&#20540;&#12290;&#25105;&#20204;&#30340;&#35266;&#28857;&#26159;&#65292;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#65292;&#25506;&#32034;&#26377;&#36259;&#29366;&#24577;&#30340;&#20851;&#38190;&#26159;&#20195;&#29702;&#19981;&#30830;&#23450;&#23454;&#20307;&#65288;&#22914;&#20195;&#29702;&#25110;&#29289;&#20307;&#65289;&#26159;&#21542;&#30456;&#20114;&#24433;&#21709;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"ELDEN: &#22522;&#20110;&#26412;&#22320;&#20381;&#36182;&#30340;&#25506;&#32034;"&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#20419;&#36827;&#20102;&#23545;&#26032;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To deal with this problem, the community has proposed to augment the reward function with intrinsic reward, a bonus signal that encourages the agent to visit interesting states. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent's actions may change the value of one entity that, in order, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain whether (as opposed to how) entities such as the agent or objects have some influence on each other. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward that encourages the discovery of new interactions between en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#25552;&#39640;Atari&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24615;&#33021;&#30340;&#26041;&#27861;</title><link>http://arxiv.org/abs/2310.08683</link><description>&lt;p&gt;
Atari&#24378;&#21270;&#23398;&#20064;&#30340;&#34394;&#25311;&#22686;&#24378;&#29616;&#23454;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Virtual Augmented Reality for Atari Reinforcement Learning. (arXiv:2310.08683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#25552;&#39640;Atari&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24615;&#33021;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#28216;&#25103;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#23588;&#20854;&#26159;&#35895;&#27468;DeepMind&#30340;AlphaGo&#20987;&#36133;&#20102;&#20154;&#31867;&#22260;&#26827;&#20896;&#20891;&#26607;&#27905;&#12290;&#36825;&#19968;&#32988;&#21033;&#20063;&#24471;&#30410;&#20110;Atari&#23398;&#20064;&#29615;&#22659;(ALE)&#65292;ALE&#22312;RL&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20419;&#36827;&#20102;AlphaGo&#21644;&#20854;&#20182;RL&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#24403;&#21069;&#30340;Atari&#35270;&#39057;&#28216;&#25103;RL&#30740;&#31350;&#20013;&#65292;RL&#26234;&#33021;&#20307;&#23545;&#29615;&#22659;&#30340;&#24863;&#30693;&#22522;&#20110;Atari&#35270;&#39057;&#28216;&#25103;&#23631;&#24149;&#30340;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#65292;&#19988;&#22270;&#20687;&#39044;&#22788;&#29702;&#24456;&#23569;&#12290;&#30456;&#21453;&#65292;&#26368;&#20808;&#36827;&#30340;ML&#30740;&#31350;&#22312;&#22686;&#24378;&#22270;&#20687;&#24863;&#30693;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#20854;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#26159;Meta Research&#30340;&#8220;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#20998;&#21106;&#27169;&#22411;(SAM)&#65292;&#23427;&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#27809;&#26377;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20998;&#21106;&#22270;&#20687;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#35770;&#38382;&#39064;&#65306;&#20687;SAM&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#33021;&#21542;&#25552;&#39640;RL&#26234;&#33021;&#20307;&#22312;&#29609;Atari&#35270;&#39057;&#28216;&#25103;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has achieved significant milestones in the gaming domain, most notably Google DeepMind's AlphaGo defeating human Go champion Ken Jie. This victory was also made possible through the Atari Learning Environment (ALE): The ALE has been foundational in RL research, facilitating significant RL algorithm developments such as AlphaGo and others. In current Atari video game RL research, RL agents' perceptions of its environment is based on raw pixel data from the Atari video game screen with minimal image preprocessing. Contrarily, cutting-edge ML research, external to the Atari video game RL research domain, is focusing on enhancing image perception. A notable example is Meta Research's "Segment Anything Model" (SAM), a foundation model capable of segmenting images without prior training (zero-shot). This paper addresses a novel methodical question: Can state-of-the-art image segmentation models such as SAM improve the performance of RL agents playing Atari video g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#20998;&#26512;&#19978;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27169;&#25311;CFA&#32771;&#35797;&#20013;&#20855;&#26377;&#19968;&#23450;&#30340;&#34920;&#29616;&#65292;&#20026;&#23558;&#26469;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.08678</link><description>&lt;p&gt;
GPT&#27169;&#22411;&#33021;&#25104;&#20026;&#37329;&#34701;&#20998;&#26512;&#24072;&#21527;&#65311;&#23545;&#27169;&#25311;CFA&#32771;&#35797;&#20013;&#30340;ChatGPT&#21644;GPT-4&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams. (arXiv:2310.08678v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#20998;&#26512;&#19978;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27169;&#25311;CFA&#32771;&#35797;&#20013;&#20855;&#26377;&#19968;&#23450;&#30340;&#34920;&#29616;&#65292;&#20026;&#23558;&#26469;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#33021;&#19982;&#29978;&#33267;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#37329;&#34701;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21033;&#29992;&#29305;&#35768;&#37329;&#34701;&#20998;&#26512;&#24072;&#65288;CFA&#65289;&#32771;&#35797;&#30340;&#27169;&#25311;&#39064;&#30446;&#23545;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#20998;&#26512;&#20013;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#38646;&#26679;&#26412;&#65288;ZS&#65289;&#12289;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#21644;&#23569;&#26679;&#26412;&#65288;FS&#65289;&#22330;&#26223;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#36890;&#36807;CFA&#32771;&#35797;&#30340;&#21487;&#33021;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25552;&#39640;LLM&#22312;&#37329;&#34701;&#39046;&#22495;&#24212;&#29992;&#24615;&#30340;&#28508;&#22312;&#31574;&#30053;&#21644;&#25913;&#36827;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#24076;&#26395;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#32487;&#32493;&#36890;&#36807;&#20005;&#26684;&#35780;&#20272;&#26469;&#25552;&#21319;LLM&#22312;&#37329;&#34701;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance on a wide range of Natural Language Processing (NLP) tasks, often matching or even beating state-of-the-art task-specific models. This study aims at assessing the financial reasoning capabilities of LLMs. We leverage mock exam questions of the Chartered Financial Analyst (CFA) Program to conduct a comprehensive evaluation of ChatGPT and GPT-4 in financial analysis, considering Zero-Shot (ZS), Chain-of-Thought (CoT), and Few-Shot (FS) scenarios. We present an in-depth analysis of the models' performance and limitations, and estimate whether they would have a chance at passing the CFA exams. Finally, we outline insights into potential strategies and improvements to enhance the applicability of LLMs in finance. In this perspective, we hope this work paves the way for future studies to continue enhancing LLMs for financial reasoning through rigorous evaluation.
&lt;/p&gt;</description></item><item><title>GDL-DS&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#30340;&#20449;&#24687;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2310.08677</link><description>&lt;p&gt;
GDL-DS: &#20998;&#24067;&#36716;&#25442;&#19979;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GDL-DS: A Benchmark for Geometric Deep Learning under Distribution Shifts. (arXiv:2310.08677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08677
&lt;/p&gt;
&lt;p&gt;
GDL-DS&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#30340;&#20449;&#24687;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;(GDL)&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#25797;&#38271;&#23545;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#30340;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#20854;&#22312;&#22788;&#29702;&#20998;&#24067;&#36716;&#25442;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#35768;&#22810;&#30456;&#20851;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GDL-DS&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;GDL&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#31890;&#23376;&#29289;&#29702;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#21040;&#29983;&#29289;&#21270;&#23398;&#30340;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#65292;&#24182;&#21253;&#25324;&#21508;&#31181;&#20998;&#24067;&#36716;&#25442;&#65292;&#21253;&#25324;&#26465;&#20214;&#12289;&#21327;&#21464;&#21644;&#27010;&#24565;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26469;&#33258;&#36229;&#20986;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#30340;&#20449;&#24687;&#35775;&#38382;&#30340;&#19977;&#20010;&#32423;&#21035;&#65292;&#21253;&#25324;&#27809;&#26377;&#36229;&#20986;&#20998;&#24067;&#30340;&#20449;&#24687;&#12289;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#21644;&#24102;&#26377;&#23569;&#25968;&#26631;&#31614;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#28041;&#21450;30&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#35780;&#20272;3&#31181;&#20449;&#24687;&#35775;&#38382;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning (GDL) has gained significant attention in various scientific fields, chiefly for its proficiency in modeling data with intricate geometric structures. Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many relevant applications. To bridge this gap, we propose GDL-DS, a comprehensive benchmark designed for evaluating the performance of GDL models in scenarios with distribution shifts. Our evaluation datasets cover diverse scientific domains from particle physics and materials science to biochemistry, and encapsulate a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. Furthermore, we study three levels of information access from the out-of-distribution (OOD) testing data, including no OOD information, only OOD features without labels, and OOD features with a few labels. Overall, our benchmark results in 30 different experiment settings, and evaluates 3 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08660</link><description>&lt;p&gt;
&#22312;&#19981;&#25506;&#32034;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;RL&#31574;&#30053;&#65306;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach. (arXiv:2310.08660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32593;&#32476;&#21442;&#25968;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#36895;&#29575;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#23558;&#20854;&#26500;&#24314;&#20026;&#21151;&#29575;&#25511;&#21046;&#12289;&#27874;&#26463;&#25104;&#24418;&#21644;&#24178;&#25200;&#28040;&#38500;&#30340;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#20010;&#22522;&#31449;&#19982;&#22810;&#20010;&#29992;&#25143;&#35774;&#22791;&#36890;&#20449;&#30340;&#24773;&#20917;&#12290;&#30001;&#20110;&#31351;&#20030;&#25628;&#32034;&#30340;&#25351;&#25968;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#26469;&#35299;&#20915;&#35813;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#20195;&#36890;&#20449;&#31995;&#32479;&#20197;&#20854;&#38590;&#20197;&#20934;&#30830;&#24314;&#27169;&#30340;&#34892;&#20026;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;RL&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#20197;&#20415;&#20195;&#29702;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22833;&#36133;&#30340;&#39640;&#25104;&#26412;&#65292;&#23558;&#31639;&#27861;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#25506;&#32034;&#21644;&#23398;&#20064;&#26159;&#19981;&#26126;&#26234;&#30340;&#12290;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#22522;&#20110;RL&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;&#22914;&#22522;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#30340;&#25511;&#21046;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#19968;&#31181;&#31163;&#31574;&#30053;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this project, we consider the problem of network parameter optimization for rate maximization. We frame this as a joint optimization problem of power control, beam forming, and interference cancellation. We consider the setting where multiple Base Stations (BSs) are communicating with multiple user equipments (UEs). Because of the exponential computational complexity of brute force search, we instead solve this non-convex optimization problem using deep reinforcement learning (RL) techniques. The modern communication systems are notorious for their difficulty in exactly modeling their behaviour. This limits us in using RL based algorithms as interaction with the environment is needed for the agent to explore and learn efficiently. Further, it is ill advised to deploy the algorithm in real world for exploration and learning because of the high cost of failure. In contrast to the previous RL-based solutions proposed, such as deep-Q network (DQN) based control, we propose taking an off
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;BERT&#27169;&#22411;&#20998;&#26512;&#38463;&#23500;&#27735;&#27494;&#35013;&#20914;&#31361;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#36890;&#36807;&#23545;&#20107;&#20214;&#30340;&#25551;&#36848;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#20854;&#26159;&#21542;&#33268;&#21629;&#12290;&#27169;&#22411;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.08653</link><description>&lt;p&gt;
&#22312;&#38463;&#23500;&#27735;&#27494;&#35013;&#20914;&#31361;&#20013;&#36890;&#36807;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#27515;&#20129;&#20998;&#31867;&#30340;&#20998;&#26512;&#65306;&#19968;&#31181;BERT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing Textual Data for Fatality Classification in Afghanistan's Armed Conflicts: A BERT Approach. (arXiv:2310.08653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08653
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;BERT&#27169;&#22411;&#20998;&#26512;&#38463;&#23500;&#27735;&#27494;&#35013;&#20914;&#31361;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#36890;&#36807;&#23545;&#20107;&#20214;&#30340;&#25551;&#36848;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#20854;&#26159;&#21542;&#33268;&#21629;&#12290;&#27169;&#22411;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23500;&#27735;&#22312;&#21382;&#21490;&#19978;&#32463;&#21382;&#20102;&#35768;&#22810;&#27494;&#35013;&#20914;&#31361;&#65292;&#23588;&#20854;&#26159;&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65307;&#36825;&#20123;&#20107;&#20214;&#23545;&#20154;&#31867;&#29983;&#27963;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#21253;&#25324;&#20891;&#20107;&#20154;&#21592;&#21644;&#24179;&#27665;&#65292;&#21487;&#33021;&#23548;&#33268;&#27515;&#20129;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#26681;&#25454;&#30001;&#27494;&#35013;&#20914;&#31361;&#20301;&#32622;&#21644;&#20107;&#20214;&#25968;&#25454;&#39033;&#30446;&#65288;ACLED&#65289;&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#23545;&#38463;&#23500;&#27735;&#27494;&#35013;&#20914;&#31361;&#30340;&#32467;&#26524;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#20854;&#26159;&#21542;&#33268;&#21629;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;2021&#24180;8&#26376;&#33267;2023&#24180;3&#26376;&#22312;&#38463;&#23500;&#27735;&#21457;&#29983;&#30340;&#27494;&#35013;&#20914;&#31361;&#30340;&#35814;&#32454;&#25551;&#36848;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;BERT&#65288;&#21452;&#21521;&#32534;&#30721;&#22120;&#36716;&#25442;&#22120;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65289;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20808;&#36827;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#12290;&#20998;&#31867;&#22120;&#21033;&#29992;&#20107;&#20214;&#30340;&#21407;&#22987;&#25991;&#26412;&#25551;&#36848;&#26469;&#20272;&#35745;&#20107;&#20214;&#23548;&#33268;&#27515;&#20129;&#30340;&#21487;&#33021;&#24615;&#12290;&#35813;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Afghanistan has witnessed many armed conflicts throughout history, especially in the past 20 years; these events have had a significant impact on human lives, including military and civilians, with potential fatalities. In this research, we aim to leverage state-of-the-art machine learning techniques to classify the outcomes of Afghanistan armed conflicts to either fatal or non-fatal based on their textual descriptions provided by the Armed Conflict Location &amp; Event Data Project (ACLED) dataset. The dataset contains comprehensive descriptions of armed conflicts in Afghanistan that took place from August 2021 to March 2023. The proposed approach leverages the power of BERT (Bidirectional Encoder Representations from Transformers), a cutting-edge language representation model in natural language processing. The classifier utilizes the raw textual description of an event to estimate the likelihood of the event resulting in a fatality. The model achieved impressive performance on the test 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#23454;&#29616;&#30005;&#32593;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#38477;&#32500;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#23545;SCADA&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#26816;&#27979;&#20986;&#20854;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.08650</link><description>&lt;p&gt;
&#36890;&#36807;&#24352;&#37327;&#20998;&#35299;&#23454;&#29616;&#30005;&#32593;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Electrical Grid Anomaly Detection via Tensor Decomposition. (arXiv:2310.08650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#23454;&#29616;&#30005;&#32593;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#38477;&#32500;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#23545;SCADA&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#26816;&#27979;&#20986;&#20854;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#25511;&#19982;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#65288;SCADA&#65289;&#32463;&#24120;&#20316;&#20026;&#30005;&#32593;&#20998;&#31449;&#30340;&#31070;&#32463;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#23454;&#29616;&#20102;&#23454;&#26102;&#30417;&#25511;&#12289;&#25968;&#25454;&#37319;&#38598;&#12289;&#35774;&#22791;&#25511;&#21046;&#65292;&#24182;&#30830;&#20445;&#20998;&#31449;&#21450;&#20854;&#36830;&#25509;&#35774;&#22791;&#30340;&#24179;&#31283;&#39640;&#25928;&#36816;&#34892;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#22522;&#20110;&#38477;&#32500;&#30340;&#26041;&#27861;&#65292;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#65292;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;SCADA&#31995;&#32479;&#20013;&#30340;&#24322;&#24120;&#12290;&#34429;&#28982;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#24182;&#27809;&#26377;&#19987;&#38376;&#24212;&#29992;&#20110;SCADA&#65292;&#20294;&#23427;&#22312;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#26816;&#27979;&#24322;&#24120;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#26080;&#30417;&#30563;&#26041;&#27861;&#36890;&#36807;&#23545;&#27491;&#24120;&#25110;&#39044;&#26399;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#35782;&#21035;&#20559;&#31163;&#39044;&#26399;&#34892;&#20026;&#30340;&#20107;&#20214;&#65292;&#20174;&#32780;&#26816;&#27979;&#26410;&#30693;&#31867;&#22411;&#30340;&#25915;&#20987;&#25110;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#23545;SCADA&#31995;&#32479;&#20013;&#33258;&#28982;&#23384;&#22312;&#30340;&#22797;&#26434;&#22810;&#32500;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#33021;&#22815;&#23545;SCADA&#31995;&#32479;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#24314;&#27169;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervisory Control and Data Acquisition (SCADA) systems often serve as the nervous system for substations within power grids. These systems facilitate real-time monitoring, data acquisition, control of equipment, and ensure smooth and efficient operation of the substation and its connected devices. Previous work has shown that dimensionality reduction-based approaches, such as Principal Component Analysis (PCA), can be used for accurate identification of anomalies in SCADA systems. While not specifically applied to SCADA, non-negative matrix factorization (NMF) has shown strong results at detecting anomalies in wireless sensor networks. These unsupervised approaches model the normal or expected behavior and detect the unseen types of attacks or anomalies by identifying the events that deviate from the expected behavior. These approaches; however, do not model the complex and multi-dimensional interactions that are naturally present in SCADA systems. Differently, non-negative tensor de
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08644</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25968;&#21313;&#24180;&#26469;&#33268;&#21147;&#20110;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#26102;&#38388;&#24207;&#21015;&#28436;&#21270;&#30340;&#29289;&#29702;-&#27010;&#24565; (PC) &#27169;&#22411;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064; (ML) &#30340;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20174;ML&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#29289;&#29702;&#29702;&#35299;&#30340;&#22256;&#38590;&#20351;&#24471;&#20854;&#22312;&#22686;&#24378;&#23545;&#31995;&#32479;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#24212;&#29992;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#29289;&#29702;&#24615;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120; (MCP) &#20316;&#20026;&#24357;&#21512;PC&#27169;&#22411;&#21644;ML&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;MCP&#21033;&#29992;PC&#27169;&#22411;&#21644;GRNNs&#32972;&#21518;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#20869;&#22312;&#21516;&#26500;&#24615;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26126;&#30830;&#34920;&#31034;&#29289;&#29702;&#36807;&#31243;&#30340;&#36136;&#37327;&#20445;&#25345;&#24615;&#36136;&#65292;&#21516;&#26102;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#21644;&#29616;&#25104;&#30340;ML&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36825;&#31181;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#65288;&#21487;&#35299;&#37322;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#23545;AI&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#35299;&#37322;&#26377;&#21161;&#20110;&#20154;&#20204;&#26816;&#27979;&#30452;&#25509;&#20559;&#35265;&#65292;&#20294;&#23545;&#38388;&#25509;&#20559;&#35265;&#30340;&#21457;&#29616;&#33021;&#21147;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.08617</link><description>&lt;p&gt;
AI&#20915;&#31574;&#20013;&#35299;&#37322;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65306;&#21463;&#20445;&#25252;&#29305;&#24449; vs &#20195;&#29702;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features. (arXiv:2310.08617v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08617
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#23545;AI&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#35299;&#37322;&#26377;&#21161;&#20110;&#20154;&#20204;&#26816;&#27979;&#30452;&#25509;&#20559;&#35265;&#65292;&#20294;&#23545;&#38388;&#25509;&#20559;&#35265;&#30340;&#21457;&#29616;&#33021;&#21147;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;AI&#31995;&#32479;&#21487;&#33021;&#20250;&#25918;&#22823;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#12290;&#35299;&#37322;&#21487;&#33021;&#26377;&#21161;&#20110;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36890;&#24120;&#65292;&#35299;&#37322;&#20391;&#37325;&#20110;&#31361;&#20986;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#22914;&#26524;&#27169;&#22411;&#23545;&#26576;&#20010;&#21463;&#20445;&#25252;&#30340;&#32676;&#20307;&#23384;&#22312;&#20559;&#35265;&#65292;&#21017;&#35299;&#37322;&#21487;&#33021;&#21253;&#25324;&#26174;&#31034;&#36825;&#31181;&#20559;&#35265;&#30340;&#29305;&#24449;&#65292;&#20294;&#24403;&#20559;&#35265;&#36890;&#36807;&#20195;&#29702;&#29305;&#24449;&#23454;&#29616;&#26102;&#65292;&#36825;&#20010;&#20195;&#29702;&#29305;&#24449;&#19982;&#21463;&#20445;&#25252;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#23545;&#20154;&#31867;&#32780;&#35328;&#19981;&#22826;&#28165;&#26224;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#21463;&#20445;&#25252;&#29305;&#24449;&#21644;&#20195;&#29702;&#29305;&#24449;&#23545;&#21442;&#19982;&#32773;&#23545;&#27169;&#22411;&#20844;&#24179;&#24615;&#21644;&#25552;&#39640;&#20154;&#21475;&#24179;&#34913;&#33021;&#21147;&#30340;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#22788;&#29702;&#26041;&#24335;&#65288;&#35299;&#37322;&#12289;&#27169;&#22411;&#20559;&#35265;&#25259;&#38706;&#21644;&#20195;&#29702;&#30456;&#20851;&#24615;&#25259;&#38706;&#65289;&#23545;&#20844;&#24179;&#24863;&#30693;&#21644;&#24179;&#31561;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35299;&#37322;&#26377;&#21161;&#20110;&#20154;&#20204;&#26816;&#27979;&#30452;&#25509;&#20559;&#35265;&#65292;&#20294;&#26080;&#27861;&#21457;&#29616;&#38388;&#25509;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#20559;&#35265;&#31867;&#22411;&#22914;&#20309;&#65292;&#35299;&#37322;&#20542;&#21521;&#20110;&#22686;&#21152;&#23545;&#27169;&#22411;&#20559;&#35265;&#30340;&#35748;&#21516;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI systems have been known to amplify biases in real world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants' perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments -- explanations, model bias disclosure and proxy correlation disclosure -- affect fairness perception and parity. We find that explanations help people detect direct biases but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model bia
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#30830;&#20445;&#20102;SafeDPA&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08602</link><description>&lt;p&gt;
&#23433;&#20840;&#28145;&#24230;&#31574;&#30053;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Safe Deep Policy Adaptation. (arXiv:2310.08602v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#30830;&#20445;&#20102;SafeDPA&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#20351;&#33258;&#20027;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#24555;&#36895;&#36866;&#24212;&#12290;&#32463;&#20856;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#23433;&#20840;&#25511;&#21046;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#21644;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#20294;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#31995;&#32479;&#31867;&#21035;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31574;&#30053;&#36866;&#24212;&#25552;&#20379;&#20102;&#36890;&#29992;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;RL&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#39044;&#27979;&#29615;&#22659;&#37197;&#32622;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#23545;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;RL&#31574;&#30053;&#20043;&#19978;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#20197;&#30830;&#20445;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;SafeDPA&#30340;&#29702;&#35770;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;SafeDPA&#23545;&#23398;&#20064;&#35823;&#24046;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35814;&#32454;&#22238;&#39038;&#20102;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;DL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#21644;&#23454;&#29616;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08598</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization for Medical Image Analysis: A Survey. (arXiv:2310.08598v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35814;&#32454;&#22238;&#39038;&#20102;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;DL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#21644;&#23454;&#29616;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65288;MedIA&#65289;&#24050;&#25104;&#20026;&#21307;&#23398;&#21644;&#20445;&#20581;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#30142;&#30149;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#27835;&#30103;&#35268;&#21010;&#26041;&#38754;&#36215;&#21040;&#20102;&#24456;&#22823;&#30340;&#20316;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26368;&#26032;&#25104;&#21151;&#20026;&#20854;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;MedIA&#30340;DL&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26679;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#19979;&#24456;&#38590;&#27867;&#21270;&#65292;&#36825;&#34987;&#31216;&#20026;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#24320;&#21457;&#21508;&#31181;DL&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#24182;&#22312;&#26410;&#30693;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31283;&#20581;&#22320;&#36816;&#34892;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;&#19987;&#38376;&#38024;&#23545;MedIA&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#22312;&#26356;&#22823;&#33539;&#22260;MedIA&#31995;&#32479;&#20869;&#30340;&#20132;&#20114;&#26041;&#24335;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#19981;&#20165;&#20165;&#32771;&#34385;&#26041;&#27861;&#23398;&#65292;&#36824;&#32771;&#34385;&#20102;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#20998;&#20026;&#25968;&#25454;&#23618;&#27425;&#30340;&#26041;&#27861;&#8230;
&lt;/p&gt;
&lt;p&gt;
Medical Image Analysis (MedIA) has become an essential tool in medicine and healthcare, aiding in disease diagnosis, prognosis, and treatment planning, and recent successes in deep learning (DL) have made significant contributions to its advances. However, DL models for MedIA remain challenging to deploy in real-world situations, failing for generalization under the distributional gap between training and testing samples, known as a distribution shift problem. Researchers have dedicated their efforts to developing various DL methods to adapt and perform robustly on unknown and out-of-distribution data distributions. This paper comprehensively reviews domain generalization studies specifically tailored for MedIA. We provide a holistic view of how domain generalization techniques interact within the broader MedIA system, going beyond methodologies to consider the operational implications on the entire MedIA workflow. Specifically, we categorize domain generalization methods into data-lev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;TD3&#31639;&#27861;&#30340;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#65292;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#20013;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#31283;&#23450;&#25910;&#25947;&#21644;&#25913;&#36827;&#23433;&#20840;&#24615;&#33021;&#65292;&#24182;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#31561;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08595</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#20132;&#21449;&#36335;&#21475;&#23548;&#33322;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation. (arXiv:2310.08595v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;TD3&#31639;&#27861;&#30340;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#65292;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#20013;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#31283;&#23450;&#25910;&#25947;&#21644;&#25913;&#36827;&#23433;&#20840;&#24615;&#33021;&#65292;&#24182;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#31561;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23494;&#38598;&#20132;&#36890;&#22330;&#26223;&#20013;&#65292;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#26102;&#22320;&#20351;AVs&#20570;&#20986;&#23433;&#20840;&#39640;&#25928;&#30340;&#20915;&#31574;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;TD3&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20302;&#25104;&#26412;&#12289;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;T&#22411;&#36335;&#21475;&#19978;&#30340;&#39640;&#25928;&#23433;&#20840;&#23548;&#33322;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25105;&#20204;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#19978;&#23545;&#25105;&#20204;&#30340;TD3&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;&#35813;&#26041;&#27861;&#21576;&#29616;&#20986;&#31283;&#23450;&#30340;&#25910;&#25947;&#24615;&#21644;&#25913;&#36827;&#30340;&#23433;&#20840;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#23494;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;AV&#33021;&#22815;&#26377;&#25928;&#22320;&#23548;&#33322;T&#22411;&#36335;&#21475;&#65292;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23545;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#39046;&#22495;&#30340;&#24212;&#29992;&#36129;&#29486;&#20102;&#26032;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs). Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time. Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm. We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities. Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost. This study contributes to the growing body of knowledge on reinforceme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#24191;&#20041;&#24615;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#26377;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.08008</link><description>&lt;p&gt;
BERT&#24191;&#20041;&#24615;&#30340;&#24433;&#21709;&#65306;&#20154;&#20026;&#23545;&#25239;&#26679;&#26412;&#21644;&#21451;&#22909;&#26679;&#26412;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Effects of Human Adversarial and Affable Samples on BERT Generalizability. (arXiv:2310.08008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#24191;&#20041;&#24615;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#26377;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#39046;&#20808;&#27036;&#19978;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#22312;&#38656;&#35201;&#27867;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#26426;&#22120;&#23398;&#20064;&#27867;&#21270;&#33021;&#21147;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#30740;&#31350;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#25968;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#20004;&#20010;&#29305;&#24449;&#65306;&#20154;&#20026;&#23545;&#25239;&#26679;&#26412;&#65288;&#20855;&#26377;&#30475;&#20284;&#24494;&#23567;&#24046;&#24322;&#20294;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#23545;&#65289;&#21644;&#20154;&#20026;&#21451;&#22909;&#26679;&#26412;&#65288;&#20855;&#26377;&#24494;&#23567;&#24046;&#24322;&#20294;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#23545;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#20197;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#20026;&#32463;&#39564;&#65292;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#26368;&#22810;20&#20010;&#30334;&#20998;&#28857;&#12290;&#36229;&#36807;&#27492;&#33539;&#22260;&#30340;&#22686;&#21152;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
BERT-based models have had strong performance on leaderboards, yet have been demonstrably worse in real-world settings requiring generalization. Limited quantities of training data is considered a key impediment to achieving generalizability in machine learning. In this paper, we examine the impact of training \textit{data quality}, not quantity, on a model's generalizability. We consider two characteristics of training data: the portion of human-adversarial (h-adversarial), i.e., sample pairs with seemingly minor differences but different ground-truth labels, and human-affable (h-affable) training samples, i.e., sample pairs with minor differences but the same ground-truth label. We find that for a fixed size of training samples, as a rule of thumb, having 10-30\% h-adversarial instances improves the precision, and therefore F1, by up to 20 points in the tasks of text classification and relation extraction. Increasing h-adversarials beyond this range can result in performance plateaus
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#20027;&#35266;&#24615;&#20250;&#36127;&#38754;&#24433;&#21709;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#20855;&#26377;&#37325;&#35201;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07849</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#65306;&#28508;&#21147;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations. (arXiv:2310.07849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#20027;&#35266;&#24615;&#20250;&#36127;&#38754;&#24433;&#21709;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#20855;&#26377;&#37325;&#35201;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#21644;&#25972;&#29702;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#24320;&#21457;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#20276;&#38543;&#30528;&#24040;&#22823;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#25237;&#20837;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#25903;&#25345;&#27169;&#22411;&#35757;&#32451;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#35843;&#33410;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#26377;&#25928;&#24615;&#30340;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20998;&#31867;&#30340;&#20027;&#35266;&#24615;&#22914;&#20309;&#24433;&#21709;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35266;&#24615;&#22312;&#20219;&#21153;&#23618;&#38754;&#21644;&#23454;&#20363;&#23618;&#38754;&#19978;&#37117;&#19982;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#21576;&#36127;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#21033;&#29992;LLM&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM fo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.07805</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20301;&#38543;&#26426;&#26725;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26159;&#29992;&#20110;&#36830;&#32493;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;DMs&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#65288;&#21363;&#20301;&#32622;&#31354;&#38388;&#65289;&#20013;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21453;&#28436;&#26469;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20301;&#31354;&#38388;&#21160;&#21147;&#23398;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#20854;&#20013;&#30456;&#20301;&#31354;&#38388;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#21253;&#25324;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#22686;&#24378;&#31354;&#38388;&#12290;&#21033;&#29992;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#30456;&#20301;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#27979;&#24230;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37319;&#26679;&#12290;&#19982;DMs&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#23601;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#12290;&#36825;&#31181;&#26089;&#26399;&#39044;&#27979;&#20026;&#36890;&#36807;&#27839;&#36712;&#36857;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23567;&#20989;&#25968;&#35780;&#20272;&#25968;&#37327;&#30340;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;iT2I&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;Mini-DALLE3&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#65292;&#29992;&#25143;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#12289;&#32534;&#36753;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.07653</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;&#65306;Mini-DALLE3&#65306;&#36890;&#36807;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models. (arXiv:2310.07653v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07653
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;iT2I&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;Mini-DALLE3&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#65292;&#29992;&#25143;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#12289;&#32534;&#36753;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20869;&#23481;&#29983;&#25104;&#30340;&#38761;&#21629;&#22312;&#32321;&#30427;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#20013;&#24471;&#21040;&#20102;&#24555;&#36895;&#25512;&#36827;&#12290;&#22312;&#20165;&#20165;&#20004;&#24180;&#30340;&#21457;&#23637;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#33021;&#22815;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#21019;&#36896;&#21147;&#29983;&#25104;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26222;&#36941;&#23384;&#22312;&#30340;&#19968;&#20010;&#38480;&#21046;&#26159;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#36825;&#20123;&#27969;&#34892;&#30340;T2I&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;Stable Diffusion&#65289;&#36827;&#34892;&#26377;&#25928;&#30340;&#20132;&#27969;&#12290;&#36825;&#36890;&#24120;&#20351;&#24471;&#33719;&#21462;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#22270;&#20687;&#21464;&#24471;&#22256;&#38590;&#65292;&#38500;&#38750;&#22312;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#20013;&#20855;&#26377;&#19987;&#19994;&#30693;&#35782;&#65292;&#21253;&#25324;&#22797;&#26434;&#30340;&#21333;&#35789;&#32452;&#21512;&#12289;&#39764;&#27861;&#26631;&#31614;&#21644;&#27880;&#37322;&#12290;&#21463;&#26368;&#36817;&#21457;&#24067;&#30340;DALLE3&#30340;&#21551;&#21457;&#65292;DALLE3&#26159;&#19968;&#20010;&#30452;&#25509;&#20869;&#32622;&#20110;ChatGPT&#20013;&#20197;&#20154;&#31867;&#35821;&#35328;&#36827;&#34892;&#23545;&#35805;&#30340;T2I&#27169;&#22411;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#30340;T2I&#31995;&#32479;&#65292;&#21162;&#21147;&#23454;&#29616;&#20154;&#31867;&#24847;&#22270;&#30340;&#23545;&#40784;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153; - &#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;iT2I&#65289;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#20154;&#20204;&#21487;&#20197;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#20132;&#26367;&#29983;&#25104;/&#32534;&#36753;/&#20248;&#21270;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#24182;&#36827;&#34892;&#38382;&#31572;&#65292;&#20197;&#33719;&#24471;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The revolution of artificial intelligence content generation has been rapidly accelerated with the booming text-to-image (T2I) diffusion models. Within just two years of development, it was unprecedentedly of high-quality, diversity, and creativity that the state-of-the-art models could generate. However, a prevalent limitation persists in the effective communication with these popular T2I models, such as Stable Diffusion, using natural language descriptions. This typically makes an engaging image hard to obtain without expertise in prompt engineering with complex word compositions, magic tags, and annotations. Inspired by the recently released DALLE3 - a T2I model directly built-in ChatGPT that talks human language, we revisit the existing T2I systems endeavoring to align human intent and introduce a new task - interactive text to image (iT2I), where people can interact with LLM for interleaved high-quality image generation/edit/refinement and question answering with stronger images a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#22885;&#36187;&#32599;-GPT&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#32447;&#24615;&#34920;&#31034;&#23545;&#31435;&#26827;&#23376;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#32447;&#24615;&#19990;&#30028;&#34920;&#31034;&#21644;&#22240;&#26524;&#20915;&#31574;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21450;&#20854;&#19982;&#23618;&#28145;&#24230;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07582</link><description>&lt;p&gt;
&#31616;&#21333;&#21464;&#21387;&#22120;&#20013;&#30340;&#32447;&#24615;&#28508;&#22312;&#19990;&#30028;&#27169;&#22411;: &#22885;&#36187;&#32599;-GPT&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT. (arXiv:2310.07582v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#22885;&#36187;&#32599;-GPT&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#32447;&#24615;&#34920;&#31034;&#23545;&#31435;&#26827;&#23376;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#32447;&#24615;&#19990;&#30028;&#34920;&#31034;&#21644;&#22240;&#26524;&#20915;&#31574;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21450;&#20854;&#19982;&#23618;&#28145;&#24230;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#22312;&#20915;&#31574;&#21644;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#23545;&#19990;&#30028;&#30340;&#30495;&#27491;&#29702;&#35299;&#19982;&#32431;&#31929;&#30340;&#38543;&#26426;&#27169;&#20223;&#30456;&#23545;&#30340;&#35752;&#35770;&#25345;&#32493;&#23384;&#22312;&#12290;&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#22312;&#22885;&#36187;&#32599;&#20013;&#35757;&#32451;&#30340;&#31616;&#21333;&#21464;&#21387;&#22120;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#20197;&#22686;&#24378;&#23545;&#22885;&#36187;&#32599;-GPT&#26032;&#20852;&#19990;&#30028;&#27169;&#22411;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22885;&#36187;&#32599;-GPT&#21253;&#21547;&#20102;&#23545;&#31435;&#26827;&#23376;&#30340;&#32447;&#24615;&#34920;&#31034;&#65292;&#36825;&#19968;&#22240;&#32032;&#22312;&#39537;&#21160;&#20854;&#20915;&#31574;&#36807;&#31243;&#20013;&#36215;&#21040;&#22240;&#26524;&#24615;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#32447;&#24615;&#19990;&#30028;&#34920;&#31034;&#19982;&#22240;&#26524;&#20915;&#31574;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#23618;&#28145;&#24230;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#24050;&#32463;&#23558;&#20195;&#30721;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models exhibit significant capabilities in decision-making and logical deductions. Nonetheless, a continuing discourse persists regarding their genuine understanding of the world as opposed to mere stochastic mimicry. This paper meticulously examines a simple transformer trained for Othello, extending prior research to enhance comprehension of the emergent world model of Othello-GPT. The investigation reveals that Othello-GPT encapsulates a linear representation of opposing pieces, a factor that causally steers its decision-making process. This paper further elucidates the interplay between the linear world representation and causal decision-making, and their dependence on layer depth and model complexity. We have made the code public.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#12290;&#35813;&#25968;&#25454;&#38598;&#36136;&#37327;&#39640;&#65292;&#26377;&#21161;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2310.07397</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#30340;&#20010;&#24615;&#21270;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#65306;&#38382;&#39064;&#24418;&#24335;&#21270;&#19982;&#25968;&#25454;&#38598;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation. (arXiv:2310.07397v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07397
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#12290;&#35813;&#25968;&#25454;&#38598;&#36136;&#37327;&#39640;&#65292;&#26377;&#21161;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20027;&#21160;&#24341;&#23548;&#23545;&#35805;&#26397;&#21521;&#39044;&#23450;&#30340;&#30446;&#26631;&#25110;&#36798;&#25104;&#29305;&#23450;&#30340;&#31995;&#32479;&#30446;&#26631;&#65292;&#22312;&#23545;&#35805;&#23436;&#25104;&#36807;&#31243;&#20013;&#32771;&#34385;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35282;&#33394;&#25198;&#28436;&#26041;&#27861;&#30340;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#65292;&#21253;&#21547;&#32422;18K&#20010;&#22810;&#36718;&#23545;&#35805;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#65292;&#21487;&#20197;&#29992;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a &lt;dialogue act, topic&gt; pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one from scratch requires tremendous human effort. To address this, we propose an automatic dataset curation framework using a role-playing approach. Based on this framework, we construct a large-scale personalized target-oriented dialogue dataset, TopDial, which comprises about 18K multi-turn dialogues. The experimental results show that this dataset is of high quality and could contribute to exploring personalized target-oriented dialogue.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07321</link><description>&lt;p&gt;
&#20851;&#20110;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#22312;&#36890;&#29992;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#19978;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#31361;&#26174;&#20102;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#32771;&#23519;&#25968;&#25454;&#22810;&#26679;&#24615;&#39640;&#20110;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#39046;&#22495;&#25991;&#26412;&#30340;&#24503;&#35821;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#26088;&#22312;&#21253;&#21547;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21442;&#25968;&#33539;&#22260;&#20174;122M&#21040;750M&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20351;&#29992;&#36136;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#20808;&#21069;&#26368;&#20808;&#36827;&#32467;&#26524;&#19978;&#25552;&#20986;&#20102;&#39640;&#36798;4.45%&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;https://huggingface.co/ikim-uk-essen&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#20171;&#32461;&#26368;&#26032;&#30340;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#28085;&#30422;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#27010;&#24565;&#21644;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#24635;&#32467;&#20102;&#20010;&#20154;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#21644;&#21453;&#28436;&#31561;&#37325;&#35201;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.07204</link><description>&lt;p&gt;
&#35270;&#35273;&#35745;&#31639;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
State of the Art on Diffusion Models for Visual Computing. (arXiv:2310.07204v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07204
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#20171;&#32461;&#26368;&#26032;&#30340;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#28085;&#30422;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#27010;&#24565;&#21644;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#24635;&#32467;&#20102;&#20010;&#20154;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#21644;&#21453;&#28436;&#31561;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#23427;&#20026;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#22330;&#26223;&#30340;&#29983;&#25104;&#12289;&#32534;&#36753;&#21644;&#37325;&#24314;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#26159;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#39318;&#36873;&#26550;&#26500;&#12290;&#20165;&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#24037;&#20855;&#21644;&#24212;&#29992;&#30340;&#25991;&#29486;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#24182;&#19988;&#28041;&#21450;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#30456;&#20851;&#35770;&#25991;&#27599;&#22825;&#37117;&#22312;arXiv&#19978;&#21457;&#34920;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#20351;&#24471;&#36319;&#19978;&#25152;&#26377;&#26368;&#26032;&#21457;&#23637;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20221;&#26368;&#26032;&#25216;&#26415;&#25253;&#21578;&#65288;STAR&#65289;&#30340;&#30446;&#26631;&#26159;&#20171;&#32461;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#25968;&#23398;&#27010;&#24565;&#12289;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#29616;&#32454;&#33410;&#21644;&#35774;&#35745;&#36873;&#25321;&#65292;&#20197;&#21450;&#27010;&#36848;&#36825;&#20123;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#12289;&#21453;&#28436;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Mor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#31232;&#30095;&#24494;&#35843;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;SquareHead&#65292;&#21487;&#20197;&#22312;&#39640;&#31232;&#30095;&#24615;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65307;&#21516;&#26102;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21487;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;</title><link>http://arxiv.org/abs/2310.06927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31232;&#30095;&#24494;&#35843;&#30340;&#25512;&#29702;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Sparse Finetuning for Inference Acceleration of Large Language Models. (arXiv:2310.06927v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#31232;&#30095;&#24494;&#35843;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;SquareHead&#65292;&#21487;&#20197;&#22312;&#39640;&#31232;&#30095;&#24615;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65307;&#21516;&#26102;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21487;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#35757;&#32451;&#36807;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#31934;&#30830;&#30340;&#31232;&#30095;&#24494;&#35843;&#65292;&#21363;&#22312;&#19987;&#38376;&#20219;&#21153;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#22312;&#26435;&#37325;&#19978;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#20934;&#24494;&#35843;&#21487;&#33021;&#26080;&#27861;&#24674;&#22797;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#31232;&#30095;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#33976;&#39311;&#31867;&#22411;&#30340;&#25439;&#22833;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;SquareHead&#65292;&#21363;&#20351;&#22312;&#26356;&#39640;&#30340;&#31232;&#30095;&#24615;&#19979;&#65292;&#23427;&#20063;&#33021;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#22411;&#31867;&#22411;&#12290;&#22312;&#23454;&#38469;&#25928;&#29575;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#24615;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#12290;&#34429;&#28982;&#26631;&#20934;&#26041;&#27861;&#26159;&#21033;&#29992;&#31232;&#30095;&#24615;&#36827;&#34892;&#35745;&#31639;&#20943;&#23569;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#20197;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#20110;&#31232;&#30095;&#24615;&#23548;&#33268;&#30340;&#36895;&#24230;&#25552;&#21319;&#20197;&#21450;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#31471;&#21040;&#31471;&#32467;&#26524;&#65292;&#24212;&#29992;&#20110;T5 (&#35821;&#35328;&#32763;&#35793;)&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of accurate sparse finetuning of large language models (LLMs), that is, finetuning pretrained LLMs on specialized tasks, while inducing sparsity in their weights. On the accuracy side, we observe that standard loss-based finetuning may fail to recover accuracy, especially at high sparsities. To address this, we perform a detailed study of distillation-type losses, determining an L2-based distillation approach we term SquareHead which enables accurate recovery even at higher sparsities, across all model types. On the practical efficiency side, we show that sparse LLMs can be executed with speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While the standard approach is to leverage sparsity for computational reduction, we observe that in the case of memory-bound LLMs sparsity can also be leveraged for reducing memory bandwidth. We exhibit end-to-end results showing speedups due to sparsity, while recovering accuracy, on T5 (language translati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;Faster R-CNN&#19982;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#12289;&#20351;&#29992;&#21487;&#21464;&#24418;&#32593;&#32476;&#20197;&#21450;&#24341;&#20837;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#27169;&#22359;&#31561;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DANet&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#26816;&#27979;&#21046;&#36896;&#19994;&#29615;&#22659;&#20013;&#30340;&#23567;&#29289;&#20307;&#65292;&#21253;&#25324;&#24494;&#23567;&#19988;&#22797;&#26434;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.05768</link><description>&lt;p&gt;
DANet&#65306;&#36890;&#36807;&#39640;&#25928;&#30340;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#32593;&#32476;&#25552;&#39640;&#23567;&#30446;&#26631;&#26816;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DANet: Enhancing Small Object Detection through an Efficient Deformable Attention Network. (arXiv:2310.05768v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05768
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;Faster R-CNN&#19982;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#12289;&#20351;&#29992;&#21487;&#21464;&#24418;&#32593;&#32476;&#20197;&#21450;&#24341;&#20837;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#27169;&#22359;&#31561;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DANet&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#26816;&#27979;&#21046;&#36896;&#19994;&#29615;&#22659;&#20013;&#30340;&#23567;&#29289;&#20307;&#65292;&#21253;&#25324;&#24494;&#23567;&#19988;&#22797;&#26434;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#20934;&#30830;&#22320;&#26816;&#27979;&#21046;&#36896;&#19994;&#29615;&#22659;&#20013;&#30340;&#23567;&#29289;&#20307;&#65288;&#22914;&#32570;&#38519;&#21644;&#35010;&#32441;&#65289;&#23545;&#20110;&#30830;&#20445;&#20135;&#21697;&#36136;&#37327;&#21644;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;Faster R-CNN&#19982;&#20808;&#36827;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#23558;Faster R-CNN&#19982;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#21046;&#36896;&#29615;&#22659;&#20013;&#22266;&#26377;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21487;&#21464;&#24418;&#32593;&#32476;&#65292;&#21487;&#20197;&#26681;&#25454;&#32570;&#38519;&#30340;&#20960;&#20309;&#21464;&#21270;&#36827;&#34892;&#21464;&#24418;&#65292;&#24182;&#31934;&#30830;&#26816;&#27979;&#24494;&#23567;&#32780;&#22797;&#26434;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#22522;&#30784;&#30340;ResNet50&#32593;&#32476;&#30340;&#27599;&#20010;&#22359;&#20013;&#21152;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#36873;&#25321;&#24615;&#22320;&#24378;&#35843;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#24182;&#21387;&#21046;&#19981;&#22826;&#26377;&#29992;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#25105;&#20204;&#37319;&#29992;&#20102;RoI Align&#26469;&#26367;&#20195;RoI Pooling&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#23545;&#40784;&#65292;&#26368;&#21518;&#38598;&#25104;Focal Loss&#26469;&#26377;&#25928;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient and accurate detection of small objects in manufacturing settings, such as defects and cracks, is crucial for ensuring product quality and safety. To address this issue, we proposed a comprehensive strategy by synergizing Faster R-CNN with cutting-edge methods. By combining Faster R-CNN with Feature Pyramid Network, we enable the model to efficiently handle multi-scale features intrinsic to manufacturing environments. Additionally, Deformable Net is used that contorts and conforms to the geometric variations of defects, bringing precision in detecting even the minuscule and complex features. Then, we incorporated an attention mechanism called Convolutional Block Attention Module in each block of our base ResNet50 network to selectively emphasize informative features and suppress less useful ones. After that we incorporated RoI Align, replacing RoI Pooling for finer region-of-interest alignment and finally the integration of Focal Loss effectively handles class imbalance, cruc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#21512;&#30740;&#31350;&#23637;&#31034;&#20102;&#35270;&#35273;Transformer&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#35270;&#35273;&#38382;&#39064;&#19978;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.05664</link><description>&lt;p&gt;
ViTs&#26080;&#22788;&#19981;&#22312;&#65306;&#23637;&#31034;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#35270;&#35273;Transformer&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ViTs are Everywhere: A Comprehensive Study Showcasing Vision Transformers in Different Domain. (arXiv:2310.05664v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05664
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#21512;&#30740;&#31350;&#23637;&#31034;&#20102;&#35270;&#35273;Transformer&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#35270;&#35273;&#38382;&#39064;&#19978;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#35774;&#35745;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;Transformer&#35774;&#35745;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25104;&#21151;&#26368;&#36817;&#24341;&#36215;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#12290;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30456;&#27604;&#65292;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#27491;&#22312;&#25104;&#20026;&#35768;&#22810;&#35270;&#35273;&#38382;&#39064;&#30340;&#27969;&#34892;&#21644;&#20027;&#23548;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35270;&#35273;&#22522;&#20934;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#31561;&#20854;&#20182;&#31867;&#22411;&#32593;&#32476;&#12290;&#26412;&#24037;&#20316;&#36890;&#36807;&#23558;&#21508;&#31181;&#35270;&#35273;Transformer&#27169;&#22411;&#21010;&#20998;&#20026;&#19981;&#21516;&#20219;&#21153;&#24182;&#30740;&#31350;&#20854;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;ViTs&#33021;&#22815;&#20811;&#26381;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21487;&#33021;&#36935;&#21040;&#30340;&#19968;&#20123;&#22256;&#38590;&#12290;&#26412;&#35843;&#26597;&#30340;&#30446;&#26631;&#26159;&#23637;&#31034;ViTs&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#39318;&#27425;&#24212;&#29992;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#23545;&#36866;&#29992;ViTs&#30340;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#35782;&#21035;&#12289;&#22270;&#20687;&#20998;&#21106;&#12289;&#35270;&#39057;Transformer&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer design is the de facto standard for natural language processing tasks. The success of the transformer design in natural language processing has lately piqued the interest of researchers in the domain of computer vision. When compared to Convolutional Neural Networks (CNNs), Vision Transformers (ViTs) are becoming more popular and dominant solutions for many vision problems. Transformer-based models outperform other types of networks, such as convolutional and recurrent neural networks, in a range of visual benchmarks. We evaluate various vision transformer models in this work by dividing them into distinct jobs and examining their benefits and drawbacks. ViTs can overcome several possible difficulties with convolutional neural networks (CNNs). The goal of this survey is to show the first use of ViTs in CV. In the first phase, we categorize various CV applications where ViTs are appropriate. Image classification, object identification, image segmentation, video transformer, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#12290;&#36890;&#36807;&#32479;&#19968;&#24314;&#27169;&#21644;&#26377;&#25928;&#20449;&#24687;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#21644;&#20302;&#25928;&#20197;&#21450;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05364</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#22320;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths. (arXiv:2310.05364v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#12290;&#36890;&#36807;&#32479;&#19968;&#24314;&#27169;&#21644;&#26377;&#25928;&#20449;&#24687;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#21644;&#20302;&#25928;&#20197;&#21450;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#20174;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#30830;&#23450;&#31561;&#20215;&#30340;&#23454;&#20307;&#23545;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#32479;&#19968;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#22823;&#22810;&#25968;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#27169;&#24577;&#65292;&#32570;&#20047;&#23545;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#25506;&#32034;&#12290;&#23569;&#25968;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#20570;&#20986;&#20102;&#19981;&#38169;&#30340;&#23581;&#35797;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#65306;(1)&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#19988;&#20302;&#25928;&#65292;&#20026;&#27599;&#20010;&#27169;&#24577;&#35774;&#35745;&#22797;&#26434;&#21644;&#29420;&#31435;&#30340;&#27169;&#22411;&#65307;(2)&#30001;&#20110;&#23454;&#20307;&#23545;&#40784;&#20013;&#27169;&#24577;&#30340;&#24322;&#26500;&#24615;&#65292;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PathFusion&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;(1) MSP&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36830;&#25509;&#23454;&#20307;&#21644;&#27169;&#24577;&#33410;&#28857;&#20197;&#34920;&#31034;&#22810;&#20010;&#27169;&#24577;&#30340;&#36335;&#24452;&#65292;&#31616;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#65307;(2) IRF&#65292;&#19968;&#31181;&#36845;&#20195;&#34701;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#36335;&#24452;&#20316;&#20026;&#20449;&#24687;&#36733;&#20307;&#65292;&#26377;&#25928;&#22320;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of Entity Alignment (EA) is to identify equivalent entity pairs from multiple Knowledge Graphs (KGs) and create a more comprehensive and unified KG. The majority of EA methods have primarily focused on the structural modality of KGs, lacking exploration of multi-modal information. A few multi-modal EA methods have made good attempts in this field. Still, they have two shortcomings: (1) inconsistent and inefficient modality modeling that designs complex and distinct models for each modality; (2) ineffective modality fusion due to the heterogeneous nature of modalities in EA. To tackle these challenges, we propose PathFusion, consisting of two main components: (1) MSP, a unified modeling approach that simplifies the alignment process by constructing paths connecting entities and modality nodes to represent multiple modalities; (2) IRF, an iterative fusion method that effectively combines information from different modalities using the path as an information carrier. Experim
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05280</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#38543;&#26426;&#40550;&#40521;&#26356;&#21361;&#38505;&#21527;&#65311;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#20854;&#33021;&#22815;&#25353;&#29031;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#65292;&#21253;&#25324;&#22312;&#23545;&#35805;&#20013;&#27169;&#20223;&#36890;&#29992;&#25110;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#20154;&#26684;&#12290;&#36890;&#29992;&#20154;&#26684;&#25351;&#30340;&#26159;&#26469;&#33258;&#26576;&#19968;&#20154;&#21475;&#32676;&#20307;&#30340;&#20010;&#20307;&#65288;&#20363;&#22914;&#20122;&#27954;&#20154;&#65289;&#65292;&#32780;&#29305;&#23450;&#20154;&#26684;&#21487;&#20197;&#26159;&#21382;&#21490;&#20154;&#29289;&#30340;&#23454;&#38469;&#22995;&#21517;&#12290;&#34429;&#28982;&#37319;&#29992;&#20154;&#26684;&#20351;&#23545;&#35805;&#31995;&#32479;&#26356;&#20855;&#21560;&#24341;&#21147;&#21644;&#20146;&#21644;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#65292;&#21487;&#33021;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#32780;&#21152;&#21095;&#31038;&#20250;&#20559;&#35265;&#65292;&#36827;&#19968;&#27493;&#36896;&#25104;&#31038;&#20250;&#20260;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#8220;&#20154;&#26684;&#20559;&#35265;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26377;&#23475;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#23545;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23558;&#20154;&#26684;&#20559;&#35265;&#20998;&#20026;&#26377;&#23475;&#34920;&#36798;&#21644;&#26377;&#23475;&#35748;&#21516;&#20004;&#31867;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#34913;&#37327;&#20116;&#20010;&#26041;&#38754;&#30340;&#20154;&#26684;&#20559;&#35265;&#65306;&#20882;&#29359;&#24615;&#12289;&#26377;&#27602;&#24310;&#32493;&#12289;&#20851;&#24576;&#12289;&#21051;&#26495;&#21360;&#35937;&#30340;&#35748;&#21516;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
&lt;/p&gt;</description></item><item><title>Text2NKG&#26159;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;N&#20803;&#20851;&#31995;&#30693;&#35782;&#22270;&#30340;&#32454;&#31890;&#24230;N&#20803;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#25903;&#25345;&#22810;&#31181;NKG&#27169;&#24335;&#65292;&#20855;&#26377;&#39640;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05185</link><description>&lt;p&gt;
Text2NKG: &#38754;&#21521;N&#20803;&#20851;&#31995;&#30693;&#35782;&#22270;&#26500;&#24314;&#30340;&#32454;&#31890;&#24230;N&#20803;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction. (arXiv:2310.05185v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05185
&lt;/p&gt;
&lt;p&gt;
Text2NKG&#26159;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;N&#20803;&#20851;&#31995;&#30693;&#35782;&#22270;&#30340;&#32454;&#31890;&#24230;N&#20803;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#25903;&#25345;&#22810;&#31181;NKG&#27169;&#24335;&#65292;&#20855;&#26377;&#39640;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#20256;&#32479;&#30340;&#20108;&#20803;&#20851;&#31995;&#20107;&#23454;&#22806;&#65292;N&#20803;&#20851;&#31995;&#30693;&#35782;&#22270;(NKGs)&#30001;&#21253;&#21547;&#20004;&#20010;&#20197;&#19978;&#23454;&#20307;&#30340;N&#20803;&#20851;&#31995;&#20107;&#23454;&#32452;&#25104;&#65292;&#26356;&#25509;&#36817;&#20110;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#30495;&#23454;&#19990;&#30028;&#20107;&#23454;&#12290;&#28982;&#32780;&#65292;NKG&#30340;&#26500;&#24314;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#21171;&#21160;&#65292;&#24182;&#19988;N&#20803;&#20851;&#31995;&#25277;&#21462;&#20173;&#28982;&#20572;&#30041;&#22312;&#31895;&#31890;&#24230;&#27700;&#24179;&#65292;&#36890;&#24120;&#26159;&#22312;&#21333;&#19968;&#27169;&#24335;&#21644;&#22266;&#23450;&#30340;&#23454;&#20307;&#25968;&#37327;&#19978;&#25805;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Text2NKG&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;N&#20803;&#20851;&#31995;&#30693;&#35782;&#22270;&#26500;&#24314;&#30340;&#32454;&#31890;&#24230;N&#20803;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36328;&#24230;&#20803;&#32452;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#24322;&#26500;&#25490;&#24207;&#21512;&#24182;&#26469;&#23454;&#29616;&#19981;&#21516;&#24230;&#30340;&#32454;&#31890;&#24230;N&#20803;&#20851;&#31995;&#25277;&#21462;&#12290;&#27492;&#22806;&#65292;Text2NKG&#25903;&#25345;&#22235;&#31181;&#20856;&#22411;&#30340;NKG&#27169;&#24335;&#65306;&#36229;&#20851;&#31995;&#27169;&#24335;&#12289;&#22522;&#20110;&#20107;&#20214;&#30340;&#27169;&#24335;&#12289;&#22522;&#20110;&#35282;&#33394;&#30340;&#27169;&#24335;&#21644;&#36229;&#22270;&#27169;&#24335;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Text2NKG&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;N&#20803;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two entities, which are closer to real-world facts with broader applications. However, the construction of NKGs still significantly relies on manual labor, and n-ary relation extraction still remains at a course-grained level, which is always in a single schema and fixed arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality. Experimental results demonstrate that Text2NKG outperforms the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04668</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMS&#65289;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Label-free Node Classification on Graphs with Large Language Models (LLMS). (arXiv:2310.04668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Graph Neural Networks&#65292;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30830;&#20445;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#39640;&#25928;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#25512;&#29702;&#25104;&#26412;&#36739;&#39640;&#12290;&#37492;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26080;&#26631;&#31614;&#22270;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;LLM-GNN&#12290;&#23427;&#38598;&#25104;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLMs&#34987;&#29992;&#26469;&#27880;&#37322;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;GNNs&#33021;&#22815;&#39044;&#27979;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#12290;LLM-GNN&#30340;&#23454;&#29616;&#38754;&#20020;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#25105;&#20204;&#22914;&#20309;&#20027;&#21160;&#36873;&#25321;&#35201;&#30001;LLMs&#27880;&#37322;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#22686;&#24378;GNN&#30340;&#35757;&#32451;&#65311;&#25105;&#20204;&#22914;&#20309;&#21033;&#29992;LLMs&#26469;&#20248;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#65292;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#20026;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#12290;</title><link>http://arxiv.org/abs/2310.04443</link><description>&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;&#23637;&#26395;&#35770;&#25991;&#65289;
&lt;/p&gt;
&lt;p&gt;
Human Mobility Question Answering (Vision Paper). (arXiv:2310.04443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#65292;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#20026;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26681;&#25454;&#32473;&#23450;&#30340;&#30693;&#35782;&#28304;&#65288;&#20363;&#22914;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22270;&#20687;&#65289;&#23398;&#20064;&#22238;&#31572;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#25366;&#25496;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#23545;&#20110;&#26234;&#33021;&#22478;&#24066;&#35268;&#21010;&#12289;&#30123;&#24773;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24341;&#20837;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#20026;&#31227;&#21160;&#39044;&#27979;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#65292;&#24182;&#36827;&#19968;&#27493;&#20419;&#36827;&#20102;&#20154;&#31867;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25903;&#25345;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#36825;&#31687;&#23637;&#26395;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#35774;&#35745;&#21644;&#19968;&#20010;&#28508;&#22312;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) systems have attracted much attention from the artificial intelligence community as they can learn to answer questions based on the given knowledge source (e.g., images in visual question answering). However, the research into question answering systems with human mobility data remains unexplored. Mining human mobility data is crucial for various applications such as smart city planning, pandemic management, and personalised recommendation system. In this paper, we aim to tackle this gap and introduce a novel task, that is, human mobility question answering (MobQA). The aim of the task is to let the intelligent system learn from mobility data and answer related questions. This task presents a new paradigm change in mobility prediction research and further facilitates the research of human mobility recommendation systems. To better support this novel research topic, this vision paper also proposes an initial design of the dataset and a potential deep learning mod
&lt;/p&gt;</description></item><item><title>SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.03684</link><description>&lt;p&gt;
SmoothLLM&#65306;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03684
&lt;/p&gt;
&lt;p&gt;
SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21162;&#21147;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#65288;&#22914;GPT&#12289;Llama&#12289;Claude&#21644;PaLM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#65292;&#21363;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#27450;&#39575;&#65292;&#20197;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;LLM&#19978;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#30340;&#25552;&#31034;&#23545;&#23383;&#31526;&#32423;&#21035;&#30340;&#25913;&#21464;&#24456;&#33030;&#24369;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#39318;&#20808;&#38543;&#26426;&#25200;&#21160;&#32473;&#23450;&#36755;&#20837;&#25552;&#31034;&#30340;&#22810;&#20010;&#21103;&#26412;&#65292;&#28982;&#21518;&#27719;&#24635;&#30456;&#24212;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;SmoothLLM&#23558;&#20247;&#22810;&#28909;&#38376;LLM&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#23545;&#25915;&#20987;&#32531;&#35299;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#20351;&#29992;&#30340;&#26597;&#35810;&#25968;&#37327;&#27604;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#19982;&#20219;&#20309;LLM&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#36890;&#36807;&#35748;&#30693;&#36335;&#24452;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#20840;&#38754;&#25512;&#29702;&#21644;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#36716;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#22312;&#21333;&#36710;&#20219;&#21153;&#21644;&#22797;&#26434;&#39550;&#39542;&#34892;&#20026;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36825;&#26159;&#22240;&#20026;&#20854;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03026</link><description>&lt;p&gt;
LanguageMPC&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#32773;
&lt;/p&gt;
&lt;p&gt;
LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. (arXiv:2310.03026v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#36890;&#36807;&#35748;&#30693;&#36335;&#24452;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#20840;&#38754;&#25512;&#29702;&#21644;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#36716;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#22312;&#21333;&#36710;&#20219;&#21153;&#21644;&#22797;&#26434;&#39550;&#39542;&#34892;&#20026;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36825;&#26159;&#22240;&#20026;&#20854;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#29702;&#35299;&#39640;&#32423;&#20449;&#24687;&#12289;&#25512;&#24191;&#32597;&#35265;&#20107;&#20214;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#38656;&#35201;&#20154;&#31867;&#24120;&#35782;&#29702;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#35748;&#30693;&#36335;&#24452;&#65292;&#20351;LLMs&#33021;&#22815;&#36827;&#34892;&#20840;&#38754;&#25512;&#29702;&#65292;&#24182;&#24320;&#21457;&#20102;&#23558;LLM&#20915;&#31574;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;LLM&#20915;&#31574;&#36890;&#36807;&#24341;&#23548;&#21442;&#25968;&#30697;&#38453;&#36866;&#24212;&#19982;&#20302;&#32423;&#25511;&#21046;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#21333;&#36710;&#20219;&#21153;&#20013;&#22987;&#32456;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#33021;&#22788;&#29702;&#22797;&#26434;&#30340;&#39550;&#39542;&#34892;&#20026;&#65292;&#29978;&#33267;&#22810;&#36710;&#21327;&#35843;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;LLMs&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;LLMs&#20316;&#20026;&#26377;&#25928;&#20915;&#31574;&#32773;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-make
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;UMLS&#30340;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#30340;&#20107;&#23454;&#24615;&#12290;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#21644;&#21307;&#29983;&#35780;&#20272;&#65292;&#30740;&#31350;&#20154;&#21592;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02778</link><description>&lt;p&gt;
&#19968;&#20010;&#22686;&#24378;&#30340; UMLS &#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare. (arXiv:2310.02778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02778
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;UMLS&#30340;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#30340;&#20107;&#23454;&#24615;&#12290;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#21644;&#21307;&#29983;&#35780;&#20272;&#65292;&#30740;&#31350;&#20154;&#21592;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#30495;&#23454;&#20020;&#24202;&#22330;&#26223;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#19982;&#24050;&#24314;&#31435;&#21307;&#23398;&#20107;&#23454;&#20559;&#31163;&#30340;&#20869;&#23481;&#65292;&#29978;&#33267;&#21487;&#33021;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#30340;&#22686;&#24378;&#22411;LLM&#26694;&#26550;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#26381;&#21153;&#21307;&#30103;&#20445;&#20581;&#31038;&#21306;&#12290;&#25105;&#20204;&#37319;&#29992;LLaMa2-13b-chat&#21644;ChatGPT-3.5&#20316;&#20026;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;ROUGE&#20998;&#25968;&#21644;BERT&#20998;&#25968;&#22312;LiveQA&#27979;&#35797;&#38598;&#30340;104&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26681;&#25454;&#20107;&#23454;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#35835;&#24615;&#21644;&#30456;&#20851;&#24615;&#22235;&#20010;&#32500;&#24230;&#24314;&#31435;&#20102;&#21307;&#29983;&#35780;&#20272;&#26631;&#20934;&#12290;ChatGPT-3.5&#29992;&#20110;&#21307;&#29983;&#35780;&#20272;&#65292;&#38024;&#23545;LiveQA&#27979;&#35797;&#38598;&#30340;20&#20010;&#38382;&#39064;&#12290;&#22810;&#20301;&#20303;&#38498;&#21307;&#24072;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conduct
&lt;/p&gt;</description></item><item><title>MagicDrive&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#24182;&#25429;&#25417;&#20102;&#32454;&#33268;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.02601</link><description>&lt;p&gt;
MagicDrive: &#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#19979;&#30340;&#34903;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02601
&lt;/p&gt;
&lt;p&gt;
MagicDrive&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#24182;&#25429;&#25417;&#20102;&#32454;&#33268;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#20855;&#26377;2D&#25511;&#21046;&#30340;&#25968;&#25454;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#34903;&#26223;&#29983;&#25104;&#20013;&#31934;&#30830;&#30340;&#19977;&#32500;&#25511;&#21046;&#22312;&#19977;&#32500;&#24863;&#30693;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#40479;&#30640;&#22270;&#20316;&#20026;&#20027;&#35201;&#26465;&#20214;&#24120;&#24120;&#23548;&#33268;&#20960;&#20309;&#25511;&#21046;&#65288;&#22914;&#39640;&#24230;&#65289;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24433;&#21709;&#29289;&#20307;&#24418;&#29366;&#12289;&#36974;&#25377;&#27169;&#24335;&#21644;&#36947;&#36335;&#34920;&#38754;&#39640;&#31243;&#31561;&#23545;&#24863;&#30693;&#25968;&#25454;&#21512;&#25104;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicDrive&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#36890;&#36807;&#23450;&#21046;&#30340;&#32534;&#30721;&#31574;&#30053;&#23454;&#29616;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#36824;&#37319;&#29992;&#20102;&#36328;&#35270;&#22270;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#30830;&#20445;&#22810;&#20010;&#30456;&#26426;&#35270;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;MagicDrive&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#25429;&#25417;&#21040;&#20102;&#31934;&#32454;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MagicDrive, a novel street view generation framework offering diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MagicDrive, we achieve high-fidelity street-view synthesis that captures nuanced 3D ge
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.02071</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond. (arXiv:2310.02071v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#25913;&#36827;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#30001;&#20110;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20687;GPT4-Vision&#36825;&#26679;&#30340;MLLM&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;MLLMs&#33021;&#21542;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#22788;&#29702;&#20855;&#36523;&#20915;&#31574;&#65292;&#24182;&#19988;LLMs&#21644;MLLMs&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#21542;&#33021;&#22686;&#24378;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;PCA-EVAL&#30340;&#26032;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#20174;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#35282;&#24230;&#35780;&#20272;&#20855;&#36523;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOLMES&#65292;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#21033;&#29992;MLLMs&#21644;APIs&#33719;&#21462;&#22810;&#27169;&#24577;&#20449;&#24687;&#20197;&#36827;&#34892;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#27604;&#36739;&#20102;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#21644;HOLMES&#65292;&#24182;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#20013;&#38754;&#20020;&#30340;&#21516;&#36136;&#21270;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#36755;&#20986;&#32780;&#19981;&#29306;&#29298;&#29983;&#20135;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10448</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#20197;&#21450;&#31038;&#20250;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Human-AI Interactions and Societal Pitfalls. (arXiv:2309.10448v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#20013;&#38754;&#20020;&#30340;&#21516;&#36136;&#21270;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#36755;&#20986;&#32780;&#19981;&#29306;&#29298;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21512;&#20316;&#26102;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#30475;&#21040;&#29983;&#20135;&#21147;&#30340;&#25552;&#21319;&#65292;&#20294;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21487;&#33021;&#19981;&#23436;&#20840;&#31526;&#21512;&#20182;&#20204;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20854;&#20013;&#24322;&#36136;&#29992;&#25143;&#36873;&#25321;&#19982;AI&#20849;&#20139;&#22810;&#23569;&#20449;&#24687;&#65292;&#38754;&#20020;&#36755;&#20986;&#20445;&#30495;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20010;&#20307;&#20915;&#31574;&#19982;AI&#35757;&#32451;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#25361;&#25112;&#12290;&#36755;&#20986;&#21487;&#33021;&#21464;&#24471;&#26356;&#21152;&#21516;&#36136;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;AI&#22312;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#12290;&#32780;&#20219;&#20309;AI&#30340;&#20559;&#35265;&#21487;&#33021;&#25104;&#20026;&#31038;&#20250;&#20559;&#35265;&#12290;&#35299;&#20915;&#21516;&#36136;&#21270;&#21644;&#20559;&#35265;&#38382;&#39064;&#30340;&#21150;&#27861;&#26159;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#36755;&#20986;&#32780;&#19981;&#29306;&#29298;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When working with generative artificial intelligence (AI), users may see productivity gains, but the AI-generated content may not match their preferences exactly. To study this effect, we introduce a Bayesian framework in which heterogeneous users choose how much information to share with the AI, facing a trade-off between output fidelity and communication cost. We show that the interplay between these individual-level decisions and AI training may lead to societal challenges. Outputs may become more homogenized, especially when the AI is trained on AI-generated content. And any AI bias may become societal bias. A solution to the homogenization and bias issues is to improve human-AI interactions, enabling personalized outputs without sacrificing productivity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiscoSCMs&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#26597;&#35810;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#20998;&#26512;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#26102;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09323</link><description>&lt;p&gt;
&#29992;DiscoSCMs&#22238;&#31572;Layer 3&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Answering Layer 3 queries with DiscoSCMs. (arXiv:2309.09323v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiscoSCMs&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#26597;&#35810;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#20998;&#26512;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#26102;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#20013;&#65292;&#35299;&#20915;Pearl&#22240;&#26524;&#23618;&#27425;&#65288;PCH&#65289;&#19979;&#30340;&#20851;&#32852;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#30340;&#22240;&#26524;&#26597;&#35810;&#26159;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#19968;&#33268;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;DiscoSCMs&#65289;&#65292;&#25193;&#23637;&#20102;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#12290;&#20197;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#28508;&#22312;&#32467;&#26524;&#30340;&#30456;&#20851;&#27169;&#24335;$P(y_x, y'_{x'})$&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#23613;&#31649;&#21453;&#20107;&#23454;&#19981;&#20877;&#36864;&#21270;&#65292;&#20294;&#20173;&#26080;&#27861;&#30830;&#23450;&#12290;&#22240;&#27492;&#65292;&#23558;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#32435;&#20837;DiscoSCM&#12290;&#21457;&#29616;&#36890;&#36807;&#36866;&#24212;&#20998;&#24067;&#30340;&#23884;&#20837;&#24335;&#25512;&#26029;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing causal queries across the Pearl Causal Hierarchy (PCH) (i.e., associational, interventional and counterfactual), which is formalized as \Layer{} Valuations, is a central task in contemporary causal inference research. Counterfactual questions, in particular, pose a significant challenge as they often necessitate a complete knowledge of structural equations. This paper identifies \textbf{the degeneracy problem} caused by the consistency rule. To tackle this, the \textit{Distribution-consistency Structural Causal Models} (DiscoSCMs) is introduced, which extends both the structural causal models (SCM) and the potential outcome framework. The correlation pattern of potential outcomes in personalized incentive scenarios, described by $P(y_x, y'_{x'})$, is used as a case study for elucidation. Although counterfactuals are no longer degenerate, they remain indeterminable. As a result, the condition of independent potential noise is incorporated into DiscoSCM. It is found that by ad
&lt;/p&gt;</description></item><item><title>MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.08730</link><description>&lt;p&gt;
MusiLingo&#65306;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#38899;&#20048;&#23383;&#24149;&#21644;&#26597;&#35810;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. (arXiv:2309.08730v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08730
&lt;/p&gt;
&lt;p&gt;
MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#22810;&#27169;&#24577;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#25991;&#26412;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;&#34701;&#21512;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MusiLingo&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#21644;&#38899;&#20048;&#30456;&#20851;&#26597;&#35810;&#21709;&#24212;&#30340;&#26032;&#31995;&#32479;&#12290;MusiLingo&#20351;&#29992;&#19968;&#20010;&#25237;&#24433;&#23618;&#26469;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#38899;&#20048;&#38899;&#39057;&#27169;&#22411;MERT&#21644;&#20923;&#32467;&#30340;LLaMA&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#20048;&#34920;&#31034;&#65292;&#23454;&#29616;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#25351;&#23548;&#24615;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#38382;&#31572;&#25968;&#25454;&#38598;&#31232;&#32570;&#65292;&#25105;&#20204;&#20174;MusicCaps&#21019;&#24314;&#20102;MusicInstruct&#65288;MI&#65289;&#25968;&#25454;&#38598;&#65292;&#19987;&#20026;&#24320;&#25918;&#24335;&#38899;&#20048;&#26597;&#35810;&#32780;&#35774;&#35745;&#12290;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#23427;&#22312;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#32452;&#32455;&#38899;&#20048;&#30456;&#20851;&#38382;&#31572;&#23545;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#30340;&#25968;&#25454;&#38598;&#22312;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&amp;A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&amp;A pairs. Our introduced dataset enables notable advancements beyond previous ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#35299;&#20915;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#26679;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2309.07038</link><description>&lt;p&gt;
&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning for Jumping Monopods. (arXiv:2309.07038v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#35299;&#20915;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#26679;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#21363;&#20351;&#21333;&#33050;&#26426;&#22120;&#20154;&#33021;&#22815;&#36339;&#21040;&#20219;&#20309;&#26041;&#21521;&#65292;&#20854;&#33050;&#19979;&#30340;&#22320;&#24418;&#21487;&#33021;&#26159;&#19981;&#24179;&#30340;&#65292;&#25105;&#20204;&#35201;&#20351;&#23427;&#36798;&#21040;&#30446;&#26631;&#20301;&#32622;&#12290;&#36825;&#26159;&#19968;&#20010;&#26356;&#22823;&#31867;&#21035;&#38382;&#39064;&#30340;&#27169;&#26495;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#25216;&#26415;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#24378;&#21270;&#23398;&#20064; (RL) &#21487;&#33021;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23436;&#20840;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22312; RL &#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#24102;&#26469;&#20102;&#24191;&#27867;&#30340;&#22909;&#22788;&#65292;&#22914;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#25191;&#34892;&#36816;&#21160;&#30340;&#20302;&#32423;&#25511;&#21046;&#22120;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#22522;&#20110;&#20248;&#21270;&#21644;&#31471;&#21040;&#31471; RL &#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider the complex control problem of making a monopod reach a target with a jump. The monopod can jump in any direction and the terrain underneath its foot can be uneven. This is a template of a much larger class of problems, which are extremely challenging and computationally expensive to solve using standard optimisation-based techniques. Reinforcement Learning (RL) could be an interesting alternative, but the application of an end-to-end approach in which the controller must learn everything from scratch, is impractical. The solution advocated in this paper is to guide the learning process within an RL framework by injecting physical knowledge. This expedient brings to widespread benefits, such as a drastic reduction of the learning time, and the ability to learn and compensate for possible errors in the low-level controller executing the motion. We demonstrate the advantage of our approach with respect to both optimization-based and end-to-end RL approaches.
&lt;/p&gt;</description></item><item><title>&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03084</link><description>&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03084
&lt;/p&gt;
&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#30446;&#21069;&#35299;&#20915;&#22823;&#35268;&#27169;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;&#26412;&#25991;&#22312;CFR&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32431;CFR&#65288;PCFR&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;PCFR&#21487;&#20197;&#30475;&#20316;&#26159;CFR&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#30340;&#32467;&#21512;&#65292;&#32487;&#25215;&#20102;CFR&#30340;&#21453;&#20107;&#23454;&#36951;&#25022;&#65288;&#20540;&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#20351;&#29992;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#32780;&#19981;&#26159;&#36951;&#25022;&#21305;&#37197;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;PCFR&#21487;&#20197;&#23454;&#29616;Blackwell&#21487;&#36798;&#24615;&#65292;&#20351;PCFR&#33021;&#22815;&#19982;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#22312;&#20869;&#30340;&#20219;&#20309;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#32431;MCCFR&#65288;PMCCFR&#65289;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;PMCCFR&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#27604;MCCFR&#24555;&#19977;&#20493;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PMCCFR&#19981;&#36890;&#36807;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21160;&#31639;&#27861;&#65292;&#21463;&#21040;&#20102;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11131</link><description>&lt;p&gt;
ReLLa: &#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#34987;&#31215;&#26497;&#25506;&#32034;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36866;&#24212;&#21644;&#22686;&#24378;&#32431;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#25512;&#33616;&#39046;&#22495;&#20013;LLMs&#26080;&#27861;&#20174;&#38271;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ReLLa&#65289;&#12290;&#38024;&#23545;&#38646;&#26679;&#26412;&#25512;&#33616;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
&lt;/p&gt;</description></item><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.02182</link><description>&lt;p&gt;
AutoML4ETC: &#33258;&#21160;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23454;&#29616;&#29616;&#23454;&#19990;&#30028;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification. (arXiv:2308.02182v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02182
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#29615;&#22659;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;DL&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#38543;&#26102;&#38388;&#19981;&#21487;&#36991;&#20813;&#22320;&#19979;&#38477;&#12290;&#20165;&#20165;&#23545;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#21482;&#33021;&#37096;&#20998;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25163;&#21160;&#35843;&#25972;&#27169;&#22411;&#26550;&#26500;&#20197;&#28385;&#36275;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26399;&#26395;&#32791;&#26102;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;AutoML4ETC&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#20197;&#36827;&#34892;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#24378;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#19987;&#38376;&#38024;&#23545;&#20351;&#29992;&#25968;&#25454;&#21253;&#22836;&#23383;&#33410;&#36827;&#34892;&#36817;&#23454;&#26102;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#25628;&#32034;&#31354;&#38388;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AutoML4ETC&#29983;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13421</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#19977;&#20010;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#20043;&#19968;&#26469;&#23398;&#20064;&#65292;&#20998;&#21035;&#31216;&#20026;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#12290;&#36825;&#19977;&#31181;&#33539;&#24335;&#37117;&#26159;&#20026;&#20102;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21363;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#28966;&#28857;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#8220;&#36873;&#25321;&#8221;&#36755;&#20837;&#20013;&#30340;&#27491;&#30830;&#8220;&#29255;&#27573;&#8221;&#65292;&#21644;&#19968;&#20010;&#8220;&#20998;&#31867;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#36873;&#23450;&#30340;&#29255;&#27573;&#22788;&#29702;&#25104;&#30446;&#26631;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#21160;&#24577;&#21644;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36825;&#20123;&#33539;&#24335;&#23398;&#20064;&#30340;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#35299;&#37322;&#20026;&#22312;&#28966;&#28857;&#27169;&#22411;&#22266;&#23450;&#26102;&#65292;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#25152;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#36825;&#20123;&#33539;&#24335;&#65292;&#24182;&#25512;&#23548;&#20986;&#26799;&#24230;&#27969;&#19979;&#21442;&#25968;&#36712;&#36857;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#22312;&#36719;&#27880;&#24847;&#21147;&#25439;&#22833;&#19979;&#65292;&#28966;&#28857;&#27169;&#22411;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#24555;&#36895;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
&lt;/p&gt;</description></item><item><title>Espaloma-0.3.0&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#65292;&#36890;&#36807;&#33021;&#37327;&#21644;&#21147;&#30340;&#25311;&#21512;&#32435;&#20837;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07085</link><description>&lt;p&gt;
Espaloma-0.3.0: &#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#21450;&#20854;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Espaloma-0.3.0: Machine-learned molecular mechanics force field for the simulation of protein-ligand systems and beyond. (arXiv:2307.07085v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07085
&lt;/p&gt;
&lt;p&gt;
Espaloma-0.3.0&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#65292;&#36890;&#36807;&#33021;&#37327;&#21644;&#21147;&#30340;&#25311;&#21512;&#32435;&#20837;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21147;&#23398;&#65288;MM&#65289;&#21147;&#22330;&#26159;&#36890;&#36807;&#31616;&#21333;&#30340;&#19968;&#23545;&#19968;&#21644;&#22810;&#39033;&#24335;&#39033;&#26469;&#34920;&#24449;&#20998;&#23376;&#31995;&#32479;&#33021;&#37327;&#26223;&#35266;&#30340;&#27169;&#22411;&#12290;&#20256;&#32479;&#19978;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#20154;&#24037;&#19987;&#23478;&#31574;&#21010;&#12289;&#19981;&#28789;&#27963;&#19988;&#38590;&#20197;&#25193;&#23637;&#30340;&#31163;&#25955;&#21270;&#21270;&#23398;&#21442;&#25968;&#36171;&#20540;&#35268;&#21017;&#65292;&#21363;&#21407;&#23376;&#25110;&#20215;&#24577;&#31867;&#22411;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27492;&#36807;&#31243;&#65292;&#24182;&#20351;&#21442;&#25968;&#21270;&#26041;&#26696;&#33021;&#22815;&#30452;&#25509;&#20174;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#25110;&#20957;&#32858;&#30456;&#25968;&#25454;&#20013;&#20197;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#20998;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#34920;&#31034;&#20986;&#20102;&#24040;&#22823;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#33021;&#37327;&#21644;&#21147;&#30340;&#36866;&#24212;&#24615;&#25311;&#21512;&#30452;&#25509;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#65292;&#25193;&#23637;&#20102;Espaloma&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#21147;&#22330;&#26500;&#24314;&#26041;&#27861;&#12290;&#22522;&#20110;OpenMM SPICE&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#19982;&#29983;&#29289;&#20998;&#23376;&#24314;&#27169;&#24191;&#27867;&#30456;&#20851;&#30340;&#21270;&#23398;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#23567;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;RNA&#12290;&#26368;&#32456;&#24471;&#21040;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular mechanics (MM) force fields -- the models that characterize the energy landscape of molecular systems via simple pairwise and polynomial terms -- have traditionally relied on human expert-curated, inflexible, and poorly extensible discrete chemical parameter assignment rules, namely atom or valence types. Recently, there has been significant interest in using graph neural networks to replace this process, while enabling the parametrization scheme to be learned in an end-to-end differentiable manner directly from quantum chemical calculations or condensed-phase data. In this paper, we extend the Espaloma end-to-end differentiable force field construction approach by incorporating both energy and force fitting directly to quantum chemical data into the training process. Building on the OpenMM SPICE dataset, we curate a dataset containing chemical spaces highly relevant to the broad interest of biomolecular modeling, covering small molecules, proteins, and RNA. The resulting for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;ChatGPT&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#19968;&#20010;&#39069;&#22806;&#30340;&#31354;&#26684;&#25104;&#20026;&#20102;&#35268;&#36991;&#26816;&#27979;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2307.02599</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#31354;&#26684;&#32469;&#36807;ChatGPT&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evade ChatGPT Detectors via A Single Space. (arXiv:2307.02599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;ChatGPT&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#19968;&#20010;&#39069;&#22806;&#30340;&#31354;&#26684;&#25104;&#20026;&#20102;&#35268;&#36991;&#26816;&#27979;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#31038;&#20250;&#20215;&#20540;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;AI&#29983;&#25104;&#20869;&#23481;&#28389;&#29992;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#26816;&#27979;&#20986;&#20869;&#23481;&#26159;&#30001;ChatGPT&#29983;&#25104;&#36824;&#26159;&#20154;&#31867;&#29983;&#25104;&#30340;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#26159;&#24314;&#31435;&#22312;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#30340;&#20551;&#35774;&#19978;&#30340;&#12290;&#36825;&#20123;&#24046;&#36317;&#36890;&#24120;&#26159;&#36890;&#36807;&#32479;&#35745;&#20449;&#24687;&#25110;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36136;&#30097;&#20102;&#26816;&#27979;&#22120;&#20013;&#30340;&#20998;&#24067;&#24046;&#36317;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#22320;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#21644;&#39118;&#26684;&#24046;&#36317;&#12290;&#30456;&#21453;&#65292;"&#24494;&#23567;&#30340;&#24046;&#24322;"&#65292;&#22914;&#39069;&#22806;&#30340;&#19968;&#20010;&#31354;&#26684;&#65292;&#22312;&#26816;&#27979;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpaceInfi&#31574;&#30053;&#26469;&#35268;&#36991;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#31574;&#30053;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#26816;&#27979;&#22120;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#20026;&#20160;&#20040;SpaceInfi&#33021;&#25104;&#21151;&#35268;&#36991;&#26816;&#27979;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT brings revolutionary social value but also raises concerns about the misuse of AI-generated content. Consequently, an important question is how to detect whether content is generated by ChatGPT or by human. Existing detectors are built upon the assumption that there are distributional gaps between human-generated and AI-generated content. These gaps are typically identified using statistical information or classifiers. Our research challenges the distributional gap assumption in detectors. We find that detectors do not effectively discriminate the semantic and stylistic gaps between human-generated and AI-generated content. Instead, the "subtle differences", such as an extra space, become crucial for detection. Based on this discovery, we propose the SpaceInfi strategy to evade detection. Experiments demonstrate the effectiveness of this strategy across multiple benchmarks and detectors. We also provide a theoretical explanation for why SpaceInfi is successful in evading perple
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;OCDiGCN&#26469;&#26816;&#27979;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#30340;&#22270;&#32423;&#21035;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00527</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#19982;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks based Log Anomaly Detection and Explanation. (arXiv:2307.00527v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00527
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;OCDiGCN&#26469;&#26816;&#27979;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#30340;&#22270;&#32423;&#21035;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26085;&#24535;&#34987;&#24191;&#27867;&#29992;&#20110;&#35760;&#24405;&#39640;&#31185;&#25216;&#31995;&#32479;&#30340;&#29366;&#24577;&#65292;&#22240;&#27492;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#30417;&#25511;&#36825;&#20123;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#23558;&#26085;&#24535;&#20107;&#20214;&#35745;&#25968;&#30697;&#38453;&#25110;&#26085;&#24535;&#20107;&#20214;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#21033;&#29992;&#26085;&#24535;&#20107;&#20214;&#20043;&#38388;&#30340;&#23450;&#37327;&#21644;/&#25110;&#39034;&#24207;&#20851;&#31995;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#20165;&#32771;&#34385;&#23450;&#37327;&#25110;&#39034;&#24207;&#20851;&#31995;&#21487;&#33021;&#23548;&#33268;&#26816;&#27979;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;Logs2Graphs&#65292;&#23427;&#39318;&#20808;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#28982;&#21518;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;One-Class Digraph Inception Convolutional Networks&#65288;OCDiGCN&#65289;&#65292;&#29992;&#20110;&#22312;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#26816;&#27979;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#12290;&#36890;&#36807;&#23558;&#22270;&#34920;&#31034;&#19982;&#23646;&#24615;&#34920;&#31034;&#32806;&#21512;&#36215;&#26469;&#65292;&#22312;&#22270;&#32423;&#21035;&#19978;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event logs are widely used to record the status of high-tech systems, making log anomaly detection important for monitoring those systems. Most existing log anomaly detection methods take a log event count matrix or log event sequences as input, exploiting quantitative and/or sequential relationships between log events to detect anomalies. Unfortunately, only considering quantitative or sequential relationships may result in low detection accuracy. To alleviate this problem, we propose a graph-based method for unsupervised log anomaly detection, dubbed Logs2Graphs, which first converts event logs into attributed, directed, and weighted graphs, and then leverages graph neural networks to perform graph-level anomaly detection. Specifically, we introduce One-Class Digraph Inception Convolutional Networks, abbreviated as OCDiGCN, a novel graph neural network model for detecting graph-level anomalies in a collection of attributed, directed, and weighted graphs. By coupling the graph represe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;&#21069;&#20307;-&#24322;&#24120;&#26816;&#27979;&#65288;PoA&#26816;&#27979;&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#19981;&#21516;&#65292;PoA&#26816;&#27979;&#26088;&#22312;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;17&#20010;&#22522;&#20934;&#32447;&#21644;3&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15489</link><description>&lt;p&gt;
&#38024;&#23545;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#21069;&#20307;
&lt;/p&gt;
&lt;p&gt;
Precursor-of-Anomaly Detection for Irregular Time Series. (arXiv:2306.15489v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;&#21069;&#20307;-&#24322;&#24120;&#26816;&#27979;&#65288;PoA&#26816;&#27979;&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#19981;&#21516;&#65292;PoA&#26816;&#27979;&#26088;&#22312;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;17&#20010;&#22522;&#20934;&#32447;&#21644;3&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#26088;&#22312;&#35782;&#21035;&#24847;&#22806;&#30340;&#27169;&#24335;&#25110;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#65292;&#23588;&#20854;&#26159;&#22312;&#37329;&#34701;&#12289;&#21046;&#36896;&#12289;&#32593;&#32476;&#23433;&#20840;&#31561;&#24212;&#29992;&#20013;&#12290;&#34429;&#28982;&#24322;&#24120;&#26816;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#21069;&#20307;-&#24322;&#24120;&#8221;&#65288;PoA&#65289;&#26816;&#27979;&#12290;&#19982;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#19981;&#21516;&#65292;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#20391;&#37325;&#20110;&#30830;&#23450;&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#35266;&#27979;&#20540;&#26159;&#21542;&#20026;&#24322;&#24120;&#65292;&#32780;PoA&#26816;&#27979;&#26088;&#22312;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;17&#20010;&#22522;&#20934;&#32447;&#21644;3&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#35268;&#21017;&#21644;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is an important field that aims to identify unexpected patterns or data points, and it is closely related to many real-world problems, particularly to applications in finance, manufacturing, cyber security, and so on. While anomaly detection has been studied extensively in various fields, detecting future anomalies before they occur remains an unexplored territory. In this paper, we present a novel type of anomaly detection, called \emph{\textbf{P}recursor-of-\textbf{A}nomaly} (PoA) detection. Unlike conventional anomaly detection, which focuses on determining whether a given time series observation is an anomaly or not, PoA detection aims to detect future anomalies before they happen. To solve both problems at the same time, we present a neural controlled differential equation-based neural network and its multi-task learning algorithm. We conduct experiments using 17 baselines and 3 datasets, including regular and irregular time series, and demonstrate that our prese
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#35821;&#20041;&#24863;&#30693;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#65292;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.14941</link><description>&lt;p&gt;
SIMF: &#33258;&#21160;&#39550;&#39542;&#30340;&#35821;&#20041;&#24863;&#30693;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SIMF: Semantics-aware Interactive Motion Forecasting for Autonomous Driving. (arXiv:2306.14941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#35821;&#20041;&#24863;&#30693;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#65292;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#23545;&#21608;&#22260;&#22810;&#20010;&#34892;&#20026;&#20307;&#65288;&#34892;&#20154;&#21644;&#36710;&#36742;&#65289;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#65292;&#20197;&#20570;&#20986;&#26368;&#20248;&#23548;&#33322;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#34892;&#20026;&#20307;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#65292;&#24182;&#26410;&#33021;&#25429;&#25417;&#21040;&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#19982;&#22330;&#26223;&#20013;&#34892;&#20026;&#20307;&#25968;&#37327;&#22686;&#21152;&#30456;&#20851;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#27431;&#27663;&#36317;&#31163;&#26469;&#21098;&#26525;&#36828;&#31163;&#30340;&#34892;&#20026;&#20307;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#26080;&#27861;&#36873;&#25321;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#24182;&#20934;&#30830;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#20197;&#21450;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#26041;&#27861;&#65292;&#23558;&#20854;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20256;&#36882;&#65292;&#20197;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles require motion forecasting of their surrounding multi-agents (pedestrians and vehicles) to make optimal decisions for navigation. The existing methods focus on techniques to utilize the positions and velocities of these agents and fail to capture semantic information from the scene. Moreover, to mitigate the increase in computational complexity associated with the number of agents in the scene, some works leverage Euclidean distance to prune far-away agents. However, distance-based metric alone is insufficient to select relevant agents and accurately perform their predictions. To resolve these issues, we propose Semantics-aware Interactive Motion Forecasting (SIMF) method to capture semantics along with spatial information, and optimally select relevant agents for motion prediction. Specifically, we achieve this by implementing a semantic-aware selection of relevant agents from the scene and passing them through an attention mechanism to extract global encodings. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#29076;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#22686;&#24378;&#20854;&#20934;&#30830;&#24615;&#21644;&#25511;&#21046;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13345</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30693;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;&#35745;&#31639;&#29076;&#28857;&#24182;&#25511;&#21046;&#19981;&#30830;&#23450;&#24615;&#21644;&#26368;&#20248;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
A physics-informed AI method for calculating melting points with uncertainty control and optimal sampling. (arXiv:2306.13345v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#29076;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#22686;&#24378;&#20854;&#20934;&#30830;&#24615;&#21644;&#25511;&#21046;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;NPT&#38598;&#21512;&#20013;&#30340;&#20849;&#23384;&#27169;&#25311;&#33258;&#21160;&#35745;&#31639;&#29076;&#28857;&#12290;&#32473;&#23450;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20915;&#23450;&#36827;&#34892;&#27169;&#25311;&#30340;&#21407;&#23376;&#25968;&#37327;&#21644;&#28201;&#24230;&#65292;&#24182;&#22522;&#20110;&#25910;&#38598;&#30340;&#25968;&#25454;&#39044;&#27979;&#29076;&#28857;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#26356;&#22810;&#30340;&#25968;&#25454;&#31995;&#32479;&#22320;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#22266;&#28082;&#20849;&#23384;&#28436;&#21270;&#30340;&#29289;&#29702;&#27169;&#22411;&#32435;&#20837;AI&#26041;&#27861;&#65292;&#22686;&#24378;&#20854;&#20934;&#30830;&#24615;&#65292;&#24182;&#23454;&#29616;&#26377;&#25928;&#38477;&#20302;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#20248;&#20915;&#31574;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#22823;&#32422;20&#20010;&#25991;&#29486;&#20013;&#30340;&#29076;&#28857;&#35745;&#31639;&#36827;&#34892;&#27604;&#36739;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#32422;&#19977;&#20998;&#20043;&#19968;&#30340;&#24773;&#20917;&#19979;&#35266;&#23519;&#21040;&#26174;&#33879;&#20559;&#24046;&#65292;&#31361;&#26174;&#20986;&#26448;&#26009;&#24615;&#36136;&#35745;&#31639;&#38656;&#35201;&#20934;&#30830;&#21487;&#38752;&#30340;&#22522;&#20110;AI&#30340;&#31639;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an artificial intelligence (AI) method for automatically computing the melting point based on coexistence simulations in the NPT ensemble. Given the interatomic interaction model, the method makes decisions regarding the number of atoms and temperature at which to conduct simulations, and based on the collected data predicts the melting point along with the uncertainty, which can be systematically improved with more data. We demonstrate how incorporating physical models of the solid-liquid coexistence evolution enhances the AI method's accuracy and enables optimal decision-making to effectively reduce predictive uncertainty. To validate our approach, we compare our results with approximately 20 melting point calculations from the literature. Remarkably, we observe significant deviations in about one-third of the cases, underscoring the need for accurate and reliable AI-based algorithms for materials property calculations.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#24863;&#30693;&#27169;&#22359;&#65292;&#31216;&#20026;&#35821;&#20041;&#20960;&#20309;&#34920;&#31034;&#65288;SGR&#65289;&#65292;&#35813;&#27169;&#22359;&#32467;&#21512;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;3D&#31354;&#38388;&#25512;&#29702;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.10474</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#36890;&#29992;&#35821;&#20041;&#20960;&#20309;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Universal Semantic-Geometric Representation for Robotic Manipulation. (arXiv:2306.10474v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#24863;&#30693;&#27169;&#22359;&#65292;&#31216;&#20026;&#35821;&#20041;&#20960;&#20309;&#34920;&#31034;&#65288;SGR&#65289;&#65292;&#35813;&#27169;&#22359;&#32467;&#21512;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;3D&#31354;&#38388;&#25512;&#29702;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22312;&#24863;&#30693;&#21644;&#19982;&#19990;&#30028;&#20114;&#21160;&#26102; heavily relies &#20256;&#24863;&#22120;&#65292;&#29305;&#21035;&#26159;RGB&#21644;&#28145;&#24230;&#30456;&#26426;&#12290;RGB&#30456;&#26426;&#35760;&#24405;&#20102;&#20855;&#26377;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#30340;2D&#22270;&#20687;&#65292;&#20294;&#32570;&#20047;&#31934;&#30830;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28145;&#24230;&#30456;&#26426;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;3D&#20960;&#20309;&#25968;&#25454;&#65292;&#20294;&#25429;&#25417;&#21040;&#30340;&#35821;&#20041;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25972;&#21512;&#20004;&#31181;&#27169;&#24577;&#23545;&#20110;&#23398;&#20064;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20854;&#20013;&#19968;&#31181;&#27169;&#24577;&#19978;&#65292;&#24182;&#24573;&#30053;&#20102;&#32467;&#21512;&#20004;&#32773;&#30340;&#22909;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textbf{&#35821;&#20041;&#20960;&#20309;&#34920;&#31034;} (\textbf{SGR})$&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#30340;&#36890;&#29992;&#24863;&#30693;&#27169;&#22359;&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25215;&#32487;&#20102;3D&#31354;&#38388;&#25512;&#29702;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SGR&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25104;&#21151;&#23436;&#25104;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#65292;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots rely heavily on sensors, especially RGB and depth cameras, to perceive and interact with the world. RGB cameras record 2D images with rich semantic information while missing precise spatial information. On the other side, depth cameras offer critical 3D geometry data but capture limited semantics. Therefore, integrating both modalities is crucial for learning representations for robotic perception and control. However, current research predominantly focuses on only one of these modalities, neglecting the benefits of incorporating both. To this end, we present $\textbf{Semantic-Geometric Representation} (\textbf{SGR})$, a universal perception module for robotics that leverages the rich semantic information of large-scale pre-trained 2D models and inherits the merits of 3D spatial reasoning. Our experiments demonstrate that SGR empowers the agent to successfully complete a diverse range of simulated and real-world robotic manipulation tasks, outperforming state-of-the-art methods 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05880</link><description>&lt;p&gt;
&#22522;&#20110;Implicit Neural Representations&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#29992;&#20110;&#25554;&#20540;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26102;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;Implicit Neural Representations (INR)&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25110;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#35843;&#21046;INR&#21442;&#25968;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#35265;&#26679;&#26412;&#21644;&#26102;&#38388;&#31383;&#21475;&#31227;&#20301;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#26041;&#38754;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.02865</link><description>&lt;p&gt;
&#25235;&#20303;&#24847;&#22806;&#25910;&#33719;&#65306;&#22312;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#20215;&#20540;(arXiv:2306.02865v2 [cs.LG]&#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39640;&#36136;&#37327;&#30340; Q &#20540;&#20989;&#25968;&#22312;&#35768;&#22810;&#29616;&#20195;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (RL) &#31639;&#27861;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#35299;&#20915;&#37319;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#21644;&#31163;&#32447;&#23398;&#20064;&#25152;&#23548;&#33268;&#30340;&#20540;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#19982;&#36825;&#31181;&#26222;&#36941;&#35266;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040; Q &#20540;&#22312; RL &#35757;&#32451;&#36807;&#31243;&#30340;&#21518;&#26399;&#23454;&#38469;&#19978;&#34987;&#20302;&#20272;&#20102;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36125;&#23572;&#26364;&#26356;&#26032;&#20013;&#65292;&#24403;&#21069;&#31574;&#30053;&#20351;&#29992;&#27604;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#26356;&#20248;&#30340;&#21160;&#20316;&#26679;&#26412;&#24046;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20010;&#38271;&#26399;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#21487;&#33021;&#38459;&#30861;&#20102;&#31574;&#30053;&#23398;&#20064;&#65292;&#38477;&#20302;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#22312;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#30340;&#21516;&#26102;&#65292;&#32467;&#21512;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#21033;&#29992;&#21644;&#25506;&#32034; (BEE) &#25805;&#20316;&#31526;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21382;&#21490;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#21160;&#20316;&#21644;&#24403;&#21069;&#31574;&#30053;&#29983;&#25104;&#30340;&#21160;&#20316;&#26469;&#26356;&#26032; Q &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;CALVIN&#22522;&#20934;&#27979;&#35797;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.19075</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#27169;&#20223;&#23398;&#20064;&#19982;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data. (arXiv:2305.19075v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;CALVIN&#22522;&#20934;&#27979;&#35797;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#29702;&#35299;&#21644;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#25805;&#20316;&#29289;&#20307;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#35821;&#35328;&#26465;&#20214;&#26041;&#27861;&#22312;&#29087;&#24713;&#30340;&#29615;&#22659;&#20013;&#22788;&#29702;&#20219;&#21153;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#35774;&#32622;&#26041;&#38754;&#36935;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#38646;-shot&#35774;&#32622;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;CALVIN&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#36229;&#36807;&#20102;&#20197;&#21069;&#25253;&#21578;&#30340;&#24471;&#20998;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;-shot&#22810;&#29615;&#22659;&#35774;&#32622;&#20013;&#12290;&#23436;&#25104;&#20219;&#21153;&#30340;&#24179;&#22343;&#38271;&#24230;&#20026;...
&lt;/p&gt;
&lt;p&gt;
The growing interest in language-conditioned robot manipulation aims to develop robots capable of understanding and executing complex tasks, with the objective of enabling robots to interpret language commands and manipulate objects accordingly. While language-conditioned approaches demonstrate impressive capabilities for addressing tasks in familiar environments, they encounter limitations in adapting to unfamiliar environment settings. In this study, we propose a general-purpose, language-conditioned approach that combines base skill priors and imitation learning under unstructured data to enhance the algorithm's generalization in adapting to unfamiliar environments. We assess our model's performance in both simulated and real-world environments using a zero-shot setting. In the simulated environment, the proposed approach surpasses previously reported scores for CALVIN benchmark, especially in the challenging Zero-Shot Multi-Environment setting. The average completed task length, in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#36335;&#23398;&#20064;&#36755;&#20837;&#20302;&#32500;&#24230;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25511;&#21046;&#20102;&#35268;&#24459;&#24615;&#65292;&#24182;&#21033;&#29992;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.19008</link><description>&lt;p&gt;
&#23398;&#20064;&#29305;&#24449;&#20013;&#30340;&#29942;&#39048;&#32467;&#26500;&#65306;&#20302;&#32500;&#24230;&#19982;&#35268;&#24459;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff. (arXiv:2305.19008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#36335;&#23398;&#20064;&#36755;&#20837;&#20302;&#32500;&#24230;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25511;&#21046;&#20102;&#35268;&#24459;&#24615;&#65292;&#24182;&#21033;&#29992;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#22823;&#28145;&#24230;$L$&#21644;$L_{2}$&#27491;&#21017;&#21270;&#30340;DNN&#20559;&#21521;&#20110;&#23398;&#20064;&#36755;&#20837;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#26368;&#23567;&#21270;&#23398;&#20064;&#20989;&#25968;$f$&#30340;&#31209;$R^{(0)}(f)$&#30340;&#27010;&#24565;&#65292;&#20854;&#34987;&#25512;&#27979;&#20026;&#29942;&#39048;&#31209;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;&#36825;&#20010;&#32467;&#26524;&#30340;&#26377;&#38480;&#28145;&#24230;&#20462;&#27491;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#24230;&#37327;$R^{(1)}$&#30340;&#35268;&#24459;&#24615;&#65292;&#23427;&#25511;&#21046;&#20102;&#38597;&#21487;&#27604;&#30697;&#38453;$\left|Jf(x)\right|_{+}$&#30340;&#20266;&#34892;&#21015;&#24335;&#24182;&#22312;&#32452;&#21512;&#21644;&#21152;&#27861;&#19979;&#26159;&#27425;&#21487;&#21152;&#30340;&#12290;&#36825;&#20351;&#24471;&#32593;&#32476;&#21487;&#20197;&#22312;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#20174;&#32780;&#23398;&#20064;&#8220;&#27491;&#30830;&#8221;&#30340;&#20869;&#37096;&#23610;&#23544;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22823;&#23398;&#20064;&#36895;&#29575;&#22914;&#20309;&#25511;&#21046;&#23398;&#20064;&#20989;&#25968;&#30340;&#35268;&#24459;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#22312;$L\to\infty$&#26102;&#22312;&#23398;&#20064;&#29305;&#24449;&#20013;&#30340;&#29468;&#24819;&#65306;&#23545;&#20110;&#22823;&#28145;&#24230;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#38544;&#34255;&#34920;&#31034;&#37117;&#38598;&#20013;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Previous work has shown that DNNs with large depth $L$ and $L_{2}$-regularization are biased towards learning low-dimensional representations of the inputs, which can be interpreted as minimizing a notion of rank $R^{(0)}(f)$ of the learned function $f$, conjectured to be the Bottleneck rank. We compute finite depth corrections to this result, revealing a measure $R^{(1)}$ of regularity which bounds the pseudo-determinant of the Jacobian $\left|Jf(x)\right|_{+}$ and is subadditive under composition and addition. This formalizes a balance between learning low-dimensional representations and minimizing complexity/irregularity in the feature maps, allowing the network to learn the `right' inner dimension. We also show how large learning rates also control the regularity of the learned function. Finally, we use these theoretical tools to prove the conjectured bottleneck structure in the learned features as $L\to\infty$: for large depths, almost all hidden representations concentrates aroun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14707</link><description>&lt;p&gt;
&#23398;&#29983;&#36229;&#36234;&#20102;&#22823;&#24072;&#65306;&#22522;&#20110;GPT-3&#30340;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#26041;&#27861;&#30340;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
The student becomes the master: Matching GPT3 on Scientific Factual Error Correction. (arXiv:2305.14707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21019;&#24314;&#38169;&#35823;&#26657;&#27491;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#26497;&#39640;&#65292;&#22823;&#22810;&#25968;&#20107;&#23454;&#20027;&#24352;&#26657;&#27491;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#39564;&#35777;&#27169;&#22411;&#26469;&#25351;&#23548;&#26657;&#27491;&#36807;&#31243;&#12290;&#36825;&#23548;&#33268;&#22312;&#31185;&#23398;&#20107;&#23454;&#26657;&#27491;&#31561;&#39046;&#22495;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#22909;&#30340;&#39564;&#35777;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#23384;&#22312;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#19988;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#20294;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021; - &#22312;SciFact&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;94&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#22312;SciFact-Open&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;62.5&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#20998;&#21035;&#27604;&#19979;&#19968;&#20010;&#26368;&#22909;&#30340;&#26041;&#27861;&#39640;&#20986;0.5&#65285;&#21644;1.50&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#20013;&#30340;&#25552;&#31034;&#21151;&#33021;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#21019;&#24314;&#19968;&#20010;&#20016;&#23500;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#23436;&#20840;&#30417;&#30563;&#30340;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#32416;&#27491;&#20027;&#24352;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29992;&#20110;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;LLM&#30456;&#31454;&#20105;&#65292;&#35777;&#26126;&#20102;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#35757;&#32451;&#25552;&#39640;&#31185;&#23398;&#20027;&#24352;&#26657;&#27491;&#20219;&#21153;&#24615;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work, we introduce a claim correction system that makes no domain assumptions and does not require a verifier but is able to outperform existing methods by an order of magnitude -- achieving 94% correction accuracy on the SciFact dataset, and 62.5% on the SciFact-Open dataset, compared to the next best methods 0.5% and 1.50% respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method is competitive with the very LLM that was
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#21516;&#20041;&#35789;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#21516;&#20041;&#35789;&#36317;&#31163;&#21644;&#22122;&#22768;&#25200;&#21160;&#27491;&#21017;&#21270;&#39033;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#30340;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.13066</link><description>&lt;p&gt;
&#22522;&#20110;&#35789;&#20856;&#30340;&#21516;&#20041;&#35789;&#27867;&#21270;&#30340;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Biomedical Named Entity Recognition via Dictionary-based Synonym Generalization. (arXiv:2305.13066v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#21516;&#20041;&#35789;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#21516;&#20041;&#35789;&#36317;&#31163;&#21644;&#22122;&#22768;&#25200;&#21160;&#27491;&#21017;&#21270;&#39033;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#20043;&#19968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#30417;&#30563;/&#38388;&#25509;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#24037;&#20316;&#12290;&#20026;&#20102;&#20943;&#36731;&#20154;&#21147;&#21171;&#21160;&#30340;&#38656;&#27714;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#65292;&#20165;&#26681;&#25454;&#32473;&#23450;&#30340;&#35789;&#20856;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#65292;&#23427;&#20204;&#38590;&#20197;&#35782;&#21035;&#32473;&#23450;&#35789;&#20856;&#20013;&#26410;&#21015;&#20986;&#30340;&#27010;&#24565;&#21516;&#20041;&#35789;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#21516;&#20041;&#35789;&#27867;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#20041;&#35789;&#27867;&#21270;&#65288;SynGen&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#36328;&#24230;&#30340;&#39044;&#27979;&#35782;&#21035;&#36755;&#20837;&#25991;&#26412;&#20013;&#21253;&#21547;&#30340;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SynGen&#24341;&#20837;&#20102;&#20004;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#21363;&#65288;1&#65289;&#21516;&#20041;&#35789;&#36317;&#31163;&#27491;&#21017;&#21270;&#39033;&#65307;&#21644;&#65288;2&#65289;&#22122;&#22768;&#25200;&#21160;&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Biomedical named entity recognition is one of the core tasks in biomedical natural language processing (BioNLP). To tackle this task, numerous supervised/distantly supervised approaches have been proposed. Despite their remarkable success, these approaches inescapably demand laborious human effort. To alleviate the need of human effort, dictionary-based approaches have been proposed to extract named entities simply based on a given dictionary. However, one downside of existing dictionary-based approaches is that they are challenged to identify concept synonyms that are not listed in the given dictionary, which we refer as the synonym generalization problem. In this study, we propose a novel Synonym Generalization (SynGen) framework that recognizes the biomedical concepts contained in the input text using span-based predictions. In particular, SynGen introduces two regularization terms, namely, (1) a synonym distance regularizer; and (2) a noise perturbation regularizer, to minimize the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;CACD&#65289;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#35299;&#20915;&#20102;&#22240;&#26524;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21464;&#38271;&#20250;&#35805;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.02615</link><description>&lt;p&gt;
&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#25512;&#29702;&#65306;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Affective Reasoning at Utterance Level in Conversations: A Causal Discovery Approach. (arXiv:2305.02615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;CACD&#65289;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#35299;&#20915;&#20102;&#22240;&#26524;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21464;&#38271;&#20250;&#35805;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25512;&#29702;&#20219;&#21153;&#26159;&#21253;&#25324;&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#12289;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25277;&#21462;&#21644;&#24773;&#24863;-&#21407;&#22240;&#36328;&#24230;&#35782;&#21035;&#22312;&#20869;&#30340;&#19968;&#32452;&#26032;&#20852;&#30340;&#22522;&#20110;&#24773;&#24863;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#20551;&#35774;&#34920;&#38754;&#20851;&#31995;&#26102;&#24573;&#30053;&#20102;&#22522;&#26412;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#22240;&#20026;&#39592;&#26550;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#65288;CACD&#65289;&#26041;&#27861;&#12290;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#26469;&#21457;&#29616;&#20250;&#35805;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;CACD&#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;i&#65289;&#20026;&#21464;&#38271;&#20250;&#35805;&#20013;&#30340;&#25152;&#26377;&#35805;&#35821;&#24314;&#31435;&#19968;&#20010;&#20013;&#24515;&#21270;&#30340;&#21333;&#19968;&#22270;&#33410;&#28857;&#22240;&#26524;&#39592;&#26550;&#65307;&#65288;ii&#65289;&#22240;&#26524;&#33258;&#32534;&#30721;&#22120;&#65288;CAE&#65289;&#36890;&#36807;&#29983;&#25104;&#38544;&#21547;&#21407;&#22240;&#21644;&#24050;&#30693;&#26174;&#24335;&#21407;&#22240;&#26469;&#20462;&#27491;&#39592;&#26550;&#65292;&#20174;&#32780;&#20135;&#29983;&#22240;&#26524;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The affective reasoning task is a set of emerging affect-based tasks in conversation, including Emotion Recognition in Conversation (ERC),Emotion-Cause Pair Extraction (ECPE), and Emotion-Cause Span Recognition (ECSR). Existing methods make various assumptions on the apparent relationship while neglecting the essential causal model due to the nonuniqueness of skeletons and unobservability of implicit causes. This paper settled down the above two problems and further proposed Conversational Affective Causal Discovery (CACD). It is a novel causal discovery method showing how to discover causal relationships in a conversation via designing a common skeleton and generating a substitute for implicit causes. CACD contains two steps: (i) building a common centering one graph node causal skeleton for all utterances in variable-length conversations; (ii) Causal Auto-Encoder (CAE) correcting the skeleton to yield causal representation through generated implicit causes and known explicit causes. 
&lt;/p&gt;</description></item><item><title>ReelFramer&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#31038;&#20132;&#23186;&#20307;&#20849;&#21516;&#21019;&#20316;&#26032;&#38395;&#29255;&#27573;&#12290;&#23427;&#21487;&#20197;&#24110;&#21161;&#35760;&#32773;&#25506;&#32034;&#19968;&#20010;&#25925;&#20107;&#30340;&#22810;&#31181;&#21465;&#20107;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#33050;&#26412;&#12289;&#35282;&#33394;&#26495;&#21644;&#25925;&#20107;&#26495;&#12290;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#35813;&#31995;&#32479;&#22823;&#22823;&#20943;&#36731;&#20102;&#23558;&#19968;&#31687;&#20070;&#38754;&#25253;&#36947;&#36716;&#21270;&#20026;&#26032;&#38395;&#29255;&#27573;&#30340;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2304.09653</link><description>&lt;p&gt;
ReelFramer&#65306;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#31038;&#20132;&#23186;&#20307;&#20849;&#21516;&#21019;&#20316;&#26032;&#38395;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
ReelFramer: Co-creating News Reels on Social Media with Generative AI. (arXiv:2304.09653v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09653
&lt;/p&gt;
&lt;p&gt;
ReelFramer&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#31038;&#20132;&#23186;&#20307;&#20849;&#21516;&#21019;&#20316;&#26032;&#38395;&#29255;&#27573;&#12290;&#23427;&#21487;&#20197;&#24110;&#21161;&#35760;&#32773;&#25506;&#32034;&#19968;&#20010;&#25925;&#20107;&#30340;&#22810;&#31181;&#21465;&#20107;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#33050;&#26412;&#12289;&#35282;&#33394;&#26495;&#21644;&#25925;&#20107;&#26495;&#12290;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#35813;&#31995;&#32479;&#22823;&#22823;&#20943;&#36731;&#20102;&#23558;&#19968;&#31687;&#20070;&#38754;&#25253;&#36947;&#36716;&#21270;&#20026;&#26032;&#38395;&#29255;&#27573;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#30701;&#35270;&#39057;&#26159;&#35768;&#22810;&#24180;&#36731;&#20154;&#21457;&#29616;&#21644;&#28040;&#36153;&#20869;&#23481;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#26032;&#38395;&#26426;&#26500;&#24076;&#26395;&#36890;&#36807;&#26032;&#38395;&#29255;&#27573;&#25509;&#35302;&#21463;&#20247;&#65292;&#20294;&#30446;&#21069;&#38590;&#20197;&#23558;&#20256;&#32479;&#30340;&#26032;&#38395;&#25253;&#36947;&#26684;&#24335;&#36716;&#21270;&#20026;&#19982;&#24179;&#21488;&#39118;&#26684;&#30456;&#21305;&#37197;&#30340;&#30701;&#23567;&#26377;&#36259;&#35270;&#39057;&#12290;&#26377;&#22810;&#31181;&#26041;&#27861;&#21487;&#20197;&#22260;&#32469;&#26032;&#38395;&#20107;&#20214;&#26500;&#24314;&#29255;&#27573;&#24335;&#21465;&#20107;&#65292;&#32780;&#36873;&#23450;&#20854;&#20013;&#19968;&#31181;&#21017;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19981;&#21516;&#30340;&#26032;&#38395;&#25925;&#20107;&#38656;&#35201;&#19981;&#21516;&#30340;&#21465;&#36848;&#26694;&#26550;&#65292;&#24182;&#38656;&#35201;&#22312;&#23089;&#20048;&#24615;&#21644;&#20449;&#24687;&#37327;&#20043;&#38388;&#36798;&#21040;&#19981;&#21516;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReelFramer&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#26469;&#24110;&#21161;&#35760;&#32773;&#25506;&#32034;&#19968;&#20010;&#25925;&#20107;&#30340;&#22810;&#31181;&#21465;&#20107;&#26694;&#26550;&#65292;&#28982;&#21518;&#29983;&#25104;&#20182;&#20204;&#21487;&#20197;&#32534;&#36753;&#21644;&#36845;&#20195;&#30340;&#33050;&#26412;&#12289;&#35282;&#33394;&#26495;&#21644;&#25925;&#20107;&#26495;&#12290;&#22312;&#19968;&#39033;&#30001;&#20116;&#21517;&#26032;&#38395;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#29983;&#21442;&#19982;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#31995;&#32479;&#22823;&#22823;&#20943;&#36731;&#20102;&#23558;&#19968;&#31687;&#20070;&#38754;&#25253;&#36947;&#36716;&#21270;&#20026;&#26032;&#38395;&#29255;&#27573;&#30340;&#36127;&#25285;&#65292;&#24182;&#25506;&#32034;&#21465;&#20107;&#26694;&#26550;&#20197;&#25214;&#21040;&#27491;&#30830;&#30340;&#26694;&#26550;&#36807;&#31243;&#26159;&#38750;&#24120;&#26377;&#24847;&#20041;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short videos on social media are a prime way many young people find and consume content. News outlets would like to reach audiences through news reels, but currently struggle to translate traditional journalistic formats into the short, entertaining videos that match the style of the platform. There are many ways to frame a reel-style narrative around a news story, and selecting one is a challenge. Different news stories call for different framings, and require a different trade-off between entertainment and information. We present a system called ReelFramer that uses text and image generation to help journalists explore multiple narrative framings for a story, then generate scripts, character boards and storyboards they can edit and iterate on. A user study of five graduate students in journalism-related fields found the system greatly eased the burden of transforming a written story into a reel, and that exploring framings to find the right one was a rewarding process.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09716</link><description>&lt;p&gt;
&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#25919;&#31574;&#36845;&#20195;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#20004;&#20010;&#38454;&#27573;: &#23398;&#20064;&#38454;&#27573;&#21644;&#35268;&#21010;&#38454;&#27573;&#12290;&#22312;&#26631;&#20934;MDPs&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#20215;&#20540;&#36845;&#20195;&#25110;&#31574;&#30053;&#36845;&#20195;&#26469;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#20294;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65292;&#20197;&#21069;&#30340;&#23581;&#35797;&#37117;&#26377;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#36845;&#20195;&#21464;&#20307;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
&lt;/p&gt;</description></item><item><title>KD-BIRL&#26159;&#19968;&#31181;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22797;&#26434;&#21644;&#26080;&#38480;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2303.06827</link><description>&lt;p&gt;
&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Kernel Density Bayesian Inverse Reinforcement Learning. (arXiv:2303.06827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06827
&lt;/p&gt;
&lt;p&gt;
KD-BIRL&#26159;&#19968;&#31181;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22797;&#26434;&#21644;&#26080;&#38480;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#34892;&#20026;&#26469;&#25512;&#26029;&#20854;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20294;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#28857;&#20272;&#35745;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#22240;&#20026;&#21487;&#33021;&#26377;&#22810;&#20010;&#20989;&#25968;&#33021;&#22815;&#24456;&#22909;&#22320;&#25551;&#36848;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#37319;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#27169;&#25311;&#20505;&#36873;&#22870;&#21169;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;Q&#20540;&#20989;&#25968;&#20195;&#26367;&#20284;&#28982;&#20989;&#25968;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#21518;&#39564;&#35745;&#31639;&#37327;&#22823;&#65292;&#29702;&#35770;&#20445;&#35777;&#23569;&#65292;&#24182;&#19988;Q&#20540;&#20989;&#25968;&#36890;&#24120;&#23545;&#20284;&#28982;&#20989;&#25968;&#30340;&#36924;&#36817;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;KD-BIRL&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26465;&#20214;&#26680;&#23494;&#24230;&#20272;&#35745;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#22312;&#32463;&#36807;&#25913;&#36827;&#30340;&#22870;&#21169;&#20989;&#25968;&#21442;&#25968;&#21270;&#19979;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning~(IRL) is a powerful framework to infer an agent's reward function by observing its behavior, but IRL algorithms that learn point estimates of the reward function can be misleading because there may be several functions that describe an agent's behavior equally well. A Bayesian approach to IRL models a distribution over candidate reward functions, alleviating the shortcomings of learning a point estimate. However, several Bayesian IRL algorithms use a $Q$-value function in place of the likelihood function. The resulting posterior is computationally intensive to calculate, has few theoretical guarantees, and the $Q$-value function is often a poor approximation for the likelihood. We introduce kernel density Bayesian IRL (KD-BIRL), which uses conditional kernel density estimation to directly approximate the likelihood, providing an efficient framework that, with a modified reward function parameterization, is applicable to environments with complex and infin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;LLMs&#22522;&#20110;MBTI&#27979;&#35797;&#35780;&#20272;&#20154;&#31867;&#20010;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#26080;&#20559;&#20506;&#30340;&#25552;&#31034;&#12289;&#28789;&#27963;&#26597;&#35810;&#21644;&#27491;&#30830;&#24615;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#20351;LLMs&#33021;&#22815;&#28789;&#27963;&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#30340;&#20010;&#24615;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.01248</link><description>&lt;p&gt;
ChatGPT&#33021;&#35780;&#20272;&#20154;&#31867;&#20010;&#24615;&#21527;&#65311;&#19968;&#20010;&#36890;&#29992;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Assess Human Personalities? A General Evaluation Framework. (arXiv:2303.01248v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;LLMs&#22522;&#20110;MBTI&#27979;&#35797;&#35780;&#20272;&#20154;&#31867;&#20010;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#26080;&#20559;&#20506;&#30340;&#25552;&#31034;&#12289;&#28789;&#27963;&#26597;&#35810;&#21644;&#27491;&#30830;&#24615;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#20351;LLMs&#33021;&#22815;&#28789;&#27963;&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#30340;&#20010;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#26159;ChatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#28508;&#22312;&#30340;&#20154;&#31867;&#21270;&#24515;&#29702;&#29305;&#24449;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30740;&#31350;LLMs&#30340;&#34394;&#25311;&#20010;&#24615;&#65292;&#32780;&#24456;&#23569;&#25506;&#32034;&#36890;&#36807;LLMs&#20998;&#26512;&#20154;&#31867;&#20010;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#36808;&#23572;&#26031;&#183;&#24067;&#37324;&#26684;&#26031;&#20154;&#26684;&#31867;&#22411;&#25351;&#26631;&#65288;MBTI&#65289;&#27979;&#35797;&#35780;&#20272;LLMs&#30340;&#20154;&#31867;&#20010;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#38543;&#26426;&#25490;&#21015;MBTI&#38382;&#39064;&#20013;&#30340;&#36873;&#39033;&#26469;&#35774;&#35745;&#26080;&#20559;&#20506;&#30340;&#25552;&#31034;&#65292;&#37319;&#29992;&#24179;&#22343;&#27979;&#35797;&#32467;&#26524;&#26469;&#40723;&#21169;&#26356;&#23458;&#35266;&#30340;&#31572;&#26696;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#35758;&#26367;&#25442;&#38382;&#39064;&#38472;&#36848;&#20013;&#30340;&#20027;&#35821;&#65292;&#23454;&#29616;&#23545;LLMs&#19978;&#19981;&#21516;&#20027;&#20307;&#30340;&#28789;&#27963;&#26597;&#35810;&#21644;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20197;&#27491;&#30830;&#24615;&#35780;&#20272;&#30340;&#26041;&#24335;&#37325;&#26032;&#26500;&#24314;&#38382;&#39064;&#25351;&#20196;&#65292;&#20197;&#20415;&#20419;&#20351;LLMs&#29983;&#25104;&#26356;&#28165;&#26224;&#30340;&#22238;&#24212;&#12290;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#20351;LLMs&#33021;&#22815;&#28789;&#27963;&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#30340;&#20010;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored. Existing works study the virtual personalities of LLMs but rarely explore the possibility of analyzing human personalities via LLMs. This paper presents a generic evaluation framework for LLMs to assess human personalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically, we first devise unbiased prompts by randomly permuting options in MBTI questions and adopt the average testing result to encourage more impartial answer generation. Then, we propose to replace the subject in question statements to enable flexible queries and assessments on different subjects from LLMs. Finally, we re-formulate the question instructions in a manner of correctness evaluation to facilitate LLMs to generate clearer responses. The proposed framework enables LLMs to flexibly assess personalities of different groups of people.
&lt;/p&gt;</description></item><item><title>Orca&#26159;&#20013;&#25991;&#23545;&#35805;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#35774;&#32622;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#22810;&#26679;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19982;&#22238;&#31572;&#30456;&#20851;&#30340;&#27573;&#33853;&#26469;&#26356;&#21512;&#29702;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.13619</link><description>&lt;p&gt;
Orca: &#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#23545;&#35805;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#23569;&#26679;&#26412;&#27979;&#35797;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension. (arXiv:2302.13619v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13619
&lt;/p&gt;
&lt;p&gt;
Orca&#26159;&#20013;&#25991;&#23545;&#35805;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#35774;&#32622;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#22810;&#26679;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19982;&#22238;&#31572;&#30456;&#20851;&#30340;&#27573;&#33853;&#26469;&#26356;&#21512;&#29702;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;CMRC&#65289;&#20219;&#21153;&#26088;&#22312;&#22238;&#31572;&#23545;&#35805;&#20013;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#20854;&#24191;&#27867;&#24212;&#29992;&#65292;&#36817;&#24180;&#26469;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CMRC&#22522;&#20934;&#22312;&#27599;&#20010;&#23545;&#35805;&#20013;&#20998;&#37197;&#19968;&#20010;&#38745;&#24577;&#27573;&#33853;&#65292;&#19982;&#30495;&#23454;&#22330;&#26223;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#24456;&#38590;&#21512;&#29702;&#35780;&#20272;&#27169;&#22411;&#23545;&#30495;&#23454;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20013;&#25991;CMRC&#22522;&#20934;Orca&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#22810;&#26679;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;831&#20010;&#28909;&#38376;&#35805;&#39064;&#39537;&#21160;&#30340;&#23545;&#35805;&#65292;&#20849;&#35745;4,742&#36718;&#12290;&#27599;&#20010;&#23545;&#35805;&#30340;&#27599;&#20010;&#36718;&#27425;&#37117;&#20250;&#20998;&#37197;&#19968;&#20010;&#19982;&#22238;&#31572;&#26377;&#20851;&#30340;&#27573;&#33853;&#65292;&#26088;&#22312;&#26356;&#21512;&#29702;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23545;&#35805;&#30340;&#20027;&#39064;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#28085;&#30422;33&#20010;&#39046;&#22495;&#65292;&#21147;&#20105;&#19982;&#30495;&#23454;&#22330;&#26223;&#20445;&#25345;&#19968;&#33268;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;Orca&#20013;&#30340;&#31572;&#26696;&#37117;&#26159;&#32463;&#36807;&#33391;&#22909;&#27880;&#37322;&#30340;&#33258;&#28982;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conversational machine reading comprehension (CMRC) task aims to answer questions in conversations, which has been a hot research topic in recent years because of its wide applications. However, existing CMRC benchmarks in which each conversation is assigned a static passage are inconsistent with real scenarios. Thus, model's comprehension ability towards real scenarios are hard to evaluate reasonably. To this end, we propose the first Chinese CMRC benchmark Orca and further provide zero-shot/few-shot settings to evaluate model's generalization ability towards diverse domains. We collect 831 hot-topic driven conversations with 4,742 turns in total. Each turn of a conversation is assigned with a response-related passage, aiming to evaluate model's comprehension ability more reasonably. The topics of conversations are collected from social media platform and cover 33 domains, trying to be consistent with real scenarios. Importantly, answers in Orca are all well-annotated natural resp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36319;&#36394;&#21160;&#24577;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#31038;&#21306;&#65292;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#31038;&#21306;&#30340;&#19968;&#31995;&#21015;&#37325;&#35201;&#20107;&#20214;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#38408;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#26356;&#40065;&#26834;&#30340;&#36319;&#36394;&#12290;</title><link>http://arxiv.org/abs/2302.12759</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#29992;&#20110;&#36319;&#36394;&#21160;&#24577;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#31038;&#21306;
&lt;/p&gt;
&lt;p&gt;
Modularity-based approach for tracking communities in dynamic social networks. (arXiv:2302.12759v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36319;&#36394;&#21160;&#24577;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#31038;&#21306;&#65292;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#31038;&#21306;&#30340;&#19968;&#31995;&#21015;&#37325;&#35201;&#20107;&#20214;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#38408;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#26356;&#40065;&#26834;&#30340;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#35299;&#26512;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#22797;&#26434;&#21160;&#24577;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#20986;&#29616;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#29992;&#25143;&#20043;&#38388;&#20132;&#20114;&#30340;&#25968;&#37327;&#21644;&#36895;&#24230;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#20998;&#26512;&#31038;&#20132;&#32676;&#20307;&#30340;&#22522;&#26412;&#32467;&#26500;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#23545;&#20110;&#23454;&#38469;&#31038;&#20132;&#32593;&#32476;&#20013;&#29992;&#25143;&#32452;&#30340;&#28436;&#21464;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#20294;&#30446;&#21069;&#30340;&#31038;&#21306;&#26816;&#27979;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;&#32593;&#32476;&#20869;&#30340;&#31038;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#21160;&#24577;&#32593;&#32476;&#20013;&#36319;&#36394;&#31038;&#21306;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26032;&#26694;&#26550;&#65292;&#20026;&#27599;&#20010;&#31038;&#21306;&#35782;&#21035;&#20102;&#19968;&#31995;&#21015;&#37325;&#35201;&#20107;&#20214;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#22522;&#20110;&#27169;&#22359;&#21270;&#30340;&#31574;&#30053;&#65292;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#12289;&#26356;&#40065;&#26834;&#22320;&#36319;&#36394;&#21160;&#24577;&#31038;&#21306;&#12290;&#36890;&#36807;&#22312;&#23884;&#20837;&#20107;&#20214;&#30340;&#21512;&#25104;&#32593;&#32476;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is a crucial task to unravel the intricate dynamics of online social networks. The emergence of these networks has dramatically increased the volume and speed of interactions among users, presenting researchers with unprecedented opportunities to explore and analyze the underlying structure of social communities. Despite a growing interest in tracking the evolution of groups of users in real-world social networks, the predominant focus of community detection efforts has been on communities within static networks. In this paper, we introduce a novel framework for tracking communities over time in a dynamic network, where a series of significant events is identified for each community. Our framework adopts a modularity-based strategy and does not require a predefined threshold, leading to a more accurate and robust tracking of dynamic communities. We validated the efficacy of our framework through extensive experiments on synthetic networks featuring embedded events. 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#20301;&#32622;&#19982;&#24615;&#33021;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#21306;&#22495;&#65292;&#19988;&#36825;&#20123;&#21306;&#22495;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32469;&#36807;&#36825;&#20123;&#21306;&#22495;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#26032;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.04863</link><description>&lt;p&gt;
&#30693;&#35782;&#26159;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#37325;&#31354;&#38388;&#30340;&#19968;&#20010;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Knowledge is a Region in Weight Space for Fine-tuned Language Models. (arXiv:2302.04863v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04863
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#20301;&#32622;&#19982;&#24615;&#33021;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#21306;&#22495;&#65292;&#19988;&#36825;&#20123;&#21306;&#22495;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32469;&#36807;&#36825;&#20123;&#21306;&#22495;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#26032;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#19968;&#30452;&#19987;&#27880;&#20110;&#29702;&#35299;&#21333;&#20010;&#27169;&#22411;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#25110;&#27979;&#35797;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#20102;&#35299;&#29978;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#27169;&#22411;&#30340;&#26435;&#37325;&#31354;&#38388;&#21644;&#28508;&#22312;&#30340;&#25439;&#22833;&#22320;&#24418;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#39640;&#24615;&#33021;&#32780;&#36827;&#34892;&#24494;&#35843;&#20248;&#21270;&#30340;&#27169;&#22411;&#23384;&#22312;&#20110;&#26435;&#37325;&#31354;&#38388;&#20013;&#23450;&#20041;&#26126;&#30830;&#30340;&#21306;&#22495;&#20013;&#65292;&#21453;&#20043;&#20134;&#28982;&#8212;&#8212;&#20219;&#20309;&#22312;&#36825;&#20123;&#21306;&#22495;&#20013;&#30340;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#24418;&#25104;&#19968;&#20010;&#32039;&#23494;&#30340;&#32858;&#31867;&#65292;&#32780;&#22312;&#30456;&#21516;&#22522;&#30784;&#20219;&#21153;&#19979;&#20174;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#21017;&#24418;&#25104;&#19968;&#20010;&#36739;&#26494;&#25955;&#30340;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#32469;&#36807;&#27169;&#22411;&#20043;&#38388;&#30340;&#21306;&#22495;&#20250;&#29983;&#25104;&#24615;&#33021;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#26032;&#27169;&#22411;&#65292;&#29978;&#33267;&#22312;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on neural networks has focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, particularly those trained or tested on different datasets. We address this by studying how the weight space and the underlying loss landscape of different models are interconnected.  Specifically, we demonstrate that finetuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa -- that any model that resides anywhere in those regions also exhibits high performance. Notably, we show that language models that have been finetuned on the same dataset form a tight cluster in the weight space, while models finetuned on different datasets from the same underlying task form a looser cluster. Moreover, traversing around the region between the models leads to new models that perform comparably or even better than models obtained via finetuning, even on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23545;&#19981;&#21516;&#25805;&#20316;&#21078;&#38754;&#30340;&#19981;&#21516;&#38454;&#27573;&#36827;&#34892;&#32771;&#34385;&#30340;&#23545;&#25239;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.01704</link><description>&lt;p&gt;
&#22522;&#20110;&#25805;&#20316;&#21078;&#38754;&#23545;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation via Alignment of Operation Profile for Remaining Useful Lifetime Prediction. (arXiv:2302.01704v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23545;&#19981;&#21516;&#25805;&#20316;&#21078;&#38754;&#30340;&#19981;&#21516;&#38454;&#27573;&#36827;&#34892;&#32771;&#34385;&#30340;&#23545;&#25239;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#20381;&#36182;&#20110;&#21097;&#20313;&#23551;&#21629;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#25216;&#26415;&#20005;&#37325;&#20381;&#36182;&#20110;&#21487;&#29992;&#30340;&#22833;&#25928;&#26102;&#38388;&#36712;&#36857;&#30340;&#20195;&#34920;&#24615;&#12290;&#22240;&#27492;&#65292;&#24403;&#24212;&#29992;&#20110;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#21516;&#25805;&#20316;&#26465;&#20214;&#30456;&#27604;&#36739;&#30340;&#36710;&#38431;&#20013;&#30340;&#26032;&#21333;&#20803;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20063;&#34987;&#31216;&#20026;&#39046;&#22495;&#28418;&#31227;&#12290;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#26469;&#35299;&#20915;&#39046;&#22495;&#28418;&#31227;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#27809;&#26377;&#21306;&#20998;&#19981;&#21516;&#30340;&#25805;&#20316;&#38454;&#27573;&#65292;&#22914;&#31283;&#24577;&#25110;&#30636;&#24577;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#19981;&#21516;&#25805;&#20316;&#38454;&#27573;&#30340;&#27424;&#34920;&#31034;&#25110;&#36807;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#19981;&#23545;&#40784;&#12290;&#26412;&#25991;&#22522;&#20110;&#23545;&#19981;&#21516;&#25805;&#20316;&#21078;&#38754;&#30340;&#19981;&#21516;&#38454;&#27573;&#36827;&#34892;&#32771;&#34385;&#30340;&#23545;&#25239;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective Prognostics and Health Management (PHM) relies on accurate prediction of the Remaining Useful Life (RUL). Data-driven RUL prediction techniques rely heavily on the representativeness of the available time-to-failure trajectories. Therefore, these methods may not perform well when applied to data from new units of a fleet that follow different operating conditions than those they were trained on. This is also known as domain shifts. Domain adaptation (DA) methods aim to address the domain shift problem by extracting domain invariant features. However, DA methods do not distinguish between the different phases of operation, such as steady states or transient phases. This can result in misalignment due to under- or over-representation of different operation phases. This paper proposes two novel DA approaches for RUL prediction based on an adversarial domain adaptation framework that considers the different phases of the operation profiles separately. The proposed methodologies a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#20219;&#21153;&#25511;&#21046;&#20013;&#38754;&#20020;&#30340;&#24320;&#25918;&#19990;&#30028;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;Goal-Sensitive Backbone&#40723;&#21169;&#20986;&#29616;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#35270;&#35273;&#29366;&#24577;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35270;&#37326;&#39044;&#27979;&#27169;&#22359;&#26469;&#20943;&#36731;&#38750;&#38745;&#24577;&#21160;&#24577;&#24341;&#36215;&#30340;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.10034</link><description>&lt;p&gt;
&#36890;&#36807;&#30446;&#26631;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#21644;&#33258;&#36866;&#24212;&#35270;&#37326;&#39044;&#27979;&#30340;&#24320;&#25918;&#19990;&#30028;&#22810;&#20219;&#21153;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction. (arXiv:2301.10034v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#20219;&#21153;&#25511;&#21046;&#20013;&#38754;&#20020;&#30340;&#24320;&#25918;&#19990;&#30028;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;Goal-Sensitive Backbone&#40723;&#21169;&#20986;&#29616;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#35270;&#35273;&#29366;&#24577;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35270;&#37326;&#39044;&#27979;&#27169;&#22359;&#26469;&#20943;&#36731;&#38750;&#38745;&#24577;&#21160;&#24577;&#24341;&#36215;&#30340;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;Minecraft&#20013;&#23398;&#20064;&#30446;&#26631;&#26465;&#20214;&#19979;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;Goal-Sensitive Backbone&#65288;GSB&#65289;&#20351;&#25919;&#31574;&#40723;&#21169;&#20986;&#29616;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#35270;&#35273;&#29366;&#24577;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35270;&#37326;&#39044;&#27979;&#27169;&#22359;&#20943;&#36731;&#22240;&#38750;&#38745;&#24577;&#21160;&#24577;&#24341;&#36215;&#30340;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;20&#20010;Minecraft&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#22522;&#32447;&#65292;&#20854;&#20013;&#35768;&#22810;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#22522;&#32447;&#30340;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning goal-conditioned policies in Minecraft, a popular, widely accessible yet challenging open-ended environment for developing human-level multi-task agents. We first identify two main challenges of learning such policies: 1) the indistinguishability of tasks from the state distribution, due to the vast scene diversity, and 2) the non-stationary nature of environment dynamics caused by partial observability. To tackle the first challenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage the emergence of goal-relevant visual state representations. To tackle the second challenge, the policy is further fueled by an adaptive horizon prediction module that helps alleviate the learning uncertainty brought by the non-stationary dynamics. Experiments on 20 Minecraft tasks show that our method significantly outperforms the best baseline so far; in many of them, we double the performance. Our ablation and exploratory studies then explain how our a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#26234;&#33021;&#23548;&#24072;&#65292;&#29992;&#20110;&#22238;&#31572;&#23398;&#29983;&#30340;&#32534;&#31243;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#24110;&#21161;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.09918</link><description>&lt;p&gt;
&#32534;&#31243;&#35838;&#31243;&#20013;&#25552;&#20379;&#21453;&#39304;&#30340;&#26234;&#33021;&#23548;&#24072;
&lt;/p&gt;
&lt;p&gt;
Smart tutor to provide feedback in programming courses. (arXiv:2301.09918v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09918
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#26234;&#33021;&#23548;&#24072;&#65292;&#29992;&#20110;&#22238;&#31572;&#23398;&#29983;&#30340;&#32534;&#31243;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#24110;&#21161;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#21487;&#20197;&#23436;&#25104;&#36807;&#21435;&#38590;&#20197;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;AI&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20351;&#29992;&#65292;&#20294;&#22312;&#25945;&#32946;&#29615;&#22659;&#20043;&#22806;&#24182;&#19981;&#24120;&#35265;&#12290;&#25945;&#32946;&#29615;&#22659;&#20013;&#20351;&#29992;AI&#26469;&#33258;&#23450;&#20041;&#20869;&#23481;&#25110;&#20026;&#23398;&#29983;&#25552;&#20379;&#20010;&#24615;&#21270;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#22312;&#32534;&#31243;&#25945;&#23398;&#20013;&#20351;&#29992;AI&#20173;&#28982;&#38656;&#35201;&#25506;&#32034;&#65292;&#22240;&#20026;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#36890;&#24120;&#21482;&#33021;&#25214;&#21040;&#35780;&#20272;&#24037;&#20855;&#26469;&#35780;&#20998;&#23398;&#29983;&#30340;&#20316;&#19994;&#65292;&#32780;&#25214;&#19981;&#21040;&#24456;&#22810;&#24037;&#20855;&#26469;&#24110;&#21161;&#23398;&#29983;&#22312;&#21019;&#24314;&#31243;&#24207;&#30340;&#36807;&#31243;&#20013;&#25552;&#20379;&#21453;&#39304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#26234;&#33021;&#23548;&#24072;&#65292;&#29992;&#20110;&#22238;&#31572;&#23398;&#29983;&#30340;&#32534;&#31243;&#38382;&#39064;&#12290;&#36825;&#20010;&#24037;&#20855;&#22312;URJC&#30340;&#22823;&#23398;&#29983;&#20013;&#36827;&#34892;&#20102;&#25972;&#20010;&#35838;&#31243;&#30340;&#27979;&#35797;&#12290;&#23613;&#31649;&#36825;&#20010;&#24037;&#20855;&#20173;&#22788;&#20110;&#21021;&#27493;&#38454;&#27573;&#65292;&#20294;&#23427;&#23545;&#23398;&#29983;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#20934;&#30830;&#21644;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is becoming more and more popular as time passes, allowing to perform tasks that were difficult to do in the past. From predictions to customization, AI is being used in many areas, not being educational environments outside this situation. AI is being used in educational settings to customize contents or to provide personalized feedback to the students, among others. In this scenario, AI in programming teaching is something that still has to be explored, since in this area we usually find assessment tools that allow grading the students work, but we can not find many tools aimed towards providing feedback to the students in the process of creating their program. In this work we present an AI based intelligent tutor that answers students programming questions. The tool has been tested by university students at the URJC along a whole course. Even if the tool is still in its preliminary phase, it helped the students with their questions, providing accurate an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20174;&#24341;&#23548;&#24335;&#28216;&#25103;&#23398;&#20064;&#65288;LfGP&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#20010;&#36741;&#21161;&#20219;&#21153;&#21644;&#20027;&#20219;&#21153;&#30340;&#19987;&#23478;&#28436;&#31034;&#65292;&#25552;&#21319;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;AIL&#22312;&#23398;&#20064;&#25805;&#20316;&#20219;&#21153;&#26102;&#21487;&#33021;&#38519;&#20837;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.00051</link><description>&lt;p&gt;
&#20174;&#24341;&#23548;&#24335;&#28216;&#25103;&#23398;&#20064;&#20013;&#25552;&#21319;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning from Guided Play: Improving Exploration for Adversarial Imitation Learning with Simple Auxiliary Tasks. (arXiv:2301.00051v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20174;&#24341;&#23548;&#24335;&#28216;&#25103;&#23398;&#20064;&#65288;LfGP&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#20010;&#36741;&#21161;&#20219;&#21153;&#21644;&#20027;&#20219;&#21153;&#30340;&#19987;&#23478;&#28436;&#31034;&#65292;&#25552;&#21319;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;AIL&#22312;&#23398;&#20064;&#25805;&#20316;&#20219;&#21153;&#26102;&#21487;&#33021;&#38519;&#20837;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#24050;&#25104;&#20026;&#20943;&#23569;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#27969;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#38454;&#27573;&#65292;AIL&#38656;&#35201;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#12289;&#22825;&#30495;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#22914;&#26524;&#20351;&#29992;AIL&#23398;&#20064;&#30340;&#31574;&#30053;&#19982;&#19987;&#23478;&#20998;&#24067;&#36275;&#22815;&#21305;&#37197;&#20294;&#27809;&#26377;&#23436;&#20840;&#23398;&#20250;&#25152;&#38656;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#20250;&#34920;&#29616;&#20026;&#19968;&#20010;&#27425;&#20248;&#30340;&#23616;&#37096;&#26368;&#22823;&#20540;&#12290;&#36825;&#23545;&#20110;&#25805;&#20316;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#23588;&#20026;&#28798;&#38590;&#65292;&#22240;&#20026;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20043;&#38388;&#30340;&#24046;&#21035;&#36890;&#24120;&#26159;&#24494;&#22937;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#24341;&#23548;&#28216;&#25103;&#20013;&#23398;&#20064;&#65288;LfGP&#65289;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#20010;&#25506;&#32034;&#24615;&#36741;&#21161;&#20219;&#21153;&#30340;&#19987;&#23478;&#28436;&#31034;&#65292;&#38500;&#20102;&#19968;&#20010;&#20027;&#20219;&#21153;&#12290;&#36825;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#28155;&#21152;&#24378;&#21046;&#20195;&#29702;&#20154;&#25506;&#32034;&#26631;&#20934;AIL&#21487;&#33021;&#23398;&#20250;&#24573;&#35270;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#29305;&#23450;&#30340;&#20844;&#24335;&#20801;&#35768;&#36741;&#21161;&#20219;&#21153;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial imitation learning (AIL) has become a popular alternative to supervised imitation learning that reduces the distribution shift suffered by the latter. However, AIL requires effective exploration during an online reinforcement learning phase. In this work, we show that the standard, naive approach to exploration can manifest as a suboptimal local maximum if a policy learned with AIL sufficiently matches the expert distribution without fully learning the desired task. This can be particularly catastrophic for manipulation tasks, where the difference between an expert and a non-expert state-action pair is often subtle. We present Learning from Guided Play (LfGP), a framework in which we leverage expert demonstrations of multiple exploratory, auxiliary tasks in addition to a main task. The addition of these auxiliary tasks forces the agent to explore states and actions that standard AIL may learn to ignore. Additionally, this particular formulation allows for the reusability of
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#30456;&#26426;&#30340;&#35835;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#23454;&#26102;&#12289;&#21160;&#24577;&#22320;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#23545;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2211.11744</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24863;&#30693;&#23454;&#29616;&#25163;&#25345;&#28789;&#24039;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Visual Dexterity: In-hand Dexterous Manipulation from Depth. (arXiv:2211.11744v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11744
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#30456;&#26426;&#30340;&#35835;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#23454;&#26102;&#12289;&#21160;&#24577;&#22320;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#23545;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#25345;&#29289;&#20307;&#30340;&#37325;&#26032;&#23450;&#21521;&#23545;&#20110;&#25191;&#34892;&#35768;&#22810;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#38750;&#24120;&#24517;&#35201;&#65292;&#20363;&#22914;&#22312;&#24403;&#21069;&#26426;&#22120;&#20154;&#26080;&#27861;&#35302;&#21450;&#30340;&#32467;&#26500;&#19981;&#22826;&#23436;&#21892;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#24037;&#20855;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#37325;&#26032;&#23450;&#21521;&#31995;&#32479;&#65292;&#20551;&#35774;&#20197;&#19979;&#24773;&#20917;&#20043;&#19968;&#25110;&#22810;&#31181;&#24773;&#20917;&#21516;&#26102;&#23384;&#22312;&#65306;&#20165;&#37325;&#26032;&#23450;&#21521;&#20855;&#26377;&#31616;&#21333;&#24418;&#29366;&#30340;&#29305;&#23450;&#29289;&#20307;&#12289;&#37325;&#26032;&#23450;&#21521;&#33539;&#22260;&#26377;&#38480;&#12289;&#24930;&#36895;&#25110;&#20934;&#38745;&#24577;&#25805;&#20316;&#12289;&#20165;&#27169;&#25311;&#32467;&#26524;&#12289;&#38656;&#35201;&#19987;&#29992;&#19988;&#26114;&#36149;&#30340;&#20256;&#24863;&#22120;&#22871;&#20214;&#20197;&#21450;&#20854;&#20182;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#37096;&#32626;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20570;&#36825;&#20123;&#20551;&#35774;&#30340;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#12290;&#23427;&#20351;&#29992;&#26469;&#33258;&#21333;&#20010;&#26222;&#36890;&#28145;&#24230;&#25668;&#20687;&#26426;&#30340;&#35835;&#25968;&#65292;&#20197;&#23454;&#26102;&#26041;&#24335;&#36890;&#36807;&#20219;&#24847;&#26059;&#36716;&#21160;&#24577;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#19988;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#19978;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324; ...
&lt;/p&gt;
&lt;p&gt;
In-hand object reorientation is necessary for performing many dexterous manipulation tasks, such as tool use in less structured environments that remain beyond the reach of current robots. Prior works built reorientation systems assuming one or many of the following: reorienting only specific objects with simple shapes, limited range of reorientation, slow or quasistatic manipulation, simulation-only results, the need for specialized and costly sensor suites, and other constraints which make the system infeasible for real-world deployment. We present a general object reorientation controller that does not make these assumptions. It uses readings from a single commodity depth camera to dynamically reorient complex and new object shapes by any rotation in real-time, with the median reorientation time being close to seven seconds. The controller is trained using reinforcement learning in simulation and evaluated in the real world on new object shapes not used for training, including the m
&lt;/p&gt;</description></item><item><title>ViNL&#26159;&#36890;&#36807;&#35270;&#35273;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#22312;&#26410;&#30693;&#23460;&#20869;&#29615;&#22659;&#20013;&#23454;&#29616;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#36816;&#21160;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#26080;&#27169;&#22411;&#30340;&#35270;&#35273;&#23548;&#33322;&#31574;&#30053;&#21644;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#65292;&#24182;&#33021;&#22815;&#36991;&#20813;&#36393;&#21040;&#23567;&#38556;&#30861;&#29289;&#12290;ViNL&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#19988;&#31283;&#23450;&#30340;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#36816;&#21160;&#65292;&#26080;&#38656;&#29615;&#22659;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2210.14791</link><description>&lt;p&gt;
ViNL&#65306;&#36890;&#36807;&#35270;&#35273;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#34892;&#36208;&#36991;&#20813;&#38556;&#30861;&#29289;
&lt;/p&gt;
&lt;p&gt;
ViNL: Visual Navigation and Locomotion Over Obstacles. (arXiv:2210.14791v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14791
&lt;/p&gt;
&lt;p&gt;
ViNL&#26159;&#36890;&#36807;&#35270;&#35273;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#22312;&#26410;&#30693;&#23460;&#20869;&#29615;&#22659;&#20013;&#23454;&#29616;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#36816;&#21160;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#26080;&#27169;&#22411;&#30340;&#35270;&#35273;&#23548;&#33322;&#31574;&#30053;&#21644;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#65292;&#24182;&#33021;&#22815;&#36991;&#20813;&#36393;&#21040;&#23567;&#38556;&#30861;&#29289;&#12290;ViNL&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#19988;&#31283;&#23450;&#30340;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#36816;&#21160;&#65292;&#26080;&#38656;&#29615;&#22659;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ViNL&#65288;Visual Navigation and Locomotion&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#36335;&#24452;&#19978;&#36328;&#36807;&#23567;&#38556;&#30861;&#29289;&#65288;&#20363;&#22914;&#38795;&#23376;&#12289;&#29609;&#20855;&#12289;&#30005;&#32518;&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#21644;&#23456;&#29289;&#22312;&#34892;&#36208;&#26102;&#25260;&#36215;&#33050;&#27493;&#36229;&#36807;&#29289;&#20307;&#12290;ViNL&#21253;&#25324;&#65306;&#65288;1&#65289;&#19968;&#20010;&#35270;&#35273;&#23548;&#33322;&#31574;&#30053;&#65292;&#36755;&#20986;&#32447;&#24615;&#21644;&#35282;&#36895;&#24230;&#21629;&#20196;&#65292;&#25351;&#23548;&#26426;&#22120;&#20154;&#22312;&#38476;&#29983;&#30340;&#23460;&#20869;&#29615;&#22659;&#20013;&#36798;&#21040;&#30446;&#26631;&#22352;&#26631;&#65307;&#65288;2&#65289;&#19968;&#20010;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#65292;&#36890;&#36807;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#20851;&#33410;&#65292;&#36991;&#20813;&#36393;&#21040;&#38556;&#30861;&#29289;&#65292;&#24182;&#25353;&#29031;&#25552;&#20379;&#30340;&#36895;&#24230;&#21629;&#20196;&#31227;&#21160;&#12290;&#36825;&#20004;&#20010;&#31574;&#30053;&#37117;&#26159;&#23436;&#20840;&#8220;&#26080;&#27169;&#22411;&#8221;&#30340;&#65292;&#21363;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#20256;&#24863;&#22120;&#21040;&#34892;&#21160;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20004;&#32773;&#22312;&#20004;&#20010;&#23436;&#20840;&#19981;&#21516;&#30340;&#27169;&#25311;&#22120;&#20013;&#29420;&#31435;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#23548;&#33322;&#22120;&#30340;&#36895;&#24230;&#21629;&#20196;&#36755;&#20837;&#21040;&#23450;&#20301;&#22120;&#20013;&#21327;&#21516;&#37096;&#32626;&#65292;&#23436;&#20840;&#8220;&#38646;&#35757;&#32451;&#8221;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#29992;&#20110;&#35270;&#35273;&#23548;&#33322;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#27169;&#22411;&#20808;&#39564;&#21644;/&#25110;&#20154;&#24037;&#29305;&#24449;&#24037;&#31243;&#12290;ViNL&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#29615;&#22659;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26410;&#30693;&#30340;&#23460;&#20869;&#29615;&#22659;&#20013;&#23454;&#29616;&#39640;&#25928;&#19988;&#31283;&#23450;&#30340;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Visual Navigation and Locomotion over obstacles (ViNL), which enables a quadrupedal robot to navigate unseen apartments while stepping over small obstacles that lie in its path (e.g., shoes, toys, cables), similar to how humans and pets lift their feet over objects as they walk. ViNL consists of: (1) a visual navigation policy that outputs linear and angular velocity commands that guides the robot to a goal coordinate in unfamiliar indoor environments; and (2) a visual locomotion policy that controls the robot's joints to avoid stepping on obstacles while following provided velocity commands. Both the policies are entirely "model-free", i.e. sensors-to-actions neural networks trained end-to-end. The two are trained independently in two entirely different simulators and then seamlessly co-deployed by feeding the velocity commands from the navigator to the locomotor, entirely "zero-shot" (without any co-training). While prior works have developed learning methods for visual na
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#12290;&#20182;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.06366</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalist Framework for Panoptic Segmentation of Images and Videos. (arXiv:2210.06366v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#12290;&#20182;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#20998;&#21106;&#20026;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#37197;&#35821;&#20041;&#21644;&#23454;&#20363;ID&#26631;&#31614;&#12290;&#30001;&#20110;&#23454;&#20363;ID&#30340;&#25490;&#21015;&#20063;&#26159;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#20219;&#21153;&#38656;&#35201;&#23398;&#20064;&#39640;&#32500;&#24230;&#30340;&#19968;&#23545;&#22810;&#26144;&#23556;&#12290;&#22240;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#23450;&#21046;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#19981;&#20381;&#36182;&#20219;&#21153;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#65292;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#36890;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23558;&#36807;&#21435;&#30340;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#20449;&#21495;&#28155;&#21152;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#31867;&#20284;&#30340;&#35774;&#32622;&#19979;&#33021;&#22815;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic segmentation assigns semantic and instance ID labels to every pixel of an image. As permutations of instance IDs are also valid solutions, the task requires learning of high-dimensional one-to-many mapping. As a result, state-of-the-art approaches use customized architectures and task-specific loss functions. We formulate panoptic segmentation as a discrete data generation problem, without relying on inductive bias of the task. A diffusion model is proposed to model panoptic masks, with a simple architecture and generic loss function. By simply adding past predictions as a conditioning signal, our method is capable of modeling video (in a streaming setting) and thereby learns to track object instances automatically. With extensive experiments, we demonstrate that our simple approach can perform competitively to state-of-the-art specialist methods in similar settings.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#65292;&#22312;&#22686;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.03029</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#22686;&#24378;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt. (arXiv:2210.03029v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03029
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#65292;&#22312;&#22686;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#25928;&#29575;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#35201;&#20040;&#36890;&#36807;&#25193;&#23637;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24635;&#25968;&#65292;&#35201;&#20040;&#22686;&#21152;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24494;&#35843;&#33719;&#21462;&#36719;&#25552;&#31034;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#20219;&#21153;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24494;&#35843;&#20026;&#27599;&#20010;&#25552;&#31034;&#35757;&#32451;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#23384;&#20648;&#19982;&#25552;&#31034;&#23884;&#20837;&#26144;&#23556;&#30340;&#35757;&#32451;&#23454;&#20363;&#26679;&#26412;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26816;&#32034;&#26368;&#25509;&#36817;&#26597;&#35810;&#23454;&#20363;&#30340;&#35757;&#32451;&#23454;&#20363;&#23545;&#24212;&#30340;&#25552;&#31034;&#23884;&#20837;&#12290;&#34429;&#28982;&#21482;&#22686;&#21152;&#20102;0.007%&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#26816;&#32034;&#36719;&#25552;&#31034;&#25552;&#39640;&#20102;T0&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;11&#20010;&#25968;&#25454;&#38598;&#20013;&#26377;10&#20010;&#34920;&#29616;&#20248;&#20110;T0&#65292;&#24182;&#19988;&#23558;T0&#22312;BIG-bench&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2.39&#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#20010;&#26377;&#24847;&#24605;&#30340;&#21457;&#29616;&#65292;&#21363;&#26816;&#32034;&#22312;&#30456;&#20284;&#31572;&#26696;&#36873;&#25321;&#26684;&#24335;&#19978;&#35757;&#32451;&#30340;&#28304;&#23884;&#20837;&#27604;&#25552;&#31034;&#23884;&#20837;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancing the zero-shot performance of instruction-following models requires heavy computation, either by scaling the total number of training datasets or the model size. In this work, we explore how retrieval of soft prompts obtained through prompt tuning can efficiently assist hard prompts in zero-shot task generalization. Specifically, we train soft prompt embeddings for each prompt through prompt tuning, store the samples of the training instances mapped with the prompt embeddings, and retrieve the corresponding prompt embedding of the training instance closest to the query instance during inference. While only adding 0.007% additional parameters, retrieval of soft prompt enhances the performance of T0 on unseen tasks by outperforming it on 10 out of 11 datasets as well as improving the mean accuracy of T0 on BIG-bench benchmark by 2.39% points. Also, we report an interesting finding that retrieving source embeddings trained on similar answer choice formats is more important than t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;2021&#24180;iGibson&#25361;&#25112;&#36187;&#30340;&#33719;&#32988;&#21442;&#36187;&#20316;&#21697;&#65292;&#26088;&#22312;&#35780;&#20272;&#19981;&#21516;&#30340;&#22686;&#24378;&#25216;&#26415;&#23545;&#26234;&#33021;&#20307;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#25913;&#36827;&#25928;&#26524;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#21160;&#24577;&#38556;&#30861;&#29289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27979;&#35797;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#19982;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#30456;&#32467;&#21512;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#23545;&#27169;&#25311;&#22120;&#20043;&#38388;&#30340;&#36801;&#31227;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2109.10493</link><description>&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#23398;&#20064;&#40065;&#26834;&#23548;&#33322;&#26234;&#33021;&#20307;&#30340;&#22686;&#24378;&#26041;&#27861;&#65306;2021&#24180;iGibson&#25361;&#25112;&#36187;&#30340;&#33719;&#32988;&#21442;&#36187;&#20316;&#21697;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Augmentation Methods for Learning Robust Navigation Agents: the Winning Entry of the 2021 iGibson Challenge. (arXiv:2109.10493v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.10493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;2021&#24180;iGibson&#25361;&#25112;&#36187;&#30340;&#33719;&#32988;&#21442;&#36187;&#20316;&#21697;&#65292;&#26088;&#22312;&#35780;&#20272;&#19981;&#21516;&#30340;&#22686;&#24378;&#25216;&#26415;&#23545;&#26234;&#33021;&#20307;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#25913;&#36827;&#25928;&#26524;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#21160;&#24577;&#38556;&#30861;&#29289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27979;&#35797;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#19982;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#30456;&#32467;&#21512;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#23545;&#27169;&#25311;&#22120;&#20043;&#38388;&#30340;&#36801;&#31227;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#21487;&#25193;&#23637;&#30340;&#36924;&#30495;&#20223;&#30495;&#25216;&#26415;&#30340;&#36827;&#27493;&#24050;&#32463;&#20351;&#24471;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#23548;&#33322;&#26041;&#38754;&#30340;&#20855;&#20307;AI&#24840;&#21457;&#25104;&#29087;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#25945;&#23548;&#20307;&#39564;&#21270;&#26234;&#33021;&#20307;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#23548;&#33322;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#21253;&#21547;&#31227;&#21160;&#34892;&#20154;&#25110;&#21487;&#31227;&#21160;&#38556;&#30861;&#29289;&#30340;&#26356;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#30340;&#36827;&#23637;&#36739;&#23569;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#19981;&#21516;&#22686;&#24378;&#25216;&#26415;&#23545;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#25552;&#39640;&#26234;&#33021;&#20307;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#22330;&#26223;&#20013;&#28155;&#21152;&#22810;&#20010;&#21160;&#24577;&#38556;&#30861;&#29289;&#21487;&#26174;&#33879;&#25552;&#39640;&#26234;&#33021;&#20307;&#22312;&#27979;&#35797;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25104;&#21151;&#29575;&#36828;&#39640;&#20110;&#22522;&#20934;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21487;&#20197;&#23558;&#35813;&#26041;&#27861;&#19982;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20174;&#19968;&#31181;&#27169;&#25311;&#22120;&#36716;&#31227;&#21040;&#21478;&#19968;&#31181;&#27169;&#25311;&#22120;&#26102;&#27604;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#23548;&#33322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep reinforcement learning and scalable photorealistic simulation have led to increasingly mature embodied AI for various visual tasks, including navigation. However, while impressive progress has been made for teaching embodied agents to navigate static environments, much less progress has been made on more dynamic environments that may include moving pedestrians or movable obstacles. In this study, we aim to benchmark different augmentation techniques for improving the agent's performance in these challenging environments. We show that adding several dynamic obstacles into the scene during training confers significant improvements in test-time generalization, achieving much higher success rates than baseline agents. We find that this approach can also be combined with image augmentation methods to achieve even higher success rates. Additionally, we show that this approach is also more robust to sim-to-sim transfer than image augmentation methods. Finally, we demon
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#23436;&#25104;&#26102;&#38388;&#20026;&#26435;&#37325;&#30340;&#25104;&#21151;&#24230;&#37327;&#65288;SCT&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#31227;&#21160;&#26426;&#22120;&#20154;&#23548;&#33322;&#24615;&#33021;&#65292;&#24182;&#35774;&#35745;&#20102;&#38024;&#23545;&#21333;&#36718;&#23567;&#36710;&#21160;&#24577;&#27169;&#22411;&#30340;&#31639;&#27861;RRT*-Unicycle&#12290;&#35813;&#25351;&#26631;&#21487;&#20197;&#20934;&#30830;&#34913;&#37327;&#20195;&#29702;&#27169;&#25311;&#26368;&#24555;&#23548;&#33322;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2103.08022</link><description>&lt;p&gt;
&#20197;&#23436;&#25104;&#26102;&#38388;&#20026;&#26435;&#37325;&#30340;&#25104;&#21151;&#24230;&#37327;&#65306;&#19968;&#31181;&#38024;&#23545;&#34892;&#21160;&#23548;&#33322;&#30340;&#21160;&#24577;&#24863;&#30693;&#35780;&#20272;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation. (arXiv:2103.08022v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.08022
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#23436;&#25104;&#26102;&#38388;&#20026;&#26435;&#37325;&#30340;&#25104;&#21151;&#24230;&#37327;&#65288;SCT&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#31227;&#21160;&#26426;&#22120;&#20154;&#23548;&#33322;&#24615;&#33021;&#65292;&#24182;&#35774;&#35745;&#20102;&#38024;&#23545;&#21333;&#36718;&#23567;&#36710;&#21160;&#24577;&#27169;&#22411;&#30340;&#31639;&#27861;RRT*-Unicycle&#12290;&#35813;&#25351;&#26631;&#21487;&#20197;&#20934;&#30830;&#34913;&#37327;&#20195;&#29702;&#27169;&#25311;&#26368;&#24555;&#23548;&#33322;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#23436;&#25104;&#26102;&#38388;&#20026;&#26435;&#37325;&#30340;&#25104;&#21151;&#24230;&#37327;&#65288;SCT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35780;&#20272;&#31227;&#21160;&#26426;&#22120;&#20154;&#23548;&#33322;&#24615;&#33021;&#30340;&#26032;&#25351;&#26631;&#12290;&#35768;&#22810;&#19982;&#23548;&#33322;&#30456;&#20851;&#30340;&#30740;&#31350;&#20351;&#29992;&#20197;&#36335;&#24452;&#38271;&#24230;&#20026;&#26435;&#37325;&#30340;&#25104;&#21151;&#24230;&#37327;&#65288;SPL&#65289;&#20316;&#20026;&#35780;&#20272;&#20195;&#29702;&#21040;&#36798;&#30446;&#26631;&#20301;&#32622;&#36335;&#24452;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20294;&#26159;SPL&#22312;&#35780;&#20272;&#20855;&#26377;&#22797;&#26434;&#21160;&#24577;&#29305;&#24615;&#30340;&#20195;&#29702;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;SCT&#26126;&#30830;&#32771;&#34385;&#20102;&#20195;&#29702;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#26088;&#22312;&#20934;&#30830;&#25429;&#25417;&#20195;&#29702;&#27169;&#25311;&#20854;&#21160;&#24577;&#25152;&#20801;&#35768;&#30340;&#26368;&#24555;&#23548;&#33322;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#34429;&#28982;&#19968;&#20123;&#34892;&#21160;&#23548;&#33322;&#20316;&#21697;&#20351;&#29992;&#28857;&#36716;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#21333;&#36718;&#23567;&#36710;&#21160;&#24577;&#27169;&#22411;&#26469;&#35774;&#35745;&#25105;&#20204;&#30340;&#20195;&#29702;&#65292;&#36825;&#26356;&#22909;&#22320;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#24179;&#21488;&#65288;&#20363;&#22914;LoCoBot&#65292;TurtleBot&#65292;Fetch&#31561;&#65289;&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#36718;&#23567;&#36710;&#21160;&#21147;&#23398;&#30340;RRT*-Unicycle&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20272;&#35745;&#20174;&#36215;&#22987;&#20301;&#23039;&#21040;&#30446;&#26631;&#20301;&#32622;&#30340;&#26368;&#24555;&#26080;&#30896;&#25758;&#36335;&#24452;&#21644;&#23436;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent's dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exemplifies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an envir
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#35774;&#35745;&#21644;&#29983;&#25104;&#21512;&#25104;&#21019;&#26032;&#32593;&#32476;&#35789;&#20856;&#65292;&#30740;&#31350;&#20102;&#26032;&#31526;&#21495;&#21457;&#29616;&#22312;&#22686;&#21152;&#35789;&#27719;&#30340;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#21019;&#26032;&#36807;&#31243;&#30340;&#25972;&#20307;&#32479;&#35745;&#21644;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/1806.07722</link><description>&lt;p&gt;
&#21019;&#26032;&#39118;&#26684;&#21270;&#65306;&#36890;&#36807;&#36880;&#27493;&#21487;&#29992;&#38543;&#26426;&#21270;&#35789;&#20856;&#36827;&#34892;&#26102;&#38388;&#36724;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Stylized innovation: generating timelines by interrogating incrementally available randomised dictionaries. (arXiv:1806.07722v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1806.07722
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#35774;&#35745;&#21644;&#29983;&#25104;&#21512;&#25104;&#21019;&#26032;&#32593;&#32476;&#35789;&#20856;&#65292;&#30740;&#31350;&#20102;&#26032;&#31526;&#21495;&#21457;&#29616;&#22312;&#22686;&#21152;&#35789;&#27719;&#30340;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#21019;&#26032;&#36807;&#31243;&#30340;&#25972;&#20307;&#32479;&#35745;&#21644;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21019;&#26032;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23427;&#26159;&#19968;&#20010;&#21160;&#24577;&#12289;&#25345;&#32493;&#30340;&#36807;&#31243;&#65292;&#21487;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#25991;&#21270;&#12289;&#32463;&#27982;&#25110;&#36816;&#27668;&#31561;&#30636;&#24687;&#19975;&#21464;&#30340;&#22240;&#32032;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#30495;&#23454;&#19990;&#30028;&#36807;&#31243;&#30340;&#20219;&#20309;&#20998;&#26512;&#24517;&#28982;&#26159;&#21382;&#21490;&#24615;&#30340;&#65292;&#22240;&#27492;&#21487;&#33021;&#20026;&#26102;&#24050;&#26202;&#65292;&#20294;&#20063;&#26080;&#27861;&#30830;&#23450;&#21019;&#26032;&#20043;&#38388;&#30340;&#36830;&#25509;&#32467;&#26500;&#25110;&#23646;&#24615;&#26159;&#20160;&#20040;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#23581;&#35797;&#36890;&#36807;&#35774;&#35745;&#21644;&#29983;&#25104;&#19968;&#32452;&#21512;&#25104;&#21019;&#26032;&#32593;&#32476;&#8220;&#35789;&#20856;&#8221;&#65292;&#29992;&#20110;&#25215;&#36733;&#26679;&#26412;&#21019;&#26032;&#26102;&#38388;&#36724;&#65292;&#25506;&#27979;&#36825;&#20123;&#36807;&#31243;&#30340;&#25972;&#20307;&#32479;&#35745;&#21644;&#34892;&#20026;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#23545;&#32467;&#26500;&#25110;&#29983;&#25104;&#31639;&#27861;&#30340;&#20381;&#36182;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;Fink&#12289;Reeves&#12289;Palma&#21644;Farr&#65288;2017&#65289;&#20851;&#20110;&#35821;&#35328;&#12289;&#32654;&#39135;&#21644;&#25216;&#26415;&#21019;&#26032;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#30740;&#31350;&#20102;&#26032;&#31526;&#21495;&#21457;&#29616;&#22914;&#20309;&#20197;&#39069;&#22806;&#30340;&#8220;&#35789;&#27719;&#8221;&#35789;&#20856;&#20013;&#30340;&#21333;&#35789;&#22686;&#21152;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge when trying to understand innovation is that it is a dynamic, ongoing process, which can be highly contingent on ephemeral factors such as culture, economics, or luck. This means that any analysis of the real-world process must necessarily be historical - and thus probably too late to be most useful - but also cannot be sure what the properties of the web of connections between innovations is or was. Here I try to address this by designing and generating a set of synthetic innovation web "dictionaries" that can be used to host sampled innovation timelines, probe the overall statistics and behaviours of these processes, and determine the degree of their reliance on the structure or generating algorithm. Thus, inspired by the work of Fink, Reeves, Palma and Farr (2017) on innovation in language, gastronomy, and technology, I study how new symbol discovery manifests itself in terms of additional "word" vocabulary being available from dictionaries generated from a finite nu
&lt;/p&gt;</description></item></channel></rss>