<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>VAR&#37325;&#26032;&#23450;&#20041;&#20102;&#22270;&#20687;&#19978;&#30340;&#33258;&#22238;&#24402;&#23398;&#20064;&#65292;&#36890;&#36807;&#31895;&#21040;&#32454;&#30340;&#8220;&#19979;&#19968;&#23610;&#24230;&#39044;&#27979;&#8221;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#35270;&#35273;&#20998;&#24067;&#24182;&#36229;&#36234;&#20102;&#25193;&#25955;&#21464;&#21387;&#22120;&#12290;</title><link>https://arxiv.org/abs/2404.02905</link><description>&lt;p&gt;
&#21487;&#35270;&#33258;&#22238;&#24402;&#24314;&#27169;&#65306;&#36890;&#36807;&#19979;&#19968;&#23610;&#24230;&#39044;&#27979;&#23454;&#29616;&#21487;&#20280;&#32553;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02905
&lt;/p&gt;
&lt;p&gt;
VAR&#37325;&#26032;&#23450;&#20041;&#20102;&#22270;&#20687;&#19978;&#30340;&#33258;&#22238;&#24402;&#23398;&#20064;&#65292;&#36890;&#36807;&#31895;&#21040;&#32454;&#30340;&#8220;&#19979;&#19968;&#23610;&#24230;&#39044;&#27979;&#8221;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#35270;&#35273;&#20998;&#24067;&#24182;&#36229;&#36234;&#20102;&#25193;&#25955;&#21464;&#21387;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#35270;&#33258;&#22238;&#24402;&#24314;&#27169;&#65288;VAR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#37325;&#26032;&#23450;&#20041;&#33258;&#22238;&#24402;&#23398;&#20064;&#22312;&#22270;&#20687;&#19978;&#30340;&#29983;&#25104;&#33539;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#20174;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#8220;&#19979;&#19968;&#23610;&#24230;&#39044;&#27979;&#8221;&#25110;&#8220;&#19979;&#19968;&#20998;&#36776;&#29575;&#39044;&#27979;&#8221;&#65292;&#20559;&#31163;&#20102;&#26631;&#20934;&#30340;&#20809;&#26629;&#25195;&#25551;&#8220;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#8221;&#12290;&#36825;&#31181;&#31616;&#21333;&#30452;&#35266;&#30340;&#26041;&#27861;&#20801;&#35768;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#21464;&#21387;&#22120;&#24555;&#36895;&#23398;&#20064;&#35270;&#35273;&#20998;&#24067;&#24182;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65306;VAR&#39318;&#27425;&#20351;AR&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#36229;&#36234;&#20102;&#25193;&#25955;&#21464;&#21387;&#22120;&#12290;&#22312;ImageNet 256x256&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;VAR&#36890;&#36807;&#23558;Frechet&#20837;&#20405;&#36317;&#31163;&#65288;FID&#65289;&#20174;18.65&#25552;&#39640;&#21040;1.80&#65292;&#23558;inception&#20998;&#25968;&#65288;IS&#65289;&#20174;80.4&#25552;&#39640;&#21040;356.4&#65292;&#25512;&#26029;&#36895;&#24230;&#21152;&#24555;&#20102;&#22823;&#32422;20&#20493;&#65292;&#26174;&#30528;&#25913;&#21892;&#20102;AR&#22522;&#32447;&#12290;&#32463;&#39564;&#35777;&#65292;VAR&#22312;&#22270;&#20687;&#36136;&#37327;&#12289;&#25512;&#26029;&#36895;&#24230;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#31561;&#22810;&#20010;&#32500;&#24230;&#19978;&#20248;&#20110;&#25193;&#25955;&#21464;&#21387;&#22120;&#65288;DiT&#65289;&#12290;&#25193;&#22823;VAR&#27169;&#22411;&#30340;&#35268;&#27169;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#24130;&#24459;&#25193;&#23637;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02905v1 Announce Type: cross  Abstract: We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling la
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27979;&#37327;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26631;&#20934;ALOHa&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#24187;&#35273;&#23545;&#35937;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#27604;&#29616;&#26377;&#25351;&#26631;CHAIR&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#12290;</title><link>https://arxiv.org/abs/2404.02904</link><description>&lt;p&gt;
ALOHa&#65306;&#27979;&#37327;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26032;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
ALOHa: A New Measure for Hallucination in Captioning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27979;&#37327;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26631;&#20934;ALOHa&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#24187;&#35273;&#23545;&#35937;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#27604;&#29616;&#26377;&#25351;&#26631;CHAIR&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35270;&#35273;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#20250;&#20135;&#29983;&#21253;&#21547;&#38169;&#35823;&#30340;&#23383;&#24149;&#65292;&#27604;&#22914;&#22312;&#22330;&#26223;&#20013;&#23384;&#22312;&#24187;&#35273;&#23545;&#35937;&#12290;&#29616;&#26377;&#30340;&#20027;&#35201;&#24187;&#35273;&#23545;&#35937;&#24230;&#37327;&#26631;&#20934;CHAIR&#65292;&#20165;&#38480;&#20110;&#19968;&#32452;&#22266;&#23450;&#30340;MS COCO&#23545;&#35937;&#21644;&#21516;&#20041;&#35789;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#20195;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#24230;&#37327;&#26631;&#20934;ALOHa&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#34913;&#37327;&#23545;&#35937;&#24187;&#35273;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#20174;&#20505;&#36873;&#23383;&#24149;&#20013;&#25552;&#21462;&#21487;&#36830;&#25509;&#30340;&#23545;&#35937;&#65292;&#34913;&#37327;&#23427;&#20204;&#19982;&#23383;&#24149;&#21644;&#23545;&#35937;&#26816;&#27979;&#20013;&#21442;&#32771;&#23545;&#35937;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#24182;&#20351;&#29992;&#21256;&#29273;&#21033;&#21305;&#37197;&#29983;&#25104;&#26368;&#32456;&#30340;&#24187;&#35273;&#24471;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ALOHa&#22312;HAT&#19978;&#27604;CHAIR&#22312;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#24187;&#35273;&#26631;&#35760;&#30340;MS COCO&#23383;&#24149;&#30340;&#37329;&#26631;&#20934;&#23376;&#38598;&#19978;&#27491;&#30830;&#35782;&#21035;&#20102;&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#65288;&#22810;&#20986;13.6%&#65289;&#65292;&#22312;nocaps&#19978;&#65288;&#20854;&#20013;&#23545;&#35937;&#36229;&#20986;&#20102;MS COCO&#31867;&#21035;&#65289;&#35782;&#21035;&#20102;&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#65288;&#22810;&#33267;30.8%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02904v1 Announce Type: cross  Abstract: Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene. The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations. Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories.
&lt;/p&gt;</description></item><item><title>DeiT-LT&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26377;&#25928;&#30340;&#33976;&#39311;&#26041;&#24335;&#65292;&#23558;CNN&#33976;&#39311;&#21040;ViT&#20013;&#65292;&#20197;&#24212;&#23545;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;ViT&#26102;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2404.02900</link><description>&lt;p&gt;
DeiT-LT&#33976;&#39311;&#37325;&#36820;&#65292;&#29992;&#20110;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#30340;Vision Transformer&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02900
&lt;/p&gt;
&lt;p&gt;
DeiT-LT&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26377;&#25928;&#30340;&#33976;&#39311;&#26041;&#24335;&#65292;&#23558;CNN&#33976;&#39311;&#21040;ViT&#20013;&#65292;&#20197;&#24212;&#23545;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;ViT&#26102;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer&#65288;ViT&#65289;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#31361;&#20986;&#30340;&#26550;&#26500;&#12290;&#22312; ViT &#20013;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#25104;&#34917;&#19969;&#20196;&#29260;&#65292;&#24182;&#36890;&#36807;&#19968;&#22534;&#33258;&#25105;&#27880;&#24847;&#22359;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#19981;&#21516;&#65292;ViT &#30340;&#31616;&#21333;&#26550;&#26500;&#27809;&#26377;&#20449;&#24687;&#24615;&#24402;&#32435;&#20559;&#24046;&#65288;&#20363;&#22914;&#23616;&#37096;&#24615;&#31561;&#65289;&#12290;&#30001;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;ViT &#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#26041;&#27861;&#65288;DeiT&#65289;&#26469;&#26377;&#25928;&#22320;&#35757;&#32451;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;ViT&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#24456;&#23569;&#35752;&#35770;&#20351;&#29992;ViT&#26469;&#22788;&#29702;&#38271;&#23614;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;DeiT-LT&#26469;&#35299;&#20915;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#30340;ViT&#30340;&#38382;&#39064;&#12290;&#22312; DeiT-LT &#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36229;&#20986;&#20998;&#24067;&#22270;&#20687;&#21644;&#37325;&#26032;&#21152;&#26435;&#33976;&#39311;&#25439;&#22833;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33976;&#39311;&#26041;&#24335;&#65292;&#36890;&#36807;&#33976;&#39311; DIST &#20196;&#29260;&#20174;CNN&#36827;&#34892;&#33976;&#39311;&#65292;&#20197;&#22686;&#24378;&#23545;&#23614;&#37096;&#31867;&#21035;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02900v1 Announce Type: cross  Abstract: Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks. In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks. However, unlike Convolutional Neural Networks (CNN), ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a large amount of data for pre-training. Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively. However, limited literature discusses the use of ViT for datasets with long-tailed imbalances. In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an efficient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes. Thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38024;&#23545;&#25193;&#25955;&#22411;T2I&#27169;&#22411;&#36827;&#34892;&#28040;&#34701;&#23454;&#39564;&#65292;&#21457;&#29616;&#22686;&#21152;transformer&#22359;&#23545;&#20110;&#25913;&#21892;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#27604;&#22686;&#21152;&#36890;&#36947;&#25968;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.02883</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Scalability of Diffusion-based Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38024;&#23545;&#25193;&#25955;&#22411;T2I&#27169;&#22411;&#36827;&#34892;&#28040;&#34701;&#23454;&#39564;&#65292;&#21457;&#29616;&#22686;&#21152;transformer&#22359;&#23545;&#20110;&#25913;&#21892;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#27604;&#22686;&#21152;&#36890;&#36947;&#25968;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;LLMs&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#25193;&#23637;&#27861;&#21017;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#25193;&#23637;&#27169;&#22411;&#20197;&#22312;&#38477;&#20302;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#20063;&#19981;&#22826;&#28165;&#26970;&#12290;&#19981;&#21516;&#30340;&#35757;&#32451;&#35774;&#32622;&#21644;&#26114;&#36149;&#30340;&#35757;&#32451;&#25104;&#26412;&#20351;&#24471;&#36827;&#34892;&#20844;&#24179;&#30340;&#27169;&#22411;&#27604;&#36739;&#21464;&#24471;&#26497;&#20026;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21435;&#22122;&#39592;&#24178;&#21644;&#35757;&#32451;&#38598;&#30340;&#22823;&#37327;&#32780;&#20005;&#26684;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#23545;&#25193;&#25955;&#22411;T2I&#27169;&#22411;&#30340;&#25193;&#23637;&#29305;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21253;&#25324;&#22312;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;600M&#22270;&#20687;&#30340;&#33539;&#22260;&#20869;&#35757;&#32451;&#21442;&#25968;&#20174;0.4B&#21040;4B&#30340;&#32553;&#25918;UNet&#21644;Transformer&#21464;&#20307;&#12290;&#23545;&#20110;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#25105;&#20204;&#21457;&#29616;&#36328;&#20851;&#27880;&#30340;&#20301;&#32622;&#21644;&#25968;&#37327;&#21306;&#20998;&#20102;&#29616;&#26377;UNet&#35774;&#35745;&#30340;&#24615;&#33021;&#12290;&#22686;&#21152;transformer&#22359;&#23545;&#20110;&#25552;&#39640;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#27604;&#22686;&#21152;&#36890;&#36947;&#25968;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02883v1 Announce Type: cross  Abstract: Scaling up model and data size has been quite successful for the evolution of LLMs. However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored. It is also unclear how to efficiently scale the model for better performance at reduced cost. The different training settings and expensive training cost make a fair model comparison extremely difficult. In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images. For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs. And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel nu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.02877</link><description>&lt;p&gt;
FlightScope: &#21355;&#26143;&#22270;&#20687;&#20013;&#39134;&#34892;&#22120;&#26816;&#27979;&#31639;&#27861;&#30340;&#28145;&#24230;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#36965;&#24863;&#21355;&#26143;&#22270;&#20687;&#20013;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#29289;&#29702;&#23398;&#21644;&#29615;&#22659;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#19981;&#26029;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#22312;&#24120;&#35265;&#30340;&#22522;&#20110;&#22320;&#38754;&#25293;&#25668;&#30340;&#29031;&#29255;&#19978;&#23454;&#26045;&#21644;&#27979;&#35797;&#12290;&#26412;&#25991;&#23545;&#19968;&#22871;&#38024;&#23545;&#22312;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#36825;&#19968;&#20219;&#21153;&#23450;&#21046;&#30340;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#21033;&#29992;&#22823;&#22411;HRPlanesV2&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19982;GDIT&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;&#35813;&#30740;&#31350;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;YOLO&#29256;&#26412;5&#21644;8&#12289;Faster RCNN&#12289;CenterNet&#12289;RetinaNet&#12289;RTMDet&#21644;DETR&#65292;&#22343;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#12290;&#36825;&#39033;&#20840;&#38754;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;YOLOv5&#20316;&#20026;&#35782;&#21035;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#39134;&#26426;&#36825;&#19968;&#29305;&#23450;&#26696;&#20363;&#30340;&#21331;&#36234;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#30340;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 Announce Type: cross  Abstract: Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20248;&#21270;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20154;&#31867;&#35299;&#37322;&#21644;&#28436;&#31034;&#20934;&#30830;&#22320;&#36716;&#21270;&#20026;LTL&#35268;&#33539;&#12290;</title><link>https://arxiv.org/abs/2404.02872</link><description>&lt;p&gt;
&#25972;&#21512;&#35299;&#37322;&#22312;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;LTL&#35268;&#33539;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Integrating Explanations in Learning LTL Specifications from Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02872
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20248;&#21270;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20154;&#31867;&#35299;&#37322;&#21644;&#28436;&#31034;&#20934;&#30830;&#22320;&#36716;&#21270;&#20026;LTL&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26159;&#21542;&#26377;&#21161;&#20110;&#23558;&#20154;&#31867;&#35299;&#37322;&#36716;&#21270;&#20026;&#33021;&#22815;&#31283;&#20581;&#25903;&#25345;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#26684;&#24335;&#12290;LLMs&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#37117;&#21487;&#20197;&#20174;&#28436;&#31034;&#20013;&#25552;&#21462;LTL&#35268;&#33539;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#30528;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#12290;LLMs&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#24182;&#25972;&#21512;&#20154;&#31867;&#35299;&#37322;&#65292;&#20294;&#32570;&#20047;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#24418;&#24335;&#21270;&#20445;&#35777;&#65292;&#20294;&#26080;&#27861;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#24182;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24544;&#23454;&#22320;&#23558;&#20154;&#31867;&#35299;&#37322;&#21644;&#28436;&#31034;&#36716;&#21270;&#20026;LTL&#35268;&#33539;&#12290;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;Janaka&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02872v1 Announce Type: new  Abstract: This paper investigates whether recent advances in Large Language Models (LLMs) can assist in translating human explanations into a format that can robustly support learning Linear Temporal Logic (LTL) from demonstrations. Both LLMs and optimization-based methods can extract LTL specifications from demonstrations; however, they have distinct limitations. LLMs can quickly generate solutions and incorporate human explanations, but their lack of consistency and reliability hampers their applicability in safety-critical domains. On the other hand, optimization-based methods do provide formal guarantees but cannot process natural language explanations and face scalability challenges. We present a principled approach to combining LLMs and optimization-based methods to faithfully translate human explanations and demonstrations into LTL specifications. We have implemented a tool called Janaka based on our approach. Our experiments demonstrate th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#30340;&#21152;&#36895;&#24230;&#35745;&#25429;&#33719;&#19981;&#21516;&#26085;&#24120;&#27963;&#21160;&#30340;&#25968;&#25454;&#65292;&#25552;&#21462;&#29305;&#24449;&#24182;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#23454;&#26102;&#27963;&#21160;&#35782;&#21035;&#21644;&#21345;&#36335;&#37324;&#28040;&#32791;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2404.02869</link><description>&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition using Smartphones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#30340;&#21152;&#36895;&#24230;&#35745;&#25429;&#33719;&#19981;&#21516;&#26085;&#24120;&#27963;&#21160;&#30340;&#25968;&#25454;&#65292;&#25552;&#21462;&#29305;&#24449;&#24182;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#23454;&#26102;&#27963;&#21160;&#35782;&#21035;&#21644;&#21345;&#36335;&#37324;&#28040;&#32791;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26159;&#24403;&#21069;&#30340;&#30740;&#31350;&#28909;&#28857;&#20043;&#19968;&#65292;&#22312;&#36828;&#31243;&#21307;&#30103;&#12289;&#32769;&#24180;&#20154;&#25110;&#27531;&#38556;&#20154;&#22763;&#27963;&#21160;&#36319;&#36394;&#12289;&#28040;&#32791;&#21345;&#36335;&#37324;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#25105;&#20204;&#30340;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;Android&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#23454;&#26102;&#35782;&#21035;&#26085;&#24120;&#20154;&#31867;&#27963;&#21160;&#24182;&#35745;&#31639;&#28040;&#32791;&#30340;&#21345;&#36335;&#37324;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#30340;&#20869;&#32622;&#21152;&#36895;&#24230;&#35745;&#25429;&#33719;&#20102;&#19981;&#21516;&#26085;&#24120;&#20154;&#31867;&#27963;&#21160;&#30340;&#26631;&#35760;&#19977;&#36724;&#21152;&#36895;&#24230;&#35835;&#25968;&#12290;&#28982;&#21518;&#20351;&#29992;&#20013;&#20540;&#28388;&#27874;&#23545;&#36825;&#20123;&#35835;&#25968;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#25552;&#21462;&#20102;42&#20010;&#29305;&#24449;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#38477;&#32500;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#22312;&#25105;&#20204;&#30340;Android&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#25552;&#20379;&#20102;&#26368;&#39640;&#20934;&#30830;&#24615;&#21644;&#26368;&#30701;&#27169;&#22411;&#26500;&#24314;&#26102;&#38388;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#36825;&#29992;&#20110;&#23454;&#26102;&#27963;&#21160;&#35782;&#21035;&#21644;&#21033;&#29992;&#22522;&#20110;&#20195;&#35874;&#30340;&#20844;&#24335;&#35745;&#31639;&#28040;&#32791;&#30340;&#21345;&#36335;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02869v1 Announce Type: cross  Abstract: Human Activity Recognition is a subject of great research today and has its applications in remote healthcare, activity tracking of the elderly or the disables, calories burnt tracking etc. In our project, we have created an Android application that recognizes the daily human activities and calculate the calories burnt in real time. We first captured labeled triaxial acceleration readings for different daily human activities from the smartphone's embedded accelerometer. These readings were preprocessed using a median filter. 42 features were extracted using various methods. We then tested various machine learning algorithms along with dimensionality reduction. Finally, in our Android application, we used the machine learning algorithm and a subset of features that provided maximum accuracy and minimum model building time. This is used for real-time activity recognition and calculation of calories burnt using a formula based on Metaboli
&lt;/p&gt;</description></item><item><title>I-Design&#26159;&#19968;&#20010;&#20010;&#24615;&#21270;&#23460;&#20869;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#29983;&#25104;&#21644;&#21487;&#35270;&#21270;&#29992;&#25143;&#35774;&#35745;&#30446;&#26631;&#65292;&#20174;&#32780;&#20351;&#23460;&#20869;&#35774;&#35745;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02838</link><description>&lt;p&gt;
I-Design: &#20010;&#24615;&#21270;&#30340;LLM&#23460;&#20869;&#35774;&#35745;&#24072;
&lt;/p&gt;
&lt;p&gt;
I-Design: Personalized LLM Interior Designer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02838
&lt;/p&gt;
&lt;p&gt;
I-Design&#26159;&#19968;&#20010;&#20010;&#24615;&#21270;&#23460;&#20869;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#29983;&#25104;&#21644;&#21487;&#35270;&#21270;&#29992;&#25143;&#35774;&#35745;&#30446;&#26631;&#65292;&#20174;&#32780;&#20351;&#23460;&#20869;&#35774;&#35745;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#35774;&#35745;&#35753;&#25105;&#20204;&#23637;&#31034;&#30495;&#23454;&#30340;&#33258;&#25105;&#65292;&#29983;&#27963;&#26041;&#24335;&#20063;&#30001;&#27492;&#23637;&#29616;&#8212;&#8212;&#27599;&#20010;&#35774;&#35745;&#37117;&#22914;&#21516;&#25105;&#20204;&#21508;&#33258;&#29420;&#29305;&#30340;&#20010;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#19987;&#19994;&#20154;&#21592;&#26469;&#35828;&#65292;&#34920;&#36798;&#21644;&#23454;&#29616;&#36825;&#19968;&#28857;&#24182;&#19981;&#26159;&#20214;&#36731;&#26494;&#30340;&#20107;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#23558;&#21151;&#33021;&#24615;&#21644;&#35270;&#35273;&#26399;&#26395;&#19982;&#29289;&#29702;&#31354;&#38388;&#30340;&#38480;&#21046;&#30456;&#21327;&#35843;&#65307;&#36825;&#20351;&#24471;&#23460;&#20869;&#35774;&#35745;&#25104;&#20026;&#19968;&#31181;&#22882;&#20360;&#12290;&#20026;&#20102;&#35753;&#23427;&#26356;&#26131;&#20110;&#33719;&#24471;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-Design&#65292;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#23460;&#20869;&#35774;&#35745;&#24072;&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#29983;&#25104;&#21644;&#21487;&#35270;&#21270;&#20182;&#20204;&#30340;&#35774;&#35745;&#30446;&#26631;&#12290;I-Design&#20174;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22242;&#38431;&#24320;&#22987;&#65292;&#20182;&#20204;&#36890;&#36807;&#23545;&#35805;&#21644;&#36923;&#36753;&#25512;&#29702;&#30456;&#20114;&#20132;&#20114;&#65292;&#23558;&#25991;&#26412;&#29992;&#25143;&#36755;&#20837;&#36716;&#25442;&#20026;&#20855;&#26377;&#30456;&#23545;&#23545;&#35937;&#20851;&#31995;&#30340;&#21487;&#34892;&#22330;&#26223;&#22270;&#35774;&#35745;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#25918;&#32622;&#31639;&#27861;&#30830;&#23450;&#20102;&#22330;&#26223;&#20869;&#27599;&#20010;&#23545;&#35937;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#26368;&#32456;&#35774;&#35745;&#20250;&#36890;&#36807;&#20174;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02838v1 Announce Type: new  Abstract: Interior design allows us to be who we are and live how we want - each design is as unique as our distinct personality. However, it is not trivial for non-professionals to express and materialize this since it requires aligning functional and visual expectations with the constraints of physical space; this renders interior design a luxury. To make it more accessible, we present I-Design, a personalized interior designer that allows users to generate and visualize their design goals through natural language communication. I-Design starts with a team of large language model agents that engage in dialogues and logical reasoning with one another, transforming textual user input into feasible scene graph designs with relative object relationships. Subsequently, an effective placement algorithm determines optimal locations for each object within the scene. The final design is then constructed in 3D by retrieving and integrating assets from an 
&lt;/p&gt;</description></item><item><title>AI&#31185;&#23398;&#23478;&#20195;&#29702;&#32467;&#21512;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36171;&#33021;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#65292;&#23545;&#22810;&#20010;&#39046;&#22495;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02831</link><description>&lt;p&gt;
&#21033;&#29992;AI&#20195;&#29702;&#36171;&#33021;&#29983;&#29289;&#21307;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empowering Biomedical Discovery with AI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02831
&lt;/p&gt;
&lt;p&gt;
AI&#31185;&#23398;&#23478;&#20195;&#29702;&#32467;&#21512;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36171;&#33021;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#65292;&#23545;&#22810;&#20010;&#39046;&#22495;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#24819;&#8220;AI&#31185;&#23398;&#23478;&#8221;&#20316;&#20026;&#31995;&#32479;&#65292;&#33021;&#22815;&#36827;&#34892;&#24576;&#30097;&#24615;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#19982;&#23454;&#39564;&#24179;&#21488;&#38598;&#25104;&#30340;&#21327;&#20316;&#20195;&#29702;&#65292;&#36171;&#33021;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#12290;&#29983;&#29289;&#21307;&#23398;AI&#20195;&#29702;&#19981;&#26159;&#35201;&#21076;&#38500;&#20154;&#31867;&#22312;&#21457;&#29616;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65292;&#32780;&#26159;&#32467;&#21512;&#20154;&#31867;&#30340;&#21019;&#36896;&#21147;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#21033;&#29992;AI&#20998;&#26512;&#22823;&#22411;&#25968;&#25454;&#38598;&#12289;&#23548;&#33322;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#25191;&#34892;&#37325;&#22797;&#24615;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#20195;&#29702;&#25797;&#38271;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#33258;&#25105;&#35780;&#20272;&#21644;&#35268;&#21010;&#21457;&#29616;&#24037;&#20316;&#27969;&#31243;&#12290;&#23427;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#32467;&#26500;&#21270;&#35760;&#24518;&#65292;&#20197;&#20415;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#25972;&#21512;&#31185;&#23398;&#30693;&#35782;&#12289;&#29983;&#29289;&#21407;&#29702;&#21644;&#29702;&#35770;&#12290;AI&#20195;&#29702;&#21487;&#20197;&#24433;&#21709;&#20174;&#28151;&#21512;&#32454;&#32990;&#27169;&#25311;&#12289;&#21487;&#32534;&#31243;&#25511;&#21046;&#34920;&#22411;&#21040;&#32454;&#32990;&#30005;&#36335;&#35774;&#35745;&#20197;&#21450;&#26032;&#30103;&#27861;&#24320;&#21457;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02831v1 Announce Type: new  Abstract: We envision 'AI scientists' as systems capable of skeptical learning and reasoning that empower biomedical research through collaborative agents that integrate machine learning tools with experimental platforms. Rather than taking humans out of the discovery process, biomedical AI agents combine human creativity and expertise with AI's ability to analyze large datasets, navigate hypothesis spaces, and execute repetitive tasks. AI agents are proficient in a variety of tasks, including self-assessment and planning of discovery workflows. These agents use large language models and generative models to feature structured memory for continual learning and use machine learning tools to incorporate scientific knowledge, biological principles, and theories. AI agents can impact areas ranging from hybrid cell simulation, programmable control of phenotypes, and the design of cellular circuits to the development of new therapies.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;ProtoVerse&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26894;&#20307;&#39592;&#25240;&#30340;&#20998;&#31867;&#20915;&#31574;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02830</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#21407;&#22411;&#25552;&#21319;&#26894;&#20307;&#39592;&#25240;&#20998;&#32423;&#30340;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Interpretability of Vertebrae Fracture Grading using Human-interpretable Prototypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02830
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ProtoVerse&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26894;&#20307;&#39592;&#25240;&#30340;&#20998;&#31867;&#20915;&#31574;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26894;&#20307;&#39592;&#25240;&#20998;&#32423;&#20998;&#31867;&#39592;&#25240;&#20005;&#37325;&#31243;&#24230;&#65292;&#36825;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#12290;&#23613;&#31649;DL&#36741;&#21161;&#21307;&#23398;&#35786;&#26029;&#31561;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#36879;&#26126;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#23581;&#35797;&#20351;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#35201;&#20040;&#20381;&#36182;&#20110;&#20107;&#21518;&#26041;&#27861;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#39069;&#22806;&#27880;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;-by-design&#26041;&#27861;ProtoVerse&#65292;&#20197;&#22312;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#26041;&#24335;&#20013;&#25214;&#21040;&#30456;&#20851;&#30340;&#26894;&#20307;&#39592;&#25240;&#23376;&#37096;&#20998;&#65288;&#21407;&#22411;&#65289;&#65292;&#21487;&#21487;&#38752;&#22320;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26679;&#24615;&#20419;&#36827;&#25439;&#22833;&#65292;&#20197;&#20943;&#36731;&#22312;&#20855;&#26377;&#22797;&#26434;&#35821;&#20041;&#30340;&#23567;&#25968;&#25454;&#38598;&#20013;&#21407;&#22411;&#37325;&#22797;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;VerSe'19&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02830v1 Announce Type: cross  Abstract: Vertebral fracture grading classifies the severity of vertebral fractures, which is a challenging task in medical imaging and has recently attracted Deep Learning (DL) models. Only a few works attempted to make such models human-interpretable despite the need for transparency and trustworthiness in critical use cases like DL-assisted medical diagnosis. Moreover, such models either rely on post-hoc methods or additional annotations. In this work, we propose a novel interpretable-by-design method, ProtoVerse, to find relevant sub-parts of vertebral fractures (prototypes) that reliably explain the model's decision in a human-understandable way. Specifically, we introduce a novel diversity-promoting loss to mitigate prototype repetitions in small datasets with intricate semantics. We have experimented with the VerSe'19 dataset and outperformed the existing prototype-based method. Further, our model provides superior interpretability agains
&lt;/p&gt;</description></item><item><title>Conifer&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;LLMs&#39537;&#21160;&#30340;&#32454;&#21270;&#36807;&#31243;&#65292;&#20197;&#21450;&#28176;&#36827;&#23398;&#20064;&#26041;&#26696;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20855;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#22810;&#23618;&#25351;&#20196;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2404.02823</link><description>&lt;p&gt;
Conifer: &#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#32422;&#26463;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02823
&lt;/p&gt;
&lt;p&gt;
Conifer&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;LLMs&#39537;&#21160;&#30340;&#32454;&#21270;&#36807;&#31243;&#65292;&#20197;&#21450;&#28176;&#36827;&#23398;&#20064;&#26041;&#26696;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20855;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#22810;&#23618;&#25351;&#20196;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#19968;&#20123;&#30740;&#31350;&#25351;&#20986;&#65292;LLMs&#22312;&#38754;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#25351;&#20196;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#22797;&#26434;&#32422;&#26463;&#30340;&#25351;&#20196;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Conifer&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#36981;&#24490;&#20855;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#22810;&#23618;&#25351;&#20196;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;LLM&#39537;&#21160;&#30340;&#32454;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#31574;&#21010;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#35843;&#26131;&#20110;&#38590;&#30340;&#28176;&#36827;&#23398;&#20064;&#26041;&#26696;&#65292;&#24182;&#20174;&#36807;&#31243;&#21453;&#39304;&#20013;&#23398;&#20064;&#12290;&#20351;&#29992;Conifer&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#21892;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24102;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#25351;&#20196;&#12290;&#22312;&#20960;&#20010;&#36981;&#24490;&#25351;&#20196;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;7B&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02823v1 Announce Type: cross  Abstract: The ability of large language models (LLMs) to follow instructions is crucial to real-world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi-level instructions with complex constraints. Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints. On several instruction-following benchmarks, our 7B model outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02817</link><description>&lt;p&gt;
&#20248;&#21270;&#22411;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#32508;&#36848;&#65306;&#20174;&#32463;&#20856;&#21040;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#23558;&#39640;&#23618;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#23618;&#36816;&#21160;&#35268;&#21010;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#35299;&#20915;&#38271;&#26102;&#22495;&#12289;&#21160;&#24577;&#20219;&#21153;&#12290;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#19987;&#27880;&#20110;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30446;&#26631;&#26465;&#20214;&#30340;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#24320;&#25918;&#24335;&#30446;&#26631;&#12289;&#26426;&#22120;&#20154;&#21160;&#24577;&#21644;&#26426;&#22120;&#20154;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#29289;&#29702;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#29305;&#21035;&#36866;&#21512;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#65292;&#28085;&#30422;&#20102;&#65288;i&#65289;&#35268;&#21010;&#39046;&#22495;&#34920;&#31034;&#65292;&#21253;&#25324;&#21160;&#20316;&#25551;&#36848;&#35821;&#35328;&#21644;&#26102;&#24577;&#36923;&#36753;&#65292;&#65288;ii&#65289;TAMP&#21508;&#32452;&#20214;&#30340;&#20010;&#21035;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;&#20154;&#24037;&#26234;&#33021;&#35268;&#21010;&#21644;&#36712;&#36857;&#20248;&#21270;&#65288;TO&#65289;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22522;&#20110;&#36923;&#36753;&#30340;&#20219;&#21153;&#35268;&#21010;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;TO&#20043;&#38388;&#30340;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02817v1 Announce Type: cross  Abstract: Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#26377;&#38480;&#20803;&#20998;&#26512;&#26694;&#26550;&#26469;&#20010;&#24615;&#21270;&#20272;&#35745;&#24515;&#33039;&#32452;&#32455;&#30340;&#34987;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#36890;&#36807;&#23884;&#22871;&#20248;&#21270;&#26041;&#26696;&#30340;&#20351;&#29992;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36924;&#36817;&#21305;&#37197;&#22270;&#20687;&#25968;&#25454;&#30340;&#26448;&#26009;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.02807</link><description>&lt;p&gt;
&#19968;&#20010;&#20010;&#24615;&#21270;&#34987;&#21160;&#24515;&#33039;&#21147;&#23398;&#30340;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Optimization Framework to Personalize Passive Cardiac Mechanics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#26377;&#38480;&#20803;&#20998;&#26512;&#26694;&#26550;&#26469;&#20010;&#24615;&#21270;&#20272;&#35745;&#24515;&#33039;&#32452;&#32455;&#30340;&#34987;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#36890;&#36807;&#23884;&#22871;&#20248;&#21270;&#26041;&#26696;&#30340;&#20351;&#29992;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36924;&#36817;&#21305;&#37197;&#22270;&#20687;&#25968;&#25454;&#30340;&#26448;&#26009;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#24515;&#33039;&#21147;&#23398;&#24314;&#27169;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#24515;&#33039;&#21151;&#33021;&#22312;&#20581;&#24247;&#21644;&#30142;&#30149;&#20013;&#30340;&#29983;&#29289;&#21147;&#23398;&#24182;&#24110;&#21161;&#27835;&#30103;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#20165;&#38480;&#20110;&#20351;&#29992;&#22312;&#21333;&#19968;&#24515;&#33039;&#30456;&#20301;&#33719;&#21462;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#36890;&#24120;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#20687;&#33719;&#21462;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36870;&#26377;&#38480;&#20803;&#20998;&#26512;&#65288;iFEA&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#26469;&#20272;&#35745;&#24515;&#33039;&#32452;&#32455;&#30340;&#34987;&#21160;&#26426;&#26800;&#29305;&#24615;&#12290;&#35813;iFEA&#26694;&#26550;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#22871;&#20248;&#21270;&#26041;&#26696;&#65292;&#20854;&#20013;&#22806;&#37096;&#36845;&#20195;&#21033;&#29992;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#26469;&#26368;&#20339;&#36924;&#36817;&#21305;&#37197;&#22270;&#20687;&#25968;&#25454;&#30340;&#26448;&#26009;&#21442;&#25968;&#65292;&#32780;&#20869;&#37096;&#36845;&#20195;&#37319;&#29992;&#22686;&#24191;Sellier&#31639;&#27861;&#26469;&#20272;&#35745;&#26080;&#24212;&#21147;&#21442;&#32771;&#26500;&#22411;&#12290;&#37325;&#28857;&#25918;&#22312;&#34920;&#24449;&#34987;&#21160;&#26426;&#26800;&#34892;&#20026;&#19978;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02807v1 Announce Type: cross  Abstract: Personalized cardiac mechanics modeling is a powerful tool for understanding the biomechanics of cardiac function in health and disease and assisting in treatment planning. However, current models are limited to using medical images acquired at a single cardiac phase, often limiting their applicability for processing dynamic image acquisitions. This study introduces an inverse finite element analysis (iFEA) framework to estimate the passive mechanical properties of cardiac tissue using time-dependent medical image data. The iFEA framework relies on a novel nested optimization scheme, in which the outer iterations utilize a traditional optimization method to best approximate material parameters that fit image data, while the inner iterations employ an augmented Sellier's algorithm to estimate the stress-free reference configuration. With a focus on characterizing the passive mechanical behavior, the framework employs structurally based 
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#31243;&#24207;&#21592;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;RealHumanEval&#20316;&#20026;&#34913;&#37327;&#20854;&#24110;&#21161;&#24615;&#30340;&#30028;&#38754;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#23545;&#31243;&#24207;&#21592;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02806</link><description>&lt;p&gt;
RealHumanEval: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#31243;&#24207;&#21592;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02806
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#31243;&#24207;&#21592;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;RealHumanEval&#20316;&#20026;&#34913;&#37327;&#20854;&#24110;&#21161;&#24615;&#30340;&#30028;&#38754;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#23545;&#31243;&#24207;&#21592;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#20381;&#36182;&#20110;&#38745;&#24577;&#22522;&#20934;&#65292;&#21253;&#25324;HumanEval&#65288;Chen&#31561;&#65292;2021&#65289;&#65292;&#36825;&#20123;&#22522;&#20934;&#29992;&#20110;&#34913;&#37327;LLMs&#29983;&#25104;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;&#23436;&#25972;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#31243;&#24207;&#21592;&#21161;&#25163;&#65292;&#25105;&#20204;&#30740;&#31350;&#29616;&#26377;&#22522;&#20934;&#19978;&#30340;&#22686;&#30410;&#26159;&#21542;&#33021;&#36716;&#21270;&#20026;&#20351;&#29992;LLMs&#32534;&#30721;&#26102;&#31243;&#24207;&#21592;&#29983;&#20135;&#21147;&#30340;&#25552;&#21319;&#65292;&#21253;&#25324;&#32534;&#30721;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#12290;&#38500;&#20102;&#38745;&#24577;&#22522;&#20934;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21487;&#33021;&#29992;&#20316;&#24230;&#37327;LLM&#24110;&#21161;&#24615;&#20195;&#29702;&#30340;&#20559;&#22909;&#24230;&#37327;&#30340;&#23454;&#29992;&#24615;&#65292;&#20363;&#22914;&#20195;&#30721;&#25509;&#21463;&#25110;&#22797;&#21046;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RealHumanEval&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;LLMs&#36741;&#21161;&#31243;&#24207;&#21592;&#30340;&#33021;&#21147;&#30340;&#32593;&#32476;&#30028;&#38754;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#23436;&#25104;&#25110;&#32842;&#22825;&#25903;&#25345;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65288;N = 213&#65289;&#65292;&#20351;&#29992;RealHumanEval&#65292;&#20854;&#20013;&#29992;&#25143;&#19982;&#20845;&#20010;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#21508;&#24322;&#30340;LLMs&#36827;&#34892;&#20132;&#20114;&#12290;&#23613;&#31649;&#38745;&#24577;&#22522;&#20934;&#27809;&#26377;&#21253;&#21547;&#20154;&#20026;&#24178;&#39044;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02806v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#25511;&#21046;&#20174;&#20799;&#31461;&#21465;&#20107;&#25991;&#26412;&#20013;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#26088;&#22312;&#25511;&#21046;&#38382;&#39064;&#30340;&#26126;&#30830;&#24615;&#21644;&#28508;&#22312;&#21465;&#20107;&#20803;&#32032;&#65292;&#36890;&#36807;&#19982;&#21442;&#32771;&#27169;&#22411;&#24182;&#34892;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#35821;&#20041;&#25509;&#36817;&#24230;&#35780;&#20272;&#21644;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#22810;&#26679;&#24615;&#21644;&#36830;&#36143;&#24615;&#26041;&#38754;&#65292;&#23569;&#26679;&#26412;&#31574;&#30053;&#36229;&#36234;&#20102;&#21442;&#32771;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.02800</link><description>&lt;p&gt;
&#26377;&#20851;&#21465;&#20107;&#29702;&#35299;&#20013;&#21487;&#25511;&#38382;&#39064;&#22238;&#31572;&#29983;&#25104;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02800
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#25511;&#21046;&#20174;&#20799;&#31461;&#21465;&#20107;&#25991;&#26412;&#20013;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#26088;&#22312;&#25511;&#21046;&#38382;&#39064;&#30340;&#26126;&#30830;&#24615;&#21644;&#28508;&#22312;&#21465;&#20107;&#20803;&#32032;&#65292;&#36890;&#36807;&#19982;&#21442;&#32771;&#27169;&#22411;&#24182;&#34892;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#35821;&#20041;&#25509;&#36817;&#24230;&#35780;&#20272;&#21644;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#22810;&#26679;&#24615;&#21644;&#36830;&#36143;&#24615;&#26041;&#38754;&#65292;&#23569;&#26679;&#26412;&#31574;&#30053;&#36229;&#36234;&#20102;&#21442;&#32771;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#29983;&#25104;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#12290;&#21487;&#25511;&#38382;&#39064;&#29983;&#25104;&#26041;&#26696;&#20391;&#37325;&#20110;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#25511;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#29992;&#20110;&#25511;&#21046;&#20174;&#20799;&#31461;&#21465;&#20107;&#25991;&#26412;&#20013;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#26088;&#22312;&#25511;&#21046;&#20004;&#20010;&#23646;&#24615;&#65306;&#38382;&#39064;&#30340;&#26126;&#30830;&#24615;&#21644;&#28508;&#22312;&#21465;&#20107;&#20803;&#32032;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#37319;&#29992;&#23569;&#26679;&#26412;&#25552;&#31034;&#19982;&#21442;&#32771;&#27169;&#22411;&#24182;&#34892;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#23569;&#26679;&#26412;&#31574;&#30053;&#36229;&#36234;&#21442;&#32771;&#27169;&#22411;&#30340;&#23454;&#20363;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#20041;&#25509;&#36817;&#24230;&#35780;&#20272;&#20197;&#21450;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#22810;&#26679;&#24615;&#21644;&#36830;&#36143;&#24615;&#31561;&#22330;&#26223;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25913;&#36827;&#24182;&#19981;&#24635;&#26159;&#32479;&#35745;&#19978;&#26174;&#33879;&#30340;&#12290;&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02800v1 Announce Type: cross  Abstract: Question Generation aims to automatically generate questions based on a given input provided as context. A controllable question generation scheme focuses on generating questions with specific attributes, allowing better control. In this study, we propose a few-shot prompting strategy for controlling the generation of question-answer pairs from children's narrative texts. We aim to control two attributes: the question's explicitness and underlying narrative elements. With empirical evaluation, we show the effectiveness of controlling the generation process by employing few-shot prompting side by side with a reference model. Our experiments highlight instances where the few-shot strategy surpasses the reference model, particularly in scenarios such as semantic closeness evaluation and the diversity and coherency of question-answer pairs. However, these improvements are not always statistically significant. The code is publicly available
&lt;/p&gt;</description></item><item><title>&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2404.02785</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization through Meta-Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02785
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#26159;&#24403;&#38754;&#23545;&#20998;&#24067;&#20043;&#22806;(out-of-distribution, OOD)&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#22240;&#20026;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#19981;&#21487;&#36991;&#20813;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#34987;&#20551;&#23450;&#20026;&#20849;&#20139;&#30456;&#21516;&#20998;&#24067;&#30340;&#24120;&#35265;&#24773;&#20917;&#12290;&#23613;&#31649;DNNs&#22312;&#22823;&#37327;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#24212;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#20803;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#30340;&#31639;&#27861;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#38656;&#35201;&#20174;&#22836;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#20803;&#23398;&#20064;&#39046;&#22495;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02785v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02761</link><description>&lt;p&gt;
AQuA --&#32467;&#21512;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#35266;&#28857;&#65292;&#21033;&#29992;LLMs&#35780;&#20272;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#30923;&#21830;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25919;&#27835;&#22312;&#32447;&#35752;&#35770;&#20013;&#34913;&#37327;&#36129;&#29486;&#36136;&#37327;&#23545;&#20110;&#30740;&#31350;&#30923;&#21830;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#33258;&#21160;&#34913;&#37327;&#36825;&#20123;&#25351;&#26631;&#21464;&#24471;&#21487;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AQuA&#65292;&#23427;&#26159;&#19968;&#20010;&#28155;&#21152;&#20998;&#25968;&#65292;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#35745;&#31639;&#27599;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#12290;&#19982;&#20854;&#20182;&#29305;&#23450;&#20998;&#25968;&#19981;&#21516;&#65292;AQuA&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#23384;&#22312;&#30340;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02761v1 Announce Type: cross  Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices int
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31232;&#30095;&#36755;&#20837;&#23398;&#20064;&#21344;&#29992;&#22330;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#36793;&#30028;&#19981;&#30830;&#23450;&#24615;&#30340;&#37319;&#26679;&#21644;&#26368;&#23567;&#29109;&#22330;&#20248;&#21270;&#26469;&#35299;&#20915;&#20174;3D&#28857;&#20113;&#23398;&#20064;SDF&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.02759</link><description>&lt;p&gt;
&#26469;&#33258;&#31232;&#30095;&#28857;&#20113;&#30340;&#26080;&#30417;&#30563;&#21344;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Occupancy Learning from Sparse Point Cloud
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02759
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31232;&#30095;&#36755;&#20837;&#23398;&#20064;&#21344;&#29992;&#22330;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#36793;&#30028;&#19981;&#30830;&#23450;&#24615;&#30340;&#37319;&#26679;&#21644;&#26368;&#23567;&#29109;&#22330;&#20248;&#21270;&#26469;&#35299;&#20915;&#20174;3D&#28857;&#20113;&#23398;&#20064;SDF&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#25429;&#33719;&#22797;&#26434;&#30340;&#25968;&#25454;&#27169;&#24577;&#65292;&#28085;&#30422;&#20174;3D&#24418;&#29366;&#21040;&#22270;&#20687;&#21644;&#38899;&#39057;&#31561;&#24191;&#27867;&#33539;&#22260;&#12290;&#22312;3D&#24418;&#29366;&#34920;&#31034;&#39046;&#22495;&#65292;&#31070;&#32463;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#23637;&#29616;&#20986;&#20174;&#26681;&#26412;&#19978;&#32534;&#30721;&#22797;&#26434;&#24418;&#29366;&#20960;&#20309;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#22320;&#38754;&#30495;&#23454;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;3D&#28857;&#20113;&#20013;&#23398;&#20064;SDF&#20173;&#28982;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25512;&#26029;&#21344;&#29992;&#22330;&#32780;&#19981;&#26159;SDF&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#23481;&#26131;&#20174;&#31232;&#30095;&#36755;&#20837;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#36793;&#30028;&#19981;&#30830;&#23450;&#24615;&#30340;&#36793;&#38469;&#27979;&#37327;&#65292;&#19981;&#21516;&#22320;&#20174;&#21344;&#29992;&#20989;&#25968;&#30340;&#20915;&#31574;&#36793;&#30028;&#20013;&#37319;&#26679;&#65292;&#24182;&#20351;&#29992;&#36755;&#20837;&#28857;&#20113;&#30417;&#30563;&#37319;&#26679;&#30340;&#36793;&#30028;&#28857;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#36890;&#36807;&#23558;&#21344;&#29992;&#20989;&#25968;&#20559;&#21521;&#26368;&#23567;&#29109;&#22330;&#26469;&#36827;&#19968;&#27493;&#31283;&#23450;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02759v1 Announce Type: cross  Abstract: Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. However, learning SDFs from 3D point clouds in the absence of ground truth supervision remains a very challenging task. In this paper, we propose a method to infer occupancy fields instead of SDFs as they are easier to learn from sparse inputs. We leverage a margin-based uncertainty measure to differentially sample from the decision boundary of the occupancy function and supervise the sampled boundary points using the input point cloud. We further stabilize the optimization process at the early stages of the training by biasing the occupancy function towards minimal entropy fields while max
&lt;/p&gt;</description></item><item><title>DIBS&#26159;&#19968;&#31181;&#29992;&#20110;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#65288;DVC&#65289;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#30340;&#20107;&#20214;&#23383;&#24149;&#21644;&#20266;&#20107;&#20214;&#36793;&#30028;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#22312;&#32447;&#36793;&#30028;&#32454;&#21270;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#26410;&#26631;&#27880;&#35270;&#39057;&#19978;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02755</link><description>&lt;p&gt;
&#36890;&#36807;&#20266;&#36793;&#30028;&#20016;&#23500;&#21644;&#22312;&#32447;&#32454;&#21270;&#25552;&#39640;&#26080;&#26631;&#27880;&#35270;&#39057;&#30340;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#36136;&#37327;&#30340; DIBS: &#29992;&#26410;&#26631;&#30340;&#35270;&#39057;&#22686;&#24378;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02755
&lt;/p&gt;
&lt;p&gt;
DIBS&#26159;&#19968;&#31181;&#29992;&#20110;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#65288;DVC&#65289;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#30340;&#20107;&#20214;&#23383;&#24149;&#21644;&#20266;&#20107;&#20214;&#36793;&#30028;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#22312;&#32447;&#36793;&#30028;&#32454;&#21270;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#26410;&#26631;&#27880;&#35270;&#39057;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; Dive Into the BoundarieS (DIBS)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#65288;DVC&#65289;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#22914;&#20309;&#20174;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20013;&#25552;&#39640;&#29983;&#25104;&#30340;&#20107;&#20214;&#23383;&#24149;&#21450;&#20854;&#20851;&#32852;&#20266;&#20107;&#20214;&#36793;&#30028;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#29983;&#25104;&#20016;&#23500;&#30340;&#38754;&#21521;DVC&#30340;&#23383;&#24149;&#20505;&#36873;&#39033;&#65292;&#24182;&#26681;&#25454;&#20960;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#30446;&#26631;&#20248;&#21270;&#30456;&#24212;&#30340;&#20266;&#36793;&#30028;&#65292;&#32771;&#34385;&#21040;&#22810;&#26679;&#24615;&#12289;&#20107;&#20214;&#20013;&#24515;&#24615;&#12289;&#26102;&#38388;&#25490;&#24207;&#21644;&#36830;&#36143;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#36793;&#30028;&#32454;&#21270;&#31574;&#30053;&#65292;&#36845;&#20195;&#22320;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#39640;&#20266;&#36793;&#30028;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#26469;&#26816;&#39564;&#25152;&#25552;&#20986;&#25216;&#26415;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#25968;&#25454;&#65292;&#22914; HowTo100M&#65292;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340; DVC &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02755v1 Announce Type: cross  Abstract: We present Dive Into the BoundarieS (DIBS), a novel pretraining framework for dense video captioning (DVC), that elaborates on improving the quality of the generated event captions and their associated pseudo event boundaries from unlabeled videos. By leveraging the capabilities of diverse large language models (LLMs), we generate rich DVC-oriented caption candidates and optimize the corresponding pseudo boundaries under several meticulously designed objectives, considering diversity, event-centricity, temporal ordering, and coherence. Moreover, we further introduce a novel online boundary refinement strategy that iteratively improves the quality of pseudo boundaries during training. Comprehensive experiments have been conducted to examine the effectiveness of the proposed technique components. By leveraging a substantial amount of unlabeled video data, such as HowTo100M, we achieve a remarkable advancement on standard DVC datasets lik
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#36882;&#24402;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#24207;&#21015;&#21560;&#24341;&#23376;&#65292;&#20197;&#31283;&#20581;&#22320;&#23384;&#20648;&#21644;&#26816;&#32034;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#24207;&#21015;&#65292;&#32467;&#26524;&#34920;&#26126;&#32593;&#32476;&#38656;&#35201;&#21253;&#21547;&#38544;&#34255;&#31070;&#32463;&#20803;&#26469;&#23384;&#20648;&#20219;&#24847;&#27169;&#24335;&#24207;&#21015;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23616;&#37096;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2404.02729</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#36882;&#24402;&#32593;&#32476;&#20013;&#23398;&#20064;&#24207;&#21015;&#21560;&#24341;&#23376;
&lt;/p&gt;
&lt;p&gt;
Learning Sequence Attractors in Recurrent Networks with Hidden Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#36882;&#24402;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#24207;&#21015;&#21560;&#24341;&#23376;&#65292;&#20197;&#31283;&#20581;&#22320;&#23384;&#20648;&#21644;&#26816;&#32034;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#24207;&#21015;&#65292;&#32467;&#26524;&#34920;&#26126;&#32593;&#32476;&#38656;&#35201;&#21253;&#21547;&#38544;&#34255;&#31070;&#32463;&#20803;&#26469;&#23384;&#20648;&#20219;&#24847;&#27169;&#24335;&#24207;&#21015;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23616;&#37096;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#34987;&#35774;&#35745;&#29992;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22823;&#33041;&#26159;&#22914;&#20309;&#23398;&#20064;&#23384;&#20648;&#21644;&#26816;&#32034;&#24207;&#21015;&#35760;&#24518;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20108;&#36827;&#21046;&#31070;&#32463;&#20803;&#30340;&#36882;&#24402;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#24207;&#21015;&#21560;&#24341;&#23376;&#65292;&#20197;&#23384;&#20648;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#24207;&#21015;&#24182;&#31283;&#20581;&#22320;&#26816;&#32034;&#23427;&#20204;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20026;&#20102;&#23384;&#20648;&#20219;&#24847;&#27169;&#24335;&#24207;&#21015;&#65292;&#32593;&#32476;&#38656;&#35201;&#21253;&#21547;&#38544;&#34255;&#31070;&#32463;&#20803;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#26174;&#31034;&#24207;&#21015;&#35760;&#24518;&#26041;&#38754;&#30340;&#20316;&#29992;&#26159;&#38388;&#25509;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23616;&#37096;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#32593;&#32476;&#20013;&#30340;&#24207;&#21015;&#21560;&#24341;&#23376;&#12290;&#35813;&#31639;&#27861;&#34987;&#35777;&#26126;&#20250;&#25910;&#25947;&#24182;&#23548;&#33268;&#24207;&#21015;&#21560;&#24341;&#23376;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#31283;&#20581;&#22320;&#23384;&#20648;&#21644;&#26816;&#32034;&#24207;&#21015;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#33021;&#22815;&#20026;&#29702;&#35299;&#22823;&#33041;&#20013;&#30340;&#24207;&#21015;&#35760;&#24518;&#21644;&#26102;&#38388;&#20449;&#24687;&#22788;&#29702;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02729v1 Announce Type: cross  Abstract: The brain is targeted for processing temporal sequence information. It remains largely unclear how the brain learns to store and retrieve sequence memories. Here, we study how recurrent networks of binary neurons learn sequence attractors to store predefined pattern sequences and retrieve them robustly. We show that to store arbitrary pattern sequences, it is necessary for the network to include hidden neurons even though their role in displaying sequence memories is indirect. We develop a local learning algorithm to learn sequence attractors in the networks with hidden neurons. The algorithm is proven to converge and lead to sequence attractors. We demonstrate that the network model can store and retrieve sequences robustly on synthetic and real-world datasets. We hope that this study provides new insights in understanding sequence memory and temporal information processing in the brain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#25506;&#32034;&#38454;&#27573;&#23558;&#36830;&#32493;&#36816;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;, &#33258;&#21160;&#29983;&#25104;&#8220;&#21160;&#20316;&#21407;&#22411;&#8221;, &#20174;&#32780;&#23454;&#29616;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#25928;&#26524;&#39537;&#21160;&#23398;&#20064;</title><link>https://arxiv.org/abs/2404.02728</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20013;&#26377;&#25928;&#21160;&#20316;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning of Effective Actions in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02728
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#25506;&#32034;&#38454;&#27573;&#23558;&#36830;&#32493;&#36816;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;, &#33258;&#21160;&#29983;&#25104;&#8220;&#21160;&#20316;&#21407;&#22411;&#8221;, &#20174;&#32780;&#23454;&#29616;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#25928;&#26524;&#39537;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19982;&#20915;&#31574;&#30456;&#20851;&#19988;&#21487;&#20197;&#26377;&#25928;&#25191;&#34892;&#30340;&#21160;&#20316;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#24403;&#21069;&#26426;&#22120;&#20154;&#23398;&#20013;&#26368;&#20808;&#36827;&#30340;&#21160;&#20316;&#34920;&#31034;&#32570;&#20047;&#23545;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#36866;&#24403;&#25928;&#26524;&#39537;&#21160;&#23398;&#20064;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#25805;&#32437;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#32570;&#20047;&#36825;&#31181;&#33021;&#21147;&#65292;&#32780;&#19988;&#22312;&#20869;&#23384;&#25110;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#25104;&#26412;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#36830;&#32493;&#36816;&#21160;&#31354;&#38388;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#29983;&#25104;&#8220;&#21160;&#20316;&#21407;&#22411;&#8221;&#65292;&#27599;&#20010;&#21407;&#22411;&#22312;&#29615;&#22659;&#20013;&#20135;&#29983;&#19981;&#21516;&#30340;&#25928;&#26524;&#12290;&#22312;&#25506;&#32034;&#38454;&#27573;&#20043;&#21518;&#65292;&#35813;&#31639;&#27861;&#20250;&#33258;&#21160;&#26500;&#24314;&#23545;&#25928;&#26524;&#30340;&#34920;&#31034;&#65292;&#24182;&#23558;&#21160;&#20316;&#20998;&#32452;&#20026;&#21160;&#20316;&#21407;&#22411;&#65292;&#20854;&#20013;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#25928;&#26524;&#30340;&#21160;&#20316;&#27604;&#23548;&#33268;&#21487;&#24573;&#30053;&#21464;&#21270;&#30340;&#21160;&#20316;&#26356;&#22810;&#22320;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#27004;&#26799;&#25856;&#30331;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02728v1 Announce Type: cross  Abstract: Learning actions that are relevant to decision-making and can be executed effectively is a key problem in autonomous robotics. Current state-of-the-art action representations in robotics lack proper effect-driven learning of the robot's actions. Although successful in solving manipulation tasks, deep learning methods also lack this ability, in addition to their high cost in terms of memory or training data. In this paper, we propose an unsupervised algorithm to discretize a continuous motion space and generate "action prototypes", each producing different effects in the environment. After an exploration phase, the algorithm automatically builds a representation of the effects and groups motions into action prototypes, where motions more likely to produce an effect are represented more than those that lead to negligible changes. We evaluate our method on a simulated stair-climbing reinforcement learning task, and the preliminary results
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#21487;&#22609;&#24615;&#25439;&#22833;&#21644;&#31070;&#32463;&#22349;&#22604;&#30340;&#20851;&#32852;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#31070;&#32463;&#22349;&#22604;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#21487;&#22609;&#24615;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2404.02719</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#36890;&#36807;&#31070;&#32463;&#22349;&#22604;&#26469;&#29702;&#35299;&#21487;&#22609;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Understand Plasticity Through Neural Collapse?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02719
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#21487;&#22609;&#24615;&#25439;&#22833;&#21644;&#31070;&#32463;&#22349;&#22604;&#30340;&#20851;&#32852;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#31070;&#32463;&#22349;&#22604;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#21487;&#22609;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#20004;&#20010;&#26368;&#36817;&#30830;&#23450;&#30340;&#29616;&#35937;&#20043;&#38388;&#30340;&#32852;&#31995;&#65306;&#21487;&#22609;&#24615;&#25439;&#22833;&#21644;&#31070;&#32463;&#22349;&#22604;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#30340;&#21021;&#22987;&#35757;&#32451;&#38454;&#27573;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#31070;&#32463;&#22349;&#22604;&#65292;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#29305;&#23450;&#24773;&#20917;&#19979;&#20943;&#36731;&#21487;&#22609;&#24615;&#25439;&#22833;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02719v1 Announce Type: cross  Abstract: This paper explores the connection between two recently identified phenomena in deep learning: plasticity loss and neural collapse. We analyze their correlation in different scenarios, revealing a significant association during the initial training phase on the first task. Additionally, we introduce a regularization approach to mitigate neural collapse, demonstrating its effectiveness in alleviating plasticity loss in this specific setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PromptCodec&#65292;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#24449;&#24863;&#30693;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#39640;&#20445;&#30495;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#29305;&#24449;&#34920;&#31034;&#12289;&#33258;&#36866;&#24212;&#29305;&#24449;&#21152;&#26435;&#34701;&#21512;&#21644;&#25928;&#29575;&#20248;&#21270;&#26469;&#35299;&#20915;&#39640;&#21387;&#32553;&#29575;&#19979;&#30340;&#39640;&#20445;&#30495;&#38899;&#39057;&#37325;&#24314;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02702</link><description>&lt;p&gt;
PromptCodec: &#20351;&#29992;&#22522;&#20110;&#33258;&#36866;&#24212;&#29305;&#24449;&#24863;&#30693;&#30340;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#39640;&#20445;&#30495;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PromptCodec: High-Fidelity Neural Speech Codec using Disentangled Representation Learning based Adaptive Feature-aware Prompt Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PromptCodec&#65292;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#24449;&#24863;&#30693;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#39640;&#20445;&#30495;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#29305;&#24449;&#34920;&#31034;&#12289;&#33258;&#36866;&#24212;&#29305;&#24449;&#21152;&#26435;&#34701;&#21512;&#21644;&#25928;&#29575;&#20248;&#21270;&#26469;&#35299;&#20915;&#39640;&#21387;&#32553;&#29575;&#19979;&#30340;&#39640;&#20445;&#30495;&#38899;&#39057;&#37325;&#24314;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#36817;&#26469;&#22312;&#29983;&#25104;&#35821;&#38899;&#24314;&#27169;&#39046;&#22495;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#65292;&#20363;&#22914;&#35821;&#38899;&#36716;&#25442;&#12289;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#31561;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#21387;&#32553;&#29575;&#19979;&#30830;&#20445;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#30340;&#39640;&#20445;&#30495;&#38899;&#39057;&#37325;&#24314;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PromptCodec&#65292;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#24449;&#24863;&#30693;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#31471;&#21040;&#31471;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#26469;&#33258;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#39069;&#22806;&#29305;&#24449;&#34920;&#31034;&#65292;PromptCodec&#21487;&#20197;&#20998;&#37197;&#38656;&#35201;&#22788;&#29702;&#30340;&#35821;&#38899;&#20449;&#24687;&#24182;&#22686;&#24378;&#20854;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#29305;&#24449;&#21152;&#26435;&#34701;&#21512;&#26041;&#27861;&#65292;&#20197;&#25972;&#21512;&#19981;&#21516;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#36317;&#31163;&#30340;&#26032;&#39062;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;PromptCodec&#30340;&#32534;&#30721;&#22120;&#20197;&#30830;&#20445;&#20854;&#25928;&#29575;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02702v1 Announce Type: cross  Abstract: Neural speech codec has recently gained widespread attention in generative speech modeling domains, like voice conversion, text-to-speech synthesis, etc. However, ensuring high-fidelity audio reconstruction of speech codecs under high compression rates remains an open and challenging issue. In this paper, we propose PromptCodec, a novel end-to-end neural speech codec model using disentangled representation learning based feature-aware prompt encoders. By incorporating additional feature representations from prompt encoders, PromptCodec can distribute the speech information requiring processing and enhance its capabilities. Moreover, a simple yet effective adaptive feature weighted fusion approach is introduced to integrate features of different encoders. Meanwhile, we propose a novel disentangled representation learning strategy based on cosine distance to optimize PromptCodec's encoders to ensure their efficiency, thereby further impr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#39640;&#26031;&#36755;&#20837;&#19979;&#27880;&#24847;&#21147;&#24471;&#20998;&#31232;&#30095;&#24615;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#30340;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02690</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#39640;&#26031;&#20998;&#24067;&#36755;&#20837;&#19979;&#33258;&#28982;&#31232;&#30095;
&lt;/p&gt;
&lt;p&gt;
Attention is Naturally Sparse with Gaussian Distributed Input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02690
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#39640;&#26031;&#36755;&#20837;&#19979;&#27880;&#24847;&#21147;&#24471;&#20998;&#31232;&#30095;&#24615;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#30340;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35745;&#31639;&#24378;&#24230;&#26159;&#20851;&#38190;&#29942;&#39048;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;$O(n^2)$&#22797;&#26434;&#24230;&#12290;&#31232;&#30095;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#24212;&#36816;&#32780;&#29983;&#65292;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#33655;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23545;LLMs&#20869;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#31232;&#30095;&#24615;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#26031;&#36755;&#20837;&#26694;&#26550;&#19979;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#32452;&#22522;&#30784;&#20551;&#35774;&#24182;&#37319;&#29992;&#19968;&#31181;&#31995;&#32479;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#20998;&#25968;&#31232;&#30095;&#24615;&#30340;&#20869;&#22312;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#23545;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#34920;&#29616;&#24418;&#24335;&#30340;&#35814;&#32454;&#29702;&#35770;&#26816;&#26597;&#65292;&#25581;&#31034;&#20102;&#22312;&#35745;&#31639;&#33410;&#32422;&#21644;&#27169;&#22411;&#26377;&#25928;&#24615;&#20043;&#38388;&#28508;&#22312;&#26435;&#34913;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02690v1 Announce Type: cross  Abstract: The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures. Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance. This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs. By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency. Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness. This work not only advances 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;&#21644;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#20043;&#38388;&#20849;&#20139;&#32452;&#20214;&#30340;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.02684</link><description>&lt;p&gt;
&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02684
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;&#21644;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#20043;&#38388;&#20849;&#20139;&#32452;&#20214;&#30340;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;&#26550;&#26500;&#26469;&#36890;&#36807;&#25913;&#21464;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#35774;&#35745;&#23454;&#29616;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;(LCI)&#20197;&#25552;&#39640;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#26159;&#29366;&#24577;&#31354;&#38388;&#26426;&#22120;&#65288;SSMs&#65289;&#26550;&#26500;&#65292;&#23427;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#19982;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26550;&#26500;&#26356;&#25913;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#23436;&#20840;&#39044;&#35757;&#32451;&#26435;&#37325;&#65292;&#36825;&#32473;&#24076;&#26395;&#20351;&#29992;&#26032;&#26550;&#26500;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24102;&#26469;&#20102;&#24040;&#22823;&#25104;&#26412;&#12290;&#21463;&#20256;&#32479;&#32447;&#24615;&#27880;&#24847;&#21147;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;(XATL)&#65292;&#20854;&#20013;LCI&#21644;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#22120;&#20043;&#38388;&#30340;&#20849;&#20139;&#32452;&#20214;&#30340;&#26435;&#37325;&#65292;&#22914;&#23618;&#35268;&#33539;&#12289;MLP&#12289;&#36755;&#20837;/&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02684v1 Announce Type: cross  Abstract: Recently, multiple architectures has been proposed to improve the efficiency of the Transformer Language Models through changing the design of the self-attention block to have a linear-cost inference (LCI). A notable approach in this realm is the State-Space Machines (SSMs) architecture, which showed on-par performance on language modeling tasks with the self-attention transformers. However, such an architectural change requires a full pretraining of the weights from scratch, which incurs a huge cost to researchers and practitioners who want to use the new architectures. In the more traditional linear attention works, it has been proposed to approximate full attention with linear attention by swap-and-finetune framework. Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/outpu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28548;&#28165;&#35789;&#35821;&#21547;&#20041;&#26469;&#25913;&#21892;&#21388;&#24694;&#26816;&#27979;&#65292;PejorativITy&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24847;&#22823;&#21033;&#25512;&#25991;&#35821;&#26009;&#24211;&#65292;&#25581;&#31034;&#20102;&#23558;&#36140;&#25439;&#20449;&#24687;&#27880;&#20837;&#27169;&#22411;&#30340;&#20004;&#31181;&#26041;&#27861;&#22343;&#33021;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02681</link><description>&lt;p&gt;
PejorativITy&#65306;&#28040;&#38500;&#34065;&#31216;&#35789;&#20197;&#25913;&#21892;&#24847;&#22823;&#21033;&#25512;&#25991;&#20013;&#30340;&#21388;&#24694;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02681
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28548;&#28165;&#35789;&#35821;&#21547;&#20041;&#26469;&#25913;&#21892;&#21388;&#24694;&#26816;&#27979;&#65292;PejorativITy&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24847;&#22823;&#21033;&#25512;&#25991;&#35821;&#26009;&#24211;&#65292;&#25581;&#31034;&#20102;&#23558;&#36140;&#25439;&#20449;&#24687;&#27880;&#20837;&#27169;&#22411;&#30340;&#20004;&#31181;&#26041;&#27861;&#22343;&#33021;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21388;&#24694;&#24448;&#24448;&#36890;&#36807;&#27604;&#21947;&#35821;&#35328;&#34920;&#36798;&#12290;&#24403;&#19968;&#20123;&#20013;&#24615;&#35789;&#35821;&#20316;&#20026;&#36140;&#25439;&#31216;&#35859;&#26102;&#65292;&#21487;&#33021;&#20250;&#21576;&#29616;&#36127;&#38754;&#21547;&#20041;&#12290;&#28548;&#28165;&#36825;&#31867;&#35789;&#35821;&#30340;&#21547;&#20041;&#21487;&#33021;&#26377;&#21161;&#20110;&#26816;&#27979;&#21388;&#24694;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PejorativITy&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#30001;1,200&#26465;&#24847;&#22823;&#21033;&#25512;&#25991;&#26500;&#25104;&#30340;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#26631;&#27880;&#36140;&#25439;&#35821;&#35328;&#65292;&#24182;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#26631;&#27880;&#21388;&#24694;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23558;&#28548;&#28165;&#35789;&#35821;&#20449;&#24687;&#27880;&#20837;&#38024;&#23545;&#21388;&#24694;&#26816;&#27979;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27880;&#20837;&#26041;&#27861;&#65306;&#23558;&#36140;&#25439;&#20449;&#24687;&#36830;&#25509;&#22312;&#19968;&#36215;&#21644;&#23558;&#27169;&#26865;&#20004;&#21487;&#30340;&#35789;&#35821;&#26367;&#25442;&#20026;&#26126;&#30830;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#26080;&#35770;&#26159;&#22312;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#19978;&#36824;&#26159;&#22312;&#24847;&#22823;&#21033;&#25512;&#25991;&#30340;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#19978;&#65292;&#37117;&#34920;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#22343;&#23548;&#33268;&#26356;&#22823;&#30340;&#20998;&#31867;&#25913;&#21892;&#65292;&#34920;&#26126;&#35789;&#20041;&#28040;&#27495;&#26159;&#21388;&#24694;&#26816;&#27979;&#30340;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02681v1 Announce Type: cross  Abstract: Misogyny is often expressed through figurative language. Some neutral words can assume a negative connotation when functioning as pejorative epithets. Disambiguating the meaning of such terms might help the detection of misogyny. In order to address such task, we present PejorativITy, a novel corpus of 1,200 manually annotated Italian tweets for pejorative language at the word level and misogyny at the sentence level. We evaluate the impact of injecting information about disambiguated words into a model targeting misogyny detection. In particular, we explore two different approaches for injection: concatenation of pejorative information and substitution of ambiguous words with univocal terms. Our experimental results, both on our corpus and on two popular benchmarks on Italian tweets, show that both approaches lead to a major classification improvement, indicating that word sense disambiguation is a promising preliminary step for misog
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36127;&#36131;&#20219;&#22320;&#21521;&#25919;&#24220;&#12289;&#24037;&#19994;&#30028;&#21644;&#20844;&#27665;&#31038;&#20250;&#30340;&#30456;&#20851;&#26041;&#25253;&#21578;&#23433;&#20840;&#20851;&#38190;&#20449;&#24687;&#65292;&#21069;&#27839;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24320;&#21457;&#32452;&#32455;&#21487;&#20197;&#25552;&#39640;&#39118;&#38505;&#21487;&#35265;&#24230;&#65292;&#24110;&#21161;&#24320;&#21457;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#24182;&#21327;&#21161;&#20915;&#31574;&#32773;&#35774;&#35745;&#26356;&#26377;&#38024;&#23545;&#24615;&#21644;&#20581;&#20840;&#30340;&#30417;&#31649;&#22522;&#30784;&#35774;&#26045;&#12290;</title><link>https://arxiv.org/abs/2404.02675</link><description>&lt;p&gt;
&#20026;&#21069;&#27839;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36127;&#36131;&#20219;&#22320;&#25253;&#36947;
&lt;/p&gt;
&lt;p&gt;
Responsible Reporting for Frontier AI Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02675
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36127;&#36131;&#20219;&#22320;&#21521;&#25919;&#24220;&#12289;&#24037;&#19994;&#30028;&#21644;&#20844;&#27665;&#31038;&#20250;&#30340;&#30456;&#20851;&#26041;&#25253;&#21578;&#23433;&#20840;&#20851;&#38190;&#20449;&#24687;&#65292;&#21069;&#27839;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24320;&#21457;&#32452;&#32455;&#21487;&#20197;&#25552;&#39640;&#39118;&#38505;&#21487;&#35265;&#24230;&#65292;&#24110;&#21161;&#24320;&#21457;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#24182;&#21327;&#21161;&#20915;&#31574;&#32773;&#35774;&#35745;&#26356;&#26377;&#38024;&#23545;&#24615;&#21644;&#20581;&#20840;&#30340;&#30417;&#31649;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#21069;&#27839;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24102;&#26469;&#30340;&#39118;&#38505;&#38656;&#35201;&#33719;&#21462;&#26368;&#26032;&#21644;&#21487;&#38752;&#30340;&#20851;&#20110;&#36825;&#20123;&#31995;&#32479;&#30340;&#20449;&#24687;&#12290;&#24320;&#21457;&#21644;&#37096;&#32626;&#21069;&#27839;&#31995;&#32479;&#30340;&#32452;&#32455;&#23545;&#36825;&#20123;&#20449;&#24687;&#20855;&#26377;&#37325;&#35201;&#30340;&#33719;&#21462;&#36884;&#24452;&#12290;&#36890;&#36807;&#21521;&#25919;&#24220;&#12289;&#24037;&#19994;&#30028;&#21644;&#20844;&#27665;&#31038;&#20250;&#30340;&#30456;&#20851;&#26041;&#25253;&#21578;&#23433;&#20840;&#20851;&#38190;&#20449;&#24687;&#65292;&#36825;&#20123;&#32452;&#32455;&#21487;&#20197;&#22686;&#36827;&#23545;&#21069;&#27839;&#31995;&#32479;&#24102;&#26469;&#30340;&#26032;&#20852;&#39118;&#38505;&#30340;&#21487;&#35265;&#24230;&#12290;&#26377;&#20102;&#36825;&#20123;&#20449;&#24687;&#65292;&#24320;&#21457;&#32773;&#21487;&#20197;&#20570;&#20986;&#26356;&#20026;&#26126;&#26234;&#30340;&#39118;&#38505;&#31649;&#29702;&#20915;&#31574;&#65292;&#20915;&#31574;&#32773;&#20063;&#21487;&#20197;&#35774;&#35745;&#26356;&#21152;&#26377;&#38024;&#23545;&#24615;&#21644;&#20581;&#20840;&#30340;&#30417;&#31649;&#22522;&#30784;&#35774;&#26045;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#36127;&#36131;&#20219;&#22320;&#25253;&#36947;&#30340;&#20851;&#38190;&#29305;&#28857;&#24182;&#25552;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#23454;&#26045;&#23427;&#20204;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02675v1 Announce Type: cross  Abstract: Mitigating the risks from frontier AI systems requires up-to-date and reliable information about those systems. Organizations that develop and deploy frontier systems have significant access to such information. By reporting safety-critical information to actors in government, industry, and civil society, these organizations could improve visibility into new and emerging risks posed by frontier systems. Equipped with this information, developers could make better informed decisions on risk management, while policymakers could design more targeted and robust regulatory infrastructure. We outline the key features of responsible reporting and propose mechanisms for implementing them in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#37325;&#26032;&#24605;&#32771;Kullback-Leibler&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kullback-Leibler&#25955;&#24230;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#65292;&#22312;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#19982;&#20043;&#21069;&#26029;&#35328;&#30340;&#36870;Kullback-Leibler&#65288;RKL&#65289;&#25955;&#24230;&#23547;&#25214;&#27169;&#24335;&#24182;&#22240;&#27492;&#20248;&#20110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#27491;&#21521;Kullback-Leibler&#65288;FKL&#65289;&#25955;&#24230;&#30456;&#21453;&#65292;&#23454;&#38469;&#19978;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#37117;&#27809;&#26377;&#20307;&#29616;&#20986;&#23547;&#25214;&#27169;&#24335;&#25110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#29305;&#24615;&#12290;&#30456;&#21453;&#65292;&#21457;&#29616;RKL&#21644;FKL&#20855;&#26377;&#30456;&#21516;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#22312;&#36275;&#22815;&#25968;&#37327;&#30340;&#26102;&#20195;&#20043;&#21518;&#37117;&#20250;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#32422;&#26463;&#65292;LLMs&#24456;&#23569;&#34987;&#35757;&#32451;&#22914;&#27492;&#22810;&#30340;&#26102;&#20195;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;RKL&#22312;&#20998;&#24067;&#30340;&#23614;&#37096;&#65292;&#32780;FKL&#22312;&#24320;&#22987;&#26102;&#20195;&#20391;&#37325;&#20110;&#20998;&#24067;&#30340;&#22836;&#37096;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;Kullback-Leiber&#65288;AKL&#65289;&#25955;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#26435;&#37325;&#26469;&#32452;&#21512;F
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02657v1 Announce Type: cross  Abstract: Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#36890;&#36807;&#25506;&#32034;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#19981;&#21516;&#25968;&#25454;&#23646;&#24615;&#34920;&#31034;&#26469;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02656</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#38750;&#36127;&#23376;&#31354;&#38388;&#29305;&#24449;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Non-negative Subspace Feature Representation for Few-shot Learning in Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#36890;&#36807;&#25506;&#32034;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#19981;&#21516;&#25968;&#25454;&#23646;&#24615;&#34920;&#31034;&#26469;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#35270;&#35273;&#22330;&#26223;&#35782;&#21035;&#39046;&#22495;&#19981;&#21516;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35775;&#38382;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#21307;&#23398;&#22270;&#20687;&#35299;&#37322;&#24448;&#24448;&#21463;&#21040;&#25968;&#25454;&#30701;&#32570;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#25506;&#32034;&#19981;&#21516;&#25968;&#25454;&#23646;&#24615;&#34920;&#31034;&#65292;&#30740;&#31350;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#21307;&#23398;&#24433;&#20687;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;NMF&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#20854;&#30417;&#30563;&#21464;&#20307;&#65288;&#22914;&#65292;&#26377;&#24046;&#21035;&#24615;NMF&#65292;&#20197;&#21450;&#20855;&#26377;&#31232;&#30095;&#24615;&#30340;&#30417;&#30563;&#21644;&#32422;&#26463;NMF&#65289;&#65292;&#24182;&#19982;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21363;&#20174;&#29305;&#24449;&#21521;&#37327;&#20013;&#23548;&#20986;&#30340;&#22522;&#20110;&#21327;&#20316;&#34920;&#31034;&#30340;&#38477;&#32500;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02656v1 Announce Type: cross  Abstract: Unlike typical visual scene recognition domains, in which massive datasets are accessible to deep neural networks, medical image interpretations are often obstructed by the paucity of data. In this paper, we investigate the effectiveness of data-based few-shot learning in medical imaging by exploring different data attribute representations in a low-dimensional space. We introduce different types of non-negative matrix factorization (NMF) in few-shot learning, addressing the data scarcity issue in medical image classification. Extensive empirical studies are conducted in terms of validating the effectiveness of NMF, especially its supervised variants (e.g., discriminative NMF, and supervised and constrained NMF with sparseness), and the comparison with principal component analysis (PCA), i.e., the collaborative representation-based dimensionality reduction technique derived from eigenvectors. With 14 different datasets covering 11 dist
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26032;&#36884;&#24452;&#65292;&#30528;&#37325;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02650</link><description>&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards detecting unanticipated bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26032;&#36884;&#24452;&#65292;&#30528;&#37325;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#23637;&#29616;&#20986;&#19982;&#20197;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#31867;&#20284;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#20998;&#26512;&#21644;&#37327;&#21270;&#36825;&#20123;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21450;&#20854;&#23545;&#36825;&#20123;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#21046;&#23450;&#20943;&#36731;&#31574;&#30053;&#12290;&#36825;&#39033;&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#19982;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#26063;&#35028;&#21644;&#35821;&#35328;&#30456;&#20851;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#24456;&#26126;&#26174;&#65292;LLMs&#20063;&#21463;&#21040;&#20854;&#20182;&#19981;&#22826;&#26126;&#26174;&#30340;&#20869;&#38544;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#36890;&#24120;&#30340;&#19981;&#36879;&#26126;&#24615;&#20351;&#24471;&#26816;&#27979;&#36825;&#20123;&#20559;&#35265;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#28508;&#22312;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;LLMs&#20013;&#26816;&#27979;&#36825;&#20123;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26032;&#36884;&#24452;&#65292;&#20855;&#20307;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02650v1 Announce Type: cross  Abstract: Over the last year, Large Language Models (LLMs) like ChatGPT have become widely available and have exhibited fairness issues similar to those in previous machine learning systems. Current research is primarily focused on analyzing and quantifying these biases in training data and their impact on the decisions of these models, alongside developing mitigation strategies. This research largely targets well-known biases related to gender, race, ethnicity, and language. However, it is clear that LLMs are also affected by other, less obvious implicit biases. The complex and often opaque nature of these models makes detecting such biases challenging, yet this is crucial due to their potential negative impact in various applications. In this paper, we explore new avenues for detecting these unanticipated biases in LLMs, focusing specifically on Uncertainty Quantification and Explainable AI methods. These approaches aim to assess the certainty
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#21363;&#21487;&#22312;&#21508;&#31181;&#26080;&#32447;&#29615;&#22659;&#20013;&#23454;&#29616;&#39640;&#24615;&#33021;&#26816;&#27979;&#30340;&#36890;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2404.02648</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20449;&#21495;&#26816;&#27979;&#30340;&#36890;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Universal Deep Neural Network for Signal Detection in Wireless Communication Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#21363;&#21487;&#22312;&#21508;&#31181;&#26080;&#32447;&#29615;&#22659;&#20013;&#23454;&#29616;&#39640;&#24615;&#33021;&#26816;&#27979;&#30340;&#36890;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36817;&#26469;&#20316;&#20026;&#26080;&#32447;&#36890;&#20449;&#20013;&#20449;&#36947;&#20272;&#35745;&#21644;&#20449;&#21495;&#26816;&#27979;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#19981;&#26029;&#28044;&#29616;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20998;&#26512;&#26469;&#33258;&#21333;&#19968;&#20449;&#36947;&#20998;&#24067;&#65288;&#22914;&#21152;&#24615;&#30333;&#22122;&#22768;&#20449;&#36947;&#21644;&#29790;&#21033;&#20449;&#36947;&#65289;&#29983;&#25104;&#30340;&#20449;&#36947;&#33033;&#20914;&#21709;&#24212;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20026;&#20102;&#24212;&#23545;&#26080;&#32447;&#20449;&#36947;&#21160;&#24577;&#24615;&#65292;DL&#26041;&#27861;&#24517;&#39035;&#22312;&#26032;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26114;&#36149;&#12289;&#20302;&#25928;&#19988;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;Uni-DNN&#65289;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#26080;&#32447;&#29615;&#22659;&#20013;&#23454;&#29616;&#39640;&#24615;&#33021;&#26816;&#27979;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;Uni-DNN&#27169;&#22411;&#21253;&#25324;&#26080;&#32447;&#20449;&#36947;&#20998;&#31867;&#22120;&#21644;&#20449;&#21495;&#26816;&#27979;&#22120;&#65292;&#20998;&#21035;&#20351;&#29992;DNN&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02648v1 Announce Type: cross  Abstract: Recently, deep learning (DL) has been emerging as a promising approach for channel estimation and signal detection in wireless communications. The majority of the existing studies investigating the use of DL techniques in this domain focus on analysing channel impulse responses that are generated from only one channel distribution such as additive white Gaussian channel noise and Rayleigh channels. In practice, to cope with the dynamic nature of the wireless channel, DL methods must be re-trained on newly non-aged collected data which is costly, inefficient, and impractical. To tackle this challenge, this paper proposes a novel universal deep neural network (Uni-DNN) that can achieve high detection performance in various wireless environments without retraining the model. In particular, our proposed Uni-DNN model consists of a wireless channel classifier and a signal detector which are constructed by using DNNs. The wireless channel cl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25554;&#20837;&#26469;&#28304;&#27169;&#22411;&#35789;&#27719;&#30340;&#26041;&#24335;&#65292;&#25104;&#21151;&#23454;&#26045;&#20102;&#23545;&#20004;&#20010;&#28909;&#38376;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30446;&#26631;&#21163;&#25345;&#65292;&#21019;&#36896;&#20986;&#38590;&#20197;&#26816;&#27979;&#30340;&#19981;&#24341;&#20154;&#27880;&#30446;&#25351;&#20196;&#12290;</title><link>https://arxiv.org/abs/2404.02637</link><description>&lt;p&gt;
&#35789;&#27719;&#25915;&#20987;&#20197;&#21163;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Vocabulary Attack to Hijack Large Language Model Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02637
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25554;&#20837;&#26469;&#28304;&#27169;&#22411;&#35789;&#27719;&#30340;&#26041;&#24335;&#65292;&#25104;&#21151;&#23454;&#26045;&#20102;&#23545;&#20004;&#20010;&#28909;&#38376;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30446;&#26631;&#21163;&#25345;&#65292;&#21019;&#36896;&#20986;&#38590;&#20197;&#26816;&#27979;&#30340;&#19981;&#24341;&#20154;&#27880;&#30446;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#25512;&#21160;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#38543;&#30528;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#20063;&#30475;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#25915;&#20987;&#32773;&#35797;&#22270;&#26234;&#32988;&#36825;&#20123;&#31995;&#32479;&#12290;&#20182;&#20204;&#24076;&#26395;&#27169;&#22411;&#36879;&#38706;&#26426;&#23494;&#20449;&#24687;&#12289;&#29305;&#23450;&#38169;&#35823;&#20449;&#24687;&#25110;&#20882;&#29359;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#20182;&#20204;&#36890;&#36807;&#25554;&#20837;&#20998;&#38548;&#31526;&#25110;&#31995;&#32479;&#24615;&#22320;&#25913;&#20889;&#25351;&#20196;&#65292;&#30452;&#21040;&#36798;&#21040;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20247;&#19981;&#21516;&#12290;&#23427;&#25554;&#20837;&#26469;&#33258;&#27169;&#22411;&#35789;&#27719;&#30340;&#35789;&#27719;&#12290;&#25105;&#20204;&#20351;&#29992;&#20248;&#21270;&#36807;&#31243;&#21644;&#26469;&#33258;&#21478;&#19968;&#20010;LLM&#65288;&#25915;&#20987;&#32773;LLM&#65289;&#30340;&#23884;&#20837;&#26469;&#25214;&#21040;&#36825;&#20123;&#35789;&#27719;&#12290;&#25105;&#20204;&#36890;&#36807;&#21163;&#25345;&#20004;&#20010;&#28909;&#38376;&#24320;&#28304;LLM&#65288;&#20998;&#21035;&#26469;&#33258;Llama2&#21644;Flan-T5&#31995;&#21015;&#65289;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#19981;&#24341;&#20154;&#27880;&#30446;&#30340;&#25351;&#20196;&#65292;&#22240;&#27492;&#24456;&#38590;&#26816;&#27979;&#12290;&#23545;&#20110;&#35768;&#22810;&#25915;&#20987;&#26696;&#20363;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02637v1 Announce Type: cross  Abstract: The fast advancements in Large Language Models (LLMs) are driving an increasing number of applications. Together with the growing number of users, we also see an increasing number of attackers who try to outsmart these systems. They want the model to reveal confidential information, specific false information, or offensive behavior. To this end, they manipulate their instructions for the LLM by inserting separators or rephrasing them systematically until they reach their goal. Our approach is different. It inserts words from the model vocabulary. We find these words using an optimization procedure and embeddings from another LLM (attacker LLM). We prove our approach by goal hijacking two popular open-source LLMs from the Llama2 and the Flan-T5 families, respectively. We present two main findings. First, our approach creates inconspicuous instructions and therefore it is hard to detect. For many attack cases, we find that even a single 
&lt;/p&gt;</description></item><item><title>Diff-Comb Explainer&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#40657;&#30418;&#32452;&#21512;&#27714;&#35299;&#22120;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#19981;&#38656;&#35201;&#23545;&#35821;&#20041;&#32422;&#26463;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#30456;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2404.02625</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#21487;&#24494;&#20998;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02625
&lt;/p&gt;
&lt;p&gt;
Diff-Comb Explainer&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#40657;&#30418;&#32452;&#21512;&#27714;&#35299;&#22120;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#19981;&#38656;&#35201;&#23545;&#35821;&#20041;&#32422;&#26463;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#30456;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Integer Linear Programming&#65288;ILP&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#36827;&#34892;&#31934;&#30830;&#32467;&#26500;&#21644;&#35821;&#20041;&#32422;&#26463;&#32534;&#30721;&#30340;&#27491;&#24335;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ILP&#26694;&#26550;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#36825;&#32473;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36830;&#32493;&#35821;&#35328;&#34920;&#31034;&#30340;&#25972;&#21512;&#24102;&#26469;&#20102;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;Diff-Comb Explainer&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#40657;&#30418;&#32452;&#21512;&#27714;&#35299;&#22120;&#65288;DBCS&#65289;&#30340;&#35299;&#37322;&#22411;NLI&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#31526;&#21495;&#27714;&#35299;&#22120;&#19981;&#21516;&#65292;Diff-Comb Explainer&#19981;&#38656;&#35201;&#23545;&#35821;&#20041;&#32422;&#26463;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#12289;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#22320;&#23558;&#31070;&#32463;&#34920;&#31034;&#34701;&#20837;&#21040;ILP&#20844;&#24335;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;ILP&#27714;&#35299;&#22120;&#12289;&#31070;&#32463;&#31526;&#21495;&#40657;&#30418;&#27714;&#35299;&#22120;&#21644;Trans&#30456;&#27604;&#65292;Diff-Comb Explainer&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02625v1 Announce Type: cross  Abstract: Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for Natural Language Inference (NLI). However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning. In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers (DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation. Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic black-box solvers, and Trans
&lt;/p&gt;</description></item><item><title>Diffexplainer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#23454;&#29616;&#22810;&#27169;&#24577;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#30340;&#25991;&#26412;&#25552;&#31034;&#26465;&#20214;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21512;&#25104;&#22270;&#20687;&#26469;&#35299;&#37322;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#65292;&#21516;&#26102;&#25552;&#20379;&#19968;&#20010;&#23545;&#20915;&#31574;&#35299;&#37322;&#30340;&#35270;&#35273;&#24037;&#20855;&#65292;&#24182;&#33021;&#33258;&#21160;&#35782;&#21035;&#20559;&#35265;&#21644;&#34394;&#20551;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2404.02618</link><description>&lt;p&gt;
Diffexplainer&#65306;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#36328;&#27169;&#24577;&#20840;&#23616;&#35299;&#37322;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diffexplainer: Towards Cross-modal Global Explanations with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02618
&lt;/p&gt;
&lt;p&gt;
Diffexplainer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#23454;&#29616;&#22810;&#27169;&#24577;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#30340;&#25991;&#26412;&#25552;&#31034;&#26465;&#20214;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21512;&#25104;&#22270;&#20687;&#26469;&#35299;&#37322;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#65292;&#21516;&#26102;&#25552;&#20379;&#19968;&#20010;&#23545;&#20915;&#31574;&#35299;&#37322;&#30340;&#35270;&#35273;&#24037;&#20855;&#65292;&#24182;&#33021;&#33258;&#21160;&#35782;&#21035;&#20559;&#35265;&#21644;&#34394;&#20551;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DiffExplainer&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#23454;&#29616;&#22810;&#27169;&#24577;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;DiffExplainer&#21033;&#29992;&#32463;&#36807;&#20248;&#21270;&#30340;&#25991;&#26412;&#25552;&#31034;&#26465;&#20214;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21512;&#25104;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#36755;&#20986;&#21644;&#38544;&#34255;&#29305;&#24449;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#20379;&#19968;&#20010;&#35299;&#37322;&#20915;&#31574;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#23545;&#29983;&#25104;&#30340;&#35270;&#35273;&#25551;&#36848;&#30340;&#20998;&#26512;&#20801;&#35768;&#33258;&#21160;&#35782;&#21035;&#20559;&#35265;&#21644;&#34394;&#20551;&#29305;&#24449;&#65292;&#32780;&#19981;&#20687;&#20256;&#32479;&#26041;&#27861;&#37027;&#26679;&#24120;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#24178;&#39044;&#12290;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#30340;&#36328;&#27169;&#24577;&#21487;&#36801;&#31227;&#24615;&#36824;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#20197;&#26356;&#20855;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#24335;&#25551;&#36848;&#20915;&#31574;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32508;&#21512;&#23454;&#39564;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;DiffExplainer&#22312;&#29983;&#25104;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02618v1 Announce Type: cross  Abstract: We present DiffExplainer, a novel framework that, leveraging language-vision models, enables multimodal global explainability. DiffExplainer employs diffusion models conditioned on optimized text prompts, synthesizing images that maximize class outputs and hidden features of a classifier, thus providing a visual tool for explaining decisions. Moreover, the analysis of generated visual descriptions allows for automatic identification of biases and spurious features, as opposed to traditional methods that often rely on manual intervention. The cross-modal transferability of language-vision models also enables the possibility to describe decisions in a more human-interpretable way, i.e., through text. We conduct comprehensive experiments, which include an extensive user study, demonstrating the effectiveness of DiffExplainer on 1) the generation of high-quality images explaining model decisions, surpassing existing activation maximization
&lt;/p&gt;</description></item><item><title>SHIELD&#24341;&#20837;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#38544;&#34255;&#37096;&#20998;&#36755;&#20837;&#25968;&#25454;&#24182;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02611</link><description>&lt;p&gt;
SHIELD: &#19968;&#31181;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
SHIELD: A regularization technique for eXplainable Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02611
&lt;/p&gt;
&lt;p&gt;
SHIELD&#24341;&#20837;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#38544;&#34255;&#37096;&#20998;&#36755;&#20837;&#25968;&#25454;&#24182;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21508;&#20010;&#39046;&#22495;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#65292;&#23545;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#19982;&#26085;&#20465;&#22686;&#12290;&#23613;&#31649;&#31185;&#23398;&#30028;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#27169;&#22411;&#33719;&#21462;&#26356;&#22909;&#30340;&#35299;&#37322;&#19978;&#65292;&#20294;&#37325;&#35201;&#30340;&#26159;&#19981;&#35201;&#24573;&#35270;&#36825;&#20010;&#35299;&#37322;&#36807;&#31243;&#23545;&#25913;&#21892;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#40657;&#30418;&#27169;&#22411;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#19978;&#65292;&#20294;&#30452;&#25509;&#36890;&#36807;&#36825;&#20123;&#35780;&#20272;&#26469;&#22686;&#24378;&#27169;&#22411;&#20173;&#23384;&#22312;&#20851;&#38190;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SHIELD&#65288;&#36873;&#25321;&#24615;&#38544;&#34255;&#36755;&#20837;&#35780;&#20272;&#23398;&#20064;&#21160;&#24577;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#26088;&#22312;&#36890;&#36807;&#38544;&#34255;&#37096;&#20998;&#36755;&#20837;&#25968;&#25454;&#24182;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#30340;&#24046;&#24322;&#26469;&#25913;&#21892;&#27169;&#22411;&#36136;&#37327;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;SHIELD&#27491;&#21017;&#21270;&#26080;&#32541;&#38598;&#25104;&#21040;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21516;&#26102;&#20063;&#25913;&#21892;&#20102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02611v1 Announce Type: new  Abstract: As Artificial Intelligence systems become integral across domains, the demand for explainability grows. While the effort by the scientific community is focused on obtaining a better explanation for the model, it is important not to ignore the potential of this explanation process to improve training as well. While existing efforts primarily focus on generating and evaluating explanations for black-box models, there remains a critical gap in directly enhancing models through these evaluations. This paper introduces SHIELD (Selective Hidden Input Evaluation for Learning Dynamics), a regularization technique for explainable artificial intelligence designed to improve model quality by concealing portions of input data and assessing the resulting discrepancy in predictions. In contrast to conventional approaches, SHIELD regularization seamlessly integrates into the objective function, enhancing model explainability while also improving perfor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Affective-NLI&#29992;&#20110;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#23545;&#35805;&#20013;&#20154;&#26684;&#35782;&#21035;&#65292;&#20197;&#21033;&#29992;&#23545;&#35805;&#20869;&#23481;&#20013;&#30340;&#24773;&#24863;&#22240;&#32032;&#36827;&#34892;&#20934;&#30830;&#30340;&#20154;&#26684;&#35782;&#21035;</title><link>https://arxiv.org/abs/2404.02589</link><description>&lt;p&gt;
Affective-NLI: &#26397;&#30528;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#23545;&#35805;&#20013;&#20154;&#26684;&#35782;&#21035;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Affective-NLI: Towards Accurate and Interpretable Personality Recognition in Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Affective-NLI&#29992;&#20110;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#23545;&#35805;&#20013;&#20154;&#26684;&#35782;&#21035;&#65292;&#20197;&#21033;&#29992;&#23545;&#35805;&#20869;&#23481;&#20013;&#30340;&#24773;&#24863;&#22240;&#32032;&#36827;&#34892;&#20934;&#30830;&#30340;&#20154;&#26684;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#20154;&#26684;&#35782;&#21035;&#65288;PRC&#65289;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#23545;&#35805;&#20869;&#23481;&#35782;&#21035;&#35828;&#35805;&#32773;&#30340;&#20154;&#26684;&#29305;&#24449;&#12290;&#36825;&#23545;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#27835;&#30103;&#21644;&#20026;&#32769;&#24180;&#20154;&#25552;&#20379;&#20276;&#20387;&#26426;&#22120;&#20154;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20998;&#26512;&#23545;&#35805;&#20869;&#23481;&#36827;&#34892;&#20154;&#26684;&#20998;&#31867;&#65292;&#20294;&#24573;&#30053;&#20102;&#24433;&#21709;&#20854;&#24615;&#33021;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#23545;&#35805;&#20013;&#21253;&#21547;&#30340;&#20851;&#38190;&#38544;&#21547;&#22240;&#32032;&#65288;&#22914;&#21453;&#26144;&#35828;&#35805;&#32773;&#20154;&#26684;&#30340;&#24773;&#32490;&#65289;&#34987;&#24573;&#30053;&#12290;&#20854;&#27425;&#65292;&#20165;&#20851;&#27880;&#36755;&#20837;&#23545;&#35805;&#20869;&#23481;&#24573;&#30053;&#20102;&#23545;&#20010;&#24615;&#26412;&#36523;&#35821;&#20041;&#29702;&#35299;&#65292;&#38477;&#20302;&#20102;&#32467;&#26524;&#30340;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;PRC&#30340;&#24773;&#24863;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;Affective-NLI&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02589v1 Announce Type: cross  Abstract: Personality Recognition in Conversation (PRC) aims to identify the personality traits of speakers through textual dialogue content. It is essential for providing personalized services in various applications of Human-Computer Interaction (HCI), such as AI-based mental therapy and companion robots for the elderly. Most recent studies analyze the dialog content for personality classification yet overlook two major concerns that hinder their performance. First, crucial implicit factors contained in conversation, such as emotions that reflect the speakers' personalities are ignored. Second, only focusing on the input dialog content disregards the semantic understanding of personality itself, which reduces the interpretability of the results. In this paper, we propose Affective Natural Language Inference (Affective-NLI) for accurate and interpretable PRC. To utilize affectivity within dialog content for accurate personality recognition, we 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#25193;&#23637;&#21644;&#22256;&#38590;&#26597;&#35810;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22256;&#38590;&#26597;&#35810;&#30340;&#25490;&#24207;&#24615;&#33021;&#65292;&#32780;&#19981;&#38477;&#20302;&#20854;&#20182;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02587</link><description>&lt;p&gt;
&#35757;&#32451;&#25193;&#23637;&#26597;&#35810;&#30340;&#25490;&#24207;&#22120;&#30340;&#20986;&#20046;&#24847;&#26009;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Surprising Effectiveness of Rankers Trained on Expanded Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02587
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#25193;&#23637;&#21644;&#22256;&#38590;&#26597;&#35810;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22256;&#38590;&#26597;&#35810;&#30340;&#25490;&#24207;&#24615;&#33021;&#65292;&#32780;&#19981;&#38477;&#20302;&#20854;&#20182;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25490;&#24207;&#31995;&#32479;&#20013;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22788;&#29702;&#26597;&#35810;&#20998;&#24067;&#23614;&#37096;&#30340;&#22256;&#38590;&#26597;&#35810;&#12290;&#36825;&#31181;&#22256;&#38590;&#21487;&#33021;&#28304;&#20110;&#23384;&#22312;&#19981;&#24120;&#35265;&#12289;&#26410;&#26126;&#30830;&#25110;&#19981;&#23436;&#25972;&#30340;&#26597;&#35810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30456;&#20851;&#25991;&#26723;&#23545;&#35757;&#32451;&#26597;&#35810;&#36827;&#34892;&#20102;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#25193;&#23637;&#26469;&#25552;&#39640;&#22256;&#38590;&#26597;&#35810;&#30340;&#25490;&#24207;&#24615;&#33021;&#65292;&#32780;&#19981;&#25439;&#23475;&#20854;&#20182;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;LLM&#36827;&#34892;&#26597;&#35810;&#20016;&#23500;&#21270;&#65292;&#20351;&#29992;&#30456;&#20851;&#25991;&#26723;&#36827;&#34892;&#35757;&#32451;&#12290;&#25509;&#19979;&#26469;&#65292;&#19987;&#38376;&#30340;&#25490;&#24207;&#22120;&#20165;&#22312;&#20016;&#23500;&#30340;&#22256;&#38590;&#26597;&#35810;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#19981;&#26159;&#22312;&#21407;&#22987;&#26597;&#35810;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23558;&#26469;&#33258;&#19987;&#38376;&#25490;&#24207;&#22120;&#21644;&#22522;&#26412;&#25490;&#24207;&#22120;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#20197;&#21450;&#20026;&#27599;&#20010;&#26597;&#35810;&#20272;&#35745;&#30340;&#26597;&#35810;&#24615;&#33021;&#24471;&#20998;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#36890;&#24120;&#23545;&#25152;&#26377;&#26597;&#35810;&#20351;&#29992;&#21333;&#20010;&#25490;&#24207;&#22120;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#26131;&#26597;&#35810;&#26377;&#20559;&#35265;&#65292;&#26131;&#26597;&#35810;&#26500;&#25104;&#26597;&#35810;&#20998;&#24067;&#30340;&#22823;&#22810;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02587v1 Announce Type: cross  Abstract: An important problem in text-ranking systems is handling the hard queries that form the tail end of the query distribution. The difficulty may arise due to the presence of uncommon, underspecified, or incomplete queries. In this work, we improve the ranking performance of hard or difficult queries without compromising the performance of other queries. Firstly, we do LLM based query enrichment for training queries using relevant documents. Next, a specialized ranker is fine-tuned only on the enriched hard queries instead of the original queries. We combine the relevance scores from the specialized ranker and the base ranker, along with a query performance score estimated for each query. Our approach departs from existing methods that usually employ a single ranker for all queries, which is biased towards easy queries, which form the majority of the query distribution. In our extensive experiments on the DL-Hard dataset, we find that a p
&lt;/p&gt;</description></item><item><title>&#20027;&#21160;&#23398;&#20064;&#22312;&#20892;&#19994;&#39046;&#22495;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#19977;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#33719;&#21462;&#20989;&#25968;&#65288;BALD&#12289;PowerBALD&#21644;&#38543;&#26426;&#36873;&#25321;&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02580</link><description>&lt;p&gt;
&#20892;&#19994;&#31934;&#20934;&#31181;&#26893;&#20013;&#30340;&#39640;&#25928;&#20027;&#21160;&#23398;&#20064;&#65306;&#20316;&#29289;-&#26434;&#33609;&#35821;&#20041;&#20998;&#21106;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Active learning for efficient annotation in precision agriculture: a use-case on crop-weed semantic segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02580
&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#22312;&#20892;&#19994;&#39046;&#22495;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#19977;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#33719;&#21462;&#20989;&#25968;&#65288;BALD&#12289;PowerBALD&#21644;&#38543;&#26426;&#36873;&#25321;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#22270;&#20687;&#65292;&#36825;&#20010;&#36807;&#31243;&#26082;&#32791;&#26102;&#21448;&#26114;&#36149;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#27599;&#20010;&#20687;&#32032;&#37117;&#24517;&#39035;&#36827;&#34892;&#27880;&#37322;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#12290;&#32531;&#35299;&#27880;&#37322;&#24037;&#20316;&#37327;&#30340;&#19968;&#20010;&#28508;&#22312;&#31574;&#30053;&#26159;&#20027;&#21160;&#23398;&#20064;&#12290;&#20027;&#21160;&#23398;&#20064;&#26377;&#21161;&#20110;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#20013;&#35782;&#21035;&#21644;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#22270;&#20687;&#12290;&#22522;&#26412;&#20551;&#35774;&#26159;&#65292;&#36825;&#20123;&#36873;&#23450;&#30340;&#22270;&#20687;&#21487;&#20197;&#27604;&#38543;&#26426;&#36873;&#25321;&#26356;&#24555;&#22320;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#20174;&#32780;&#20943;&#23569;&#27880;&#37322;&#24037;&#20316;&#12290;&#34429;&#28982;&#20027;&#21160;&#23398;&#20064;&#22312;Cityscapes&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24050;&#32463;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20892;&#19994;&#39046;&#22495;&#30340;&#34920;&#29616;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19977;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#37319;&#38598;&#20989;&#25968;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65306;&#22522;&#20110;&#19981;&#19968;&#33268;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#65288;BALD&#65289;&#12289;&#22522;&#20110;&#38543;&#26426;&#30340;BALD (PowerBALD) &#21644;&#38543;&#26426;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02580v1 Announce Type: cross  Abstract: Optimizing deep learning models requires large amounts of annotated images, a process that is both time-intensive and costly. Especially for semantic segmentation models in which every pixel must be annotated. A potential strategy to mitigate annotation effort is active learning. Active learning facilitates the identification and selection of the most informative images from a large unlabelled pool. The underlying premise is that these selected images can improve the model's performance faster than random selection to reduce annotation effort. While active learning has demonstrated promising results on benchmark datasets like Cityscapes, its performance in the agricultural domain remains largely unexplored. This study addresses this research gap by conducting a comparative study of three active learning-based acquisition functions: Bayesian Active Learning by Disagreement (BALD), stochastic-based BALD (PowerBALD), and Random. The acqui
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#22810;&#20010;&#27169;&#22411;&#30340;&#26032;&#24402;&#32435;&#26041;&#27861;&#65292;&#27599;&#20010;&#27169;&#22411;&#20195;&#34920;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;</title><link>https://arxiv.org/abs/2404.02579</link><description>&lt;p&gt;
&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26367;&#20195;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning Alternative Ways of Performing a Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#22810;&#20010;&#27169;&#22411;&#30340;&#26032;&#24402;&#32435;&#26041;&#27861;&#65292;&#27599;&#20010;&#27169;&#22411;&#20195;&#34920;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#24335;&#26159;&#35266;&#23519;&#19987;&#23478;&#22914;&#20309;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#25191;&#34892;&#23427;&#20204;&#30340;&#26041;&#24335;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#21152;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#19987;&#23478;&#30340;&#25216;&#33021;&#25110;&#30693;&#35782;&#21487;&#33021;&#20250;&#24433;&#21709;&#22905;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#20174;&#19987;&#23478;&#37027;&#37324;&#23398;&#20064;&#20063;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#27604;&#22914;&#35757;&#32451;&#31034;&#20363;&#38598;&#36890;&#24120;&#26469;&#33258;&#20960;&#20301;&#19987;&#23478;&#65288;&#22240;&#20026;&#19987;&#23478;&#36890;&#24120;&#26159;&#26377;&#38480;&#19988;&#26114;&#36149;&#30340;&#36164;&#28304;&#65289;&#65292;&#32780;&#19988;&#25152;&#26377;&#31034;&#20363;&#37117;&#26159;&#27491;&#38754;&#31034;&#20363;&#65288;&#21363;&#20195;&#34920;&#20219;&#21153;&#25104;&#21151;&#25191;&#34892;&#30340;&#31034;&#20363;&#65289;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24182;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22522;&#20110;&#20197;&#27963;&#21160;&#24207;&#21015;&#24418;&#24335;&#21576;&#29616;&#30340;&#20219;&#21153;&#26497;&#23569;&#27425;&#25191;&#34892;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#20010;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#20195;&#34920;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02579v1 Announce Type: new  Abstract: A common way of learning to perform a task is to observe how it is carried out by experts. However, it is well known that for most tasks there is no unique way to perform them. This is especially noticeable the more complex the task is because factors such as the skill or the know-how of the expert may well affect the way she solves the task. In addition, learning from experts also suffers of having a small set of training examples generally coming from several experts (since experts are usually a limited and expensive resource), being all of them positive examples (i.e. examples that represent successful executions of the task). Traditional machine learning techniques are not useful in such scenarios, as they require extensive training data. Starting from very few executions of the task presented as activity sequences, we introduce a novel inductive approach for learning multiple models, with each one representing an alternative strateg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23433;&#20840;&#39640;&#25928;&#22320;&#23398;&#20064;&#26426;&#22120;&#20154;&#39135;&#29289;&#20999;&#21106;&#20219;&#21153;&#30340;Dual Simulator&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.02569</link><description>&lt;p&gt;
SliceIt! -- &#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#26426;&#22120;&#20154;&#39135;&#29289;&#20999;&#21106;&#30340;&#21452;&#37325;&#27169;&#25311;&#22120;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SliceIt! -- A Dual Simulator Framework for Learning Robot Food Slicing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23433;&#20840;&#39640;&#25928;&#22320;&#23398;&#20064;&#26426;&#22120;&#20154;&#39135;&#29289;&#20999;&#21106;&#20219;&#21153;&#30340;Dual Simulator&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21416;&#25151;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#20943;&#36731;&#26085;&#24120;&#28902;&#29712;&#20219;&#21153;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#23478;&#24237;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#21361;&#38505;&#24037;&#20855;&#65288;&#22914;&#21416;&#25151;&#20992;&#20855;&#65289;&#26102;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#24517;&#39035;&#22312;&#20849;&#20139;&#20154;&#31867;&#29615;&#22659;&#20013;&#28789;&#24039;&#19988;&#23433;&#20840;&#22320;&#25191;&#34892;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#20027;&#19988;&#23433;&#20840;&#22320;&#23398;&#20064;&#39135;&#29289;&#20999;&#21106;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#36866;&#24212;&#24615;&#25511;&#21046;&#65292;&#20351;&#29992;&#21327;&#20316;&#26426;&#22120;&#20154;&#25110;&#24037;&#19994;&#26426;&#22120;&#20154;&#25163;&#33218;&#25191;&#34892;&#39135;&#29289;&#20999;&#21106;&#20219;&#21153;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#26448;&#26009;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#26426;&#22120;&#20154;&#20197;&#21512;&#35268;&#26041;&#24335;&#25805;&#20316;&#20992;&#20855;&#65292;&#20943;&#23569;&#39135;&#29289;&#21644;&#20999;&#33756;&#26495;&#26045;&#21152;&#30340;&#25509;&#35302;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12289;&#21361;&#38505;&#65292;&#24182;&#23548;&#33268;&#22823;&#37327;&#39135;&#29289;&#28010;&#36153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SliceIt!&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23433;&#20840;&#39640;&#25928;&#22320;&#23398;&#20064;&#26426;&#22120;&#20154;&#39135;&#29289;&#20999;&#21106;&#20219;&#21153;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02569v1 Announce Type: cross  Abstract: Cooking robots can enhance the home experience by reducing the burden of daily chores. However, these robots must perform their tasks dexterously and safely in shared human environments, especially when handling dangerous tools such as kitchen knives. This study focuses on enabling a robot to autonomously and safely learn food-cutting tasks. More specifically, our goal is to enable a collaborative robot or industrial robot arm to perform food-slicing tasks by adapting to varying material properties using compliance control. Our approach involves using Reinforcement Learning (RL) to train a robot to compliantly manipulate a knife, by reducing the contact forces exerted by the food items and by the cutting board. However, training the robot in the real world can be inefficient, and dangerous, and result in a lot of food waste. Therefore, we proposed SliceIt!, a framework for safely and efficiently learning robot food-slicing tasks in sim
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;SDO/AIA&#25968;&#25454;&#19978;&#21019;&#24314;&#21512;&#25104;&#22826;&#38451;&#22270;&#20687;&#65292;&#20197;&#35299;&#20915;&#22826;&#38451;&#27963;&#21160;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.02552</link><description>&lt;p&gt;
&#22826;&#38451;&#21512;&#25104;&#25104;&#20687;&#65306;&#22312;SDO/AIA&#25968;&#25454;&#19978;&#24341;&#20837;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Solar synthetic imaging: Introducing denoising diffusion probabilistic models on SDO/AIA data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02552
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;SDO/AIA&#25968;&#25454;&#19978;&#21019;&#24314;&#21512;&#25104;&#22826;&#38451;&#22270;&#20687;&#65292;&#20197;&#35299;&#20915;&#22826;&#38451;&#27963;&#21160;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26174;&#33879;&#22826;&#38451;&#32768;&#26001;&#30340;&#32597;&#35265;&#24615;&#30456;&#23545;&#20110;&#36739;&#23567;&#30340;&#32768;&#26001;&#65292;&#35757;&#32451;&#26377;&#25928;&#30340;&#22826;&#38451;&#27963;&#21160;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25968;&#25454;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#65292;&#20197;&#21019;&#24314;&#22826;&#38451;&#29616;&#35937;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#21253;&#25324;&#19981;&#21516;&#24378;&#24230;&#30340;&#32768;&#26001;&#12290;&#36890;&#36807;&#21033;&#29992;SDO&#33322;&#22825;&#22120;&#19978;AIA&#20202;&#22120;&#30340;&#25968;&#25454;&#38598;&#65292;&#38598;&#20013;&#22312;&#25429;&#25417;&#21508;&#31181;&#22826;&#38451;&#27963;&#21160;&#30340;171 &#197;&#39057;&#27573;&#65292;&#24182;&#26681;&#25454;&#32768;&#26001;&#24378;&#24230;&#20351;&#29992;GOES X&#23556;&#32447;&#27979;&#37327;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20351;&#29992;&#32858;&#31867;&#25351;&#26631;&#12289;Frechet Inception Distance&#65288;FID&#65289;&#21644;F1-score&#35780;&#20272;DDPM&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#29983;&#25104;&#36924;&#30495;&#22826;&#38451;&#22270;&#20687;&#30340;&#26377;&#24076;&#26395;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20004;&#20010;&#23454;&#39564;&#65306;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#30417;&#30563;&#20998;&#31867;&#22120;&#36827;&#34892;&#20107;&#20214;&#35782;&#21035;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#22522;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02552v1 Announce Type: cross  Abstract: Given the rarity of significant solar flares compared to smaller ones, training effective machine learning models for solar activity forecasting is challenging due to insufficient data. This study proposes using generative deep learning models, specifically a Denoising Diffusion Probabilistic Model (DDPM), to create synthetic images of solar phenomena, including flares of varying intensities. By employing a dataset from the AIA instrument aboard the SDO spacecraft, focusing on the 171 {\AA} band that captures various solar activities, and classifying images with GOES X-ray measurements based on flare intensity, we aim to address the data scarcity issue. The DDPM's performance is evaluated using cluster metrics, Frechet Inception Distance (FID), and F1-score, showcasing promising results in generating realistic solar imagery. We conduct two experiments: one to train a supervised classifier for event identification and another for basic 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;GPT-3.5-Turbo&#27169;&#22411;&#20316;&#20026;AI&#36741;&#23548;&#21592;&#38598;&#25104;&#21040;APAS Artemis&#20013;&#65292;&#25506;&#35752;&#20102;&#36719;&#20214;&#24037;&#31243;&#25945;&#32946;&#20013;&#23398;&#29983;&#19982;AI&#36741;&#23548;&#21592;&#30340;&#20114;&#21160;&#27169;&#24335;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#29992;&#25143;&#31867;&#22411;&#65292;&#24182;&#24378;&#35843;&#20102;&#21450;&#26102;&#21453;&#39304;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.02548</link><description>&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#25945;&#32946;&#20013;&#30340;AI&#36741;&#23548;
&lt;/p&gt;
&lt;p&gt;
AI-Tutoring in Software Engineering Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;GPT-3.5-Turbo&#27169;&#22411;&#20316;&#20026;AI&#36741;&#23548;&#21592;&#38598;&#25104;&#21040;APAS Artemis&#20013;&#65292;&#25506;&#35752;&#20102;&#36719;&#20214;&#24037;&#31243;&#25945;&#32946;&#20013;&#23398;&#29983;&#19982;AI&#36741;&#23548;&#21592;&#30340;&#20114;&#21160;&#27169;&#24335;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#29992;&#25143;&#31867;&#22411;&#65292;&#24182;&#24378;&#35843;&#20102;&#21450;&#26102;&#21453;&#39304;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#25945;&#32946;&#34892;&#19994;&#27491;&#38754;&#20020;&#21464;&#38761;&#12290;AI&#39537;&#21160;&#24037;&#20855;&#22312;&#22686;&#24378;&#23398;&#20064;&#20307;&#39564;&#26041;&#38754;&#30340;&#28508;&#21147;&#26159;&#24040;&#22823;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#32534;&#31243;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#33258;&#21160;&#21270;&#32534;&#31243;&#35780;&#20272;&#31995;&#32479;(APASs)&#20013;&#20316;&#20026;AI&#36741;&#23548;&#21592;&#30340;&#31185;&#23398;&#35780;&#20272;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#20102;&#35299;&#23398;&#29983;&#22914;&#20309;&#19982;&#36825;&#20123;AI&#36741;&#23548;&#21592;&#20114;&#21160;&#65292;&#24182;&#20998;&#26512;&#20182;&#20204;&#30340;&#20307;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;GPT-3.5-Turbo&#27169;&#22411;&#20316;&#20026;AI&#36741;&#23548;&#21592;&#38598;&#25104;&#21040;APAS Artemis&#20013;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#26696;&#20363;&#30740;&#31350;&#12290;&#36890;&#36807;&#32467;&#21512;&#23454;&#35777;&#25968;&#25454;&#25910;&#38598;&#21644;&#25506;&#32034;&#24615;&#35843;&#26597;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22522;&#20110;&#20182;&#20204;&#19982;AI&#36741;&#23548;&#21592;&#20114;&#21160;&#27169;&#24335;&#30340;&#19981;&#21516;&#29992;&#25143;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#35832;&#22914;&#21450;&#26102;&#21453;&#39304;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22914;&#36890;&#29992;&#39046;&#22495;&#22238;&#22797;&#31561;&#20063;&#21516;&#26679;&#20540;&#24471;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02548v1 Announce Type: cross  Abstract: With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences. In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic resp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#39046;&#22495;&#30340;&#26032;&#30340;&#35745;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#26041;&#27861;&#65288;GPC&#65289;&#65292;&#20197;&#36866;&#24212;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#24809;&#32602;Q&#20540;&#30340;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.02545</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#39046;&#22495;&#30340;&#26032;&#30340;&#35745;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#26041;&#27861;&#65288;GPC&#65289;&#65292;&#20197;&#36866;&#24212;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#24809;&#32602;Q&#20540;&#30340;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#32780;&#19981;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36825;&#30830;&#20445;&#20102;&#23433;&#20840;&#24615;&#24182;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#26420;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#22833;&#36133;&#65292;&#22240;&#20026;&#30001;&#20110;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#34892;&#20026;&#24341;&#36215;&#30340;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#31639;&#27861;&#20027;&#35201;&#24809;&#32602;OOD&#34892;&#20026;&#30340;Q&#20540;&#65292;&#20854;&#32422;&#26463;&#30340;&#36136;&#37327;&#20063;&#24456;&#37325;&#35201;&#12290;&#19981;&#31934;&#30830;&#30340;&#32422;&#26463;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#65292;&#32780;&#31934;&#30830;&#30340;&#32422;&#26463;&#21017;&#38656;&#35201;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#39046;&#22495;&#35745;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#26041;&#27861;&#65288;GPC&#65289;&#65292;&#20197;&#36866;&#24403;&#22320;&#24809;&#32602;Q&#20540;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#26144;&#23556;&#21040;&#31163;&#25955;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#20266;&#35745;&#25968;&#32422;&#26463;&#23427;&#20204;&#30340;Q&#20540;&#12290;&#36825;&#26159;&#19968;&#20010;&#29702;&#35770;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02545v1 Announce Type: cross  Abstract: Offline reinforcement learning learns from a static dataset without interacting with the environment, which ensures security and thus owns a good prospect of application. However, directly applying naive reinforcement learning methods usually fails in an offline environment due to function approximation errors caused by out-of-distribution(OOD) actions. To solve this problem, existing algorithms mainly penalize the Q-value of OOD actions, the quality of whose constraints also matter. Imprecise constraints may lead to suboptimal solutions, while precise constraints require significant computational costs. In this paper, we propose a novel count-based method for continuous domains, called Grid-Mapping Pseudo-Count method(GPC), to penalize the Q-value appropriately and reduce the computational cost. The proposed method maps the state and action space to discrete space and constrains their Q-values through the pseudo-count. It is theoretic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#30334;&#24230;&#25628;&#32034;&#24341;&#25806;&#21457;&#24067;&#30340;&#22823;&#35268;&#27169;&#25628;&#32034;&#25968;&#25454;&#38598;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#22312;&#23454;&#38469;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#19982;&#25490;&#21517;&#25439;&#22833;&#21644;&#26597;&#35810;-&#25991;&#26723;&#29305;&#24449;&#36873;&#25321;&#30456;&#27604;&#65292;ULTR&#25216;&#26415;&#24182;&#26410;&#24102;&#26469;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.02543</link><description>&lt;p&gt;
&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#36935;&#21040;&#29616;&#23454;&#65306;&#30334;&#24230;&#22823;&#35268;&#27169;&#25628;&#32034;&#25968;&#25454;&#38598;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Unbiased Learning to Rank Meets Reality: Lessons from Baidu's Large-Scale Search Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#30334;&#24230;&#25628;&#32034;&#24341;&#25806;&#21457;&#24067;&#30340;&#22823;&#35268;&#27169;&#25628;&#32034;&#25968;&#25454;&#38598;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#22312;&#23454;&#38469;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#19982;&#25490;&#21517;&#25439;&#22833;&#21644;&#26597;&#35810;-&#25991;&#26723;&#29305;&#24449;&#36873;&#25321;&#30456;&#27604;&#65292;ULTR&#25216;&#26415;&#24182;&#26410;&#24102;&#26469;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#65288;ULTR&#65289;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#28857;&#20987;&#25968;&#25454;&#30340;&#25104;&#29087;&#26694;&#26550;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#21463;&#25910;&#38598;&#25968;&#25454;&#30340;&#25490;&#21517;&#32773;&#30340;&#20559;&#35265;&#24433;&#21709;&#12290;&#34429;&#28982;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#35777;&#26126;&#24182;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#65292;&#20294;ULTR&#25216;&#26415;&#32570;&#20047;&#32463;&#39564;&#39564;&#35777;&#65292;&#23588;&#20854;&#26159;&#22312;&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#20013;&#12290;&#30334;&#24230;&#25628;&#32034;&#24341;&#25806;&#21457;&#24067;&#30340;WSDM Cup 2023&#25968;&#25454;&#38598;&#20026;&#35780;&#20272;&#20027;&#35201;ULTR&#25216;&#26415;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#25552;&#20379;&#20102;&#38590;&#24471;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#22312;WSDM Cup 2023&#26399;&#38388;&#26377;&#22810;&#27425;&#25552;&#20132;&#65292;&#20197;&#21450;&#38543;&#21518;&#30340;NTCIR ULTRE-2&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#35266;&#23519;&#21040;&#30340;&#25913;&#36827;&#26159;&#21542;&#28304;&#33258;&#24212;&#29992;ULTR&#25110;&#20854;&#20182;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24182;&#25193;&#23637;&#20102;&#29616;&#26377;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#25552;&#21319;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#19982;&#25490;&#21517;&#25439;&#22833;&#21644;&#26597;&#35810;-&#25991;&#26723;&#29305;&#24449;&#36873;&#25321;&#24102;&#26469;&#30340;&#26126;&#26174;&#24046;&#24322;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02543v1 Announce Type: cross  Abstract: Unbiased learning-to-rank (ULTR) is a well-established framework for learning from user clicks, which are often biased by the ranker collecting the data. While theoretically justified and extensively tested in simulation, ULTR techniques lack empirical validation, especially on modern search engines. The dataset released for the WSDM Cup 2023, collected from Baidu's search engine, offers a rare opportunity to assess the real-world performance of prominent ULTR techniques. Despite multiple submissions during the WSDM Cup 2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed improvements stem from applying ULTR or other learning techniques. We revisit and extend the available experiments. We find that unbiased learning-to-rank techniques do not bring clear performance improvements, especially compared to the stark differences brought by the choice of ranking loss and query-document features. Our experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22235;&#20010;&#19987;&#38376;&#38024;&#23545;&#23433;&#21733;&#25289;&#35821;&#35328;&#36827;&#34892;&#24494;&#35843;&#30340;PLM&#65292;&#24182;&#20351;&#29992;&#22810;&#35821;&#35328;&#33258;&#36866;&#24212;&#24494;&#35843;&#65288;MAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#30693;&#24773;&#23884;&#20837;&#21021;&#22987;&#21270;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;MAFT&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#23558;&#22522;&#32447;&#25552;&#39640;&#20102;12.3&#20010;&#30334;&#20998;&#28857;&#65292;&#36229;&#36234;&#20102;SOTA AfroXLMR-base&#21644;OFA&#12290;</title><link>https://arxiv.org/abs/2404.02534</link><description>&lt;p&gt;
ANGOFA&#65306;&#21033;&#29992;OFA&#23884;&#20837;&#21021;&#22987;&#21270;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#23433;&#21733;&#25289;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22235;&#20010;&#19987;&#38376;&#38024;&#23545;&#23433;&#21733;&#25289;&#35821;&#35328;&#36827;&#34892;&#24494;&#35843;&#30340;PLM&#65292;&#24182;&#20351;&#29992;&#22810;&#35821;&#35328;&#33258;&#36866;&#24212;&#24494;&#35843;&#65288;MAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#30693;&#24773;&#23884;&#20837;&#21021;&#22987;&#21270;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;MAFT&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#23558;&#22522;&#32447;&#25552;&#39640;&#20102;12.3&#20010;&#30334;&#20998;&#28857;&#65292;&#36229;&#36234;&#20102;SOTA AfroXLMR-base&#21644;OFA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#21457;&#23637;&#21183;&#22836;&#36805;&#29467;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#36229;&#36234;&#35821;&#35328;&#38556;&#30861;&#12289;&#20419;&#36827;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#20027;&#35201;&#24573;&#35270;&#20102;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21253;&#21547;&#65292;&#23548;&#33268;&#22810;&#35821;&#35328;&#26223;&#35266;&#20013;&#20986;&#29616;&#26126;&#26174;&#30340;&#31354;&#30333;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22235;&#20010;&#23450;&#21046;&#30340;PLM&#65292;&#19987;&#38376;&#20026;&#23433;&#21733;&#25289;&#35821;&#35328;&#36827;&#34892;&#24494;&#35843;&#65292;&#37319;&#29992;&#22810;&#35821;&#35328;&#33258;&#36866;&#24212;&#24494;&#35843;&#65288;MAFT&#65289;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20449;&#24687;&#23884;&#20837;&#21021;&#22987;&#21270;&#21644;&#21512;&#25104;&#25968;&#25454;&#22312;&#22686;&#24378;MAFT&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#23558;&#22522;&#32447;&#25552;&#39640;&#20102;12.3&#20010;&#30334;&#20998;&#28857;&#65292;&#36229;&#36807;&#20102;&#36890;&#36807;MAFT&#24320;&#21457;&#30340;SOTA AfroXLMR-base&#21644;&#26377;&#25928;&#23884;&#20837;&#21021;&#22987;&#21270;OFA&#20998;&#21035;&#25552;&#39640;&#20102;3.8&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02534v1 Announce Type: cross  Abstract: In recent years, the development of pre-trained language models (PLMs) has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate knowledge transfer across diverse languages. However, this progress has predominantly bypassed the inclusion of very-low resource languages, creating a notable void in the multilingual landscape. This paper addresses this gap by introducing four tailored PLMs specifically finetuned for Angolan languages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In this paper, we survey the role of informed embedding initialization and synthetic data in enhancing the performance of MAFT models in downstream tasks. We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA (an effective embedding initialization) by 12.3 and 3.8 points respectively.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#25915;&#20987;&#32773;-&#20266;&#35013;&#32773;&#21338;&#24328;&#26041;&#27861;&#65292;&#23454;&#29616;&#19968;&#31181;&#24369;&#38450;&#24481;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#27169;&#22411;&#33021;&#22815;&#23433;&#20840;&#22320;&#22238;&#22797;&#25915;&#20987;&#32773;&#24182;&#38544;&#34255;&#38450;&#24481;&#24847;&#22270;</title><link>https://arxiv.org/abs/2404.02532</link><description>&lt;p&gt;
&#23398;&#20250;&#20266;&#35013;&#65306;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#25915;&#20987;&#32773;-&#20266;&#35013;&#32773;&#21338;&#24328;&#36991;&#20813;LLM&#30340;&#25298;&#32477;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02532
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#25915;&#20987;&#32773;-&#20266;&#35013;&#32773;&#21338;&#24328;&#26041;&#27861;&#65292;&#23454;&#29616;&#19968;&#31181;&#24369;&#38450;&#24481;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#27169;&#22411;&#33021;&#22815;&#23433;&#20840;&#22320;&#22238;&#22797;&#25915;&#20987;&#32773;&#24182;&#38544;&#34255;&#38450;&#24481;&#24847;&#22270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#30340;&#22686;&#24378;&#24615;&#33021;&#65292;&#22823;&#22411;&#27169;&#22411;&#21487;&#33021;&#24341;&#21457;&#28508;&#22312;&#30340;&#36947;&#24503;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#23384;&#22312;&#19968;&#20123;&#24694;&#24847;&#25915;&#20987;&#32773;&#65292;&#20182;&#20204;&#36890;&#36807;&#35832;&#22914;&#25552;&#31034;&#24037;&#31243;&#31561;&#25216;&#26415;&#35825;&#20351;&#22823;&#22411;&#27169;&#22411;&#36234;&#29425;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;&#38750;&#27861;&#12289;&#20405;&#29359;&#38544;&#31169;&#20449;&#24687;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22823;&#22411;&#27169;&#22411;&#37319;&#29992;&#23433;&#20840;&#23545;&#40784;&#31561;&#25216;&#26415;&#25269;&#24481;&#24694;&#24847;&#25915;&#20987;&#32773;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#27169;&#22411;&#36890;&#36807;&#25298;&#32477;&#22238;&#22797;&#30340;&#24378;&#38450;&#24481;&#26426;&#21046;&#23481;&#26131;&#34987;&#25915;&#20987;&#32773;&#35782;&#21035;&#65292;&#24182;&#29992;&#20110;&#21152;&#24378;&#25915;&#20987;&#32773;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#25915;&#20987;&#32773;-&#20266;&#35013;&#32773;&#21338;&#24328;&#26041;&#27861;&#65292;&#23454;&#29616;&#19968;&#31181;&#24369;&#38450;&#24481;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#27169;&#22411;&#33021;&#22815;&#23433;&#20840;&#22320;&#22238;&#22797;&#25915;&#20987;&#32773;&#24182;&#38544;&#34255;&#38450;&#24481;&#24847;&#22270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#26469;&#27169;&#25311;&#25915;&#20987;&#21644;&#38450;&#24481;&#24773;&#26223;&#65292;&#25198;&#28436;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#36127;&#36131;&#25915;&#20987;&#12289;&#20266;&#35013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02532v1 Announce Type: new  Abstract: With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers' attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disgu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23545;&#20559;&#35265;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23450;&#37327;&#25511;&#21046;&#27169;&#22411;&#20559;&#35265;&#26469;&#25805;&#32437;&#36755;&#20986;&#20005;&#37325;&#24615;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#26032;&#39062;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02530</link><description>&lt;p&gt;
&#20005;&#37325;&#25511;&#21046;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20559;&#35265;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Severity Controlled Text-to-Image Generative Model Bias Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23545;&#20559;&#35265;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23450;&#37327;&#25511;&#21046;&#27169;&#22411;&#20559;&#35265;&#26469;&#25805;&#32437;&#36755;&#20986;&#20005;&#37325;&#24615;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#26032;&#39062;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#27491;&#22312;&#24191;&#27867;&#27969;&#34892;&#65292;&#23588;&#20854;&#26159;&#22312;&#20844;&#20849;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22266;&#26377;&#30340;&#20559;&#35265;&#21644;&#28508;&#22312;&#30340;&#24694;&#24847;&#25805;&#32437;&#36824;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;T2I&#27169;&#22411;&#23545;&#27492;&#31867;&#25805;&#32437;&#30340;&#26131;&#24863;&#24615;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#36890;&#36807;&#38024;&#23545;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#19988;&#39640;&#25928;&#22320;&#21033;&#29992;&#27169;&#22411;&#20559;&#35265;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#21521;&#37327;&#20195;&#25968;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#20559;&#35265;&#36890;&#36807;&#20005;&#37325;&#24615;&#30340;&#36755;&#20986;&#25805;&#32437;&#30340;&#21487;&#25193;&#23637;&#21644;&#26041;&#20415;&#25511;&#21046;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#35813;&#25511;&#21046;&#36824;&#20801;&#35768;&#19968;&#31181;&#31934;&#30830;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#20197;&#29983;&#25104;&#36890;&#24120;&#19981;&#22826;&#21487;&#33021;&#36890;&#36807;&#24120;&#35268;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25805;&#32437;&#25216;&#26415;&#22312;&#24179;&#34913;&#29983;&#25104;&#31867;&#21035;&#39057;&#29575;&#26041;&#38754;&#30340;&#24314;&#35774;&#24212;&#29992; - &#22914;&#22312;&#27169;&#22411;&#21435;&#20559;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#24182;&#19988;&#20063;&#20197;&#21518;&#38376;&#30340;&#24418;&#24335;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02530v1 Announce Type: cross  Abstract: Text-to-image (T2I) generative models are gaining wide popularity, especially in public domains. However, their intrinsic bias and potential malicious manipulations remain under-explored. Charting the susceptibility of T2I models to such manipulation, we first expose the new possibility of a dynamic and computationally efficient exploitation of model bias by targeting the embedded language models. By leveraging mathematical foundations of vector algebra, our technique enables a scalable and convenient control over the severity of output manipulation through model bias. As a by-product, this control also allows a form of precise prompt engineering to generate images which are generally implausible with regular text prompts. We also demonstrate a constructive application of our manipulation for balancing the frequency of generated classes - as in model debiasing. Our technique does not require training and is also framed as a backdoor at
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25351;&#23548;&#20174;&#20027;&#20307;&#35270;&#35282;&#23398;&#20064;&#21487;&#25805;&#20316;&#24615;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#25163;-&#29289;&#20307;&#21644;&#24037;&#20855;-&#29289;&#20307;&#20132;&#20114;&#65292;&#26088;&#22312;&#23398;&#20064;&#25509;&#35302;&#28857;&#21644;&#25805;&#20316;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2404.02523</link><description>&lt;p&gt;
&#20174;&#20027;&#20307;&#35270;&#35282;&#23398;&#20064;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#21487;&#25805;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
Text-driven Affordance Learning from Egocentric Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02523
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25351;&#23548;&#20174;&#20027;&#20307;&#35270;&#35282;&#23398;&#20064;&#21487;&#25805;&#20316;&#24615;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#25163;-&#29289;&#20307;&#21644;&#24037;&#20855;-&#29289;&#20307;&#20132;&#20114;&#65292;&#26088;&#22312;&#23398;&#20064;&#25509;&#35302;&#28857;&#21644;&#25805;&#20316;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21487;&#25805;&#20316;&#24615;&#23398;&#20064;&#26159;&#26426;&#22120;&#20154;&#29702;&#35299;&#22914;&#20309;&#19982;&#29289;&#20307;&#20132;&#20114;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#65292;&#38024;&#23545;&#21508;&#31181;&#29289;&#20307;&#30340;&#21508;&#31181;&#21487;&#25805;&#20316;&#24615;&#12290;&#35813;&#26041;&#27861;&#28085;&#30422;&#25163;-&#29289;&#20307;&#21644;&#24037;&#20855;-&#29289;&#20307;&#20132;&#20114;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25991;&#26412;&#39537;&#21160;&#30340;&#21487;&#25805;&#20316;&#24615;&#23398;&#20064;&#65292;&#26088;&#22312;&#20174;&#20027;&#20307;&#35270;&#35282;&#26681;&#25454;&#25991;&#26412;&#25351;&#23548;&#23398;&#20064;&#25509;&#35302;&#28857;&#21644;&#25805;&#20316;&#36712;&#36857;&#12290;&#22312;&#25105;&#20204;&#30340;&#20219;&#21153;&#20013;&#65292;&#25509;&#35302;&#28857;&#34987;&#34920;&#31034;&#20026;&#28909;&#22270;&#65292;&#25805;&#20316;&#36712;&#36857;&#34987;&#34920;&#31034;&#20026;&#21253;&#21547;&#21508;&#31181;&#25805;&#20316;&#30340;&#32447;&#24615;&#21644;&#26059;&#36716;&#36816;&#21160;&#30340;&#22352;&#26631;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#20026;&#36825;&#20010;&#20219;&#21153;&#25910;&#38598;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#22810;&#26679;&#21270;&#20132;&#20114;&#30340;&#25163;&#21160;&#27880;&#37322;&#26159;&#26114;&#36149;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20266;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02523v1 Announce Type: cross  Abstract: Visual affordance learning is a key component for robots to understand how to interact with objects. Conventional approaches in this field rely on pre-defined objects and actions, falling short of capturing diverse interactions in realworld scenarios. The key idea of our approach is employing textual instruction, targeting various affordances for a wide range of objects. This approach covers both hand-object and tool-object interactions. We introduce text-driven affordance learning, aiming to learn contact points and manipulation trajectories from an egocentric view following textual instruction. In our task, contact points are represented as heatmaps, and the manipulation trajectory as sequences of coordinates that incorporate both linear and rotational movements for various manipulations. However, when we gather data for this task, manual annotations of these diverse interactions are costly. To this end, we propose a pseudo dataset c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#65292;&#20351;&#29992;&#22312;&#32447;&#26657;&#20934;&#35299;&#20915;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#30340;&#28857;&#20113;&#36864;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02515</link><description>&lt;p&gt;
&#21033;&#29992;&#22312;&#32447;&#26657;&#20934;&#36816;&#21160;&#27169;&#22411;&#30340;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#29992;&#20110;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Tightly-Coupled LiDAR-IMU-Wheel Odometry with Online Calibration of a Kinematic Model for Skid-Steering Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02515
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#65292;&#20351;&#29992;&#22312;&#32447;&#26657;&#20934;&#35299;&#20915;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#30340;&#28857;&#20113;&#36864;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38567;&#36947;&#21644;&#38271;&#24266;&#26159;&#31227;&#21160;&#26426;&#22120;&#20154;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#20026;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;LiDAR&#28857;&#20113;&#20250;&#36864;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#28857;&#20113;&#36864;&#21270;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#30340;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#22312;&#32447;&#26657;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#32447;&#24615;&#36718;&#23376;&#37324;&#31243;&#35745;&#22240;&#23376;&#65292;&#19981;&#20165;&#20316;&#20026;&#36816;&#21160;&#32422;&#26463;&#65292;&#36824;&#21487;&#20197;&#25191;&#34892;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#36816;&#21160;&#27169;&#22411;&#30340;&#22312;&#32447;&#26657;&#20934;&#12290;&#23613;&#31649;&#36816;&#21160;&#27169;&#22411;&#21160;&#24577;&#21464;&#21270;&#65288;&#20363;&#22914;&#30001;&#20110;&#32974;&#21387;&#24341;&#36215;&#30340;&#36718;&#32974;&#21322;&#24452;&#21464;&#21270;&#65289;&#21644;&#22320;&#24418;&#26465;&#20214;&#21464;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#22312;&#32447;&#26657;&#20934;&#26469;&#35299;&#20915;&#27169;&#22411;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#36864;&#21270;&#29615;&#22659;&#19979;&#65288;&#22914;&#38271;&#30452;&#24266;&#65289;&#36890;&#36807;&#26657;&#20934;&#32780;&#23454;&#29616;&#20934;&#30830;&#23450;&#20301;&#65292;&#21516;&#26102;LiDAR-IMU&#34701;&#21512;&#36816;&#20316;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#36718;&#23376;&#37324;&#31243;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#21363;&#21327;&#26041;&#24046;&#30697;&#38453;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02515v1 Announce Type: cross  Abstract: Tunnels and long corridors are challenging environments for mobile robots because a LiDAR point cloud should degenerate in these environments. To tackle point cloud degeneration, this study presents a tightly-coupled LiDAR-IMU-wheel odometry algorithm with an online calibration for skid-steering robots. We propose a full linear wheel odometry factor, which not only serves as a motion constraint but also performs the online calibration of kinematic models for skid-steering robots. Despite the dynamically changing kinematic model (e.g., wheel radii changes caused by tire pressures) and terrain conditions, our method can address the model error via online calibration. Moreover, our method enables an accurate localization in cases of degenerated environments, such as long and straight corridors, by calibration while the LiDAR-IMU fusion sufficiently operates. Furthermore, we estimate the uncertainty (i.e., covariance matrix) of the wheel o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#23458;&#25143;&#31471;&#20915;&#31574;&#26641;&#32858;&#21512;&#36807;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#27880;&#20837;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.02510</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#23458;&#25143;&#31471;&#20915;&#31574;&#26641;&#32858;&#21512;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Client Decision Tree Aggregation process for Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#23458;&#25143;&#31471;&#20915;&#31574;&#26641;&#32858;&#21512;&#36807;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#27880;&#20837;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#22312;&#24403;&#20170;&#30340;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20248;&#20808;&#32771;&#34385;&#35832;&#22914;&#40065;&#26834;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#36879;&#26126;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#38544;&#31169;&#24615;&#31561;&#21407;&#21017;&#12290;&#36825;&#23548;&#33268;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#38544;&#31169;&#21644;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#12290;&#20915;&#31574;&#26641;&#20316;&#20026;&#33258;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#22914;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#26159;&#29702;&#24819;&#30340;&#65292;&#20197;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#27880;&#20837;&#21487;&#35299;&#37322;&#24615;&#12290;&#20915;&#31574;&#26641;&#32467;&#26500;&#20351;&#24471;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#32858;&#21512;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#23427;&#20204;&#38656;&#35201;&#33021;&#22815;&#21512;&#24182;&#23427;&#20204;&#30340;&#20915;&#31574;&#36335;&#24452;&#32780;&#19981;&#24341;&#20837;&#20559;&#24046;&#25110;&#36807;&#25311;&#21512;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#20445;&#25345;&#32858;&#21512;&#20915;&#31574;&#26641;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#30340;&#21487;&#35299;&#37322;&#23458;&#25143;&#31471;&#20915;&#31574;&#26641;&#32858;&#21512;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02510v1 Announce Type: cross  Abstract: Trustworthy Artificial Intelligence solutions are essential in today's data-driven applications, prioritizing principles such as robustness, safety, transparency, explainability, and privacy among others. This has led to the emergence of Federated Learning as a solution for privacy and distributed machine learning. While decision trees, as self-explanatory models, are ideal for collaborative model training across multiple devices in resource-constrained environments such as federated learning environments for injecting interpretability in these models. Decision tree structure makes the aggregation in a federated learning environment not trivial. They require techniques that can merge their decision paths without introducing bias or overfitting while keeping the aggregated decision trees robust and generalizable. In this paper, we propose an Interpretable Client Decision Tree Aggregation process for Federated Learning scenarios that kee
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20026;&#35270;&#38556;&#20154;&#22763;&#25552;&#20379;&#35270;&#35273;&#38382;&#39064;&#31572;&#26696;&#65292;&#25552;&#20986;&#20102; VIAssist &#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19981;&#21463;&#27426;&#36814;&#30340;&#22270;&#20687;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#25805;&#20316;&#65292;&#26368;&#32456;&#20381;&#25454;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02508</link><description>&lt;p&gt;
VIAssist&#65306;&#20026;&#35270;&#35273;&#38556;&#30861;&#29992;&#25143;&#35843;&#25972;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20026;&#35270;&#38556;&#20154;&#22763;&#25552;&#20379;&#35270;&#35273;&#38382;&#39064;&#31572;&#26696;&#65292;&#25552;&#20986;&#20102; VIAssist &#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19981;&#21463;&#27426;&#36814;&#30340;&#22270;&#20687;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#25805;&#20316;&#65292;&#26368;&#32456;&#20381;&#25454;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#35270;&#35273;&#38556;&#30861;&#30340;&#20010;&#20307;&#65292;&#21253;&#25324;&#35270;&#35273;&#24863;&#30693;&#26041;&#38754;&#30340;&#37096;&#20998;&#25110;&#23436;&#20840;&#22256;&#38590;&#65292;&#34987;&#31216;&#20026;&#35270;&#38556;&#20154;&#22763;&#12290;&#20840;&#29699;&#20272;&#35745;&#26377;22&#20159;&#20154;&#21463;&#35270;&#21147;&#38556;&#30861;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#38750;&#20961;&#33021;&#21147;&#12290;&#24076;&#26395;&#36890;&#36807;MLLMs&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#26469;&#24110;&#21161;&#35270;&#38556;&#20154;&#22763;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#25429;&#25417;&#29702;&#24819;&#22270;&#20687;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#26085;&#24120;&#38656;&#27714;&#65292;&#35270;&#38556;&#20154;&#22763;&#20351;&#29992;MLLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#30446;&#26631;&#23545;&#35937;&#26410;&#23436;&#20840;&#25110;&#37096;&#20998;&#25918;&#32622;&#22312;&#22270;&#20687;&#20013;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;MLLMs&#20026;&#35270;&#38556;&#20154;&#22763;&#25552;&#20379;&#35270;&#35273;&#38382;&#39064;&#31572;&#26696;&#12290;VIAssist&#33021;&#22815;&#35782;&#21035;&#19981;&#21463;&#27426;&#36814;&#30340;&#22270;&#20687;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#25805;&#20316;&#12290;&#26368;&#21518;&#65292;VIAssist&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#26597;&#35810;&#25552;&#20379;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02508v1 Announce Type: cross  Abstract: Individuals with visual impairments, encompassing both partial and total difficulties in visual perception, are referred to as visually impaired (VI) people. An estimated 2.2 billion individuals worldwide are affected by visual impairments. Recent advancements in multi-modal large language models (MLLMs) have showcased their extraordinary capabilities across various domains. It is desirable to help VI individuals with MLLMs' great capabilities of visual understanding and reasoning. However, it is challenging for VI people to use MLLMs due to the difficulties in capturing the desirable images to fulfill their daily requests. For example, the target object is not fully or partially placed in the image. This paper explores how to leverage MLLMs for VI individuals to provide visual-question answers. VIAssist can identify undesired images and provide detailed actions. Finally, VIAssist can provide reliable answers to users' queries based on
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21160;&#24577;&#28436;&#31034;&#26816;&#32034;&#21644;&#35748;&#30693;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24773;&#32490;&#25903;&#25345;&#23545;&#35805;&#20013;&#25552;&#20379;&#30340;&#25903;&#25345;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02505</link><description>&lt;p&gt;
&#24773;&#32490;&#25903;&#25345;&#23545;&#35805;&#20013;&#30340;&#21160;&#24577;&#28436;&#31034;&#26816;&#32034;&#19982;&#35748;&#30693;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02505
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#28436;&#31034;&#26816;&#32034;&#21644;&#35748;&#30693;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24773;&#32490;&#25903;&#25345;&#23545;&#35805;&#20013;&#25552;&#20379;&#30340;&#25903;&#25345;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#25903;&#25345;&#23545;&#35805;&#65288;ESC&#65289;&#31995;&#32479;&#22312;&#25552;&#20379;&#20849;&#24773;&#20114;&#21160;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#29702;&#35299;&#21644;&#35299;&#20915;&#29992;&#25143;&#29420;&#29305;&#32463;&#21382;&#26469;&#24110;&#21161;&#29992;&#25143;&#24230;&#36807;&#28040;&#26497;&#24773;&#32490;&#29366;&#24577;&#12290;&#26412;&#25991;&#35299;&#20915;ESC&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#36890;&#36807;&#21160;&#24577;&#28436;&#31034;&#26816;&#32034;&#22686;&#24378;&#30456;&#20851;&#35821;&#22659;&#21644;&#20849;&#24773;&#24335;&#22238;&#24212;&#29983;&#25104;&#65292;&#20197;&#21450;&#25512;&#36827;&#35748;&#30693;&#29702;&#35299;&#20197;&#20840;&#38754;&#25226;&#25569;&#38544;&#21547;&#30340;&#24515;&#29702;&#29366;&#24577;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#21160;&#24577;&#28436;&#31034;&#26816;&#32034;&#21644;&#35748;&#30693;&#26041;&#38754;&#24773;&#22659;&#29702;&#35299;&#65288;\ourwork&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#20803;&#32032;&#21327;&#21516;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;ESC&#20013;&#25552;&#20379;&#30340;&#25903;&#25345;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20154;&#35774;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26816;&#32034;&#26426;&#21046;&#65292;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#19988;&#20010;&#24615;&#21270;&#30340;&#28436;&#31034;&#23545;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#29702;&#35299;&#27169;&#22359;&#65292;&#21033;&#29992;&#26469;&#33258;ATOMIC&#30693;&#35782;&#28304;&#30340;&#22235;&#31181;&#35748;&#30693;&#20851;&#31995;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#38544;&#21547;&#30340;&#24515;&#29702;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02505v1 Announce Type: cross  Abstract: Emotional Support Conversation (ESC) systems are pivotal in providing empathetic interactions, aiding users through negative emotional states by understanding and addressing their unique experiences. In this paper, we tackle two key challenges in ESC: enhancing contextually relevant and empathetic response generation through dynamic demonstration retrieval, and advancing cognitive understanding to grasp implicit mental states comprehensively. We introduce Dynamic Demonstration Retrieval and Cognitive-Aspect Situation Understanding (\ourwork), a novel approach that synergizes these elements to improve the quality of support provided in ESCs. By leveraging in-context learning and persona information, we introduce an innovative retrieval mechanism that selects informative and personalized demonstration pairs. We also propose a cognitive understanding module that utilizes four cognitive relationships from the ATOMIC knowledge source to dee
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23398;&#20064;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#22312;&#19968;&#20123; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02499</link><description>&lt;p&gt;
&#23398;&#20064;&#38754;&#21521;&#23436;&#20840;&#21487;&#35266;&#23519;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23398;&#20064;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#22312;&#19968;&#20123; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#31574;&#30053;&#20195;&#34920;&#35299;&#20915;&#22823;&#37327;&#35745;&#21010;&#38382;&#39064;&#30340;&#21453;&#24212;&#24615;&#31574;&#30053;&#65292;&#20363;&#22914;&#20174;&#32473;&#23450;&#39046;&#22495;&#20013;&#26080;&#38480;&#21487;&#35299;&#23454;&#20363;&#30340;&#38598;&#21512;&#12290; &#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19968;&#31995;&#21015;&#23567;&#35757;&#32451;&#23454;&#20363;&#20013;&#23398;&#20064;&#36825;&#31181;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#32463;&#20856;&#39046;&#22495;&#12290; &#26412;&#25991;&#25193;&#23637;&#20102;&#23398;&#20064;&#38754;&#21521;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#65288;FOND&#65289;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#20844;&#24335;&#21644;&#23548;&#33268;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#20123;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#27491;&#30830;&#24615;&#12290; &#23398;&#20064; FOND &#35745;&#21010;&#30340;&#27867;&#21270;&#31574;&#30053;&#26041;&#27861;&#23454;&#38469;&#19978;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#25628;&#32034;&#32467;&#26524;&#30340;&#21478;&#19968;&#31181; FOND &#35745;&#21010;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#26159;&#22312;&#32473;&#23450;&#29366;&#24577;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26159;&#22312;&#30001;&#24517;&#39035;&#23398;&#20064;&#30340;&#29305;&#24449;&#23450;&#20041;&#30340;&#25277;&#35937;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02499v1 Announce Type: new  Abstract: General policies represent reactive strategies for solving large families of planning problems like the infinite collection of solvable instances from a given domain. Methods for learning such policies from a collection of small training instances have been developed successfully for classical domains. In this work, we extend the formulations and the resulting combinatorial methods for learning general policies over fully observable, non-deterministic (FOND) domains. We also evaluate the resulting approach experimentally over a number of benchmark domains in FOND planning, present the general policies that result in some of these domains, and prove their correctness. The method for learning general policies for FOND planning can actually be seen as an alternative FOND planning method that searches for solutions, not in the given state space but in an abstract space defined by features that must be learned as well.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#20294;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2404.02484</link><description>&lt;p&gt;
&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
New methods for drug synergy prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02484
&lt;/p&gt;
&lt;p&gt;
&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#20294;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#23567;&#22411;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20381;&#36182;&#20110;&#39640;&#36890;&#37327;&#32452;&#21512;&#31579;&#36873;&#30340;&#33647;&#29289;&#32452;&#21512;&#21327;&#21516;&#20316;&#29992;&#30340;&#26032;&#39044;&#27979;&#26041;&#27861;&#12290;&#33258;2021&#24180;&#20197;&#26469;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#36805;&#36895;&#36827;&#23637;&#65292;&#24050;&#21457;&#34920;&#20102;&#36229;&#36807;30&#31181;&#21407;&#21019;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#32477;&#22823;&#22810;&#25968;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31361;&#26174;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#26680;&#24515;&#25216;&#26415;&#12289;&#25968;&#25454;&#26469;&#28304;&#12289;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#21644;&#21327;&#21516;&#24471;&#20998;&#65292;&#20197;&#21450;&#35770;&#25991;&#25152;&#28041;&#21450;&#30340;&#39044;&#27979;&#24773;&#26223;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#23558;&#36825;&#20123;&#35770;&#25991;&#25918;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#19979;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#65292;&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#22320;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#32780;&#28041;&#21450;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#24773;&#26223;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02484v1 Announce Type: cross  Abstract: In this mini-review, we explore the new prediction methods for drug combination synergy relying on high-throughput combinatorial screens. The fast progress of the field is witnessed in the more than thirty original machine learning methods published since 2021, a clear majority of them based on deep learning techniques. We aim to put these papers under a unifying lens by highlighting the core technologies, the data sources, the input data types and synergy scores used in the methods, as well as the prediction scenarios and evaluation protocols that the papers deal with. Our finding is that the best methods accurately solve the synergy prediction scenarios involving known drugs or cell lines while the scenarios involving new drugs or cell lines still fall short of an accurate prediction level.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#21551;&#21457;&#30340;&#26032;&#22411;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedSelect&#65292;&#33021;&#22815;&#22312;&#24494;&#35843;&#20013;&#23450;&#21046;&#36873;&#25321;&#32593;&#32476;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#20840;&#23616;&#30693;&#35782;&#23384;&#20648;&#19981;&#22815;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02478</link><description>&lt;p&gt;
FedSelect&#65306;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#36890;&#36807;&#23450;&#21046;&#21270;&#21442;&#25968;&#36873;&#25321;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02478
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#21551;&#21457;&#30340;&#26032;&#22411;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedSelect&#65292;&#33021;&#22815;&#22312;&#24494;&#35843;&#20013;&#23450;&#21046;&#36873;&#25321;&#32593;&#32476;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#20840;&#23616;&#30693;&#35782;&#23384;&#20648;&#19981;&#22815;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#23458;&#25143;&#25968;&#25454;&#20998;&#24067;&#20855;&#26377;&#20805;&#20998;&#24322;&#36136;&#24615;&#26102;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#35299;&#20915;&#20102;&#23458;&#25143;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064; - &#19968;&#31867;&#26088;&#22312;&#20010;&#24615;&#21270;&#23398;&#20064;&#30340;&#20840;&#23616;&#30693;&#35782;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#23458;&#25143;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;FL&#31639;&#27861;&#12290;&#29616;&#26377;&#30340;PFL&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22312;&#29305;&#23450;&#23618;&#65288;&#21363;&#20998;&#31867;&#22120;&#22836;&#37096;&#65289;&#19978;&#25191;&#34892;&#20010;&#24615;&#21270;&#21644;&#23545;&#20854;&#20313;&#32593;&#32476;&#36827;&#34892;&#20840;&#23616;&#32858;&#21512;&#26469;&#35299;&#32806;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#36873;&#25321;&#32593;&#32476;&#23618;&#36827;&#34892;&#20010;&#24615;&#21270;&#21487;&#33021;&#23548;&#33268;&#20840;&#23616;&#30693;&#35782;&#30340;&#23384;&#20648;&#19981;&#22815;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedSelect&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#20013;&#20351;&#29992;&#30340;&#36845;&#20195;&#23376;&#32593;&#32476;&#21457;&#29616;&#36807;&#31243;&#21551;&#21457;&#30340;&#26032;&#22411;PFL&#31639;&#27861;&#12290;FedSelect&#36880;&#27493;&#25193;&#23637;&#23376;&#32593;&#32476;&#20197;&#20010;&#24615;&#21270;&#23458;&#25143;&#21442;&#25968;&#65292;&#24182;&#21516;&#26102;&#23545;&#21097;&#20313;&#37096;&#20998;&#36827;&#34892;&#20840;&#23616;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02478v1 Announce Type: cross  Abstract: Standard federated learning approaches suffer when client data distributions have sufficient heterogeneity. Recent methods addressed the client data heterogeneity issue via personalized federated learning (PFL) - a class of FL algorithms aiming to personalize learned global knowledge to better suit the clients' local data distributions. Existing PFL methods usually decouple global updates in deep neural networks by performing personalization on particular layers (i.e. classifier heads) and global aggregation for the rest of the network. However, preselecting network layers for personalization may result in suboptimal storage of global knowledge. In this work, we propose FedSelect, a novel PFL algorithm inspired by the iterative subnetwork discovery procedure used for the Lottery Ticket Hypothesis. FedSelect incrementally expands subnetworks to personalize client parameters, concurrently conducting global aggregations on the remaining p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#21463;&#38480;&#21046;&#30340;&#22810;&#23567;&#21306;&#32593;&#32476;&#20013;&#36890;&#36807;&#26497;&#23569;&#30340;&#20449;&#24687;&#20132;&#25442;&#23454;&#29616;&#26368;&#22823;&#21270;&#24635;&#36895;&#29575;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02477</link><description>&lt;p&gt;
&#22312;&#21463;&#38480;&#21046;&#30340;&#22810;&#23567;&#21306;&#32593;&#32476;&#20013;&#22686;&#24378;&#24635;&#36895;&#29575;&#24615;&#33021;&#65306;&#19968;&#31181;&#20302;&#20449;&#24687;&#20132;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Sum-Rate Performance in Constrained Multicell Networks: A Low-Information Exchange Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02477
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#21463;&#38480;&#21046;&#30340;&#22810;&#23567;&#21306;&#32593;&#32476;&#20013;&#36890;&#36807;&#26497;&#23569;&#30340;&#20449;&#24687;&#20132;&#25442;&#23454;&#29616;&#26368;&#22823;&#21270;&#24635;&#36895;&#29575;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#20110;5G&#36890;&#20449;&#21450;&#26356;&#39640;&#32423;&#21035;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#23454;&#38469;&#24773;&#20917;&#26159;&#35768;&#22810;&#24050;&#37096;&#32626;&#30340;&#22522;&#31449;&#20165;&#37197;&#22791;&#26377;&#38480;&#25968;&#37327;&#30340;&#22825;&#32447;&#65292;&#32780;&#19981;&#26159;&#25903;&#25345;&#22823;&#35268;&#27169;MIMO&#37197;&#32622;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#28040;&#38500;&#20102;&#23567;&#21306;&#36793;&#30028;&#30340;&#26080;&#23567;&#21306;&#32593;&#32476;&#27010;&#24565;&#27491;&#22312;&#30740;&#31350;&#20013;&#65292;&#20294;&#23454;&#38469;&#37096;&#32626;&#24448;&#24448;&#21463;&#38480;&#20110;&#22522;&#31449;&#20043;&#38388;&#30340;&#26377;&#38480;&#22238;&#20256;&#36830;&#25509;&#23481;&#37327;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#36825;&#20123;&#26356;&#29616;&#23454;&#22320;&#37197;&#32622;&#30340;&#22810;&#23567;&#21306;&#32593;&#32476;&#32422;&#26463;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#24635;&#36895;&#29575;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#22522;&#31449;&#20043;&#38388;&#20449;&#24687;&#20132;&#25442;&#30340;&#38656;&#27714;&#65292;&#20165;&#38656;&#26497;&#23569;&#30340;&#27604;&#29305;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#20132;&#25442;&#25968;&#30334;&#27604;&#29305;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#35299;&#20915;&#20102;&#24403;&#21069;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#26045;&#21152;&#30340;&#38480;&#21046;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02477v1 Announce Type: cross  Abstract: Despite the extensive research on massive MIMO systems for 5G telecommunications and beyond, the reality is that many deployed base stations are equipped with a limited number of antennas rather than supporting massive MIMO configurations. Furthermore, while the cell-less network concept, which eliminates cell boundaries, is under investigation, practical deployments often grapple with significantly limited backhaul connection capacities between base stations. This letter explores techniques to maximize the sum-rate performance within the constraints of these more realistically equipped multicell networks. We propose an innovative approach that dramatically reduces the need for information exchange between base stations to a mere few bits, in stark contrast to conventional methods that require the exchange of hundreds of bits. Our proposed method not only addresses the limitations imposed by current network infrastructure but also show
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02476</link><description>&lt;p&gt;
&#29992;&#20110;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Traveling Purchaser Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#65288;TPP&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#29992;&#20110;&#25429;&#25417;&#24066;&#22330;-&#20135;&#21697;&#20851;&#31995;&#30340;TPP&#30340;&#20108;&#37096;&#22270;&#34920;&#31034;&#65292;&#20197;&#21450;&#20174;&#20108;&#37096;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#29992;&#20110;&#39034;&#24207;&#26500;&#24314;&#36335;&#30001;&#30340;&#31574;&#30053;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#21644;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02474</link><description>&lt;p&gt;
uTeBC-NLP&#22312;SemEval-2024&#20219;&#21153;9&#20013;&#65306;LLMs&#33021;&#25104;&#20026;&#27178;&#21521;&#24605;&#32771;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#21644;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;Jiang&#31561;&#20154;&#65288;2023c&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#27178;&#21521;&#24605;&#32500;&#65288;&#36229;&#36234;&#24605;&#32500;&#23450;&#21183;&#65289;&#30340;&#22522;&#20934;&#12290;&#22312;&#36825;&#19968;&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#25581;&#31034;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#12290;&#36890;&#36807;&#21442;&#21152;SemEval-2024&#30340;&#31532;9&#39033;&#20219;&#21153;&#65292;&#21363;&#21477;&#23376;&#25340;&#22270;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65306;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#21644;&#30452;&#25509;&#25552;&#31034;&#65292;&#20351;&#29992;&#20449;&#24687;&#24615;&#25551;&#36848;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31649;&#36947;&#36827;&#34892;&#24773;&#22659;&#21270;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#19977;&#31181;LLMs&#65292;&#21253;&#25324;GPT-3.5&#12289;GPT-4&#21644;Zephyr-7B-beta&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#35868;&#39064;&#21644;&#36873;&#39033;&#20043;&#38388;&#30340;&#24605;&#32500;&#36335;&#24452;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#36827;&#34892;&#20102;&#36136;&#37327;&#39564;&#35777;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02474v1 Announce Type: cross  Abstract: Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#24066;&#22330;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#30340;&#19981;&#21516;&#36755;&#20837;&#34920;&#31034;&#26041;&#27861;&#65292;&#21457;&#29616;&#31867;&#20284;&#32534;&#31243;&#35821;&#35328;&#30340;&#25552;&#31034;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#21644;&#36739;&#38271;&#26684;&#24335;&#30340;&#25552;&#31034;&#25928;&#26524;&#36739;&#24046;&#12290;</title><link>https://arxiv.org/abs/2404.02466</link><description>&lt;p&gt;
&#25552;&#31034;&#25968;&#20540;&#24207;&#21015;&#65306;&#24066;&#22330;&#35780;&#35770;&#29983;&#25104;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prompting for Numerical Sequences: A Case Study on Market Comment Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#24066;&#22330;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#30340;&#19981;&#21516;&#36755;&#20837;&#34920;&#31034;&#26041;&#27861;&#65292;&#21457;&#29616;&#31867;&#20284;&#32534;&#31243;&#35821;&#35328;&#30340;&#25552;&#31034;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#21644;&#36739;&#38271;&#26684;&#24335;&#30340;&#25552;&#31034;&#25928;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#25968;&#25454;&#36716;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;&#34920;&#26684;&#12289;&#22270;&#34920;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#20540;&#25968;&#25454;&#36716;&#25991;&#26412;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29983;&#25104;&#34920;&#26684;&#21644;&#22270;&#34920;&#31561;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25552;&#31034;&#30340;&#30740;&#31350;&#27491;&#22312;&#22686;&#38271;&#20013;&#65292;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#20540;&#25968;&#25454;&#30340;&#25552;&#31034;&#30340;&#28145;&#20837;&#30740;&#31350;&#21364;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21508;&#31181;&#36755;&#20837;&#34920;&#31034;&#65292;&#21253;&#25324;&#20196;&#29260;&#24207;&#21015;&#21644;&#32467;&#26500;&#21270;&#26684;&#24335;&#22914;HTML&#12289;LaTeX&#21644;Python&#26679;&#24335;&#20195;&#30721;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#8220;&#24066;&#22330;&#35780;&#35770;&#29983;&#25104;&#8221;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#23558;&#32929;&#20215;&#25968;&#20540;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#30456;&#24212;&#30340;&#24066;&#22330;&#35780;&#35770;&#12290;&#19982;&#25105;&#20204;&#30340;&#39044;&#26399;&#30456;&#21453;&#65292;&#32467;&#26524;&#34920;&#26126;&#31867;&#20284;&#32534;&#31243;&#35821;&#35328;&#30340;&#25552;&#31034;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#21644;&#36739;&#38271;&#26684;&#24335;&#65288;&#22914;HTML&#21644;LaTeX&#65289;&#30340;&#25552;&#31034;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02466v1 Announce Type: cross  Abstract: Large language models (LLMs) have been applied to a wide range of data-to-text generation tasks, including tables, graphs, and time-series numerical data-to-text settings. While research on generating prompts for structured data such as tables and graphs is gaining momentum, in-depth investigations into prompting for time-series numerical data are lacking. Therefore, this study explores various input representations, including sequences of tokens and structured formats such as HTML, LaTeX, and Python-style codes. In our experiments, we focus on the task of Market Comment Generation, which involves taking a numerical sequence of stock prices as input and generating a corresponding market comment. Contrary to our expectations, the results show that prompts resembling programming languages yield better outcomes, whereas those similar to natural languages and longer formats, such as HTML and LaTeX, are less effective. Our findings offer in
&lt;/p&gt;</description></item><item><title>TSNet&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#34701;&#21512;&#27169;&#22359;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#27169;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22270;&#20687;&#21435;&#38654;&#27867;&#21270;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#20004;&#38454;&#27573;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2404.02460</link><description>&lt;p&gt;
TSNet: &#19968;&#31181;&#20855;&#26377;&#22810;&#23610;&#24230;&#34701;&#21512;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#22270;&#20687;&#21435;&#38654;&#30340;&#20004;&#38454;&#27573;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TSNet:A Two-stage Network for Image Dehazing with Multi-scale Fusion and Adaptive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02460
&lt;/p&gt;
&lt;p&gt;
TSNet&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#34701;&#21512;&#27169;&#22359;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#27169;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22270;&#20687;&#21435;&#38654;&#27867;&#21270;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#20004;&#38454;&#27573;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21435;&#38654;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#30740;&#31350;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#20808;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21435;&#38654;&#26041;&#27861;&#26410;&#33021;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#25928;&#26524;&#65292;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSNet&#30340;&#20004;&#38454;&#27573;&#22270;&#20687;&#21435;&#38654;&#32593;&#32476;&#65292;&#20027;&#35201;&#30001;&#22810;&#23610;&#24230;&#34701;&#21512;&#27169;&#22359;&#65288;MSFM&#65289;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#27169;&#22359;&#65288;ALM&#65289;&#32452;&#25104;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MSFM&#21644;ALM&#22686;&#24378;&#20102;TSNet&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;MSFM&#21487;&#20197;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#33719;&#24471;&#22823;&#30340;&#24863;&#21463;&#37326;&#65292;&#24182;&#25972;&#21512;&#19981;&#21516;&#39057;&#29575;&#30340;&#29305;&#24449;&#65292;&#20197;&#20943;&#23567;&#36755;&#20837;&#19982;&#23398;&#20064;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;ALM&#21487;&#20197;&#20027;&#21160;&#23398;&#20064;&#22270;&#20687;&#20013;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#65292;&#24182;&#26356;&#26377;&#25928;&#22320;&#24674;&#22797;&#32441;&#29702;&#32454;&#33410;&#12290;&#27492;&#22806;&#65292;TSNet&#34987;&#35774;&#35745;&#20026;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02460v1 Announce Type: cross  Abstract: Image dehazing has been a popular topic of research for a long time. Previous deep learning-based image dehazing methods have failed to achieve satisfactory dehazing effects on both synthetic datasets and real-world datasets, exhibiting poor generalization. Moreover, single-stage networks often result in many regions with artifacts and color distortion in output images. To address these issues, this paper proposes a two-stage image dehazing network called TSNet, mainly consisting of the multi-scale fusion module (MSFM) and the adaptive learning module (ALM). Specifically, MSFM and ALM enhance the generalization of TSNet. The MSFM can obtain large receptive fields at multiple scales and integrate features at different frequencies to reduce the differences between inputs and learning objectives. The ALM can actively learn of regions of interest in images and restore texture details more effectively. Additionally, TSNet is designed as a t
&lt;/p&gt;</description></item><item><title>PhonologyBench&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26126;&#30830;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27809;&#26377;&#35821;&#38899;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02456</link><description>&lt;p&gt;
PhonologyBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#38901;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
PhonologyBench: Evaluating Phonological Skills of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02456
&lt;/p&gt;
&lt;p&gt;
PhonologyBench&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26126;&#30830;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27809;&#26377;&#35821;&#38899;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#38901;&#23398;&#26159;&#30740;&#31350;&#35821;&#38899;&#32467;&#26500;&#21644;&#21457;&#38899;&#35268;&#21017;&#30340;&#23398;&#31185;&#65292;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30740;&#31350;&#20013;&#19968;&#20010;&#20851;&#38190;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;LLMs&#22312;&#21508;&#31181;&#21033;&#29992;&#38899;&#38901;&#23398;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;&#25945;&#32946;&#24037;&#20855;&#21644;&#35799;&#27468;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#33021;&#20250;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#19981;&#23436;&#32654;&#30340;&#27491;&#23383;&#21644;&#38899;&#26631;&#24418;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#30340;&#38899;&#38901;&#25216;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhonologyBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19977;&#20010;&#35786;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#26126;&#30830;&#27979;&#35797;LLMs&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65306;&#24418;&#38899;&#36716;&#25442;&#12289;&#38899;&#33410;&#35745;&#25968;&#21644;&#25276;&#38901;&#35789;&#29983;&#25104;&#12290;&#23613;&#31649;&#27809;&#26377;&#35775;&#38382;&#35821;&#38899;&#25968;&#25454;&#65292;LLMs&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#25276;&#38901;&#35789;&#29983;&#25104;&#21644;&#38899;&#33410;&#35745;&#25968;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;17%&#21644;45%&#30340;&#24046;&#36317;&#65292; respectively, when...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02456v1 Announce Type: cross  Abstract: Phonology, the study of speech's structure and pronunciation rules, is a critical yet often overlooked component in Large Language Model (LLM) research. LLMs are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation. Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data. Thus, it is imperative to benchmark the phonological skills of LLMs. To this end, we present PhonologyBench, a novel benchmark consisting of three diagnostic tasks designed to explicitly test the phonological skills of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation. Despite having no access to speech data, LLMs showcased notable performance on the PhonologyBench tasks. However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#21407;&#29702;&#35770;&#25512;&#29702;&#24378;&#24230;&#21464;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;Problog&#24037;&#20855;&#35745;&#31639;&#25439;&#22833;&#24230;&#37327;&#65292;&#26368;&#32456;&#24471;&#20986;&#20102;&#20851;&#20110;&#19981;&#21516;&#36951;&#24536;&#31574;&#30053;&#24378;&#24230;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#23454;&#38469;&#24212;&#29992;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2404.02454</link><description>&lt;p&gt;
&#34913;&#37327;&#36951;&#24536;&#31574;&#30053;&#30340;&#25512;&#29702;&#24378;&#24230;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Techniques for Measuring the Inferential Strength of Forgetting Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#21407;&#29702;&#35770;&#25512;&#29702;&#24378;&#24230;&#21464;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;Problog&#24037;&#20855;&#35745;&#31639;&#25439;&#22833;&#24230;&#37327;&#65292;&#26368;&#32456;&#24471;&#20986;&#20102;&#20851;&#20110;&#19981;&#21516;&#36951;&#24536;&#31574;&#30053;&#24378;&#24230;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#23454;&#38469;&#24212;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#36951;&#24536;&#25216;&#26415;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#19988;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#30693;&#35782;&#24037;&#31243;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#30340;&#36951;&#24536;&#31574;&#30053;&#25110;&#19981;&#21516;&#36951;&#24536;&#25805;&#20316;&#31526;&#30340;&#20351;&#29992;&#22914;&#20309;&#24433;&#21709;&#21407;&#29702;&#35770;&#30340;&#25512;&#29702;&#24378;&#24230;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#26681;&#25454;&#27169;&#22411;&#35745;&#25968;&#21644;&#27010;&#29575;&#29702;&#35770;&#30340;&#30452;&#35273;&#23450;&#20041;&#29992;&#20110;&#34913;&#37327;&#25512;&#29702;&#24378;&#24230;&#21464;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#30740;&#31350;&#20102;&#27492;&#31867;&#25439;&#22833;&#24230;&#37327;&#30340;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#30693;&#35782;&#24037;&#31243;&#24037;&#20855;&#65292;&#29992;&#20110;&#20351;&#29992;Problog&#35745;&#31639;&#25439;&#22833;&#24230;&#37327;&#12290;&#35770;&#25991;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#21644;&#30830;&#23450;&#19981;&#21516;&#36951;&#24536;&#31574;&#30053;&#24378;&#24230;&#30340;&#24037;&#20316;&#26041;&#27861;&#65292;&#20197;&#21450;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;Problog&#24212;&#29992;&#29702;&#35770;&#32467;&#26524;&#30340;&#20855;&#20307;&#31034;&#20363;&#12290;&#34429;&#28982;&#37325;&#28857;&#26159;&#36951;&#24536;&#65292;&#20294;&#32467;&#26524;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#24212;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02454v1 Announce Type: new  Abstract: The technique of forgetting in knowledge representation has been shown to be a powerful and useful knowledge engineering tool with widespread application. Yet, very little research has been done on how different policies of forgetting, or use of different forgetting operators, affects the inferential strength of the original theory. The goal of this paper is to define loss functions for measuring changes in inferential strength based on intuitions from model counting and probability theory. Properties of such loss measures are studied and a pragmatic knowledge engineering tool is proposed for computing loss measures using Problog. The paper includes a working methodology for studying and determining the strength of different forgetting policies, in addition to concrete examples showing how to apply the theoretical results using Problog. Although the focus is on forgetting, the results are much more general and should have wider applicati
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#24320;&#21457;&#19968;&#31181;&#32479;&#19968;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#36328;&#22810;&#31181;&#27169;&#24335;&#30340;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2404.02450</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#32452;&#21512;&#36827;&#34892;&#31639;&#27861;&#35825;&#23548;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Task Agnostic Architecture for Algorithm Induction via Implicit Composition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02450
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#24320;&#21457;&#19968;&#31181;&#32479;&#19968;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#36328;&#22810;&#31181;&#27169;&#24335;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#25110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#19968;&#30452;&#22312;&#26500;&#24314;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#65292;&#25105;&#20204;&#27491;&#22312;&#30446;&#30585;&#19968;&#20010;&#30456;&#21453;&#30340;&#36235;&#21183;&#65292;&#21363;&#21521;&#24320;&#21457;&#26356;&#24191;&#20041;&#30340;&#26550;&#26500;&#21457;&#23637;&#65292;&#36825;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#25512;&#21160;&#30340;&#12290;&#36825;&#20123;&#26550;&#26500;&#26088;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#36328;&#22810;&#31181;&#27169;&#24335;&#30340;&#36755;&#20837;&#12290;&#23558;&#36825;&#31181;&#27867;&#21270;&#36235;&#21183;&#25512;&#21521;&#26497;&#31471;&#24847;&#21619;&#30528;&#21487;&#33021;&#23384;&#22312;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#25152;&#26377;&#20219;&#21153;&#30340;&#21333;&#19968;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#24320;&#21457;&#36825;&#26679;&#19968;&#20010;&#32479;&#19968;&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35828;&#26126;&#22914;&#20309;&#26500;&#24314;&#36825;&#26679;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#22522;&#20110;&#20197;&#19979;&#20551;&#35774;&#12290;&#39318;&#20808;&#65292;&#20219;&#21153;&#26159;&#36890;&#36807;&#25353;&#29031;&#19968;&#31995;&#21015;&#25351;&#20196;&#26469;&#35299;&#20915;&#30340;&#65292;&#36890;&#24120;&#22312;&#24120;&#35268;&#35745;&#31639;&#30828;&#20214;&#30340;&#20195;&#30721;&#20013;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02450v1 Announce Type: cross  Abstract: Different fields in applied machine learning such as computer vision, speech or natural language processing have been building domain-specialised solutions. Currently, we are witnessing an opposing trend towards developing more generalist architectures, driven by Large Language Models and multi-modal foundational models. These architectures are designed to tackle a variety of tasks, including those previously unseen and using inputs across multiple modalities. Taking this trend of generalization to the extreme suggests the possibility of a single deep network architecture capable of solving all tasks. This position paper aims to explore developing such a unified architecture and proposes a theoretical framework of how it could be constructed. Our proposal is based on the following assumptions. Firstly, tasks are solved by following a sequence of instructions, typically implemented in code for conventional computing hardware, which inhe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#30005;&#21160;&#36710;&#36742;&#30340;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#35299;&#20915;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02448</link><description>&lt;p&gt;
&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#29992;&#20110;&#24212;&#24613;&#20379;&#30005;&#65306;&#38754;&#21521;&#30005;&#20449;&#22522;&#31449;&#25937;&#21161;
&lt;/p&gt;
&lt;p&gt;
Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#30005;&#21160;&#36710;&#36742;&#30340;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#35299;&#20915;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#23478;&#30005;&#20449;&#25552;&#20379;&#21830;&#65292;&#25105;&#20204;&#20844;&#21496;&#26377;&#19968;&#20010;&#20851;&#38190;&#20351;&#21629;&#65292;&#21363;&#22312;&#20572;&#30005;&#26399;&#38388;&#20445;&#25345;&#30005;&#20449;&#26381;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#20351;&#21629;&#65292;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#32500;&#25345;&#30005;&#20449;&#22522;&#31449;&#30340;&#30005;&#21147;&#12290;&#26412;&#25991;&#32771;&#34385;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#30005;&#21160;&#36710;&#36742; (EVs) &#30452;&#25509;&#21069;&#24448;&#20854;&#20301;&#32622;&#20026;&#22522;&#31449;&#25552;&#20379;&#30005;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#23567;&#21270;&#25152;&#26377;&#30005;&#21160;&#36710;&#36742;&#30340;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#30340;EV&#36335;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#36335;&#24452;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#26032;&#22411;&#30340;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064; (EVRP) &#21464;&#20307;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#30456;&#32467;&#21512;&#30340;&#27714;&#35299;&#22120;&#12290;&#36710;&#36742;&#36873;&#25321;&#22120;&#30340;&#35268;&#21017;&#30830;&#20445;&#20102;&#25152;&#36873;EV&#24320;&#22987;&#31227;&#21160;&#26102;&#30340;&#30830;&#20999;&#29615;&#22659;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;RL&#27169;&#22411;&#30340;&#33410;&#28857;&#36873;&#25321;&#23454;&#29616;&#20102;&#24555;&#36895;&#36335;&#24452;&#29983;&#25104;&#65292;&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#23588;&#20026;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#19978;&#23545;&#25105;&#20204;&#30340;&#27714;&#35299;&#22120;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02448v1 Announce Type: cross  Abstract: As a telecom provider, our company has a critical mission to maintain telecom services even during power outages. To accomplish the mission, it is essential to maintain the power of the telecom base stations. Here we consider a solution where electric vehicles (EVs) directly supply power to base stations by traveling to their locations. The goal is to find EV routes that minimize both the total travel distance of all EVs and the number of downed base stations. In this paper, we formulate this routing problem as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a reinforcement learning (RL)-based node selector. The rule of the vehicle selector ensures the exact environmental states when the selected EV starts to move. In addition, the node selection by the RL model enables fast route generation, which is critical in emergencies. We evaluate our solver on bot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#39068;&#33394;&#31354;&#38388;&#29305;&#24449;&#34701;&#21512;&#21644;&#37327;&#23376;-&#32463;&#20856;&#22534;&#21472;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20083;&#33146;&#30284;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#26631;&#24535;&#30528;&#20010;&#24615;&#21270;&#21307;&#23398;&#35780;&#20272;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2404.02447</link><description>&lt;p&gt;
&#21033;&#29992;&#20132;&#21449;&#39068;&#33394;&#31354;&#38388;&#29305;&#24449;&#34701;&#21512;&#21644;&#37327;&#23376;-&#32463;&#20856;&#22534;&#21472;&#38598;&#25104;&#26041;&#27861;&#30340;&#20083;&#33146;&#30284;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Approach to Breast Cancer Histopathological Image Classification Using Cross-Colour Space Feature Fusion and Quantum-Classical Stack Ensemble Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#39068;&#33394;&#31354;&#38388;&#29305;&#24449;&#34701;&#21512;&#21644;&#37327;&#23376;-&#32463;&#20856;&#22534;&#21472;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20083;&#33146;&#30284;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#26631;&#24535;&#30528;&#20010;&#24615;&#21270;&#21307;&#23398;&#35780;&#20272;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#20998;&#31867;&#22312;&#30830;&#20445;&#21450;&#26102;&#35786;&#26029;&#21644;&#26377;&#25928;&#27835;&#30103;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20197;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20026;&#22522;&#30784;&#65292;&#24378;&#35843;&#21033;&#29992;&#39068;&#33394;&#31354;&#38388;&#38598;&#25104;&#21644;&#37327;&#23376;-&#32463;&#20856;&#22534;&#21472;&#30340;&#21327;&#21516;&#33021;&#21147;&#26469;&#25552;&#39640;&#20083;&#33146;&#30284;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#30340;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;RGB&#12289;HSV&#21644;CIE L*u*v&#31561;&#19981;&#21516;&#39068;&#33394;&#31354;&#38388;&#65292;&#20316;&#32773;&#20204;&#24320;&#23637;&#20102;&#19968;&#39033;&#30001;&#20808;&#36827;&#26041;&#27861;&#24341;&#23548;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#21033;&#29992;DenseNet121&#26550;&#26500;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#20316;&#32773;&#20204;&#21033;&#29992;&#20102;&#38543;&#26426;&#26862;&#26519;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;QSVC&#21644;VQC&#20998;&#31867;&#22120;&#30340;&#31283;&#20581;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#28085;&#30422;&#20102;&#39068;&#33394;&#31354;&#38388;&#38598;&#25104;&#20013;&#30340;&#29420;&#29305;&#29305;&#24449;&#34701;&#21512;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21152;&#28145;&#20102;&#25105;&#20204;&#23545;&#20083;&#33146;&#30284;&#20998;&#31867;&#30340;&#29702;&#35299;&#65292;&#36824;&#22312;&#20010;&#24615;&#21270;&#21307;&#23398;&#35780;&#20272;&#20013;&#36798;&#21040;&#20102;&#37324;&#31243;&#30865;&#12290;&#37327;&#23376;&#21644;&#32463;&#20856;&#20998;&#31867;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02447v1 Announce Type: cross  Abstract: Breast cancer classification stands as a pivotal pillar in ensuring timely diagnosis and effective treatment. This study with histopathological images underscores the profound significance of harnessing the synergistic capabilities of colour space ensembling and quantum-classical stacking to elevate the precision of breast cancer classification. By delving into the distinct colour spaces of RGB, HSV and CIE L*u*v, the authors initiated a comprehensive investigation guided by advanced methodologies. Employing the DenseNet121 architecture for feature extraction the authors have capitalized on the robustness of Random Forest, SVM, QSVC, and VQC classifiers. This research encompasses a unique feature fusion technique within the colour space ensemble. This approach not only deepens our comprehension of breast cancer classification but also marks a milestone in personalized medical assessment. The amalgamation of quantum and classical classi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;NLP&#25216;&#26415;&#35780;&#20272;&#25945;&#32946;&#20013;&#22810;&#31181;&#39640;&#25512;&#29702;&#25945;&#23398;&#23454;&#36341;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#39318;&#27425;&#24212;&#29992;NLP&#26469;&#34913;&#37327;&#23545;&#26377;&#29305;&#27530;&#38656;&#27714;&#23398;&#29983;&#29305;&#21035;&#26377;&#25928;&#30340;&#25945;&#23398;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2404.02444</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25945;&#32946;&#20013;&#25945;&#23398;&#36136;&#37327;&#30340;&#25215;&#35834;&#19982;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;NLP&#25216;&#26415;&#35780;&#20272;&#25945;&#32946;&#20013;&#22810;&#31181;&#39640;&#25512;&#29702;&#25945;&#23398;&#23454;&#36341;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#39318;&#27425;&#24212;&#29992;NLP&#26469;&#34913;&#37327;&#23545;&#26377;&#29305;&#27530;&#38656;&#27714;&#23398;&#29983;&#29305;&#21035;&#26377;&#25928;&#30340;&#25945;&#23398;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25945;&#23398;&#36136;&#37327;&#26159;&#25945;&#32946;&#31995;&#32479;&#25913;&#36827;&#21162;&#21147;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25163;&#24037;&#35780;&#20272;&#26114;&#36149;&#12289;&#20027;&#35266;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#35266;&#23519;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#29305;&#27530;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#38459;&#30861;&#20102;&#25945;&#24072;&#21450;&#26102;&#33719;&#24471;&#39057;&#32321;&#21453;&#39304;&#12290;&#19982;&#20043;&#21069;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#20302;&#25512;&#29702;&#25945;&#23398;&#23454;&#36341;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#25991;&#39318;&#27425;&#23637;&#31034;&#20102;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#35780;&#20272;&#20004;&#31181;&#19981;&#21516;&#25945;&#32946;&#29615;&#22659;&#19979;&#30340;&#22810;&#31181;&#39640;&#25512;&#29702;&#25945;&#23398;&#23454;&#36341;&#30340;&#30740;&#31350;&#65306;&#38754;&#23545;&#38754;K-12&#25945;&#23460;&#21644;&#39044;&#26381;&#21153;&#25945;&#24072;&#30340;&#27169;&#25311;&#34920;&#29616;&#20219;&#21153;&#12290;&#36825;&#20063;&#26159;&#39318;&#20010;&#24212;&#29992;NLP&#26469;&#35780;&#20272;&#34987;&#24191;&#27867;&#35748;&#20026;&#23545;&#26377;&#29305;&#27530;&#38656;&#27714;&#23398;&#29983;&#29305;&#21035;&#26377;&#25928;&#30340;&#25945;&#23398;&#23454;&#36341;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#38754;&#20020;NLP&#27169;&#22411;&#25945;&#23398;&#20998;&#26512;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02444v1 Announce Type: cross  Abstract: Assessing instruction quality is a fundamental component of any improvement efforts in the education system. However, traditional manual assessments are expensive, subjective, and heavily dependent on observers' expertise and idiosyncratic factors, preventing teachers from getting timely and frequent feedback. Different from prior research that mostly focuses on low-inference instructional practices on a singular basis, this paper presents the first study that leverages Natural Language Processing (NLP) techniques to assess multiple high-inference instructional practices in two distinct educational settings: in-person K-12 classrooms and simulated performance tasks for pre-service teachers. This is also the first study that applies NLP to measure a teaching practice that is widely acknowledged to be particularly effective for students with special needs. We confront two challenges inherent in NLP-based instructional analysis, including
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19971;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#19977;&#31181;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#20915;&#31574;&#27169;&#22411;&#20316;&#20026;&#31639;&#27861;&#35774;&#35745;&#30340;&#21442;&#32771;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.02429</link><description>&lt;p&gt;
AD4RL&#65306;&#20855;&#26377;&#22522;&#20110;&#20215;&#20540;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19971;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#19977;&#31181;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#20915;&#31574;&#27169;&#22411;&#20316;&#20026;&#31639;&#27861;&#35774;&#35745;&#30340;&#21442;&#32771;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24050;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#28508;&#21147;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#22909;&#22788;&#24050;&#34987;&#22686;&#24378;&#65292;&#20294;&#22823;&#22810;&#25968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#24320;&#21457;&#30740;&#31350;&#20173;&#20381;&#36182;&#20110;&#20855;&#26377;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#28216;&#25103;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;19&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19977;&#31181;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#19971;&#31181;&#27969;&#34892;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#26377;&#25928;&#36816;&#34892;&#65292;&#20316;&#20026;&#31639;&#27861;&#35774;&#35745;&#30340;&#21442;&#32771;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#31038;&#21306;&#36827;&#19968;&#27493;&#25506;&#32034;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#23454;&#38469;&#26041;&#38754;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02429v1 Announce Type: cross  Abstract: Offline reinforcement learning has emerged as a promising technology by enhancing its practicality through the use of pre-collected large datasets. Despite its practical benefits, most algorithm development research in offline reinforcement learning still relies on game tasks with synthetic datasets. To address such limitations, this paper provides autonomous driving datasets and benchmarks for offline reinforcement learning research. We provide 19 datasets, including real-world human driver's datasets, and seven popular offline reinforcement learning algorithms in three realistic driving scenarios. We also provide a unified decision-making process model that can operate effectively across different scenarios, serving as a reference framework in algorithm design. Our research lays the groundwork for further collaborations in the community to explore practical aspects of existing reinforcement learning methods. Dataset and codes can be 
&lt;/p&gt;</description></item><item><title>&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23545;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#30340;&#34920;&#29616;&#21463;&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#26041;&#27861;&#30340;&#20219;&#21153;&#38656;&#27714;&#36234;&#22823;&#65292;&#24615;&#33021;&#36234;&#20302;&#65292;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#23588;&#20026;&#26174;&#33879;</title><link>https://arxiv.org/abs/2404.02418</link><description>&lt;p&gt;
&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#25513;&#30422;&#20102;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Auxiliary task demands mask the capabilities of smaller language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02418
&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23545;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#30340;&#34920;&#29616;&#21463;&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#26041;&#27861;&#30340;&#20219;&#21153;&#38656;&#27714;&#36234;&#22823;&#65292;&#24615;&#33021;&#36234;&#20302;&#65292;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#23588;&#20026;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#20204;&#23545;&#35748;&#30693;&#33021;&#21147;&#22914;&#35821;&#35328;&#29702;&#35299;&#25110;&#24515;&#28789;&#29702;&#35770;&#20309;&#26102;&#20986;&#29616;&#36827;&#34892;&#20102;&#20105;&#35770;&#12290;&#36825;&#20123;&#36777;&#35770;&#24120;&#24120;&#20851;&#27880;"&#20219;&#21153;&#38656;&#27714;"&#30340;&#27010;&#24565;--&#25191;&#34892;&#29305;&#23450;&#35780;&#20272;&#26102;&#25152;&#20276;&#38543;&#30340;&#36741;&#21161;&#25361;&#25112;--&#36825;&#20123;&#25361;&#25112;&#21487;&#33021;&#25513;&#30422;&#20102;&#20799;&#31461;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#24403;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#33021;&#21147;&#26102;&#65292;&#21516;&#26679;&#30340;&#38382;&#39064;&#20063;&#20250;&#20986;&#29616;&#65306;&#20219;&#21153;&#34920;&#29616;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#32467;&#21512;&#20102;&#27169;&#22411;&#35299;&#37322;&#21644;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#20854;&#21487;&#29992;&#36164;&#28304;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#65292;&#20855;&#26377;&#26356;&#22823;&#20219;&#21153;&#38656;&#27714;&#30340;&#35780;&#20272;&#26041;&#27861;&#20250;&#27604;&#38477;&#20302;&#38656;&#27714;&#30340;&#35780;&#20272;&#24471;&#21040;&#26356;&#20302;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#26174;&#33879;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LM&#30340;&#24615;&#33021;&#19981;&#24212;&#34987;&#35299;&#37322;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02418v1 Announce Type: cross  Abstract: Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of "task demands" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying competence, combined with the model's ability to interpret and perform the task given its available resources. Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This "demand gap" is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpret
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25511;&#21046;&#20219;&#21153;&#20316;&#20026;&#22522;&#20110;&#36807;&#21435;&#35266;&#23519;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#30340;&#24403;&#21069;&#26368;&#20248;&#21160;&#20316;&#39044;&#27979;&#26469;&#28040;&#38500;&#20272;&#35745;&#22120;&#35774;&#35745;&#38656;&#27714;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#31995;&#21015;&#21021;&#22987;&#21270;&#20915;&#31574;Transformer&#65292;&#28982;&#21518;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#23545;&#20854;&#36827;&#34892;&#25511;&#21046;&#20219;&#21153;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2404.02407</link><description>&lt;p&gt;
&#20915;&#31574;Transformer&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#36830;&#32493;&#25511;&#21046;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Decision Transformer as a Foundation Model for Partially Observable Continuous Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25511;&#21046;&#20219;&#21153;&#20316;&#20026;&#22522;&#20110;&#36807;&#21435;&#35266;&#23519;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#30340;&#24403;&#21069;&#26368;&#20248;&#21160;&#20316;&#39044;&#27979;&#26469;&#28040;&#38500;&#20272;&#35745;&#22120;&#35774;&#35745;&#38656;&#27714;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#31995;&#21015;&#21021;&#22987;&#21270;&#20915;&#31574;Transformer&#65292;&#28982;&#21518;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#23545;&#20854;&#36827;&#34892;&#25511;&#21046;&#20219;&#21153;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#20855;&#26377;&#37096;&#20998;&#29366;&#24577;&#21487;&#35266;&#27979;&#24615;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#38381;&#29615;&#25511;&#21046;&#38656;&#35201;&#19987;&#23478;&#23545;&#19968;&#32452;&#19981;&#22826;&#26631;&#20934;&#21270;&#30340;&#29702;&#35770;&#24037;&#20855;&#26377;&#28145;&#20837;&#20102;&#35299;&#65292;&#27492;&#22806;&#65292;&#23427;&#36824;&#38656;&#35201;&#25511;&#21046;&#22120;&#21644;&#20272;&#35745;&#22120;&#35774;&#35745;&#30340;&#31934;&#24515;&#34701;&#21512;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#31995;&#32479;&#34892;&#20026;&#12290;&#20026;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#30340;&#25511;&#21046;&#22120;&#21512;&#25104;&#26694;&#26550;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20915;&#31574;Transformer&#65288;DT&#65289;&#26550;&#26500;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#25511;&#21046;&#20219;&#21153;&#26694;&#26550;&#21270;&#20026;&#22522;&#20110;&#36807;&#21435;&#30340;&#35266;&#23519;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#39044;&#27979;&#24403;&#21069;&#26368;&#20248;&#21160;&#20316;&#65292;&#28040;&#38500;&#20102;&#23545;&#21333;&#29420;&#20272;&#35745;&#22120;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#65288;GPT&#65289;&#31995;&#21015;&#65292;&#26469;&#21021;&#22987;&#21270;DT&#65292;&#24182;&#38543;&#21518;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#23545;&#20854;&#36827;&#34892;&#25511;&#21046;&#20219;&#21153;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#28085;&#30422;&#33322;&#31354;&#33322;&#22825;&#31995;&#32479;&#30340;&#25805;&#32437;&#21040;&#25511;&#21046;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02407v1 Announce Type: cross  Abstract: Closed-loop control of nonlinear dynamical systems with partial-state observability demands expert knowledge of a diverse, less standardized set of theoretical tools. Moreover, it requires a delicate integration of controller and estimator designs to achieve the desired system behavior. To establish a general controller synthesis framework, we explore the Decision Transformer (DT) architecture. Specifically, we first frame the control task as predicting the current optimal action based on past observations, actions, and rewards, eliminating the need for a separate estimator design. Then, we leverage the pre-trained language models, i.e., the Generative Pre-trained Transformer (GPT) series, to initialize DT and subsequently train it for control tasks using low-rank adaptation (LoRA). Our comprehensive experiments across five distinct control tasks, ranging from maneuvering aerospace systems to controlling partial differential equations 
&lt;/p&gt;</description></item><item><title>&#32842;&#22825;&#27169;&#22411;&#22240;&#20026;&#22810;&#36718;&#20132;&#20114;&#26684;&#24335;&#30340;&#28789;&#27963;&#24615;&#22686;&#21152;&#20102;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#35813;&#35770;&#25991;&#25581;&#31034;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;</title><link>https://arxiv.org/abs/2404.02406</link><description>&lt;p&gt;
&#25506;&#35752;&#32842;&#22825;&#27169;&#22411;&#30340;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Exploring Backdoor Vulnerabilities of Chat Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02406
&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#27169;&#22411;&#22240;&#20026;&#22810;&#36718;&#20132;&#20114;&#26684;&#24335;&#30340;&#28789;&#27963;&#24615;&#22686;&#21152;&#20102;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#35813;&#35770;&#25991;&#25581;&#31034;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#31216;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#24403;&#21069;&#23545;LLMs&#30340;&#21518;&#38376;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#65292;&#32780;&#24573;&#30053;&#20102;&#21478;&#19968;&#31181;&#29616;&#23454;&#22330;&#26223;&#65292;&#21363;&#23558;LLMs&#22312;&#22810;&#36718;&#23545;&#35805;&#25968;&#25454;&#19978;&#24494;&#35843;&#20026;&#32842;&#22825;&#27169;&#22411;&#12290;&#32842;&#22825;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#22330;&#26223;&#65292;&#22240;&#27492;&#32842;&#22825;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#20540;&#24471;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25105;&#20204;&#25351;&#20986;&#65292;&#28789;&#27963;&#30340;&#22810;&#36718;&#20132;&#20114;&#26684;&#24335;&#22686;&#21152;&#20102;&#35302;&#21457;&#35774;&#35745;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#22686;&#21152;&#20102;&#32842;&#22825;&#27169;&#22411;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32842;&#22825;&#27169;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#35302;&#21457;&#22330;&#26223;&#20998;&#24067;&#22312;&#29992;&#25143;&#36755;&#20837;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02406v1 Announce Type: cross  Abstract: Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on LLMs predominantly focus on instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs
&lt;/p&gt;</description></item><item><title>Token Trails&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#25928;&#26524;&#65292;&#22312;&#20419;&#36827;&#19978;&#19979;&#25991;&#24847;&#35782;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#21069;&#27839;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02402</link><description>&lt;p&gt;
&#20351;&#29992;ChatLLM&#22312;&#23545;&#35805;AI&#20013;&#23548;&#33322;&#35821;&#22659;&#28145;&#24230;&#30340;Token Trails
&lt;/p&gt;
&lt;p&gt;
Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02402
&lt;/p&gt;
&lt;p&gt;
Token Trails&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#25928;&#26524;&#65292;&#22312;&#20419;&#36827;&#19978;&#19979;&#25991;&#24847;&#35782;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#21069;&#27839;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23545;&#35805;&#24314;&#27169;&#38656;&#35201;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#32454;&#33268;&#29702;&#35299;&#65292;&#20197;&#29983;&#25104;&#36830;&#36143;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#22797;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Token Trails&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#26469;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;token-type&#23884;&#20837;&#26469;&#21306;&#20998;&#29992;&#25143;&#35805;&#35821;&#21644;&#26426;&#22120;&#20154;&#22238;&#22797;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#22238;&#22797;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Token Trails&#22312;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#23545;&#35805;AI&#20013;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;Token Trails&#22312;&#25512;&#21160;&#35813;&#39046;&#22495;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20026;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02402v1 Announce Type: cross  Abstract: Conversational modeling using Large Language Models (LLMs) requires a nuanced understanding of context to generate coherent and contextually relevant responses. In this paper, we present Token Trails, a novel approach that leverages token-type embeddings to navigate the intricate contextual nuances within conversations. Our framework utilizes token-type embeddings to distinguish between user utterances and bot responses, facilitating the generation of context-aware replies. Through comprehensive experimentation and evaluation, we demonstrate the effectiveness of Token Trails in improving conversational understanding and response generation, achieving state-of-the-art performance. Our results highlight the significance of contextual modeling in conversational AI and underscore the promising potential of Token Trails to advance the field, paving the way for more sophisticated and contextually aware chatbot interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#35774;&#35745;&#30340;&#27969;&#31243;&#65292;&#23398;&#20064;&#32467;&#26500;&#30340;&#28145;&#21051;&#21547;&#20041;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2404.02389</link><description>&lt;p&gt;
&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65306;&#26469;&#33258;&#25991;&#26412;&#21040;SQL&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#35774;&#35745;&#30340;&#27969;&#31243;&#65292;&#23398;&#20064;&#32467;&#26500;&#30340;&#28145;&#21051;&#21547;&#20041;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#34920;&#26684;&#12289;&#25968;&#25454;&#24211;&#21644;&#30693;&#35782;&#22270;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#22312;&#20854;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290; &#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#24320;&#22987;&#36716;&#21521;&#22522;&#20110;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#22788;&#29702;&#20026;&#39034;&#24207;&#26631;&#35760;&#27969;&#65292;&#32780;&#19981;&#26159;&#20316;&#20026;&#22270;&#24418;&#26126;&#30830;&#22320;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290; &#26412;&#25991;&#25506;&#35752;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;T5&#65289;&#20013;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#32447;&#24615;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290; &#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#35774;&#35745;&#30340;&#27969;&#31243;&#65292;&#27604;&#22914;&#27169;&#24335;&#38142;&#25509;&#21644;&#35821;&#27861;&#39044;&#27979;&#65292;&#34920;&#26126;&#27169;&#22411;&#23545;&#32467;&#26500;&#30340;&#28145;&#21051;&#12289;&#26377;&#24847;&#20041;&#30340;&#23398;&#20064;&#36828;&#36828;&#36229;&#36807;&#31616;&#21333;&#30340;&#26631;&#35760;&#25490;&#24207;&#12290; &#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;&#32467;&#26500;&#33410;&#28857;&#30340;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02389v1 Announce Type: cross  Abstract: Structured data, prevalent in tables, databases, and knowledge graphs, poses a significant challenge in its representation. With the advent of large language models (LLMs), there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly model structure, often as a graph. Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear. This work investigates the linear handling of structured data in encoder-decoder language models, specifically T5. Our findings reveal the model's ability to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing. We also uncover insights into the model's internal mechanisms, including the ego-centric nature of structure nod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#30524;&#21160;&#25968;&#25454;&#21644;&#25991;&#26412;&#25552;&#31034;&#65292;&#21033;&#29992;Vision-Language Models&#65288;VLMs&#65289;&#22686;&#24378;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#20851;&#27880;&#24230;&#65292;&#26377;&#25928;&#22686;&#24378;&#20102;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02370</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#30524;&#30555;&#27880;&#35270;&#27169;&#24335;&#22686;&#24378;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02370
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#30524;&#21160;&#25968;&#25454;&#21644;&#25991;&#26412;&#25552;&#31034;&#65292;&#21033;&#29992;Vision-Language Models&#65288;VLMs&#65289;&#22686;&#24378;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#20851;&#27880;&#24230;&#65292;&#26377;&#25928;&#22686;&#24378;&#20102;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#30340;&#36827;&#23637;&#22312;&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#19982;&#25918;&#23556;&#31185;&#21307;&#24072;&#20043;&#38388;&#30340;&#20114;&#21160;&#20027;&#35201;&#20165;&#38480;&#20110;&#36755;&#20837;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30524;&#21160;&#25968;&#25454;&#19982;&#25991;&#26412;&#25552;&#31034;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21033;&#29992;&#22686;&#24378;&#20102;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#20851;&#27880;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26469;&#22686;&#24378;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#30524;&#21160;&#25968;&#25454;&#29983;&#25104;&#30340;&#28909;&#22270;&#65292;&#23558;&#23427;&#20204;&#21472;&#21152;&#21040;&#21307;&#23398;&#22270;&#20687;&#19978;&#65292;&#20197;&#31361;&#20986;&#25918;&#23556;&#31185;&#21307;&#24072;&#22312;&#33016;&#37096;X&#23556;&#32447;&#35780;&#20272;&#36807;&#31243;&#20013;&#20851;&#27880;&#24378;&#24230;&#36739;&#39640;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#12289;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#33258;&#21160;&#21270;&#12289;&#38169;&#35823;&#26816;&#27979;&#21644;&#37492;&#21035;&#35786;&#26029;&#31561;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21253;&#21547;&#30524;&#21160;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#33016;&#37096;X&#23556;&#32447;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02370v1 Announce Type: cross  Abstract: Recent advancements in Computer Assisted Diagnosis have shown promising performance in medical imaging tasks, particularly in chest X-ray analysis. However, the interaction between these models and radiologists has been primarily limited to input images. This work proposes a novel approach to enhance human-computer interaction in chest X-ray analysis using Vision-Language Models (VLMs) enhanced with radiologists' attention by incorporating eye gaze data alongside textual prompts. Our approach leverages heatmaps generated from eye gaze data, overlaying them onto medical images to highlight areas of intense radiologist's focus during chest X-ray evaluation. We evaluate this methodology in tasks such as visual question answering, chest X-ray report automation, error detection, and differential diagnosis. Our results demonstrate the inclusion of eye gaze information significantly enhances the accuracy of chest X-ray analysis. Also, the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;EnergAIze&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#33021;&#28304;&#31649;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36710;&#36742;&#21040;&#30005;&#32593;&#33021;&#37327;&#31649;&#29702;&#20013;&#30340;&#23454;&#38469;&#36866;&#24212;&#24615;&#12289;&#20840;&#29699;&#20248;&#21270;&#20197;&#21450;&#29992;&#25143;&#21442;&#19982;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.02361</link><description>&lt;p&gt;
EnergAIze: &#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#29992;&#20110;&#36710;&#36742;&#21040;&#30005;&#32593;&#33021;&#37327;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EnergAIze&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#33021;&#28304;&#31649;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36710;&#36742;&#21040;&#30005;&#32593;&#33021;&#37327;&#31649;&#29702;&#20013;&#30340;&#23454;&#38469;&#36866;&#24212;&#24615;&#12289;&#20840;&#29699;&#20248;&#21270;&#20197;&#21450;&#29992;&#25143;&#21442;&#19982;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#20877;&#29983;&#33021;&#28304;&#65288;RES&#65289;&#21644;&#30005;&#21160;&#27773;&#36710;&#65288;EVs&#65289;&#26085;&#30410;&#22686;&#21152;&#30340;&#35282;&#33394;&#12290;&#34429;&#28982;&#26631;&#24535;&#30528;&#21487;&#25345;&#32493;&#33021;&#28304;&#26032;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;EV&#37319;&#29992;&#29575;&#32972;&#26223;&#19979;&#24179;&#34913;&#20379;&#38656;&#12289;&#24179;&#28369;&#23792;&#20540;&#28040;&#32791;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#38656;&#35201;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#12289;&#33021;&#37327;&#28789;&#27963;&#24615;&#31649;&#29702;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#31038;&#21306;&#65288;RECs&#65289;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;EVs&#65292;&#36824;&#38656;&#35201;&#36710;&#36742;&#23545;&#30005;&#32593;&#65288;V2G&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;V2G&#26041;&#27861;&#24448;&#24448;&#22312;&#29616;&#23454;&#36866;&#24212;&#24615;&#12289;&#20840;&#29699;REC&#20248;&#21270;&#19982;&#20854;&#20182;&#28789;&#27963;&#36164;&#20135;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#29992;&#25143;&#21442;&#19982;&#26041;&#38754;&#23384;&#22312;&#30701;&#26495;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;EnergAIze&#65292;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#33021;&#28304;&#31649;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;MADDPG&#65289;&#31639;&#27861;&#12290;EnergAIze&#23454;&#29616;&#20102;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#21644;&#22810;&#30446;&#26631;&#33021;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02361v1 Announce Type: cross  Abstract: This paper investigates the increasing roles of Renewable Energy Sources (RES) and Electric Vehicles (EVs). While indicating a new era of sustainable energy, these also introduce complex challenges, including the need to balance supply and demand and smooth peak consumptions amidst rising EV adoption rates. Addressing these challenges requires innovative solutions such as Demand Response (DR), energy flexibility management, Renewable Energy Communities (RECs), and more specifically for EVs, Vehicle-to-Grid (V2G). However, existing V2G approaches often fall short in real-world adaptability, global REC optimization with other flexible assets, scalability, and user engagement. To bridge this gap, this paper introduces EnergAIze, a Multi-Agent Reinforcement Learning (MARL) energy management framework, leveraging the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. EnergAIze enables user-centric and multi-objective energy 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02353</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#22312;&#22270;&#20687;&#20013;&#36827;&#34892;&#35821;&#20041;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Semantic Augmentation in Images using Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02353
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#32570;&#20047;&#36825;&#20123;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38480;&#21046;&#20854;&#27867;&#21270;&#21040;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#21033;&#29992;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02353v1 Announce Type: cross  Abstract: Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#26680;&#24515;&#27169;&#22411;&#21644;&#22810;&#22871;&#39046;&#22495;&#29305;&#23450;&#21442;&#25968;&#65292;&#32467;&#21512;&#25552;&#31034;&#35843;&#25972;&#21644;&#36866;&#37197;&#22120;&#25216;&#26415;&#65292;&#20197;&#21450;&#39069;&#22806;&#23618;&#27425;&#26469;&#23454;&#29616;&#20302;&#36164;&#28304;&#22810;&#39046;&#22495;&#36866;&#24212;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#19982;&#27599;&#20010;&#39046;&#22495;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.02335</link><description>&lt;p&gt;
Multi-BERT&#65306;&#21033;&#29992;&#36866;&#37197;&#22120;&#21644;&#25552;&#31034;&#35843;&#25972;&#36827;&#34892;&#20302;&#36164;&#28304;&#22810;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02335
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#26680;&#24515;&#27169;&#22411;&#21644;&#22810;&#22871;&#39046;&#22495;&#29305;&#23450;&#21442;&#25968;&#65292;&#32467;&#21512;&#25552;&#31034;&#35843;&#25972;&#21644;&#36866;&#37197;&#22120;&#25216;&#26415;&#65292;&#20197;&#21450;&#39069;&#22806;&#23618;&#27425;&#26469;&#23454;&#29616;&#20302;&#36164;&#28304;&#22810;&#39046;&#22495;&#36866;&#24212;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#19982;&#27599;&#20010;&#39046;&#22495;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#24613;&#21095;&#25193;&#23637;&#25552;&#20986;&#20102;&#22810;&#39046;&#22495;&#29615;&#22659;&#20013;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#22312;&#27874;&#26031;&#35821;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#35774;&#32622;&#20013;&#20063;&#24456;&#26126;&#26174;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#39046;&#22495;&#65292;&#36824;&#26159;&#20026;&#27599;&#20010;&#39046;&#22495;&#20351;&#29992;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#32463;&#24120;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#38480;&#21046;&#12290;&#21333;&#19968;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25429;&#25417;&#21508;&#31181;&#39046;&#22495;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#32780;&#20351;&#29992;&#22810;&#20010;&#22823;&#22411;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36164;&#28304;&#38480;&#21046;&#65292;&#20351;&#20026;&#27599;&#20010;&#39046;&#22495;&#35757;&#32451;&#27169;&#22411;&#20960;&#20046;&#19981;&#20999;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30001;&#19968;&#20010;&#26680;&#24515;&#27169;&#22411;&#21644;&#22810;&#22871;&#39046;&#22495;&#29305;&#23450;&#21442;&#25968;&#32452;&#25104;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#21644;&#36866;&#37197;&#22120;&#31561;&#25216;&#26415;&#65292;&#32467;&#21512;&#24341;&#20837;&#39069;&#22806;&#23618;&#65292;&#28155;&#21152;&#25105;&#20204;&#21487;&#20197;&#20026;&#29305;&#23450;&#39046;&#22495;&#35757;&#32451;&#30340;&#21442;&#25968;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#19982;&#20026;&#27599;&#20010;&#39046;&#22495;&#35757;&#32451;&#30340;&#21333;&#29420;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02335v1 Announce Type: cross  Abstract: The rapid expansion of texts' volume and diversity presents formidable challenges in multi-domain settings. These challenges are also visible in the Persian name entity recognition (NER) settings. Traditional approaches, either employing a unified model for multiple domains or individual models for each domain, frequently pose significant limitations. Single models often struggle to capture the nuances of diverse domains, while utilizing multiple large models can lead to resource constraints, rendering the training of a model for each domain virtually impractical. Therefore, this paper introduces a novel approach composed of one core model with multiple sets of domain-specific parameters. We utilize techniques such as prompt tuning and adapters, combined with the incorporation of additional layers, to add parameters that we can train for the specific domains. This enables the model to perform comparably to individual models for each do
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#39537;&#21160;&#26415;&#35821;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;Llama2-7B&#12289;GPT-3.5&#21644;Falcon-7B&#22312;Inspec&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02330</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#39046;&#22495;&#39537;&#21160;&#26415;&#35821;&#25552;&#21462;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Domain Driven Terms Extraction Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#39537;&#21160;&#26415;&#35821;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;Llama2-7B&#12289;GPT-3.5&#21644;Falcon-7B&#22312;Inspec&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#22312;&#20154;&#31867;&#29702;&#35299;&#21644;&#26426;&#22120;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#26550;&#36215;&#37325;&#35201;&#26725;&#26753;&#65292;&#23545;&#20110;&#25968;&#25454;&#20016;&#23500;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26500;&#25104;&#20102;&#35814;&#32454;&#27880;&#37322;&#30340;&#22522;&#30784;&#65292;&#25552;&#20379;&#20102;&#23545;&#22522;&#30784;&#25968;&#25454;&#26356;&#28145;&#20837;&#21644;&#20840;&#38754;&#30340;&#35270;&#35282;&#12290;&#20851;&#38190;&#35789;/&#39046;&#22495;&#39537;&#21160;&#26415;&#35821;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26377;&#21161;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#25991;&#26723;&#25688;&#35201;&#21644;&#20869;&#23481;&#20998;&#31867;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#65292;&#24378;&#35843;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20351;&#29992;&#65306;Llama2-7B&#12289;GPT-3.5&#21644;Falcon-7B&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#33258;&#23450;&#20041;&#30340;Python&#21253;&#19982;&#36825;&#20123;LLMs&#36827;&#34892;&#20132;&#20114;&#65292;&#31616;&#21270;&#20102;&#20851;&#38190;&#35789;&#25552;&#21462;&#36807;&#31243;&#12290;&#25105;&#20204;&#21033;&#29992;Inspec&#21644;PubMed&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#35780;&#20215;&#20351;&#29992;Jaccard&#30456;&#20284;&#24615;&#25351;&#25968;&#65292;&#24471;&#21040;&#20102;GPT-3.5&#30340;&#35780;&#20998;&#20998;&#21035;&#20026;0.64&#65288;Inspec&#65289;&#21644;0.21&#65288;PubMed&#65289;&#65292;Llama2-7B&#30340;&#35780;&#20998;&#20998;&#21035;&#20026;0.40&#21644;0.17&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02330v1 Announce Type: cross  Abstract: Keywords play a crucial role in bridging the gap between human understanding and machine processing of textual data. They are essential to data enrichment because they form the basis for detailed annotations that provide a more insightful and in-depth view of the underlying data. Keyword/domain driven term extraction is a pivotal task in natural language processing, facilitating information retrieval, document summarization, and content categorization. This review focuses on keyword extraction methods, emphasizing the use of three major Large Language Models(LLMs): Llama2-7B, GPT-3.5, and Falcon-7B. We employed a custom Python package to interface with these LLMs, simplifying keyword extraction. Our study, utilizing the Inspec and PubMed datasets, evaluates the performance of these models. The Jaccard similarity index was used for assessment, yielding scores of 0.64 (Inspec) and 0.21 (PubMed) for GPT-3.5, 0.40 and 0.17 for Llama2-7B, a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SAMMO&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02319</link><description>&lt;p&gt;
Prompt&#20316;&#20026;&#31243;&#24207;&#65306;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39640;&#25928;&#32534;&#35793;&#26102;Prompt&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SAMMO&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29616;&#22312;&#33021;&#22788;&#29702;&#26356;&#38271;&#26356;&#22797;&#26434;&#30340;&#36755;&#20837;&#65292;&#36825;&#20419;&#36827;&#20102;&#26356;&#22797;&#26434;&#25552;&#31034;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25552;&#31034;&#36890;&#24120;&#38656;&#35201;&#19968;&#20123;&#35843;&#25972;&#20197;&#25552;&#39640;&#37096;&#32626;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#38543;&#30528;&#25552;&#31034;&#22797;&#26434;&#24230;&#21644;LLM&#24378;&#24230;&#30340;&#22686;&#21152;&#65292;&#35768;&#22810;&#25552;&#31034;&#20248;&#21270;&#25216;&#26415;&#24050;&#19981;&#20877;&#36275;&#22815;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SAMMO&#65292;&#19968;&#20010;&#29992;&#20110;&#20803;&#25552;&#31034;&#31243;&#24207;&#30340;{\em &#32534;&#35793;&#26102;}&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#25552;&#31034;&#34920;&#31034;&#20026;&#32467;&#26500;&#21270;&#23545;&#35937;&#65292;&#20801;&#35768;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#25628;&#32034;&#19968;&#32452;&#20016;&#23500;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;SAMMO&#25512;&#24191;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#25351;&#20196;&#35843;&#25972;&#12289;RAG&#31649;&#32447;&#35843;&#25972;&#21644;&#25552;&#31034;&#21387;&#32553;&#26041;&#38754;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#25918;&#25152;&#26377;&#20195;&#30721;&#20379;&#22823;&#23478;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02319v1 Announce Type: cross  Abstract: Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts. However, prompts often require some tuning to improve performance for deployment. Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\em meta prompt programs}. To address this, we introduce SAMMO, a framework for {\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs.   We make all code available open-sou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#65292;&#24182;&#35774;&#35745;&#20102;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#20351;&#24471;&#22312;&#40657;&#21283;&#23376;&#35774;&#32622;&#19979;&#65292;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#29305;&#23450;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;</title><link>https://arxiv.org/abs/2404.02314</link><description>&lt;p&gt;
&#20998;&#23376;&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#21542;&#30495;&#30340;&#38656;&#35201;&#20803;&#35757;&#32451;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Meta-training Really Necessary for Molecular Few-Shot Learning ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#65292;&#24182;&#35774;&#35745;&#20102;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#20351;&#24471;&#22312;&#40657;&#21283;&#23376;&#35774;&#32622;&#19979;&#65292;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#29305;&#23450;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#32780;&#26368;&#36817;&#24555;&#36895;&#22686;&#38271;&#30340;&#25991;&#29486;&#22823;&#22810;&#28041;&#21450;&#22797;&#26434;&#30340;&#20803;&#23398;&#20064;&#31574;&#30053;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26356;&#20026;&#30452;&#25509;&#30340;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#36991;&#20813;&#20102;&#25105;&#20204;&#25439;&#22833;&#20989;&#25968;&#30340;&#36864;&#21270;&#35299;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#33719;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#40657;&#21283;&#23376;&#35774;&#32622;&#65292;&#24182;&#28040;&#38500;&#20102;&#29305;&#23450;&#24773;&#33410;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#31454;&#20105;&#26041;&#27861;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#24494;&#35843;&#22522;&#32447;&#22987;&#32456;&#27604;&#20803;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02314v1 Announce Type: cross  Abstract: Few-shot learning has recently attracted significant interest in drug discovery, with a recent, fast-growing literature mostly involving convoluted meta-learning strategies. We revisit the more straightforward fine-tuning approach for molecular data, and propose a regularized quadratic-probe loss based on the the Mahalanobis distance. We design a dedicated block-coordinate descent optimizer, which avoid the degenerate solutions of our loss. Interestingly, our simple fine-tuning approach achieves highly competitive performances in comparison to state-of-the-art methods, while being applicable to black-box settings and removing the need for specific episodic pre-training strategies. Furthermore, we introduce a new benchmark to assess the robustness of the competing methods to domain shifts. In this setting, our fine-tuning baseline obtains consistently better results than meta-learning methods.
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24310;&#38271;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#23548;&#33268;&#37325;&#22797;&#21644;&#23849;&#28291;&#30340;&#26631;&#35760;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.02305</link><description>&lt;p&gt;
&#33258;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Collapse of Self-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02305
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24310;&#38271;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#23548;&#33268;&#37325;&#22797;&#21644;&#23849;&#28291;&#30340;&#26631;&#35760;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#30693;&#35782;&#21019;&#36896;&#39046;&#22495;&#65292;&#21253;&#25324;&#31185;&#23398;&#65292;&#26032;&#24605;&#24819;&#24448;&#24448;&#24314;&#31435;&#22312;&#29616;&#26377;&#20449;&#24687;&#20043;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20010;&#27010;&#24565;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22312;&#20854;&#33258;&#36523;&#36755;&#20986;&#19978;&#30340;&#28508;&#21147;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#24182;&#24314;&#31435;&#22312;&#20182;&#20204;&#20197;&#21069;&#30340;&#24605;&#24819;&#21644;&#34892;&#21160;&#19978;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;&#30452;&#35266;&#19978;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#30340;&#23454;&#38469;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;GPT-2&#27169;&#22411;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#33258;&#35757;&#32451;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#23548;&#33268;&#37325;&#22797;&#21644;&#23849;&#28291;&#30340;&#20196;&#29260;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02305v1 Announce Type: cross  Abstract: In various fields of knowledge creation, including science, new ideas often build on pre-existing information. In this work, we explore this concept within the context of language models. Specifically, we explore the potential of self-training models on their own outputs, akin to how humans learn and build on their previous thoughts and actions. While this approach is intuitively appealing, our research reveals its practical limitations. We find that extended self-training of the GPT-2 model leads to a significant degradation in performance, resulting in repetitive and collapsed token output.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Graph Neural Networks&#20998;&#26512;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#34394;&#25311;&#20256;&#24863;&#22120;&#65292;&#33021;&#22815;&#20174;&#20256;&#24863;&#22120;&#28378;&#36718;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#25805;&#20316;&#26465;&#20214;&#26144;&#23556;&#20026;&#36724;&#25215;&#36127;&#36733;&#12290;</title><link>https://arxiv.org/abs/2404.02304</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#26500;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#26102;&#36724;&#25215;&#36127;&#36733;&#39044;&#27979;&#30340;&#34394;&#25311;&#20256;&#24863;&#22120;
&lt;/p&gt;
&lt;p&gt;
Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02304
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Graph Neural Networks&#20998;&#26512;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#34394;&#25311;&#20256;&#24863;&#22120;&#65292;&#33021;&#22815;&#20174;&#20256;&#24863;&#22120;&#28378;&#36718;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#25805;&#20316;&#26465;&#20214;&#26144;&#23556;&#20026;&#36724;&#25215;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#36724;&#25215;&#36127;&#36733;&#30417;&#27979;&#23545;&#20110;&#20854;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#26377;&#21161;&#20110;&#25439;&#20260;&#35780;&#20272;&#12289;&#30952;&#25439;&#39044;&#27979;&#21644;&#20027;&#21160;&#32500;&#25252;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#31354;&#38388; - &#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#34394;&#25311;&#20256;&#24863;&#22120;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20998;&#26512;&#36724;&#25215;&#36127;&#36733;&#20043;&#38388;&#30340;&#31354;&#38388; - &#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02304v1 Announce Type: cross  Abstract: Accurate bearing load monitoring is essential for their Prognostics and Health Management (PHM), enabling damage assessment, wear prediction, and proactive maintenance. While bearing sensors are typically placed on the bearing housing, direct load monitoring requires sensors inside the bearing itself. Recently introduced sensor rollers enable direct bearing load monitoring but are constrained by their battery life. Data-driven virtual sensors can learn from sensor roller data collected during a batterys lifetime to map operating conditions to bearing loads. Although spatially distributed bearing sensors offer insights into load distribution (e.g., correlating temperature with load), traditional machine learning algorithms struggle to fully exploit these spatial-temporal dependencies. To address this gap, we introduce a graph-based virtual sensor that leverages Graph Neural Networks (GNNs) to analyze spatial-temporal dependencies among 
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#36890;&#29992;&#25200;&#21160;&#26041;&#27861;&#26550;&#36215;&#20102;2D&#25200;&#21160;&#19982;&#31867;&#20284;3D&#25915;&#20987;&#33021;&#21147;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20026;&#29983;&#25104;3D&#29289;&#20307;&#35782;&#21035;&#20013;&#30340;&#22810;&#35270;&#35282;&#23545;&#25239;&#26679;&#26412;&#25552;&#20379;&#20102;&#23454;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02287</link><description>&lt;p&gt;
&#19968;&#31181;&#22122;&#22768;&#32479;&#27835;&#25152;&#26377;&#65306;&#22810;&#35270;&#35282;&#24102;&#26222;&#36941;&#25200;&#21160;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
One Noise to Rule Them All: Multi-View Adversarial Attacks with Universal Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02287
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#36890;&#29992;&#25200;&#21160;&#26041;&#27861;&#26550;&#36215;&#20102;2D&#25200;&#21160;&#19982;&#31867;&#20284;3D&#25915;&#20987;&#33021;&#21147;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20026;&#29983;&#25104;3D&#29289;&#20307;&#35782;&#21035;&#20013;&#30340;&#22810;&#35270;&#35282;&#23545;&#25239;&#26679;&#26412;&#25552;&#20379;&#20102;&#23454;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#25200;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;3D&#29289;&#20307;&#35782;&#21035;&#20013;&#30340;&#22810;&#35270;&#35282;&#23545;&#25239;&#26679;&#26412;&#12290;&#19982;&#38480;&#21046;&#22312;&#21333;&#20010;&#35270;&#22270;&#19978;&#30340;&#20256;&#32479;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25805;&#20316;&#20110;&#22810;&#20010;2D&#22270;&#20687;&#65292;&#20026;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#23454;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#36890;&#29992;&#26041;&#27861;&#23558;2D&#25200;&#21160;&#19982;&#31867;&#20284;3D&#30340;&#25915;&#20987;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02287v1 Announce Type: cross  Abstract: This paper presents a novel universal perturbation method for generating robust multi-view adversarial examples in 3D object recognition. Unlike conventional attacks limited to single views, our approach operates on multiple 2D images, offering a practical and scalable solution for enhancing model scalability and robustness. This generalizable method bridges the gap between 2D perturbations and 3D-like attack capabilities, making it suitable for real-world applications.   Existing adversarial attacks may become ineffective when images undergo transformations like changes in lighting, camera position, or natural deformations. We address this challenge by crafting a single universal noise perturbation applicable to various object views. Experiments on diverse rendered 3D objects demonstrate the effectiveness of our approach. The universal perturbation successfully identified a single adversarial noise for each given set of 3D object rend
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;ChatGPT&#22312;&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#35268;&#33539;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#33391;&#22909;&#34920;&#29616;&#20294;&#20063;&#21457;&#29616;&#20102;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.02269</link><description>&lt;p&gt;
&#36890;&#36807;ChatGPT&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#35268;&#33539;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Extracting Norms from Contracts Via ChatGPT: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02269
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;ChatGPT&#22312;&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#35268;&#33539;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#33391;&#22909;&#34920;&#29616;&#20294;&#20063;&#21457;&#29616;&#20102;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;ChatGPT&#22312;&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#35268;&#33539;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#35268;&#33539;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#26469;&#36890;&#36807;&#25429;&#25417;&#22914;&#20309;&#31649;&#29702;&#20004;&#20010;&#25110;&#22810;&#20010;&#33258;&#27835;&#26041;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#35774;&#35745;&#22810;&#20027;&#20307;&#31995;&#32479;&#12290;&#25105;&#20204;&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#25215;&#35834;&#12289;&#31105;&#27490;&#12289;&#25480;&#26435;&#21644;&#26435;&#21147;&#31561;&#35268;&#33539;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#35268;&#33539;&#35201;&#32032;&#65288;&#28041;&#21450;&#30340;&#24403;&#20107;&#26041;&#12289;&#21069;&#22240;&#21644;&#21518;&#26524;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;ChatGPT&#22312;&#20174;&#21512;&#21516;&#20013;&#25552;&#21462;&#35268;&#33539;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;ChatGPT&#23637;&#31034;&#20102;&#22312;&#25552;&#21462;&#35268;&#33539;&#26041;&#38754;&#30340;&#33391;&#22909;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#22240;&#27492;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#36825;&#22312;&#35813;&#39046;&#22495;&#36890;&#24120;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;ChatGPT&#22312;&#25552;&#21462;&#36825;&#20123;&#35268;&#33539;&#26041;&#38754;&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#35268;&#33539;&#25552;&#21462;&#12290;&#36825;&#20123;&#38480;&#21046;&#21253;&#25324;&#23545;&#20851;&#38190;&#32454;&#33410;&#30340;&#24573;&#35270;&#12289;&#33222;&#24819;&#12289;&#23545;&#36830;&#25509;&#35789;&#30340;&#38169;&#35823;&#35299;&#26512;&#20197;&#21450;&#31354;&#35268;&#33539;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02269v1 Announce Type: cross  Abstract: We investigate the effectiveness of ChatGPT in extracting norms from contracts. Norms provide a natural way to engineer multiagent systems by capturing how to govern the interactions between two or more autonomous parties. We extract norms of commitment, prohibition, authorization, and power, along with associated norm elements (the parties involved, antecedents, and consequents) from contracts. Our investigation reveals ChatGPT's effectiveness and limitations in norm extraction from contracts. ChatGPT demonstrates promising performance in norm extraction without requiring training or fine-tuning, thus obviating the need for annotated data, which is not generally available in this domain. However, we found some limitations of ChatGPT in extracting these norms that lead to incorrect norm extractions. The limitations include oversight of crucial details, hallucination, incorrect parsing of conjunctions, and empty norm elements. Enhanced 
&lt;/p&gt;</description></item><item><title>OFMPNet&#26159;&#19968;&#31181;&#28145;&#24230;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22478;&#24066;&#29615;&#22659;&#20013;&#25152;&#26377;&#21160;&#24577;&#23545;&#35937;&#30340;&#26410;&#26469;&#34892;&#20026;&#65292;&#32467;&#21512;&#20102;&#21344;&#26377;&#22270;&#21644;&#22330;&#26223;&#21160;&#24577;&#27969;&#37327;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2404.02263</link><description>&lt;p&gt;
OFMPNet: &#28145;&#24230;&#31471;&#21040;&#31471;&#27169;&#22411;&#29992;&#20110;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#21344;&#26377;&#21644;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
OFMPNet: Deep End-to-End Model for Occupancy and Flow Prediction in Urban Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02263
&lt;/p&gt;
&lt;p&gt;
OFMPNet&#26159;&#19968;&#31181;&#28145;&#24230;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22478;&#24066;&#29615;&#22659;&#20013;&#25152;&#26377;&#21160;&#24577;&#23545;&#35937;&#30340;&#26410;&#26469;&#34892;&#20026;&#65292;&#32467;&#21512;&#20102;&#21344;&#26377;&#22270;&#21644;&#22330;&#26223;&#21160;&#24577;&#27969;&#37327;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#20854;&#25552;&#20379;&#20102;&#22312;&#21608;&#22260;&#29615;&#22659;&#20013;&#36873;&#25321;&#36710;&#36742;&#34892;&#20026;&#31574;&#30053;&#30340;&#20851;&#38190;&#25968;&#25454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#29615;&#22659;&#20013;&#25152;&#26377;&#21160;&#24577;&#23545;&#35937;&#30340;&#26410;&#26469;&#34892;&#20026;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21344;&#26377;&#22270;&#21644;&#22330;&#26223;&#21160;&#24577;&#27969;&#37327;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#26500;&#24314;&#21517;&#20026;OFMPNet&#30340;&#28145;&#24230;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#21508;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#19968;&#31995;&#21015;&#20463;&#35270;&#36947;&#36335;&#22270;&#20687;&#65292;&#21344;&#26377;&#26684;&#21644;&#20808;&#21069;&#30340;&#36816;&#21160;&#27969;&#37327;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#12290;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#21487;&#20197;&#34701;&#21512;&#21464;&#21387;&#22120;&#12289;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25110;&#21367;&#31215;&#21333;&#20803;&#12290;&#35299;&#30721;&#22120;&#32771;&#34385;&#20102;&#20351;&#29992;&#21367;&#31215;&#27169;&#22359;&#21644;&#24490;&#29615;&#22359;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02263v1 Announce Type: cross  Abstract: The task of motion prediction is pivotal for autonomous driving systems, providing crucial data to choose a vehicle behavior strategy within its surroundings. Existing motion prediction techniques primarily focus on predicting the future trajectory of each agent in the scene individually, utilizing its past trajectory data. In this paper, we introduce an end-to-end neural network methodology designed to predict the future behaviors of all dynamic objects in the environment. This approach leverages the occupancy map and the scene's motion flow. We are investigatin various alternatives for constructing a deep encoder-decoder model called OFMPNet. This model uses a sequence of bird's-eye-view road images, occupancy grid, and prior motion flow as input data. The encoder of the model can incorporate transformer, attention-based, or convolutional units. The decoder considers the use of both convolutional modules and recurrent blocks. Additio
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02261</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#24490;&#29615;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02261
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35821;&#35328;&#36164;&#28304;&#21644;&#25968;&#25454;&#26631;&#27880;&#19987;&#19994;&#30693;&#35782;&#26377;&#38480;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#32597;&#35265;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#33410;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#35780;&#20272;&#20197;&#35780;&#20272;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#36873;&#25321;&#36866;&#24403;&#30340;LLM&#27880;&#37322;&#32773;&#12290;&#28982;&#21518;&#65292;&#36873;&#25321;&#30340;&#27880;&#37322;&#32773;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;&#26597;&#35810;&#25968;&#25454;&#37327;&#12290;&#23454;&#35777;&#35780;&#20272;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;GPT-4-Turbo&#65292;&#23637;&#31034;&#20102;&#20960;&#20046;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#65292;&#30001;&#20272;&#31639;&#30340;&#28508;&#22312;&#24615;&#33021;&#25351;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02261v1 Announce Type: cross  Abstract: Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential co
&lt;/p&gt;</description></item><item><title>LM2&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;$\texttt{LM}^\texttt{2}$&#65292;&#35813;&#27169;&#22411;&#23558;&#20998;&#35299;&#12289;&#35299;&#20915;&#21644;&#39564;&#35777;&#27169;&#22359;&#21270;&#65292;&#36890;&#36807;&#20998;&#35299;&#22120;&#35782;&#21035;&#20851;&#38190;&#27010;&#24565;&#24182;&#29983;&#25104;&#36880;&#27493;&#23376;&#38382;&#39064;&#65292;&#20174;&#32780;&#21327;&#21516;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02255</link><description>&lt;p&gt;
$\texttt{LM}^\texttt{2}$: &#19968;&#31181;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
$\texttt{LM}^\texttt{2}$: A Simple Society of Language Models Solves Complex Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02255
&lt;/p&gt;
&lt;p&gt;
LM2&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;$\texttt{LM}^\texttt{2}$&#65292;&#35813;&#27169;&#22411;&#23558;&#20998;&#35299;&#12289;&#35299;&#20915;&#21644;&#39564;&#35777;&#27169;&#22359;&#21270;&#65292;&#36890;&#36807;&#20998;&#35299;&#22120;&#35782;&#21035;&#20851;&#38190;&#27010;&#24565;&#24182;&#29983;&#25104;&#36880;&#27493;&#23376;&#38382;&#39064;&#65292;&#20174;&#32780;&#21327;&#21516;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23637;&#31034;&#20102;&#20986;&#29616;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMS)&#32463;&#24120;&#20250;&#22312;&#22797;&#26434;&#30340;&#12289;&#22810;&#27493;&#39588;&#30340;&#25512;&#29702;&#20013;&#36855;&#22833;&#26041;&#21521;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#38382;&#39064;&#26469;&#25552;&#20379;&#25351;&#23548;&#65292;&#21487;&#20197;&#24341;&#21457;LLM&#25512;&#29702;&#30340;&#26356;&#24378;&#20581;&#24615;&#8212;&#8212;&#19968;&#20010;&#20998;&#35299;&#22120;&#29983;&#25104;&#23376;&#38382;&#39064;&#65292;&#19968;&#20010;&#35299;&#31639;&#22120;&#35299;&#20915;&#27599;&#20010;&#36825;&#20123;&#23376;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#26410;&#33021;&#36866;&#24212;&#20998;&#35299;&#22120;&#21644;&#35299;&#31639;&#22120;&#27169;&#22359;&#20043;&#38388;&#30340;&#21327;&#35843;(&#26080;&#35770;&#26159;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#36824;&#26159;&#22312;&#19981;&#21516;&#30340;&#19987;&#38376;&#27169;&#22411;&#20013;)&#8212;&#8212;&#20998;&#35299;&#22120;&#27809;&#26377;&#36319;&#36394;&#35299;&#31639;&#22120;&#25353;&#29031;&#20998;&#35299;&#30340;&#25512;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LM2&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;LM2&#23558;&#20998;&#35299;&#12289;&#35299;&#20915;&#21644;&#39564;&#35777;&#27169;&#22359;&#21270;&#20026;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20998;&#35299;&#22120;&#27169;&#22359;&#35782;&#21035;&#35299;&#20915;&#38382;&#39064;&#25152;&#38656;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#24182;&#26681;&#25454;&#25512;&#29702;&#35201;&#27714;&#29983;&#25104;&#36880;&#27493;&#23376;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02255v1 Announce Type: cross  Abstract: Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems elicits more robustness in LLM reasoning -- a decomposer generates the subproblems, and a solver solves each of these subproblems. However, these techniques fail to accommodate coordination between the decomposer and the solver modules (either in a single model or different specialized ones) -- the decomposer does not keep track of the ability of the solver to follow the decomposed reasoning. In this paper, we propose LM2 to address these challenges. LM2 modularizes the decomposition, solution, and verification into three different language models. The decomposer module identifies the key concepts necessary to solve the problem and generates step-by-step subquestions according to the reasoning requ
&lt;/p&gt;</description></item><item><title>RAT&#27169;&#22411;&#26159;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;CTR&#39044;&#27979;&#27169;&#22411;&#20165;&#20851;&#27880;&#26679;&#26412;&#20869;&#29305;&#24449;&#20132;&#20114;&#32780;&#24573;&#30053;&#36328;&#26679;&#26412;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#30340;&#20840;&#38754;&#29305;&#24449;&#20132;&#20114;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;CTR&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02249</link><description>&lt;p&gt;
RAT: &#26816;&#32034;&#22686;&#24378;&#21464;&#25442;&#22120;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02249
&lt;/p&gt;
&lt;p&gt;
RAT&#27169;&#22411;&#26159;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;CTR&#39044;&#27979;&#27169;&#22411;&#20165;&#20851;&#27880;&#26679;&#26412;&#20869;&#29305;&#24449;&#20132;&#20114;&#32780;&#24573;&#30053;&#36328;&#26679;&#26412;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#30340;&#20840;&#38754;&#29305;&#24449;&#20132;&#20114;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;CTR&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#26159;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#35774;&#35745;&#26377;&#25928;&#30340;&#29305;&#24449;&#20132;&#20114;&#27169;&#22411;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23545;&#21333;&#20010;&#26679;&#26412;&#20869;&#30340;&#29305;&#24449;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#24573;&#30053;&#20102;&#21487;&#20197;&#20316;&#20026;&#21442;&#32771;&#32972;&#26223;&#26469;&#22686;&#24378;&#39044;&#27979;&#30340;&#28508;&#22312;&#36328;&#26679;&#26412;&#20851;&#31995;&#12290;&#20026;&#24357;&#34917;&#36825;&#31181;&#19981;&#36275;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#21464;&#25442;&#22120;&#65288;RAT&#65289;&#65292;&#26088;&#22312;&#33719;&#21462;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#20132;&#20114;&#12290;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#30446;&#26631;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#12290;&#28982;&#21518;&#21033;&#29992;&#32423;&#32852;&#27880;&#24847;&#21147;&#26500;&#24314;Transformer&#23618;&#65292;&#20197;&#25429;&#33719;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#29305;&#24449;&#20132;&#20114;&#65292;&#20419;&#36827;&#20840;&#38754;&#25512;&#29702;&#20197;&#25913;&#21892;CTR&#39044;&#27979;&#30340;&#21516;&#26102;&#20445;&#25345;&#25928;&#29575;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;RAT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02249v1 Announce Type: cross  Abstract: Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and sugge
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#29305;&#24449;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#23578;&#26410;&#34987;&#26126;&#30830;&#34920;&#24449;&#12290;&#30740;&#31350;&#35797;&#22270;&#29702;&#35299;&#25506;&#32034;&#29305;&#24449;&#19982;&#25913;&#36827;&#24615;&#33021;&#21644;&#25928;&#29575;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#20851;&#31995;</title><link>https://arxiv.org/abs/2404.02235</link><description>&lt;p&gt;
&#25506;&#32034;&#26159;&#24744;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;&#21527;&#65311;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#36716;&#31227;&#30340;&#26377;&#25928;&#25506;&#32034;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02235
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#29305;&#24449;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#23578;&#26410;&#34987;&#26126;&#30830;&#34920;&#24449;&#12290;&#30740;&#31350;&#35797;&#22270;&#29702;&#35299;&#25506;&#32034;&#29305;&#24449;&#19982;&#25913;&#36827;&#24615;&#33021;&#21644;&#25928;&#29575;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#20013;&#65292;&#20154;&#20204;&#27491;&#22312;&#21162;&#21147;&#35774;&#35745;&#26356;&#39640;&#25928;&#12289;&#26356;&#20855;&#29983;&#20135;&#21147;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#38382;&#39064;&#12290;&#36825;&#20123;&#25506;&#32034;&#26041;&#27861;&#36890;&#24120;&#20849;&#20139;&#20849;&#21516;&#21407;&#21017;&#65288;&#20363;&#22914;&#25913;&#21892;&#22810;&#26679;&#24615;&#65289;&#21644;&#23454;&#29616;&#32454;&#33410;&#65288;&#20363;&#22914;&#20869;&#22312;&#22870;&#21169;&#65289;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#38750;&#38745;&#27490;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#38656;&#35201;&#25506;&#32034;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#36866;&#24212;&#22312;&#32447;&#36716;&#31227;&#23398;&#20064;&#20013;&#29615;&#22659;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#20855;&#20307;&#25506;&#32034;&#29305;&#24449;&#19982;&#22312;&#28145;&#24230;RL&#20013;&#30340;&#26377;&#25928;&#36716;&#31227;&#23398;&#20064;&#20043;&#38388;&#30340;&#20851;&#31995;&#23578;&#26410;&#34987;&#34920;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#29702;&#35299;&#26174;&#33879;&#25506;&#32034;&#29305;&#24449;&#19982;&#25913;&#36827;&#24615;&#33021;&#21644;&#25928;&#29575;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21313;&#19968;&#20010;&#27969;&#34892;&#30340;&#25506;&#32034;&#31639;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#36716;&#31227;&#31867;&#22411;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#30830;&#23450;&#37027;&#20123;&#31215;&#26497;&#24433;&#21709;&#22312;&#32447;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02235v1 Announce Type: cross  Abstract: In deep reinforcement learning (RL) research, there has been a concerted effort to design more efficient and productive exploration methods while solving sparse-reward problems. These exploration methods often share common principles (e.g., improving diversity) and implementation details (e.g., intrinsic reward). Prior work found that non-stationary Markov decision processes (MDPs) require exploration to efficiently adapt to changes in the environment with online transfer learning. However, the relationship between specific exploration characteristics and effective transfer learning in deep RL has not been characterized. In this work, we seek to understand the relationships between salient exploration characteristics and improved performance and efficiency in transfer learning. We test eleven popular exploration algorithms on a variety of transfer types -- or ``novelties'' -- to identify the characteristics that positively affect onlin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#23450;&#20301;&#25216;&#26415;&#36827;&#34892;&#35270;&#30028;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#35270;&#37326;&#22806;&#29289;&#20307;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#22122;&#22768;&#30340;&#25361;&#25112;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02227</link><description>&lt;p&gt;
OOSTraj&#65306;&#21033;&#29992;&#35270;&#35273;&#23450;&#20301;&#21435;&#22122;&#30340;&#35270;&#30028;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02227
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#23450;&#20301;&#25216;&#26415;&#36827;&#34892;&#35270;&#30028;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#35270;&#37326;&#22806;&#29289;&#20307;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#22122;&#22768;&#30340;&#25361;&#25112;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#21160;&#39550;&#39542;&#20013;&#26159;&#22522;&#30784;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#29702;&#35299;&#34892;&#20154;&#34892;&#20026;&#21644;&#23454;&#29616;&#31215;&#26497;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#20551;&#35774;&#31934;&#30830;&#23436;&#25972;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#24573;&#35270;&#20102;&#19982;&#35270;&#37326;&#22806;&#29289;&#20307;&#21644;&#30001;&#20110;&#26377;&#38480;&#25668;&#20687;&#22836;&#33539;&#22260;&#12289;&#29289;&#29702;&#38556;&#30861;&#20197;&#21450;&#32570;&#20047;&#21435;&#22122;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#30495;&#23454;&#25968;&#25454;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#36825;&#26679;&#30340;&#20559;&#35265;&#26159;&#20851;&#38190;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#30340;&#38750;&#21487;&#35265;&#23545;&#35937;&#34987;&#24573;&#30053;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#23450;&#20301;&#25216;&#26415;&#36827;&#34892;&#35270;&#30028;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#21435;&#22122;&#22024;&#26434;&#30340;&#20256;&#24863;&#22120;&#35266;&#27979;&#65292;&#24182;&#23558;&#35270;&#37326;&#22806;&#29289;&#20307;&#30340;&#20256;&#24863;&#22120;&#36712;&#36857;&#31934;&#30830;&#26144;&#23556;&#21040;&#35270;&#35273;&#36712;&#36857;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35270;&#37326;&#22806;&#36712;&#36857;&#39044;&#27979;&#26041;&#38754;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02227v1 Announce Type: cross  Abstract: Trajectory prediction is fundamental in computer vision and autonomous driving, particularly for understanding pedestrian behavior and enabling proactive decision-making. Existing approaches in this field often assume precise and complete observational data, neglecting the challenges associated with out-of-view objects and the noise inherent in sensor data due to limited camera range, physical obstructions, and the absence of ground truth for denoised sensor data. Such oversights are critical safety concerns, as they can result in missing essential, non-visible objects. To bridge this gap, we present a novel method for out-of-sight trajectory prediction that leverages a vision-positioning technique. Our approach denoises noisy sensor observations in an unsupervised manner and precisely maps sensor-based trajectories of out-of-sight objects into visual trajectories. This method has demonstrated state-of-the-art performance in out-of-sig
&lt;/p&gt;</description></item><item><title>CHOSEN&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#30340;&#23545;&#27604;&#20551;&#35774;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#20551;&#35774;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#20013;&#23454;&#29616;&#36739;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#21644;&#27861;&#32447;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02225</link><description>&lt;p&gt;
CHOSEN&#65306;&#29992;&#20110;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#30340;&#23545;&#27604;&#20551;&#35774;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02225
&lt;/p&gt;
&lt;p&gt;
CHOSEN&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#30340;&#23545;&#27604;&#20551;&#35774;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#20551;&#35774;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#20013;&#23454;&#29616;&#36739;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#21644;&#27861;&#32447;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CHOSEN&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#28789;&#27963;&#12289;&#24378;&#22823;&#19988;&#26377;&#25928;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#26694;&#26550;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#27969;&#31243;&#20013;&#65292;&#20855;&#26377;&#38024;&#23545;&#19981;&#21516;&#22810;&#35270;&#35282;&#37319;&#38598;&#31995;&#32479;&#65288;&#22914;&#30456;&#26426;&#30456;&#23545;&#23450;&#20301;&#21644;&#38236;&#22836;&#65289;&#30340;&#30452;&#35266;&#27867;&#21270;&#33021;&#21147;&#12290;&#32473;&#23450;&#21021;&#22987;&#28145;&#24230;&#20272;&#35745;&#65292;CHOSEN&#36845;&#20195;&#22320;&#37325;&#26032;&#37319;&#26679;&#24182;&#36873;&#25321;&#26368;&#20339;&#20551;&#35774;&#65292;&#24182;&#33258;&#21160;&#36866;&#24212;&#30001;&#37319;&#38598;&#31995;&#32479;&#30830;&#23450;&#30340;&#19981;&#21516;&#24230;&#37327;&#25110;&#22266;&#26377;&#23610;&#24230;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#22312;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#20013;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#31934;&#24515;&#35774;&#35745;&#30340;&#20551;&#35774;&#29305;&#24449;&#65292;&#22522;&#20110;&#36825;&#20123;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#27491;&#36127;&#20551;&#35774;&#12290;&#23558;CHOSEN&#38598;&#25104;&#21040;&#31616;&#21333;&#30340;&#22522;&#32447;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#27969;&#31243;&#20013;&#65292;&#22312;&#28145;&#24230;&#21644;&#27861;&#32447;&#31934;&#24230;&#26041;&#38754;&#25552;&#20379;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36136;&#37327;&#65292;&#19982;&#35768;&#22810;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#27969;&#31243;&#30456;&#27604;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02225v1 Announce Type: cross  Abstract: We propose CHOSEN, a simple yet flexible, robust and effective multi-view depth refinement framework. It can be employed in any existing multi-view stereo pipeline, with straightforward generalization capability for different multi-view capture systems such as camera relative positioning and lenses. Given an initial depth estimation, CHOSEN iteratively re-samples and selects the best hypotheses, and automatically adapts to different metric or intrinsic scales determined by the capture system. The key to our approach is the application of contrastive learning in an appropriate solution space and a carefully designed hypothesis feature, based on which positive and negative hypotheses can be effectively distinguished. Integrated in a simple baseline multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of depth and normal accuracy compared to many current deep learning based multi-view stereo pipelines.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#21521;&#26032;&#25163;&#25552;&#20379;&#20174;&#19968;&#33324;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#21040;&#20855;&#20307;&#20195;&#30721;&#36741;&#21161;&#31561;&#22235;&#20010;&#32423;&#21035;&#30340;&#25552;&#31034;&#65292;&#33021;&#26356;&#22909;&#22320;&#25903;&#25345;&#20182;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2404.02213</link><description>&lt;p&gt;
&#25506;&#35752;&#22810;&#23618;&#27425;&#30340;GPT&#29983;&#25104;&#30340;&#32534;&#31243;&#25552;&#31034;&#22914;&#20309;&#25903;&#25345;&#25110;&#20351;&#26032;&#25163;&#22833;&#26395;
&lt;/p&gt;
&lt;p&gt;
Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02213
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#21521;&#26032;&#25163;&#25552;&#20379;&#20174;&#19968;&#33324;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#21040;&#20855;&#20307;&#20195;&#30721;&#36741;&#21161;&#31561;&#22235;&#20010;&#32423;&#21035;&#30340;&#25552;&#31034;&#65292;&#33021;&#26356;&#22909;&#22320;&#25903;&#25345;&#20182;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;&#19981;&#21516;&#30340;&#25945;&#32946;&#32972;&#26223;&#20013;&#65292;&#21253;&#25324;&#25552;&#20379;&#33258;&#36866;&#24212;&#32534;&#31243;&#25552;&#31034;&#65292;&#36825;&#31181;&#21453;&#39304;&#31867;&#22411;&#20391;&#37325;&#20110;&#24110;&#21161;&#23398;&#29983;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#21069;&#36827;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#31995;&#32479;&#20165;&#38480;&#20110;&#19968;&#31181;&#25552;&#31034;&#31867;&#22411;&#12290;&#20026;&#20102;&#35843;&#26597;&#19981;&#21516;&#32423;&#21035;&#30340;&#25552;&#31034;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#25903;&#25345;&#23398;&#29983;&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#23398;&#20064;&#65292;&#25105;&#20204;&#23545;12&#21517;&#26032;&#25163;&#36827;&#34892;&#20102;&#19968;&#39033;&#33258;&#36848;&#24605;&#32771;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;LLM Hint Factory&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#25552;&#20379;&#22235;&#20010;&#32423;&#21035;&#30340;&#25552;&#31034;&#65292;&#20174;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#21040;&#20855;&#20307;&#30340;&#20195;&#30721;&#36741;&#21161;&#65292;&#26684;&#24335;&#21644;&#31890;&#24230;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#21457;&#29616;&#20165;&#20351;&#29992;&#39640;&#32423;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#21487;&#33021;&#26159;&#26080;&#21161;&#30340;&#65292;&#29978;&#33267;&#20250;&#35823;&#23548;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#19979;&#19968;&#27493;&#25110;&#19982;&#35821;&#27861;&#30456;&#20851;&#30340;&#24110;&#21161;&#35831;&#27714;&#26102;&#12290;&#28155;&#21152;&#20302;&#32423;&#21035;&#30340;&#25552;&#31034;&#65292;&#22914;&#24102;&#26377;&#34892;&#20869;&#27880;&#37322;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#23398;&#29983;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23450;&#21046;&#21270;&#26410;&#26469;&#30340;&#24037;&#20316;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02213v1 Announce Type: cross  Abstract: Recent studies have integrated large language models (LLMs) into diverse educational contexts, including providing adaptive programming hints, a type of feedback focuses on helping students move forward during problem-solving. However, most existing LLM-based hint systems are limited to one single hint type. To investigate whether and how different levels of hints can support students' problem-solving and learning, we conducted a think-aloud study with 12 novices using the LLM Hint Factory, a system providing four levels of hints from general natural language guidance to concrete code assistance, varying in format and granularity. We discovered that high-level natural language hints alone can be helpless or even misleading, especially when addressing next-step or syntax-related help requests. Adding lower-level hints, like code examples with in-line comments, can better support students. The findings open up future work on customizing 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#25552;&#20379;&#32508;&#21512;&#24615;&#24615;&#21035;&#27602;&#24615;&#25351;&#26631;&#30340;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25919;&#31574;&#21046;&#23450;&#32773;&#12289;&#22312;&#32447;&#31038;&#21306;&#31649;&#29702;&#21592;&#21644;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#23478;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#31649;&#29702;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02205</link><description>&lt;p&gt;
&#19968;&#20010;&#25972;&#20307;&#24615;&#26497;&#21270;&#25351;&#26631;&#20197;&#34913;&#37327;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;
&lt;/p&gt;
&lt;p&gt;
A Holistic Indicator of Polarization to Measure Online Sexism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#25552;&#20379;&#32508;&#21512;&#24615;&#24615;&#21035;&#27602;&#24615;&#25351;&#26631;&#30340;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25919;&#31574;&#21046;&#23450;&#32773;&#12289;&#22312;&#32447;&#31038;&#21306;&#31649;&#29702;&#21592;&#21644;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#23478;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#31649;&#29702;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02205v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#30007;&#26435;&#20027;&#20041;&#32773;&#21644;&#22899;&#26435;&#20027;&#20041;&#32773;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22312;&#32447;&#36235;&#21183;&#38656;&#35201;&#19968;&#20010;&#25972;&#20307;&#24615;&#30340;&#34913;&#37327;&#24615;&#21035;&#27495;&#35270;&#27700;&#24179;&#30340;&#25351;&#26631;&#12290;&#36825;&#20010;&#25351;&#26631;&#23545;&#20110;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#22312;&#32447;&#31038;&#21306;&#30340;&#31649;&#29702;&#21592;&#65288;&#22914;subreddits&#65289;&#20197;&#21450;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#23478;&#33267;&#20851;&#37325;&#35201;&#65292;&#20182;&#20204;&#21487;&#20197;&#26681;&#25454;&#24615;&#21035;&#27495;&#35270;&#31243;&#24230;&#20462;&#25913;&#31649;&#29702;&#31574;&#30053;&#65292;&#25110;&#32773;&#21305;&#37197;&#21644;&#27604;&#36739;&#19981;&#21516;&#24179;&#21488;&#21644;&#31038;&#21306;&#19978;&#30340;&#26102;&#24577;&#24615;&#21035;&#27495;&#35270;&#65292;&#20197;&#21450;&#25512;&#26029;&#31038;&#20250;&#31185;&#23398;&#35265;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#21487;&#27604;&#36739;&#30340;&#38024;&#23545;&#30007;&#24615;&#12289;&#22899;&#24615;&#36523;&#20221;&#20197;&#21450;&#30007;&#24615;&#12289;&#22899;&#24615;&#20010;&#20154;&#30340;&#27602;&#24615;&#25972;&#20307;&#25351;&#26631;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30417;&#30563;NLP&#26041;&#27861;&#38656;&#35201;&#22312;&#30446;&#26631;&#32423;&#21035;&#23545;&#26377;&#27602;&#35780;&#35770;&#36827;&#34892;&#27880;&#37322;&#65288;&#20363;&#22914;&#27880;&#37322;&#29305;&#21035;&#38024;&#23545;&#22899;&#24615;&#26377;&#27602;&#30340;&#35780;&#35770;&#65289;&#26469;&#26816;&#27979;&#38024;&#23545;&#24615;&#26377;&#27602;&#35780;&#35770;&#65292;&#25105;&#20204;&#30340;&#25351;&#26631;&#21033;&#29992;&#30417;&#30563;&#24335;NLP&#26469;&#26816;&#27979;&#27602;&#24615;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02205v1 Announce Type: cross  Abstract: The online trend of the manosphere and feminist discourse on social networks requires a holistic measure of the level of sexism in an online community. This indicator is important for policymakers and moderators of online communities (e.g., subreddits) and computational social scientists, either to revise moderation strategies based on the degree of sexism or to match and compare the temporal sexism across different platforms and communities with real-time events and infer social scientific insights.   In this paper, we build a model that can provide a comparable holistic indicator of toxicity targeted toward male and female identity and male and female individuals. Despite previous supervised NLP methods that require annotation of toxic comments at the target level (e.g. annotating comments that are specifically toxic toward women) to detect targeted toxic comments, our indicator uses supervised NLP to detect the presence of toxicity 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20843;&#20010;&#26032;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24341;&#36215;&#22312;NAS&#24320;&#21457;&#20013;&#30340;&#20851;&#27880;&#24182;&#40723;&#21169;&#20316;&#32773;&#32771;&#34385;&#27169;&#22411;&#22312;&#24320;&#21457;&#26102;&#26410;&#30693;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.02189</link><description>&lt;p&gt;
&#20174;&#20043;&#21069;&#26410;&#35265;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Insights from the Use of Previously Unseen Neural Architecture Search Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02189
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20843;&#20010;&#26032;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24341;&#36215;&#22312;NAS&#24320;&#21457;&#20013;&#30340;&#20851;&#27880;&#24182;&#40723;&#21169;&#20316;&#32773;&#32771;&#34385;&#27169;&#22411;&#22312;&#24320;&#21457;&#26102;&#26410;&#30693;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#21487;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#29992;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#19981;&#21516;&#65292;&#22240;&#27492;&#38656;&#35201;&#28145;&#24230;&#23398;&#20064;&#19987;&#23478;&#26469;&#30830;&#23450;&#26368;&#20339;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#36829;&#32972;&#20102;&#28040;&#38500;&#19987;&#23478;&#38656;&#27714;&#30340;&#24076;&#26395;&#12290;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#26368;&#20339;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;NAS&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#19968;&#23567;&#32452;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#20195;&#34920;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20843;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#19968;&#31995;&#21015;NAS&#25361;&#25112;&#65306;AddNIST&#65292;Language&#65292;MultNIST&#65292;CIFARTile&#65292;Gutenberg&#65292;Isabella&#65292;GeoClassing &#21644; Chesseract&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;&#26088;&#22312;&#24341;&#36215;NAS&#24320;&#21457;&#20013;&#30340;&#27880;&#24847;&#21644;&#40723;&#21169;&#20316;&#32773;&#32771;&#34385;&#20182;&#20204;&#30340;&#27169;&#22411;&#22312;&#24320;&#21457;&#26102;&#26410;&#30693;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02189v1 Announce Type: cross  Abstract: The boundless possibility of neural networks which can be used to solve a problem -- each with different performance -- leads to a situation where a Deep Learning expert is required to identify the best neural network. This goes against the hope of removing the need for experts. Neural Architecture Search (NAS) offers a solution to this by automatically identifying the best architecture. However, to date, NAS work has focused on a small set of datasets which we argue are not representative of real-world problems. We introduce eight new datasets created for a series of NAS Challenges: AddNIST, Language, MultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These datasets and challenges are developed to direct attention to issues in NAS development and to encourage authors to consider how their models will perform on datasets unknown to them at development time. We present experimentation using standard Deep Learning met
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#20107;&#25925;&#20005;&#37325;&#24615;&#24314;&#27169;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35299;&#20915;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#37325;&#37319;&#26679;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#31163;&#25955;&#39118;&#38505;&#22240;&#32032;&#23849;&#28291;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.02187</link><description>&lt;p&gt;
&#37319;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#20107;&#25925;&#20005;&#37325;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
A Generative Deep Learning Approach for Crash Severity Modeling with Imbalanced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02187
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#20107;&#25925;&#20005;&#37325;&#24615;&#24314;&#27169;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35299;&#20915;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#37325;&#37319;&#26679;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#31163;&#25955;&#39118;&#38505;&#22240;&#32032;&#23849;&#28291;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25758;&#36710;&#25968;&#25454;&#36890;&#24120;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#24179;&#34913;&#24615;&#65292;&#22823;&#22810;&#25968;&#20107;&#25925;&#26159;&#38750;&#33268;&#21629;&#20107;&#25925;&#65292;&#21482;&#26377;&#23569;&#25968;&#26159;&#30001;&#20110;&#20854;&#32597;&#35265;&#24615;&#32780;&#33268;&#21629;&#20107;&#25925;&#12290;&#36825;&#31181;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#23545;&#20107;&#25925;&#20005;&#37325;&#24615;&#24314;&#27169;&#26500;&#25104;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38590;&#20197;&#25311;&#21512;&#21644;&#35299;&#37322;&#20855;&#26377;&#38750;&#24120;&#26377;&#38480;&#26679;&#26412;&#30340;&#33268;&#21629;&#20107;&#25925;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#25968;&#25454;&#37325;&#37319;&#26679;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#31181;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22914;&#27424;&#37319;&#26679;&#21644;&#36807;&#37319;&#26679;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#22914;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36830;&#32493;&#21464;&#37327;&#12290;&#23613;&#31649;&#19968;&#20123;&#37325;&#37319;&#26679;&#26041;&#27861;&#24050;&#32463;&#25913;&#36827;&#20197;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#21464;&#37327;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#31232;&#30095;&#31163;&#25955;&#39118;&#38505;&#22240;&#32032;&#30456;&#20851;&#30340;&#23849;&#28291;&#38382;&#39064;&#19978;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#20840;&#38754;&#30740;&#31350;&#26469;&#27604;&#36739;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02187v1 Announce Type: cross  Abstract: Crash data is often greatly imbalanced, with the majority of crashes being non-fatal crashes, and only a small number being fatal crashes due to their rarity. Such data imbalance issue poses a challenge for crash severity modeling since it struggles to fit and interpret fatal crash outcomes with very limited samples. Usually, such data imbalance issues are addressed by data resampling methods, such as under-sampling and over-sampling techniques. However, most traditional and deep learning-based data resampling methods, such as synthetic minority oversampling technique (SMOTE) and generative Adversarial Networks (GAN) are designed dedicated to processing continuous variables. Though some resampling methods have improved to handle both continuous and discrete variables, they may have difficulties in dealing with the collapse issue associated with sparse discrete risk factors. Moreover, there is a lack of comprehensive studies that compar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#26694;&#26550;&#65288;SoA&#65289;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#20195;&#30721;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#29983;&#25104;&#21644;&#20248;&#21270;&#65292;&#20195;&#29702;&#21487;&#33258;&#20027;&#36816;&#20316;&#29983;&#25104;&#21644;&#20462;&#25913;&#20195;&#30721;&#32452;&#20214;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#22797;&#26434;&#24615;&#21160;&#24577;&#22686;&#21152;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02183</link><description>&lt;p&gt;
&#33258;&#32452;&#32455;&#20195;&#29702;&#65306;&#38754;&#21521;&#36229;&#22823;&#35268;&#27169;&#20195;&#30721;&#29983;&#25104;&#21644;&#20248;&#21270;&#30340;LLM&#22810;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02183
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#26694;&#26550;&#65288;SoA&#65289;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#20195;&#30721;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#29983;&#25104;&#21644;&#20248;&#21270;&#65292;&#20195;&#29702;&#21487;&#33258;&#20027;&#36816;&#20316;&#29983;&#25104;&#21644;&#20462;&#25913;&#20195;&#30721;&#32452;&#20214;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#22797;&#26434;&#24615;&#21160;&#24577;&#22686;&#21152;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#36827;&#34892;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#20351;&#25105;&#20204;&#26356;&#25509;&#36817;&#33258;&#21160;&#36719;&#20214;&#24320;&#21457;&#30340;&#26410;&#26469;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21333;&#20195;&#29702;&#26041;&#27861;&#22312;&#29983;&#25104;&#21644;&#25913;&#36827;&#22823;&#35268;&#27169;&#22797;&#26434;&#20195;&#30721;&#24211;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#25152;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#26694;&#26550;&#65288;SoA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#20195;&#30721;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#29983;&#25104;&#21644;&#20248;&#21270;&#12290;&#22312;SoA&#20013;&#65292;&#33258;&#32452;&#32455;&#20195;&#29702;&#29420;&#31435;&#36816;&#20316;&#65292;&#20197;&#29983;&#25104;&#21644;&#20462;&#25913;&#20195;&#30721;&#32452;&#20214;&#65292;&#21516;&#26102;&#26080;&#32541;&#21327;&#20316;&#26500;&#24314;&#25972;&#20307;&#20195;&#30721;&#24211;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#22522;&#20110;&#38382;&#39064;&#22797;&#26434;&#24615;&#33258;&#21160;&#22686;&#21152;&#20195;&#29702;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#21160;&#24577;&#25193;&#23637;&#24615;&#12290;&#36825;&#20351;&#24471;&#25972;&#20307;&#20195;&#30721;&#37327;&#21487;&#20197;&#26681;&#25454;&#20195;&#29702;&#25968;&#37327;&#26080;&#38480;&#22686;&#21152;&#65292;&#21516;&#26102;&#30001;&#27599;&#20010;&#20195;&#29702;&#31649;&#29702;&#30340;&#20195;&#30721;&#37327;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02183v1 Announce Type: cross  Abstract: Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by eac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#24265;&#20215;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;INDT-ASD&#21360;&#24230;&#25968;&#25454;&#24211;&#26089;&#26399;&#26816;&#27979;&#33258;&#38381;&#30151;&#12290;</title><link>https://arxiv.org/abs/2404.02181</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36890;&#36807;INDT-ASD&#21360;&#24230;&#25968;&#25454;&#24211;&#26089;&#26399;&#26816;&#27979;&#33258;&#38381;&#30151;
&lt;/p&gt;
&lt;p&gt;
Leveraging Machine Learning for Early Autism Detection via INDT-ASD Indian Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#24265;&#20215;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;INDT-ASD&#21360;&#24230;&#25968;&#25454;&#24211;&#26089;&#26399;&#26816;&#27979;&#33258;&#38381;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#21307;&#30103;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#31070;&#32463;&#21457;&#32946;&#38382;&#39064;&#30340;&#35786;&#26029;&#26159;&#21307;&#30103;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#20840;&#29699;&#22686;&#38271;&#26368;&#24555;&#30340;&#21457;&#23637;&#24615;&#38556;&#30861;&#20043;&#19968;&#12290;&#20020;&#24202;&#31579;&#26597;&#27979;&#35797;&#29992;&#20110;&#35782;&#21035;&#33258;&#38381;&#30151;&#30151;&#29366;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#20294;&#29616;&#22312;&#38543;&#30528;ML&#30340;&#36827;&#23637;&#65292;&#26089;&#26399;&#35782;&#21035;&#33258;&#38381;&#30151;&#21464;&#24471;&#21487;&#34892;&#12290;&#20808;&#21069;&#36827;&#34892;&#36807;&#35768;&#22810;&#19981;&#21516;&#25216;&#26415;&#30340;&#25506;&#32034;&#65292;&#20294;&#22312;&#21033;&#29992;&#32463;&#36807;&#20020;&#24202;&#39564;&#35777;&#30340;&#21360;&#24230;ASD&#25968;&#25454;&#24211;&#39044;&#27979;&#33258;&#38381;&#30151;&#29305;&#24449;&#30340;&#33021;&#21147;&#26041;&#38754;&#65292;&#27809;&#26377;&#19968;&#20010;&#20135;&#29983;&#39044;&#26399;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;ML&#24320;&#21457;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#24265;&#20215;&#30340;&#25216;&#26415;&#26469;&#35782;&#21035;ASD&#12290;&#37319;&#29992;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;Adaboost&#65288;AB&#65289;&#12289;Gradient Boost&#65288;GB&#65289;&#12289;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#12289;&#36923;&#36753;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02181v1 Announce Type: cross  Abstract: Machine learning (ML) has advanced quickly, particularly throughout the area of health care. The diagnosis of neurodevelopment problems using ML is a very important area of healthcare. Autism spectrum disorder (ASD) is one of the developmental disorders that is growing the fastest globally. The clinical screening tests used to identify autistic symptoms are expensive and time-consuming. But now that ML has been advanced, it's feasible to identify autism early on. Previously, many different techniques have been used in investigations. Still, none of them have produced the anticipated outcomes when it comes to the capacity to predict autistic features utilizing a clinically validated Indian ASD database. Therefore, this study aimed to develop a simple, quick, and inexpensive technique for identifying ASD by using ML. Various machine learning classifiers, including Adaboost (AB), Gradient Boost (GB), Decision Tree (DT), Logistic Regressio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;</title><link>https://arxiv.org/abs/2404.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#22320;&#36136;&#21046;&#22270;&#30340;&#36965;&#24863;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Remote sensing framework for geological mapping via stacked autoencoders and clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36965;&#24863;&#22320;&#36136;&#21046;&#22270;&#20013;&#38754;&#20020;&#30528;&#30001;&#20110;&#20934;&#30830;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#32780;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25581;&#31034;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#38477;&#32500;&#26041;&#27861;&#20855;&#26377;&#22312;&#25552;&#39640;&#22320;&#36136;&#22270;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#38477;&#32500;&#26041;&#27861;&#21487;&#33021;&#22312;&#38750;&#32447;&#24615;&#25968;&#25454;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#20294;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#27169;&#25311;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#23618;&#65292;&#29992;&#20110;&#25429;&#33719;&#23545;&#36965;&#24863;&#25968;&#25454;&#26377;&#29992;&#30340;&#20998;&#23618;&#25968;&#25454;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#22788;&#29702;&#36965;&#24863;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02180v1 Announce Type: cross  Abstract: Supervised learning methods for geological mapping via remote sensing face limitations due to the scarcity of accurately labelled training data. In contrast, unsupervised learning methods, such as dimensionality reduction and clustering have the ability to uncover patterns and structures in remote sensing data without relying on predefined labels. Dimensionality reduction methods have the potential to play a crucial role in improving the accuracy of geological maps. Although conventional dimensionality reduction methods may struggle with nonlinear data, unsupervised deep learning models such as autoencoders have the ability to model nonlinear relationship in data. Stacked autoencoders feature multiple interconnected layers to capture hierarchical data representations that can be useful for remote sensing data. In this study, we present an unsupervised machine learning framework for processing remote sensing data by utilizing stacked au
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#19968;&#31181;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#36890;&#20449;&#32422;&#26463;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#26696;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#34701;&#21512;&#20013;&#24515;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02179</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21644;&#36895;&#29575;&#33258;&#36866;&#24212;&#29305;&#24449;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Distributed and Rate-Adaptive Feature Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02179
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#19968;&#31181;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#36890;&#20449;&#32422;&#26463;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#26696;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#34701;&#21512;&#20013;&#24515;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#21644;&#36895;&#29575;&#33258;&#36866;&#24212;&#29305;&#24449;&#21387;&#32553;&#38382;&#39064;&#65292;&#38024;&#23545;&#32447;&#24615;&#22238;&#24402;&#12290;&#19968;&#32452;&#20998;&#24067;&#24335;&#20256;&#24863;&#22120;&#25910;&#38598;&#22238;&#24402;&#22120;&#25968;&#25454;&#30340;&#19981;&#30456;&#20132;&#29305;&#24449;&#12290;&#20551;&#23450;&#34701;&#21512;&#20013;&#24515;&#21253;&#21547;&#19968;&#20010;&#22312;&#25972;&#20010;&#26410;&#21387;&#32553;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;&#20256;&#24863;&#22120;&#21387;&#32553;&#20854;&#35266;&#27979;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#36890;&#20449;&#21463;&#38480;&#30340;&#20449;&#36947;&#23558;&#20854;&#21457;&#36865;&#21040;&#34701;&#21512;&#20013;&#24515;&#65292;&#36825;&#20123;&#20449;&#36947;&#30340;&#36895;&#29575;&#21487;&#20197;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#36890;&#20449;&#32422;&#26463;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#26696;&#65292;&#21516;&#26102;&#22312;&#34701;&#21512;&#20013;&#24515;&#26368;&#22823;&#21270;&#25512;&#26029;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#33719;&#24471;&#20102;&#20551;&#23450;&#20102;&#23545;&#24213;&#23618;&#22238;&#24402;&#22120;&#25968;&#25454;&#20998;&#24067;&#30693;&#35782;&#30340;&#26368;&#20248;&#37327;&#21270;&#22120;&#30340;&#24418;&#24335;&#12290;&#22312;&#19968;&#20010;&#23454;&#38469;&#21512;&#29702;&#30340;&#36817;&#20284;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#19968;&#32500;&#25237;&#24433;&#36827;&#34892;&#37327;&#21270;&#24037;&#20316;&#30340;&#20998;&#24067;&#24335;&#21387;&#32553;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02179v1 Announce Type: cross  Abstract: We study the problem of distributed and rate-adaptive feature compression for linear regression. A set of distributed sensors collect disjoint features of regressor data. A fusion center is assumed to contain a pretrained linear regression model, trained on a dataset of the entire uncompressed data. At inference time, the sensors compress their observations and send them to the fusion center through communication-constrained channels, whose rates can change with time. Our goal is to design a feature compression {scheme} that can adapt to the varying communication constraints, while maximizing the inference performance at the fusion center. We first obtain the form of optimal quantizers assuming knowledge of underlying regressor data distribution. Under a practically reasonable approximation, we then propose a distributed compression scheme which works by quantizing a one-dimensional projection of the sensor data. We also propose a simp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#24341;&#23548;&#25193;&#25955;&#31574;&#30053;&#23454;&#29616;2D&#21644;3D&#36335;&#32447;&#35268;&#21010;&#65292;&#25552;&#20379;&#20805;&#20998;&#36828;&#35265;&#24182;&#25351;&#23548;&#20195;&#29702;&#30340;&#25506;&#32034;&#21644;&#30446;&#26631;&#23547;&#25214;&#34892;&#20026;&#65292;&#26080;&#38656;&#19987;&#23478;&#24178;&#39044;</title><link>https://arxiv.org/abs/2404.02176</link><description>&lt;p&gt;
&#36890;&#36807;&#20215;&#20540;&#24341;&#23548;&#25193;&#25955;&#31574;&#30053;&#23454;&#29616;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#22810;&#21151;&#33021;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Versatile Navigation under Partial Observability via Value-guided Diffusion Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#24341;&#23548;&#25193;&#25955;&#31574;&#30053;&#23454;&#29616;2D&#21644;3D&#36335;&#32447;&#35268;&#21010;&#65292;&#25552;&#20379;&#20805;&#20998;&#36828;&#35265;&#24182;&#25351;&#23548;&#20195;&#29702;&#30340;&#25506;&#32034;&#21644;&#30446;&#26631;&#23547;&#25214;&#34892;&#20026;&#65292;&#26080;&#38656;&#19987;&#23478;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#38024;&#23545;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#23548;&#33322;&#36335;&#32447;&#35268;&#21010;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#36335;&#32447;&#35268;&#21010;&#26041;&#27861;&#21487;&#20998;&#20026;&#20256;&#32479;&#33258;&#22238;&#24402;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#21069;&#32773;&#24120;&#24120;&#30001;&#20110;&#20854;&#36817;&#35270;&#29305;&#24615;&#32780;&#22833;&#36133;&#65292;&#32780;&#21518;&#32773;&#35201;&#20040;&#20551;&#35774;&#23436;&#20840;&#21487;&#35266;&#23519;&#24615;&#65292;&#35201;&#20040;&#22312;&#19982;&#26469;&#33258;&#19987;&#23478;&#30340;&#34892;&#20026;&#20811;&#38534;&#30340;&#24378;&#32806;&#21512;&#19979;&#38590;&#20197;&#36866;&#24212;&#38476;&#29983;&#24773;&#24418;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;2D&#21644;3D&#36335;&#32447;&#35268;&#21010;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20215;&#20540;&#24341;&#23548;&#25193;&#25955;&#31574;&#30053;&#39318;&#20808;&#29983;&#25104;&#35745;&#21010;&#20197;&#39044;&#27979;&#21508;&#20010;&#26102;&#38388;&#27493;&#30340;&#21160;&#20316;&#65292;&#20026;&#35268;&#21010;&#25552;&#20379;&#20805;&#20998;&#30340;&#36828;&#35265;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#20855;&#26377;&#29366;&#24577;&#20272;&#35745;&#30340;&#21487;&#24494;&#20998;&#35268;&#21010;&#22120;&#24471;&#20986;&#20540;&#20989;&#25968;&#65292;&#25351;&#23548;&#20195;&#29702;&#30340;&#25506;&#32034;&#21644;&#23547;&#25214;&#30446;&#26631;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#23547;&#27714;&#19987;&#23478;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02176v1 Announce Type: cross  Abstract: Route planning for navigation under partial observability plays a crucial role in modern robotics and autonomous driving. Existing route planning approaches can be categorized into two main classes: traditional autoregressive and diffusion-based methods. The former often fails due to its myopic nature, while the latter either assumes full observability or struggles to adapt to unfamiliar scenarios, due to strong couplings with behavior cloning from experts. To address these deficiencies, we propose a versatile diffusion-based approach for both 2D and 3D route planning under partial observability. Specifically, our value-guided diffusion policy first generates plans to predict actions across various timesteps, providing ample foresight to the planning. It then employs a differentiable planner with state estimations to derive a value function, directing the agent's exploration and goal-seeking behaviors without seeking experts while expl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21338;&#24328;&#35770;&#26041;&#27861;&#25506;&#35752;PinFi&#26426;&#21046;&#21450;&#20854;&#26356;&#24191;&#27867;&#24433;&#21709;&#65292;&#30740;&#31350;&#21457;&#29616;PinFi&#33021;&#22815;&#22312;LPs&#12289;&#21334;&#23478;&#21644;&#20080;&#23478;&#20043;&#38388;&#24314;&#31435;&#21160;&#24577;&#22343;&#34913;&#12290;</title><link>https://arxiv.org/abs/2404.02174</link><description>&lt;p&gt;
&#35802;&#23454;&#30340;PinFi&#31995;&#32479;&#20013;&#22359;&#22870;&#21169;&#30340;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Bounds of Block Rewards in Honest PinFi Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02174
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#26041;&#27861;&#25506;&#35752;PinFi&#26426;&#21046;&#21450;&#20854;&#26356;&#24191;&#27867;&#24433;&#21709;&#65292;&#30740;&#31350;&#21457;&#29616;PinFi&#33021;&#22815;&#22312;LPs&#12289;&#21334;&#23478;&#21644;&#20080;&#23478;&#20043;&#38388;&#24314;&#31435;&#21160;&#24577;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PinFi&#26159;&#19968;&#31867;&#26032;&#22411;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#23450;&#20215;&#36880;&#28176;&#36140;&#20540;&#30340;&#36164;&#20135;&#65292;&#20854;&#20215;&#20540;&#38543;&#30528;&#26102;&#38388;&#33258;&#28982;&#19979;&#38477;&#12290;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#65288;LPs&#65289;&#22312;&#21327;&#35758;&#21151;&#33021;&#21644;&#24066;&#22330;&#25928;&#29575;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#21327;&#35758;&#20013;&#30340;&#20851;&#38190;&#31283;&#23450;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#25361;&#25112;&#65292;&#21363;LPs&#26356;&#20542;&#21521;&#20110;&#22312;&#22806;&#37096;&#24066;&#22330;&#20986;&#21806;&#32780;&#38750;&#21442;&#19982;&#21327;&#35758;&#65307;&#22312;PinFi&#31995;&#32479;&#20869;&#20986;&#21806;&#32780;&#38750;&#20316;&#20026;LPs&#36827;&#34892;&#36129;&#29486;&#65307;&#20197;&#21450;LPs&#19981;&#24895;&#22312;&#21327;&#35758;&#20869;&#20986;&#21806;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#25506;&#35752;PinFi&#30340;&#26426;&#21046;&#21450;&#20854;&#26356;&#24191;&#27867;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21508;&#31181;&#24120;&#35265;&#26465;&#20214;&#19979;&#65292;&#24182;&#19988;&#20551;&#35774;&#21442;&#19982;&#32773;&#35802;&#23454;&#65292;PinFi&#33021;&#22815;&#22312;LPs&#12289;&#21334;&#23478;&#21644;&#20080;&#23478;&#20043;&#38388;&#24314;&#31435;&#21160;&#24577;&#22343;&#34913;&#12290;&#36825;&#31181;&#24179;&#34913;&#26159;&#36890;&#36807;&#31934;&#24515;&#26657;&#20934;&#30340;&#22870;&#21169;&#26426;&#21046;&#26469;&#32500;&#25345;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02174v1 Announce Type: cross  Abstract: PinFi is a class of novel protocols for decentralized pricing of dissipative assets, whose value naturally declines over time. Central to the protocol's functionality and its market efficiency is the role of liquidity providers (LPs). This study addresses critical stability and sustainability challenges within the protocol, namely: the propensity of LPs to prefer selling in external markets over participation in the protocol; a similar inclination towards selling within the PinFi system rather than contributing as LPs; and a scenario where LPs are disinclined to sell within the protocol. Employing a game-theoretic approach, we explore PinFi's mechanisms and its broader ramifications. Our findings reveal that, under a variety of common conditions and with an assumption of participant integrity, PinFi is capable of fostering a dynamic equilibrium among LPs, sellers, and buyers. This balance is maintained through a carefully calibrated ra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;CLIP&#24341;&#23548;&#26041;&#27861;RAVE&#65292;&#36890;&#36807;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#24182;&#25552;&#39640;&#20102;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01889</link><description>&lt;p&gt;
RAVE: CLIP&#24341;&#23548;&#30340;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;CLIP&#24341;&#23548;&#26041;&#27861;RAVE&#65292;&#36890;&#36807;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#24182;&#25552;&#39640;&#20102;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#21453;&#24046;&#24322;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#25351;&#23548;&#36827;&#34892;&#20102;&#26032;&#39062;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#26368;&#20808;&#36827;&#30340;CLIP-LIT&#26041;&#27861;&#22522;&#30784;&#20043;&#19978;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32422;&#26463;&#22312;CLIP&#23884;&#20837;&#31354;&#38388;&#20013;&#19968;&#20010;&#25552;&#31034;&#23545;&#20043;&#38388;&#30340;&#25991;&#26412;-&#22270;&#20687;&#30456;&#20284;&#24615;&#26469;&#23398;&#20064;&#19968;&#20010;&#25552;&#31034;&#23545;&#65288;&#36127;/&#27491;&#26679;&#26412;&#65289;&#21644;&#30456;&#24212;&#22270;&#20687;&#65288;&#32972;&#20809;&#22270;&#20687;/&#20809;&#29031;&#33391;&#22909;&#30340;&#22270;&#20687;&#65289;&#12290;&#23398;&#20064;&#30340;&#25552;&#31034;&#28982;&#21518;&#25351;&#23548;&#22270;&#20687;&#22686;&#24378;&#32593;&#32476;&#12290;&#22522;&#20110;CLIP-LIT&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;CLIP&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#35843;&#25972;&#25552;&#31034;&#32780;&#19981;&#25439;&#22833;&#36136;&#37327;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35843;&#25972;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#21152;&#24555;&#35757;&#32451;&#24182;&#28508;&#22312;&#22320;&#23454;&#29616;&#20351;&#29992;&#27809;&#26377;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#20854;&#20182;&#32534;&#30721;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01889v1 Announce Type: cross  Abstract: In this paper we propose a novel modification of Contrastive Language-Image Pre-Training (CLIP) guidance for the task of unsupervised backlit image enhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which learns a prompt pair by constraining the text-image similarity between a prompt (negative/positive sample) and a corresponding image (backlit image/well-lit image) in the CLIP embedding space. Learned prompts then guide an image enhancement network. Based on the CLIP-LIT framework, we propose two novel methods for CLIP guidance. First, we show that instead of tuning prompts in the space of text embeddings, it is possible to directly tune their embeddings in the latent space without any loss in quality. This accelerates training and potentially enables the use of additional encoders that do not have a text encoder. Second, we propose a novel approach that does not require any prompt tuning. Instead, based on CLIP e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GFaiR&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#35777;&#26126;&#25554;&#20837;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#20102;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;</title><link>https://arxiv.org/abs/2404.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#24402;&#32467;&#21453;&#39539;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#36890;&#29992;&#19988;&#21487;&#38752;&#30340;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01677
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GFaiR&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#35777;&#26126;&#25554;&#20837;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#20102;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#36827;&#34892;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#26377;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#21487;&#27867;&#21270;&#19988;&#21487;&#38752;&#25512;&#29702;&#22120;&#65288;GFaiR&#65289;&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01677v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks. However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language. This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue. As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability. To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation. Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system's completeness can be improved by introducing resolution refutation. Experimental results demonstrate that our system o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#24320;&#21457;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#38477;&#20302;&#28216;&#25103;&#24320;&#21457;&#30340;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00276</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Instruction-Driven Game Engines on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00276
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#24320;&#21457;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#38477;&#20302;&#28216;&#25103;&#24320;&#21457;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instruction-Driven Game Engine (IDGE) &#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36981;&#24490;&#33258;&#30001;&#24418;&#24335;&#30340;&#28216;&#25103;&#35268;&#21017;&#24182;&#33258;&#21160;&#29983;&#25104;&#28216;&#25103;&#36807;&#31243;&#26469;&#20351;&#28216;&#25103;&#24320;&#21457;&#27665;&#20027;&#21270;&#12290;IDGE&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#21457;&#20986;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#28216;&#25103;&#24320;&#21457;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#23558;IDGE&#30340;&#23398;&#20064;&#36807;&#31243;&#35270;&#20026;&#19979;&#19968;&#20010;&#29366;&#24577;&#39044;&#27979;&#20219;&#21153;&#65292;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#39044;&#27979;&#29609;&#23478;&#34892;&#21160;&#32473;&#20986;&#30340;&#28216;&#25103;&#29366;&#24577;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28216;&#25103;&#29366;&#24577;&#30340;&#35745;&#31639;&#24517;&#39035;&#20934;&#30830;&#65307;&#21542;&#21017;&#65292;&#36731;&#24494;&#30340;&#38169;&#35823;&#21487;&#33021;&#20250;&#30772;&#22351;&#28216;&#25103;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;&#35838;&#31243;&#26041;&#24335;&#35757;&#32451;IDGE&#65292;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#23545;&#22797;&#26434;&#22330;&#26223;&#30340;&#25509;&#35302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00276v1 Announce Type: new  Abstract: The Instruction-Driven Game Engine (IDGE) project aims to democratize game development by enabling a large language model (LLM) to follow free-form game rules and autonomously generate game-play processes. The IDGE allows users to create games by issuing simple natural language instructions, which significantly lowers the barrier for game development. We approach the learning process for IDGEs as a Next State Prediction task, wherein the model autoregressively predicts in-game states given player actions. It is a challenging task because the computation of in-game states must be precise; otherwise, slight errors could disrupt the game-play. To address this, we train the IDGE in a curriculum manner that progressively increases the model's exposure to complex scenarios.   Our initial progress lies in developing an IDGE for Poker, a universally cherished card game. The engine we've designed not only supports a wide range of poker variants b
&lt;/p&gt;</description></item><item><title>InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2404.00228</link><description>&lt;p&gt;
InfLoRA&#65306;&#26080;&#24178;&#25200;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00228
&lt;/p&gt;
&lt;p&gt;
InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#35201;&#27714;&#27169;&#22411;&#20381;&#27425;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#12290;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#24212;&#20855;&#22791;&#22312;&#26087;&#20219;&#21153;&#19978;&#32500;&#25345;&#24615;&#33021;&#65288;&#31283;&#23450;&#24615;&#65289;&#21644;&#19981;&#26029;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#65288;&#21487;&#22609;&#24615;&#65289;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#29616;&#26377;&#22522;&#20110;PEFT&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#38750;PEFT&#26041;&#27861;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#22914;&#20309;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00228v1 Announce Type: cross  Abstract: Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.20208</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#39044;&#27979;&#34920;&#26684;&#20219;&#21153;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#65292;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#32570;&#22833;&#20540;&#22635;&#20805;&#31561;&#39044;&#27979;&#20219;&#21153;&#26159;&#19982;&#34920;&#26684;&#25968;&#25454;&#30456;&#20851;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#20219;&#21153;&#12290;&#23613;&#31649;LLMs&#25797;&#38271;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#65292;&#20294;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25910;&#38598;&#24102;&#26377;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;Llama-2&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;-shot&#39044;&#27979;&#12289;&#23569;-shot&#39044;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#29983;&#25104;&#21644;&#21033;&#29992;&#21512;&#25104;&#22270;&#20687;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#26725;&#25509;&#36801;&#31227;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#20197;&#35299;&#20915;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.19866</link><description>&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#26377;&#29992;&#21527;&#65311;&#20851;&#20110;&#25968;&#25454;&#29983;&#25104;&#12289;&#25968;&#37327;&#21644;&#21033;&#29992;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Is Synthetic Image Useful for Transfer Learning? An Investigation into Data Generation, Volume, and Utilization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19866
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#29983;&#25104;&#21644;&#21033;&#29992;&#21512;&#25104;&#22270;&#20687;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#26725;&#25509;&#36801;&#31227;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#20197;&#35299;&#20915;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#29983;&#25104;&#20195;&#34920;&#20102;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#29305;&#21035;&#26159;&#22312;&#36801;&#31227;&#23398;&#20064;&#39046;&#22495;&#65292;&#22240;&#20026;&#22312;&#29305;&#23450;&#39046;&#22495;&#33719;&#21462;&#30495;&#23454;&#22270;&#20687;&#21487;&#33021;&#20250;&#30001;&#20110;&#38544;&#31169;&#21644;&#30693;&#35782;&#20135;&#26435;&#32771;&#34385;&#32780;&#21464;&#24471;&#20195;&#20215;&#39640;&#26114;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#33719;&#21462;&#30340;&#21512;&#25104;&#22270;&#20687;&#30340;&#29983;&#25104;&#21644;&#21033;&#29992;&#65292;&#20197;&#20419;&#36827;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#12290;&#23613;&#31649;&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;&#39640;&#24230;&#30340;&#35270;&#35273;&#36924;&#30495;&#24230;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23558;&#36825;&#20123;&#22270;&#20687;&#22825;&#30495;&#22320;&#32435;&#20837;&#29616;&#26377;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#22987;&#32456;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#20110;&#21512;&#25104;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#22266;&#26377;&#30340;&#20998;&#24067;&#24046;&#24322;&#25152;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26725;&#25509;&#36801;&#31227;&#8221;&#30340;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#21033;&#29992;&#21512;&#25104;&#22270;&#20687;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#25552;&#39640;&#20854;&#21487;&#36801;&#31227;&#24615;&#65292;&#38543;&#21518;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19866v1 Announce Type: cross  Abstract: Synthetic image data generation represents a promising avenue for training deep learning models, particularly in the realm of transfer learning, where obtaining real images within a specific domain can be prohibitively expensive due to privacy and intellectual property considerations. This work delves into the generation and utilization of synthetic images derived from text-to-image generative models in facilitating transfer learning paradigms. Despite the high visual fidelity of the generated images, we observe that their naive incorporation into existing real-image datasets does not consistently enhance model performance due to the inherent distribution gap between synthetic and real images. To address this issue, we introduce a novel two-stage framework called bridged transfer, which initially employs synthetic images for fine-tuning a pre-trained model to improve its transferability and subsequently uses real data for rapid adaptat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Gegenbauer-based graph convolutional (GegenConv)&#31639;&#23376;&#65292;&#29992;&#20110;&#25552;&#39640;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.19800</link><description>&lt;p&gt;
Gegenbauer&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19800
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Gegenbauer-based graph convolutional (GegenConv)&#31639;&#23376;&#65292;&#29992;&#20110;&#25552;&#39640;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26500;&#26102;&#21464;&#22270;&#20449;&#21495;&#65288;&#25110;&#22270;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20174;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#21040;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#20934;&#30830;&#25429;&#25417;&#36825;&#20123;&#20449;&#21495;&#22266;&#26377;&#30340;&#26102;&#31354;&#20449;&#24687;&#23545;&#20110;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#26102;&#38388;&#24046;&#30340;&#24179;&#28369;&#24615;&#20551;&#35774;&#21644;&#31616;&#21333;&#30340;&#20984;&#20248;&#21270;&#25216;&#26415;&#65292;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23398;&#20064;&#27169;&#22359;&#20197;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;Gegenbauer&#22810;&#39033;&#24335;&#29702;&#35770;&#30340;Gegenbauer-based graph convolutional&#65288;GegenConv&#65289;&#31639;&#23376;&#65292;&#36825;&#26159;&#20256;&#32479;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#30340;&#25512; generalization&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19800v1 Announce Type: cross  Abstract: Reconstructing time-varying graph signals (or graph time-series imputation) is a critical problem in machine learning and signal processing with broad applications, ranging from missing data imputation in sensor networks to time-series forecasting. Accurately capturing the spatio-temporal information inherent in these signals is crucial for effectively addressing these tasks. However, existing approaches relying on smoothness assumptions of temporal differences and simple convex optimization techniques have inherent limitations. To address these challenges, we propose a novel approach that incorporates a learning module to enhance the accuracy of the downstream task. To this end, we introduce the Gegenbauer-based graph convolutional (GegenConv) operator, which is a generalization of the conventional Chebyshev graph convolution by leveraging the theory of Gegenbauer polynomials. By deviating from traditional convex problems, we expand t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18742</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Learning Dynamics of Alignment with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#27169;&#22411;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#34429;&#28982;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29702;&#35770;&#19978;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#27491;&#24335;&#23637;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36824;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#29616;&#35937;&#65292;&#21363;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24403;&#20195;LLMs&#21644;&#23545;&#40784;&#20219;&#21153;&#19978;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24378;&#21270;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23545;&#40784;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#20813;&#36131;&#22768;&#26126;&#65306;&#26412;&#25991;&#21253;&#21547;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32508;&#21512;&#21033;&#29992;NFT&#20132;&#26131;&#35760;&#24405;&#21644;&#22806;&#37096;&#39033;&#30446;&#29305;&#24449;&#31561;&#22810;&#31181;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#25968;&#25454;&#39640;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#24182;&#21033;&#29992;&#36229;&#20986;&#29992;&#25143;-&#39033;&#30446;&#20114;&#21160;&#30340;&#36755;&#20837;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18305</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#39033;&#30446;&#29305;&#24449;&#30340;NFT&#21487;&#25910;&#34255;&#21697;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Recommender System for NFT Collectibles with Item Feature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32508;&#21512;&#21033;&#29992;NFT&#20132;&#26131;&#35760;&#24405;&#21644;&#22806;&#37096;&#39033;&#30446;&#29305;&#24449;&#31561;&#22810;&#31181;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#25968;&#25454;&#39640;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#24182;&#21033;&#29992;&#36229;&#20986;&#29992;&#25143;-&#39033;&#30446;&#20114;&#21160;&#30340;&#36755;&#20837;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24050;&#34987;&#31215;&#26497;&#30740;&#31350;&#24182;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#20197;&#35299;&#20915;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20851;&#20110;&#30005;&#24433;&#12289;&#38899;&#20048;&#21644;&#30005;&#23376;&#21830;&#21153;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#20294;&#30456;&#27604;&#20043;&#19979;&#65292;&#23613;&#31649;NFT&#24066;&#22330;&#25345;&#32493;&#22686;&#38271;&#65292;&#23545;&#20110;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#21364;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#21508;&#31181;&#25968;&#25454;&#28304;&#65292;&#20174;NFT&#20132;&#26131;&#35760;&#24405;&#21040;&#22806;&#37096;&#39033;&#30446;&#29305;&#24449;&#65292;&#29983;&#25104;&#31526;&#21512;&#20010;&#20154;&#20559;&#22909;&#30340;&#31934;&#30830;&#25512;&#33616;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#27599;&#20010;&#39033;&#30446;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;&#33410;&#28857;&#29305;&#24449;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#30340;&#33410;&#28857;&#65288;&#39033;&#30446;&#65289;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36229;&#20986;&#29992;&#25143;-&#39033;&#30446;&#20114;&#21160;&#30340;&#36755;&#20837;&#65292;&#22914;&#22270;&#20687;&#29305;&#24449;&#12289;&#25991;&#26412;&#29305;&#24449;&#21644;&#20215;&#26684;&#29305;&#24449;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18305v1 Announce Type: cross  Abstract: Recommender systems have been actively studied and applied in various domains to deal with information overload. Although there are numerous studies on recommender systems for movies, music, and e-commerce, comparatively less attention has been paid to the recommender system for NFTs despite the continuous growth of the NFT market. This paper presents a recommender system for NFTs that utilizes a variety of data sources, from NFT transaction records to external item features, to generate precise recommendations that cater to individual preferences. We develop a data-efficient graph-based recommender system to efficiently capture the complex relationship between each item and users and generate node(item) embeddings which incorporate both node feature information and graph structure. Furthermore, we exploit inputs beyond user-item interactions, such as image feature, text feature, and price feature. Numerical experiments verify the perf
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#35299;&#20915;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#31215;&#21387;&#38382;&#39064;&#20855;&#26377;&#25112;&#30053;&#24212;&#23545;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#22312;&#39564;&#35777;&#21644;&#22686;&#24378;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16424</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#20026;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#25351;&#23450;LCSH&#20027;&#39064;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#35299;&#20915;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#31215;&#21387;&#38382;&#39064;&#20855;&#26377;&#25112;&#30053;&#24212;&#23545;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#22312;&#39564;&#35777;&#21644;&#22686;&#24378;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#65288;LCSH&#65289;&#30340;&#28508;&#21147;&#12290;&#20316;&#32773;&#20351;&#29992;ChatGPT&#26681;&#25454;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#30340;&#26631;&#39064;&#21644;&#25688;&#35201;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#19968;&#20123;&#29983;&#25104;&#30340;&#20027;&#39064;&#26631;&#22836;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23384;&#22312;&#29305;&#23450;&#24615;&#21644;&#35814;&#23613;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#20316;&#20026;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#30340;&#25112;&#30053;&#24615;&#24212;&#23545;&#25514;&#26045;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#24555;&#36895;&#29983;&#25104;LCSH&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#26159;&#39564;&#35777;&#21644;&#22686;&#24378;LLMs&#29983;&#25104;&#30340;LCSH&#30340;&#26377;&#25928;&#24615;&#12289;&#35814;&#23613;&#24615;&#21644;&#29305;&#23450;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16424v1 Announce Type: new  Abstract: This study delves into the potential use of Large Language Models (LLMs) for generating Library of Congress Subject Headings (LCSH). The authors employed ChatGPT to generate subject headings for electronic theses and dissertations (ETDs) based on their titles and summaries. The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness. The study showcases that LLMs can serve as a strategic response to the backlog of items awaiting cataloging in academic libraries, while also offering a cost-effective approach for promptly generating LCSH. Nonetheless, human catalogers remain essential for verifying and enhancing the validity, exhaustiveness, and specificity of LCSH generated by LLMs.
&lt;/p&gt;</description></item><item><title>GPT-4V&#22312;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#26816;&#27979;&#19978;&#23578;&#26410;&#20934;&#22791;&#22909;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#35786;&#26029;&#29992;&#36884;</title><link>https://arxiv.org/abs/2403.15528</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#35780;&#20272;GPT-4&#22312;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#20013;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15528
&lt;/p&gt;
&lt;p&gt;
GPT-4V&#22312;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#26816;&#27979;&#19978;&#23578;&#26410;&#20934;&#22791;&#22909;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#35786;&#26029;&#29992;&#36884;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4V&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#35013;&#22791;&#26377;&#35270;&#35273;&#35782;&#21035;&#21151;&#33021;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;100&#24352;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;GPT-4V&#36824;&#19981;&#36866;&#29992;&#20110;&#35299;&#37322;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#23454;&#38469;&#35786;&#26029;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15528v1 Announce Type: cross  Abstract: The study examines the application of GPT-4V, a multi-modal large language model equipped with visual recognition, in detecting radiological findings from a set of 100 chest radiographs and suggests that GPT-4V is currently not ready for real-world diagnostic usage in interpreting chest radiographs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#65292;&#36890;&#36807;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#65292;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15027</link><description>&lt;p&gt;
&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Grey-informed neural network for time-series forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#65292;&#36890;&#36807;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#65292;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#34987;&#35270;&#20026;&#40657;&#30418;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24320;&#21457;&#12290;&#22240;&#27492;&#65292;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#26500;&#24314;&#36866;&#24403;&#30340;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24314;&#35758;&#23454;&#26045;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#12290;GINN &#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#28784;&#33394;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#20351;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#24050;&#34987;&#35266;&#23519;&#21040;&#33021;&#22815;&#25581;&#31034;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#24182;&#22522;&#20110;&#32463;&#39564;&#25968;&#25454;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15027v1 Announce Type: cross  Abstract: Neural network models have shown outstanding performance and successful resolutions to complex problems in various fields. However, the majority of these models are viewed as black-box, requiring a significant amount of data for development. Consequently, in situations with limited data, constructing appropriate models becomes challenging due to the lack of transparency and scarcity of data. To tackle these challenges, this study suggests the implementation of a grey-informed neural network (GINN). The GINN ensures that the output of the neural network follows the differential equation model of the grey system, improving interpretability. Moreover, incorporating prior knowledge from grey system theory enables traditional neural networks to effectively handle small data samples. Our proposed model has been observed to uncover underlying patterns in the real world and produce reliable forecasts based on empirical data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11322</link><description>&lt;p&gt;
&#20351;&#29992;StateFlow&#22686;&#24378;LLM&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#36890;&#36807;&#29366;&#24577;&#39537;&#21160;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#36235;&#21183;&#26085;&#30410;&#26126;&#26174;&#65292;&#20363;&#22914;&#38656;&#35201;&#19968;&#31995;&#21015;&#25805;&#20316;&#21644;&#19982;&#24037;&#20855;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;StateFlow&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#27714;&#35299;&#33539;&#24335;&#65292;&#23558;&#30001;LLM&#25903;&#25345;&#30340;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#12290;&#36890;&#36807;&#27491;&#30830;&#26500;&#24314;&#29366;&#24577;&#21644;&#23450;&#20041;&#29366;&#24577;&#36716;&#25442;&#65292;StateFlow&#30830;&#23450;&#20102;&#20219;&#21153;&#27714;&#35299;&#30340;&#36827;&#23637;&#65292;&#30830;&#20445;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;LLM&#22312;&#25972;&#20010;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#30340;&#21709;&#24212;&#12290;&#22312;&#27599;&#20010;&#29366;&#24577;&#20013;&#65292;StateFlow&#20801;&#35768;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#19981;&#20165;&#21253;&#25324;&#26681;&#25454;&#29305;&#23450;&#25552;&#31034;&#25351;&#23548;&#29983;&#25104;LLM&#21709;&#24212;&#65292;&#36824;&#21253;&#25324;&#26681;&#25454;&#38656;&#35201;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#12290;&#29366;&#24577;&#36716;&#25442;&#30001;LLM&#20570;&#20986;&#30340;&#29305;&#23450;&#35268;&#21017;&#25110;&#20915;&#31574;&#25511;&#21046;&#65292;&#20801;&#35768;&#36890;&#36807;&#20219;&#21153;&#30340;&#39044;&#23450;&#20041;StateFlow&#27169;&#22411;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;FELT&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#26469;&#35299;&#20915;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05839</link><description>&lt;p&gt;
&#38271;&#26399;&#24103;&#20107;&#20214;&#35270;&#35273;&#36319;&#36394;&#65306;&#22522;&#20934;&#25968;&#25454;&#38598;&#19982;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;FELT&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#26469;&#35299;&#20915;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#20107;&#20214;/&#24103;&#20107;&#20214;&#30340;&#36319;&#36394;&#22120;&#22312;&#30701;&#26399;&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#30495;&#23454;&#22330;&#26223;&#30340;&#36319;&#36394;&#28041;&#21450;&#38271;&#26399;&#36319;&#36394;&#65292;&#29616;&#26377;&#36319;&#36394;&#31639;&#27861;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;FELT&#12290;&#23427;&#21253;&#21547;742&#20010;&#35270;&#39057;&#21644;1,594,474&#20010;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#23545;&#65292;&#24182;&#24050;&#25104;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#24103;&#20107;&#20214;&#36319;&#36394;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20197;&#20379;&#26410;&#26469;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#30001;&#20110;&#25361;&#25112;&#22240;&#32032;&#30340;&#24433;&#21709;&#21644;&#31354;&#38388;&#31232;&#30095;&#30340;&#20107;&#20214;&#27969;&#32780;&#33258;&#28982;&#19981;&#23436;&#25972;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#20316;&#20026;&#32479;&#19968;&#39592;&#24178;&#65292;&#36890;&#36807;&#23558;&#29616;&#20195;Hopfield&#23618;&#24341;&#20837;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05839v1 Announce Type: cross  Abstract: Current event-/frame-event based trackers undergo evaluation on short-term tracking datasets, however, the tracking of real-world scenarios involves long-term tracking, and the performance of existing tracking algorithms in these scenarios remains unclear. In this paper, we first propose a new long-term and large-scale frame-event single object tracking dataset, termed FELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs and has become the largest frame-event tracking dataset to date. We re-train and evaluate 15 baseline trackers on our dataset for future works to compare. More importantly, we find that the RGB frames and event streams are naturally incomplete due to the influence of challenging factors and spatially sparse event flow. In response to this, we propose a novel associative memory Transformer network as a unified backbone by introducing modern Hopfield layers into multi-head self-attention blocks to
&lt;/p&gt;</description></item><item><title>MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02694</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24863;&#30693;&#35821;&#20041;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Semantic Cache for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02694
&lt;/p&gt;
&lt;p&gt;
MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;Google Bard&#12289;Claude&#21644;Llama 2&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25628;&#32034;&#24341;&#25806;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36896;&#25104;&#20102;&#24322;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MeanCache&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#20197;&#30830;&#23450;&#32531;&#23384;&#21629;&#20013;&#25110;&#26410;&#21629;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316; via Optimal Transport&#65288;FedOTP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#38024;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00041</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Global and Local Prompts Cooperation via Optimal Transport for Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316; via Optimal Transport&#65288;FedOTP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#38024;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#23398;&#20064;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#28789;&#27963;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#23558;&#36825;&#31181;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25972;&#21512;&#21040;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#24182;&#20419;&#36827;&#23545;&#25968;&#25454;&#19981;&#36275;&#30340;&#23616;&#37096;&#35757;&#32451;&#12290;&#20026;&#20102;&#24212;&#23545;&#24403;&#21069;&#32852;&#37030;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#31995;&#32479;&#21270;&#35299;&#20915;&#20005;&#37325;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#21363;&#28041;&#21450;&#26631;&#31614;&#21644;&#29305;&#24449;&#36716;&#31227;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316;&#65288;FedOTP&#65289;&#65292;&#23427;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#22522;&#30784;&#19978;&#25429;&#25417;&#19981;&#21516;&#30340;&#31867;&#21035;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#25552;&#31034;&#26469;&#25552;&#21462;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20849;&#35782;&#30693;&#35782;&#65292;&#36824;&#23398;&#20064;&#19968;&#20010;&#26412;&#22320;&#25552;&#31034;&#26469;&#25429;&#33719;&#29305;&#23450;&#23458;&#25143;&#31471;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00041v1 Announce Type: cross  Abstract: Prompt learning in pretrained visual-language models has shown remarkable flexibility across various downstream tasks. Leveraging its inherent lightweight nature, recent research attempted to integrate the powerful pretrained models into federated learning frameworks to simultaneously reduce communication costs and promote local training on insufficient data. Despite these efforts, current federated prompt learning methods lack specialized designs to systematically address severe data heterogeneities, e.g., data distribution with both label and feature shifts involved. To address this challenge, we present Federated Prompts Cooperation via Optimal Transport (FedOTP), which introduces efficient collaborative prompt learning strategies to capture diverse category traits on a per-client basis. Specifically, for each client, we learn a global prompt to extract consensus knowledge among clients, and a local prompt to capture client-specific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.17128</link><description>&lt;p&gt;
OSCaR:&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
OSCaR: Object State Captioning and State Change Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38754;&#21521;&#20154;&#31867;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#35270;&#35282;&#65292;&#26234;&#33021;&#27169;&#22411;&#25512;&#26029;&#21644;&#29702;&#35299;&#23545;&#35937;&#29366;&#24577;&#30340;&#21464;&#21270;&#33021;&#21147;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#25551;&#36848;&#22797;&#26434;&#30340;&#35270;&#35273;&#29615;&#22659;&#65292;&#35782;&#21035;&#27963;&#36291;&#23545;&#35937;&#65292;&#20197;&#21450;&#36890;&#36807;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#30340;&#21464;&#21270;&#12290;&#20256;&#32479;&#26041;&#27861;&#23558;&#23545;&#35937;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#26816;&#27979;&#36827;&#34892;&#38548;&#31163;&#65292;&#25552;&#20379;&#20102;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#26377;&#38480;&#35270;&#22270;&#12290;&#27492;&#22806;&#65292;&#20381;&#36182;&#20110;&#19968;&#23567;&#22871;&#31526;&#21495;&#21270;&#35789;&#27719;&#26469;&#34920;&#31034;&#21464;&#21270;&#38480;&#21046;&#20102;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;&#65288;OSCaR&#65289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;OSCaR&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#20027;&#35266;&#35270;&#35282;&#35270;&#39057;&#38598;&#21512;&#30340;14,084&#20010;&#24102;&#27880;&#37322;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#36817;1,000&#20010;&#29420;&#29305;&#23545;&#35937;&#12290;&#23427;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 Announce Type: cross  Abstract: The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of the language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large langua
&lt;/p&gt;</description></item><item><title>Text2Pic Swift&#26694;&#26550;&#38024;&#23545;&#22823;&#35268;&#27169;&#24211;&#20013;&#25991;&#26412;&#25551;&#36848;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#31574;&#30053;&#35299;&#20915;&#20102;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15276</link><description>&lt;p&gt;
Text2Pic Swift&#65306;&#22686;&#24378;&#22823;&#35268;&#27169;&#24211;&#20013;&#38271;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale Libraries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15276
&lt;/p&gt;
&lt;p&gt;
Text2Pic Swift&#26694;&#26550;&#38024;&#23545;&#22823;&#35268;&#27169;&#24211;&#20013;&#25991;&#26412;&#25551;&#36848;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#31574;&#30053;&#35299;&#20915;&#20102;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15276v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#26816;&#32034;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#25968;&#23383;&#22270;&#20070;&#39302;&#12289;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#21644;&#22810;&#23186;&#20307;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#26597;&#35810;&#26469;&#25628;&#32034;&#22270;&#20687;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#27169;&#31946;&#30340;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#26174;&#30528;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#29983;&#25104;&#21487;&#27880;&#20837;&#30340;&#23884;&#20837;&#25152;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Text2Pic Swift&#26694;&#26550;&#65292;&#19987;&#20026;&#22312;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#26377;&#25928;&#21644;&#31283;&#20581;&#22320;&#26816;&#32034;&#19982;&#24191;&#27867;&#25991;&#26412;&#25551;&#36848;&#23545;&#24212;&#30340;&#22270;&#20687;&#32780;&#35774;&#35745;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#21021;&#22987;&#22522;&#20110;&#23454;&#20307;&#30340;&#25490;&#24207;&#65288;ER&#65289;&#38454;&#27573;&#36890;&#36807;&#22810;&#26597;&#35810;&#23545;&#22810;&#30446;&#26631;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#22266;&#26377;&#30340;&#27495;&#20041;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#21487;&#33021;&#30340;&#20505;&#36873;&#39033;&#65292;&#20197;&#20415;&#36827;&#34892;&#21518;&#32493;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15276v1 Announce Type: cross  Abstract: Text-to-image retrieval plays a crucial role across various applications, including digital libraries, e-commerce platforms, and multimedia databases, by enabling the search for images using text queries. Despite the advancements in Multimodal Large Language Models (MLLMs), which offer leading-edge performance, their applicability in large-scale, varied, and ambiguous retrieval scenarios is constrained by significant computational demands and the generation of injective embeddings. This paper introduces the Text2Pic Swift framework, tailored for efficient and robust retrieval of images corresponding to extensive textual descriptions in sizable datasets. The framework employs a two-tier approach: the initial Entity-based Ranking (ER) stage addresses the ambiguity inherent in lengthy text queries through a multiple-queries-to-multiple-targets strategy, effectively narrowing down potential candidates for subsequent analysis. Following thi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#30340;&#26080;&#32541;&#36866;&#24212;</title><link>https://arxiv.org/abs/2402.14505</link><description>&lt;p&gt;
&#20026;&#23454;&#29616;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26080;&#32541;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#30340;&#26080;&#32541;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#29992;&#36890;&#29992;&#30340;&#35270;&#35273;&#23398;&#20064;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#21487;&#20197;&#20026;&#21508;&#31181;&#35270;&#35273;&#24863;&#30693;&#38382;&#39064;&#25552;&#20379;&#26377;&#29992;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#23581;&#35797;&#21033;&#29992;&#22312;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#65288;VPR&#65289;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#30001;&#20110;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;VPR&#20219;&#21153;&#20043;&#38388;&#22312;&#35757;&#32451;&#30446;&#26631;&#21644;&#25968;&#25454;&#26041;&#38754;&#30340;&#22266;&#26377;&#24046;&#24322;&#65292;&#22914;&#20309;&#24357;&#21512;&#24046;&#36317;&#24182;&#20805;&#20998;&#21457;&#25381;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;VPR&#20013;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;VPR&#30340;&#26080;&#32541;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28151;&#21512;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20840;&#23616;&#21644;&#23616;&#37096;&#36866;&#24212;&#65292;&#20174;&#32780;&#33719;&#24471;&#26082;&#20851;&#27880;&#26174;&#33879;&#22320;&#26631;&#29992;&#20110;&#21306;&#20998;&#22320;&#28857;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14505v1 Announce Type: cross  Abstract: Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to gu
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.09664</link><description>&lt;p&gt;
CodeMind:&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CodeMind: A Framework to Challenge Large Language Models for Code Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09664
&lt;/p&gt;
&lt;p&gt;
CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#30721;&#21512;&#25104;&#33021;&#21147;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#35780;&#20272;&#25110;&#20419;&#36827;&#20855;&#26377;&#25968;&#25454;&#27844;&#28431;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeMind&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;CodeMind&#30446;&#21069;&#25903;&#25345;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#65306;&#29420;&#31435;&#25191;&#34892;&#25512;&#29702;&#65288;IER&#65289;&#12289;&#20381;&#36182;&#25191;&#34892;&#25512;&#29702;&#65288;DER&#65289;&#21644;&#35268;&#33539;&#25512;&#29702;&#65288;SR&#65289;&#12290;&#21069;&#20004;&#32773;&#35780;&#20272;&#27169;&#22411;&#20197;&#39044;&#27979;&#20219;&#24847;&#20195;&#30721;&#30340;&#25191;&#34892;&#36755;&#20986;&#65292;&#25110;&#32773;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#21512;&#25104;&#30340;&#20195;&#30721;&#12290;&#31532;&#19977;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#23454;&#29616;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;CodeMind&#23545;&#20004;&#31181;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20116;&#20010;&#22522;&#20934;&#19979;&#30340;&#20061;&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06530</link><description>&lt;p&gt;
&#25913;&#36827;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#22312;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#65288;MI&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#39044;&#38450;&#36827;&#19968;&#27493;&#24515;&#32908;&#25439;&#20260;&#38750;&#24120;&#37325;&#35201;&#65292;MI&#26159;&#30001;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#24341;&#36215;&#30340;&#19968;&#31181;&#20005;&#37325;&#30142;&#30149;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#65288;OCC&#65289;&#31639;&#27861;&#36827;&#34892;&#26089;&#26399;MI&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#22810;&#27169;&#24577;&#23376;&#31354;&#38388;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#30340;&#26032;&#26041;&#27861;&#20811;&#26381;&#20102;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#25552;&#20986;&#30340;&#25216;&#26415;&#28041;&#21450;&#19968;&#31181;&#29305;&#27530;&#30340;MI&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#22797;&#21512;&#26680;&#22312;&#38750;&#32447;&#24615;&#25237;&#24433;&#25216;&#24039;&#20013;&#34701;&#21512;&#39640;&#26031;&#21644;&#25289;&#26222;&#25289;&#26031;sigmoid&#20989;&#25968;&#65292;&#23558;&#22810;&#35270;&#22270;&#36229;&#22768;&#24515;&#21160;&#22270;&#32467;&#21512;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#35843;&#25972;&#25237;&#24433;&#30697;&#38453;&#30340;&#26368;&#22823;&#21270;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#25237;&#24433;&#30697;&#38453;&#26356;&#26032;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#20248;&#21270;&#30340;&#20302;&#32500;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;MI&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early detection of myocardial infarction (MI), a critical condition arising from coronary artery disease (CAD), is vital to prevent further myocardial damage. This study introduces a novel method for early MI detection using a one-class classification (OCC) algorithm in echocardiography. Our study overcomes the challenge of limited echocardiography data availability by adopting a novel approach based on Multi-modal Subspace Support Vector Data Description. The proposed technique involves a specialized MI detection framework employing multi-view echocardiography incorporating a composite kernel in the non-linear projection trick, fusing Gaussian and Laplacian sigmoid functions. Additionally, we enhance the update strategy of the projection matrices by adapting maximization for both or one of the modalities in the optimization process. Our method boosts MI detection capability by efficiently transforming features extracted from echocardiography data into an optimized lower-dimensional su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03678</link><description>&lt;p&gt;
&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20351;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#23398;&#20064;&#22810;&#26679;&#21270;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#12290;&#20026;&#20102;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#65292;&#22914;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL$_f$&#65289;&#20844;&#24335;&#25110;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#65292;&#26469;&#25351;&#23548;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;LSTS&#19981;&#20551;&#35774;&#29615;&#22659;&#21160;&#21147;&#23398;&#25110;&#22870;&#21169;&#26426;&#22120;&#30340;&#20449;&#24687;&#65292;&#24182;&#21160;&#24577;&#37319;&#26679;&#23548;&#33268;&#25104;&#21151;&#30446;&#26631;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#19978;&#35780;&#20272;&#20102;LSTS&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshol
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17377</link><description>&lt;p&gt;
&#26080;&#38480;-gram&#65306;&#23558;&#26080;&#38480;n-gram&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#19975;&#20159;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17377
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;n-gram&#35821;&#35328;&#27169;&#22411;&#36824;&#20855;&#26377;&#30456;&#20851;&#24615;&#21527;&#65311;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#25913;&#36827;&#31070;&#32463;LLM&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#20004;&#20010;&#26041;&#38754;&#23545;n-gram&#27169;&#22411;&#36827;&#34892;&#29616;&#20195;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#31070;&#32463;LLM&#30456;&#21516;&#30340;&#25968;&#25454;&#35268;&#27169;&#35757;&#32451;- 1.4&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26500;&#24314;&#30340;&#26368;&#22823;&#30340;n-gram&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;n-gram&#27169;&#22411;&#20351;&#29992;&#30340;n&#24456;&#23567;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20801;&#35768;n&#21487;&#20197;&#26159;&#20219;&#24847;&#22823;&#30340;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26080;&#38480;-gram LM&#19982;&#22238;&#36864;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21518;&#32512;&#25968;&#32452;&#35745;&#31639;&#26080;&#38480;-gram&#65288;&#20197;&#21450;&#20219;&#24847;n&#30340;n-gram&#65289;&#27010;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;n-gram&#35745;&#25968;&#34920;&#65288;&#36825;&#23558;&#38750;&#24120;&#26114;&#36149;&#65289;&#12290;&#26080;&#38480;-gram&#26694;&#26550;&#21644;infini-gram&#24341;&#25806;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#20154;&#31867;&#20889;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35768;&#22810;&#26032;&#39062;&#21644;&#26377;&#24847;&#24605;&#30340;&#20998;&#26512;&#65306;&#25105;&#20204;&#21457;&#29616;&#26080;&#38480;-gram LM...
&lt;/p&gt;
&lt;p&gt;
Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#26631;&#35760;&#32423;&#21035;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#24187;&#35273;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#26631;&#35760;&#26469;&#25552;&#39640;LLMs&#22312;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.14346</link><description>&lt;p&gt;
&#19981;&#35201;&#36731;&#20449;&#19968;&#20999;&#65306;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#22686;&#24378;&#25688;&#35201;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#26631;&#35760;&#32423;&#21035;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#24187;&#35273;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#26631;&#35760;&#26469;&#25552;&#39640;LLMs&#22312;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#25991;&#26412;&#25805;&#32437;&#8212;&#8212;&#20363;&#22914;&#26426;&#22120;&#32763;&#35793;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#36825;&#21487;&#33021;&#23545;&#27169;&#22411;&#25552;&#20379;&#30340;&#20219;&#20309;&#31572;&#26696;&#30340;&#24544;&#23454;&#24230;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697;&#33268;&#21147;&#20110;&#23545;&#25239;LLMs&#20013;&#30340;&#24187;&#35273;&#65292;&#36825;&#20123;&#20316;&#21697;&#28041;&#21450;&#35782;&#21035;&#34394;&#26500;&#30340;&#21477;&#23376;&#20197;&#21450;&#23545;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#19981;&#21516;&#26041;&#24335;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;LLMs&#22312;&#24187;&#35273;&#26041;&#38754;&#30340;&#34892;&#20026;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#24187;&#35273;&#65292;&#24182;&#36827;&#19968;&#27493;&#21033;&#29992;&#35813;&#26631;&#35760;&#26469;&#25552;&#39640;LLMs&#22312;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;&#36890;&#36807;&#36825;&#19968;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14346v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) are adept at text manipulation -- tasks such as machine translation and text summarization. However, these models can also be prone to hallucination, which can be detrimental to the faithfulness of any answers that the model provides. Recent works in combating hallucinations in LLMs deal with identifying hallucinated sentences and categorizing the different ways in which models hallucinate. This paper takes a deep dive into LLM behavior with respect to hallucinations, defines a token-level approach to identifying different kinds of hallucinations, and further utilizes this token-level tagging to improve the interpretability and faithfulness of LLMs in dialogue summarization tasks. Through this, the paper presents a new, enhanced dataset and a new training paradigm.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RadEdit&#30340;&#26032;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#19978;&#23545;&#29983;&#29289;&#21307;&#23398;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#65292;&#20174;&#32780;&#27169;&#25311;&#25968;&#25454;&#38598;&#36716;&#31227;&#65292;&#35786;&#26029;&#22833;&#25928;&#27169;&#24335;&#65292;&#24182;&#30830;&#20445;&#32534;&#36753;&#22270;&#20687;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.12865</link><description>&lt;p&gt;
RadEdit&#65306;&#36890;&#36807;&#25193;&#25955;&#22270;&#20687;&#32534;&#36753;&#23545;&#29983;&#29289;&#21307;&#23398;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RadEdit: stress-testing biomedical vision models via diffusion image editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RadEdit&#30340;&#26032;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#19978;&#23545;&#29983;&#29289;&#21307;&#23398;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#65292;&#20174;&#32780;&#27169;&#25311;&#25968;&#25454;&#38598;&#36716;&#31227;&#65292;&#35786;&#26029;&#22833;&#25928;&#27169;&#24335;&#65292;&#24182;&#30830;&#20445;&#32534;&#36753;&#22270;&#20687;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#36890;&#24120;&#35268;&#27169;&#36739;&#23567;&#19988;&#23384;&#22312;&#20559;&#35265;&#65292;&#36825;&#24847;&#21619;&#30528;&#39044;&#27979;&#27169;&#22411;&#30340;&#23454;&#38469;&#34920;&#29616;&#24448;&#24448;&#36828;&#20302;&#20110;&#20869;&#37096;&#27979;&#35797;&#25152;&#39044;&#26399;&#30340;&#27700;&#24179;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#22270;&#20687;&#32534;&#36753;&#26469;&#27169;&#25311;&#25968;&#25454;&#38598;&#36716;&#31227;&#65292;&#24182;&#35786;&#26029;&#29983;&#29289;&#21307;&#23398;&#35270;&#35273;&#27169;&#22411;&#30340;&#22833;&#25928;&#27169;&#24335;&#65307;&#36825;&#21487;&#20197;&#22312;&#37096;&#32626;&#21069;&#29992;&#20110;&#35780;&#20272;&#23601;&#32490;&#24615;&#65292;&#21487;&#33021;&#20943;&#23569;&#25104;&#26412;&#21644;&#24739;&#32773;&#21361;&#23475;&#12290;&#29616;&#26377;&#30340;&#32534;&#36753;&#26041;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#33391;&#21464;&#21270;&#65292;&#30001;&#20110;&#30142;&#30149;&#19982;&#27835;&#30103;&#24178;&#39044;&#30340;&#20849;&#21516;&#20986;&#29616;&#32780;&#23398;&#21040;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23454;&#38469;&#24212;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RadEdit&#30340;&#26032;&#32534;&#36753;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;&#25513;&#33180;&#65288;&#22914;&#26524;&#23384;&#22312;&#65289;&#26469;&#32422;&#26463;&#26356;&#25913;&#65292;&#24182;&#30830;&#20445;&#32534;&#36753;&#22270;&#20687;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#19977;&#31181;&#25968;&#25454;&#38598;&#36716;&#31227;&#31867;&#22411;&#65306;&#33719;&#21462;&#36716;&#31227;&#12289;&#34920;&#29616;&#36716;&#31227;&#21644;&#20154;&#21475;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12865v3 Announce Type: replace-cross  Abstract: Biomedical imaging datasets are often small and biased, meaning that real-world performance of predictive models can be substantially lower than expected from internal testing. This work proposes using generative image editing to simulate dataset shifts and diagnose failure modes of biomedical vision models; this can be used in advance of deployment to assess readiness, potentially reducing cost and patient harm. Existing editing methods can produce undesirable changes, with spurious correlations learned due to the co-occurrence of disease and treatment interventions, limiting practical applicability. To address this, we train a text-to-image diffusion model on multiple chest X-ray datasets and introduce a new editing method RadEdit that uses multiple masks, if present, to constrain changes and ensure consistency in the edited images. We consider three types of dataset shifts: acquisition shift, manifestation shift, and populat
&lt;/p&gt;</description></item><item><title>Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2312.11805</link><description>&lt;p&gt;
Gemini&#65306;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemini: A Family of Highly Capable Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11805
&lt;/p&gt;
&lt;p&gt;
Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#31995;&#21015;Gemini&#65292;&#23637;&#31034;&#20986;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;Gemini&#31995;&#21015;&#21253;&#25324;Ultra&#12289;Pro&#21644;Nano&#23610;&#23544;&#65292;&#36866;&#29992;&#20110;&#20174;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21040;&#35774;&#22791;&#20869;&#23384;&#21463;&#38480;&#24212;&#29992;&#30340;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;32&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;30&#20010;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839; - &#26174;&#33879;&#22320;&#26159;&#31532;&#19968;&#20010;&#22312;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#32771;&#35797;&#22522;&#20934;&#27979;&#35797;MMLU&#19978;&#23454;&#29616;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#34920;&#29616;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#27599;&#19968;&#20010;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#25913;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#12290;&#25105;&#20204;&#30456;&#20449;Gemini&#31995;&#21015;&#22312;&#36328;&#27169;&#24577;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26032;&#33021;&#21147;&#23558;&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#29992;&#20363;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#22320;&#21521;&#29992;&#25143;&#25552;&#20379;Gemini&#27169;&#22411;&#30340;&#35757;&#32451;&#21518;&#21644;&#37096;&#32626;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11805v2 Announce Type: replace-cross  Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services includi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#21017;&#21270;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#35270;&#35273;&#23548;&#33322;&#31561;&#26085;&#24120;&#20219;&#21153;&#20013;&#20986;&#29616;&#22806;&#35266;&#21464;&#21270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.09056</link><description>&lt;p&gt;
ReCoRe: &#27491;&#21017;&#21270;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ReCoRe: Regularized Contrastive Representation Learning of World Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09056
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#35270;&#35273;&#23548;&#33322;&#31561;&#26085;&#24120;&#20219;&#21153;&#20013;&#20986;&#29616;&#22806;&#35266;&#21464;&#21270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#22312;&#28216;&#25103;&#29615;&#22659;&#20013;&#24050;&#32463;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#35270;&#35273;&#23548;&#33322;&#31561;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20986;&#29616;&#26174;&#33879;&#22806;&#35266;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19990;&#30028;&#27169;&#22411;&#65292;&#36890;&#36807;&#65288;i&#65289;&#23545;&#27604;&#24230;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#65288;ii&#65289;&#20171;&#20837;&#19981;&#21464;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#12290;&#23398;&#20064;&#19990;&#30028;&#21160;&#21147;&#23398;&#30340;&#26174;&#24335;&#34920;&#31034;&#65292;&#21363;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#23545;&#27604;&#24230;&#23398;&#20064;&#38544;&#21547;&#22320;&#24378;&#21270;&#20102;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#65292;&#25913;&#21892;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#23558;&#23545;&#27604;&#24230;&#25439;&#22833;&#38598;&#25104;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#20026;&#22522;&#20110;&#19990;&#30028;&#27169;&#22411;&#30340;RL&#26041;&#27861;&#29420;&#31435;&#20248;&#21270;&#34920;&#31034;&#23398;&#20064;&#21644;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09056v2 Announce Type: replace-cross  Abstract: While recent model-free Reinforcement Learning (RL) methods have demonstrated human-level effectiveness in gaming environments, their success in everyday tasks like visual navigation has been limited, particularly under significant appearance variations. This limitation arises from (i) poor sample efficiency and (ii) over-fitting to training scenarios. To address these challenges, we present a world model that learns invariant features using (i) contrastive unsupervised learning and (ii) an intervention-invariant regularizer. Learning an explicit representation of the world dynamics i.e. a world model, improves sample efficiency while contrastive learning implicitly enforces learning of invariant features, which improves generalization. However, the na\"ive integration of contrastive loss to world models is not good enough, as world-model-based RL methods independently optimize representation learning and agent policy. To overc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2312.00326</link><description>&lt;p&gt;
Agent-OM&#65306;&#21033;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Agent-OM: Leveraging LLM Agents for Ontology Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#65288;OM&#65289;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#65292;&#36890;&#36807;&#23545;&#40784;&#30456;&#20851;&#23454;&#20307;&#26469;&#35299;&#20915;&#20854;&#27010;&#24565;&#24322;&#26500;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;LLM&#35774;&#35745;&#33539;&#24335;&#65292;&#21629;&#21517;&#20026;Agent-OM&#65292;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#26816;&#32034;&#21644;&#21305;&#37197;&#30340;&#21516;&#20307;&#20195;&#29702;&#20197;&#21450;&#19968;&#32452;&#22522;&#20110;&#25552;&#31034;&#30340;&#31616;&#21333;OM&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00326v2 Announce Type: replace  Abstract: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese agents for retrieval and matching, with a set of simple prompt-based OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAE
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21306;&#22495;&#23454;&#29616;&#20102;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#26080;&#38656;&#29992;&#25143;&#25552;&#20379;&#36974;&#32617;&#25110;&#33609;&#22270;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#25552;&#31034;&#30340;&#29305;&#28857;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#23637;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.16432</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#21306;&#22495;&#36827;&#34892;&#25991;&#26412;&#39537;&#21160;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Text-Driven Image Editing via Learnable Regions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21306;&#22495;&#23454;&#29616;&#20102;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#26080;&#38656;&#29992;&#25143;&#25552;&#20379;&#36974;&#32617;&#25110;&#33609;&#22270;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#25552;&#31034;&#30340;&#29305;&#28857;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#23637;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24050;&#32463;&#25104;&#20026;&#22270;&#20687;&#32534;&#36753;&#30340;&#33258;&#28982;&#25509;&#21475;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#20854;&#30001;&#25991;&#26412;&#25552;&#31034;&#39537;&#21160;&#65292;&#26080;&#38656;&#29992;&#25143;&#25552;&#20379;&#30340;&#36974;&#32617;&#25110;&#33609;&#22270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#36793;&#30028;&#26694;&#29983;&#25104;&#22120;&#26469;&#35782;&#21035;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#30340;&#32534;&#36753;&#21306;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#24403;&#21069;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20860;&#23481;&#30340;&#28789;&#27963;&#32534;&#36753;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#22810;&#20010;&#23545;&#35937;&#12289;&#22797;&#26434;&#21477;&#23376;&#25110;&#36739;&#38271;&#27573;&#33853;&#30340;&#22797;&#26434;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19982;&#25552;&#20379;&#30340;&#35821;&#35328;&#25551;&#36848;&#30456;&#23545;&#24212;&#30340;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#21644;&#36924;&#30495;&#24230;&#30340;&#22270;&#20687;&#25805;&#20316;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#39029;&#21487;&#20197;&#22312;&#27492;&#25214;&#21040;&#65306;ht
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16432v2 Announce Type: replace-cross  Abstract: Language has emerged as a natural interface for image editing. In this paper, we introduce a method for region-based image editing driven by textual prompts, without the need for user-provided masks or sketches. Specifically, our approach leverages an existing pre-trained text-to-image model and introduces a bounding box generator to identify the editing regions that are aligned with the textual prompts. We show that this simple approach enables flexible editing that is compatible with current image generation models, and is able to handle complex prompts featuring multiple objects, complex sentences, or lengthy paragraphs. We conduct an extensive user study to compare our method against state-of-the-art methods. The experiments demonstrate the competitive performance of our method in manipulating images with high fidelity and realism that correspond to the provided language descriptions. Our project webpage can be found at: ht
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#21442;&#32771;&#26550;&#26500;&#65292;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#20195;&#29702;&#26102;&#25552;&#20379;&#25351;&#23548;&#65292;&#24378;&#35843;&#20102;&#36127;&#36131;&#20219;AI&#21644;&#36719;&#20214;&#36136;&#37327;&#23646;&#24615;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.13148</link><description>&lt;p&gt;
&#36808;&#21521;&#36127;&#36131;&#20219;&#30340;&#29983;&#25104;&#24335;AI&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#35774;&#35745;&#20195;&#29702;&#30340;&#21442;&#32771;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Towards Responsible Generative AI: A Reference Architecture for Designing Foundation Model based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#21442;&#32771;&#26550;&#26500;&#65292;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#20195;&#29702;&#26102;&#25552;&#20379;&#25351;&#23548;&#65292;&#24378;&#35843;&#20102;&#36127;&#36131;&#20219;AI&#21644;&#36719;&#20214;&#36136;&#37327;&#23646;&#24615;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21644;&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20855;&#26377;&#25512;&#29702;&#33021;&#21147;&#30340;&#35745;&#21010;&#65292;&#24050;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;AI&#25216;&#26415;&#12290;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20195;&#29702;&#20174;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#20013;&#33719;&#24471;&#33258;&#20027;&#24615;&#65292;&#36825;&#20351;&#23427;&#20204;&#33021;&#22815;&#33258;&#20027;&#23558;&#32473;&#23450;&#30340;&#30446;&#26631;&#20998;&#35299;&#20026;&#19968;&#32452;&#21487;&#31649;&#29702;&#30340;&#20219;&#21153;&#65292;&#24182;&#32534;&#25490;&#20219;&#21153;&#25191;&#34892;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#23613;&#31649;&#22312;&#26500;&#24314;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20195;&#29702;&#26041;&#38754;&#20184;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#20294;&#20195;&#29702;&#30340;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#20351;&#29992;&#20195;&#29702;&#36827;&#34892;&#35268;&#21010;&#21644;&#25191;&#34892;&#26377;&#30528;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#26377;&#20851;&#36127;&#36131;&#20219;AI&#30456;&#20851;&#36719;&#20214;&#36136;&#37327;&#23646;&#24615;&#65288;&#22914;&#23433;&#20840;&#24615;&#21644;&#38382;&#36131;&#21046;&#65289;&#23384;&#22312;&#20005;&#37325;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#21442;&#32771;&#26550;&#26500;&#65292;&#21487;&#29992;&#20316;&#22312;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#20195;&#29702;&#26102;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13148v3 Announce Type: replace  Abstract: Foundation models, such as large language models (LLMs), have been widely recognised as transformative AI technologies due to their capabilities to understand and generate content, including plans with reasoning capabilities. Foundation model based agents derive their autonomy from the capabilities of foundation models, which enable them to autonomously break down a given goal into a set of manageable tasks and orchestrate task execution to meet the goal. Despite the huge efforts put into building foundation model based agents, the architecture design of the agents has not yet been systematically explored. Also, while there are significant benefits of using agents for planning and execution, there are serious considerations regarding responsible AI related software quality attributes, such as security and accountability. Therefore, this paper presents a pattern-oriented reference architecture that serves as guidance when designing fo
&lt;/p&gt;</description></item><item><title>MetaCloak&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#39069;&#22806;&#36716;&#25442;&#25277;&#26679;&#36807;&#31243;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#40065;&#26834;&#30340;&#25200;&#21160;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.13127</link><description>&lt;p&gt;
&#38024;&#23545;&#26410;&#32463;&#25480;&#26435;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#21512;&#25104;&#30340;&#40065;&#26834;&#24615;&#19981;&#21487;&#23519;&#35273;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Toward Robust Imperceptible Perturbation against Unauthorized Text-to-image Diffusion-based Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13127
&lt;/p&gt;
&lt;p&gt;
MetaCloak&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#39069;&#22806;&#36716;&#25442;&#25277;&#26679;&#36807;&#31243;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#40065;&#26834;&#30340;&#25200;&#21160;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13127v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#20195;&#20132;&#21449; &#25688;&#35201;: &#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20801;&#35768;&#20174;&#23569;&#37327;&#21442;&#32771;&#29031;&#29255;&#26080;&#32541;&#29983;&#25104;&#20010;&#24615;&#21270;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#22914;&#26524;&#33853;&#20837;&#38169;&#35823;&#30340;&#25163;&#20013;&#65292;&#21487;&#33021;&#21046;&#36896;&#35823;&#23548;&#24615;&#25110;&#26377;&#23475;&#20869;&#23481;&#65292;&#21361;&#23475;&#20010;&#20154;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25237;&#27602;&#30340;&#26041;&#27861;&#20197;&#19968;&#31181;&#19981;&#21487;&#23519;&#35273;&#30340;&#26041;&#24335;&#25200;&#21160;&#29992;&#25143;&#22270;&#20687;&#65292;&#20197;&#20351;&#20854;&#26080;&#27861;&#34987;&#24694;&#24847;&#20351;&#29992;&#32773;&#23398;&#20064;&#12290;&#25105;&#20204;&#30830;&#23450;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;i) &#30001;&#20110;&#25163;&#24037;&#21551;&#21457;&#24335;&#35299;&#20915;&#38590;&#35299;&#21452;&#23618;&#20248;&#21270;&#32780;&#23548;&#33268;&#27425;&#20248;&#65307;ii) &#32570;&#20047;&#23545;&#31616;&#21333;&#25968;&#25454;&#36716;&#25442;&#65288;&#22914;&#39640;&#26031;&#28388;&#27874;&#65289;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaCloak&#65292;&#23427;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#21452;&#32423;&#25237;&#27602;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#39069;&#22806;&#30340;&#36716;&#25442;&#25277;&#26679;&#36807;&#31243;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#40065;&#26834;&#30340;&#25200;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#32452;&#26367;&#20195;&#25193;&#25955;&#27169;&#22411;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13127v2 Announce Type: replace-cross  Abstract: Text-to-image diffusion models allow seamless generation of personalized images from scant reference photos. Yet, these tools, in the wrong hands, can fabricate misleading or harmful content, endangering individuals. To address this problem, existing poisoning-based approaches perturb user images in an imperceptible way to render them "unlearnable" from malicious uses. We identify two limitations of these defending approaches: i) sub-optimal due to the hand-crafted heuristics for solving the intractable bilevel optimization and ii) lack of robustness against simple data transformations like Gaussian filtering. To solve these challenges, we propose MetaCloak, which solves the bi-level poisoning problem with a meta-learning framework with an additional transformation sampling process to craft transferable and robust perturbation. Specifically, we employ a pool of surrogate diffusion models to craft transferable and model-agnostic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TWEAK&#30340;&#20165;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20551;&#35774;&#39564;&#35777;&#27169;&#22411;&#26469;&#25552;&#39640;&#30693;&#35782;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2311.09467</link><description>&lt;p&gt;
&#20889;&#20316;&#26102;&#24605;&#32771;&#65306;&#20551;&#35774;&#39564;&#35777;&#20419;&#36827;&#24544;&#23454;&#30340;&#30693;&#35782;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09467
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TWEAK&#30340;&#20165;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20551;&#35774;&#39564;&#35777;&#27169;&#22411;&#26469;&#25552;&#39640;&#30693;&#35782;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#21040;&#25991;&#26412;&#29983;&#25104;&#22120;&#32463;&#24120;&#38590;&#20197;&#24544;&#23454;&#22320;&#20026;&#36755;&#20837;&#20107;&#23454;&#29983;&#25104;&#25551;&#36848;&#65306;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#19982;&#36755;&#20837;&#30456;&#30683;&#30462;&#30340;&#24187;&#35273;&#65292;&#25110;&#25551;&#36848;&#36755;&#20837;&#20013;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#20943;&#23569;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#35299;&#30721;&#30340;&#26041;&#27861;TWEAK&#65288;&#24605;&#32771;&#32780;&#26377;&#25928;&#34920;&#36798;&#30693;&#35782;&#65289;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#29983;&#25104;&#22120;&#38598;&#25104;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;TWEAK&#23558;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#30340;&#29983;&#25104;&#24207;&#21015;&#21450;&#20854;&#26410;&#26469;&#24207;&#21015;&#35270;&#20026;&#20551;&#35774;&#65292;&#24182;&#26681;&#25454;&#20854;&#20551;&#35774;&#21463;&#21040;&#36755;&#20837;&#20107;&#23454;&#25903;&#25345;&#30340;&#31243;&#24230;&#65292;&#20351;&#29992;&#20551;&#35774;&#39564;&#35777;&#27169;&#22411;&#65288;HVM&#65289;&#23545;&#27599;&#20010;&#29983;&#25104;&#20505;&#36873;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#20316;&#20026;HVM&#23637;&#31034;&#20102;TWEAK&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25253;&#21578;&#20102;&#23545;&#36136;&#37327;&#24433;&#21709;&#24456;&#23567;&#30340;&#25913;&#21892;&#24544;&#23454;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;NLI&#27169;&#22411;&#26367;&#25442;&#20026;&#20351;&#29992;FATE&#65288;&#20107;&#23454;&#23545;&#40784;&#25991;&#26412;&#65289;&#39318;&#21019;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#29305;&#23450;&#20219;&#21153;HVM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09467v2 Announce Type: replace-cross  Abstract: Knowledge-to-text generators often struggle to faithfully generate descriptions for the input facts: they may produce hallucinations that contradict the input, or describe facts not present in the input. To reduce hallucinations, we propose a decoding-only method, TWEAK (Think While Effectively Articulating Knowledge), which can be integrated with any generator without retraining. TWEAK treats the generated sequences at each decoding step and its future sequences as hypotheses, and ranks each generation candidate based on the extent to which their hypotheses are supported by the input facts using a Hypothesis Verification Model (HVM). We first demonstrate the effectiveness of TWEAK by using a Natural Language Inference (NLI) model as the HVM and report improved faithfulness with a minimal impact on the quality. We then replace the NLI model with a task-specific HVM trained with a first-of-a-kind dataset, FATE (Fact-Aligned Text
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;-LM&#35299;&#30721;&#30446;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;Anti-Language Model&#30446;&#26631;&#21644;&#19968;&#20010;&#35774;&#35745;&#33391;&#22909;&#30340;&#34928;&#20943;&#22240;&#23376;&#65292;&#35299;&#20915;&#20102;&#38646;&#32763;&#35793;&#19978;&#19979;&#25991;&#26426;&#22120;&#32763;&#35793;&#30340;&#24369;&#28857;&#65292;&#19982;&#20854;&#20182;&#35299;&#30721;&#30446;&#26631;&#30456;&#27604;&#65292;&#22312;&#26576;&#20123;&#35774;&#32622;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;20&#20010;BLEU&#28857;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2311.08324</link><description>&lt;p&gt;
&#38646;&#32763;&#35793;&#19978;&#19979;&#25991;&#26426;&#22120;&#32763;&#35793;&#30340;&#21453;-LM&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Anti-LM Decoding for Zero-shot In-context Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08324
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;-LM&#35299;&#30721;&#30446;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;Anti-Language Model&#30446;&#26631;&#21644;&#19968;&#20010;&#35774;&#35745;&#33391;&#22909;&#30340;&#34928;&#20943;&#22240;&#23376;&#65292;&#35299;&#20915;&#20102;&#38646;&#32763;&#35793;&#19978;&#19979;&#25991;&#26426;&#22120;&#32763;&#35793;&#30340;&#24369;&#28857;&#65292;&#19982;&#20854;&#20182;&#35299;&#30721;&#30446;&#26631;&#30456;&#27604;&#65292;&#22312;&#26576;&#20123;&#35774;&#32622;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;20&#20010;BLEU&#28857;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#32763;&#35793;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#25351;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#31616;&#21333;&#30340;&#25351;&#20196;&#25191;&#34892;&#20219;&#21153;&#30340;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#26657;&#20934;&#19981;&#20339;&#12290;&#22788;&#29702;&#36825;&#31181;&#20559;&#35265;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#26159;&#37319;&#29992;&#23545;&#27604;&#35299;&#30721;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#32771;&#34385;&#36890;&#36807;&#22312;&#26576;&#20123;&#19978;&#19979;&#25991;&#19978;&#19979;&#25991;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#20808;&#39564;&#27010;&#29575;&#12290;&#26412;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;Anti-Language Model&#30446;&#26631;&#65292;&#24102;&#26377;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#19978;&#19979;&#25991;&#26426;&#22120;&#32763;&#35793;&#30340;&#24369;&#28857;&#30340;&#34928;&#20943;&#22240;&#23376;&#12290;&#25105;&#20204;&#22312;3&#31181;&#27169;&#22411;&#31867;&#22411;&#21644;&#22823;&#23567;&#65292;3&#31181;&#35821;&#35328;&#26041;&#21521;&#19978;&#20197;&#21450;&#36138;&#23146;&#35299;&#30721;&#21644;&#27874;&#26463;&#25628;&#32034;&#65288;$B=5$&#65289;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#22312;&#26576;&#20123;&#35774;&#32622;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35299;&#30721;&#30446;&#26631;&#65292;&#35266;&#23519;&#21040;&#27604;&#40664;&#35748;&#30446;&#26631;&#39640;&#36798;20&#20010;BLEU&#28857;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08324v2 Announce Type: replace-cross  Abstract: Zero-shot In-context learning is the phenomenon where models can perform the task simply given the instructions. However, pre-trained large language models are known to be poorly calibrated for this task. One of the most effective approaches to handling this bias is to adopt a contrastive decoding objective, which accounts for the prior probability of generating the next token by conditioning on some context. This work introduces an Anti-Language Model objective with a decay factor designed to address the weaknesses of In-context Machine Translation. We conduct our experiments across 3 model types and sizes, 3 language directions, and for both greedy decoding and beam search ($B=5$). The proposed method outperforms other state-of-art decoding objectives, with up to $20$ BLEU point improvement from the default objective observed in some settings.
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#27861;&#25552;&#20379;&#27604;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2311.07484</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#27979;&#37327;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Psychometric Predictive Power of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07484
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#27861;&#25552;&#20379;&#27604;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07484v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#20132;&#21449; &#25688;&#35201;:&#25351;&#20196;&#35843;&#25972;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21709;&#24212;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#23613;&#31649;&#22312;&#20154;-LLM&#23545;&#40784;&#26041;&#38754;&#36827;&#34892;&#20102;&#21162;&#21147;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#24182;&#19981;&#24635;&#26159;&#20351;LLMs&#20174;&#35748;&#30693;&#24314;&#27169;&#30340;&#35282;&#24230;&#30475;&#36215;&#26469;&#26356;&#20687;&#20154;&#31867;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#30001;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#20272;&#35745;&#30340;&#19979;&#19968;&#20010;&#35789;&#27010;&#29575;&#24448;&#24448;&#27604;&#22522;&#30784;LLM&#20272;&#35745;&#30340;&#27010;&#29575;&#26356;&#31967;&#31957;&#65292;&#26080;&#27861;&#27169;&#25311;&#20154;&#31867;&#38405;&#35835;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#27169;&#25311;&#20154;&#31867;&#38405;&#35835;&#34892;&#20026;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#26144;&#29305;&#23450;&#35821;&#35328;&#20551;&#35774;&#30340;&#25552;&#31034;&#21487;&#20197;&#25552;&#39640;PPP&#65292;&#20294;&#20173;&#19981;&#21450;&#23567;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;PPP&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20986;LLMs&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#27604;&#22522;&#30784;LLMs&#30452;&#25509;&#27010;&#29575;&#27979;&#37327;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32431;&#31929;&#30340;&#19979;&#19968;&#20010;&#35789;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07484v2 Announce Type: replace-cross  Abstract: Instruction tuning aligns the response of large language models (LLMs) with human preferences. Despite such efforts in human--LLM alignment, we report that, interestingly, instruction tuning does not always make LLMs human-like from a cognitive modeling perspective. More specifically, next-word probabilities estimated by instruction-tuned LLMs are often worse at simulating human reading behavior than those estimated by base LLMs. In addition, we explore prompting methodologies in simulating human reading behavior with LLMs. Our results show that prompts reflecting a particular linguistic hypothesis improve PPP but are still inferior to PPP from small base models. These findings highlight that recent advancements in LLMs, i.e., instruction tuning and prompting, do not offer better estimates than direct probability measurements from base LLMs in cognitive modeling. In other words, our experiments highlight that pure next-word pro
&lt;/p&gt;</description></item><item><title>BatteryML&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#36890;&#36807;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#25552;&#39640;&#20102;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.14714</link><description>&lt;p&gt;
BatteryML&#65306;&#19968;&#20010;&#29992;&#20110;&#30005;&#27744;&#34928;&#20943;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
BatteryML:An Open-source platform for Machine Learning on Battery Degradation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14714
&lt;/p&gt;
&lt;p&gt;
BatteryML&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#36890;&#36807;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#25552;&#39640;&#20102;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#34928;&#20943;&#20173;&#28982;&#26159;&#33021;&#28304;&#23384;&#20648;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#25512;&#21160;&#27934;&#23519;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24037;&#20855;&#27491;&#22312;&#23835;&#36215;&#12290;&#28982;&#32780;&#65292;&#30005;&#21270;&#23398;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39046;&#22495;&#24102;&#26469;&#20102;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#32463;&#24120;&#22312;&#22788;&#29702;&#30005;&#27744;&#31185;&#23398;&#30340;&#22797;&#26434;&#24615;&#19978;&#33510;&#33510;&#25379;&#25166;&#65292;&#32780;&#30005;&#27744;&#30740;&#31350;&#20154;&#21592;&#21017;&#38754;&#20020;&#30528;&#23558;&#22797;&#26434;&#27169;&#22411;&#35843;&#25972;&#21040;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#28085;&#30422;&#25968;&#25454;&#26684;&#24335;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#32479;&#19968;&#26631;&#20934;&#12290;&#37492;&#20110;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BatteryML - &#19968;&#20010;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#19988;&#24320;&#28304;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#32479;&#19968;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#20256;&#32479;&#21644;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#23454;&#29616;&#12290;&#36825;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14714v4 Announce Type: replace-cross  Abstract: Battery degradation remains a pivotal concern in the energy storage domain, with machine learning emerging as a potent tool to drive forward insights and solutions. However, this intersection of electrochemical science and machine learning poses complex challenges. Machine learning experts often grapple with the intricacies of battery science, while battery researchers face hurdles in adapting intricate models tailored to specific datasets. Beyond this, a cohesive standard for battery degradation modeling, inclusive of data formats and evaluative benchmarks, is conspicuously absent. Recognizing these impediments, we present BatteryML - a one-step, all-encompass, and open-source platform designed to unify data preprocessing, feature extraction, and the implementation of both traditional and state-of-the-art models. This streamlined approach promises to enhance the practicality and efficiency of research applications. BatteryML s
&lt;/p&gt;</description></item><item><title>LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2310.05746</link><description>&lt;p&gt;
&#35753;&#34892;&#21160;&#32988;&#20110;&#38596;&#36777;&#65306;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#20013;&#30340;&#25112;&#30053;&#35268;&#21010;&#19982;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05746
&lt;/p&gt;
&lt;p&gt;
LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28982;&#32780;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#20110;&#38745;&#24577;&#22522;&#20934;&#12290;&#35780;&#20272;&#36825;&#19968;&#28857;&#38656;&#35201;&#27979;&#35797;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;&#30340;&#29615;&#22659;&#65292;&#36825;&#31181;&#29615;&#22659;&#38656;&#35201;&#22312;&#21160;&#24577;&#30340;&#31454;&#20105;&#22330;&#26223;&#20013;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AucArena&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#25311;&#25293;&#21334;&#30340;&#26032;&#39062;&#35780;&#20272;&#22871;&#20214;&#65292;&#36873;&#25321;&#36825;&#20010;&#35774;&#32622;&#26159;&#22240;&#20026;&#23427;&#38750;&#24120;&#19981;&#21487;&#39044;&#27979;&#65292;&#28041;&#21450;&#19982;&#36164;&#28304;&#21644;&#39118;&#38505;&#31649;&#29702;&#30456;&#20851;&#30340;&#35768;&#22810;&#25216;&#33021;&#65292;&#21516;&#26102;&#20063;&#26131;&#20110;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLM&#39537;&#21160;&#31454;&#26631;&#20195;&#29702;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#20182;&#20204;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;LLM&#20855;&#26377;&#25293;&#21334;&#21442;&#19982;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#22914;&#39044;&#31639;&#31649;&#29702;&#21644;&#30446;&#26631;&#36981;&#20174;&#65292;&#36825;&#20123;&#25216;&#33021;&#20250;&#38543;&#30528;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#25913;&#36827;&#32780;&#25552;&#39640;&#12290;&#36825;&#31361;&#20986;&#20102;LLM&#22312;&#24314;&#27169;&#31454;&#25216;&#32972;&#26223;&#19979;&#30340;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05746v2 Announce Type: replace-cross  Abstract: Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26500;&#24314;&#23545;&#27604;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#30340;&#20559;&#22909;&#23545;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#25928;&#26524;&#65292;&#24182;&#19988;&#36890;&#36807;DPO&#23545;&#27604;&#25216;&#26415;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#23545;&#40784;&#65292;&#26368;&#32456;&#20351;&#32463;&#36807;&#35843;&#20248;&#30340;&#25351;&#23548;&#23398;&#20064;&#27169;&#22411;Orca&#36229;&#36234;&#20102;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2310.02263</link><description>&lt;p&gt;
&#23545;&#27604;&#21518;&#35757;&#32451;&#30340;&#33258;&#21160;&#23545;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Automatic Pair Construction for Contrastive Post-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02263
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26500;&#24314;&#23545;&#27604;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#30340;&#20559;&#22909;&#23545;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#25928;&#26524;&#65292;&#24182;&#19988;&#36890;&#36807;DPO&#23545;&#27604;&#25216;&#26415;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#23545;&#40784;&#65292;&#26368;&#32456;&#20351;&#32463;&#36807;&#35843;&#20248;&#30340;&#25351;&#23548;&#23398;&#20064;&#27169;&#22411;Orca&#36229;&#36234;&#20102;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#20316;&#20026;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36208;&#21521;&#20154;&#31867;&#20559;&#22909;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26500;&#24314;LLM&#23545;&#27604;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#19981;&#21516;&#24378;&#24230;&#27169;&#22411;&#65288;&#20363;&#22914;InstructGPT&#12289;ChatGPT&#21644;GPT-4&#65289;&#30340;&#20559;&#22909;&#23545;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;SLiC&#21644;DPO&#30340;&#23545;&#27604;&#25216;&#26415;&#19982;SFT&#22522;&#32447;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#22312;&#32487;&#32493;SFT&#39281;&#21644;&#21518;&#65292;DPO&#20173;&#28982;&#25552;&#20379;&#20102;&#19968;&#20010;&#38454;&#36291;&#24335;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#23545;&#27604;&#21518;&#35757;&#32451;&#30340;&#25968;&#25454;&#35838;&#31243;&#23398;&#20064;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20174;&#8220;&#26356;&#23481;&#26131;&#8221;&#30340;&#23545;&#24320;&#22987;&#23398;&#20064;&#65292;&#28982;&#21518;&#36807;&#28193;&#21040;&#8220;&#26356;&#38590;&#8221;&#30340;&#23545;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23545;&#40784;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26356;&#22810;&#25968;&#25454;&#21644;&#20687;Orca&#36825;&#26679;&#30340;&#26356;&#22823;&#22411;&#27169;&#22411;&#26469;&#25193;&#22823;&#23454;&#39564;&#35268;&#27169;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#33258;&#21160;&#23545;&#27604;&#21518;&#35757;&#32451;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;Orca&#30340;&#24615;&#33021;&#65292;&#23427;&#24050;&#32463;&#26159;&#19968;&#20010;&#36890;&#36807;GPT-4&#36755;&#20986;&#35843;&#20248;&#30340;&#26368;&#20808;&#36827;&#25351;&#23548;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#36229;&#36234;&#20102;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02263v2 Announce Type: replace-cross  Abstract: Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from "easier" pairs and transitioning to "harder" ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#38598;&#25104;&#30340;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550; DPoE&#65292;&#26088;&#22312;&#36890;&#36807;&#21435;&#22122;&#35774;&#35745;&#21644;&#25429;&#25417;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#27973;&#23618;&#27169;&#22411;&#65292;&#20197;&#21450;&#38450;&#27490;&#23398;&#20064;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#20027;&#27169;&#22411;&#65292;&#26377;&#25928;&#25269;&#24481;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2305.14910</link><description>&lt;p&gt;
&#20174;&#24555;&#25463;&#26041;&#24335;&#21040;&#35302;&#21457;&#22120;&#65306;&#20351;&#29992;&#21435;&#22122; PoE &#36827;&#34892;&#21518;&#38376;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
From Shortcuts to Triggers: Backdoor Defense with Denoised PoE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14910
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#38598;&#25104;&#30340;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550; DPoE&#65292;&#26088;&#22312;&#36890;&#36807;&#21435;&#22122;&#35774;&#35745;&#21644;&#25429;&#25417;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#27973;&#23618;&#27169;&#22411;&#65292;&#20197;&#21450;&#38450;&#27490;&#23398;&#20064;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#20027;&#27169;&#22411;&#65292;&#26377;&#25928;&#25269;&#24481;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#38754;&#20020;&#22810;&#26679;&#30340;&#21518;&#38376;&#25915;&#20987;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25968;&#25454;&#27745;&#26579;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#38450;&#24481;&#35299;&#20915;&#26041;&#26696;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#24102;&#26377;&#26174;&#24335;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#19978;&#65292;&#23545;&#25239;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#19982;&#19981;&#21516;&#35302;&#21457;&#22120;&#30340;&#36890;&#29992;&#38450;&#24481;&#26041;&#27861;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#38598;&#25104;&#30340;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550; DPoE&#65288;Denoised Product-of-Experts&#65289;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#20197;&#25269;&#24481;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#12290;DPoE &#21253;&#21547;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#25429;&#25417;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#27973;&#23618;&#27169;&#22411;&#21644;&#19968;&#20010;&#34987;&#38459;&#27490;&#23398;&#20064;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#20027;&#27169;&#22411;&#12290;&#20026;&#20102;&#22788;&#29702;&#21518;&#38376;&#25915;&#20987;&#32773;&#24341;&#36215;&#30340;&#26631;&#31614;&#32763;&#36716;&#65292;DPoE &#34701;&#20837;&#20102;&#21435;&#22122;&#35774;&#35745;&#12290;&#23545; SST-2 &#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DPoE &#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14910v3 Announce Type: replace-cross  Abstract: Language models are often at risk of diverse backdoor attacks, especially data poisoning. Thus, it is important to investigate defense solutions for addressing them. Existing backdoor defense methods mainly focus on backdoor attacks with explicit triggers, leaving a universal defense against various backdoor attacks with diverse triggers largely unexplored. In this paper, we propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised Product-of-Experts), which is inspired by the shortcut nature of backdoor attacks, to defend various backdoor attacks. DPoE consists of two models: a shallow model that captures the backdoor shortcuts and a main model that is prevented from learning the backdoor shortcuts. To address the label flip caused by backdoor attackers, DPoE incorporates a denoising design. Experiments on SST-2 dataset show that DPoE significantly improves the defense performance against various types of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38271;&#23485;&#27604;&#25935;&#24863;&#30340;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#22120;ARS-DETR&#65292;&#37319;&#29992;&#39640;&#31934;&#24230;&#24230;&#37327;AP$_{75}$&#26469;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#35282;&#24230;&#20998;&#31867;&#26041;&#27861;&#21644;&#26059;&#36716;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2303.04989</link><description>&lt;p&gt;
ARS-DETR: &#38754;&#21521;&#38271;&#23485;&#27604;&#25935;&#24863;&#30340;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#19982;Transformer
&lt;/p&gt;
&lt;p&gt;
ARS-DETR: Aspect Ratio Sensitive Oriented Object Detection with Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.04989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38271;&#23485;&#27604;&#25935;&#24863;&#30340;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#22120;ARS-DETR&#65292;&#37319;&#29992;&#39640;&#31934;&#24230;&#24230;&#37327;AP$_{75}$&#26469;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#35282;&#24230;&#20998;&#31867;&#26041;&#27861;&#21644;&#26059;&#36716;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#24230;&#37327;AP$_{50}$&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;AP$_{50}$&#22312;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#20013;&#26412;&#36136;&#19978;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#23545;&#35282;&#24230;&#20559;&#24046;&#20855;&#26377;&#36739;&#22823;&#30340;&#23481;&#24525;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#20351;&#29992;&#39640;&#31934;&#24230;&#24230;&#37327;&#65292;&#20363;&#22914;AP$_{75}$&#65292;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;Transformer&#30340;&#38754;&#21521;&#38271;&#23485;&#27604;&#25935;&#24863;&#30340;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#31216;&#20026;ARS-DETR&#65292;&#22312;&#39640;&#31934;&#24230;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;&#38754;&#21521;&#38271;&#23485;&#27604;&#30340;&#22278;&#28369;&#26631;&#31614;&#65288;AR-CSL&#65289;&#65292;&#20197;&#26356;&#21512;&#29702;&#22320;&#24179;&#28369;&#35282;&#24230;&#26631;&#31614;&#65292;&#24182;&#20002;&#24323;&#20808;&#21069;&#24037;&#20316;&#24341;&#20837;&#30340;&#36229;&#21442;&#25968;&#65288;&#20363;&#22914;CSL&#65289;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26059;&#36716;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#26681;&#25454;&#30456;&#24212;&#35282;&#24230;&#26059;&#36716;&#37319;&#26679;&#28857;&#65292;&#24182;&#28040;&#38500;&#20043;&#21069;&#24037;&#20316;&#24341;&#20837;&#30340;&#26679;&#26412;&#28857;&#19982;&#29305;&#24449;&#28857;&#20043;&#38388;&#30340;&#38169;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.04989v2 Announce Type: replace-cross  Abstract: Existing oriented object detection methods commonly use metric AP$_{50}$ to measure the performance of the model. We argue that AP$_{50}$ is inherently unsuitable for oriented object detection due to its large tolerance in angle deviation. Therefore, we advocate using high-precision metric, e.g. AP$_{75}$, to measure the performance of models. In this paper, we propose an Aspect Ratio Sensitive Oriented Object Detector with Transformer, termed ARS-DETR, which exhibits a competitive performance in high-precision oriented object detection. Specifically, a new angle classification method, calling Aspect Ratio aware Circle Smooth Label (AR-CSL), is proposed to smooth the angle label in a more reasonable way and discard the hyperparameter that introduced by previous work (e.g. CSL). Then, a rotated deformable attention module is designed to rotate the sampling points with the corresponding angles and eliminate the misalignment betwe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;GIT&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#33021;&#22815;&#36890;&#36807;&#20449;&#21495;&#26799;&#24230;&#20272;&#35745;&#22120;&#38477;&#20302;&#24178;&#39044;&#27425;&#25968;&#65292;&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2211.13715</link><description>&lt;p&gt;
&#30456;&#20449;&#24744;&#30340; $\nabla$: &#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#29992;&#20110;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.13715
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;GIT&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#33021;&#22815;&#36890;&#36807;&#20449;&#21495;&#26799;&#24230;&#20272;&#35745;&#22120;&#38477;&#20302;&#24178;&#39044;&#27425;&#25968;&#65292;&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26159;&#31185;&#23398;&#20013;&#19968;&#39033;&#20855;&#26377;&#22522;&#30784;&#37325;&#35201;&#24615;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#35266;&#27979;&#25968;&#25454;&#36890;&#24120;&#19981;&#36275;&#20197;&#21807;&#19968;&#30830;&#23450;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#34429;&#28982;&#36827;&#34892;&#24178;&#39044;&#65288;&#21363;&#23454;&#39564;&#65289;&#21487;&#20197;&#25913;&#21892;&#21487;&#35782;&#21035;&#24615;&#65292;&#20294;&#36825;&#20123;&#26679;&#26412;&#36890;&#24120;&#38590;&#20197;&#33719;&#24471;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#22240;&#26524;&#21457;&#29616;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20272;&#35745;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#24178;&#39044;&#30446;&#26631;&#26469;&#26368;&#23567;&#21270;&#24178;&#39044;&#27425;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;&#31616;&#31216;&#20026;GIT&#65292;&#23427;&#8216;&#30456;&#20449;&#8217;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#20197;&#25552;&#20379;&#24178;&#39044;&#37319;&#38598;&#20989;&#25968;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;GIT&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#24403;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.13715v4 Announce Type: replace-cross  Abstract: Inferring causal structure from data is a challenging task of fundamental importance in science. Observational data are often insufficient to identify a system's causal structure uniquely. While conducting interventions (i.e., experiments) can improve the identifiability, such samples are usually challenging and expensive to obtain. Hence, experimental design approaches for causal discovery aim to minimize the number of interventions by estimating the most informative intervention target. In this work, we propose a novel Gradient-based Intervention Targeting method, abbreviated GIT, that 'trusts' the gradient estimator of a gradient-based causal discovery framework to provide signals for the intervention acquisition function. We provide extensive experiments in simulated and real-world datasets and demonstrate that GIT performs on par with competitive baselines, surpassing them in the low-data regime.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25193;&#25955;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#24555;&#36895;&#31227;&#21160;&#29289;&#20307;&#30340;&#23454;&#26102;&#25318;&#25130;&#21644;&#36991;&#38556;</title><link>https://arxiv.org/abs/2209.13628</link><description>&lt;p&gt;
&#32479;&#19968;&#25511;&#21046;&#26694;&#26550;&#65306;&#21033;&#29992;&#25193;&#25955;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#24555;&#36895;&#31227;&#21160;&#29289;&#20307;&#30340;&#23454;&#26102;&#25318;&#25130;&#21644;&#36991;&#38556;
&lt;/p&gt;
&lt;p&gt;
Unified Control Framework for Real-Time Interception and Obstacle Avoidance of Fast-Moving Objects with Diffusion Variational Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.13628
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#24555;&#36895;&#31227;&#21160;&#29289;&#20307;&#30340;&#23454;&#26102;&#25318;&#25130;&#21644;&#36991;&#38556;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36890;&#36807;&#26426;&#22120;&#20154;&#33218;&#23454;&#29616;&#23545;&#24555;&#36895;&#31227;&#21160;&#29289;&#20307;&#30340;&#23454;&#26102;&#25318;&#25130;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#38656;&#35201;&#22312;&#21160;&#24577;&#38556;&#30861;&#29289;&#20013;&#36805;&#36895;&#20570;&#20986;&#21453;&#24212;&#65292;&#36890;&#24120;&#22312;&#27627;&#31186;&#32423;&#21035;&#20869;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25511;&#21046;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#36890;&#36807;&#21516;&#26102;&#25318;&#25130;&#21160;&#24577;&#29289;&#20307;&#21644;&#36991;&#24320;&#31227;&#21160;&#38556;&#30861;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#65292;&#20197;&#25191;&#34892;&#29289;&#20307;&#25318;&#25130;&#21644;&#38556;&#30861;&#29289;&#36991;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.13628v2 Announce Type: replace-cross  Abstract: Real-time interception of fast-moving objects by robotic arms in dynamic environments poses a formidable challenge due to the need for rapid reaction times, often within milliseconds, amidst dynamic obstacles. This paper introduces a unified control framework to address the above challenge by simultaneously intercepting dynamic objects and avoiding moving obstacles. Central to our approach is using diffusion-based variational autoencoder for motion planning to perform both object interception and obstacle avoidance. We begin by encoding the high-dimensional temporal information from streaming events into a two-dimensional latent manifold, enabling the discrimination between safe and colliding trajectories, culminating in the construction of an offline densely connected trajectory graph. Subsequently, we employ an extended Kalman filter to achieve precise real-time tracking of the moving object. Leveraging a graph-traversing str
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MA-Trace&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#20855;&#26377;&#39640;&#21487;&#20280;&#32553;&#24615;&#65292;&#24182;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#20316;&#20026;&#31163;&#31574;&#30053;&#20462;&#27491;&#26041;&#27861;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#20998;&#24067;&#30340;&#36136;&#37327;&#21644;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2111.11229</link><description>&lt;p&gt;
&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#31574;&#30053;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Correction For Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.11229
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MA-Trace&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#20855;&#26377;&#39640;&#21487;&#20280;&#32553;&#24615;&#65292;&#24182;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#20316;&#20026;&#31163;&#31574;&#30053;&#20462;&#27491;&#26041;&#27861;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#20998;&#24067;&#30340;&#36136;&#37327;&#21644;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20026;&#28041;&#21450;&#22810;&#20010;&#30456;&#20114;&#20316;&#29992;&#26234;&#20307;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#23613;&#31649;&#34920;&#38754;&#19978;&#19982;&#21333;&#26234;&#20307;&#24773;&#20917;&#30456;&#20284;&#65292;&#20294;&#22810;&#26234;&#20307;&#38382;&#39064;&#24448;&#24448;&#26356;&#38590;&#35757;&#32451;&#21644;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MA-Trace&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#23558;V-Trace&#25193;&#23637;&#21040;MARL&#35774;&#23450;&#20013;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20854;&#22312;&#22810;&#24037;&#20316;&#22120;&#35774;&#32622;&#20013;&#30340;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;&#20026;&#27492;&#65292;MA-Trace&#21033;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#20316;&#20026;&#31163;&#31574;&#30053;&#20462;&#27491;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#19981;&#24433;&#21709;&#35757;&#32451;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35745;&#31639;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#22312;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#30340; - &#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#20445;&#35777;&#25910;&#25947;&#30340;&#19981;&#21160;&#28857;&#23450;&#29702;&#12290;&#25105;&#20204;&#22312;StarCraft&#22810;&#26234;&#20307;&#25361;&#25112;&#36187;&#19978;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#36825;&#26159;&#22810;&#26234;&#20307;&#31639;&#27861;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;MA-Trace&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.11229v3 Announce Type: replace-cross  Abstract: Multi-agent reinforcement learning (MARL) provides a framework for problems involving multiple interacting agents. Despite apparent similarity to the single-agent case, multi-agent problems are often harder to train and analyze theoretically. In this work, we propose MA-Trace, a new on-policy actor-critic algorithm, which extends V-Trace to the MARL setting. The key advantage of our algorithm is its high scalability in a multi-worker setting. To this end, MA-Trace utilizes importance sampling as an off-policy correction method, which allows distributing the computations with no impact on the quality of training. Furthermore, our algorithm is theoretically grounded - we prove a fixed-point theorem that guarantees convergence. We evaluate the algorithm extensively on the StarCraft Multi-Agent Challenge, a standard benchmark for multi-agent algorithms. MA-Trace achieves high performance on all its tasks and exceeds state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#22122;&#22768;&#21644;&#24402;&#32435;&#20559;&#35265;&#30340;&#20316;&#29992;&#20351;&#24471;&#32452;&#21512;&#24335;&#27807;&#36890;&#33258;&#21457;&#20135;&#29983;&#65292;&#24182;&#19988;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#22122;&#22768;&#26377;&#21161;&#20110;&#20419;&#36827;&#32452;&#21512;&#24615;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2111.06464</link><description>&lt;p&gt;
&#22122;&#22768;&#22312;&#32452;&#21512;&#24335;&#27807;&#36890;&#20013;&#30340;&#20652;&#21270;&#20316;&#29992;&#21644;&#24402;&#32435;&#20559;&#35265;&#30340;&#24517;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Catalytic Role Of Noise And Necessity Of Inductive Biases In The Emergence Of Compositional Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.06464
&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#21644;&#24402;&#32435;&#20559;&#35265;&#30340;&#20316;&#29992;&#20351;&#24471;&#32452;&#21512;&#24335;&#27807;&#36890;&#33258;&#21457;&#20135;&#29983;&#65292;&#24182;&#19988;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#22122;&#22768;&#26377;&#21161;&#20110;&#20419;&#36827;&#32452;&#21512;&#24615;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27807;&#36890;&#26159;&#32452;&#21512;&#24335;&#30340;&#65292;&#22914;&#26524;&#22797;&#26434;&#20449;&#21495;&#21487;&#20197;&#34920;&#31034;&#20026;&#36739;&#31616;&#21333;&#23376;&#37096;&#20998;&#30340;&#32452;&#21512;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#35757;&#32451;&#26694;&#26550;&#21644;&#25968;&#25454;&#19978;&#30340;&#24402;&#32435;&#20559;&#35265;&#23545;&#20110;&#21457;&#23637;&#32452;&#21512;&#24335;&#27807;&#36890;&#26159;&#24517;&#35201;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#32452;&#21512;&#24615;&#20250;&#22312;&#20449;&#21495;&#21338;&#24328;&#20013;&#33258;&#21457;&#20986;&#29616;&#65292;&#20195;&#29702;&#22312;&#22024;&#26434;&#36890;&#36947;&#19978;&#20256;&#36755;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#19968;&#31995;&#21015;&#22122;&#22768;&#27700;&#24179;&#65288;&#21462;&#20915;&#20110;&#27169;&#22411;&#21644;&#25968;&#25454;&#65289;&#30830;&#23454;&#20419;&#36827;&#20102;&#32452;&#21512;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24182;&#25253;&#21578;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#32452;&#21512;&#24615;&#24230;&#37327;&#32467;&#26524;&#65306;&#25299;&#25169;&#30456;&#20284;&#24615;&#12289;&#20914;&#31361;&#35745;&#25968;&#21644;&#19978;&#19979;&#25991;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.06464v2 Announce Type: replace-cross  Abstract: Communication is compositional if complex signals can be represented as a combination of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. We experimentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#22806;&#37096;&#35760;&#24518;&#23384;&#20648;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#24182;&#29992;&#20110;&#35299;&#37322;&#20998;&#31867;&#36755;&#20986;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#30456;&#20851;&#35299;&#37322;&#19988;&#20445;&#25345;&#25110;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2110.00125</link><description>&lt;p&gt;
&#23558;&#21464;&#21387;&#22120;&#19982;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Combining Transformers with Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.00125
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#22806;&#37096;&#35760;&#24518;&#23384;&#20648;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#24182;&#29992;&#20110;&#35299;&#37322;&#20998;&#31867;&#36755;&#20986;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#30456;&#20851;&#35299;&#37322;&#19988;&#20445;&#25345;&#25110;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#38656;&#35201;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25104;&#21151;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#21253;&#25324;&#21464;&#21387;&#22120;&#65292;&#20173;&#28982;&#32570;&#20047;&#26377;&#25928;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#24314;&#31435;&#35299;&#37322;&#21487;&#33021;&#30340;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#20381;&#36182;&#24314;&#31435;&#26469;&#33258;&#39046;&#22495;&#30693;&#35782;&#30340;&#35299;&#37322;&#65292;&#36825;&#20123;&#30693;&#35782;&#36890;&#24120;&#20197;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#24418;&#24335;&#23384;&#22312;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#21040;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#35760;&#24518;&#26469;&#23384;&#20648;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#35299;&#37322;&#20998;&#31867;&#36755;&#20986;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#39046;&#22495;&#65292;&#27861;&#24459;&#25991;&#26412;&#20998;&#26512;&#21644;&#35770;&#25454;&#25366;&#25496;&#65292;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#20197;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#30456;&#20851;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#29978;&#33267;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.00125v3 Announce Type: replace-cross  Abstract: Many NLP applications require models to be interpretable. However, many successful neural architectures, including transformers, still lack effective interpretation methods. A possible solution could rely on building explanations from domain knowledge, which is often available as plain, natural language text. We thus propose an extension to transformer models that makes use of external memories to store natural language explanations and use them to explain classification outputs. We conduct an experimental evaluation on two domains, legal text analysis and argument mining, to show that our approach can produce relevant explanations while retaining or even improving classification performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23376;&#30446;&#26631;&#25628;&#32034;&#65288;kSubS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#20135;&#29983;&#22810;&#26679;&#24615;&#30340;&#23376;&#30446;&#26631;&#65292;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#24182;&#22312;Sokoban&#12289;&#39764;&#26041;&#21644;&#19981;&#31561;&#24335;&#35777;&#26126;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2108.11204</link><description>&lt;p&gt;
&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Subgoal Search For Complex Reasoning Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.11204
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23376;&#30446;&#26631;&#25628;&#32034;&#65288;kSubS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#20135;&#29983;&#22810;&#26679;&#24615;&#30340;&#23376;&#30446;&#26631;&#65292;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#24182;&#22312;Sokoban&#12289;&#39764;&#26041;&#21644;&#19981;&#31561;&#24335;&#35777;&#26126;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25797;&#38271;&#36890;&#36807;&#20174;&#19968;&#20010;&#24819;&#27861;&#31227;&#21160;&#21040;&#30456;&#20851;&#30340;&#24819;&#27861;&#30340;&#24605;&#32500;&#36807;&#31243;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23376;&#30446;&#26631;&#25628;&#32034;&#65288;kSubS&#65289;&#26041;&#27861;&#12290;&#20854;&#20851;&#38190;&#32452;&#20214;&#26159;&#19968;&#20010;&#23398;&#20064;&#30340;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#65292;&#20135;&#29983;&#22810;&#26679;&#24615;&#30340;&#26082;&#21487;&#23454;&#29616;&#21448;&#25509;&#36817;&#35299;&#20915;&#26041;&#26696;&#30340;&#23376;&#30446;&#26631;&#12290;&#20351;&#29992;&#23376;&#30446;&#26631;&#21487;&#20197;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#24341;&#20837;&#36866;&#21512;&#39640;&#25928;&#35268;&#21010;&#30340;&#39640;&#32423;&#25628;&#32034;&#22270;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#23376;&#30446;&#26631;&#27169;&#22359;&#32467;&#21512;&#32463;&#20856;&#30340;&#26368;&#20339;&#20248;&#20808;&#25628;&#32034;&#26694;&#26550;&#26469;&#23454;&#29616;kSubS&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#29983;&#25104;&#31532;$k$&#27493;&#23376;&#30446;&#26631;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#25928;&#29575;&#65306;&#20004;&#20010;&#27969;&#34892;&#30340;&#30410;&#26234;&#28216;&#25103;Sokoban&#21644;&#39764;&#26041;&#20197;&#21450;&#19981;&#31561;&#24335;&#35777;&#26126;&#22522;&#20934;INT&#12290;kSubS&#22312;&#36866;&#24230;&#30340;&#35745;&#31639;&#39044;&#31639;&#20869;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#22312;INT&#19978;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.11204v3 Announce Type: replace  Abstract: Humans excel in solving complex reasoning tasks through a mental process of moving from one idea to a related one. Inspired by this, we propose Subgoal Search (kSubS) method. Its key component is a learned subgoal generator that produces a diversity of subgoals that are both achievable and closer to the solution. Using subgoals reduces the search space and induces a high-level search graph suitable for efficient planning. In this paper, we implement kSubS using a transformer-based subgoal module coupled with the classical best-first search framework. We show that a simple approach of generating $k$-th step ahead subgoals is surprisingly efficient on three challenging domains: two popular puzzle games, Sokoban and the Rubik's Cube, and an inequality proving benchmark INT. kSubS achieves strong results including state-of-the-art on INT within a modest computational budget.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#27169;&#25311;&#24378;&#21270;&#23398;&#20064;&#21644;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#25511;&#21046;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#27169;&#25311;&#21040;&#30495;&#23454;&#31574;&#30053;&#36716;&#31227;&#65292;&#24182;&#20998;&#26512;&#20102;&#35774;&#35745;&#20915;&#31574;&#23545;&#30495;&#23454;&#19990;&#30028;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/1911.12905</link><description>&lt;p&gt;
&#27169;&#25311;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Simulation-based reinforcement learning for real-world autonomous driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1911.12905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#27169;&#25311;&#24378;&#21270;&#23398;&#20064;&#21644;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#25511;&#21046;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#27169;&#25311;&#21040;&#30495;&#23454;&#31574;&#30053;&#36716;&#31227;&#65292;&#24182;&#20998;&#26512;&#20102;&#35774;&#35745;&#20915;&#31574;&#23545;&#30495;&#23454;&#19990;&#30028;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#27169;&#25311;&#24378;&#21270;&#23398;&#20064;&#26469;&#33719;&#24471;&#25511;&#21046;&#20840;&#23610;&#23544;&#30495;&#23454;&#19990;&#30028;&#36710;&#36742;&#30340;&#39550;&#39542;&#31995;&#32479;&#12290;&#39550;&#39542;&#31574;&#30053;&#20197;&#26469;&#33258;&#21333;&#20010;&#25668;&#20687;&#22836;&#30340;RGB&#22270;&#20687;&#21450;&#20854;&#35821;&#20041;&#20998;&#21106;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#20027;&#35201;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#21482;&#26377;&#22312;&#20998;&#21106;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#25165;&#20986;&#29616;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30830;&#35748;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#31574;&#30053;&#36716;&#31227;&#12290;&#22522;&#20110;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20851;&#20110;&#24863;&#30693;&#12289;&#25511;&#21046;&#21644;&#35757;&#32451;&#30340;&#35774;&#35745;&#20915;&#31574;&#22914;&#20309;&#24433;&#21709;&#30495;&#23454;&#19990;&#30028;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1911.12905v4 Announce Type: replace-cross  Abstract: We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.   Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.   In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;</title><link>http://arxiv.org/abs/2401.15269</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25913;&#21892;&#21307;&#30103;&#25512;&#29702;&#33021;&#21147;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19987;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;GPT-4&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#20174;&#22810;&#39033;&#36873;&#25321;&#39064;&#21040;&#38271;&#31687;&#29983;&#25104;&#31561;&#22810;&#26679;&#21270;&#25361;&#25112;&#30340;&#37324;&#31243;&#30865;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#32534;&#30721;&#30693;&#35782;&#26080;&#27861;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#25628;&#32034;&#25991;&#26723;&#24182;&#26080;&#26465;&#20214;&#25110;&#26377;&#36873;&#25321;&#22320;&#23558;&#20854;&#38468;&#21152;&#21040;LLMs&#30340;&#36755;&#20837;&#26469;&#36827;&#34892;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#26102;&#65292;&#20986;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#33719;&#21462;&#19981;&#27491;&#30830;&#30340;&#25991;&#26723;&#25110;&#20570;&#20986;&#19981;&#20934;&#30830;&#30340;&#21028;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#21307;&#23398;&#25991;&#26412;&#26694;&#26550;Self-BioRAG&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#21644;&#33258;&#25105;&#21453;&#24605;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;84k&#20010;&#32463;&#36807;&#36807;&#28388;&#30340;&#29983;&#29289;&#21307;&#23398;&#25351;&#20196;&#38598;&#26469;&#35757;&#32451;Self-BioRAG&#65292;&#23427;&#20855;&#22791;&#35780;&#20272;&#33258;&#24049;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
&lt;/p&gt;</description></item><item><title>MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13201</link><description>&lt;p&gt;
MLLMReID: &#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13201
&lt;/p&gt;
&lt;p&gt;
MLLMReID&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#29289;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#24182;&#23558;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#24178;&#36827;&#34892;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;MLLM&#22312;ReID&#20219;&#21153;&#20013;&#30340;&#35774;&#35745;&#25351;&#20196;&#21644;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20154;&#29289;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;&#22914;&#20309;&#23558;&#23427;&#20204;&#36866;&#24212;&#20110;ReID&#20219;&#21153;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#24819;&#27861;&#26159;&#20351;&#29992;ReID&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20316;&#20026;ReID&#30340;&#20027;&#24178;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#20004;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20026;ReID&#35774;&#35745;&#25351;&#20196;&#26102;&#65292;MLLM&#21487;&#33021;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#25351;&#20196;&#65292;&#32780;&#35774;&#35745;&#21508;&#31181;&#25351;&#20196;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#25104;&#26412;&#12290;&#65288;2&#65289;LLM&#30340;&#28508;&#22312;&#22270;&#20687;&#29305;&#24449;&#21521;&#37327;&#27809;&#26377;&#21442;&#19982;&#25439;&#22833;&#35745;&#31639;&#12290;&#25351;&#20196;&#23398;&#20064;&#65292;&#23545;&#40784;&#22270;&#20687;-&#25991;&#26412;&#29305;&#24449;&#65292;&#23548;&#33268;&#38388;&#25509;&#20248;&#21270;&#21644;&#23398;&#20064;&#30446;&#26631;&#19981;&#20805;&#20998;&#21033;&#29992;&#29305;&#24449;&#65292;&#38480;&#21046;&#20102;&#20154;&#29289;&#29305;&#24449;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MLLMReID&#65306;&#22522;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;ReID&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#20849;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instru
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12255</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#23545;&#27169;&#22411;&#36827;&#34892;&#25351;&#32441;&#35782;&#21035;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#25104;&#20026;&#24517;&#35201;&#65292;&#36890;&#36807;&#25152;&#26377;&#26435;&#35748;&#35777;&#24182;&#30830;&#20445;&#19979;&#28216;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#65288;&#22914;&#38480;&#21046;&#21830;&#19994;&#20351;&#29992;&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#25351;&#32441;&#35782;&#21035;&#30340;&#35797;&#28857;&#30740;&#31350;&#65292;&#20316;&#20026;&#19968;&#31181;&#38750;&#24120;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#24418;&#24335;&#12290;&#27169;&#22411;&#21457;&#24067;&#32773;&#25351;&#23450;&#19968;&#20010;&#26426;&#23494;&#30340;&#31169;&#38053;&#65292;&#24182;&#23558;&#20854;&#26893;&#20837;&#20026;&#19968;&#20010;&#25351;&#20196;&#21518;&#38376;&#65292;&#24403;&#23494;&#38053;&#23384;&#22312;&#26102;&#65292;&#23548;&#33268;LLM&#29983;&#25104;&#29305;&#23450;&#30340;&#25991;&#26412;&#12290;&#23545;11&#20010;&#24120;&#29992;LLMs&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36731;&#37327;&#32423;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#12290;&#23427;&#36824;&#21487;&#20197;&#38450;&#27490;&#21457;&#24067;&#32773;&#36807;&#24230;&#23459;&#31216;&#65292;&#23545;&#25351;&#32441;&#29468;&#27979;&#21644;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#31867;&#20284;&#20110;MIT&#35768;&#21487;&#35777;&#30340;&#22810;&#38454;&#27573;&#25351;&#32441;&#35782;&#21035;&#12290;&#20195;&#30721;&#21487;&#22312;https://cnut1648.github.io/Model-Fingerprint/&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10831</link><description>&lt;p&gt;
&#36890;&#36807;&#36890;&#29992;&#27010;&#24565;&#21457;&#29616;&#29702;&#35299;&#35270;&#39057;Transformer
&lt;/p&gt;
&lt;p&gt;
Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35797;&#22270;&#35299;&#37322;&#22522;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#39640;&#23618;&#26102;&#31354;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20197;&#24448;&#20851;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#20165;&#38598;&#20013;&#22312;&#22270;&#20687;&#32423;&#20219;&#21153;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35270;&#39057;&#27169;&#22411;&#22788;&#29702;&#20102;&#39069;&#22806;&#30340;&#26102;&#38388;&#32500;&#24230;&#65292;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#21160;&#24577;&#27010;&#24565;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;(VTCD)&#31639;&#27861;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21333;&#20803;&#65288;&#27010;&#24565;&#65289;&#24182;&#23545;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#21517;&#12290;&#24471;&#21040;&#30340;&#27010;&#24565;&#20855;&#26377;&#24456;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21019;&#24314;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#29616;&#35937;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#21644;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05827</link><description>&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hallucination Benchmark in Medical Visual Question Answering. (arXiv:2401.05827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05827
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21019;&#24314;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#29616;&#35937;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#21644;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#65288;Med-VQA&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#26174;&#31034;&#20986;&#20102;&#20026;&#21307;&#30103;&#25552;&#20379;&#26377;&#25928;&#35270;&#35273;&#21161;&#25163;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#19978;&#24182;&#27809;&#26377;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21307;&#23398;&#22270;&#20687;&#37197;&#23545;&#38382;&#39064;-&#22238;&#31572;&#38598;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of large language and vision models on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models limitations and reveals the effectiveness of various prompting strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#28155;&#21152;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#35299;&#37322;&#27169;&#22359;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#21516;&#26102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22359;&#26469;&#21306;&#20998;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04647</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25512;&#36827;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks. (arXiv:2401.04647v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#28155;&#21152;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#35299;&#37322;&#27169;&#22359;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#21516;&#26102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22359;&#26469;&#21306;&#20998;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#28155;&#21152;&#21040;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35299;&#37322;&#27169;&#22359;&#34987;&#20248;&#21270;&#20197;&#20174;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22359;&#21017;&#26088;&#22312;&#21306;&#20998;&#20174;&#27010;&#24565;&#20013;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#36825;&#31181;&#32852;&#21512;&#35757;&#32451;&#26041;&#26696;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23558;&#20854;&#20869;&#37096;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#20154;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#23646;&#24615;&#38544;&#24335;&#22320;&#23545;&#40784;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20135;&#29983;&#20102;&#36830;&#36143;&#30340;&#27010;&#24565;&#28608;&#27963;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#21327;&#35758;&#20013;&#30340;&#25200;&#21160;&#23545;&#20998;&#31867;&#21644;&#27010;&#24565;&#33719;&#21462;&#30340;&#24433;&#21709;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25512;&#36827;&#20102;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel concept learning framework for enhancing model interpretability and performance in visual classification tasks. Our approach appends an unsupervised explanation generator to the primary classifier network and makes use of adversarial training. During training, the explanation module is optimized to extract visual concepts from the classifier's latent representations, while the GAN-based module aims to discriminate images generated from concepts, from true images. This joint training scheme enables the model to implicitly align its internally learned concepts with human-interpretable visual properties. Comprehensive experiments demonstrate the robustness of our approach, while producing coherent concept activations. We analyse the learned concepts, showing their semantic concordance with object parts and visual attributes. We also study how perturbations in the adversarial training protocol impact both classification and concept acquisition. In summary, this 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#24494;&#35843;&#29616;&#24577;&#36319;&#36394;&#22120;&#65292;&#21487;&#20197;&#25552;&#39640;&#36861;&#36394;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.12433</link><description>&lt;p&gt;
&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tracking Any Object Amodally. (arXiv:2312.12433v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#24494;&#35843;&#29616;&#24577;&#36319;&#36394;&#22120;&#65292;&#21487;&#20197;&#25552;&#39640;&#36861;&#36394;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#29616;&#24577;&#24863;&#30693;&#26159;&#19968;&#31181;&#20174;&#37096;&#20998;&#21487;&#35265;&#24615;&#20013;&#29702;&#35299;&#23436;&#25972;&#29289;&#20307;&#32467;&#26500;&#30340;&#22522;&#26412;&#25216;&#33021;&#65292;&#23427;&#23545;&#20110;&#23156;&#20799;&#29978;&#33267;&#26159;&#25104;&#20154;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#23427;&#30340;&#37325;&#35201;&#24615;&#24310;&#20280;&#21040;&#20102;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#39046;&#22495;&#65292;&#23545;&#20110;&#29702;&#35299;&#37325;&#21472;&#29289;&#20307;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#31639;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#19968;&#20851;&#38190;&#33021;&#21147;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#20351;&#29992;&#30340;&#26159;&#29616;&#24577;&#26631;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#29616;&#24577;&#25968;&#25454;&#30340;&#21294;&#20047;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TAO-Amodal&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;880&#20010;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#21487;&#35265;&#21644;&#36974;&#25377;&#23545;&#35937;&#30340;&#38750;&#29616;&#24577;&#21644;&#29616;&#24577;&#36793;&#30028;&#26694;&#65292;&#21253;&#25324;&#37096;&#20998;&#36229;&#20986;&#30011;&#38754;&#33539;&#22260;&#30340;&#29289;&#20307;&#12290;&#20026;&#20102;&#22686;&#24378;&#38750;&#29616;&#24577;&#36861;&#36394;&#30340;&#30446;&#26631;&#27704;&#20037;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#21363;&#38750;&#29616;&#24577;&#25193;&#23637;&#22120;&#65292;&#36890;&#36807;&#23545;&#20960;&#30334;&#20010;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#24494;&#35843;&#65292;&#23558;&#26631;&#20934;&#30340;&#29616;&#24577;&#36319;&#36394;&#22120;&#36716;&#21270;&#20026;&#38750;&#29616;&#24577;&#36319;&#36394;&#22120;&#12290;&#25105;&#20204;&#21462;&#24471;&#20102;3.3&#65285;&#21644;1.6&#65285;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amodal perception, the ability to comprehend complete object structures from partial visibility, is a fundamental skill, even for infants. Its significance extends to applications like autonomous driving, where a clear understanding of heavily occluded objects is essential. However, modern detection and tracking algorithms often overlook this critical capability, perhaps due to the prevalence of modal annotations in most datasets. To address the scarcity of amodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse categories in thousands of video sequences. Our dataset includes amodal and modal bounding boxes for visible and occluded objects, including objects that are partially out-of-frame. To enhance amodal tracking with object permanence, we leverage a lightweight plug-in module, the amodal expander, to transform standard, modal trackers into amodal ones through fine-tuning on a few hundred video sequences with data augmentation. We achieve a 3.3\% and 1.6\% improve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;AI&#21103;&#39550;&#39542;&#21592;&#26469;&#23548;&#33322;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#65292;&#24182;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#36741;&#21161;&#20195;&#29702;&#30340;&#20986;&#29616;&#23545;&#20110;&#25903;&#25345;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#30340;&#28508;&#21147;&#21644;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01235</link><description>&lt;p&gt;
&#20351;&#29992;AI&#21103;&#39550;&#39542;&#21592;&#23548;&#33322;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Navigating Complex Search Tasks with AI Copilots. (arXiv:2311.01235v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01235
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;AI&#21103;&#39550;&#39542;&#21592;&#26469;&#23548;&#33322;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#65292;&#24182;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#36741;&#21161;&#20195;&#29702;&#30340;&#20986;&#29616;&#23545;&#20110;&#25903;&#25345;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#30340;&#28508;&#21147;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#22914;&#20449;&#24687;&#26816;&#32034;(IR)&#30740;&#31350;&#30028;&#30340;&#35768;&#22810;&#20154;&#25152;&#30693;&#21644;&#27427;&#36175;&#30340;&#37027;&#26679;&#65292;&#25628;&#32034;&#36828;&#26410;&#35299;&#20915;&#12290;&#27599;&#22825;&#37117;&#26377;&#25968;&#30334;&#19975;&#20154;&#22312;&#25628;&#32034;&#24341;&#25806;&#19978;&#38754;&#23545;&#20219;&#21153;&#30340;&#22256;&#38590;&#12290;&#20182;&#20204;&#30340;&#22256;&#38590;&#36890;&#24120;&#19982;&#20219;&#21153;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#20197;&#21450;&#25628;&#32034;&#31995;&#32479;&#26080;&#27861;&#23436;&#20840;&#29702;&#35299;&#20219;&#21153;&#21644;&#25552;&#20379;&#30456;&#20851;&#32467;&#26524;&#26377;&#20851;&#12290;&#20219;&#21153;&#28608;&#21457;&#20102;&#25628;&#32034;&#65292;&#21019;&#24314;&#20102;&#25628;&#32034;&#32773;&#23581;&#35797;&#36830;&#25509;/&#35299;&#20915;&#30340;&#24046;&#36317;/&#38382;&#39064;&#24773;&#20917;&#65292;&#24182;&#22312;&#20182;&#20204;&#22788;&#29702;&#19981;&#21516;&#20219;&#21153;&#26041;&#38754;&#26102;&#39537;&#21160;&#25628;&#32034;&#34892;&#20026;&#12290;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#38656;&#35201;&#30340;&#19981;&#20165;&#26159;&#22522;&#26412;&#20107;&#23454;&#26597;&#25214;&#25110;&#25628;&#32034;&#30340;&#25903;&#25345;&#12290;&#25903;&#25345;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#30740;&#31350;&#21253;&#25324;&#29983;&#25104;&#26597;&#35810;&#21644;&#32593;&#31449;&#24314;&#35758;&#65292;&#20010;&#24615;&#21270;&#21644;&#19978;&#19979;&#25991;&#21270;&#25628;&#32034;&#65292;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25628;&#32034;&#20307;&#39564;&#65292;&#21253;&#25324;&#36328;&#26102;&#38388;&#21644;&#31354;&#38388;&#12290;&#26368;&#36817;&#20852;&#36215;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;(AI)&#21644;&#22522;&#20110;&#35813;&#25216;&#26415;&#30340;&#36741;&#21161;&#20195;&#29702;&#65292;&#25110;&#32773;&#35828;&#21103;&#39550;&#39542;&#21592;&#65292;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
As many of us in the information retrieval (IR) research community know and appreciate, search is far from being a solved problem. Millions of people struggle with tasks on search engines every day. Often, their struggles relate to the intrinsic complexity of their task and the failure of search systems to fully understand the task and serve relevant results. The task motivates the search, creating the gap/problematic situation that searchers attempt to bridge/resolve and drives search behavior as they work through different task facets. Complex search tasks require more than support for rudimentary fact finding or re-finding. Research on methods to support complex tasks includes work on generating query and website suggestions, personalizing and contextualizing search, and developing new search experiences, including those that span time and space. The recent emergence of generative artificial intelligence (AI) and the arrival of assistive agents, or copilots, based on this technology
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01230</link><description>&lt;p&gt;
&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#25512;&#23548;
&lt;/p&gt;
&lt;p&gt;
Multi-Operational Mathematical Derivations in Latent Space. (arXiv:2311.01230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#22810;&#25805;&#20316;&#34920;&#31034;&#33539;&#24335;&#65292;&#23558;&#25968;&#23398;&#36816;&#31639;&#24314;&#27169;&#20026;&#26174;&#24335;&#30340;&#20960;&#20309;&#21464;&#25442;&#12290;&#36890;&#36807;&#21033;&#29992;&#31526;&#21495;&#24341;&#25806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;61K&#20010;&#21069;&#25552;&#21644;6&#20010;&#36816;&#31639;&#31526;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#20102;&#27599;&#20010;&#33539;&#24335;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#26102;&#30340;&#24615;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22914;&#20309;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#65292;&#24182;&#25506;&#35752;&#20102;&#23398;&#20064;&#19981;&#21516;&#36816;&#31639;&#31526;&#21644;&#22312;&#21333;&#20010;&#36816;&#31639;&#20013;&#19987;&#38376;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#25903;&#25345;&#22810;&#27493;&#25512;&#23548;&#21644;&#36229;&#36234;&#20998;&#24067;&#24191;&#20041;&#21270;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#22810;&#25805;&#20316;&#33539;&#24335;&#23545;&#20110;&#35299;&#24320;&#19981;&#21516;&#36816;&#31639;&#31526;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21516;&#26102;&#21487;&#20197;&#21306;&#20998;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the possibility of approximating multiple mathematical operations in latent space for expression derivation. To this end, we introduce different multi-operational representation paradigms, modelling mathematical operations as explicit geometric transformations. By leveraging a symbolic engine, we construct a large-scale dataset comprising 1.7M derivation steps stemming from 61K premises and 6 operators, analysing the properties of each paradigm when instantiated with state-of-the-art neural encoders. Specifically, we investigate how different encoding mechanisms can approximate equational reasoning in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation. Our empirical analysis reveals that the multi-operational paradigm is crucial for disentangling different operators, while discriminating the conclusion
&lt;/p&gt;</description></item><item><title>BioImage.IO Chatbot &#26159;&#19968;&#20010;&#26681;&#25454;&#29992;&#25143;&#20010;&#24615;&#21270;&#38656;&#27714;&#25552;&#20379;&#31572;&#26696;&#30340;AI&#32842;&#22825;&#21161;&#25163;&#65292;&#36890;&#36807;&#27719;&#38598;&#21644;&#35299;&#37322;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#24037;&#20855;&#25991;&#26723;&#21644;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#20026;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#20351;&#29992;&#32773;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#65292;&#20026;&#21487;&#35775;&#38382;&#30340;&#31185;&#23398;&#30740;&#31350;&#35774;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.18351</link><description>&lt;p&gt;
BioImage.IO Chatbot: &#19968;&#20010;&#20197;&#31038;&#21306;&#30693;&#35782;&#24211;&#22686;&#24378;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#20010;&#20154;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
BioImage.IO Chatbot: A Personalized Assistant for BioImage Analysis Augmented by Community Knowledge Base. (arXiv:2310.18351v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18351
&lt;/p&gt;
&lt;p&gt;
BioImage.IO Chatbot &#26159;&#19968;&#20010;&#26681;&#25454;&#29992;&#25143;&#20010;&#24615;&#21270;&#38656;&#27714;&#25552;&#20379;&#31572;&#26696;&#30340;AI&#32842;&#22825;&#21161;&#25163;&#65292;&#36890;&#36807;&#27719;&#38598;&#21644;&#35299;&#37322;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#24037;&#20855;&#25991;&#26723;&#21644;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#20026;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#20351;&#29992;&#32773;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#65292;&#20026;&#21487;&#35775;&#38382;&#30340;&#31185;&#23398;&#30740;&#31350;&#35774;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25193;&#23637;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#26223;&#35266;&#32473;&#19987;&#23478;&#21644;&#26032;&#26469;&#32773;&#37117;&#24102;&#26469;&#20102;&#23548;&#33322;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#25628;&#32034;&#26041;&#27861;&#22312;&#36825;&#20010;&#22797;&#26434;&#29615;&#22659;&#20013;&#24120;&#24120;&#26080;&#27861;&#25552;&#20379;&#24110;&#21161;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioImage.IO Chatbot&#65292;&#19968;&#20010;&#20026;&#29983;&#29289;&#22270;&#20687;&#31038;&#21306;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23545;&#35805;&#21161;&#25163;&#12290;&#36825;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#24314;&#31435;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#32858;&#21512;&#21644;&#35299;&#37322;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#29305;&#23450;&#24037;&#20855;&#25991;&#26723;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#31572;&#26696;&#12290;&#36890;&#36807;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#32463;&#36807;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;BioImage.IO Chatbot &#19981;&#20165;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20114;&#21160;&#65292;&#36824;&#25552;&#20379;&#20016;&#23500;&#30340;&#30693;&#35782;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#12290;&#23427;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#29983;&#29289;&#23398;&#23478;&#12289;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24072;&#21644;&#24320;&#21457;&#32773;&#23548;&#33322;&#21644;&#21033;&#29992;&#20808;&#36827;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#26041;&#24335;&#65292;&#20026;&#31038;&#21306;&#39537;&#21160;&#30340;&#21487;&#35775;&#38382;&#31185;&#23398;&#30740;&#31350;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly expanding landscape of bioimage analysis tools presents a navigational challenge for both experts and newcomers. Traditional search methods often fall short in assisting users in this complex environment. To address this, we introduce the BioImage$.$IO Chatbot, an AI-driven conversational assistant tailored for the bioimage community. Built upon large language models, this chatbot provides personalized, context-aware answers by aggregating and interpreting information from diverse databases, tool-specific documentation, and structured data sources. Enhanced by a community-contributed knowledge base and fine-tuned retrieval methods, the BioImage$.$IO Chatbot offers not just a personalized interaction but also a knowledge-enriched, context-aware experience. It fundamentally transforms the way biologists, bioimage analysts, and developers navigate and utilize advanced bioimage analysis tools, setting a new standard for community-driven, accessible scientific research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14814</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#23545;&#27169;&#22411;&#33258;&#20449;&#24230;&#39640;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#24120;&#20351;&#29992;softmax&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#33258;&#20449;&#24230;&#24230;&#37327;&#65292;&#23613;&#31649;&#24050;&#30693;&#23427;&#20204;&#23545;&#38169;&#35823;&#39044;&#27979;&#20063;&#36807;&#20110;&#33258;&#20449;&#12290;&#24403;&#25968;&#25454;&#26631;&#27880;&#21463;&#21040;&#26576;&#31181;&#32422;&#26463;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20026;&#26126;&#26174;&#65292;&#21363;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;$\mathcal{T}$-&#30456;&#20284;&#24230;&#65292;&#23427;&#22522;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#31283;&#23450;&#28857;&#24182;&#25551;&#36848;&#21333;&#20010;&#25104;&#21592;&#30340;&#22810;&#26679;&#24615;&#19982;&#20854;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25552;&#20379;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#33258;&#20449;&#24230;&#24230;&#37327;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#26041;&#27861;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10833</link><description>&lt;p&gt;
&#36866;&#24403;&#30340;Laplacian&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Proper Laplacian Representation Learning. (arXiv:2310.10833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#26041;&#27861;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#23398;&#20064;&#29366;&#24577;&#30340;&#33391;&#22909;&#34920;&#31034;&#23545;&#20110;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;Laplacian&#34920;&#31034;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20869;&#22312;&#22870;&#21169;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26102;&#38388;&#24310;&#38271;&#30340;&#21160;&#20316;&#21457;&#29616;&#21644;&#22870;&#21169;&#22609;&#36896;&#65292;&#20197;&#21450;&#20449;&#24687;&#20016;&#23500;&#30340;&#29366;&#24577;&#32534;&#30721;&#12290;&#20026;&#20102;&#33719;&#24471;Laplacian&#34920;&#31034;&#65292;&#38656;&#35201;&#35745;&#31639;&#22270;Laplacian&#30340;&#29305;&#24449;&#31995;&#32479;&#65292;&#36825;&#36890;&#24120;&#36890;&#36807;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20860;&#23481;&#30340;&#20248;&#21270;&#30446;&#26631;&#36827;&#34892;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36817;&#20284;&#26041;&#27861;&#20381;&#36182;&#20110;&#26080;&#27861;&#39640;&#25928;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#65292;&#25910;&#25947;&#21040;&#25152;&#38656;&#29305;&#24449;&#21521;&#37327;&#30340;&#20219;&#24847;&#26059;&#36716;&#65292;&#24182;&#19988;&#26080;&#27861;&#31934;&#30830;&#22320;&#24674;&#22797;&#30456;&#24212;&#30340;&#29305;&#24449;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing intrinsic rewards for temporally-extended action discovery and reward shaping, and informative state encoding. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally reco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.09270</link><description>&lt;p&gt;
Retro-fallback: &#38754;&#21521;&#19981;&#30830;&#23450;&#19990;&#30028;&#30340;&#36870;&#21512;&#25104;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Retro-fallback: retrosynthetic planning in an uncertain world. (arXiv:2310.09270v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21512;&#25104;&#26159;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#21270;&#23398;&#21453;&#24212;&#20174;&#26356;&#31616;&#21333;&#12289;&#21487;&#36141;&#20080;&#30340;&#20998;&#23376;&#21019;&#24314;&#25152;&#38656;&#20998;&#23376;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#31639;&#27861;&#26469;&#23547;&#25214;&#19968;&#31995;&#21015;&#24230;&#37327;&#25351;&#26631;&#65288;&#20363;&#22914;&#26368;&#30701;&#36335;&#24452;&#12289;&#26368;&#20302;&#25104;&#26412;&#65289;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#24573;&#35270;&#20102;&#25105;&#20204;&#23545;&#21487;&#33021;&#21453;&#24212;&#31354;&#38388;&#30340;&#19981;&#23436;&#20840;&#20102;&#35299;&#65292;&#36825;&#24847;&#21619;&#30528;&#31639;&#27861;&#29983;&#25104;&#30340;&#35745;&#21010;&#21487;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#26080;&#27861;&#23454;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#36807;&#31243;&#30340;&#36870;&#21512;&#25104;&#26032;&#39062;&#34920;&#36848;&#65292;&#20197;&#32771;&#34385;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36138;&#23146;&#31639;&#27861;&#31216;&#20026; Retro-fallback&#65292;&#26368;&#22823;&#21270;&#33267;&#23569;&#26377;&#19968;&#31181;&#21512;&#25104;&#35745;&#21010;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#25191;&#34892;&#30340;&#27010;&#29575;&#12290;&#20351;&#29992;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126; Retro-fallback &#36890;&#24120;&#29983;&#25104;&#27604;&#27969;&#34892;&#30340; MCTS &#21644; retro* &#31639;&#27861;&#26356;&#22909;&#30340;&#19968;&#32452;&#21512;&#25104;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.
&lt;/p&gt;</description></item><item><title>CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16705</link><description>&lt;p&gt;
CReHate: &#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset. (arXiv:2308.16705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16705
&lt;/p&gt;
&lt;p&gt;
CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#25968;&#25454;&#38598;&#20027;&#35201;&#21453;&#26144;&#20102;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#25991;&#21270;&#20559;&#24046;&#12290;&#36825;&#22312;&#21463;&#20027;&#35266;&#24615;&#24433;&#21709;&#36739;&#22823;&#30340;&#20219;&#21153;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#29305;&#21035;&#26377;&#38382;&#39064;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#22914;&#20309;&#29702;&#35299;&#20167;&#24680;&#35328;&#35770;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CReHate&#65292;&#23545;&#25277;&#26679;&#30340;SBIC&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#27880;&#37322;&#65306;&#28595;&#22823;&#21033;&#20122;&#12289;&#26032;&#21152;&#22369;&#12289;&#21335;&#38750;&#12289;&#33521;&#22269;&#21644;&#32654;&#22269;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#22269;&#31821;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#21482;&#26377;59.4%&#30340;&#26679;&#26412;&#22312;&#25152;&#26377;&#22269;&#23478;&#20043;&#38388;&#36798;&#25104;&#20849;&#35782;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#22269;&#31821;&#30340;&#35266;&#28857;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20167;&#24680;&#35328;&#35770;&#30340;&#32454;&#24494;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
English datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. This is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. To delve into how individuals from different countries perceive hate speech, we introduce CReHate, a cross-cultural re-annotation of the sampled SBIC dataset. This dataset includes annotations from five distinct countries: Australia, Singapore, South Africa, the United Kingdom, and the United States. Our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. We also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. These findings underscore the need to re-evaluate certain aspects of NLP research, especially with regard to the nuanced nature of hate spe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#21487;&#35299;&#37322;&#27169;&#22411;&#20013;&#27010;&#29575;&#37325;&#24314;&#25968;&#25454;&#38598;&#12290;&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#37327;&#21270;&#20449;&#24687;&#27844;&#28431;&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15099</link><description>&lt;p&gt;
&#20174;&#21487;&#35299;&#37322;&#27169;&#22411;&#20013;&#23454;&#29616;&#27010;&#29575;&#25968;&#25454;&#38598;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Dataset Reconstruction from Interpretable Models. (arXiv:2308.15099v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#21487;&#35299;&#37322;&#27169;&#22411;&#20013;&#27010;&#29575;&#37325;&#24314;&#25968;&#25454;&#38598;&#12290;&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#37327;&#21270;&#20449;&#24687;&#27844;&#28431;&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#32463;&#24120;&#34987;&#35748;&#20026;&#26159;&#21487;&#20449;&#36182;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21644;&#21457;&#24067;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#20250;&#27844;&#38706;&#26377;&#20851;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#36825;&#31181;&#25259;&#38706;&#21487;&#33021;&#30452;&#25509;&#19982;&#38544;&#31169;&#20914;&#31361;&#65292;&#22240;&#27492;&#20934;&#30830;&#37327;&#21270;&#36825;&#31181;&#27844;&#28431;&#24102;&#26469;&#30340;&#38544;&#31169;&#24433;&#21709;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#21033;&#29992;&#20915;&#31574;&#26641;&#30340;&#32467;&#26500;&#26500;&#24314;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27010;&#29575;&#37325;&#24314;&#65292;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#20449;&#24687;&#27844;&#28431;&#30340;&#19968;&#20010;&#30456;&#20851;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#25512;&#24191;&#20102;&#36825;&#20123;&#27010;&#29575;&#37325;&#24314;&#65292;&#20351;&#20854;&#21487;&#20197;&#22788;&#29702;&#20854;&#20182;&#24418;&#24335;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#21644;&#26356;&#19968;&#33324;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#32467;&#26500;&#36827;&#34892;&#23454;&#38469;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is often pointed out as a key requirement for trustworthy machine learning. However, learning and releasing models that are inherently interpretable leaks information regarding the underlying training data. As such disclosure may directly conflict with privacy, a precise quantification of the privacy impact of such breach is a fundamental problem. For instance, previous work have shown that the structure of a decision tree can be leveraged to build a probabilistic reconstruction of its training dataset, with the uncertainty of the reconstruction being a relevant metric for the information leak. In this paper, we propose of a novel framework generalizing these probabilistic reconstructions in the sense that it can handle other forms of interpretable models and more generic types of knowledge. In addition, we demonstrate that under realistic assumptions regarding the interpretable models' structure, the uncertainty of the reconstruction can be computed efficiently. Final
&lt;/p&gt;</description></item><item><title>&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#31995;&#32479;&#24615;&#25925;&#38556;, &#21363;&#20351;&#21333;&#20010;&#27169;&#22411;&#22312;&#24635;&#20307;&#19978;&#30340;&#25913;&#21892;&#20063;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;</title><link>http://arxiv.org/abs/2307.05862</link><description>&lt;p&gt;
&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#25581;&#31034;&#20102;&#21516;&#36136;&#21270;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes. (arXiv:2307.05862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05862
&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#31995;&#32479;&#24615;&#25925;&#38556;, &#21363;&#20351;&#21333;&#20010;&#27169;&#22411;&#22312;&#24635;&#20307;&#19978;&#30340;&#25913;&#21892;&#20063;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#22312;&#27169;&#22411;&#23618;&#38754;&#36827;&#34892;&#30740;&#31350;&#65306;&#30740;&#31350;&#20154;&#21592;&#34913;&#37327;&#21644;&#25913;&#36827;&#29305;&#23450;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#20559;&#35265;&#12289;&#25928;&#29575;&#21644;&#20854;&#20182;&#32500;&#24230;&#12290;&#23454;&#38469;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#31038;&#20250;&#24433;&#21709;&#21462;&#20915;&#20110;&#26426;&#22120;&#23398;&#20064;&#37096;&#32626;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#65306;&#19981;&#26159;&#20998;&#26512;&#21333;&#20010;&#27169;&#22411;&#65292;&#32780;&#26159;&#32771;&#34385;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#37096;&#32626;&#30340;&#25152;&#26377;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#20363;&#22914;&#65292;&#22312;&#25307;&#32856;&#20013;&#36827;&#34892;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#24847;&#21619;&#30528;&#35748;&#35782;&#21040;&#19968;&#20010;&#27714;&#32844;&#32773;&#30340;&#32467;&#26524;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#21333;&#20010;&#25307;&#32856;&#31639;&#27861;&#25110;&#20844;&#21496;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#20182;&#20204;&#30003;&#35831;&#30340;&#25152;&#26377;&#20844;&#21496;&#30340;&#38598;&#20307;&#20915;&#31574;&#12290;&#22312;&#19977;&#31181;&#27169;&#24335;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35821;&#38899;&#65289;&#21644;11&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#36235;&#21183;&#65306;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#23481;&#26131;&#20986;&#29616;&#31995;&#32479;&#24615;&#25925;&#38556;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20123;&#29992;&#25143;&#34987;&#25152;&#26377;&#21487;&#29992;&#30340;&#27169;&#22411;&#38169;&#35823;&#20998;&#31867;&#12290;&#21363;&#20351;&#22312;&#20010;&#20307;&#27169;&#22411;&#38543;&#26102;&#38388;&#22312;&#24635;&#20307;&#27700;&#24179;&#19978;&#25913;&#21892;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments. To capture this, we introduce ecosystem-level analysis: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are not only determined by a single hiring algorithm or firm but instead by the collective decisions of all the firms they applied to. Across three modalities (text, images, speech) and 11 datasets, we establish a clear trend: deployed machine learning is prone to systemic failure, meaning some users are exclusively misclassified by all models available. Even when individual models improve at the population level over time, we fin
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#30340;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#23427;&#25552;&#20379;&#20102;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.01449</link><description>&lt;p&gt;
&#23558;&#23454;&#39564;&#25968;&#25454;&#19982;&#35266;&#27979;&#25968;&#25454;&#32467;&#21512;&#30340;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Double Machine Learning Approach to Combining Experimental and Observational Data. (arXiv:2307.01449v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01449
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#30340;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#23427;&#25552;&#20379;&#20102;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#36890;&#24120;&#30001;&#20110;&#26080;&#27861;&#27979;&#35797;&#30340;&#20551;&#35774;&#32780;&#32570;&#20047;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#20174;&#19994;&#20154;&#21592;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#36739;&#36731;&#30340;&#20551;&#35774;&#19979;&#27979;&#35797;&#22806;&#37096;&#25928;&#24230;&#21644;&#21487;&#24573;&#35270;&#24615;&#30340;&#36829;&#21453;&#24773;&#20917;&#12290;&#24403;&#21482;&#26377;&#19968;&#20010;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#24378;&#35843;&#20102;&#20934;&#30830;&#35782;&#21035;&#36829;&#21453;&#30340;&#20551;&#35774;&#23545;&#19968;&#33268;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one assumption is violated, we provide semi-parametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. We demonstrate the applicability of our approach in three real-world case studies, highlighting its relevance for practical settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15550</link><description>&lt;p&gt;
CamemBERT-bio&#65306;&#19968;&#31181;&#26356;&#20581;&#24247;&#30340;&#27861;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CamemBERT-bio: a Tasty French Language Model Better for your Health. (arXiv:2306.15550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20020;&#24202;&#25968;&#25454;&#20179;&#24211;&#65292;&#21307;&#38498;&#20013;&#30340;&#20020;&#24202;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#23481;&#26131;&#29992;&#20110;&#30740;&#31350;&#65292;&#28982;&#32780;&#36825;&#20123;&#25991;&#20214;&#37117;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20174;&#21307;&#30103;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#20020;&#24202;&#30740;&#31350;&#12290;&#20351;&#29992;CamemBERT&#31561;BERT-like&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20026;&#36890;&#29992;&#35821;&#35328;&#35757;&#32451;&#30340;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#25928;&#26524;&#36739;&#24369;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27861;&#35821;&#20844;&#20849;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#23545;CamemBERT&#36827;&#34892;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CamemBERT-bio&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#65292;&#23427;&#26159;&#19968;&#31181;&#20026;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#20844;&#20849;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical data in hospitals are increasingly accessible for research through clinical data warehouses, however these documents are unstructured. It is therefore necessary to extract information from medical reports to conduct clinical studies. Transfer learning with BERT-like models such as CamemBERT has allowed major advances, especially for named entity recognition. However, these models are trained for plain language and are less efficient on biomedical data. This is why we propose a new French public biomedical dataset on which we have continued the pre-training of CamemBERT. Thus, we introduce a first version of CamemBERT-bio, a specialized public model for the French biomedical domain that shows 2.54 points of F1 score improvement on average on different biomedical named entity recognition tasks.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.14710</link><description>&lt;p&gt;
&#35757;&#32451;&#25351;&#20196;&#20316;&#20026;&#21518;&#38376;: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#30340;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14710
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20854;&#30446;&#30340;&#26159;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35813;&#22521;&#35757;&#33539;&#20363;&#30456;&#20851;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#22312;&#25104;&#21315;&#19978;&#19975;&#30340;&#25968;&#25454;&#20013;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#65292;&#20415;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#27602;&#21270;&#26469;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#29978;&#33267;&#26080;&#38656;&#20462;&#25913;&#25968;&#25454;&#23454;&#20363;&#25110;&#26631;&#31614;&#26412;&#36523;&#12290;&#36890;&#36807;&#36825;&#31181;&#25351;&#20196;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#22235;&#20010;&#24120;&#29992;&#30340; NLP &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#36229;&#36807;90% &#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#24341;&#36215;&#26131;&#20110;&#36716;&#31227;&#21040; 15 &#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25345;&#20037;&#21518;&#38376;&#12290;&#36825;&#31181;&#25915;&#20987;&#36824;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#26377;&#27602;&#25351;&#20196;&#12290;&#26368;&#21518;&#65292;&#35813;&#25915;&#20987;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#25512;&#29702;&#26102;&#38450;&#24481;&#30340;&#25269;&#25239;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#38656;&#35201;&#26356;&#20026;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#36825;&#20010;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25910;&#38598;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#25512;&#36827;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.00553</link><description>&lt;p&gt;
&#20174;&#23396;&#31435;&#30340;&#23707;&#23679;&#21040;&#27867;&#22823;&#38470;&#65306;&#32479;&#19968;&#35821;&#20041;&#31354;&#38388;&#29992;&#20110;&#20154;&#31867;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding. (arXiv:2304.00553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#36825;&#20010;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25910;&#38598;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#25512;&#36827;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#29702;&#35299;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#24182;&#19988;&#22791;&#21463;&#20851;&#27880;&#12290;&#23427;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20174;&#34892;&#20026;&#30340;&#29289;&#29702;&#31354;&#38388;&#21040;&#35821;&#20041;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36890;&#24120;&#65292;&#30740;&#31350;&#20154;&#21592;&#20250;&#26681;&#25454;&#29420;&#29305;&#30340;&#36873;&#25321;&#26500;&#24314;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#20197;&#23450;&#20041;&#21508;&#31181;&#31867;&#21035;&#24182;&#23558;&#22522;&#20934;&#32447;&#25512;&#21521;&#26497;&#38480;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#21644;&#19981;&#21516;&#30340;&#31867;&#21035;&#31890;&#24230;&#65292;&#23601;&#20687;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#19968;&#26679;&#20114;&#19981;&#20860;&#23481;&#65292;&#20363;&#22914;&#25968;&#25454;&#38598;A&#20013;&#30340;&#23478;&#21153;&#21644;&#25968;&#25454;&#38598;B&#20013;&#30340;&#27927;&#30424;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#20010;&#26356;&#20855;&#21407;&#21017;&#24615;&#30340;&#35821;&#20041;&#31354;&#38388;&#26469;&#38598;&#20013;&#31038;&#21306;&#30340;&#21147;&#37327;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#19968;&#36215;&#20351;&#29992;&#25152;&#26377;&#25968;&#25454;&#38598;&#20197;&#36861;&#27714;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#32473;&#23450;&#21160;&#35789;&#20998;&#31867;&#23618;&#27425;&#32467;&#26500;&#24182;&#28085;&#30422;&#22823;&#37327;&#34892;&#20026;&#12290;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#25105;&#20204;&#30340;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25105;&#20204;&#23558;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#25910;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#20351;&#29992;&#32479;&#19968;&#30340;&#26631;&#31614;&#31995;&#32479;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#35821;&#20041;&#31354;&#38388;&#21644;&#32479;&#19968;&#25968;&#25454;&#24211;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action understanding matters and attracts attention. It can be formed as the mapping from the action physical space to the semantic space. Typically, researchers built action datasets according to idiosyncratic choices to define classes and push the envelope of benchmarks respectively. Thus, datasets are incompatible with each other like "Isolated Islands" due to semantic gaps and various class granularities, e.g., do housework in dataset A and wash plate in dataset B. We argue that a more principled semantic space is an urgent need to concentrate the community efforts and enable us to use all datasets together to pursue generalizable action learning. To this end, we design a Poincare action semantic space given verb taxonomy hierarchy and covering massive actions. By aligning the classes of previous datasets to our semantic space, we gather (image/video/skeleton/MoCap) datasets into a unified database in a unified label system, i.e., bridging "isolated islands" into a "Pangea". Accord
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#22312;&#32447;&#25511;&#21046;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#36866;&#24212;&#36873;&#25321;&#21551;&#21457;&#24335;&#31574;&#30053;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#65292;&#20197;&#33719;&#24471;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#65292;&#23545;&#24212;&#29992;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2211.00759</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#22312;&#32447;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Online Control of Adaptive Large Neighborhood Search using Deep Reinforcement Learning. (arXiv:2211.00759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#22312;&#32447;&#25511;&#21046;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#36866;&#24212;&#36873;&#25321;&#21551;&#21457;&#24335;&#31574;&#30053;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#65292;&#20197;&#33719;&#24471;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#65292;&#23545;&#24212;&#29992;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#65288;ALNS&#65289;&#31639;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65288;COPs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#25104;&#21151;&#12290;ALNS&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21508;&#31181;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#21183;&#26469;&#25214;&#21040;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#12290;&#28982;&#32780;&#65292;ALNS&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20854;&#36873;&#25321;&#21644;&#25509;&#21463;&#21442;&#25968;&#30340;&#27491;&#30830;&#37197;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#36873;&#25321;&#21551;&#21457;&#24335;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#22522;&#20110;&#25628;&#32034;&#30340;&#29366;&#24577;&#23398;&#20064;&#22914;&#20309;&#37197;&#32622;&#19979;&#19968;&#27425;ALNS&#36845;&#20195;&#20197;&#33719;&#24471;&#22909;&#30340;&#20248;&#21270;&#38382;&#39064;&#35299;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;&#21547;&#26377;&#38543;&#26426;&#26435;&#37325;&#21644;&#26102;&#38388;&#31383;&#21475;&#30340;&#23548;&#33322;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#38382;&#39064;&#29992;&#20110;IJCAI&#31454;&#36187;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26222;&#36890;&#30340;ALNS&#21644;&#20855;&#26377;&#40664;&#35748;&#21442;&#25968;&#35774;&#32622;&#30340;ALNS&#65292;&#23637;&#31034;&#20102;DRL&#26041;&#27861;&#22312;&#22312;&#32447;&#25511;&#21046;ALNS&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive Large Neighborhood Search (ALNS) algorithm has shown considerable success in solving complex combinatorial optimization problems (COPs). ALNS selects various heuristics adaptively during the search process, leveraging their strengths to find good solutions for optimization problems. However, the effectiveness of ALNS depends on the proper configuration of its selection and acceptance parameters. To address this limitation, we propose a Deep Reinforcement Learning (DRL) approach that selects heuristics, adjusts parameters, and controls the acceptance criteria during the search process. The proposed method aims to learn, based on the state of the search, how to configure the next iteration of the ALNS to obtain good solutions to the underlying optimization problem. We evaluate the proposed method on a time-dependent orienteering problem with stochastic weights and time windows, used in an IJCAI competition. The results show that our approach outperforms vanilla ALNS and ALNS
&lt;/p&gt;</description></item></channel></rss>