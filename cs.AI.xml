<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#36873;&#25321;&#20013;&#30340;&#24230;&#37327;&#25197;&#26354;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25237;&#31080;&#35268;&#21017;&#65292;&#35813;&#35268;&#21017;&#21487;&#20197;&#36873;&#25321;&#36317;&#31163;&#36873;&#27665;&#24179;&#22343;&#36317;&#31163;&#36739;&#23567;&#30340;&#20505;&#36873;&#20154;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#30830;&#23450;&#24615;&#25237;&#31080;&#35268;&#21017;&#30340;&#24230;&#37327;&#25197;&#26354;&#38480;&#21046;&#20026;3&#65292;&#20294;&#22312;&#26080;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#35813;&#38382;&#39064;&#20173;&#28982;&#20102;&#35299;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.17838</link><description>&lt;p&gt;
&#25171;&#30772;&#24230;&#37327;&#25237;&#31080;&#25197;&#26354;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Breaking the Metric Voting Distortion Barrier. (arXiv:2306.17838v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#36873;&#25321;&#20013;&#30340;&#24230;&#37327;&#25197;&#26354;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25237;&#31080;&#35268;&#21017;&#65292;&#35813;&#35268;&#21017;&#21487;&#20197;&#36873;&#25321;&#36317;&#31163;&#36873;&#27665;&#24179;&#22343;&#36317;&#31163;&#36739;&#23567;&#30340;&#20505;&#36873;&#20154;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#30830;&#23450;&#24615;&#25237;&#31080;&#35268;&#21017;&#30340;&#24230;&#37327;&#25197;&#26354;&#38480;&#21046;&#20026;3&#65292;&#20294;&#22312;&#26080;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#35813;&#38382;&#39064;&#20173;&#28982;&#20102;&#35299;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#31038;&#20250;&#36873;&#25321;&#20013;&#24230;&#37327;&#25197;&#26354;&#30340;&#32463;&#20856;&#38382;&#39064;&#12290;&#20551;&#35774;&#25105;&#20204;&#26377;&#19968;&#20010;&#36873;&#20030;&#65292;&#26377;n&#21517;&#36873;&#27665;&#21644;m&#21517;&#20505;&#36873;&#20154;&#65292;&#20182;&#20204;&#20301;&#20110;&#19968;&#20010;&#20849;&#20139;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#24076;&#26395;&#35774;&#35745;&#19968;&#20010;&#25237;&#31080;&#35268;&#21017;&#65292;&#36873;&#25321;&#19968;&#20010;&#24179;&#22343;&#36317;&#31163;&#36873;&#27665;&#36739;&#23567;&#30340;&#20505;&#36873;&#20154;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#19981;&#33021;&#30452;&#25509;&#33719;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#20449;&#24687;&#65292;&#27599;&#20010;&#36873;&#27665;&#21482;&#33021;&#32473;&#20986;&#20505;&#36873;&#20154;&#30340;&#25490;&#24207;&#21015;&#34920;&#12290;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#19968;&#26465;&#35268;&#21017;&#65292;&#26080;&#35770;&#36873;&#20030;&#23454;&#20363;&#21644;&#24213;&#23618;&#24230;&#37327;&#31354;&#38388;&#22914;&#20309;&#65292;&#37117;&#33021;&#36873;&#25321;&#20986;&#19968;&#20010;&#19982;&#30495;&#27491;&#26368;&#20248;&#35299;&#30340;&#20195;&#20215;&#21482;&#30456;&#24046;&#19968;&#20010;&#23567;&#22240;&#23376;&#65288;&#31216;&#20026;&#25197;&#26354;&#24230;&#65289;&#30340;&#20505;&#36873;&#20154;&#65311;&#35768;&#22810;&#30740;&#31350;&#30340;&#25104;&#26524;&#23558;&#30830;&#23450;&#24615;&#25237;&#31080;&#35268;&#21017;&#30340;&#24230;&#37327;&#25197;&#26354;&#38480;&#21046;&#20026;3&#65292;&#36825;&#26159;&#30830;&#23450;&#24615;&#35268;&#21017;&#21644;&#35768;&#22810;&#20854;&#20182;&#25237;&#31080;&#35268;&#21017;&#31867;&#21035;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#35813;&#38382;&#39064;&#20173;&#28982;&#20102;&#35299;&#26377;&#38480;&#65306;&#23613;&#31649;&#26368;&#20339;&#19979;&#30028;&#24050;&#32463;&#38477;&#20302;&#21040;2.112&#65292;&#20294;&#29616;&#26377;&#35268;&#21017;&#30340;&#25197;&#26354;&#24230;&#20173;&#28982;&#30456;&#23545;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the following well studied problem of metric distortion in social choice. Suppose we have an election with $n$ voters and $m$ candidates who lie in a shared metric space. We would like to design a voting rule that chooses a candidate whose average distance to the voters is small. However, instead of having direct access to the distances in the metric space, each voter gives us a ranked list of the candidates in order of distance. Can we design a rule that regardless of the election instance and underlying metric space, chooses a candidate whose cost differs from the true optimum by only a small factor (known as the distortion)?  A long line of work culminated in finding deterministic voting rules with metric distortion $3$, which is the best possible for deterministic rules and many other classes of voting rules. However, without any restrictions, there is still a significant gap in our understanding: Even though the best lower bound is substantially lower at $2.112$, the b
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#20248;&#21270;&#38382;&#39064;&#30340;&#39118;&#26223;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#24046;&#24322;&#36739;&#22823;&#26102;&#65292;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#21487;&#20197;&#36991;&#20813;&#27745;&#26579;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17833</link><description>&lt;p&gt;
&#37325;&#32622;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#22120;&#65306;&#19968;&#20010;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Resetting the Optimizer in Deep RL: An Empirical Study. (arXiv:2306.17833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17833
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#20248;&#21270;&#38382;&#39064;&#30340;&#39118;&#26223;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#24046;&#24322;&#36739;&#22823;&#26102;&#65292;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#21487;&#20197;&#36991;&#20813;&#27745;&#26579;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36817;&#20284;&#35745;&#31639;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#21253;&#25324;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#35299;&#20915;&#19968;&#31995;&#21015;&#19981;&#21516;&#36845;&#20195;&#20013;&#30446;&#26631;&#20989;&#25968;&#21487;&#33021;&#25913;&#21464;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#29616;&#20195;&#21464;&#31181;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22914;Adam&#12290;&#36825;&#20123;&#20248;&#21270;&#22120;&#20445;&#25345;&#33258;&#24049;&#30340;&#20869;&#37096;&#21442;&#25968;&#65292;&#22914;&#26799;&#24230;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#65292;&#24182;&#38543;&#26102;&#38388;&#26356;&#26032;&#36825;&#20123;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#20043;&#21069;&#36845;&#20195;&#30340;&#20449;&#24687;&#34987;&#29992;&#26469;&#22312;&#24403;&#21069;&#36845;&#20195;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#20043;&#21069;&#36845;&#20195;&#30340;&#20248;&#21270;&#39118;&#26223;&#19982;&#24403;&#21069;&#36845;&#20195;&#30456;&#24046;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#21487;&#33021;&#20250;&#27745;&#26579;&#25152;&#20351;&#29992;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#24433;&#21709;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#26159;&#22312;&#24320;&#22987;&#26032;&#30340;&#36845;&#20195;&#26102;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of approximately solving a sequence of optimization problems where the objective function can change per iteration. The common approach to solving the problem is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first and the second moment of the gradient, and update these parameters over time. Therefore, information obtained in previous iterations is being used to solve the optimization problem in the current iteration. We hypothesize that this can contaminate the internal parameters of the employed optimizer in situations where the optimization landscape of the previous iterations is quite different from the current iteration. To hedge against this effect, a simple idea is to reset the internal parameters of the optimizer when starting a n
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#25968;&#25454;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#26679;&#26412;&#30340;&#23646;&#24615;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17828</link><description>&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#27010;&#24565;&#24433;&#21709;&#29702;&#35299;&#19981;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Unfairness via Training Concept Influence. (arXiv:2306.17828v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17828
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#25968;&#25454;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#26679;&#26412;&#30340;&#23646;&#24615;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#21407;&#22240;&#26377;&#21161;&#20110;&#20174;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#29702;&#35299;&#20182;&#20204;&#30340;&#25968;&#25454;&#21644;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22521;&#35757;&#25968;&#25454;&#36825;&#19968;&#20027;&#35201;&#19981;&#20844;&#24179;&#26469;&#28304;&#30340;&#35270;&#35282;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20197;&#19979;&#38382;&#39064;&#65306;&#22914;&#26524;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#20123;&#26679;&#26412;&#65288;1&#65289;&#26469;&#33258;&#19981;&#21516;&#30340;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#65289;&#32676;&#20307;&#65292;&#65288;2&#65289;&#26631;&#35760;&#26041;&#24335;&#19981;&#21516;&#65292;&#25110;&#32773;&#65288;3&#65289;&#26576;&#20123;&#29305;&#24449;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#37027;&#20040;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#34920;&#29616;&#20250;&#21457;&#29983;&#24590;&#26679;&#30340;&#21464;&#21270;&#65311;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#21453;&#20107;&#23454;&#22320;&#23545;&#22522;&#20110;&#39044;&#23450;&#20041;&#27010;&#24565;&#30340;&#26679;&#26412;&#36827;&#34892;&#24178;&#39044;&#21644;&#25913;&#21464;&#65292;&#37327;&#21270;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30456;&#23545;&#20110;&#27010;&#24565;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#27010;&#24565;&#29983;&#25104;&#21453;&#20107;&#23454;&#29256;&#26412;&#30340;&#26679;&#26412;&#65292;&#21363;&#22914;&#26524;&#27010;&#24565;&#21457;&#29983;&#21464;&#21270;&#65292;&#26679;&#26412;&#30340;&#21453;&#20107;&#23454;&#29256;&#26412;&#12290;&#28982;&#21518;&#25105;&#20204;&#35745;&#31639;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
Knowing the causes of a model's unfairness helps practitioners better understand their data and algorithms. This is an important yet relatively unexplored task. We look into this problem through the lens of the training data - one of the major sources of unfairness. We ask the following questions: how would a model's fairness performance change if, in its training data, some samples (1) were collected from a different (e.g. demographic) group, (2) were labeled differently, or (3) some features were changed? In other words, we quantify the fairness influence of training samples by counterfactually intervening and changing samples based on predefined concepts, i.e. data attributes such as features (X), labels (Y), or sensitive attributes (A). To calculate a training sample's influence on the model's unfairness w.r.t a concept, we first generate counterfactual samples based on the concept, i.e. the counterfactual versions of the sample if the concept were changed. We then calculate the re
&lt;/p&gt;</description></item><item><title>Act3D&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#20316;&#20026;3D&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#20197;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#12290;&#23427;&#22312;&#39640;&#24230;&#31934;&#30830;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.17817</link><description>&lt;p&gt;
Act3D&#65306;&#26080;&#38480;&#20998;&#36776;&#29575;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26816;&#27979;Transformer
&lt;/p&gt;
&lt;p&gt;
Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation. (arXiv:2306.17817v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17817
&lt;/p&gt;
&lt;p&gt;
Act3D&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#20316;&#20026;3D&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#20197;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#12290;&#23427;&#22312;&#39640;&#24230;&#31934;&#30830;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24863;&#30693;&#34920;&#24449;&#38750;&#24120;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#32534;&#30721;&#36974;&#25377;&#24773;&#20917;&#24182;&#31616;&#21270;&#31354;&#38388;&#25512;&#29702;&#12290;&#35768;&#22810;&#25805;&#32437;&#20219;&#21153;&#38656;&#35201;&#23545;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#39044;&#27979;&#36827;&#34892;&#39640;&#31354;&#38388;&#31934;&#24230;&#65292;&#36890;&#24120;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;3D&#24863;&#30693;&#32593;&#26684;&#36827;&#34892;&#35745;&#31639;&#65292;&#36825;&#22312;&#22788;&#29702;&#19978;&#38750;&#24120;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25805;&#20316;&#31574;&#30053;&#30452;&#25509;&#22312;2D&#20013;&#36816;&#20316;&#65292;&#25918;&#24323;&#20102;3D&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Act3D&#65292;&#19968;&#31181;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#35270;&#20026;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#25805;&#20316;&#31574;&#30053;Transformer&#12290;&#23427;&#20197;&#19968;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#26426;&#35270;&#22270;&#30340;&#26410;&#25237;&#24433;3D&#29305;&#24449;&#20113;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#31895;-&#31934;&#26041;&#24335;&#22312;&#33258;&#30001;&#31354;&#38388;&#20013;&#36845;&#20195;&#37319;&#26679;3D&#28857;&#32593;&#26684;&#65292;&#20351;&#29992;&#30456;&#23545;&#31354;&#38388;&#27880;&#24847;&#21147;&#23558;&#20854;&#29305;&#24449;&#21270;&#20026;&#29289;&#29702;&#29305;&#24449;&#20113;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#28857;&#36827;&#34892;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#39044;&#27979;&#12290;Act3D&#22312;&#24050;&#24314;&#31435;&#30340;&#25805;&#32437;&#22522;&#20934;RLbench&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#22909;&#25104;&#32489;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#20013;&#23454;&#29616;&#20102;10%&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D perceptual representations are well suited for robot manipulation as they easily encode occlusions and simplify spatial reasoning. Many manipulation tasks require high spatial precision in end-effector pose prediction, typically demanding high-resolution 3D perceptual grids that are computationally expensive to process. As a result, most manipulation policies operate directly in 2D, foregoing 3D inductive biases. In this paper, we propose Act3D, a manipulation policy Transformer that casts 6-DoF keypose prediction as 3D detection with adaptive spatial computation. It takes as input 3D feature clouds unprojected from one or more camera views, iteratively samples 3D point grids in free space in a coarse-to-fine manner, featurizes them using relative spatial attention to the physical feature cloud, and selects the best feature point for end-effector pose prediction. Act3D sets a new state-of-the-art in RLbench, an established manipulation benchmark. Our model achieves 10% absolute impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#38544;&#34255;&#35268;&#21017;&#28216;&#25103;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#23398;&#20064;&#65292;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#20219;&#21153;&#32467;&#26500;&#30340;&#23454;&#39564;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.17766</link><description>&lt;p&gt;
&#21033;&#29992;&#38544;&#34255;&#35268;&#21017;&#28216;&#25103;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Comparing Reinforcement Learning and Human Learning using the Game of Hidden Rules. (arXiv:2306.17766v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#38544;&#34255;&#35268;&#21017;&#28216;&#25103;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#23398;&#20064;&#65292;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#20219;&#21153;&#32467;&#26500;&#30340;&#23454;&#39564;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38656;&#35201;&#23545;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#20197;&#21450;&#19982;&#20154;&#31867;&#30340;&#27604;&#36739;&#36827;&#34892;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;&#20154;&#26426;&#31995;&#32479;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#32780;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#20381;&#36182;&#20110;&#23545;&#20154;&#31867;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#23545;&#23398;&#20064;&#20219;&#21153;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#24615;&#33021;&#36827;&#34892;&#34920;&#24449;&#12290;&#34429;&#28982;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#22522;&#20934;&#29615;&#22659;&#24050;&#32463;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#29615;&#22659;&#24456;&#38590;&#29992;&#26469;&#19987;&#38376;&#30740;&#31350;&#20219;&#21153;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25903;&#25345;&#20005;&#23494;&#30740;&#31350;&#20219;&#21153;&#32467;&#26500;&#23545;&#20154;&#31867;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#24433;&#21709;&#30340;&#23398;&#20064;&#29615;&#22659;&#12290;&#36890;&#36807;&#20219;&#21153;&#32467;&#26500;&#30340;&#31034;&#20363;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#29615;&#22659;&#22312;&#36825;&#26041;&#38754;&#30740;&#31350;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#26174;&#31034;&#20986;&#20154;&#31867;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable real-world deployment of reinforcement learning (RL) methods requires a nuanced understanding of their strengths and weaknesses and how they compare to those of humans. Human-machine systems are becoming more prevalent and the design of these systems relies on a task-oriented understanding of both human learning (HL) and RL. Thus, an important line of research is characterizing how the structure of a learning task affects learning performance. While increasingly complex benchmark environments have led to improved RL capabilities, such environments are difficult to use for the dedicated study of task structure. To address this challenge we present a learning environment built to support rigorous study of the impact of task structure on HL and RL. We demonstrate the environment's utility for such study through example experiments in task structure that show performance differences between humans and RL algorithms.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#36827;&#21270;&#21338;&#24328;&#29702;&#35770;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;&#21482;&#24110;&#21161;&#31526;&#21512;&#26465;&#20214;&#30340;&#20154;&#30340;&#27495;&#35270;&#24615;&#20154;&#24037;&#26234;&#33021;&#65292;&#25746;&#39532;&#21033;&#20122;&#24335;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26356;&#22909;&#22320;&#20419;&#36827;&#20154;&#31867;&#38388;&#30340;&#21512;&#20316;&#12290;&#36825;&#23545;&#20110;&#32531;&#24930;&#21457;&#23637;&#30340;&#31038;&#20250;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#31038;&#20250;&#20013;&#65292;&#21464;&#21270;&#34987;&#35270;&#20026;&#35880;&#24910;&#21644;&#25269;&#25239;&#12290;</title><link>http://arxiv.org/abs/2306.17747</link><description>&lt;p&gt;
&#27495;&#35270;&#24615;&#25110;&#25746;&#39532;&#21033;&#20122;&#24335; -- &#20154;&#31867;&#38656;&#35201;&#21738;&#31181;&#20154;&#24037;&#26234;&#33021;&#65311;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#20154;&#21475;&#30340;&#36827;&#21270;&#21338;&#24328;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Discriminatory or Samaritan -- which AI is needed for humanity? An Evolutionary Game Theory Analysis of Hybrid Human-AI populations. (arXiv:2306.17747v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17747
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#36827;&#21270;&#21338;&#24328;&#29702;&#35770;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;&#21482;&#24110;&#21161;&#31526;&#21512;&#26465;&#20214;&#30340;&#20154;&#30340;&#27495;&#35270;&#24615;&#20154;&#24037;&#26234;&#33021;&#65292;&#25746;&#39532;&#21033;&#20122;&#24335;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26356;&#22909;&#22320;&#20419;&#36827;&#20154;&#31867;&#38388;&#30340;&#21512;&#20316;&#12290;&#36825;&#23545;&#20110;&#32531;&#24930;&#21457;&#23637;&#30340;&#31038;&#20250;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#31038;&#20250;&#20013;&#65292;&#21464;&#21270;&#34987;&#35270;&#20026;&#35880;&#24910;&#21644;&#25269;&#25239;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36234;&#26469;&#36234;&#23884;&#20837;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#23427;&#20204;&#30340;&#23384;&#22312;&#23548;&#33268;&#20102;&#22609;&#36896;&#25105;&#20204;&#30340;&#34892;&#20026;&#12289;&#20915;&#31574;&#21644;&#31038;&#20250;&#20114;&#21160;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#20114;&#21160;&#19978;&#65292;&#24573;&#35270;&#20102;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#23548;&#33268;&#30340;&#29420;&#29305;&#21160;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#36827;&#21270;&#21338;&#24328;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#22312;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#20154;&#21475;&#20013;&#36827;&#34892;&#19968;&#27425;&#24615;&#22234;&#24466;&#22256;&#22659;&#28216;&#25103;&#30340;&#21512;&#20316;&#28436;&#21270;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#23436;&#20840;&#28151;&#21512;&#20154;&#21475;&#21644;&#32467;&#26500;&#21270;&#20154;&#21475;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#21521;&#27599;&#20010;&#20154;&#37117;&#26080;&#26465;&#20214;&#25552;&#20379;&#24110;&#21161;&#65292;&#21253;&#25324;&#21467;&#24466;&#65292;&#36824;&#26159;&#21482;&#24110;&#21161;&#34987;&#35748;&#20026;&#20540;&#24471;&#25110;&#21512;&#20316;&#30340;&#20154;&#30340;&#25746;&#39532;&#21033;&#20122;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20154;&#65292;&#23545;&#20110;&#25512;&#21160;&#20154;&#31867;&#21512;&#20316;&#30340;&#27700;&#24179;&#26469;&#35828;&#65292;&#21518;&#32773;&#35201;&#27604;&#21069;&#32773;&#26356;&#26377;&#21033;&#65292;&#29305;&#21035;&#26159;&#22312;&#21464;&#21270;&#34987;&#35880;&#24910;&#25110;&#25269;&#25239;&#35270;&#20026;&#30340;&#32531;&#24930;&#21457;&#23637;&#30340;&#31038;&#20250;&#20013;&#65288;&#36873;&#25321;&#30340;&#24378;&#24230;&#36739;&#23567;&#65289;&#12290;&#30452;&#35266;&#19978;&#65292;&#22312;&#21464;&#21270;&#24555;&#36895;&#30340;&#31038;&#20250;&#20013;&#65288;&#36873;&#25321;&#30340;&#24378;&#24230;&#36739;&#39640;&#65289;&#65292;&#27495;&#35270;&#24615;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26356;&#22909;&#22320;&#25512;&#21160;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) systems are increasingly embedded in our lives, their presence leads to interactions that shape our behaviour, decision-making, and social interactions. Existing theoretical research has primarily focused on human-to-human interactions, overlooking the unique dynamics triggered by the presence of AI. In this paper, resorting to methods from evolutionary game theory, we study how different forms of AI influence the evolution of cooperation in a human population playing the one-shot Prisoner's Dilemma game in both well-mixed and structured populations. We found that Samaritan AI agents that help everyone unconditionally, including defectors, can promote higher levels of cooperation in humans than Discriminatory AI that only help those considered worthy/cooperative, especially in slow-moving societies where change is viewed with caution or resistance (small intensities of selection). Intuitively, in fast-moving societies (high intensities of selection), Dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Token-Event-Role&#32467;&#26500;&#30340;&#22810;&#36890;&#36947;&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#39044;&#27979;&#27169;&#22359;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;token-event&#23545;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23454;&#20307;&#21644;&#22810;&#20107;&#20214;&#25277;&#21462;&#30340;&#38598;&#25104;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17733</link><description>&lt;p&gt;
&#22522;&#20110;Token-Event-Role&#32467;&#26500;&#30340;&#22810;&#36890;&#36947;&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Token-Event-Role Structure-based Multi-Channel Document-Level Event Extraction. (arXiv:2306.17733v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Token-Event-Role&#32467;&#26500;&#30340;&#22810;&#36890;&#36947;&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#39044;&#27979;&#27169;&#22359;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;token-event&#23545;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23454;&#20307;&#21644;&#22810;&#20107;&#20214;&#25277;&#21462;&#30340;&#38598;&#25104;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;&#26159;&#19968;&#20010;&#21382;&#21490;&#24736;&#20037;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#65292;&#28041;&#21450;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#65306;&#23454;&#20307;&#25277;&#21462;&#12289;&#20107;&#20214;&#31867;&#22411;&#21028;&#26029;&#21644;&#29305;&#23450;&#20107;&#20214;&#31867;&#22411;&#30340;&#22810;&#20107;&#20214;&#25277;&#21462;&#12290;&#28982;&#32780;&#65292;&#23558;&#38382;&#39064;&#35270;&#20026;&#22810;&#20010;&#23398;&#20064;&#20219;&#21153;&#20250;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#36328;&#36234;&#19981;&#21516;&#20107;&#20214;&#30340;&#23454;&#20307;&#30340;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#20107;&#20214;&#25277;&#21462;&#24615;&#33021;&#26377;&#38480;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#31216;&#20026;token-event-role&#30340;&#26032;&#25968;&#25454;&#32467;&#26500;&#21644;&#19968;&#20010;&#22810;&#36890;&#36947;&#21442;&#25968;&#35282;&#33394;&#39044;&#27979;&#27169;&#22359;&#12290;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#32467;&#26500;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25581;&#31034;&#22810;&#20010;&#20107;&#20214;&#20013;token&#30340;&#20027;&#35201;&#20316;&#29992;&#65292;&#20174;&#32780;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#36890;&#36947;&#39044;&#27979;&#27169;&#22359;&#65292;&#25105;&#20204;&#23558;&#23454;&#20307;&#21644;&#22810;&#20107;&#20214;&#25277;&#21462;&#36716;&#21270;&#20026;&#39044;&#27979;token-event&#23545;&#30340;&#21333;&#19968;&#20219;&#21153;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event extraction is a long-standing challenging information retrieval problem involving a sequence of sub-tasks: entity extraction, event type judgment, and event type-specific multi-event extraction. However, addressing the problem as multiple learning tasks leads to increased model complexity. Also, existing methods insufficiently utilize the correlation of entities crossing different events, resulting in limited event extraction performance. This paper introduces a novel framework for document-level event extraction, incorporating a new data structure called token-event-role and a multi-channel argument role prediction module. The proposed data structure enables our model to uncover the primary role of tokens in multiple events, facilitating a more comprehensive understanding of event relationships. By leveraging the multi-channel prediction module, we transform entity and multi-event extraction into a single task of predicting token-event pairs, thereby reducing the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25552;&#31034;&#26816;&#32034;&#30340;&#29983;&#25104;&#24335;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#26816;&#32034;&#21040;&#30340;&#25552;&#31034;&#21644;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#29983;&#25104;&#31572;&#26696;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#24555;&#36895;&#38646;&#26679;&#26412;&#33258;&#36866;&#24212;&#33021;&#21147;&#65292;&#24182;&#22312;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#27604;&#38750;&#26816;&#32034;&#27169;&#22411;&#39640;&#20986;30%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.17675</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#25552;&#31034;&#26816;&#32034;&#29992;&#20110;&#29983;&#25104;&#24335;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Multimodal Prompt Retrieval for Generative Visual Question Answering. (arXiv:2306.17675v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25552;&#31034;&#26816;&#32034;&#30340;&#29983;&#25104;&#24335;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#26816;&#32034;&#21040;&#30340;&#25552;&#31034;&#21644;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#29983;&#25104;&#31572;&#26696;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#24555;&#36895;&#38646;&#26679;&#26412;&#33258;&#36866;&#24212;&#33021;&#21147;&#65292;&#24182;&#22312;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#27604;&#38750;&#26816;&#32034;&#27169;&#22411;&#39640;&#20986;30%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#65289;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#22312;&#35270;&#35273;&#38382;&#31572;&#26041;&#38754;&#24050;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#21028;&#21035;&#24335;&#20844;&#24335;&#65292;&#22312;&#39044;&#23450;&#20041;&#30340;&#26631;&#31614;&#38598;&#20869;&#39044;&#27979;&#31572;&#26696;&#65292;&#23548;&#33268;&#22312;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#65288;&#22914;&#21307;&#23398;&#39046;&#22495;&#65289;&#30340;&#20302;&#36164;&#28304;&#39046;&#22495;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#19988;&#22312;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#26102;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#26816;&#32034;&#65288;MPR&#65289;&#26469;&#22686;&#24378;&#65292;&#23427;&#23558;&#26816;&#32034;&#21040;&#30340;&#25552;&#31034;&#21644;&#22810;&#27169;&#24577;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#20197;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#36827;&#34892;&#38646;&#26679;&#26412;&#25968;&#25454;&#38598;&#33258;&#36866;&#24212;&#65292;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#24320;&#25918;&#24335;&#31572;&#26696;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#35774;&#32622;&#20013;&#65292;MPR&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#38750;&#26816;&#32034;&#27169;&#22411;&#26368;&#22810;30&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed impressive results of pre-trained vision-language models on knowledge-intensive tasks such as visual question answering (VQA). Despite the recent advances in VQA, existing methods mainly adopt a discriminative formulation that predicts answers within a pre-defined label set, leading to easy overfitting on low-resource domains with limited labeled data (e.g., medicine) and poor generalization under domain shift to another dataset. To tackle this limitation, we propose a novel generative model enhanced by multimodal prompt retrieval (MPR) that integrates retrieved prompts and multimodal features to generate answers in free text. Our generative model enables rapid zero-shot dataset adaptation to unseen data distributions and open-set answer labels across datasets. Our experiments on medical VQA tasks show that MPR outperforms its non-retrieval counterpart by up to 30% accuracy points in a few-shot domain adaptation setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;POMDP&#20540;&#36845;&#20195;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#20256;&#32479;&#31526;&#21495;&#25216;&#26415;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#29366;&#24577;&#32622;&#20449;&#24230;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#36830;&#32493;&#29366;&#24577;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17639</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;POMDP&#20540;&#36845;&#20195;
&lt;/p&gt;
&lt;p&gt;
Point-based Value Iteration for Neuro-Symbolic POMDPs. (arXiv:2306.17639v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;POMDP&#20540;&#36845;&#20195;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#20256;&#32479;&#31526;&#21495;&#25216;&#26415;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#29366;&#24577;&#32622;&#20449;&#24230;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#36830;&#32493;&#29366;&#24577;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26159;&#32467;&#21512;&#20256;&#32479;&#31526;&#21495;&#25216;&#26415;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#26412;&#25991;&#32771;&#34385;&#20854;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#31526;&#21495;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;NS-POMDPs&#65289;&#65292;&#35813;&#27169;&#22411;&#25551;&#36848;&#20102;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#24863;&#30693;&#36830;&#32493;&#29366;&#24577;&#29615;&#22659;&#24182;&#36827;&#34892;&#31526;&#21495;&#20915;&#31574;&#30340;&#20195;&#29702;&#65292;&#24182;&#30740;&#31350;&#20102;&#20248;&#21270;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36830;&#32493;&#29366;&#24577;&#32622;&#20449;&#24230;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#27573;&#32447;&#24615;&#21644;&#20984;&#34920;&#31034;&#65288;P-PWLC&#65289;&#65292;&#36890;&#36807;&#35206;&#30422;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;&#22810;&#38754;&#20307;&#21644;&#20540;&#21521;&#37327;&#23454;&#29616;&#65292;&#24182;&#23558;Bellman backups&#25193;&#23637;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20540;&#20989;&#25968;&#30340;&#20984;&#24615;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20540;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36830;&#32493;&#29366;&#24577;&#27169;&#22411;&#21644;&#31070;&#32463;&#24863;&#30693;&#26426;&#21046;&#30340;&#24213;&#23618;&#32467;&#26500;&#26469;&#20445;&#35777;&#26377;&#38480;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic artificial intelligence is an emerging area that combines traditional symbolic techniques with neural networks. In this paper, we consider its application to sequential decision making under uncertainty. We introduce neuro-symbolic partially observable Markov decision processes (NS-POMDPs), which model an agent that perceives a continuous-state environment using a neural network and makes decisions symbolically, and study the problem of optimising discounted cumulative rewards. This requires functions over continuous-state beliefs, for which we propose a novel piecewise linear and convex representation (P-PWLC) in terms of polyhedra covering the continuous-state space and value vectors, and extend Bellman backups to this representation. We prove the convexity and continuity of value functions and present two value iteration algorithms that ensure finite representability by exploiting the underlying structure of the continuous-state model and the neural perception mechani
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21333;&#20010;ToF&#30456;&#26426;&#30340;&#32418;&#22806;&#21644;&#28145;&#24230;&#22270;&#20687;&#65292;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;RGB-D&#30456;&#26426;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#19987;&#29992;&#21367;&#31215;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#36710;&#20869;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17636</link><description>&lt;p&gt;
&#29992;&#21333;&#20010;ToF&#30456;&#26426;&#23454;&#29616;&#19982;RGB-D&#30456;&#26426;&#30456;&#24403;&#30340;&#20998;&#21106;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Achieving RGB-D level Segmentation Performance from a Single ToF Camera. (arXiv:2306.17636v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21333;&#20010;ToF&#30456;&#26426;&#30340;&#32418;&#22806;&#21644;&#28145;&#24230;&#22270;&#20687;&#65292;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;RGB-D&#30456;&#26426;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#19987;&#29992;&#21367;&#31215;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#36710;&#20869;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#19968;&#31181;&#27169;&#24577;&#65292;&#36890;&#24120;&#20316;&#20026;RGB&#30340;&#34917;&#20805;&#20449;&#24687;&#30001;RGB-D&#30456;&#26426;&#25552;&#20379;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#21333;&#20010;ToF&#30456;&#26426;&#30340;&#32418;&#22806;&#65288;IR&#65289;&#21644;&#28145;&#24230;&#22270;&#20687;&#21487;&#20197;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#19978;&#33719;&#24471;&#19982;RGB-D&#30456;&#26426;&#30456;&#21516;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#34701;&#21512;ToF&#30456;&#26426;&#30340;&#32418;&#22806;&#21644;&#28145;&#24230;&#27169;&#24577;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#21033;&#29992;&#28145;&#24230;&#19987;&#29992;&#21367;&#31215;&#30340;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#23545;&#36710;&#20869;&#20998;&#21106;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26356;&#26114;&#36149;&#30340;RGB-D&#26041;&#27861;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth is a very important modality in computer vision, typically used as complementary information to RGB, provided by RGB-D cameras. In this work, we show that it is possible to obtain the same level of accuracy as RGB-D cameras on a semantic segmentation task using infrared (IR) and depth images from a single Time-of-Flight (ToF) camera. In order to fuse the IR and depth modalities of the ToF camera, we introduce a method utilizing depth-specific convolutions in a multi-task learning framework. In our evaluation on an in-car segmentation dataset, we demonstrate the competitiveness of our method against the more costly RGB-D approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#23450;&#21046;&#30340;&#24863;&#24212;&#30005;&#21160;&#26426;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22810;&#20010;&#30005;&#26426;&#35774;&#35745;&#23454;&#20363;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#65292;&#20351;&#24471;&#30005;&#26426;&#35774;&#35745;&#33258;&#21160;&#21270;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#25805;&#20316;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.17626</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#24863;&#24212;&#30005;&#26426;
&lt;/p&gt;
&lt;p&gt;
Design of Induction Machines using Reinforcement Learning. (arXiv:2306.17626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17626
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#23450;&#21046;&#30340;&#24863;&#24212;&#30005;&#21160;&#26426;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22810;&#20010;&#30005;&#26426;&#35774;&#35745;&#23454;&#20363;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#65292;&#20351;&#24471;&#30005;&#26426;&#35774;&#35745;&#33258;&#21160;&#21270;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#25805;&#20316;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30005;&#30913;&#21644;&#28909;&#32422;&#26463;&#30340;&#19981;&#21516;&#65292;&#24863;&#24212;&#30005;&#26426;&#30340;&#35774;&#35745;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#38144;&#21806;&#24037;&#20855;&#20013;&#24555;&#36895;&#20272;&#31639;&#26426;&#22120;&#30340;&#23610;&#23544;&#23545;&#20110;&#26681;&#25454;&#29305;&#23450;&#35201;&#27714;&#32473;&#23458;&#25143;&#25552;&#20379;&#24555;&#36895;&#25253;&#20215;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20010;&#36807;&#31243;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#36873;&#25321;&#19981;&#21516;&#30340;&#35774;&#35745;&#21442;&#25968;&#65292;&#22914;&#38271;&#24230;&#12289;&#30452;&#24452;&#12289;&#40831;&#23574;&#39640;&#24230;&#21644;&#32469;&#32452;&#21277;&#25968;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#30340;&#29305;&#23450;&#25197;&#30697;&#12289;&#30005;&#27969;&#21644;&#28201;&#24230;&#12290;&#30005;&#26426;&#35774;&#35745;&#24072;&#36890;&#36807;&#20182;&#20204;&#30340;&#32463;&#39564;&#30693;&#36947;&#22914;&#20309;&#25913;&#21464;&#19981;&#21516;&#30340;&#26426;&#22120;&#35774;&#35745;&#21442;&#25968;&#65292;&#20197;&#28385;&#36275;&#23458;&#25143;&#30340;&#29305;&#23450;&#25805;&#20316;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35774;&#35745;&#23450;&#21046;&#30340;&#24863;&#24212;&#30005;&#21160;&#26426;&#12290;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#27169;&#25311;&#30005;&#26426;&#35774;&#35745;&#28216;&#25103;&#30340;&#19981;&#21516;&#23454;&#20363;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#65292;&#24403;&#20570;&#20986;&#33391;&#22909;&#25110;&#19981;&#33391;&#35774;&#35745;&#36873;&#25321;&#26102;&#65292;&#20351;&#29992;&#22870;&#21169;&#25110;&#24809;&#32602;&#20989;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#21270;&#30005;&#26426;&#35774;&#35745;&#65292;&#32780;&#26080;&#38656;&#24212;&#29992;&#20219;&#20309;&#20154;&#20026;&#21046;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of induction machine is a challenging task due to different electromagnetic and thermal constraints. Quick estimation of machine's dimensions is important in the sales tool to provide quick quotations to customers based on specific requirements. The key part of this process is to select different design parameters like length, diameter, tooth tip height and winding turns to achieve certain torque, current and temperature of the machine. Electrical machine designers, with their experience know how to alter different machine design parameters to achieve a customer specific operation requirements. We propose a reinforcement learning algorithm to design a customised induction motor. The neural network model is trained off-line by simulating different instances of of electrical machine design game with a reward or penalty function when a good or bad design choice is made. The results demonstrate that the suggested method automates electrical machine design without applying any hu
&lt;/p&gt;</description></item><item><title>Sphere2Vec&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17624</link><description>&lt;p&gt;
Sphere2Vec&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#29699;&#38754;&#19978;&#36890;&#29992;&#20301;&#32622;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions. (arXiv:2306.17624v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17624
&lt;/p&gt;
&lt;p&gt;
Sphere2Vec&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20026;&#31354;&#38388;&#20013;&#30340;&#28857;&#29983;&#25104;&#36866;&#21512;&#23398;&#20064;&#30340;&#34920;&#31034;&#26159;&#19968;&#20010;&#22522;&#26412;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#32534;&#30721;&#26041;&#26696;&#65288;&#22914;Space2Vec&#21644;NeRF&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#23558;&#20108;&#32500;/&#19977;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#28857;&#32534;&#30721;&#20026;&#39640;&#32500;&#21521;&#37327;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25152;&#26377;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#20301;&#32622;&#32534;&#30721;&#22120;&#37117;&#26159;&#35774;&#35745;&#29992;&#26469;&#27169;&#25311;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;&#22312;&#24212;&#29992;&#20110;&#38656;&#35201;&#22312;&#29699;&#38754;&#19978;&#36827;&#34892;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;GPS&#22352;&#26631;&#25968;&#25454;&#38598;&#26102;&#65292;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#37117;&#20250;&#20986;&#29616;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#22320;&#22270;&#25237;&#24433;&#22833;&#30495;&#38382;&#39064;&#65288;2D&#65289;&#21644;&#29699;&#38754;&#21040;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#36817;&#20284;&#35823;&#24046;&#65288;3D&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Sphere2Vec&#30340;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#22312;&#29699;&#38754;&#19978;&#32534;&#30721;&#28857;&#22352;&#26631;&#26102;&#20445;&#25345;&#29699;&#38754;&#36317;&#31163;&#12290;&#25105;&#20204;&#22312;&#29699;&#38754;&#19978;&#30340;&#20301;&#32622;&#32534;&#30721;&#30340;&#36317;&#31163;&#20445;&#25345;&#32534;&#30721;&#30340;&#32479;&#19968;&#35270;&#35282;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating learning-friendly representations for points in space is a fundamental and long-standing problem in ML. Recently, multi-scale encoding schemes (such as Space2Vec and NeRF) were proposed to directly encode any point in 2D/3D Euclidean space as a high-dimensional vector, and has been successfully applied to various geospatial prediction and generative tasks. However, all current 2D and 3D location encoders are designed to model point distances in Euclidean space. So when applied to large-scale real-world GPS coordinate datasets, which require distance metric learning on the spherical surface, both types of models can fail due to the map projection distortion problem (2D) and the spherical-to-Euclidean distance approximation error (3D). To solve these problems, we propose a multi-scale location encoder called Sphere2Vec which can preserve spherical distances when encoding point coordinates on a spherical surface. We developed a unified view of distance-reserving encoding on sph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#20248;&#21270;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#30340;&#35206;&#30422;&#26102;&#38388;&#26368;&#22810;&#20026;&#26368;&#20248;&#35299;&#30340;&#22235;&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#20943;&#23569;&#38382;&#39064;&#30340;&#35268;&#27169;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.17609</link><description>&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#29992;&#20110;&#39640;&#25928;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#26102;&#38388;&#20248;&#21270;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Mixed Integer Programming for Time-Optimal Multi-Robot Coverage Path Planning with Efficient Heuristics. (arXiv:2306.17609v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#20248;&#21270;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#30340;&#35206;&#30422;&#26102;&#38388;&#26368;&#22810;&#20026;&#26368;&#20248;&#35299;&#30340;&#22235;&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#20943;&#23569;&#38382;&#39064;&#30340;&#35268;&#27169;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26088;&#22312;&#26368;&#23567;&#21270;&#35206;&#30422;&#26102;&#38388;&#65288;&#23450;&#20041;&#20026;&#25152;&#26377;&#26426;&#22120;&#20154;&#30340;&#26368;&#22823;&#34892;&#31243;&#26102;&#38388;&#65289;&#30340;&#26410;&#21152;&#26435;&#21644;&#21152;&#26435;&#22320;&#24418;&#30340;&#26102;&#38388;&#20248;&#21270;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#65288;MCPP&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;MCPP&#20943;&#23569;&#21040;&#20102;&#26681;&#26368;&#23567;&#26368;&#22823;&#26641;&#35206;&#30422;&#65288;RMMTC&#65289;&#12290;&#39318;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#27169;&#22411;&#26469;&#26368;&#20248;&#35299;&#20915;RMMTC&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;MCPP&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#35206;&#30422;&#26102;&#38388;&#26368;&#22810;&#20026;&#26368;&#20248;&#35299;&#30340;&#22235;&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#27425;&#20248;&#20294;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;MIP&#27169;&#22411;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#22823;&#35268;&#27169;MCPP&#23454;&#20363;&#19978;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#21551;&#21457;&#24335;&#31639;&#27861;&#23548;&#33268;&#20102;&#32553;&#23567;&#21518;&#30340;MIP&#27169;&#22411;&#65292;&#23545;&#20110;&#25152;&#26377;RMMTC&#23454;&#20363;&#26469;&#35828;&#20173;&#28982;&#26159;&#23436;&#25972;&#30340;&#65288;&#21363;&#20445;&#35777;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#27169;&#22411;&#20248;&#21270;&#30340;&#28909;&#21551;&#21160;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21407;&#22987;MIP&#27169;&#22411;&#21644;&#32553;&#23567;&#21518;&#30340;MIP&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate time-optimal Multi-Robot Coverage Path Planning (MCPP) for both unweighted and weighted terrains, which aims to minimize the coverage time, defined as the maximum travel time of all robots. Specifically, we focus on a reduction from MCPP to Rooted Min-Max Tree Cover (RMMTC). For the first time, we propose a Mixed Integer Programming (MIP) model to optimally solve RMMTC, resulting in an MCPP solution with a coverage time that is provably at most four times the optimal. Moreover, we propose two suboptimal yet effective heuristics that reduce the number of variables in the MIP model, thus improving its efficiency for large-scale MCPP instances. We show that both heuristics result in reduced-size MIP models that remain complete (i.e., guarantee to find a solution if one exists) for all RMMTC instances. Additionally, we explore the use of model optimization warm-startup to further improve the efficiency of both the original MIP model and the reduced-size MIP models. We valida
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#19968;&#20010;&#37319;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#21644;&#23398;&#20064;&#22411;&#36319;&#36394;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#29289;&#20307;&#30340;&#20960;&#20309;&#36816;&#21160;&#21644;&#21464;&#21270;&#65292;&#24182;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17602</link><description>&lt;p&gt;
S.T.A.R.-Track&#65306;&#33258;&#36866;&#24212;&#26102;&#31354;&#22806;&#35980;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations. (arXiv:2306.17602v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#19968;&#20010;&#37319;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#21644;&#23398;&#20064;&#22411;&#36319;&#36394;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#29289;&#20307;&#30340;&#20960;&#20309;&#36816;&#21160;&#21644;&#21464;&#21270;&#65292;&#24182;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#36319;&#36394;-&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#22522;&#20110;Transformer&#30340;3D&#36319;&#36394;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36319;&#36394;&#26041;&#27861;&#36890;&#36807;&#20960;&#20309;&#36816;&#21160;&#27169;&#22411;&#34701;&#21512;&#24103;&#20043;&#38388;&#30340;&#29289;&#20307;&#21644;&#33258;&#36816;&#21160;&#30340;&#20960;&#20309;&#25928;&#24212;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#26469;&#35843;&#25972;&#23545;&#35937;&#26597;&#35810;&#65292;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30452;&#25509;&#32771;&#34385;&#35270;&#35282;&#21644;&#20809;&#29031;&#26465;&#20214;&#30340;&#21464;&#21270;&#65292;&#21516;&#26102;&#26126;&#30830;&#24314;&#27169;&#20960;&#20309;&#36816;&#21160;&#12290;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#30340;&#36319;&#36394;&#23884;&#20837;&#65292;&#26377;&#21161;&#20110;&#24314;&#27169;&#36712;&#36857;&#30340;&#23384;&#22312;&#27010;&#29575;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36319;&#36394;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#22522;&#20110;&#26597;&#35810;&#30340;&#26816;&#27979;&#22120;&#38598;&#25104;&#12290;&#22312;nuScenes&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;DETR3D&#30340;&#36319;&#36394;&#22120;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#36712;&#36857;&#30340;&#36523;&#20221;&#36716;&#25442;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the tracking-by-attention paradigm, this paper introduces an object-centric, transformer-based framework for tracking in 3D. Traditional model-based tracking approaches incorporate the geometric effect of object- and ego motion between frames with a geometric motion model. Inspired by this, we propose S.T.A.R.-Track, which uses a novel latent motion model (LMM) to additionally adjust object queries to account for changes in viewing direction and lighting conditions directly in the latent space, while still modeling the geometric motion explicitly. Combined with a novel learnable track embedding that aids in modeling the existence probability of tracks, this results in a generic tracking framework that can be integrated with any query-based detector. Extensive experiments on the nuScenes benchmark demonstrate the benefits of our approach, showing state-of-the-art performance for DETR3D-based trackers while drastically reducing the number of identity switches of tracks at the s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25511;&#21046;&#24494;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#24615;&#30340;&#33647;&#29289;&#36755;&#36865;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17598</link><description>&lt;p&gt;
&#24494;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#23548;&#33322;&#20197;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#33647;&#29289;&#36755;&#36865;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Navigation of micro-robot swarms for targeted delivery using reinforcement learning. (arXiv:2306.17598v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17598
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25511;&#21046;&#24494;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#24615;&#30340;&#33647;&#29289;&#36755;&#36865;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#22411;&#26426;&#22120;&#20154;&#22312;&#26377;&#38024;&#23545;&#24615;&#30340;&#33647;&#29289;&#36755;&#36865;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24494;&#23567;&#30340;&#23610;&#23544;&#65292;&#21333;&#29420;&#25511;&#21046;&#27599;&#20010;&#26426;&#22120;&#20154;&#26159;&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21333;&#19968;&#25511;&#21046;&#22120;&#25511;&#21046;&#22810;&#20010;&#26426;&#22120;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#25104;&#21151;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Proximal Policy Optimization (PPO)&#21644;Robust Policy Optimization (RPO)&#26469;&#25511;&#21046;&#19968;&#32676;&#24494;&#22411;&#28216;&#27891;&#26426;&#22120;&#20154;&#65292;&#22312;&#21463;&#27700;&#21160;&#21147;&#23398;&#25928;&#24212;&#25511;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23427;&#20204;&#30340;&#26041;&#21521;&#26397;&#21521;&#19968;&#20010;&#22278;&#24418;&#21560;&#25910;&#30446;&#26631;&#12290;&#25105;&#20204;&#32771;&#26597;&#20102;PPO&#21644;RPO&#22312;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#24773;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#27979;&#35797;&#20102;&#23427;&#20204;&#22312;&#38543;&#26426;&#30446;&#26631;&#20301;&#32622;&#21644;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#23398;&#20064;&#25511;&#21046;&#19968;&#32676;25&#20010;&#28216;&#27891;&#26426;&#22120;&#20154;&#24182;&#23558;&#23427;&#20204;&#23548;&#33322;&#21040;&#19968;&#20010;&#23637;&#31034;&#30446;&#26631;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Micro robotics is quickly emerging to be a promising technological solution to many medical treatments with focus on targeted drug delivery. They are effective when working in swarms whose individual control is mostly infeasible owing to their minute size. Controlling a number of robots with a single controller is thus important and artificial intelligence can help us perform this task successfully. In this work, we use the Reinforcement Learning (RL) algorithms Proximal Policy Optimization (PPO) and Robust Policy Optimization (RPO) to navigate a swarm of 4, 9 and 16 microswimmers under hydrodynamic effects, controlled by their orientation, towards a circular absorbing target. We look at both PPO and RPO performances with limited state information scenarios and also test their robustness for random target location and size. We use curriculum learning to improve upon the performance and demonstrate the same in learning to navigate a swarm of 25 swimmers and steering the swarm to exempli
&lt;/p&gt;</description></item><item><title>Razor SNN&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#39640;&#25928;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#31232;&#30095;&#30340;&#20107;&#20214;&#27969;&#20013;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.17597</link><description>&lt;p&gt;
Razor SNN: &#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#39640;&#25928;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Razor SNN: Efficient Spiking Neural Network with Temporal Embeddings. (arXiv:2306.17597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17597
&lt;/p&gt;
&lt;p&gt;
Razor SNN&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#39640;&#25928;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#31232;&#30095;&#30340;&#20107;&#20214;&#27969;&#20013;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#35270;&#35273;&#20256;&#24863;&#22120;(DVS)&#29983;&#25104;&#30340;&#20107;&#20214;&#27969;&#22312;&#31354;&#38388;&#22495;&#20013;&#26159;&#31232;&#30095;&#21644;&#38750;&#22343;&#21248;&#30340;&#65292;&#32780;&#22312;&#26102;&#38388;&#22495;&#20013;&#21364;&#26159;&#23494;&#38598;&#19988;&#20887;&#20313;&#30340;&#12290;&#34429;&#28982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNN)&#20316;&#20026;&#19968;&#31181;&#20107;&#20214;&#39537;&#21160;&#30340;&#31070;&#32463;&#24418;&#24577;&#27169;&#22411;&#26377;&#28508;&#21147;&#20174;&#20107;&#20214;&#27969;&#20013;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#22815;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#22522;&#20110;&#20197;&#19978;&#21407;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Razor SNN&#30340;&#20107;&#20214;&#31232;&#30095;&#21270;&#33033;&#20914;&#26694;&#26550;&#65292;&#36880;&#27493;&#20462;&#21098;&#26080;&#24847;&#20041;&#30340;&#20107;&#20214;&#24103;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22522;&#20110;&#20840;&#23616;&#26102;&#38388;&#23884;&#20837;&#30340;&#21160;&#24577;&#26426;&#21046;&#65292;&#37325;&#26500;&#29305;&#24449;&#65292;&#24182;&#22312;&#35757;&#32451;&#38454;&#27573;&#33258;&#36866;&#24212;&#22320;&#24378;&#35843;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#26681;&#25454;&#35757;&#32451;&#24471;&#21040;&#30340;&#26102;&#38388;&#23884;&#20837;&#29983;&#25104;&#30340;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#23618;&#27425;&#21270;&#22320;&#28040;&#38500;&#26080;&#29992;&#30340;&#24103;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;Razor SNN&#22312;&#22235;&#20010;&#22522;&#20110;&#20107;&#20214;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#23454;&#29616;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65306;DVS 128&#25163;&#21183;&#12289;N-Caltech 101&#12289;CIFAR10-DVS&#21644;...
&lt;/p&gt;
&lt;p&gt;
The event streams generated by dynamic vision sensors (DVS) are sparse and non-uniform in the spatial domain, while still dense and redundant in the temporal domain. Although spiking neural network (SNN), the event-driven neuromorphic model, has the potential to extract spatio-temporal features from the event streams, it is not effective and efficient. Based on the above, we propose an events sparsification spiking framework dubbed as Razor SNN, pruning pointless event frames progressively. Concretely, we extend the dynamic mechanism based on the global temporal embeddings, reconstruct the features, and emphasize the events effect adaptively at the training stage. During the inference stage, eliminate fruitless frames hierarchically according to a binary mask generated by the trained temporal embeddings. Comprehensive experiments demonstrate that our Razor SNN achieves competitive performance consistently on four events-based benchmarks: DVS 128 Gesture, N-Caltech 101, CIFAR10-DVS and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17582</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65306;&#35774;&#35745;&#21407;&#21017;&#21644;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Robotics: Design Principles and Model Abilities. (arXiv:2306.17582v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;OpenAI&#30340;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23558;&#25552;&#31034;&#24037;&#31243;&#30340;&#35774;&#35745;&#21407;&#21017;&#19982;&#39640;&#32423;&#20989;&#25968;&#24211;&#30340;&#21019;&#24314;&#30456;&#32467;&#21512;&#65292;&#20351;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12289;&#27169;&#25311;&#22120;&#21644;&#24418;&#24577;&#12290;&#25105;&#20204;&#37325;&#28857;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#21644;&#23545;&#35805;&#31574;&#30053;&#23545;&#25191;&#34892;&#21508;&#31181;&#31867;&#22411;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#20351;&#29992;&#33258;&#30001;&#24418;&#24335;&#23545;&#35805;&#12289;&#35299;&#26512;XML&#26631;&#35760;&#21644;&#21512;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#20989;&#25968;&#21644;&#36890;&#36807;&#23545;&#35805;&#36827;&#34892;&#38381;&#29615;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20174;&#22522;&#26412;&#30340;&#36923;&#36753;&#12289;&#20960;&#20309;&#21644;&#25968;&#23398;&#25512;&#29702;&#21040;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#22914;&#31354;&#20013;&#23548;&#33322;&#12289;&#25805;&#32437;&#21644;&#20855;&#36523;&#20195;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#26041;&#38754;&#21487;&#20197;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#21516;&#26102;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing us
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;&#22823;&#23398;&#24405;&#21462;&#36807;&#31243;&#20013;&#25490;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#20250;&#23548;&#33268;&#39044;&#27979;&#34920;&#29616;&#19979;&#38477;&#65292;&#32780;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20449;&#24687;&#21487;&#20197;&#37096;&#20998;&#24674;&#22797;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17575</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22686;&#24378;&#22823;&#23398;&#24405;&#21462;&#20013;&#30340;&#25972;&#20307;&#35780;&#20272;&#65292;&#20197;&#20998;&#26512;&#35770;&#25991;&#21644;&#25512;&#33616;&#20449;
&lt;/p&gt;
&lt;p&gt;
Augmenting Holistic Review in University Admission using Natural Language Processing for Essays and Recommendation Letters. (arXiv:2306.17575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17575
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;&#22823;&#23398;&#24405;&#21462;&#36807;&#31243;&#20013;&#25490;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#20250;&#23548;&#33268;&#39044;&#27979;&#34920;&#29616;&#19979;&#38477;&#65292;&#32780;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20449;&#24687;&#21487;&#20197;&#37096;&#20998;&#24674;&#22797;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39640;&#24230;&#36873;&#25321;&#24615;&#30340;&#26426;&#26500;&#20013;&#65292;&#22823;&#23398;&#24405;&#21462;&#37319;&#29992;&#20840;&#38754;&#35780;&#20272;&#36807;&#31243;&#65292;&#32771;&#34385;&#30003;&#35831;&#30340;&#25152;&#26377;&#26041;&#38754;&#65292;&#21253;&#25324;&#38544;&#31169;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#12289;&#24615;&#21035;&#65289;&#12289;&#25104;&#32489;&#12289;&#35770;&#25991;&#21644;&#25512;&#33616;&#20449;&#65292;&#20197;&#32452;&#25104;&#19968;&#25903;&#20248;&#31168;&#21644;&#22810;&#26679;&#21270;&#30340;&#29677;&#32423;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23454;&#35777;&#35780;&#20272;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#39044;&#27979;&#24405;&#21462;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#25991;&#26412;&#20449;&#24687;&#65288;&#22914;&#20010;&#20154;&#35770;&#25991;&#12289;&#25945;&#24072;&#25512;&#33616;&#20449;&#65289;&#22312;&#27169;&#22411;&#20013;&#20195;&#26367;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;2022-2023&#23398;&#24180;&#22312;&#19968;&#25152;&#20855;&#26377;&#36873;&#25321;&#24615;&#30340;&#32654;&#22269;&#26412;&#31185;&#20837;&#23398;&#21150;&#20844;&#23460;&#30340;14,915&#21517;&#30003;&#35831;&#20154;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;ML&#27169;&#22411;&#20013;&#25490;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#20250;&#26174;&#33879;&#38477;&#20302;&#39044;&#27979;&#24405;&#21462;&#34920;&#29616;&#12290;&#36890;&#36807;TF-IDF&#34920;&#31034;&#21644;&#38544;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#27169;&#22411;&#65292;&#25991;&#26412;&#20449;&#24687;&#30340;&#21253;&#21547;&#37096;&#20998;&#24674;&#22797;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
University admission at many highly selective institutions uses a holistic review process, where all aspects of the application, including protected attributes (e.g., race, gender), grades, essays, and recommendation letters are considered, to compose an excellent and diverse class. In this study, we empirically evaluate how influential protected attributes are for predicting admission decisions using a machine learning (ML) model, and in how far textual information (e.g., personal essay, teacher recommendation) may substitute for the loss of protected attributes in the model. Using data from 14,915 applicants to an undergraduate admission office at a selective U.S. institution in the 2022-2023 cycle, we find that the exclusion of protected attributes from the ML model leads to substantially reduced admission-prediction performance. The inclusion of textual information via both a TF-IDF representation and a Latent Dirichlet allocation (LDA) model partially restores model performance, b
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22240;&#26524;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#19979;&#30340;&#20849;&#21516;&#21407;&#22240;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#26469;&#35782;&#21035;&#20849;&#21516;&#21407;&#22240;C&#65292;&#19982;&#26368;&#22823;&#29109;&#21407;&#21017;&#23494;&#20999;&#30456;&#20851;&#12290;&#23545;&#20110;&#20004;&#20010;&#20108;&#20803;&#23545;&#31216;&#21464;&#37327;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#31867;&#20284;&#20110;&#20108;&#38454;&#30456;&#21464;&#30340;&#26465;&#20214;&#27010;&#29575;&#38750;&#35299;&#26512;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.17557</link><description>&lt;p&gt;
&#26368;&#21487;&#33021;&#30340;&#20849;&#21516;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
The most likely common cause. (arXiv:2306.17557v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17557
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22240;&#26524;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#19979;&#30340;&#20849;&#21516;&#21407;&#22240;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#26469;&#35782;&#21035;&#20849;&#21516;&#21407;&#22240;C&#65292;&#19982;&#26368;&#22823;&#29109;&#21407;&#21017;&#23494;&#20999;&#30456;&#20851;&#12290;&#23545;&#20110;&#20004;&#20010;&#20108;&#20803;&#23545;&#31216;&#21464;&#37327;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#31867;&#20284;&#20110;&#20108;&#38454;&#30456;&#21464;&#30340;&#26465;&#20214;&#27010;&#29575;&#38750;&#35299;&#26512;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;A&#21644;B&#30340;&#20849;&#21516;&#21407;&#22240;&#21407;&#21017;&#22312;&#22240;&#26524;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24403;&#23427;&#20204;&#30340;&#20849;&#21516;&#21407;&#22240;C&#34987;&#35748;&#20026;&#24050;&#32463;&#23384;&#22312;&#65292;&#20294;&#21482;&#35266;&#27979;&#21040;&#20102;A&#21644;B&#30340;&#32852;&#21512;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;C&#19981;&#33021;&#34987;&#21807;&#19968;&#30830;&#23450;&#65288;&#28508;&#22312;&#28151;&#26434;&#22240;&#23376;&#38382;&#39064;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24191;&#20041;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#19988;&#20801;&#35768;&#35782;&#21035;&#19982;&#20849;&#21516;&#21407;&#22240;&#21407;&#21017;&#19968;&#33268;&#30340;C&#12290;&#23427;&#19982;&#26368;&#22823;&#29109;&#21407;&#21017;&#23494;&#20999;&#30456;&#20851;&#12290;&#23545;&#20004;&#20010;&#20108;&#20803;&#23545;&#31216;&#21464;&#37327;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26465;&#20214;&#27010;&#29575;&#30340;&#38750;&#35299;&#26512;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#20108;&#38454;&#30456;&#21464;&#12290;&#36825;&#21457;&#29983;&#22312;&#35266;&#23519;&#21040;&#30340;&#27010;&#29575;&#20998;&#24067;&#20174;&#30456;&#20851;&#21040;&#21453;&#30456;&#20851;&#30340;&#36807;&#28193;&#26399;&#38388;&#12290;&#35752;&#35770;&#20102;&#24191;&#20041;&#20284;&#28982;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;&#39044;&#27979;&#20284;&#28982;&#21644;&#26368;&#23567;&#20849;&#21516;&#21407;&#22240;&#29109;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The common cause principle for two random variables $A$ and $B$ is examined in the case of causal insufficiency, when their common cause $C$ is known to exist, but only the joint probability of $A$ and $B$ is observed. As a result, $C$ cannot be uniquely identified (the latent confounder problem). We show that the generalized maximum likelihood method can be applied to this situation and allows identification of $C$ that is consistent with the common cause principle. It closely relates to the maximum entropy principle. Investigation of the two binary symmetric variables reveals a non-analytic behavior of conditional probabilities reminiscent of a second-order phase transition. This occurs during the transition from correlation to anti-correlation in the observed probability distribution. The relation between the generalized likelihood approach and alternative methods, such as predictive likelihood and the minimum common cause entropy, is discussed. The consideration of the common cause
&lt;/p&gt;</description></item><item><title>TTSWING&#26159;&#19968;&#31181;&#19987;&#20026;&#20050;&#20051;&#29699;&#25381;&#25293;&#20998;&#26512;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#38598;&#25104;&#20256;&#24863;&#22120;&#33719;&#21462;&#35814;&#32454;&#20449;&#24687;&#24182;&#19982;&#36816;&#21160;&#21592;&#25968;&#25454;&#19968;&#36215;&#21457;&#24067;&#12290;&#23545;&#20110;&#20050;&#20051;&#29699;&#20998;&#26512;&#30340;&#21019;&#26032;&#30740;&#31350;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23545;&#31185;&#23398;&#30028;&#26469;&#35828;&#26159;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.17550</link><description>&lt;p&gt;
TTSWING&#65306;&#19968;&#31181;&#29992;&#20110;&#20050;&#20051;&#29699;&#25381;&#25293;&#20998;&#26512;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TTSWING: a Dataset for Table Tennis Swing Analysis. (arXiv:2306.17550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17550
&lt;/p&gt;
&lt;p&gt;
TTSWING&#26159;&#19968;&#31181;&#19987;&#20026;&#20050;&#20051;&#29699;&#25381;&#25293;&#20998;&#26512;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#38598;&#25104;&#20256;&#24863;&#22120;&#33719;&#21462;&#35814;&#32454;&#20449;&#24687;&#24182;&#19982;&#36816;&#21160;&#21592;&#25968;&#25454;&#19968;&#36215;&#21457;&#24067;&#12290;&#23545;&#20110;&#20050;&#20051;&#29699;&#20998;&#26512;&#30340;&#21019;&#26032;&#30740;&#31350;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23545;&#31185;&#23398;&#30028;&#26469;&#35828;&#26159;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TTSWING&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20050;&#20051;&#29699;&#25381;&#25293;&#20998;&#26512;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#38598;&#25104;&#21040;&#23450;&#21046;&#30340;&#29699;&#25293;&#25569;&#25226;&#19978;&#30340;9&#36724;&#20256;&#24863;&#22120;&#33719;&#21462;&#20102;&#35814;&#32454;&#30340;&#25381;&#25293;&#20449;&#24687;&#65292;&#24182;&#38468;&#24102;&#20102;&#36816;&#21160;&#21592;&#30340;&#21311;&#21517;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#27880;&#37322;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25381;&#25293;&#20998;&#26512;&#30740;&#31350;&#12290;TTSWING&#22312;&#20419;&#36827;&#20050;&#20051;&#29699;&#20998;&#26512;&#30340;&#21019;&#26032;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#19988;&#23545;&#31185;&#23398;&#30028;&#26469;&#35828;&#26159;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#22312;https://github.com/DEPhantom/TTSWING&#19978;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce TTSWING, a novel dataset designed for table tennis swing analysis. This dataset comprises comprehensive swing information obtained through 9-axis sensors integrated into custom-made racket grips, accompanied by anonymized demographic data of the players. We detail the data collection and annotation procedures. Furthermore, we conduct pilot studies utilizing diverse machine learning models for swing analysis. TTSWING holds tremendous potential to facilitate innovative research in table tennis analysis and is a valuable resource for the scientific community. We release the dataset and experimental codes at https://github.com/DEPhantom/TTSWING.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#22320;&#22270;&#26469;&#25913;&#36827;&#21160;&#24577;&#36710;&#36742;&#26816;&#27979;&#65292;&#26080;&#38656;&#20351;&#29992;3D&#22320;&#22270;&#25110;&#20687;&#32032;&#32423;&#22320;&#22270;&#26597;&#35810;&#23545;&#24212;&#12290;&#36890;&#36807;&#35270;&#35273;&#22330;&#25152;&#35782;&#21035;&#21644;&#20108;&#36827;&#21046;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25104;&#21151;&#20248;&#21270;&#20102;&#21021;&#22987;&#30340;&#20505;&#36873;&#29289;&#20307;&#26816;&#27979;&#65292;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#22312;&#24694;&#21155;&#30340;&#22825;&#27668;&#21644;&#20809;&#29031;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17536</link><description>&lt;p&gt;
&#36890;&#36807;&#24694;&#21155;&#24773;&#20917;&#19979;&#30340;&#35270;&#35273;&#22330;&#25152;&#35782;&#21035;&#25913;&#36827;&#21160;&#24577;&#36710;&#36742;&#26816;&#27979;&#8212;&#8212;&#36801;&#31227;&#23545;&#35937;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
DisPlacing Objects: Improving Dynamic Vehicle Detection via Visual Place Recognition under Adverse Conditions. (arXiv:2306.17536v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#22320;&#22270;&#26469;&#25913;&#36827;&#21160;&#24577;&#36710;&#36742;&#26816;&#27979;&#65292;&#26080;&#38656;&#20351;&#29992;3D&#22320;&#22270;&#25110;&#20687;&#32032;&#32423;&#22320;&#22270;&#26597;&#35810;&#23545;&#24212;&#12290;&#36890;&#36807;&#35270;&#35273;&#22330;&#25152;&#35782;&#21035;&#21644;&#20108;&#36827;&#21046;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25104;&#21151;&#20248;&#21270;&#20102;&#21021;&#22987;&#30340;&#20505;&#36873;&#29289;&#20307;&#26816;&#27979;&#65292;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#22312;&#24694;&#21155;&#30340;&#22825;&#27668;&#21644;&#20809;&#29031;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24694;&#21155;&#30340;&#22825;&#27668;&#21644;&#20809;&#29031;&#26465;&#20214;&#19979;&#65292;&#30693;&#36947;&#33258;&#24049;&#25152;&#22788;&#30340;&#20301;&#32622;&#26159;&#21542;&#26377;&#21161;&#20110;&#24863;&#30693;&#21608;&#22260;&#30340;&#29289;&#20307;&#65311;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#20808;&#39564;&#22320;&#22270;&#26469;&#24110;&#21161;&#26816;&#27979;&#22330;&#26223;&#20013;&#30340;&#21160;&#24577;&#29289;&#20307;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;3D&#22320;&#22270;&#25110;&#20687;&#32032;&#32423;&#22320;&#22270;&#26597;&#35810;&#23545;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#20808;&#39564;&#22320;&#22270;&#20248;&#21270;&#21021;&#22987;&#30340;&#20505;&#36873;&#29289;&#20307;&#26816;&#27979;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#32463;&#36807;&#31934;&#30830;&#20462;&#27491;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35270;&#35273;&#22330;&#25152;&#35782;&#21035;&#65288;VPR&#65289;&#26469;&#20026;&#32473;&#23450;&#30340;&#26597;&#35810;&#22270;&#20687;&#26816;&#32034;&#21442;&#32771;&#22320;&#22270;&#22270;&#20687;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#20010;&#20108;&#36827;&#21046;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#27604;&#36739;&#26597;&#35810;&#21644;&#21442;&#32771;&#22270;&#20687;&#21306;&#22495;&#20197;&#39564;&#35777;&#26597;&#35810;&#26816;&#27979;&#12290;&#24403;&#25105;&#20204;&#30340;&#20998;&#31867;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#65292;&#22312;&#22823;&#32422;1000&#23545;&#26597;&#35810;-&#22320;&#22270;&#22270;&#20687;&#23545;&#19978;&#65292;&#19982;&#29616;&#26377;&#30340;&#29616;&#25104;&#36710;&#36742;&#26816;&#27979;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#23427;&#33021;&#22815;&#25552;&#39640;&#36710;&#36742;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can knowing where you are assist in perceiving objects in your surroundings, especially under adverse weather and lighting conditions? In this work we investigate whether a prior map can be leveraged to aid in the detection of dynamic objects in a scene without the need for a 3D map or pixel-level map-query correspondences. We contribute an algorithm which refines an initial set of candidate object detections and produces a refined subset of highly accurate detections using a prior map. We begin by using visual place recognition (VPR) to retrieve a reference map image for a given query image, then use a binary classification neural network that compares the query and mapping image regions to validate the query detection. Once our classification network is trained, on approximately 1000 query-map image pairs, it is able to improve the performance of vehicle detection when combined with an existing off-the-shelf vehicle detector. We demonstrate our approach using standard datasets across
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#36710;&#36742;&#26045;&#21152;&#30340;&#36816;&#21160;&#32422;&#26463;&#26469;&#25913;&#21892;&#35270;&#35273;&#23450;&#20301;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#32972;&#26223;&#19979;&#20351;&#29992;&#21160;&#24577;&#36710;&#36742;&#22312;&#23450;&#20301;&#27969;&#31243;&#20013;&#25552;&#20379;&#26377;&#38480;&#23039;&#24577;&#32422;&#26463;&#20449;&#24687;&#65292;&#20248;&#21270;&#23039;&#24577;&#20272;&#35745;&#24182;&#35745;&#31639;&#26410;&#26469;&#23039;&#24577;&#20272;&#35745;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23450;&#20301;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17529</link><description>&lt;p&gt;
&#38145;&#23450;&#65306;&#21033;&#29992;&#21160;&#24577;&#36710;&#36742;&#26045;&#21152;&#30340;&#36816;&#21160;&#32422;&#26463;&#25913;&#21892;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization. (arXiv:2306.17529v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#36710;&#36742;&#26045;&#21152;&#30340;&#36816;&#21160;&#32422;&#26463;&#26469;&#25913;&#21892;&#35270;&#35273;&#23450;&#20301;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#32972;&#26223;&#19979;&#20351;&#29992;&#21160;&#24577;&#36710;&#36742;&#22312;&#23450;&#20301;&#27969;&#31243;&#20013;&#25552;&#20379;&#26377;&#38480;&#23039;&#24577;&#32422;&#26463;&#20449;&#24687;&#65292;&#20248;&#21270;&#23039;&#24577;&#20272;&#35745;&#24182;&#35745;&#31639;&#26410;&#26469;&#23039;&#24577;&#20272;&#35745;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23450;&#20301;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;6&#33258;&#30001;&#24230;&#30340;&#23450;&#20301;&#21644;SLAM&#31995;&#32479;&#20351;&#29992;&#38745;&#24577;&#22320;&#26631;&#65292;&#20294;&#24573;&#30053;&#21160;&#24577;&#29289;&#20307;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#26377;&#29992;&#22320;&#32435;&#20837;&#21040;&#20856;&#22411;&#30340;&#27969;&#31243;&#20013;&#12290;&#22312;&#24050;&#32463;&#32435;&#20837;&#21160;&#24577;&#29289;&#20307;&#30340;&#24773;&#20917;&#19979;&#65292;&#20856;&#22411;&#30340;&#26041;&#27861;&#23581;&#35797;&#30528;&#30456;&#23545;&#22797;&#26434;&#22320;&#35782;&#21035;&#21644;&#23450;&#20301;&#36825;&#20123;&#29289;&#20307;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#25110;&#36890;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25240;&#20013;&#30340;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#28436;&#31034;&#65292;&#21033;&#29992;&#21160;&#24577;&#36710;&#36742;&#22312;6&#33258;&#30001;&#24230;&#36880;&#24103;PnP-RANSAC&#23450;&#20301;&#27969;&#31243;&#20013;&#25552;&#20379;&#26377;&#38480;&#30340;&#23039;&#24577;&#32422;&#26463;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#36816;&#21160;&#27169;&#22411;&#23545;&#21021;&#22987;&#23039;&#24577;&#20272;&#35745;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26410;&#26469;&#23039;&#24577;&#20272;&#35745;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#29615;&#22659;&#20013;&#30456;&#23545;&#24103;&#38388;&#20301;&#32622;&#30340;&#36816;&#21160;&#26159;&#21542;&#21463;&#21040;&#21160;&#24577;&#36710;&#36742;&#30340;&#32422;&#26463;&#26469;&#35302;&#21457;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26816;&#27979;&#21644;&#35782;&#21035;&#36866;&#21512;&#30340;&#21160;&#24577;&#36710;&#36742;&#26469;&#23450;&#20041;&#36825;&#20123;&#23039;&#24577;&#32422;&#26463;&#65292;&#20197;&#20462;&#25913;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most 6-DoF localization and SLAM systems use static landmarks but ignore dynamic objects because they cannot be usefully incorporated into a typical pipeline. Where dynamic objects have been incorporated, typical approaches have attempted relatively sophisticated identification and localization of these objects, limiting their robustness or general utility. In this research, we propose a middle ground, demonstrated in the context of autonomous vehicles, using dynamic vehicles to provide limited pose constraint information in a 6-DoF frame-by-frame PnP-RANSAC localization pipeline. We refine initial pose estimates with a motion model and propose a method for calculating the predicted quality of future pose estimates, triggered based on whether or not the autonomous vehicle's motion is constrained by the relative frame-to-frame location of dynamic vehicles in the environment. Our approach detects and identifies suitable dynamic vehicles to define these pose constraints to modify a pose f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;OASIS 2&#26412;&#20307;&#20013;&#34920;&#31034;&#36807;&#31243;&#21644;&#31243;&#24207;&#30340;&#34892;&#20026;&#20027;&#20041;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20195;&#29702;&#21644;&#34892;&#20026;&#30340;&#33021;&#21147;&#19982;&#36807;&#31243;&#21644;&#31243;&#24207;&#30340;&#27010;&#24565;&#21270;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#22788;&#29702;&#20195;&#29702;&#24179;&#21488;&#12289;&#36807;&#31243;&#21644;&#31243;&#24207;&#30340;&#22522;&#30784;OWL&#26412;&#20307;&#12290;</title><link>http://arxiv.org/abs/2306.17514</link><description>&lt;p&gt;
&#22312;OASIS 2&#26412;&#20307;&#20013;&#34920;&#31034;&#36807;&#31243;&#21644;&#31243;&#24207;&#30340;&#34892;&#20026;&#20027;&#20041;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A behaviouristic approach to representing processes and procedures in the OASIS 2 ontology. (arXiv:2306.17514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17514
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;OASIS 2&#26412;&#20307;&#20013;&#34920;&#31034;&#36807;&#31243;&#21644;&#31243;&#24207;&#30340;&#34892;&#20026;&#20027;&#20041;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20195;&#29702;&#21644;&#34892;&#20026;&#30340;&#33021;&#21147;&#19982;&#36807;&#31243;&#21644;&#31243;&#24207;&#30340;&#27010;&#24565;&#21270;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#22788;&#29702;&#20195;&#29702;&#24179;&#21488;&#12289;&#36807;&#31243;&#21644;&#31243;&#24207;&#30340;&#22522;&#30784;OWL&#26412;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#23545;&#20110;&#26377;&#25928;&#34920;&#31034;&#36807;&#31243;&#21644;&#31243;&#24207;&#30340;&#22522;&#30784;&#26412;&#20307;&#19981;&#21463;&#24191;&#27867;&#30740;&#31350;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22312;&#38656;&#35201;&#32771;&#34385;&#20934;&#30830;&#25351;&#31034;&#30340;&#23454;&#38469;&#24773;&#26223;&#20013;&#35821;&#20041;&#26041;&#27861;&#30340;&#23454;&#38469;&#37319;&#29992;&#12290;&#27492;&#22806;&#65292;&#34920;&#31034;&#24212;&#21253;&#25324;&#20195;&#29702;&#24212;&#22914;&#20309;&#25191;&#34892;&#19982;&#36807;&#31243;&#20851;&#32852;&#30340;&#25805;&#20316;&#65292;&#26080;&#35770;&#20195;&#29702;&#33021;&#21542;&#25191;&#34892;&#36825;&#20123;&#25805;&#20316;&#65292;&#20197;&#21450;&#21487;&#33021;&#21457;&#25381;&#30340;&#35282;&#33394;&#21644;&#30456;&#20851;&#20107;&#20214;&#12290;OASIS&#26412;&#20307;&#25552;&#20379;&#20102;&#19968;&#20010;&#25429;&#25417;&#20195;&#29702;&#21644;&#20854;&#20132;&#20114;&#30340;&#25104;&#29087;&#27169;&#22411;&#65292;&#20294;&#32570;&#20047;&#34920;&#31034;&#30001;&#20195;&#29702;&#25191;&#34892;&#30340;&#36807;&#31243;&#21644;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;&#36825;&#28608;&#21457;&#20102;&#26412;&#25991;&#20013;&#25152;&#25552;&#20986;&#30340;&#30740;&#31350;&#65292;&#20854;&#25552;&#20379;&#20102;OASIS 2&#26412;&#20307;&#30340;&#25193;&#23637;&#65292;&#23558;&#34920;&#31034;&#20195;&#29702;&#21644;&#20854;&#34892;&#20026;&#30340;&#33021;&#21147;&#19982;&#36807;&#31243;&#21644;&#31243;&#24207;&#30340;&#23436;&#25972;&#27010;&#24565;&#21270;&#30456;&#32467;&#21512;&#12290;&#24635;&#20307;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#22788;&#29702;&#20195;&#29702;&#24179;&#21488;&#12289;&#36807;&#31243;&#21644;&#31243;&#24207;&#30340;&#22522;&#30784;OWL&#26412;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational ontologies devoted to the effective representation of processes and procedures are not widely investigated at present, thereby limiting the practical adoption of semantic approaches in real scenarios where the precise instructions to follow must be considered. Also, the representation ought to include how agents should carry out the actions associated with the process, whether or not agents are able to perform those actions, the possible roles played as well as the related events.  The OASIS ontology provides an established model to capture agents and their interactions but lacks means for representing processes and procedures carried out by agents. This motivates the research presented in this article, which delivers an extension of the OASIS 2 ontology to combine the capabilities for representing agents and their behaviours with the full conceptualization of processes and procedures. The overarching goal is to deliver a foundational OWL ontology that deals with agent pla
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#19968;&#31181;&#31232;&#30095;&#25200;&#21160;&#30340;&#38160;&#21270;&#24863;&#30693;&#26368;&#23567;&#21270;&#20248;&#21270;&#22120;&#65288;Sparse SAM&#65289;&#65292;&#35813;&#20248;&#21270;&#22120;&#36890;&#36807;&#20108;&#36827;&#21046;&#25513;&#30721;&#23454;&#29616;&#31232;&#30095;&#25200;&#21160;&#65292;&#26377;&#25928;&#22320;&#24179;&#28369;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#12290;</title><link>http://arxiv.org/abs/2306.17504</link><description>&lt;p&gt;
&#31232;&#30095;&#25200;&#21160;&#30340;&#38160;&#21270;&#24863;&#30693;&#26368;&#23567;&#21270;&#20248;&#21270;&#22120;&#30340;&#31995;&#32479;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Systematic Investigation of Sparse Perturbed Sharpness-Aware Minimization Optimizer. (arXiv:2306.17504v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#19968;&#31181;&#31232;&#30095;&#25200;&#21160;&#30340;&#38160;&#21270;&#24863;&#30693;&#26368;&#23567;&#21270;&#20248;&#21270;&#22120;&#65288;Sparse SAM&#65289;&#65292;&#35813;&#20248;&#21270;&#22120;&#36890;&#36807;&#20108;&#36827;&#21046;&#25513;&#30721;&#23454;&#29616;&#31232;&#30095;&#25200;&#21160;&#65292;&#26377;&#25928;&#22320;&#24179;&#28369;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#22797;&#26434;&#19988;&#38750;&#20984;&#30340;&#25439;&#22833;&#26223;&#35266;&#32780;&#32463;&#24120;&#22312;&#27867;&#21270;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#38160;&#21270;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26435;&#37325;&#28155;&#21152;&#25200;&#21160;&#26102;&#35757;&#32451;&#25439;&#22833;&#30340;&#26368;&#22823;&#21270;&#21464;&#21270;&#26469;&#24179;&#28369;&#25439;&#22833;&#26223;&#35266;&#12290;&#28982;&#32780;&#65292;SAM&#23545;&#25152;&#26377;&#21442;&#25968;&#30340;&#19981;&#21152;&#21306;&#20998;&#30340;&#25200;&#21160;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#19988;&#23548;&#33268;&#35745;&#31639;&#36807;&#22810;&#65292;&#20004;&#20493;&#20110;&#24120;&#35265;&#20248;&#21270;&#22120;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#35757;&#32451;&#26041;&#26696;Sparse SAM&#65288;SSAM&#65289;&#65292;&#36890;&#36807;&#20108;&#36827;&#21046;&#25513;&#30721;&#23454;&#29616;&#31232;&#30095;&#25200;&#21160;&#12290;&#20026;&#20102;&#33719;&#24471;&#31232;&#30095;&#25513;&#30721;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;Fisher&#20449;&#24687;&#21644;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#30340;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#38750;&#32467;&#26500;&#21270;&#12289;&#32467;&#26500;&#21270;&#21644;N:M&#32467;&#26500;&#21270;&#27169;&#24335;&#65292;&#20197;&#21450;&#23454;&#29616;&#31232;&#30095;&#25200;&#21160;&#30340;&#26174;&#24335;&#21644;&#38544;&#24335;&#24418;&#24335;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;SSAM&#21487;&#20197;&#20197;&#30456;&#21516;&#30340;&#36895;&#24230;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often suffer from poor generalization due to complex and non-convex loss landscapes. Sharpness-Aware Minimization (SAM) is a popular solution that smooths the loss landscape by minimizing the maximized change of training loss when adding a perturbation to the weight. However, indiscriminate perturbation of SAM on all parameters is suboptimal and results in excessive computation, double the overhead of common optimizers like Stochastic Gradient Descent (SGD). In this paper, we propose Sparse SAM (SSAM), an efficient and effective training scheme that achieves sparse perturbation by a binary mask. To obtain the sparse mask, we provide two solutions based on Fisher information and dynamic sparse training, respectively. We investigate the impact of different masks, including unstructured, structured, and $N$:$M$ structured patterns, as well as explicit and implicit forms of implementing sparse perturbation. We theoretically prove that SSAM can converge at the same rate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22768;&#23398;&#32972;&#26223;&#21644;&#38899;&#20301;&#36793;&#30028;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#23616;&#37096;&#26631;&#35760;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20998;&#24067;&#24335;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#29702;&#35299;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2306.17500</link><description>&lt;p&gt;
&#35821;&#38899;&#22768;&#23398;&#32972;&#26223;&#21644;&#24773;&#24863;&#35782;&#21035;&#20851;&#31995;&#30340;&#32463;&#39564;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Empirical Interpretation of the Relationship Between Speech Acoustic Context and Emotion Recognition. (arXiv:2306.17500v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22768;&#23398;&#32972;&#26223;&#21644;&#38899;&#20301;&#36793;&#30028;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#23616;&#37096;&#26631;&#35760;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20998;&#24067;&#24335;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#29702;&#35299;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#23545;&#20110;&#33719;&#21462;&#24773;&#24863;&#26234;&#33021;&#21644;&#29702;&#35299;&#35821;&#38899;&#30340;&#35821;&#22659;&#21547;&#20041;&#33267;&#20851;&#37325;&#35201;&#12290;&#36741;&#38899;-&#20803;&#38899;&#65288;CV&#65289;&#38899;&#20301;&#36793;&#30028;&#30340;&#21464;&#21270;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#32447;&#32034;&#20016;&#23500;&#22768;&#23398;&#32972;&#26223;&#65292;&#20174;&#32780;&#24433;&#21709;SER&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#35821;&#38899;&#24773;&#24863;&#34987;&#35270;&#20026;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#30340;&#19968;&#20010;&#22768;&#23398;&#29255;&#27573;&#30340;&#21333;&#20010;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;&#20013;&#30340;&#38899;&#20301;&#36793;&#30028;&#19981;&#26159;&#31163;&#25955;&#30340;&#20107;&#20214;&#65292;&#22240;&#27492;&#24863;&#30693;&#21040;&#30340;&#24773;&#24863;&#29366;&#24577;&#20063;&#24212;&#22312;&#28508;&#22312;&#36830;&#32493;&#30340;&#26102;&#38388;&#31383;&#21475;&#19978;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#25506;&#35752;&#20102;&#22768;&#23398;&#32972;&#26223;&#21644;&#38899;&#20301;&#36793;&#30028;&#23545;SER&#20013;&#30340;&#23616;&#37096;&#26631;&#35760;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20132;&#21449;&#25968;&#25454;&#38598;&#20998;&#26512;&#23454;&#39564;&#30340;&#32467;&#26524;&#25903;&#25345;&#20351;&#29992;&#20998;&#24067;&#24335;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#29702;&#35299;&#30340;&#22909;&#22788;&#12290;&#23454;&#39564;&#20013;&#65292;&#23558;&#38899;&#20301;&#21644;&#35789;&#35821;&#26144;&#23556;&#21040;&#27880;&#24847;&#21147;&#21521;&#37327;&#20197;&#21450;&#22522;&#39057;&#65292;&#20197;&#35266;&#23519;&#37325;&#21472;&#20998;&#24067;&#21644;&#22240;&#27492;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition (SER) is vital for obtaining emotional intelligence and understanding the contextual meaning of speech. Variations of consonant-vowel (CV) phonemic boundaries can enrich acoustic context with linguistic cues, which impacts SER. In practice, speech emotions are treated as single labels over an acoustic segment for a given time duration. However, phone boundaries within speech are not discrete events, therefore the perceived emotion state should also be distributed over potentially continuous time-windows.  This research explores the implication of acoustic context and phone boundaries on local markers for SER using an attention-based approach. The benefits of using a distributed approach to speech emotion understanding are supported by the results of cross-corpora analysis experiments. Experiments where phones and words are mapped to the attention vectors along with the fundamental frequency to observe the overlapping distributions and thereby the relationship
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Preference Ranking Optimization (PRO)&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#65292;&#37319;&#29992;&#20559;&#22909;&#25490;&#24207;&#30340;&#26041;&#24335;&#26469;&#30452;&#25509;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17492</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#40784;&#30340;&#20559;&#22909;&#25490;&#24207;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference Ranking Optimization for Human Alignment. (arXiv:2306.17492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Preference Ranking Optimization (PRO)&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#65292;&#37319;&#29992;&#20559;&#22909;&#25490;&#24207;&#30340;&#26041;&#24335;&#26469;&#30452;&#25509;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#21253;&#21547;&#35823;&#23548;&#24615;&#20869;&#23481;&#65292;&#24378;&#35843;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#20197;&#30830;&#20445;&#23433;&#20840;&#30340;AI&#31995;&#32479;&#30340;&#24517;&#35201;&#24615;&#12290;&#37319;&#29992;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#37197;&#23545;&#27604;&#36739;&#30340;&#22870;&#21169;&#27169;&#22411;&#19982;Proximal Policy Optimization&#65288;PPO&#65289;&#31561;RL&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#26469;&#20248;&#21270;LLM&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;RLHF&#34920;&#29616;&#20986;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Preference Ranking Optimization&#65288;PRO&#65289;&#20316;&#20026;PPO&#30340;&#21478;&#19968;&#31181;&#30452;&#25509;&#23558;LLM&#19982;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;PRO&#23558;&#37197;&#23545;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#25193;&#23637;&#21040;&#36866;&#24212;&#20219;&#24847;&#38271;&#24230;&#30340;&#20559;&#22909;&#25490;&#24207;&#12290;&#36890;&#36807;&#21453;&#22797;&#23545;&#27604;&#29983;&#25104;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#65292;PRO&#25351;&#23548;LLM&#20248;&#20808;&#32771;&#34385;&#26368;&#20339;&#21709;&#24212;&#65292;&#24182;&#36880;&#28176;&#23545;&#21097;&#20313;&#30340;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;PRO&#23558;&#20154;&#31867;&#23545;&#40784;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#27010;&#29575;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secur AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment by combining a reward model, typically based on Bradley-Terry paired comparison, with an RL algorithm such as Proximal Policy Optimization (PPO) to optimize LLM responses. However, RLHF exhibits complexity, instability, and sensitivity to hyperparameters. In this paper, we propose Preference Ranking Optimization (PRO) as an alternative to PPO for directly aligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise Bradley-Terry comparison to accommodate preference rankings of any length. By iteratively contrasting the likelihood of generating responses, PRO instructs the LLM to prioritize the best response while progressively ranking the remaining responses. In this manner, PRO effectively transforms human alignment into aligning the prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Graphtester&#30340;&#26032;&#24037;&#20855;&#65292;&#29992;&#20110;&#25506;&#32034;&#22270;&#25968;&#25454;&#38598;&#19978;GNNs&#30340;&#29702;&#35770;&#36793;&#30028;&#12290;&#36890;&#36807;&#20998;&#26512;40&#22810;&#20010;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21508;&#31181;GNNs&#24615;&#33021;&#30340;&#19978;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#23545;&#22270;&#21464;&#25442;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Graphtester&#29983;&#25104;&#30340;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.17482</link><description>&lt;p&gt;
Graphtester&#65306; &#22312;&#22270;&#25968;&#25454;&#38598;&#19978;&#25506;&#32034;GNNs&#30340;&#29702;&#35770;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Graphtester: Exploring Theoretical Boundaries of GNNs on Graph Datasets. (arXiv:2306.17482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Graphtester&#30340;&#26032;&#24037;&#20855;&#65292;&#29992;&#20110;&#25506;&#32034;&#22270;&#25968;&#25454;&#38598;&#19978;GNNs&#30340;&#29702;&#35770;&#36793;&#30028;&#12290;&#36890;&#36807;&#20998;&#26512;40&#22810;&#20010;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21508;&#31181;GNNs&#24615;&#33021;&#30340;&#19978;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#23545;&#22270;&#21464;&#25442;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Graphtester&#29983;&#25104;&#30340;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#22312;&#21487;&#20197;&#21306;&#20998;&#30340;&#32467;&#26500;&#26041;&#38754;&#20063;&#26377;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#32593;&#32476;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#29616;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Graphtester&#30340;&#26032;&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#24471;&#20998;&#30340;GNNs&#30340;&#29702;&#35770;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;Graphtester&#20998;&#26512;&#20102;40&#22810;&#20010;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#26681;&#25454;&#23618;&#25968;&#30830;&#23450;&#20102;&#21508;&#31181;GNNs&#24615;&#33021;&#30340;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#23545;&#20351;&#29992;&#20301;&#32622;&#33410;&#28857;&#32534;&#30721;&#30340;&#22270;&#21464;&#25442;&#22120;&#30340;&#36866;&#29992;&#24615;&#65292;&#20174;&#32780;&#25193;&#22823;&#20102;&#20854;&#33539;&#22260;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Graphtester&#29983;&#25104;&#30340;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#22270;&#21464;&#25442;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#65288;&#22914;&#20301;&#32622;&#32534;&#30721;&#65289;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#35813;&#36719;&#20214;&#21253;&#21487;&#20197;&#20813;&#36153;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a powerful tool for learning from graph-structured data. However, even state-of-the-art architectures have limitations on what structures they can distinguish, imposing theoretical limits on what the networks can achieve on different datasets. In this paper, we provide a new tool called Graphtester for a comprehensive analysis of the theoretical capabilities of GNNs for various datasets, tasks, and scores. We use Graphtester to analyze over 40 different graph datasets, determining upper bounds on the performance of various GNNs based on the number of layers. Further, we show that the tool can also be used for Graph Transformers using positional node encodings, thereby expanding its scope. Finally, we demonstrate that features generated by Graphtester can be used for practical applications such as Graph Transformers, and provide a synthetic dataset to benchmark node and edge features, such as positional encodings. The package is freely availa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-4&#22312;&#20154;&#24037;&#26234;&#33021;&#35838;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#23398;&#20064;&#30446;&#26631;&#30340;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#23398;&#20064;&#30446;&#26631;&#30340;&#37325;&#35201;&#24615;&#21644;&#25776;&#20889;&#39640;&#36136;&#37327;&#30446;&#26631;&#30340;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17459</link><description>&lt;p&gt;
&#22312;&#35838;&#31243;&#35774;&#35745;&#20013;&#21033;&#29992;LLMs: &#20351;&#29992;GPT-4&#25903;&#25345;&#23398;&#20064;&#30446;&#26631;&#30340;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Harnessing LLMs in Curricular Design: Using GPT-4 to Support Authoring of Learning Objectives. (arXiv:2306.17459v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-4&#22312;&#20154;&#24037;&#26234;&#33021;&#35838;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#23398;&#20064;&#30446;&#26631;&#30340;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#23398;&#20064;&#30446;&#26631;&#30340;&#37325;&#35201;&#24615;&#21644;&#25776;&#20889;&#39640;&#36136;&#37327;&#30446;&#26631;&#30340;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT-4&#65289;&#22312;&#23454;&#36341;&#23548;&#21521;&#30340;&#20154;&#24037;&#26234;&#33021;&#22823;&#23398;&#35838;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#23398;&#20064;&#30446;&#26631;&#65288;LOs&#65289;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#25945;&#32946;&#20013;&#36825;&#19968;&#26032;&#20852;&#25216;&#26415;&#30340;&#26426;&#20250;&#65288;&#20363;&#22914;&#20869;&#23481;&#29983;&#25104;&#65292;&#35299;&#37322;&#65289;&#21644;&#39118;&#38505;&#65288;&#20363;&#22914;&#20316;&#24330;&#65289;&#30340;&#35752;&#35770;&#26085;&#30410;&#21152;&#24378;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#36824;&#27809;&#26377;&#19968;&#39033;&#30740;&#31350;&#35780;&#20272;&#27169;&#22411;&#22312;&#35838;&#31243;&#35774;&#35745;&#21644;LOs&#25776;&#20889;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;LOs&#28165;&#26224;&#34920;&#36798;&#20102;&#23398;&#20064;&#32773;&#36890;&#36807;&#21442;&#19982;&#35838;&#31243;&#25152;&#26399;&#26395;&#33719;&#24471;&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#20026;&#20102;&#26377;&#25928;&#65292;LOs&#24517;&#39035;&#19987;&#27880;&#20110;&#23398;&#29983;&#30340;&#39044;&#26399;&#25104;&#23601;&#65292;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#19988;&#21487;&#20197;&#34913;&#37327;&#12290;&#22240;&#27492;&#65292;&#25776;&#20889;&#39640;&#36136;&#37327;LOs&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#65288;&#21363;&#26114;&#36149;&#65289;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23545;127&#20010;LOs&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#20123;LOs&#26159;&#22522;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65288;&#20851;&#20110;&#39640;&#36136;&#37327;LOs&#25776;&#20889;&#30340;&#35814;&#32454;&#25351;&#21335;&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluated the capability of a generative pre-trained transformer (GPT-4) to automatically generate high-quality learning objectives (LOs) in the context of a practically oriented university course on Artificial Intelligence. Discussions of opportunities (e.g., content generation, explanation) and risks (e.g., cheating) of this emerging technology in education have intensified, but to date there has not been a study of the models' capabilities in supporting the course design and authoring of LOs. LOs articulate the knowledge and skills learners are intended to acquire by engaging with a course. To be effective, LOs must focus on what students are intended to achieve, focus on specific cognitive processes, and be measurable. Thus, authoring high-quality LOs is a challenging and time consuming (i.e., expensive) effort. We evaluated 127 LOs that were automatically generated based on a carefully crafted prompt (detailed guidelines on high-quality LOs authoring) submitted to GPT-4 for con
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21462;&#26679;&#25366;&#25496;&#25216;&#26415;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28145;&#24230;&#24863;&#30693;&#25366;&#25496;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#28145;&#24230;&#39044;&#27979;&#36136;&#37327;&#30340;&#35780;&#20272;&#21644;&#25366;&#25496;&#65292;&#25913;&#36827;&#20102;&#21333;&#30446;3D&#26816;&#27979;&#20013;&#30340;&#28145;&#24230;&#24863;&#30693;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17450</link><description>&lt;p&gt;
GMM&#65306;&#28145;&#20837;&#30740;&#31350;&#26799;&#24230;&#24863;&#30693;&#21644;&#27169;&#22411;&#24863;&#30693;&#28145;&#24230;&#25366;&#25496;&#29992;&#20110;&#21333;&#30446;3D&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GMM: Delving into Gradient Aware and Model Perceive Depth Mining for Monocular 3D Detection. (arXiv:2306.17450v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21462;&#26679;&#25366;&#25496;&#25216;&#26415;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28145;&#24230;&#24863;&#30693;&#25366;&#25496;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#28145;&#24230;&#39044;&#27979;&#36136;&#37327;&#30340;&#35780;&#20272;&#21644;&#25366;&#25496;&#65292;&#25913;&#36827;&#20102;&#21333;&#30446;3D&#26816;&#27979;&#20013;&#30340;&#28145;&#24230;&#24863;&#30693;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24863;&#30693;&#26159;&#21333;&#30446;3D&#26816;&#27979;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#24120;&#28041;&#21450;&#21040;&#19981;&#36866;&#23450;&#38382;&#39064;&#12290;&#37492;&#20110;&#22312;2D&#29289;&#20307;&#26816;&#27979;&#20013;&#21462;&#26679;&#25366;&#25496;&#25216;&#26415;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25366;&#25496;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#36827;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#28145;&#24230;&#24863;&#30693;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#24230;&#37327;&#26469;&#35780;&#20272;&#28145;&#24230;&#39044;&#27979;&#30340;&#36136;&#37327;&#65292;&#36873;&#25321;&#25366;&#25496;&#26679;&#26412;&#29992;&#20110;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26799;&#24230;&#24863;&#30693;&#21644;&#27169;&#22411;&#24863;&#30693;&#25366;&#25496;&#31574;&#30053;&#65288;GMM&#65289;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#31616;&#21333;&#25366;&#25496;&#21033;&#29992;&#39044;&#27979;&#30340;&#28145;&#24230;&#36136;&#37327;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;GMM&#26159;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#24212;&#29992;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#21333;&#30446;3D&#26816;&#27979;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#30456;&#24403;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#21462;&#26679;&#25366;&#25496;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth perception is a crucial component of monoc-ular 3D detection tasks that typically involve ill-posed problems. In light of the success of sample mining techniques in 2D object detection, we propose a simple yet effective mining strategy for improving depth perception in 3D object detection. Concretely, we introduce a plain metric to evaluate the quality of depth predictions, which chooses the mined sample for the model. Moreover, we propose a Gradient-aware and Model-perceive Mining strategy (GMM) for depth learning, which exploits the predicted depth quality for better depth learning through easy mining. GMM is a general strategy that can be readily applied to several state-of-the-art monocular 3D detectors, improving the accuracy of depth prediction. Extensive experiments on the nuScenes dataset demonstrate that the proposed methods significantly improve the performance of 3D object detection while outperforming other state-of-the-art sample mining techniques by a considerable m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#36816;&#21160;&#25216;&#33021;&#65288;DEMOS&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#25955;&#25511;&#21046;&#30005;&#26426;&#32452;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17411</link><description>&lt;p&gt;
&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#21435;&#20013;&#24515;&#21270;&#36816;&#21160;&#25216;&#33021;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Motor Skill Learning for Complex Robotic Systems. (arXiv:2306.17411v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17411
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#36816;&#21160;&#25216;&#33021;&#65288;DEMOS&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#25955;&#25511;&#21046;&#30005;&#26426;&#32452;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#65288;&#20363;&#22914;&#22235;&#36275;&#21160;&#20316;&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#36890;&#24120;&#34987;&#23454;&#29616;&#20026;&#19968;&#20010;&#20855;&#26377;&#36830;&#25509;&#35266;&#23519;&#36755;&#20837;&#30340;&#21333;&#19968;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#25152;&#23545;&#24212;&#30340;&#23398;&#20064;&#31574;&#30053;&#39640;&#24230;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#12290;&#30001;&#20110;&#25152;&#26377;&#30005;&#26426;&#20197;&#38598;&#20013;&#30340;&#26041;&#24335;&#36827;&#34892;&#25511;&#21046;&#65292;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23616;&#37096;&#35266;&#23519;&#21487;&#20197;&#36890;&#36807;&#21333;&#19968;&#32806;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#24433;&#21709;&#20840;&#23616;&#30005;&#26426;&#12290;&#30456;&#21453;&#65292;&#21160;&#29289;&#21644;&#20154;&#31867;&#21487;&#20197;&#20998;&#21035;&#25511;&#21046;&#20182;&#20204;&#30340;&#32930;&#20307;&#12290;&#21463;&#21040;&#36825;&#19968;&#29983;&#29289;&#29616;&#35937;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#25955;&#24335;&#36816;&#21160;&#25216;&#33021;&#65288;DEMOS&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#33258;&#21160;&#21457;&#29616;&#21487;&#20197;&#30456;&#20114;&#35299;&#32806;&#30340;&#30005;&#26426;&#32452;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#30340;&#36816;&#21160;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;&#23545;&#22235;&#36275;&#21644;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#22312;&#24615;&#33021;&#19978;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has achieved remarkable success in complex robotic systems (eg. quadruped locomotion). In previous works, the RL-based controller was typically implemented as a single neural network with concatenated observation input. However, the corresponding learned policy is highly task-specific. Since all motors are controlled in a centralized way, out-of-distribution local observations can impact global motors through the single coupled neural network policy. In contrast, animals and humans can control their limbs separately. Inspired by this biological phenomenon, we propose a Decentralized motor skill (DEMOS) learning algorithm to automatically discover motor groups that can be decoupled from each other while preserving essential connections and then learn a decentralized motor control policy. Our method improves the robustness and generalization of the policy without sacrificing performance. Experiments on quadruped and humanoid robots demonstrate that the learned
&lt;/p&gt;</description></item><item><title>LMBot&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.17408</link><description>&lt;p&gt;
LMBot: &#23558;&#22270;&#24418;&#30693;&#35782;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#26080;&#22270;&#24418;&#37096;&#32626;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection. (arXiv:2306.17408v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17408
&lt;/p&gt;
&lt;p&gt;
LMBot&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#34892;&#20026;&#32773;&#20351;&#29992;&#36234;&#26469;&#36234;&#20808;&#36827;&#21644;&#24191;&#27867;&#30340;&#26426;&#22120;&#20154;&#26469;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#25805;&#32437;&#33286;&#35770;&#65292;&#25512;&#29305;&#26426;&#22120;&#20154;&#30340;&#26816;&#27979;&#24050;&#25104;&#20026;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#25512;&#29702;&#20381;&#36182;&#20110;&#36317;&#31163;&#30446;&#26631;&#29992;&#25143;&#22810;&#36339;&#30340;&#37051;&#23621;&#29992;&#25143;&#65292;&#24182;&#19988;&#33719;&#21462;&#37051;&#23621;&#29992;&#25143;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#21487;&#33021;&#24341;&#20837;&#20559;&#24046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#19978;&#24494;&#35843;&#21518;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31454;&#20105;&#24615;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#22270;&#24418;&#32467;&#26500;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;LMBot&#65292;&#23427;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#30693;&#35782;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;(LMs)&#65292;&#20197;&#22312;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#36827;&#34892;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;LMBot&#23545;&#22522;&#20110;&#22270;&#24418;&#21644;&#19981;&#20351;&#29992;&#22270;&#24418;&#30340;&#25968;&#25454;&#38598;&#20860;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#19968;&#27573;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
As malicious actors employ increasingly advanced and widespread bots to disseminate misinformation and manipulate public opinion, the detection of Twitter bots has become a crucial task. Though graph-based Twitter bot detection methods achieve state-of-the-art performance, we find that their inference depends on the neighbor users multi-hop away from the targets, and fetching neighbors is time-consuming and may introduce bias. At the same time, we find that after finetuning on Twitter bot detection, pretrained language models achieve competitive performance and do not require a graph structure during deployment. Inspired by this finding, we propose a novel bot detection framework LMBot that distills the knowledge of graph neural networks (GNNs) into language models (LMs) for graph-less deployment in Twitter bot detection to combat the challenge of data dependency. Moreover, LMBot is compatible with graph-based and graph-less datasets. Specifically, we first represent each user as a tex
&lt;/p&gt;</description></item><item><title>HVTSurv&#26159;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21270;&#35270;&#35273;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20840;&#24187;&#28783;&#22270;&#20687;&#39044;&#27979;&#24739;&#32773;&#30340;&#29983;&#23384;&#24773;&#20917;&#12290;&#20854;&#36890;&#36807;&#29305;&#24449;&#39044;&#22788;&#29702;&#21644;&#36880;&#23618;&#32534;&#30721;&#30340;&#26041;&#24335;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#23616;&#37096;&#31354;&#38388;&#20449;&#24687;&#12289;&#22686;&#24378;&#19978;&#19979;&#25991;&#24863;&#30693;&#65292;&#24314;&#31435;&#24739;&#32773;&#32423;&#21035;&#30340;&#23618;&#27425;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2306.17373</link><description>&lt;p&gt;
HVTSurv: &#22522;&#20110;&#23618;&#27425;&#21270;&#35270;&#35273;Transformer&#30340;&#20840;&#24187;&#28783;&#22270;&#20687;&#24739;&#32773;&#27700;&#24179;&#29983;&#23384;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HVTSurv: Hierarchical Vision Transformer for Patient-Level Survival Prediction from Whole Slide Image. (arXiv:2306.17373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17373
&lt;/p&gt;
&lt;p&gt;
HVTSurv&#26159;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21270;&#35270;&#35273;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20840;&#24187;&#28783;&#22270;&#20687;&#39044;&#27979;&#24739;&#32773;&#30340;&#29983;&#23384;&#24773;&#20917;&#12290;&#20854;&#36890;&#36807;&#29305;&#24449;&#39044;&#22788;&#29702;&#21644;&#36880;&#23618;&#32534;&#30721;&#30340;&#26041;&#24335;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#23616;&#37096;&#31354;&#38388;&#20449;&#24687;&#12289;&#22686;&#24378;&#19978;&#19979;&#25991;&#24863;&#30693;&#65292;&#24314;&#31435;&#24739;&#32773;&#32423;&#21035;&#30340;&#23618;&#27425;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20840;&#24187;&#28783;&#22270;&#20687;&#65288;WSIs&#65289;&#36827;&#34892;&#24739;&#32773;&#27700;&#24179;&#30340;&#29983;&#23384;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#21040;&#24739;&#32773;&#30340;&#22823;&#37327;&#25968;&#25454;&#65288;&#21333;&#20010;&#25110;&#22810;&#20010;&#21315;&#20806;&#20687;&#32032;WSIs&#65289;&#21644;WSI&#30340;&#19981;&#35268;&#21017;&#24418;&#29366;&#29305;&#24615;&#65292;&#22240;&#27492;&#24456;&#38590;&#23436;&#20840;&#25506;&#32034;&#24739;&#32773;&#27700;&#24179;&#32972;&#21253;&#20013;&#30340;&#31354;&#38388;&#12289;&#19978;&#19979;&#25991;&#21644;&#23618;&#27425;&#20132;&#20114;&#12290;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#38543;&#26426;&#37319;&#26679;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;WSI&#32423;&#21035;&#30340;&#32858;&#21512;&#27169;&#22411;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#22320;&#20002;&#22833;&#20102;&#24739;&#32773;&#27700;&#24179;&#32972;&#21253;&#20013;&#30340;&#20851;&#38190;&#39044;&#21518;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HVTSurv&#30340;&#23618;&#27425;&#21270;&#35270;&#35273;Transformer&#26694;&#26550;&#65292;&#21487;&#20197;&#32534;&#30721;&#23616;&#37096;&#32423;&#21035;&#30340;&#30456;&#23545;&#31354;&#38388;&#20449;&#24687;&#65292;&#22686;&#24378;WSI&#32423;&#21035;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#36890;&#20449;&#65292;&#24182;&#24314;&#31435;&#24739;&#32773;&#32423;&#21035;&#30340;&#23618;&#27425;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival prediction based on whole slide images (WSIs) is a challenging task for patient-level multiple instance learning (MIL). Due to the vast amount of data for a patient (one or multiple gigapixels WSIs) and the irregularly shaped property of WSI, it is difficult to fully explore spatial, contextual, and hierarchical interaction in the patient-level bag. Many studies adopt random sampling pre-processing strategy and WSI-level aggregation models, which inevitably lose critical prognostic information in the patient-level bag. In this work, we propose a hierarchical vision Transformer framework named HVTSurv, which can encode the local-level relative spatial information, strengthen WSI-level context-aware communication, and establish patient-level hierarchical interaction. Firstly, we design a feature pre-processing strategy, including feature rearrangement and random window masking. Then, we devise three layers to progressively obtain patient-level representation, including a local-l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#23581;&#35797;&#23558;&#24046;&#20998;&#38544;&#31169;&#21644;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#26694;&#26550;&#65288;DPSIAF&#65289;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#29616;&#26377;&#30340;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#36731;&#26494;&#25913;&#36896;&#20026;&#31169;&#26377;&#29256;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31169;&#26377;&#31639;&#27861;&#30340;&#24615;&#33021;&#19981;&#20005;&#26684;&#21463;&#21040;&#38544;&#31169;&#39044;&#31639;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17370</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#21487;&#33021;&#22312;&#38500;&#20102;&#20445;&#25252;&#38544;&#31169;&#20043;&#22806;&#23545;&#19968;&#20123;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#20135;&#29983;&#28508;&#22312;&#30340;&#20248;&#21270;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy May Have a Potential Optimization Effect on Some Swarm Intelligence Algorithms besides Privacy-preserving. (arXiv:2306.17370v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#23581;&#35797;&#23558;&#24046;&#20998;&#38544;&#31169;&#21644;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#26694;&#26550;&#65288;DPSIAF&#65289;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#29616;&#26377;&#30340;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#36731;&#26494;&#25913;&#36896;&#20026;&#31169;&#26377;&#29256;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31169;&#26377;&#31639;&#27861;&#30340;&#24615;&#33021;&#19981;&#20005;&#26684;&#21463;&#21040;&#38544;&#31169;&#39044;&#31639;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32487;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#21518;&#65292;&#30446;&#21069;&#26426;&#22120;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#32467;&#21512;&#30740;&#31350;&#21313;&#20998;&#27963;&#36291;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21478;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#8212;&#8212;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#65292;&#34429;&#28982;&#20063;&#24341;&#21457;&#20102;&#38544;&#31169;&#20851;&#20999;&#65292;&#20294;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#32972;&#26223;&#19979;&#21364;&#40092;&#26377;&#30740;&#31350;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#23558;&#24046;&#20998;&#38544;&#31169;&#19982;&#32676;&#20307;&#26234;&#33021;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#26694;&#26550;&#65288;DPSIAF&#65289;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#25351;&#25968;&#26426;&#21046;&#65292;&#21487;&#20197;&#23558;&#29616;&#26377;&#30340;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#36731;&#26494;&#21457;&#23637;&#20026;&#31169;&#26377;&#29256;&#26412;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;DPSIAF&#24212;&#29992;&#20110;&#22235;&#31181;&#24120;&#35265;&#30340;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#30456;&#24212;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#25105;&#20204;&#30340;&#31169;&#26377;&#31639;&#27861;&#26469;&#35828;&#65292;&#20854;&#24615;&#33021;&#24182;&#19981;&#20005;&#26684;&#21463;&#21040;&#38544;&#31169;&#39044;&#31639;&#30340;&#24433;&#21709;&#65292;
&lt;/p&gt;
&lt;p&gt;
Differential privacy (DP), as a promising privacy-preserving model, has attracted great interest from researchers in recent years. Currently, the study on combination of machine learning and DP is vibrant. In contrast, another widely used artificial intelligence technique, the swarm intelligence (SI) algorithm, has received little attention in the context of DP even though it also triggers privacy concerns. For this reason, this paper attempts to combine DP and SI for the first time, and proposes a general differentially private swarm intelligence algorithm framework (DPSIAF). Based on the exponential mechanism, this framework can easily develop existing SI algorithms into the private versions. As examples, we apply the proposed DPSIAF to four popular SI algorithms, and corresponding analyses demonstrate its effectiveness. More interestingly, the experimental results show that, for our private algorithms, their performance is not strictly affected by the privacy budget, and one of the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17366</link><description>&lt;p&gt;
$\lambda$-AC&#65306;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces. (arXiv:2306.17366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#22312;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21363;&#27169;&#22411;&#22312;&#20915;&#31574;&#21046;&#23450;&#26102;&#24212;&#35813;&#26159;&#20934;&#30830;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#21033;&#29992;&#20915;&#31574;&#24863;&#30693;&#25439;&#22833;&#30340;&#31639;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#33021;&#22815;&#23454;&#29616;&#33391;&#22909;&#31639;&#27861;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#31639;&#27861;&#24605;&#24819;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#22312;MuZero&#31995;&#21015;&#24037;&#20316;&#20013;&#25152;&#24314;&#31435;&#30340;&#32463;&#39564;&#24615;&#35774;&#35745;&#20915;&#31574;&#23545;&#20110;&#30456;&#20851;&#31639;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#19981;&#21516;&#30340;&#20215;&#20540;&#24863;&#30693;&#31639;&#27861;&#23454;&#20363;&#20043;&#38388;&#34892;&#20026;&#24046;&#24322;&#12290;&#22312;&#36825;&#20123;&#35265;&#35299;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#27169;&#22411;&#39537;&#21160;&#20915;&#31574;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;$\lambda$-AC&#12290;
&lt;/p&gt;
&lt;p&gt;
The idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decisio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#22312;&#30456;&#20851;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35782;&#21035;&#21151;&#33021;&#26426;&#21046;&#30340;&#21464;&#21270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#25972;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.17361</link><description>&lt;p&gt;
iSCAN&#65306;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models. (arXiv:2306.17361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#22312;&#30456;&#20851;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35782;&#21035;&#21151;&#33021;&#26426;&#21046;&#30340;&#21464;&#21270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#25972;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20197;&#34920;&#31034;&#22797;&#26434;&#31995;&#32479;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#30495;&#27491;&#30340;&#24213;&#23618;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#32467;&#26500;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#20174;&#35266;&#27979;&#25968;&#25454;&#25110;&#24178;&#39044;&#25968;&#25454;&#20013;&#30830;&#23450;&#23427;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#35782;&#21035;&#30456;&#20851;SCM&#20043;&#38388;&#30340;&#22240;&#26524;&#26426;&#21046;&#30340;&#21464;&#21270;(&#36716;&#21464;)&#32780;&#19981;&#26159;&#24674;&#22797;&#25972;&#20010;&#24213;&#23618;DAG&#32467;&#26500;&#12290;&#20363;&#23376;&#21253;&#25324;&#20998;&#26512;&#20581;&#24247;&#21644;&#30284;&#30151;&#24739;&#32773;&#20043;&#38388;&#30340;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#32467;&#26500;&#21464;&#21270;&#65292;&#25110;&#32773;&#22312;&#19981;&#21516;&#32454;&#32990;&#29615;&#22659;&#19979;&#29702;&#35299;&#29983;&#29289;&#36884;&#24452;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#30456;&#21516;&#30340;&#21464;&#37327;&#38598;&#19978;&#35782;&#21035;&#20004;&#20010;&#25110;&#22810;&#20010;&#30456;&#20851;SCM&#20013;&#30340;$\textit{&#21151;&#33021;}$&#26426;&#21046;&#36716;&#21464;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#27599;&#20010;SCM&#30340;&#25972;&#20010;DAG&#32467;&#26500;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20551;&#35774;&#20351;&#29992;&#20102;&#20855;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#32447;&#24615;&#27169;&#22411;&#65307;&#32780;&#26412;&#25991;&#20013;&#25105;&#20204;&#21017;&#32771;&#34385;&#20102;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structural causal models (SCMs) are widely used in various disciplines to represent causal relationships among variables in complex systems. Unfortunately, the true underlying directed acyclic graph (DAG) structure is often unknown, and determining it from observational or interventional data remains a challenging task. However, in many situations, the end goal is to identify changes (shifts) in causal mechanisms between related SCMs rather than recovering the entire underlying DAG structure. Examples include analyzing gene regulatory network structure changes between healthy and cancerous individuals or understanding variations in biological pathways under different cellular contexts. This paper focuses on identifying $\textit{functional}$ mechanism shifts in two or more related SCMs over the same set of variables -$\textit{without estimating the entire DAG structure of each SCM}$. Prior work under this setting assumed linear models with Gaussian noises; instead, in this work we ass
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#35786;&#26029;&#19981;&#30830;&#23450;&#24615;&#30340;&#21307;&#30103;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24403;&#30456;&#21516;&#30340;&#29305;&#24449;&#21487;&#20197;&#23545;&#24212;&#19981;&#21516;&#39118;&#38505;&#30340;&#35786;&#26029;&#26102;&#39118;&#38505;&#20302;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17337</link><description>&lt;p&gt;
&#35786;&#26029;&#19981;&#30830;&#23450;&#27169;&#22411;&#29992;&#20110;&#21307;&#30103;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diagnosis Uncertain Models For Medical Risk Prediction. (arXiv:2306.17337v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17337
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#35786;&#26029;&#19981;&#30830;&#23450;&#24615;&#30340;&#21307;&#30103;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24403;&#30456;&#21516;&#30340;&#29305;&#24449;&#21487;&#20197;&#23545;&#24212;&#19981;&#21516;&#39118;&#38505;&#30340;&#35786;&#26029;&#26102;&#39118;&#38505;&#20302;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#24739;&#32773;&#39118;&#38505;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#33719;&#24471;&#24739;&#32773;&#30340;&#29983;&#21629;&#20307;&#24449;&#12289;&#23454;&#39564;&#23460;&#25351;&#26631;&#21644;&#20808;&#21069;&#30149;&#21490;&#31561;&#29305;&#24449;&#65292;&#20294;&#26080;&#27861;&#33719;&#24471;&#24739;&#32773;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#22312;&#20837;&#38498;&#26102;&#29992;&#20110;&#20998;&#35786;&#30340;&#27169;&#22411;&#20013;&#23601;&#20250;&#36935;&#21040;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;"&#20840;&#22240;&#32032;"&#39118;&#38505;&#27169;&#22411;&#22312;&#21508;&#31181;&#35786;&#26029;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23384;&#22312;&#21487;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#24403;&#30456;&#21516;&#30340;&#23454;&#39564;&#23460;/&#29983;&#21629;&#20307;&#24449;/&#30149;&#21490;&#37197;&#32622;&#21487;&#20197;&#23548;&#33268;&#20855;&#26377;&#19981;&#21516;&#39118;&#38505;&#36718;&#24275;&#30340;&#35786;&#26029;&#26102;&#65288;&#20363;&#22914;&#65292;E.coli&#19982;MRSA&#65289;&#65292;&#39118;&#38505;&#20272;&#35745;&#26159;&#36825;&#20004;&#20010;&#36718;&#24275;&#30340;&#27010;&#29575;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#36825;&#23548;&#33268;&#23545;&#20110;&#32597;&#35265;&#20294;&#39640;&#39118;&#38505;&#30340;&#35786;&#26029;&#39118;&#38505;&#30340;&#20302;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#26126;&#30830;&#22320;&#27169;&#25311;&#30001;&#24739;&#32773;&#35786;&#26029;&#19981;&#30830;&#23450;&#24615;&#24341;&#36215;&#30340;&#39118;&#38505;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#24739;&#32773;&#30340;&#39118;&#38505;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#39118;&#38505;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a patient risk models which has access to patient features such as vital signs, lab values, and prior history but does not have access to a patient's diagnosis. For example, this occurs in a model deployed at intake time for triage purposes. We show that such `all-cause' risk models have good generalization across diagnoses but have a predictable failure mode. When the same lab/vital/history profiles can result from diagnoses with different risk profiles (e.g. E.coli vs. MRSA) the risk estimate is a probability weighted average of these two profiles. This leads to an under-estimation of risk for rare but highly risky diagnoses. We propose a fix for this problem by explicitly modeling the uncertainty in risk prediction coming from uncertainty in patient diagnoses. This gives practitioners an interpretable way to understand patient risk beyond a single risk number.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#21106;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31934;&#30830;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#65292;&#36890;&#36807;&#23884;&#20837;&#20999;&#24179;&#38754;&#27861;&#25214;&#21040;&#20102;&#23481;&#37327;VRP&#30340;&#19979;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.17283</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;Rounded Capacity Inequalities&#30340;&#31070;&#32463;&#20998;&#21106;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Neural Separation Algorithm for the Rounded Capacity Inequalities. (arXiv:2306.17283v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#21106;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31934;&#30830;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#65292;&#36890;&#36807;&#23884;&#20837;&#20999;&#24179;&#38754;&#27861;&#25214;&#21040;&#20102;&#23481;&#37327;VRP&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#24179;&#38754;&#27861;&#26159;&#25104;&#21151;&#30340;&#20998;&#25903;&#23450;&#20215;&#27861;&#21644;&#20998;&#25903;&#20999;&#21106;&#27861;&#31639;&#27861;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRPs&#65289;&#30340;&#30830;&#20999;&#26368;&#20248;&#35299;&#12290;&#22312;&#21508;&#31181;&#20999;&#21106;&#20013;&#65292;&#22278;&#35282;&#23481;&#37327;&#19981;&#31561;&#24335;&#65288;RCIs&#65289;&#26159;&#26368;&#22522;&#26412;&#30340;&#12290;&#29983;&#25104;RCIs&#38656;&#35201;&#35299;&#20915;&#20998;&#21106;&#38382;&#39064;&#65292;&#20854;&#31934;&#30830;&#35299;&#38656;&#35201;&#36739;&#38271;&#30340;&#26102;&#38388;&#33719;&#21462;&#65292;&#22240;&#27492;&#24191;&#27867;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#24102;&#26377;&#22270;&#31895;&#21270;&#30340;&#20998;&#21106;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23398;&#20064;&#31934;&#30830;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#65292;&#32463;&#36807;50&#21040;100&#20010;&#23458;&#25143;&#30340;&#23567;&#23454;&#20363;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20998;&#21106;&#31639;&#27861;&#23884;&#20837;&#20999;&#24179;&#38754;&#27861;&#20013;&#65292;&#20197;&#25214;&#21040;&#23481;&#37327;VRP&#65288;CVRP&#65289;&#30340;&#19979;&#38480;&#65292;&#20854;&#20013;&#21253;&#25324;&#39640;&#36798;1,000&#20010;&#23458;&#25143;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;CVRPSEP&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;CVRPSEP&#26159;&#29992;&#20110;&#35299;&#20915;VRP&#20013;&#21508;&#31181;&#20999;&#21106;&#38382;&#39064;&#30340;&#27969;&#34892;&#20998;&#21106;&#36719;&#20214;&#21253;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;CVRPSEP&#12290;
&lt;/p&gt;
&lt;p&gt;
The cutting plane method is a key technique for successful branch-and-cut and branch-price-and-cut algorithms that find the exact optimal solutions for various vehicle routing problems (VRPs). Among various cuts, the rounded capacity inequalities (RCIs) are the most fundamental. To generate RCIs, we need to solve the separation problem, whose exact solution takes a long time to obtain; therefore, heuristic methods are widely used. We design a learning-based separation heuristic algorithm with graph coarsening that learns the solutions of the exact separation problem with a graph neural network (GNN), which is trained with small instances of 50 to 100 customers. We embed our separation algorithm within the cutting plane method to find a lower bound for the capacitated VRP (CVRP) with up to 1,000 customers. We compare the performance of our approach with CVRPSEP, a popular separation software package for various cuts used in solving VRPs. Our computational results show that our approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#21644;&#31185;&#23398;&#20195;&#30721;&#30340;&#24320;&#21457;&#21644;&#20998;&#26512;&#20013;&#33258;&#21160;&#21270;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.17281</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24314;&#27169;&#24182;&#34892;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Modeling Parallel Programs using Large Language Models. (arXiv:2306.17281v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#21644;&#31185;&#23398;&#20195;&#30721;&#30340;&#24320;&#21457;&#21644;&#20998;&#26512;&#20013;&#33258;&#21160;&#21270;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25105;&#20204;&#36827;&#20837;&#24322;&#26500;&#35745;&#31639;&#26102;&#20195;&#65292;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#20013;&#30340;&#24182;&#34892;&#36719;&#20214;&#20195;&#30721;&#22312;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#19978;&#19981;&#26029;&#22686;&#38271;&#12290;&#26032;&#20852;&#30340;&#30828;&#20214;&#21644;&#32534;&#31243;&#33539;&#24335;&#20351;&#24471;&#24320;&#21457;&#12289;&#20248;&#21270;&#21644;&#32500;&#25252;&#24182;&#34892;&#36719;&#20214;&#23545;&#24320;&#21457;&#20154;&#21592;&#26469;&#35828;&#21464;&#24471;&#32321;&#37325;&#12290;&#32531;&#35299;&#36825;&#20123;&#36127;&#25285;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#20351;&#29992;&#33258;&#21160;&#21270;&#24320;&#21457;&#21644;&#20998;&#26512;&#24037;&#20855;&#12290;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#20026;&#24320;&#21457;&#20154;&#21592;&#25191;&#34892;&#22797;&#26434;&#21644;/&#25110;&#34917;&#25937;&#24615;&#30340;&#20219;&#21153;&#65292;&#25552;&#39640;&#20182;&#20204;&#30340;&#29983;&#20135;&#21147;&#24182;&#20943;&#23569;&#38169;&#35823;&#30340;&#21487;&#33021;&#24615;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#29992;&#20110;&#20195;&#30721;&#24320;&#21457;&#21644;&#24615;&#33021;&#20998;&#26512;&#30340;&#36825;&#20123;&#24037;&#20855;&#22312;&#25191;&#34892;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#35821;&#35328;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#29616;&#22312;&#22312;&#32447;&#19978;&#21487;&#29992;&#30340;&#22823;&#37327;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#24037;&#20855;&#24320;&#22987;&#21033;&#29992;&#39044;&#27979;&#24615;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#23436;&#25104;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#39640;&#24615;&#33021;&#21644;&#31185;&#23398;&#20195;&#30721;&#30340;&#29305;&#23450;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parallel software codes in high performance computing (HPC) continue to grow in complexity and scale as we enter the exascale era. A diverse set of emerging hardware and programming paradigms make developing, optimizing, and maintaining parallel software burdensome for developers. One way to alleviate some of these burdens is with automated development and analysis tools. Such tools can perform complex and/or remedial tasks for developers that increase their productivity and decrease the chance for error. So far, such tools for code development and performance analysis have been limited in the complexity of tasks they can perform. However, with recent advancements in language modeling, and the wealth of code related data that is now available online, these tools have started to utilize predictive language models to automate more complex tasks. In this paper, we show how large language models (LLMs) can be applied to tasks specific to high performance and scientific codes. We train LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26126;&#30830;&#26799;&#24230;&#34920;&#36798;&#24335;&#30340;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#30028;&#38480;&#35777;&#26126;&#20102;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.17279</link><description>&lt;p&gt;
&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#29575;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Constraint for Safety-Critical Reinforcement Learning. (arXiv:2306.17279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26126;&#30830;&#26799;&#24230;&#34920;&#36798;&#24335;&#30340;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#30028;&#38480;&#35777;&#26126;&#20102;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#27010;&#29575;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#23433;&#20840;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23433;&#20840;&#31574;&#30053;&#25110;&#25511;&#21046;&#22120;&#26159;&#25351;&#20197;&#39640;&#27010;&#29575;&#20445;&#25345;&#20195;&#29702;&#22312;&#32473;&#23450;&#23433;&#20840;&#38598;&#21512;&#20013;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#39057;&#32321;&#25506;&#32034;&#30340;&#32047;&#31215;&#32422;&#26463;&#38382;&#39064;&#21644;&#36825;&#31181;&#27010;&#29575;&#32422;&#26463;&#38382;&#39064;&#20043;&#38388;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#30028;&#38480;&#65292;&#38416;&#26126;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#65288;&#32422;&#26463;&#28385;&#36275;&#65289;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#22312;&#22788;&#29702;&#27010;&#29575;&#32422;&#26463;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#25506;&#32034;&#30340;&#37027;&#26679;&#65292;&#28304;&#20110;&#27809;&#26377;&#26126;&#30830;&#30340;&#26799;&#24230;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#36825;&#31181;&#26126;&#30830;&#30340;&#26799;&#24230;&#34920;&#36798;&#24335;&#65292;&#31216;&#20043;&#20026;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#26799;&#24230;SPG-Actor-Critic
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of learning safe policies for probabilistic-constrained reinforcement learning (RL). Specifically, a safe policy or controller is one that, with high probability, maintains the trajectory of the agent in a given safe set. We establish a connection between this probabilistic-constrained setting and the cumulative-constrained formulation that is frequently explored in the existing literature. We provide theoretical bounds elucidating that the probabilistic-constrained setting offers a better trade-off in terms of optimality and safety (constraint satisfaction). The challenge encountered when dealing with the probabilistic constraints, as explored in this work, arises from the absence of explicit expressions for their gradients. Our prior work provides such an explicit gradient expression for probabilistic constraints which we term Safe Policy Gradient-REINFORCE (SPG-REINFORCE). In this work, we provide an improved gradient SPG-Actor-Critic that lead
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17258</link><description>&lt;p&gt;
&#36973;&#21463;&#33510;&#38590;&#30340;&#28900;&#38754;&#21253;&#26426;
&lt;/p&gt;
&lt;p&gt;
Suffering Toasters. (arXiv:2306.17258v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#65292;&#26234;&#33021;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#23450;&#20041;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#30001;&#20110;&#25105;&#20204;&#23545;AI&#33539;&#24335;&#12289;&#26550;&#26500;&#21644;&#24037;&#20855;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#33258;&#28982;&#20135;&#29983;&#30340;AI&#24847;&#35782;&#27604;&#20197;&#24448;&#26356;&#26377;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22768;&#31216;&#25152;&#26377;&#24403;&#21069;&#30340;&#26234;&#33021;&#27979;&#35797;&#37117;&#19981;&#36275;&#20197;&#25351;&#20986;&#23384;&#22312;&#25110;&#32570;&#20047;&#35937;&#20154;&#31867;&#30452;&#35273;&#24863;&#30693;&#30340;&#26234;&#33021;&#12290;&#25105;&#20204;&#20511;&#37492;&#31185;&#23398;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#25552;&#20379;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#30340;&#26356;&#28165;&#26224;&#23450;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#30340;&#26032;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#27010;&#36848;&#20102;&#21487;&#33021;&#30340;&#23454;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#26032;&#21551;&#21457;&#24335;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#26080;&#35770;&#26159;&#21746;&#23398;&#38382;&#39064;&#36824;&#26159;&#23454;&#29616;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A widely accepted definition of intelligence in the context of Artificial Intelligence (AI) still eludes us. Due to our exceedingly rapid development of AI paradigms, architectures, and tools, the prospect of naturally arising AI consciousness seems more likely than ever. In this paper, we claim that all current intelligence tests are insufficient to point to the existence or lack of intelligence \textbf{as humans intuitively perceive it}. We draw from ideas in the philosophy of science, psychology, and other areas of research to provide a clearer definition of the problems of artificial intelligence, self-awareness, and agency. We furthermore propose a new heuristic approach to test for artificial self-awareness and outline a possible implementation. Finally, we discuss some of the questions that arise from this new heuristic, be they philosophical or implementation-oriented.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#39044;&#27979;COVID-19&#24739;&#32773;&#20986;&#38498;&#21518;&#22312;&#24613;&#35786;&#23460;&#30340;&#20877;&#35775;&#24773;&#20917;&#65292;&#26089;&#26399;&#35782;&#21035;&#26377;&#21161;&#20110;&#21307;&#29983;&#19987;&#27880;&#20110;&#21361;&#21450;&#29983;&#21629;&#30340;&#30149;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.17257</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#28304;&#36801;&#31227;&#23398;&#20064;&#39044;&#27979;COVID-19&#24739;&#32773;&#30340;&#24613;&#35786;&#23460;&#20877;&#35775;
&lt;/p&gt;
&lt;p&gt;
Prediction of COVID-19 Patients' Emergency Room Revisit using Multi-Source Transfer Learning. (arXiv:2306.17257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#39044;&#27979;COVID-19&#24739;&#32773;&#20986;&#38498;&#21518;&#22312;&#24613;&#35786;&#23460;&#30340;&#20877;&#35775;&#24773;&#20917;&#65292;&#26089;&#26399;&#35782;&#21035;&#26377;&#21161;&#20110;&#21307;&#29983;&#19987;&#27880;&#20110;&#21361;&#21450;&#29983;&#21629;&#30340;&#30149;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2019&#20896;&#29366;&#30149;&#27602;&#30149;&#65288;COVID-19&#65289;&#23548;&#33268;&#20102;&#19968;&#22330;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20005;&#37325;&#22823;&#27969;&#34892;&#12290;&#38500;&#20102;&#20855;&#26377;&#39640;&#20256;&#26579;&#24615;&#22806;&#65292;COVID-19&#30340;&#20020;&#24202;&#36827;&#23637;&#21487;&#20197;&#26377;&#24456;&#22823;&#24046;&#24322;&#65292;&#20174;&#26080;&#30151;&#29366;&#25658;&#24102;&#32773;&#21040;&#20005;&#37325;&#19988;&#28508;&#22312;&#21361;&#21450;&#29983;&#21629;&#30340;&#20581;&#24247;&#24182;&#21457;&#30151;&#12290;&#35768;&#22810;&#24739;&#32773;&#22312;&#20986;&#38498;&#21518;&#30340;&#30701;&#26102;&#38388;&#20869;&#38656;&#35201;&#20877;&#27425;&#23601;&#35786;&#24613;&#35786;&#23460;&#65288;ER&#65289;&#65292;&#36825;&#26497;&#22823;&#22686;&#21152;&#20102;&#21307;&#21153;&#20154;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#21450;&#26089;&#35782;&#21035;&#27492;&#31867;&#24739;&#32773;&#23545;&#20110;&#24110;&#21161;&#21307;&#29983;&#19987;&#27880;&#20110;&#27835;&#30103;&#21361;&#21450;&#29983;&#21629;&#30340;&#30149;&#20363;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;2020&#24180;3&#26376;&#33267;2021&#24180;1&#26376;&#26399;&#38388;&#21305;&#20857;&#22561;&#22823;&#23398;&#21307;&#23398;&#20013;&#24515;13&#20010;&#38468;&#23646;&#24613;&#35786;&#23460;&#30340;3,210&#20010;&#24739;&#32773;&#23601;&#35786;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;ScispaCy&#25552;&#21462;&#20020;&#24202;&#27010;&#24565;&#65292;&#24182;&#20351;&#29992;&#20986;&#29616;&#26368;&#39057;&#32321;&#30340;1001&#20010;&#27010;&#24565;&#20026;COVID-19&#24739;&#32773;&#22312;&#24613;&#35786;&#23460;&#20013;&#24314;&#31435;&#20102;7&#22825;&#20877;&#35775;&#27169;&#22411;&#12290;&#25105;&#20204;&#20174;13&#20010;&#24613;&#35786;&#23460;&#25910;&#38598;&#30340;&#30740;&#31350;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
The coronavirus disease 2019 (COVID-19) has led to a global pandemic of significant severity. In addition to its high level of contagiousness, COVID-19 can have a heterogeneous clinical course, ranging from asymptomatic carriers to severe and potentially life-threatening health complications. Many patients have to revisit the emergency room (ER) within a short time after discharge, which significantly increases the workload for medical staff. Early identification of such patients is crucial for helping physicians focus on treating life-threatening cases. In this study, we obtained Electronic Health Records (EHRs) of 3,210 encounters from 13 affiliated ERs within the University of Pittsburgh Medical Center between March 2020 and January 2021. We leveraged a Natural Language Processing technique, ScispaCy, to extract clinical concepts and used the 1001 most frequent concepts to develop 7-day revisit models for COVID-19 patients in ERs. The research data we collected from 13 ERs may have 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.17256</link><description>&lt;p&gt;
&#20197;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#26681;&#25454;&#29992;&#25143;&#36807;&#21435;&#30340;&#34892;&#20026;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#21382;&#21490;&#20132;&#20114;&#35760;&#24405;&#19981;&#21487;&#29992;&#26102;&#65292;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#31995;&#32479;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#22312;&#21019;&#19994;&#20225;&#19994;&#25110;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#20854;&#20013;&#31995;&#32479;&#20173;&#28982;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#21382;&#21490;&#29992;&#25143;&#21644;&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#26469;&#20026;&#26032;&#29992;&#25143;&#25110;&#29289;&#21697;&#25552;&#20379;&#25512;&#33616;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#36164;&#26009;&#21644;&#29289;&#21697;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#38656;&#35201;&#32452;&#21512;&#24615;&#21644;&#31995;&#32479;&#24615;&#25512;&#29702;&#30340;&#31639;&#26415;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#26367;&#25442;&#35268;&#21017;&#26469;&#33719;&#24471;&#33021;&#21147;&#65292;&#22312;&#21482;&#35757;&#32451;&#26368;&#31616;&#21333;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35299;&#20915;&#23884;&#22871;&#31639;&#26415;&#34920;&#36798;&#24335;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.17249</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#31616;&#21333;&#31639;&#26415;&#38382;&#39064;&#20013;&#31995;&#32479;&#21270;&#27867;&#21270;&#30340;&#28151;&#21512;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Hybrid System for Systematic Generalization in Simple Arithmetic Problems. (arXiv:2306.17249v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#38656;&#35201;&#32452;&#21512;&#24615;&#21644;&#31995;&#32479;&#24615;&#25512;&#29702;&#30340;&#31639;&#26415;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#26367;&#25442;&#35268;&#21017;&#26469;&#33719;&#24471;&#33021;&#21147;&#65292;&#22312;&#21482;&#35757;&#32451;&#26368;&#31616;&#21333;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35299;&#20915;&#23884;&#22871;&#31639;&#26415;&#34920;&#36798;&#24335;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#38656;&#35201;&#32452;&#21512;&#24615;&#21644;&#31995;&#32479;&#24615;&#30340;&#31526;&#21495;&#25512;&#29702;&#38382;&#39064;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#31526;&#21495;&#25512;&#29702;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#23558;&#25512;&#29702;&#27169;&#24335;&#25512;&#24191;&#21040;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#27979;&#35797;&#26696;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31995;&#32479;&#65292;&#33021;&#22815;&#35299;&#20915;&#38656;&#35201;&#23545;&#31526;&#21495;&#24207;&#21015;&#36827;&#34892;&#32452;&#21512;&#21644;&#31995;&#32479;&#25512;&#29702;&#30340;&#31639;&#26415;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#26367;&#25442;&#35268;&#21017;&#26469;&#33719;&#24471;&#36825;&#31181;&#25216;&#33021;&#65292;&#36825;&#20123;&#35268;&#21017;&#34987;&#36845;&#20195;&#22320;&#24212;&#29992;&#20110;&#36755;&#20837;&#23383;&#31526;&#20018;&#65292;&#30452;&#21040;&#34920;&#36798;&#24335;&#23436;&#20840;&#35299;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#22320;&#35299;&#20915;&#23884;&#22871;&#31639;&#26415;&#34920;&#36798;&#24335;&#65292;&#21363;&#20351;&#20165;&#22312;&#21253;&#25324;&#26368;&#31616;&#21333;&#24773;&#20917;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#24615;&#33021;&#20063;&#26174;&#33879;&#20248;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving symbolic reasoning problems that require compositionality and systematicity is considered one of the key ingredients of human intelligence. However, symbolic reasoning is still a great challenge for deep learning models, which often cannot generalize the reasoning pattern to out-of-distribution test cases. In this work, we propose a hybrid system capable of solving arithmetic problems that require compositional and systematic reasoning over sequences of symbols. The model acquires such a skill by learning appropriate substitution rules, which are applied iteratively to the input string until the expression is completely resolved. We show that the proposed system can accurately solve nested arithmetical expressions even when trained only on a subset including the simplest cases, significantly outperforming both a sequence-to-sequence model trained end-to-end and a state-of-the-art large language model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#26426;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#23545;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#32593;&#32476;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.17187</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#30417;&#27979;&#21644;&#26816;&#27979;&#29289;&#32852;&#32593;&#35774;&#22791;&#20837;&#20405;&#30340;&#26234;&#33021;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
An Intelligent Mechanism for Monitoring and Detecting Intrusions in IoT Devices. (arXiv:2306.17187v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#26426;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#23545;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#32593;&#32476;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#25968;&#37327;&#21450;&#20854;&#38480;&#21046;&#24050;&#32463;&#25104;&#20026;&#24694;&#24847;&#23454;&#20307;&#21033;&#29992;&#36825;&#20123;&#35774;&#22791;&#24182;&#23558;&#20854;&#29992;&#20110;&#33258;&#24049;&#33719;&#21033;&#30340;&#21160;&#26426;&#12290;&#20026;&#20102;&#38450;&#27490;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#19982;&#38598;&#20013;&#24335;&#26041;&#27861;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#24471;&#21040;&#32531;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#26426;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#31070;&#32463;&#32593;&#32476;&#26469;&#39640;&#20934;&#30830;&#24230;&#22320;&#26816;&#27979;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#24182;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current amount of IoT devices and their limitations has come to serve as a motivation for malicious entities to take advantage of such devices and use them for their own gain. To protect against cyberattacks in IoT devices, Machine Learning techniques can be applied to Intrusion Detection Systems. Moreover, privacy related issues associated with centralized approaches can be mitigated through Federated Learning. This work proposes a Host-based Intrusion Detection Systems that leverages Federated Learning and Multi-Layer Perceptron neural networks to detected cyberattacks on IoT devices with high accuracy and enhancing data privacy protection.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#21306;&#22359;&#38142;&#12289;&#26234;&#33021;&#21512;&#32422;&#21644;&#32852;&#37030;&#23398;&#20064;&#22312;&#33021;&#28304;&#20114;&#32852;&#32593;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#20856;&#22411;&#30340;&#31995;&#32479;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.17186</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#20998;&#24067;&#24335;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Blockchain-based Federated Learning for Decentralized Energy Management Systems. (arXiv:2306.17186v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#21306;&#22359;&#38142;&#12289;&#26234;&#33021;&#21512;&#32422;&#21644;&#32852;&#37030;&#23398;&#20064;&#22312;&#33021;&#28304;&#20114;&#32852;&#32593;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#20856;&#22411;&#30340;&#31995;&#32479;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#20114;&#32852;&#32593;&#26159;&#19968;&#31181;&#21033;&#29992;&#26234;&#33021;&#32593;&#32476;&#21644;&#20998;&#24067;&#24335;&#31995;&#32479;&#25216;&#26415;&#23454;&#29616;&#20998;&#25955;&#33021;&#28304;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#27169;&#24335;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#33021;&#28304;&#31995;&#32479;&#65292;&#20998;&#24067;&#24335;&#33021;&#28304;&#20114;&#32852;&#32593;&#31995;&#32479;&#28085;&#30422;&#20102;&#22810;&#20010;&#32452;&#20214;&#21644;&#36890;&#20449;&#35201;&#27714;&#65292;&#38656;&#35201;&#21019;&#26032;&#25216;&#26415;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#12289;&#21487;&#38752;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#21306;&#22359;&#38142;&#26550;&#26500;&#12289;&#26234;&#33021;&#21512;&#32422;&#21644;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#23454;&#29616;&#20998;&#24067;&#24335;&#33021;&#28304;&#20114;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#26412;&#25991;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#24212;&#29992;&#21306;&#22359;&#38142;&#12289;&#26234;&#33021;&#21512;&#32422;&#21644;&#32852;&#37030;&#23398;&#20064;&#20110;&#33021;&#28304;&#20114;&#32852;&#32593;&#39046;&#22495;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#31181;&#20856;&#22411;&#30340;&#31995;&#32479;&#27169;&#22411;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#21306;&#22359;&#38142;&#12289;&#26234;&#33021;&#21512;&#32422;&#21644;&#32852;&#37030;&#23398;&#20064;&#22312;&#33021;&#28304;&#20114;&#32852;&#32593;&#20013;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of Energy (IoE) is a distributed paradigm that leverages smart networks and distributed system technologies to enable decentralized energy systems. In contrast to the traditional centralized energy systems, distributed Energy Internet systems comprise multiple components and communication requirements that demand innovative technologies for decentralization, reliability, efficiency, and security. Recent advances in blockchain architectures, smart contracts, and distributed federated learning technologies have opened up new opportunities for realizing decentralized Energy Internet services. In this paper, we present a comprehensive analysis and classification of state-of-the-art solutions that employ blockchain, smart contracts, and federated learning for the IoE domains. Specifically, we identify four representative system models and discuss their key aspects. These models demonstrate the diverse ways in which blockchain, smart contracts, and federated learning can be inte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;NLP&#25216;&#26415;&#36741;&#21161;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#20998;&#31867;&#22120;&#29983;&#25104;&#22270;&#20687;&#26631;&#31614;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#29983;&#25104;&#30149;&#29702;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;BERT&#27169;&#22411;&#26367;&#25442;&#27491;&#24120;&#25253;&#21578;&#27169;&#26495;&#20013;&#30340;&#30456;&#24212;&#37096;&#20998;&#65292;&#26368;&#32456;&#29983;&#25104;&#23436;&#25972;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2306.17180</link><description>&lt;p&gt;
&#26367;&#25442;&#21644;&#25253;&#21578;&#65306;NLP&#36741;&#21161;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Replace and Report: NLP Assisted Radiology Report Generation. (arXiv:2306.17180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;NLP&#25216;&#26415;&#36741;&#21161;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#20998;&#31867;&#22120;&#29983;&#25104;&#22270;&#20687;&#26631;&#31614;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#29983;&#25104;&#30149;&#29702;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;BERT&#27169;&#22411;&#26367;&#25442;&#27491;&#24120;&#25253;&#21578;&#27169;&#26495;&#20013;&#30340;&#30456;&#24212;&#37096;&#20998;&#65292;&#26368;&#32456;&#29983;&#25104;&#23436;&#25972;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#23454;&#36341;&#32463;&#24120;&#20351;&#29992;&#21307;&#23398;&#25104;&#20687;&#26469;&#36827;&#34892;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#33258;&#21160;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#65292;&#25918;&#23556;&#23398;&#25253;&#21578;&#26159;&#30001;&#22810;&#20010;&#21477;&#23376;&#32452;&#25104;&#30340;&#38271;&#31687;&#21465;&#36848;&#65292;&#21253;&#25324;&#24322;&#24120;&#21644;&#27491;&#24120;&#30340;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#23558;&#20256;&#32479;&#30340;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#25972;&#20010;&#25253;&#21578;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#26159;&#35774;&#35745;&#29992;&#20110;&#31616;&#35201;&#25551;&#36848;&#22270;&#20687;&#30340;&#30701;&#21477;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#26495;&#30340;&#26041;&#27861;&#65292;&#20174;&#25918;&#23556;&#22270;&#20687;&#20013;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20197;&#19979;&#27493;&#39588;&#65306;i&#65289;&#20351;&#29992;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20026;&#36755;&#20837;&#30340;&#25918;&#23556;&#22270;&#29983;&#25104;&#26631;&#31614;&#65307;ii&#65289;&#20351;&#29992;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65292;&#26681;&#25454;&#27493;&#39588;&#65288;i&#65289;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#29983;&#25104;&#30149;&#29702;&#25551;&#36848;&#65288;&#25918;&#23556;&#22270;&#20687;&#19978;&#30340;&#24322;&#24120;&#21457;&#29616;&#30340;&#25551;&#36848;&#65289;&#65307;iii&#65289;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#22120;&#65292;&#25214;&#21040;&#27491;&#24120;&#25253;&#21578;&#27169;&#26495;&#20013;&#35201;&#26367;&#25442;&#20026;&#29983;&#25104;&#30340;&#30149;&#29702;&#25551;&#36848;&#30340;&#37096;&#20998;&#65307;iv&#65289;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#32456;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical practice frequently uses medical imaging for diagnosis and treatment. A significant challenge for automatic radiology report generation is that the radiology reports are long narratives consisting of multiple sentences for both abnormal and normal findings. Therefore, applying conventional image captioning approaches to generate the whole report proves to be insufficient, as these are designed to briefly describe images with short sentences. We propose a template-based approach to generate radiology reports from radiographs. Our approach involves the following: i) using a multilabel image classifier, produce the tags for the input radiograph; ii) using a transformer-based model, generate pathological descriptions (a description of abnormal findings seen on radiographs) from the tags generated in step (i); iii) using a BERT-based multi-label text classifier, find the spans in the normal report template to replace with the generated pathological descriptions; and iv) using a rul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#28404;&#32423;&#25968;&#25454;&#21644;&#21608;&#26399;&#39044;&#27979;&#20449;&#21495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#30340;&#39640;&#39057;&#24066;&#22330;&#20570;&#24066;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30408;&#21033;&#33021;&#21147;&#21644;&#39118;&#38505;&#31649;&#29702;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.17179</link><description>&lt;p&gt;
&#39640;&#39057;&#24066;&#22330;&#20570;&#24066;&#30340;&#25972;&#21512;&#28404;&#31574;&#30053;&#21644;&#21608;&#26399;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Integrating Tick-level Data and Periodical Signal for High-frequency Market Making. (arXiv:2306.17179v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17179
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#28404;&#32423;&#25968;&#25454;&#21644;&#21608;&#26399;&#39044;&#27979;&#20449;&#21495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#30340;&#39640;&#39057;&#24066;&#22330;&#20570;&#24066;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30408;&#21033;&#33021;&#21147;&#21644;&#39118;&#38505;&#31649;&#29702;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#39640;&#39057;&#20132;&#26131;&#20013;&#30340;&#24066;&#22330;&#20570;&#24066;&#38382;&#39064;&#12290;&#24066;&#22330;&#20570;&#24066;&#26159;&#37329;&#34701;&#24066;&#22330;&#20013;&#25552;&#20379;&#27969;&#21160;&#24615;&#30340;&#20851;&#38190;&#21151;&#33021;&#65292;&#28041;&#21450;&#36890;&#36807;&#20080;&#21334;&#36164;&#20135;&#25552;&#20379;&#27969;&#21160;&#24615;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#24066;&#22330;&#30340;&#26085;&#30410;&#22797;&#26434;&#21270;&#21644;&#28404;&#32423;&#20132;&#26131;&#25152;&#20135;&#29983;&#30340;&#22823;&#37327;&#25968;&#25454;&#20351;&#24471;&#24320;&#21457;&#26377;&#25928;&#30340;&#24066;&#22330;&#20570;&#24066;&#31574;&#30053;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#28404;&#32423;&#25968;&#25454;&#19982;&#21608;&#26399;&#39044;&#27979;&#20449;&#21495;&#34701;&#21512;&#65292;&#20197;&#24320;&#21457;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#30340;&#24066;&#22330;&#20570;&#24066;&#31574;&#30053;&#12290;&#25105;&#20204;&#22522;&#20110;&#19981;&#21516;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27169;&#25311;&#22330;&#26223;&#21644;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24471;&#21040;&#30340;&#24066;&#22330;&#20570;&#24066;&#31574;&#30053;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#30408;&#21033;&#33021;&#21147;&#21644;&#39118;&#38505;&#31649;&#29702;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the problem of market making in high-frequency trading. Market making is a critical function in financial markets that involves providing liquidity by buying and selling assets. However, the increasing complexity of financial markets and the high volume of data generated by tick-level trading makes it challenging to develop effective market making strategies. To address this challenge, we propose a deep reinforcement learning approach that fuses tick-level data with periodic prediction signals to develop a more accurate and robust market making strategy. Our results of market making strategies based on different deep reinforcement learning algorithms under the simulation scenarios and real data experiments in the cryptocurrency markets show that the proposed framework outperforms existing methods in terms of profitability and risk management.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#35780;&#20272;&#20102;ChatGPT 3.5&#12289;ChatGPT 4.0&#12289;Bing AI&#21644;Bard&#22312;&#26032;&#38395;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#30340;&#29087;&#32451;&#31243;&#24230;&#26222;&#36941;&#23621;&#20013;&#65292;&#20854;&#20013;OpenAI&#30340;GPT-4.0&#22312;&#21306;&#20998;&#30495;&#30456;&#21644;&#27450;&#39575;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.17176</link><description>&lt;p&gt;
&#26032;&#38395;&#39564;&#35777;&#32773;&#30340;&#23545;&#20915;&#65306;ChatGPT 3.5&#12289;ChatGPT 4.0&#12289;Bing AI&#21644;Bard&#22312;&#26032;&#38395;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#27604;&#36739;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT 3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking. (arXiv:2306.17176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#35780;&#20272;&#20102;ChatGPT 3.5&#12289;ChatGPT 4.0&#12289;Bing AI&#21644;Bard&#22312;&#26032;&#38395;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#30340;&#29087;&#32451;&#31243;&#24230;&#26222;&#36941;&#23621;&#20013;&#65292;&#20854;&#20013;OpenAI&#30340;GPT-4.0&#22312;&#21306;&#20998;&#30495;&#30456;&#21644;&#27450;&#39575;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#30693;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;OpenAI&#30340;ChatGPT 3.5&#21644;4.0&#12289;&#35895;&#27468;&#30340;Bard&#65288;LaMDA&#65289;&#21644;&#24494;&#36719;&#30340;Bing AI&#65292;&#22312;&#20351;&#29992;&#40657;&#30418;&#27979;&#35797;&#21306;&#20998;&#26032;&#38395;&#30495;&#23454;&#24615;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#24635;&#20849;&#25552;&#20379;&#20102;100&#26465;&#32463;&#36807;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#38395;&#65292;&#25152;&#26377;&#26032;&#38395;&#22343;&#26469;&#33258;&#29420;&#31435;&#30340;&#20107;&#23454;&#26680;&#26597;&#26426;&#26500;&#65292;&#22312;&#21463;&#25511;&#26465;&#20214;&#19979;&#21521;&#27599;&#20010;LLMs&#25552;&#20379;&#12290;&#23427;&#20204;&#30340;&#22238;&#31572;&#34987;&#24402;&#31867;&#20026;&#19977;&#31867;&#65306;&#30495;&#23454;&#12289;&#38169;&#35823;&#21644;&#37096;&#20998;&#30495;&#23454;/&#38169;&#35823;&#12290;&#22522;&#20110;&#29420;&#31435;&#26426;&#26500;&#25552;&#20379;&#30340;&#26680;&#23454;&#20107;&#23454;&#65292;&#35780;&#20272;&#20102;LLMs&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26469;&#34913;&#37327;&#20854;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#29087;&#32451;&#31243;&#24230;&#37117;&#23646;&#20110;&#20013;&#31561;&#27700;&#24179;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;65.25/100&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;OpenAI&#30340;GPT-4.0&#20197;71&#20998;&#30340;&#24471;&#20998;&#33073;&#39062;&#32780;&#20986;&#65292;&#34920;&#26126;&#36739;&#26032;&#30340;LLMs&#22312;&#21306;&#20998;&#30495;&#30456;&#21644;&#27450;&#39575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#20107;&#23454;&#26680;&#26597;&#21592;&#30340;&#34920;&#29616;&#30456;&#27604;&#65292;&#23613;&#31649;AI&#27169;&#22411;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#29087;&#32451;&#24230;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aimed to evaluate the proficiency of prominent Large Language Models (LLMs), namely OpenAI's ChatGPT 3.5 and 4.0, Google's Bard(LaMDA), and Microsoft's Bing AI in discerning the truthfulness of news items using black box testing. A total of 100 fact-checked news items, all sourced from independent fact-checking agencies, were presented to each of these LLMs under controlled conditions. Their responses were classified into one of three categories: True, False, and Partially True/False. The effectiveness of the LLMs was gauged based on the accuracy of their classifications against the verified facts provided by the independent agencies. The results showed a moderate proficiency across all models, with an average score of 65.25 out of 100. Among the models, OpenAI's GPT-4.0 stood out with a score of 71, suggesting an edge in newer LLMs' abilities to differentiate fact from deception. However, when juxtaposed against the performance of human fact-checkers, the AI models, despite
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17175</link><description>&lt;p&gt;
&#20174;&#21407;&#22987;&#30340;GP&#31508;&#35760;&#20013;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#65292;&#29992;&#20110;&#36828;&#31243;COVID-19&#21021;&#32423;&#20445;&#20581;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care. (arXiv:2306.17175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#26159;&#21521;&#24739;&#32773;&#25552;&#20379;&#36866;&#24403;&#25252;&#29702;&#30340;&#22522;&#26412;&#38454;&#27573;&#12290;&#36817;&#24180;&#26469;&#65292;&#20026;&#20102;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20570;&#20986;&#20915;&#31574;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#20010;&#20915;&#31574;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#31616;&#21333;&#30340;&#22238;&#24402;&#27169;&#22411;&#65292;&#21482;&#33021;&#32771;&#34385;&#31616;&#21333;&#30340;&#39044;&#23450;&#20041;&#22810;&#36873;&#29305;&#24449;&#65292;&#22914;&#24739;&#32773;&#24180;&#40836;&#12289;&#26082;&#24448;&#30149;&#21490;&#12289;&#21560;&#28895;&#32773;&#29366;&#20917;&#31561;&#12290;&#20915;&#31574;&#31995;&#32479;&#24403;&#21069;&#26080;&#27861;&#22788;&#29702;&#30340;&#19968;&#20010;&#29305;&#23450;&#24739;&#32773;&#25968;&#25454;&#26469;&#28304;&#26159;&#24739;&#32773;&#20250;&#35786;&#30340;GP&#31508;&#35760;&#30340;&#25910;&#38598;&#12290;&#36825;&#20123;&#31508;&#35760;&#21253;&#21547;&#20102;&#20020;&#24202;&#21307;&#29983;&#29992;&#26469;&#20570;&#20986;&#26368;&#32456;&#20915;&#31574;&#24182;&#23558;&#24739;&#32773;&#24341;&#23548;&#21040;&#36866;&#24403;&#25252;&#29702;&#30340;&#20851;&#38190;&#20307;&#24449;&#21644;&#30151;&#29366;&#12290;&#20174;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#26159;&#19968;&#20010;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#21253;&#21547;&#32553;&#20889;&#12289;&#25171;&#23383;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20010;&#20844;&#24320;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25191;&#34892;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20986;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical decision-making is a fundamental stage in delivering appropriate care to patients. In recent years several decision-making systems designed to aid the clinician in this process have been developed. However, technical solutions currently in use are based on simple regression models and are only able to take into account simple pre-defined multiple-choice features, such as patient age, pre-existing conditions, smoker status, etc. One particular source of patient data, that available decision-making systems are incapable of processing is the collection of patient consultation GP notes. These contain crucial signs and symptoms - the information used by clinicians in order to make a final decision and direct the patient to the appropriate care. Extracting information from GP notes is a technically challenging problem, as they tend to include abbreviations, typos, and incomplete sentences.  This paper addresses this open challenge. We present a framework that performs knowledge grap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#39046;&#22495;&#29983;&#25104;&#38750;&#27491;&#24335;&#25688;&#35201;&#65292;&#24182;&#36890;&#36807;&#35442;&#26041;&#27861;&#22312;&#29992;&#25143;&#20307;&#39564;&#21644;&#36127;&#36733;&#20943;&#36731;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.17174</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#39046;&#22495;&#20013;&#24378;&#21270;&#31163;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#65306;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31639;&#27861;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Empowering NLG: Offline Reinforcement Learning for Informal Summarization in Online Domains. (arXiv:2306.17174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17174
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#39046;&#22495;&#29983;&#25104;&#38750;&#27491;&#24335;&#25688;&#35201;&#65292;&#24182;&#36890;&#36807;&#35442;&#26041;&#27861;&#22312;&#29992;&#25143;&#20307;&#39564;&#21644;&#36127;&#36733;&#20943;&#36731;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#29992;&#25143;&#20307;&#39564;&#24182;&#20943;&#36731;&#20154;&#24037;&#23458;&#26381;&#20195;&#29702;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#20026;&#22312;&#32447;&#25991;&#31456;&#21644;&#24086;&#23376;&#29983;&#25104;&#38750;&#27491;&#24335;&#25688;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20840;&#38754;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#21253;&#25324;&#29228;&#34411;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#25991;&#26412;&#29983;&#25104;&#27169;&#22359;&#12290;&#36890;&#36807;&#25552;&#20986;&#36825;&#31181;&#21407;&#21019;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#20026;NLG&#39046;&#22495;&#20316;&#20986;&#20102;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#65292;&#20026;&#22312;&#32447;&#20869;&#23481;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#36890;&#36807;&#23454;&#26045;&#8220;&#22686;&#24378;NLG&#8221;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22312;&#32447;&#39046;&#22495;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22238;&#22797;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24179;&#22343;&#8220;&#21916;&#27426;&#8221;&#35780;&#20998;&#26174;&#33879;&#25552;&#39640;&#65292;&#20174;0.09954378&#22686;&#21152;&#21040;0.5000152&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research introduces an innovative Natural Language Generation (NLG) approach that aims to optimize user experience and alleviate the workload of human customer support agents. Our primary objective is to generate informal summaries for online articles and posts using an offline reinforcement learning technique. In our study, we compare our proposed method with existing approaches to text generation and provide a comprehensive overview of our architectural design, which incorporates crawling, reinforcement learning, and text generation modules. By presenting this original approach, our paper makes a valuable contribution to the field of NLG by offering a fresh perspective on generating natural language summaries for online content. Through the implementation of Empowering NLG, we are able to generate higher-quality replies in the online domain. The experimental results demonstrate a significant improvement in the average "like" score, increasing from 0.09954378 to 0.5000152. This ad
&lt;/p&gt;</description></item><item><title>&#36793;&#32536;&#20113;&#35745;&#31639;&#19979;&#35268;&#27169;&#21270;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#21033;&#29992;&#36793;&#32536;&#20113;&#35745;&#31639;&#33539;&#24335;&#26500;&#24314;GenAI&#31995;&#32479;&#30340;&#21560;&#24341;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17170</link><description>&lt;p&gt;
&#36793;&#32536;&#20113;&#35745;&#31639;&#19979;&#35268;&#27169;&#21270;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview on Generative AI at Scale with Edge-Cloud Computing. (arXiv:2306.17170v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17170
&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#20113;&#35745;&#31639;&#19979;&#35268;&#27169;&#21270;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#21033;&#29992;&#36793;&#32536;&#20113;&#35745;&#31639;&#33539;&#24335;&#26500;&#24314;GenAI&#31995;&#32479;&#30340;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#29305;&#23450;&#31867;&#21035;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#21019;&#36896;&#30340;&#26032;&#20869;&#23481;&#12290;GenAI&#31995;&#32479;&#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#32463;&#22312;&#20114;&#32852;&#32593;&#19978;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#26032;&#25968;&#25454;&#65292;&#32473;&#24403;&#21069;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26694;&#26550;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;GenAI&#26381;&#21153;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#20113;&#35745;&#31639;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#20256;&#36755;&#21644;&#22823;&#37327;&#35831;&#27714;&#65292;&#36825;&#31181;&#26381;&#21153;&#23558;&#36935;&#21040;&#39640;&#24310;&#36831;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#21327;&#21516;&#65292;&#36793;&#32536;&#20113;&#35745;&#31639;&#21487;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#20302;&#24310;&#36831;&#12290;&#22240;&#27492;&#65292;&#22312;&#36793;&#32536;&#20113;&#35745;&#31639;&#33539;&#24335;&#30340;&#25903;&#25345;&#19979;&#26500;&#24314;&#35268;&#27169;&#21270;&#30340;GenAI&#31995;&#32479;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#26412;&#32508;&#36848;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#22238;&#39038;&#20102;GenAI&#21644;&#36793;&#32536;&#20113;&#35745;&#31639;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#31034;&#20363;&#24615;&#30340;GenAI&#24212;&#29992;&#26469;&#35752;&#35770;&#25216;&#26415;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a specific category of artificial intelligence (AI), generative artificial intelligence (GenAI) generates new content that resembles what is created by humans. The rapid development of GenAI systems has created a huge amount of new data on the Internet, posing new challenges to current computing and communication frameworks. Currently, GenAI services rely on the traditional cloud computing framework due to the need for large computation resources. However, such services will encounter high latency because of data transmission and a high volume of requests. On the other hand, edge-cloud computing can provide adequate computation power and low latency at the same time through the collaboration between edges and the cloud. Thus, it is attractive to build GenAI systems at scale by leveraging the edge-cloud computing paradigm. In this overview paper, we review recent developments in GenAI and edge-cloud computing, respectively. Then, we use two exemplary GenAI applications to discuss tec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Mondrian Conformal&#39044;&#27979;&#22120;&#30340;&#20225;&#19994;&#30913;&#30424;&#39537;&#21160;&#21047;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#38656;&#35201;&#21047;&#26032;&#30340;&#30913;&#30424;&#65292;&#24182;&#25552;&#21069;&#39044;&#27979;&#20854;&#20581;&#24247;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#21487;&#38752;&#24615;&#21644;&#21151;&#29575;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.17169</link><description>&lt;p&gt;
&#22522;&#20110;Mondrian Conformal&#39044;&#27979;&#22120;&#30340;&#20225;&#19994;&#30913;&#30424;&#39537;&#21160;&#21047;&#26032;
&lt;/p&gt;
&lt;p&gt;
Enterprise Disk Drive Scrubbing Based on Mondrian Conformal Predictors. (arXiv:2306.17169v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Mondrian Conformal&#39044;&#27979;&#22120;&#30340;&#20225;&#19994;&#30913;&#30424;&#39537;&#21160;&#21047;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#38656;&#35201;&#21047;&#26032;&#30340;&#30913;&#30424;&#65292;&#24182;&#25552;&#21069;&#39044;&#27979;&#20854;&#20581;&#24247;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#21487;&#38752;&#24615;&#21644;&#21151;&#29575;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#30424;&#21047;&#26032;&#26159;&#19968;&#31181;&#36890;&#36807;&#20174;&#30913;&#30424;&#35835;&#21462;&#25968;&#25454;&#26469;&#35299;&#20915;&#35835;&#38169;&#35823;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#19968;&#27425;&#24615;&#21047;&#26032;&#25972;&#20010;&#23384;&#20648;&#25968;&#32452;&#21487;&#33021;&#20250;&#23545;&#31995;&#32479;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#36755;&#20837;/&#36755;&#20986;&#25805;&#20316;&#26399;&#38388;&#12290;&#27492;&#22806;&#65292;&#21047;&#26032;&#26102;&#36830;&#32493;&#20174;&#30913;&#30424;&#35835;&#21462;&#25968;&#25454;&#21487;&#33021;&#20250;&#23548;&#33268;&#30913;&#30424;&#30340;&#30952;&#25439;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#23481;&#37327;&#30340;&#30913;&#30424;&#65292;&#22240;&#20026;&#36825;&#28041;&#21450;&#21040;&#26174;&#33879;&#30340;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#30913;&#30424;&#21047;&#26032;&#26041;&#27861;&#65292;&#25552;&#39640;&#25968;&#25454;&#20013;&#24515;&#30340;&#25972;&#20307;&#21487;&#38752;&#24615;&#21644;&#21151;&#29575;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;Mondrian Conformal&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#23384;&#20648;&#27744;&#20013;&#27599;&#20010;&#30913;&#30424;&#30340;&#20581;&#24247;&#29366;&#24577;&#65292;&#25552;&#21069;n&#22825;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#24320;&#28304;&#25968;&#25454;&#38598;&#26469;&#35782;&#21035;&#38656;&#35201;&#21047;&#26032;&#30340;&#29305;&#23450;&#30913;&#30424;&#12290;&#23545;&#20110;&#39044;&#27979;&#20026;&#19981;&#20581;&#24247;&#30340;&#30913;&#30424;&#65292;&#25105;&#20204;&#26631;&#35760;&#23427;&#20204;&#36827;&#34892;&#26367;&#25442;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#25805;&#20316;&#12290;&#23545;&#20110;&#20581;&#24247;&#30340;&#39537;&#21160;&#22120;&#65292;&#25105;&#20204;&#21019;&#24314;&#19968;&#20010;&#38598;&#21512;&#21644;&#25968;&#37327;&#35780;&#20272;&#30340;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Disk scrubbing is a process aimed at resolving read errors on disks by reading data from the disk. However, scrubbing the entire storage array at once can adversely impact system performance, particularly during periods of high input/output operations. Additionally, the continuous reading of data from disks when scrubbing can result in wear and tear, especially on larger capacity disks, due to the significant time and energy consumption involved. To address these issues, we propose a selective disk scrubbing method that enhances the overall reliability and power efficiency in data centers. Our method employs a Machine Learning model based on Mondrian Conformal prediction to identify specific disks for scrubbing, by proactively predicting the health status of each disk in the storage pool, forecasting n-days in advance, and using an open-source dataset. For disks predicted as non-healthy, we mark them for replacement without further action. For healthy drives, we create a set and quanti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;GPT-4&#20248;&#20110;ChatGPT&#65292;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;</title><link>http://arxiv.org/abs/2306.17156</link><description>&lt;p&gt;
&#32534;&#31243;&#25945;&#32946;&#30340;&#29983;&#25104;AI&#65306;&#27604;&#36739;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;GPT-4&#20248;&#20110;ChatGPT&#65292;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#39640;&#35745;&#31639;&#26426;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20026;&#21021;&#32423;&#32534;&#31243;&#25552;&#20379;&#19979;&#19968;&#20195;&#25945;&#32946;&#25216;&#26415;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19982;&#32534;&#31243;&#25945;&#32946;&#30456;&#20851;&#30340;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#30001;&#20110;&#22810;&#31181;&#21407;&#22240;&#32780;&#21463;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#32771;&#34385;&#30340;&#26159;&#24050;&#32463;&#36807;&#26102;&#30340;&#27169;&#22411;&#25110;&#20165;&#20165;&#29305;&#23450;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#30740;&#31350;&#26469;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;ChatGPT&#65288;&#22522;&#20110;GPT-3.5&#65289;&#21644;GPT-4&#65292;&#24182;&#23558;&#20854;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#19982;&#20154;&#31867;&#23548;&#24072;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;&#21021;&#32423;Python&#32534;&#31243;&#38382;&#39064;&#21644;&#26469;&#33258;&#22312;&#32447;&#24179;&#21488;&#30340;&#30495;&#23454;&#38169;&#35823;&#31243;&#24207;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#19987;&#23478;&#35780;&#27880;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#26126;&#26174;&#20248;&#20110;ChatGPT&#65288;&#22522;&#20110;GPT-3.5&#65289;&#65292;&#24182;&#19988;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to hu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30151;&#29366;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#26032;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#39640;&#36798;97%&#12290;&#36825;&#34920;&#26126;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26159;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#31579;&#26597;&#30340;&#19968;&#20010;&#37325;&#35201;&#36164;&#28304;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#36825;&#19968;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16891</link><description>&lt;p&gt;
&#21033;&#29992;Hugging Face Transformers&#39044;&#27979;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Hugging Face Transformers for Predicting Mental Health Disorders in Social Networks. (arXiv:2306.16891v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30151;&#29366;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#26032;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#39640;&#36798;97%&#12290;&#36825;&#34920;&#26126;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26159;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#31579;&#26597;&#30340;&#19968;&#20010;&#37325;&#35201;&#36164;&#28304;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#36825;&#19968;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#35786;&#26029;&#31934;&#31070;&#38556;&#30861;&#24182;&#36827;&#34892;&#24178;&#39044;&#21487;&#20197;&#20419;&#36827;&#39044;&#38450;&#20005;&#37325;&#20260;&#23475;&#21644;&#25913;&#21892;&#27835;&#30103;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#35752;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#22914;&#20309;&#29992;&#20110;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30151;&#29366;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#27604;&#36739;&#20102;Hugging Face&#30340;&#22235;&#31181;&#19981;&#21516;BERT&#27169;&#22411;&#21644;&#36817;&#26399;&#25991;&#29486;&#20013;&#29992;&#20110;&#33258;&#21160;&#25233;&#37057;&#30151;&#35786;&#26029;&#30340;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26032;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;97%&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#34917;&#20805;&#20808;&#21069;&#30340;&#21457;&#29616;&#65292;&#23545;&#32467;&#26524;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#26159;&#24494;&#23567;&#30340;&#25968;&#25454;&#37327;&#65288;&#22914;&#29992;&#25143;&#30340;&#20010;&#20154;&#31616;&#20171;&#25551;&#36848;&#65289;&#20063;&#26377;&#39044;&#27979;&#31934;&#31070;&#38556;&#30861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#26159;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#31579;&#26597;&#30340;&#19968;&#20010;&#26497;&#22909;&#30340;&#26469;&#28304;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#33258;&#21160;&#21270;&#36825;&#19968;&#20851;&#38190;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of mental disorders and intervention can facilitate the prevention of severe injuries and the improvement of treatment results. Using social media and pre-trained language models, this study explores how user-generated data can be used to predict mental disorder symptoms. Our study compares four different BERT models of Hugging Face with standard machine learning techniques used in automatic depression diagnosis in recent literature. The results show that new models outperform the previous approach with an accuracy rate of up to 97%. Analyzing the results while complementing past findings, we find that even tiny amounts of data (like users' bio descriptions) have the potential to predict mental disorders. We conclude that social media data is an excellent source of mental health screening, and pre-trained models can effectively automate this critical task.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16703</link><description>&lt;p&gt;
&#24377;&#24615;&#32422;&#26463;&#19979;&#30340;&#20803;&#23398;&#20064;&#22120;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Elastically-Constrained Meta-Learner for Federated Learning. (arXiv:2306.16703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#31105;&#27490;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#22240;&#20026;&#21333;&#20010;&#27169;&#22411;&#26080;&#27861;&#36866;&#24212;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#20803;&#23398;&#20064;&#65288;&#22914;Per-FedAvg&#65289;&#12290;&#20803;&#23398;&#20064;&#23398;&#20064;&#36866;&#29992;&#20110;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#20849;&#20139;&#21021;&#22987;&#21442;&#25968;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#23558;&#21021;&#22987;&#21270;&#24555;&#36895;&#35843;&#25972;&#21040;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#27169;&#22411;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#37319;&#26679;&#26356;&#26032;&#30340;&#38543;&#26426;&#24615;&#65292;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#26412;&#22320;&#36866;&#24212;&#21516;&#19968;&#23458;&#25143;&#31471;&#26102;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#30446;&#26631;&#12290;&#36825;&#31181;&#19981;&#21516;&#36866;&#24212;&#26041;&#21521;&#30340;&#27874;&#21160;&#38459;&#30861;&#20102;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21382;&#21490;&#26412;&#22320;&#35843;&#25972;&#30340;&#27169;&#22411;&#26469;&#38480;&#21046;&#20869;&#24490;&#29615;&#30340;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an approach to collaboratively training machine learning models for multiple parties that prohibit data sharing. One of the challenges in federated learning is non-IID data between clients, as a single model can not fit the data distribution for all clients. Meta-learning, such as Per-FedAvg, is introduced to cope with the challenge. Meta-learning learns shared initial parameters for all clients. Each client employs gradient descent to adapt the initialization to local data distributions quickly to realize model personalization. However, due to non-convex loss function and randomness of sampling update, meta-learning approaches have unstable goals in local adaptation for the same client. This fluctuation in different adaptation directions hinders the convergence in meta-learning. To overcome this challenge, we use the historical local adapted model to restrict the direction of the inner loop and propose an elastic-constrained method. As a result, the current round
&lt;/p&gt;</description></item><item><title>DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16058</link><description>&lt;p&gt;
DUET: 2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DUET: 2D Structured and Approximately Equivariant Representations. (arXiv:2306.16058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16058
&lt;/p&gt;
&lt;p&gt;
DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;(MSSL)&#22522;&#20110;&#23398;&#20064;&#30456;&#23545;&#20110;&#19968;&#32452;&#36755;&#20837;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#21464;&#24615;&#20174;&#34920;&#31034;&#20013;&#37096;&#20998;&#25110;&#23436;&#20840;&#31227;&#38500;&#19982;&#21464;&#25442;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#38656;&#35201;&#36825;&#20123;&#20449;&#24687;&#30340;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#36896;&#25104;&#25439;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;2D&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#34920;&#31034;&#65292;&#31216;&#20026;DUET&#65292;&#23427;&#20204;&#26159;&#20197;&#30697;&#38453;&#32467;&#26500;&#32452;&#32455;&#30340;2D&#34920;&#31034;&#65292;&#24182;&#19988;&#23545;&#20316;&#29992;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#25442;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;DUET&#34920;&#31034;&#20445;&#30041;&#26377;&#20851;&#36755;&#20837;&#21464;&#25442;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#20041;&#34920;&#36798;&#33021;&#21147;&#12290;&#19982;SimCLR&#65288;Chen&#31561;&#65292;2020&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#19981;&#21464;&#24615;&#65289;&#21644;ESSL&#65288;Dangovski&#31561;&#65292;2022&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#31561;&#21464;&#24615;&#65289;&#30456;&#27604;&#65292;DUET&#34920;&#31034;&#30340;&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#24615;&#20351;&#24471;&#29983;&#25104;&#20855;&#26377;&#26356;&#20302;&#30340;&#37325;&#24314;&#35823;&#24046;&#30340;&#21487;&#25511;&#24615;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;SimCLR&#25110;ESSL&#21017;&#26080;&#27861;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;DUET&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiview Self-Supervised Learning (MSSL) is based on learning invariances with respect to a set of input transformations. However, invariance partially or totally removes transformation-related information from the representations, which might harm performance for specific downstream tasks that require such information. We propose 2D strUctured and EquivarianT representations (coined DUET), which are 2d representations organized in a matrix structure, and equivariant with respect to transformations acting on the input data. DUET representations maintain information about an input transformation, while remaining semantically expressive. Compared to SimCLR (Chen et al., 2020) (unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured and equivariant), the structured and equivariant nature of DUET representations enables controlled generation with lower reconstruction error, while controllability is not possible with SimCLR or ESSL. DUET also achieves higher accuracy fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20914;&#31361;&#35299;&#20915;&#26694;&#26550;&#65292;&#31216;&#20026;C-GMCR&#65292;&#23427;&#23558;&#33539;&#30068;&#35770;&#25972;&#21512;&#21040;&#20256;&#32479;&#30340;&#22270;&#27169;&#22411;&#20013;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#25277;&#35937;&#21644;&#36890;&#29992;&#30340;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#24212;&#29992;&#21040;&#22234;&#24466;&#22256;&#22659;&#21644;&#20854;&#20182;&#26696;&#20363;&#20013;&#65292;&#21457;&#29616;&#20998;&#31867;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#21487;&#33021;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#20914;&#31361;&#35299;&#20915;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.13961</link><description>&lt;p&gt;
&#21033;&#29992;&#33539;&#30068;&#35770;&#25972;&#21512;&#21040;&#20914;&#31361;&#35299;&#20915;&#20013;&#30340;&#22270;&#27169;&#22411;&#30340;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Categorical Approach to Conflict Resolution: Integrating Category Theory into the Graph Model for Conflict Resolution. (arXiv:2306.13961v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20914;&#31361;&#35299;&#20915;&#26694;&#26550;&#65292;&#31216;&#20026;C-GMCR&#65292;&#23427;&#23558;&#33539;&#30068;&#35770;&#25972;&#21512;&#21040;&#20256;&#32479;&#30340;&#22270;&#27169;&#22411;&#20013;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#25277;&#35937;&#21644;&#36890;&#29992;&#30340;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#24212;&#29992;&#21040;&#22234;&#24466;&#22256;&#22659;&#21644;&#20854;&#20182;&#26696;&#20363;&#20013;&#65292;&#21457;&#29616;&#20998;&#31867;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#21487;&#33021;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#20914;&#31361;&#35299;&#20915;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20914;&#31361;&#35299;&#20915;&#20013;&#30340;&#33539;&#30068;&#22270;&#27169;&#22411;&#65288;C-GMCR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#33539;&#30068;&#35770;&#25972;&#21512;&#21040;&#20256;&#32479;&#30340;&#22270;&#27169;&#22411;&#20013;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;C-GMCR&#26694;&#26550;&#25552;&#20379;&#20102;&#26356;&#25277;&#35937;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#26469;&#24314;&#27169;&#21644;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#21457;&#29616;&#26356;&#28145;&#23618;&#27425;&#30340;&#35265;&#35299;&#21644;&#32852;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;C-GMCR&#26694;&#26550;&#30340;&#22522;&#26412;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;&#21040;&#33879;&#21517;&#30340;&#22234;&#24466;&#22256;&#22659;&#21644;&#20854;&#20182;&#20195;&#34920;&#24615;&#26696;&#20363;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20998;&#31867;&#26041;&#27861;&#20026;&#31283;&#23450;&#27010;&#24565;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#26377;&#21487;&#33021;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#20914;&#31361;&#35299;&#20915;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Categorical Graph Model for Conflict Resolution (C-GMCR), a novel framework that integrates category theory into the traditional Graph Model for Conflict Resolution (GMCR). The C-GMCR framework provides a more abstract and general way to model and analyze conflict resolution, enabling researchers to uncover deeper insights and connections. We present the basic concepts, methods, and application of the C-GMCR framework to the well-known Prisoner's Dilemma and other representative cases. The findings suggest that the categorical approach offers new perspectives on stability concepts and can potentially lead to the development of more effective conflict resolution strategies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.00526</link><description>&lt;p&gt;
&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#38646;&#26679;&#26412;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#25351;&#23548;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering. (arXiv:2306.00526v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24067;&#23616;&#24863;&#30693;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#24494;&#35843;&#23545;&#20110;&#39069;&#22806;&#30340;&#35270;&#35273;&#12289;&#24067;&#23616;&#21644;&#20219;&#21153;&#27169;&#22359;&#38459;&#27490;&#20102;&#20854;&#30452;&#25509;&#21033;&#29992;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#26368;&#36817;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;&#19982;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#39046;&#22495;&#23545;&#40784;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#19982;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#23427;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#12290;&#21069;&#32773;&#36890;&#36807;&#36866;&#24403;&#30340;&#31354;&#26684;&#21644;&#25442;&#34892;&#31526;&#20174;OCR&#24037;&#20855;&#20013;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#12290;&#21518;&#32773;&#30830;&#20445;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pre-training-fine-tuning paradigm based on layout-aware multimodal pre-trained models has achieved significant progress on document image question answering. However, domain pre-training and task fine-tuning for additional visual, layout, and task modules prevent them from directly utilizing off-the-shelf instruction-tuning language foundation models, which have recently shown promising potential in zero-shot learning. Contrary to aligning language models to the domain of document image question answering, we align document image question answering to off-the-shell instruction-tuning language foundation models to utilize their zero-shot capability. Specifically, we propose layout and task aware instruction prompt called LATIN-Prompt, which consists of layout-aware document content and task-aware descriptions. The former recovers the layout information among text segments from OCR tools by appropriate spaces and line breaks. The latter ensures that the model generates answers that m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30495;&#23454;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#27979;&#35797;&#26041;&#27861;&#65306;&#21453;&#20107;&#23454;&#36755;&#20837;&#32534;&#36753;&#22120;&#21644;&#37325;&#24314;&#36755;&#20837;&#27979;&#35797;&#12290;&#36825;&#20123;&#27979;&#35797;&#23545;&#20110;&#35780;&#20272;&#26032;&#20852;&#30340;NLE&#27169;&#22411;&#65292;&#23545;&#24320;&#21457;&#30495;&#23454;&#30340;NLEs&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.18029</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#30495;&#23454;&#24615;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Faithfulness Tests for Natural Language Explanations. (arXiv:2305.18029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18029
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30495;&#23454;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#27979;&#35797;&#26041;&#27861;&#65306;&#21453;&#20107;&#23454;&#36755;&#20837;&#32534;&#36753;&#22120;&#21644;&#37325;&#24314;&#36755;&#20837;&#27979;&#35797;&#12290;&#36825;&#20123;&#27979;&#35797;&#23545;&#20110;&#35780;&#20272;&#26032;&#20852;&#30340;NLE&#27169;&#22411;&#65292;&#23545;&#24320;&#21457;&#30495;&#23454;&#30340;NLEs&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27169;&#22411;&#30340;&#35299;&#37322;&#26088;&#22312;&#25581;&#31034;&#27169;&#22411;&#39044;&#27979;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35832;&#22914;&#26174;&#33879;&#24615;&#22320;&#22270;&#25110;&#21453;&#20107;&#23454;&#35299;&#37322;&#31561;&#24403;&#21069;&#30340;&#35299;&#37322;&#26041;&#27861;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#21576;&#29616;&#19982;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#19981;&#19968;&#33268;&#30340;&#21407;&#22240;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65288;NLEs&#65289;&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#27979;&#35797;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#36755;&#20837;&#32534;&#36753;&#22120;&#65292;&#29992;&#20110;&#25554;&#20837;&#23548;&#33268;&#21453;&#20107;&#23454;&#39044;&#27979;&#20294;&#19981;&#34987;NLEs&#21453;&#26144;&#30340;&#21407;&#22240;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#26681;&#25454;&#29983;&#25104;&#30340;NLEs&#20013;&#25152;&#36848;&#30340;&#21407;&#22240;&#37325;&#24314;&#36755;&#20837;&#65292;&#24182;&#26816;&#26597;&#23427;&#20204;&#23548;&#33268;&#30456;&#21516;&#39044;&#27979;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#21487;&#20197;&#35780;&#20272;&#26032;&#20852;&#30340;NLE&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#30495;&#23454;&#30340;NLEs&#25552;&#20379;&#20102;&#22522;&#26412;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations of neural models aim to reveal a model's decision-making process for its predictions. However, recent work shows that current methods giving explanations such as saliency maps or counterfactuals can be misleading, as they are prone to present reasons that are unfaithful to the model's inner workings. This work explores the challenging question of evaluating the faithfulness of natural language explanations (NLEs). To this end, we present two tests. First, we propose a counterfactual input editor for inserting reasons that lead to counterfactual predictions but are not reflected by the NLEs. Second, we reconstruct inputs from the reasons stated in the generated NLEs and check how often they lead to the same predictions. Our tests can evaluate emerging NLE models, proving a fundamental tool in the development of faithful NLEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;sim2real&#21644;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#23427;&#20204;&#20998;&#21035;&#35299;&#20915;&#20102;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#20174;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#23398;&#20064;&#20197;&#25552;&#39640;&#20223;&#30495;&#31934;&#24230;&#30340;&#38382;&#39064;&#65292;&#20294;&#20063;&#23384;&#22312;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.01263</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;sim2real&#21644;&#25968;&#23383;&#23402;&#29983;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Sim2real and Digital Twins in Autonomous Driving: A Survey. (arXiv:2305.01263v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;sim2real&#21644;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#23427;&#20204;&#20998;&#21035;&#35299;&#20915;&#20102;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#20174;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#23398;&#20064;&#20197;&#25552;&#39640;&#20223;&#30495;&#31934;&#24230;&#30340;&#38382;&#39064;&#65292;&#20294;&#20063;&#23384;&#22312;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#21644;&#25104;&#26412;&#26159;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#24320;&#21457;&#30340;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20174;&#23398;&#26415;&#30740;&#31350;&#21040;&#21830;&#19994;&#24212;&#29992;&#65292;&#37117;&#38656;&#35201;&#20805;&#20998;&#30340;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#12290;&#36890;&#24120;&#20250;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#27979;&#35797;&#65292;&#28982;&#21518;&#23558;&#23398;&#20064;&#21040;&#30340;&#39550;&#39542;&#30693;&#35782;&#36716;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#65292;&#22240;&#27492;&#22914;&#20309;&#23558;&#22312;&#27169;&#25311;&#20013;&#23398;&#20064;&#21040;&#30340;&#39550;&#39542;&#30693;&#35782;&#36866;&#24212;&#29616;&#23454;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#34394;&#25311;&#20223;&#30495;&#19990;&#30028;&#19982;&#29616;&#23454;&#19990;&#30028;&#22312;&#35768;&#22810;&#26041;&#38754;&#65288;&#22914;&#29031;&#26126;&#12289;&#32441;&#29702;&#12289;&#36710;&#36742;&#21160;&#21147;&#23398;&#21644;&#20195;&#29702;&#34892;&#20026;&#31561;&#65289;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#24357;&#21512;&#34394;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20010;&#24046;&#36317;&#36890;&#24120;&#34987;&#31216;&#20026;&#29616;&#23454;&#24046;&#36317;&#65288;RG&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#24046;&#36317;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#22320;&#20998;&#20026;&#20004;&#31867;&#65306;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#30693;&#35782;&#36716;&#31227;&#65288;sim2real&#65289;&#21644;&#20174;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#23398;&#20064;&#20197;&#25552;&#39640;&#20223;&#30495;&#31934;&#24230;&#65288;&#25968;&#23383;&#23402;&#29983;&#65289;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;sim2real&#21644;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#65292;&#23457;&#26597;&#20102;&#24403;&#21069;&#30340;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety and cost are two important concerns for the development of autonomous driving technologies. From the academic research to commercial applications of autonomous driving vehicles, sufficient simulation and real world testing are required. In general, a large scale of testing in simulation environment is conducted and then the learned driving knowledge is transferred to the real world, so how to adapt driving knowledge learned in simulation to reality becomes a critical issue. However, the virtual simulation world differs from the real world in many aspects such as lighting, textures, vehicle dynamics, and agents' behaviors, etc., which makes it difficult to bridge the gap between the virtual and real worlds. This gap is commonly referred to as the reality gap (RG). In recent years, researchers have explored various approaches to address the reality gap issue, which can be broadly classified into two categories: transferring knowledge from simulation to reality (sim2real) and learn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Fed-SP-SC&#21644;Fed-DP-CoT&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#37030;&#25552;&#31034;&#21644;&#38142;&#24335;&#25512;&#29702;&#25913;&#36827;&#20998;&#24067;&#24335;&#21516;&#20041;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#30340;&#31934;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#20805;&#20998;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.13911</link><description>&lt;p&gt;
&#25552;&#39640;LLM&#31572;&#26696;&#20934;&#30830;&#24230;&#30340;&#32852;&#37030;&#25552;&#31034;&#21644;&#38142;&#24335;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering. (arXiv:2304.13911v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Fed-SP-SC&#21644;Fed-DP-CoT&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#37030;&#25552;&#31034;&#21644;&#38142;&#24335;&#25512;&#29702;&#25913;&#36827;&#20998;&#24067;&#24335;&#21516;&#20041;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#30340;&#31934;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#20805;&#20998;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#20998;&#24067;&#24335;&#29992;&#25143;&#25552;&#20986;&#30340;&#24120;&#35265;&#38382;&#39064;&#30340;&#22238;&#31572;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#20856;&#22411;&#24773;&#20917;&#65292;&#21363;&#29992;&#25143;&#35810;&#38382;&#28041;&#21450;&#30456;&#21516;&#30340;&#25968;&#23398;&#25512;&#29702;&#27493;&#39588;&#21644;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#30340;&#30456;&#20284;&#26597;&#35810;&#12290;&#30001;&#20110;LLMs&#29420;&#31435;&#38382;&#39064;&#30340;&#38646;-shot&#25552;&#31034;&#30340;&#20934;&#30830;&#24615;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#27965;&#24615;&#65288;SC&#65289;&#21644;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#25216;&#26415;&#26469;&#25913;&#36827;&#20998;&#24067;&#24335;&#21516;&#20041;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#20247;&#21253;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#21516;&#20041;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#32852;&#37030;&#38382;&#39064;&#27744;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#20855;&#26377;&#30456;&#21516;&#25110;&#19981;&#21516;&#21442;&#25968;&#30340;&#32852;&#37030;&#21516;&#20041;&#38382;&#39064;&#20026;SP&#38382;&#39064;&#25110;DP&#38382;&#39064;&#65292;&#20998;&#21035;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;Fed-SP-SC&#21644;Fed-DP-CoT&#65292;&#23427;&#20204;&#21487;&#20197;&#20026;&#25152;&#26377;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#32780;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#27169;&#22411;&#35843;&#25972;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#19982;&#26041;&#27861;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.13671</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#30340;&#22810;&#30446;&#26631;&#29289;&#27969;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multiobjective Logistics Optimization for Automated ATM Cash Replenishment Process. (arXiv:2304.13671v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#19982;&#26041;&#27861;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26102;&#20195;&#65292;&#23558;&#25968;&#23383;&#25216;&#26415;&#25972;&#21512;&#21040;&#38134;&#34892;&#36816;&#33829;&#30340;&#21508;&#20010;&#26041;&#38754;&#21487;&#20197;&#25913;&#21892;&#27969;&#31243;&#33258;&#21160;&#21270;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#26381;&#21153;&#27700;&#24179;&#25552;&#21319;&#12290;&#34429;&#28982;ATM&#29616;&#37329;&#29289;&#27969;&#26159;&#24433;&#21709;&#36816;&#33829;&#25104;&#26412;&#21644;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20294;&#21364;&#24456;&#23569;&#26377;&#21162;&#21147;&#26469;&#21152;&#20197;&#25913;&#36827;&#12290;&#29305;&#21035;&#26159;&#22312;&#36234;&#21335;&#65292;&#25317;&#26377;&#36229;&#36807;2&#19975;&#21488;ATM&#30340;&#24066;&#22330;&#19978;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#21644;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ATM&#29616;&#37329;&#34917;&#20805;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#36827;&#34892;&#20102;&#27010;&#25324;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the digital transformation era, integrating digital technology into every aspect of banking operations improves process automation, cost efficiency, and service level improvement. Although logistics for ATM cash is a crucial task that impacts operating costs and consumer satisfaction, there has been little effort to enhance it. Specifically, in Vietnam, with a market of more than 20,000 ATMs nationally, research and technological solutions that can resolve this issue remain scarce. In this paper, we generalized the vehicle routing problem for ATM cash replenishment, suggested a mathematical model and then offered a tool to evaluate various situations. When being evaluated on the simulated dataset, our proposed model and method produced encouraging results with the benefits of cutting ATM cash operating costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GRIL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25299;&#25169;&#29305;&#24449;&#34920;&#31034;&#25955;&#24230;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#29992;&#20110;&#19981;&#21516;&#30340;&#36807;&#28388;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.04970</link><description>&lt;p&gt;
GRIL&#65306;&#19968;&#31181;&#20108;&#21442;&#25968;&#25345;&#20037;&#24615;&#22522;&#20110;&#21521;&#37327;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GRIL: A $2$-parameter Persistence Based Vectorization for Machine Learning. (arXiv:2304.04970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GRIL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25299;&#25169;&#29305;&#24449;&#34920;&#31034;&#25955;&#24230;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#29992;&#20110;&#19981;&#21516;&#30340;&#36807;&#28388;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21442;&#25968;&#25345;&#20037;&#24615;&#21516;Topology Data Analysis (TDA)&#30456;&#20851;&#65292;&#21487;&#30740;&#31350;&#25968;&#25454;&#20013;&#38544;&#34255;&#30528;&#30340;&#36830;&#36890;&#20998;&#37327;&#21644;&#24490;&#29615;&#31561;&#25299;&#25169;&#29305;&#24449;&#12290;&#24050;&#24212;&#29992;&#20110;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20016;&#23500;&#25299;&#25169;&#29305;&#24449;&#30340;&#34920;&#31034;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#30740;&#31350;&#21452;&#36807;&#28388;&#20989;&#25968;&#35825;&#23548;&#30340;&#20108;&#21442;&#25968;&#25345;&#20037;&#24615;&#27169;&#22359;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#34920;&#31034;&#20449;&#24687;&#21152;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21521;&#37327;&#34920;&#31034;&#31216;&#20026;Generalized Rank Invariant Landscape \textsc{Gril}&#65292;&#24182;&#23558;&#20854;&#35777;&#26126;&#20026;&#22312;Lipschitz&#31283;&#23450;&#26465;&#20214;&#19979;&#21487;&#24494;&#20998;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#22522;&#30784;&#36807;&#28388;&#20989;&#25968;&#30340;&#32534;&#30721;&#21487;&#20197;&#23481;&#26131;&#22320;&#34701;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#21521;&#37327;&#34920;&#31034;&#30340;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
$1$-parameter persistent homology, a cornerstone in Topological Data Analysis (TDA), studies the evolution of topological features such as connected components and cycles hidden in data. It has been applied to enhance the representation power of deep learning models, such as Graph Neural Networks (GNNs). To enrich the representations of topological features, here we propose to study $2$-parameter persistence modules induced by bi-filtration functions. In order to incorporate these representations into machine learning models, we introduce a novel vector representation called Generalized Rank Invariant Landscape \textsc{Gril} for $2$-parameter persistence modules. We show that this vector representation is $1$-Lipschitz stable and differentiable with respect to underlying filtration functions and can be easily integrated into machine learning models to augment encoding topological features. We present an algorithm to compute the vector representation efficiently. We also test our method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02045</link><description>&lt;p&gt;
&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation by Fisher Information-based Evidential Deep Learning. (arXiv:2303.02045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Fisher Information-based Evidential Deep Learning method to address the problem of over-penalization and hindrance in evidence learning for high data uncertainty samples annotated with one-hot labels.
&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26159;&#20351;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#35777;&#25454;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#32593;&#32476;&#36755;&#20986;&#35270;&#20026;&#35777;&#25454;&#26469;&#21442;&#25968;&#21270;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#65292;&#26126;&#30830;&#32771;&#34385;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#38169;&#35823;&#26631;&#35760;&#30340;&#31867;&#21035;&#30340;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#20250;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;$\mathcal{I}$-EDL&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;Fisher&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#26469;&#34913;&#37327;&#27599;&#20010;&#26679;&#26412;&#25152;&#25658;&#24102;&#30340;&#35777;&#25454;&#30340;&#20449;&#24687;&#37327;&#65292;&#26681;&#25454;&#36825;&#20010;&#20449;&#24687;&#37327;&#65292;&#25105;&#20204;&#21487;&#20197;&#21160;&#24577;&#22320;&#37325;&#26032;&#21152;&#26435;&#30446;&#26631;&#25439;&#22833;&#39033;&#65292;&#20351;&#32593;&#32476;&#26356;&#21152;&#19987;&#27880;&#20110;&#19981;&#30830;&#23450;&#31867;&#21035;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36890;&#36807;&#20248;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focused on the representation learning of uncertain classes. The generalization ability of our network is further improved by opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558; QMLTP &#38382;&#39064;&#32763;&#35793;&#25104;&#39640;&#38454;&#36923;&#36753;&#24182;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#23884;&#20837;&#36807;&#31243;&#21487;&#38752;&#19988;&#25104;&#21151;&#65292;&#21518;&#31471; ATP &#31995;&#32479;&#30340;&#36873;&#25321;&#20250;&#26174;&#33879;&#24433;&#21709;&#23884;&#20837;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#26412;&#22320;&#27169;&#24577;&#36923;&#36753; ATP &#31995;&#32479;&#20248;&#20110;&#23884;&#20837;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.09570</link><description>&lt;p&gt;
&#23558; QMLTP &#38382;&#39064;&#32763;&#35793;&#25104;&#39640;&#38454;&#36923;&#36753;&#24182;&#35299;&#20915;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Solving QMLTP Problems by Translation to Higher-order Logic. (arXiv:2212.09570v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558; QMLTP &#38382;&#39064;&#32763;&#35793;&#25104;&#39640;&#38454;&#36923;&#36753;&#24182;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#23884;&#20837;&#36807;&#31243;&#21487;&#38752;&#19988;&#25104;&#21151;&#65292;&#21518;&#31471; ATP &#31995;&#32479;&#30340;&#36873;&#25321;&#20250;&#26174;&#33879;&#24433;&#21709;&#23884;&#20837;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#26412;&#22320;&#27169;&#24577;&#36923;&#36753; ATP &#31995;&#32479;&#20248;&#20110;&#23884;&#20837;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#23545;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126; (ATP) &#31995;&#32479;&#22312;&#20174; QMLTP &#19968;&#38454;&#27169;&#24577;&#36923;&#36753;&#38382;&#39064;&#24211;&#20013;&#21462;&#20986;&#30340;&#38382;&#39064;&#19978;&#30340;&#35780;&#20272;&#12290;&#20027;&#35201;&#26159;&#23558;&#38382;&#39064;&#20351;&#29992;&#23884;&#20837;&#26041;&#27861;&#32763;&#35793;&#20026; TPTP &#35821;&#35328;&#20013;&#30340;&#39640;&#38454;&#36923;&#36753;&#65292;&#24182;&#20351;&#29992;&#39640;&#38454;&#36923;&#36753; ATP &#31995;&#32479;&#36827;&#34892;&#27714;&#35299;&#12290;&#27492;&#22806;&#65292;&#36824;&#32771;&#34385;&#20102;&#26469;&#33258;&#26412;&#22320;&#27169;&#24577;&#36923;&#36753; ATP &#31995;&#32479;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#23884;&#20837;&#26041;&#27861;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23884;&#20837;&#36807;&#31243;&#21487;&#38752;&#19988;&#25104;&#21151;&#65292;&#21518;&#31471; ATP &#31995;&#32479;&#30340;&#36873;&#25321;&#20250;&#26174;&#33879;&#24433;&#21709;&#23884;&#20837;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#26412;&#22320;&#27169;&#24577;&#36923;&#36753; ATP &#31995;&#32479;&#20248;&#20110;&#23884;&#20837;&#26041;&#27861;&#65292;&#32780;&#23884;&#20837;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#27604;&#25152;&#32771;&#34385;&#30340;&#26412;&#22320;&#27169;&#24577;&#31995;&#32479;&#26356;&#24191;&#27867;&#30340;&#27169;&#24577;&#36923;&#36753;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes an evaluation of Automated Theorem Proving (ATP) systems on problems taken from the QMLTP library of first-order modal logic problems. Principally, the problems are translated to higher-order logic in the TPTP language using an embedding approach, and solved using higher-order logic ATP systems. Additionally, the results from native modal logic ATP systems are considered, and compared with those from the embedding approach. The findings are that the embedding process is reliable and successful, the choice of backend ATP system can significantly impact the performance of the embedding approach, native modal logic ATP systems outperform the embedding approach, and the embedding approach can cope with a wider range modal logics than the native modal systems considered.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#30340;&#26694;&#26550;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;GEC&#65292;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23558;RL&#38382;&#39064;&#21010;&#20998;&#20026;&#20302;GEC&#21644;&#39640;GEC&#20004;&#20010;&#31867;&#21035;&#65292;&#24182;&#23637;&#31034;&#20102;&#20302;GEC&#31867;&#21035;&#30340;&#20016;&#23500;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2211.01962</link><description>&lt;p&gt;
GEC: &#19968;&#31181;&#22312;MDP&#12289;POMDP&#21644;&#26356;&#22810;&#24773;&#20917;&#19979;&#20132;&#20114;&#24335;&#20915;&#31574;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond. (arXiv:2211.01962v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#30340;&#26694;&#26550;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;GEC&#65292;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23558;RL&#38382;&#39064;&#21010;&#20998;&#20026;&#20302;GEC&#21644;&#39640;GEC&#20004;&#20010;&#31867;&#21035;&#65292;&#24182;&#23637;&#31034;&#20102;&#20302;GEC&#31867;&#21035;&#30340;&#20016;&#23500;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#30340;&#26222;&#36941;&#26694;&#26550;&#19979;&#30340;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#21644;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#65288;PSR&#65289;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#12290;&#20026;&#20102;&#25214;&#21040;&#36171;&#20104;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24191;&#20041;eluder&#31995;&#25968;&#65288;GEC&#65289;&#65292;&#23427;&#34920;&#24449;&#20102;&#22312;&#32447;&#20132;&#20114;&#24335;&#20915;&#31574;&#20013;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GEC&#36890;&#36807;&#27604;&#36739;&#39044;&#27979;&#26356;&#26032;&#31574;&#30053;&#24615;&#33021;&#30340;&#35823;&#24046;&#19982;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#35780;&#20272;&#30340;&#26679;&#26412;&#20869;&#35757;&#32451;&#35823;&#24046;&#65292;&#26469;&#34913;&#37327;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20302;GEC&#30340;RL&#38382;&#39064;&#24418;&#25104;&#20102;&#19968;&#20010;&#38750;&#24120;&#20016;&#23500;&#30340;&#31867;&#21035;&#65292;&#20854;&#20013;&#21253;&#25324;&#20302;Bellman eluder&#32500;&#24230;&#38382;&#39064;&#12289;&#21452;&#32447;&#24615;&#31867;&#12289;&#20302;&#35777;&#20154;&#31209;&#38382;&#39064;&#12289;PO-&#21452;&#32447;&#24615;&#31867;&#21644;&#24191;&#20041;&#27491;&#21017;PSR&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sample efficient reinforcement learning (RL) under the general framework of interactive decision making, which includes Markov decision process (MDP), partially observable Markov decision process (POMDP), and predictive state representation (PSR) as special cases. Toward finding the minimum assumption that empowers sample efficient learning, we propose a novel complexity measure, generalized eluder coefficient (GEC), which characterizes the fundamental tradeoff between exploration and exploitation in online interactive decision making. In specific, GEC captures the hardness of exploration by comparing the error of predicting the performance of the updated policy with the in-sample training error evaluated on the historical data. We show that RL problems with low GEC form a remarkably rich class, which subsumes low Bellman eluder dimension problems, bilinear class, low witness rank problems, PO-bilinear class, and generalized regular PSR, where generalized regular PSR, a new tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#36827;&#34892;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#23427;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#24182;&#21033;&#29992;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#21487;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.11549</link><description>&lt;p&gt;
MAGIC: &#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#23454;&#29616;&#22522;&#20110;&#25513;&#30721;&#30340;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier. (arXiv:2209.11549v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#36827;&#34892;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#23427;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#24182;&#21033;&#29992;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#21487;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#25513;&#30721;&#24341;&#23548;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#24102;&#26377;&#24378;&#27491;&#21017;&#21270;&#22120;&#30340;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#26469;&#25511;&#21046;&#23545;&#21333;&#20010;&#22270;&#20687;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;MAGIC&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#30340;&#20934;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#32467;&#26500;&#21270;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#36755;&#20837;&#30340;&#35821;&#20041;&#65292;&#24182;&#20445;&#25345;&#20854;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20445;&#35777;&#21512;&#25104;&#30340;&#21487;&#20449;&#24230;&#12290;&#19982;&#30446;&#21069;&#20351;&#29992;&#22797;&#26434;&#21407;&#35821;&#26469;&#30417;&#30563;&#36807;&#31243;&#25110;&#20351;&#29992;&#27880;&#24847;&#21147;&#22270;&#20316;&#20026;&#24369;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MAGIC&#36890;&#36807;&#22312;&#36755;&#20837;&#19978;&#32858;&#21512;&#26799;&#24230;&#65292;&#30001;&#24378;&#31354;&#38388;&#20808;&#39564;&#30340;&#25351;&#23548;&#20108;&#36827;&#21046;&#25513;&#30721;&#25512;&#21160;&#12290;MAGIC&#20197;&#21333;&#20010;&#26694;&#26550;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#24418;&#29366;&#21644;&#20301;&#32622;&#25511;&#21046;&#12289;&#24378;&#28872;&#30340;&#38750;&#21018;&#24615;&#24418;&#29366;&#21464;&#24418;&#20197;&#21450;&#22312;&#37325;&#22797;&#29289;&#20307;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#22797;&#21046;/&#31227;&#21160;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#25351;&#23450;&#20108;&#36827;&#21046;&#24341;&#23548;&#25513;&#30721;&#26469;&#32473;&#29992;&#25143;&#25552;&#20379;&#24378;&#22823;&#30340;&#21512;&#25104;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We offer a method for one-shot mask-guided image synthesis that allows controlling manipulations of a single image by inverting a quasi-robust classifier equipped with strong regularizers. Our proposed method, entitled MAGIC, leverages structured gradients from a pre-trained quasi-robust classifier to better preserve the input semantics while preserving its classification accuracy, thereby guaranteeing credibility in the synthesis. Unlike current methods that use complex primitives to supervise the process or use attention maps as a weak supervisory signal, MAGIC aggregates gradients over the input, driven by a guide binary mask that enforces a strong, spatial prior. MAGIC implements a series of manipulations with a single framework achieving shape and location control, intense non-rigid shape deformations, and copy/move operations in the presence of repeating objects and gives users firm control over the synthesis by requiring to simply specify binary guide masks. Our study and findin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;A*&#25628;&#32034;&#30340;&#26032;&#35299;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26435;&#37325;&#32422;&#26463;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65288;WCSPP&#65289;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.14744</link><description>&lt;p&gt;
&#21152;&#24378;&#26435;&#37325;&#32422;&#26463;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhanced Methods for the Weight Constrained Shortest Path Problem. (arXiv:2207.14744v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;A*&#25628;&#32034;&#30340;&#26032;&#35299;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26435;&#37325;&#32422;&#26463;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65288;WCSPP&#65289;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#32422;&#26463;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65288;WCSPP&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#20449;&#21644;&#20132;&#36890;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#32467;&#21512;&#32422;&#26463;&#36335;&#24452;&#35268;&#21010;&#21644;&#21452;&#30446;&#26631;&#25628;&#32034;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;A*&#25628;&#32034;&#30340;&#26032;&#35299;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;WCSPP&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#33021;&#22815;&#35299;&#20915;&#22256;&#38590;&#30340;WCSPP&#23454;&#20363;&#65292;&#24182;&#19988;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classic problem of constrained pathfinding is a well-studied, yet challenging, topic in AI with a broad range of applications in various areas such as communication and transportation. The Weight Constrained Shortest Path Problem (WCSPP), the base form of constrained pathfinding with only one side constraint, aims to plan a cost-optimum path with limited weight/resource usage. Given the bi-criteria nature of the problem (i.e., dealing with the cost and weight of paths), methods addressing the WCSPP have some common properties with bi-objective search. This paper leverages the recent state-of-the-art techniques in both constrained pathfinding and bi-objective search and presents two new solution approaches to the WCSPP on the basis of A* search, both capable of solving hard WCSPP instances on very large graphs. We empirically evaluate the performance of our algorithms on a set of large and realistic problem instances and show their advantages over the state-of-the-art algorithms in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#36141;&#20080;&#39034;&#24207;&#20197;&#39044;&#27979;&#20182;&#20204;&#30340;&#19979;&#19968;&#27425;&#36141;&#20080;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#20449;&#29992;&#21345;&#20132;&#26131;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2207.06225</link><description>&lt;p&gt;
&#38024;&#23545;&#19979;&#19968;&#27425;&#36141;&#20080;&#39044;&#27979;&#30340;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommendation Model for Next Purchase Prediction. (arXiv:2207.06225v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#36141;&#20080;&#39034;&#24207;&#20197;&#39044;&#27979;&#20182;&#20204;&#30340;&#19979;&#19968;&#27425;&#36141;&#20080;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#20449;&#29992;&#21345;&#20132;&#26131;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#20379;&#24403;&#20195;&#25968;&#23383;&#33829;&#38144;&#20307;&#39564;&#26102;&#65292;&#25512;&#33616;&#30340;&#26102;&#25928;&#24615;&#21644;&#19978;&#19979;&#25991;&#20934;&#30830;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#30340;&#36807;&#21435;&#36141;&#20080;&#35760;&#24405;&#21521;&#29992;&#25143;&#25512;&#33616;&#30456;&#20851;&#20294;&#19981;&#21463;&#26102;&#38388;&#24433;&#21709;&#30340;&#29289;&#21697;&#12290;&#36825;&#20123;&#25512;&#33616;&#21482;&#26159;&#31526;&#21512;&#29992;&#25143;&#30340;&#19968;&#33324;&#20559;&#22909;&#65292;&#32780;&#19981;&#26159;&#29992;&#25143;&#22312;&#36141;&#20080;&#20043;&#21069;&#30340;&#20855;&#20307;&#38656;&#27714;&#12290;&#30456;&#21453;&#65292;&#32771;&#34385;&#20132;&#26131;&#12289;&#36141;&#20080;&#25110;&#20307;&#39564;&#39034;&#24207;&#26469;&#34913;&#37327;&#29992;&#25143;&#28436;&#21270;&#20559;&#22909;&#30340;&#25512;&#33616;&#31995;&#32479;&#33021;&#22815;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25512;&#33616;&#65306;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#33021;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#24403;&#21069;&#38656;&#27714;&#30340;&#34892;&#20026;&#65292;&#36824;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20221;&#21253;&#21547;&#36229;&#36807;2.7&#30334;&#19975;&#20449;&#29992;&#21345;&#20132;&#26131;&#25968;&#25454;&#21644;46K&#20010;&#25345;&#21345;&#20154;&#30340;&#29983;&#20135;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#24182;&#25490;&#21517;&#20102;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#23545;&#21407;&#22987;&#30340;&#20132;&#26131;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#28982;&#21518;&#25552;&#20132;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Timeliness and contextual accuracy of recommendations are increasingly important when delivering contemporary digital marketing experiences. Conventional recommender systems (RS) suggest relevant but time-invariant items to users by accounting for their past purchases. These recommendations only map to customers' general preferences rather than a customer's specific needs immediately preceding a purchase. In contrast, RSs that consider the order of transactions, purchases, or experiences to measure evolving preferences can offer more salient and effective recommendations to customers: Sequential RSs not only benefit from a better behavioral understanding of a user's current needs but also better predictive power. In this paper, we demonstrate and rank the effectiveness of a sequential recommendation system by utilizing a production dataset of over 2.7 million credit card transactions for 46K cardholders. The method first employs an autoencoder on raw transaction data and submits observ
&lt;/p&gt;</description></item><item><title>Plurality Veto&#26159;&#19968;&#31181;&#23454;&#29616;&#26368;&#20248;&#24230;&#37327;&#30072;&#21464;&#30340;&#31616;&#21333;&#25237;&#31080;&#35268;&#21017;&#65292;&#27599;&#20010;&#20505;&#36873;&#20154;&#30340;&#24471;&#20998;&#31561;&#20110;&#20182;&#30340;&#31532;&#19968;&#21517;&#36873;&#31080;&#25968;&#65292;&#24182;&#36890;&#36807;&#36880;&#27493;&#30340;&#21542;&#20915;&#36807;&#31243;&#36880;&#28176;&#38477;&#20302;&#24471;&#20998;&#65292;&#22312;&#20854;&#24471;&#31080;&#25968;&#36798;&#21040;&#19968;&#21322;&#26102;&#36864;&#20986;&#12290;</title><link>http://arxiv.org/abs/2206.07098</link><description>&lt;p&gt;
Plurality Veto&#65306;&#19968;&#31181;&#23454;&#29616;&#26368;&#20248;&#24230;&#37327;&#30072;&#21464;&#30340;&#31616;&#21333;&#25237;&#31080;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Plurality Veto: A Simple Voting Rule Achieving Optimal Metric Distortion. (arXiv:2206.07098v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07098
&lt;/p&gt;
&lt;p&gt;
Plurality Veto&#26159;&#19968;&#31181;&#23454;&#29616;&#26368;&#20248;&#24230;&#37327;&#30072;&#21464;&#30340;&#31616;&#21333;&#25237;&#31080;&#35268;&#21017;&#65292;&#27599;&#20010;&#20505;&#36873;&#20154;&#30340;&#24471;&#20998;&#31561;&#20110;&#20182;&#30340;&#31532;&#19968;&#21517;&#36873;&#31080;&#25968;&#65292;&#24182;&#36890;&#36807;&#36880;&#27493;&#30340;&#21542;&#20915;&#36807;&#31243;&#36880;&#28176;&#38477;&#20302;&#24471;&#20998;&#65292;&#22312;&#20854;&#24471;&#31080;&#25968;&#36798;&#21040;&#19968;&#21322;&#26102;&#36864;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24230;&#37327;&#30072;&#21464;&#26694;&#26550;&#20551;&#35774;n&#20010;&#36873;&#27665;&#21644;m&#20010;&#20505;&#36873;&#20154;&#20849;&#21516;&#23884;&#20837;&#22312;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#36873;&#27665;&#25353;&#29031;&#31163;&#20182;&#20204;&#26356;&#36817;&#30340;&#20505;&#36873;&#20154;&#36827;&#34892;&#25490;&#21517;&#12290;&#19968;&#20010;&#25237;&#31080;&#35268;&#21017;&#30340;&#30446;&#30340;&#26159;&#22312;&#21482;&#30693;&#36947;&#25490;&#21517;&#32780;&#19981;&#30693;&#36947;&#23454;&#38469;&#36317;&#31163;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#19968;&#20010;&#19982;&#36873;&#27665;&#30340;&#24635;&#36317;&#31163;&#26368;&#23567;&#30340;&#20505;&#36873;&#20154;&#12290;&#32467;&#26524;&#26159;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#30830;&#23450;&#24615;&#35268;&#21017;&#36873;&#25321;&#30340;&#20505;&#36873;&#20154;&#30340;&#24635;&#36317;&#31163;&#33267;&#23569;&#26159;&#26368;&#20248;&#35268;&#21017;&#30340;&#19977;&#20493;&#65292;&#21363;&#20855;&#26377;&#33267;&#23569;3&#30340;&#24230;&#37327;&#30072;&#21464;&#12290;&#26368;&#36817;&#30340;&#31361;&#30772;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#23454;&#29616;&#36825;&#20010;3&#30340;&#30028;&#38480;&#26159;&#21487;&#33021;&#30340;&#65307;&#28982;&#32780;&#65292;&#35777;&#26126;&#26159;&#38750;&#26500;&#36896;&#24615;&#30340;&#65292;&#24182;&#19988;&#25237;&#31080;&#35268;&#21017;&#26412;&#36523;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#31351;&#20030;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#30340;&#25237;&#31080;&#35268;&#21017;&#65292;&#31216;&#20026;&#22810;&#25968;&#21542;&#20915;&#21046;&#65292;&#23427;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#26368;&#20248;&#30072;&#21464;&#24230;&#20026;3&#12290;&#27599;&#20010;&#20505;&#36873;&#20154;&#30340;&#24471;&#20998;&#24320;&#22987;&#31561;&#20110;&#20182;&#30340;&#31532;&#19968;&#21517;&#36873;&#31080;&#25968;&#12290;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;n&#36718;&#30340;&#21542;&#20915;&#36807;&#31243;&#36880;&#28176;&#38477;&#20302;&#36825;&#20123;&#24471;&#20998;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#20505;&#36873;&#20154;&#22312;&#20854;&#24471;&#31080;&#25968;&#36798;&#21040;n&#30340;&#19968;&#21322;&#26102;&#36864;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
The metric distortion framework posits that n voters and m candidates are jointly embedded in a metric space such that voters rank candidates that are closer to them higher. A voting rule's purpose is to pick a candidate with minimum total distance to the voters, given only the rankings, but not the actual distances. As a result, in the worst case, each deterministic rule picks a candidate whose total distance is at least three times larger than that of an optimal one, i.e., has distortion at least 3. A recent breakthrough result showed that achieving this bound of 3 is possible; however, the proof is non-constructive, and the voting rule itself is a complicated exhaustive search.  Our main result is an extremely simple voting rule, called Plurality Veto, which achieves the same optimal distortion of 3. Each candidate starts with a score equal to his number of first-place votes. These scores are then gradually decreased via an n-round veto process in which a candidate drops out when hi
&lt;/p&gt;</description></item></channel></rss>