<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>LMFlow&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;&#65292;&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#24182;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#24494;&#35843;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.12420</link><description>&lt;p&gt;
LMFlow&#65306;&#29992;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. (arXiv:2306.12420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12420
&lt;/p&gt;
&lt;p&gt;
LMFlow&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;&#65292;&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#24182;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#24494;&#35843;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#23637;&#29616;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#25509;&#36817;&#20154;&#31867;&#26234;&#33021;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#27169;&#22411;&#22312;&#19987;&#19994;&#20219;&#21153;&#24212;&#29992;&#20013;&#20173;&#28982;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#32570;&#38519;&#65292;&#38656;&#35201;&#24494;&#35843;&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#21487;&#29992;&#27169;&#22411;&#21644;&#19987;&#19994;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36890;&#29992;&#24494;&#35843;&#30340;&#24037;&#20316;&#21464;&#24471;&#38750;&#24120;&#26840;&#25163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#31532;&#19968;&#27493;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;LMFlow&#65292;&#26088;&#22312;&#31616;&#21270;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#25512;&#29702;&#12290;LMFlow&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#31561;&#21151;&#33021;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models have demonstrated a great ability to achieve general human-level intelligence far beyond traditional approaches. As the technique keeps attracting attention from the AI community, more and more large foundation models have become publically available. However, most of those models exhibit a major deficiency in specialized-task applications, where the step of finetuning is still required for obtaining satisfactory performance. As the number of available models and specialized tasks keeps growing, the job of general finetuning becomes highly nontrivial. In this paper, we take the first step to address this issue. We introduce an extensible and lightweight toolkit, LMFlow, which aims to simplify the finetuning and inference of general large foundation models. LMFlow offers a complete finetuning workflow for a large foundation model to support personalized training with limited computing resources. Furthermore, it supports continuous pretraining, instruction tuning,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#27714;&#35299;&#22120;&#21644;&#37197;&#23545;&#27604;&#36739;&#36827;&#34892;&#38656;&#27714;&#20248;&#20808;&#32423;&#25490;&#24207;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#38656;&#27714;&#20998;&#26512;&#24072;&#30340;&#20132;&#20114;&#20316;&#29992;&#19979;&#31215;&#32047;&#30693;&#35782;&#65292;&#21487;&#20197;&#26174;&#33879;&#22320;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12391</link><description>&lt;p&gt;
&#36890;&#36807;&#32422;&#26463;&#27714;&#35299;&#25913;&#36827;&#36719;&#20214;&#38656;&#27714;&#20248;&#20808;&#32423;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Improving Software Requirements Prioritization through the Lens of Constraint Solving. (arXiv:2306.12391v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#27714;&#35299;&#22120;&#21644;&#37197;&#23545;&#27604;&#36739;&#36827;&#34892;&#38656;&#27714;&#20248;&#20808;&#32423;&#25490;&#24207;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#38656;&#27714;&#20998;&#26512;&#24072;&#30340;&#20132;&#20114;&#20316;&#29992;&#19979;&#31215;&#32047;&#30693;&#35782;&#65292;&#21487;&#20197;&#26174;&#33879;&#22320;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#27714;&#20248;&#20808;&#32423;&#25490;&#24207;&#26159;&#26089;&#26399;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#20851;&#38190;&#30340;&#27963;&#21160;&#20043;&#19968;&#65292;&#23427;&#20135;&#29983;&#20102;&#19968;&#32452;&#35201;&#23454;&#26045;&#30340;&#37325;&#28857;&#38656;&#27714;&#12290;&#35813;&#25490;&#24207;&#36807;&#31243;&#22522;&#20110;&#22810;&#20010;&#29305;&#24449;&#65288;&#21253;&#25324;&#26368;&#32456;&#29992;&#25143;&#30340;&#20559;&#22909;&#12289;&#23454;&#29616;&#25104;&#26412;&#21644;&#25216;&#26415;&#20381;&#36182;&#20851;&#31995;&#65289;&#20026;&#38656;&#27714;&#25552;&#20379;&#20102;&#24179;&#31561;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37197;&#23545;&#27604;&#36739;&#21644;&#32422;&#26463;&#27714;&#35299;&#22120;&#36827;&#34892;&#38656;&#27714;&#20248;&#20808;&#32423;&#25490;&#24207;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#12290;&#24403;&#30456;&#23545;&#20248;&#20808;&#32423;&#19981;&#33021;&#22522;&#20110;&#38656;&#27714;&#25991;&#26723;&#20013;&#23384;&#22312;&#30340;&#30693;&#35782;&#30830;&#23450;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#38656;&#27714;&#20998;&#26512;&#24072;&#30340;&#20132;&#20114;&#24335;&#30693;&#35782;&#31215;&#32047;&#26469;&#20915;&#23450;&#38656;&#27714;&#30340;&#30456;&#23545;&#20248;&#20808;&#32423;&#12290;&#38656;&#27714;&#30340;&#26368;&#32456;&#25490;&#24207;&#36890;&#36807;&#32422;&#26463;&#27714;&#35299;&#22120;&#21644;&#20132;&#20114;&#24335;&#37197;&#23545;&#27604;&#36739;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#30340;&#21307;&#30103;&#20445;&#20581;&#39033;&#30446;&#38656;&#27714;&#36827;&#34892;&#20102;&#26041;&#27861;&#35780;&#20272;&#12290;&#20381;&#36182;&#20110;&#32422;&#26463;&#27714;&#35299;&#22120;&#30340;&#25152;&#25552;&#20986;&#30340;&#20248;&#20808;&#32423;&#25490;&#24207;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-the-art
&lt;/p&gt;
&lt;p&gt;
Requirements prioritization is a critical activity during the early software development process, which produces a set of key requirements to implement. The prioritization process offers a parity among the requirements based on multiple characteristics, including end-users' preferences, cost to implement, and technical dependencies. This paper presents an interactive method to requirements prioritization that leverages the pairwise comparisons and a constraint solver. Our method employs an interactive accumulation of knowledge from the requirements analyst when the relative priority among the requirements cannot be determined based on the existing knowledge from the requirements documents. The final ranking of the requirements is produced via the constraint solver and interactive pairwise comparisons. We evaluate the proposed method using the requirements from a real healthcare project. The proposed prioritization method relying on a constraint solver outperforms state-of-the-art inter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#37319;&#29992;Transformer&#32593;&#32476;&#23545;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#24314;&#27169;&#25928;&#26524;&#65292;&#21457;&#29616;&#30456;&#27604;LSTM&#27169;&#22411;&#32570;&#20047;&#20248;&#21183;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#39532;&#23572;&#21487;&#22827;&#29305;&#24615;&#25152;&#23548;&#33268;&#12290;</title><link>http://arxiv.org/abs/2306.12384</link><description>&lt;p&gt;
&#29992;Transformer&#32593;&#32476;&#25506;&#31350;&#27700;&#25991;&#39044;&#27979;&#30340;&#26497;&#38480;&#65288;arXiv:2306.12384v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Probing the limit of hydrologic predictability with the Transformer network. (arXiv:2306.12384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#37319;&#29992;Transformer&#32593;&#32476;&#23545;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#24314;&#27169;&#25928;&#26524;&#65292;&#21457;&#29616;&#30456;&#27604;LSTM&#27169;&#22411;&#32570;&#20047;&#20248;&#21183;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#39532;&#23572;&#21487;&#22827;&#29305;&#24615;&#25152;&#23548;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20854;&#24341;&#20837;&#27700;&#25991;&#23398;&#39046;&#22495;&#20197;&#26469;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;&#22914;LSTM&#65289;&#22312;&#24050;&#30693;&#30340;&#21487;&#27604;&#22522;&#20934;&#19978;&#30340;&#26085;&#27700;&#25991;&#22270;&#34920;&#25351;&#26631;&#26041;&#38754;&#19968;&#30452;&#35777;&#26126;&#38590;&#20197;&#36229;&#36234;&#12290;&#38500;&#27700;&#25991;&#23398;&#22806;&#65292;Transformer&#29616;&#22312;&#24050;&#25104;&#20026;&#36830;&#32493;&#39044;&#27979;&#20219;&#21153;&#30340;&#39318;&#36873;&#27169;&#22411;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26550;&#26500;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#39321;&#33609;Transformer&#26550;&#26500;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CAMELS&#25968;&#25454;&#38598;&#19978;&#19982;LSTM&#30456;&#27604;&#27627;&#26080;&#31454;&#20105;&#21147;&#65292;&#30001;&#20110;&#30701;&#26399;&#36807;&#31243;&#32780;&#29305;&#21035;&#28382;&#21518;&#20110;&#39640;&#27969;&#37327;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#19981;&#38656;&#35201;&#36882;&#24402;&#30340;&#21464;&#20307;Transformer&#21487;&#20197;&#33719;&#24471;&#19982;LSTM&#30456;&#28151;&#21512;&#30340;&#27604;&#36739;&#65292;&#20135;&#29983;&#30456;&#21516;&#30340;Kling-Gupta&#25928;&#29575;&#31995;&#25968;&#65288;KGE&#65289;&#20197;&#21450;&#20854;&#20182;&#25351;&#26631;&#12290;Transformer&#27809;&#26377;&#20248;&#21183;&#19982;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#39532;&#23572;&#21487;&#22827;&#29305;&#24615;&#26377;&#20851;&#12290;&#19982;LSTM&#31867;&#20284;&#65292;Transformer&#20063;&#21487;&#20197;&#21512;&#24182;&#22810;&#20010;&#24378;&#21046;&#25968;&#25454;&#38598;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#23558;&#27700;&#25991;&#39044;&#27979;&#38382;&#39064;&#30340;&#20851;&#38190;&#29305;&#24449;&#36716;&#25442;&#20026;&#26356;&#25551;&#36848;&#24615;&#30340;&#31354;&#38388;&#65292;Transformer&#27169;&#22411;&#20284;&#20046;&#26080;&#27861;&#36229;&#36234;LSTM&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a number of years since its introduction to hydrology, recurrent neural networks like long short-term memory (LSTM) have proven remarkably difficult to surpass in terms of daily hydrograph metrics on known, comparable benchmarks. Outside of hydrology, Transformers have now become the model of choice for sequential prediction tasks, making it a curious architecture to investigate. Here, we first show that a vanilla Transformer architecture is not competitive against LSTM on the widely benchmarked CAMELS dataset, and lagged especially for the high-flow metrics due to short-term processes. However, a recurrence-free variant of Transformer can obtain mixed comparisons with LSTM, producing the same Kling-Gupta efficiency coefficient (KGE), along with other metrics. The lack of advantages for the Transformer is linked to the Markovian nature of the hydrologic prediction problem. Similar to LSTM, the Transformer can also merge multiple forcing dataset to improve model performance. While t
&lt;/p&gt;</description></item><item><title>SIFTER &#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#38656;&#27714;&#30340;&#20219;&#21153;&#29305;&#23450;&#23545;&#40784;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.12280</link><description>&lt;p&gt;
SIFTER: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#23884;&#20837;&#30340;&#20219;&#21153;&#29305;&#23450;&#23545;&#40784;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence Embeddings. (arXiv:2306.12280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12280
&lt;/p&gt;
&lt;p&gt;
SIFTER &#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#38656;&#27714;&#30340;&#20219;&#21153;&#29305;&#23450;&#23545;&#40784;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#21518;&#36827;&#34892;&#24494;&#35843;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#34429;&#28982;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#27867;&#21270;&#30340;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#30340;&#34920;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#20013;&#20173;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#26159;&#22240;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#23545;&#21477;&#23376;&#30340;&#25935;&#24863;&#31243;&#24230;&#20063;&#19981;&#21516;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#23884;&#20837;&#30340;&#20219;&#21153;&#29305;&#23450;&#23545;&#40784;&#31574;&#30053; SIFTER&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paradigm of pre-training followed by fine-tuning on downstream tasks has become the mainstream method in natural language processing tasks. Although pre-trained models have the advantage of generalization, their performance may still vary significantly across different domain tasks. This is because the data distribution in different domains varies. For example, the different parts of the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy married life' may have different impact for downstream tasks. For similarity calculations, words such as 'led' and 'life' are more important. On the other hand, for sentiment analysis, the word 'happy' is crucial. This indicates that different downstream tasks have different levels of sensitivity to sentence components. Our starting point is to scale information of the model and data according to the specifics of downstream tasks, enhancing domain information of relevant parts for these tasks and reducing irrelevant elements for di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#38899;&#20048;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#23436;&#20840;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#26356;&#22810;&#23545;&#38899;&#20048;&#30456;&#20284;&#24615;&#21644;&#20998;&#31867;&#31995;&#32479;&#30340;&#25511;&#21046;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.12249</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#38899;&#20048;&#30456;&#20284;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based Multimodal Music Similarity. (arXiv:2306.12249v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#38899;&#20048;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#23436;&#20840;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#26356;&#22810;&#23545;&#38899;&#20048;&#30456;&#20284;&#24615;&#21644;&#20998;&#31867;&#31995;&#32479;&#30340;&#25511;&#21046;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#30456;&#20284;&#24615;&#26159;&#38899;&#20048;&#26816;&#32034;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#38899;&#20048;&#20998;&#26512;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#32780;&#19988;&#65292;&#23545;&#20110;&#38899;&#20048;&#19987;&#23478;&#26469;&#35828;&#65292;&#30456;&#20284;&#24615;&#21487;&#20197;&#30740;&#31350;&#20316;&#26354;&#23478;&#21644;&#21382;&#21490;&#26102;&#26399;&#20043;&#38388;&#30340;&#31867;&#27604;&#21644;&#24433;&#21709;&#12290;&#30446;&#21069;&#65292;&#38024;&#23545;&#38899;&#20048;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#31526;&#21495;&#20869;&#23481;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#19988;&#19981;&#24635;&#26159;&#26131;&#20110;&#33719;&#24471;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#38899;&#39057;&#20449;&#21495;&#30340;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#26377;&#20851;&#35266;&#23519;&#21040;&#30340;&#30456;&#20284;&#24615;&#32972;&#21518;&#21407;&#22240;&#30340;&#20219;&#20309;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31526;&#21495;&#21644;&#38899;&#39057;&#20869;&#23481;&#26469;&#30740;&#31350;&#38899;&#20048;&#30456;&#20284;&#24615;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#20010;&#23436;&#20840;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#26356;&#22810;&#23545;&#38899;&#20048;&#30456;&#20284;&#24615;&#21644;&#20998;&#31867;&#31995;&#32479;&#30340;&#25511;&#21046;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music similarity is an essential aspect of music retrieval, recommendation systems, and music analysis. Moreover, similarity is of vital interest for music experts, as it allows studying analogies and influences among composers and historical periods. Current approaches to musical similarity rely mainly on symbolic content, which can be expensive to produce and is not always readily available. Conversely, approaches using audio signals typically fail to provide any insight about the reasons behind the observed similarity. This research addresses the limitations of current approaches by focusing on the study of musical similarity using both symbolic and audio content. The aim of this research is to develop a fully explainable and interpretable system that can provide end-users with more control and understanding of music similarity and classification systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#22823;&#23567;&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12230</link><description>&lt;p&gt;
&#22855;&#22937;&#30340;&#26435;&#37325;&#21450;&#20854;&#26597;&#25214;&#26041;&#27861;&#65306;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training. (arXiv:2306.12230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#20013;&#30340;&#21098;&#26525;&#20301;&#32622;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#22823;&#23567;&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#65288;DST&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#20248;&#21270;&#20854;&#31232;&#30095;&#21021;&#22987;&#21270;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;DST&#33021;&#22815;&#32988;&#36807;&#23494;&#38598;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#21098;&#26525;&#21644;&#29983;&#38271;&#26631;&#20934;&#65292;&#36825;&#20123;&#26631;&#20934;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#34987;&#21453;&#22797;&#24212;&#29992;&#20197;&#35843;&#25972;&#32593;&#32476;&#30340;&#31232;&#30095;&#36830;&#25509;&#12290;&#34429;&#28982;&#29983;&#38271;&#26631;&#20934;&#23545;DST&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#36739;&#22909;&#22320;&#30740;&#31350;&#20102;&#65292;&#20294;&#21098;&#26525;&#26631;&#20934;&#30340;&#24433;&#21709;&#20173;&#28982;&#34987;&#24573;&#35270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#23545;&#21508;&#31181;&#21098;&#26525;&#26631;&#20934;&#30340;&#24191;&#27867;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#23545; DST &#35299;&#20915;&#26041;&#26696;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#30740;&#31350;&#26041;&#27861;&#37117;&#20135;&#29983;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#22312;&#20302;&#23494;&#24230;&#33539;&#22260;&#20869;&#65292;&#26368;&#31616;&#21333;&#30340;&#25216;&#26415;&#8212;&#8212;&#22522;&#20110;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their effect on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;AutoRUL&#65292;&#29992;&#20110;&#33258;&#21160;&#39044;&#27979;&#24037;&#31243;&#31995;&#32479;&#30340;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#35813;&#26041;&#27861;&#23558;&#24494;&#35843;&#30340;&#26631;&#20934;&#22238;&#24402;&#26041;&#27861;&#19982;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;AutoML&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2306.12215</link><description>&lt;p&gt;
&#38754;&#21521;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#39044;&#27979;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Machine Learning for Remaining Useful Life Predictions. (arXiv:2306.12215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;AutoRUL&#65292;&#29992;&#20110;&#33258;&#21160;&#39044;&#27979;&#24037;&#31243;&#31995;&#32479;&#30340;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#35813;&#26041;&#27861;&#23558;&#24494;&#35843;&#30340;&#26631;&#20934;&#22238;&#24402;&#26041;&#27861;&#19982;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;AutoML&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24037;&#31243;&#31995;&#32479;&#30340;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#65288;RUL&#65289;&#26159;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;RUL&#39044;&#27979;&#20013;&#26222;&#21450;&#65292;&#30456;&#27604;&#27169;&#22411;&#39537;&#21160;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#24037;&#31243;&#31995;&#32479;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#36825;&#21482;&#26159;&#23558;&#38656;&#35201;&#30340;&#29289;&#29702;&#19987;&#19994;&#30693;&#35782;&#26367;&#25442;&#25104;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#19987;&#19994;&#30693;&#35782;&#65292;&#32780;&#36825;&#31181;&#19987;&#19994;&#30693;&#35782;&#36890;&#24120;&#20063;&#19981;&#21487;&#24471;&#12290;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#25215;&#35834;&#33258;&#21160;&#26500;&#24314;&#31471;&#21040;&#31471;&#30340;ML&#31649;&#36947;&#65292;&#20351;&#39046;&#22495;&#19987;&#23478;&#32780;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#21019;&#24314;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AutoRUL&#65292;&#19968;&#31181;AutoML&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;RUL&#39044;&#27979;&#12290;AutoRUL&#23558;&#24494;&#35843;&#30340;&#26631;&#20934;&#22238;&#24402;&#26041;&#27861;&#19982;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29992;&#20110;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#25163;&#24037;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#34920;&#26126;AutoML&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to predict the remaining useful life (RUL) of an engineering system is an important task in prognostics and health management. Recently, data-driven approaches to RUL predictions are becoming prevalent over model-based approaches since no underlying physical knowledge of the engineering system is required. Yet, this just replaces required expertise of the underlying physics with machine learning (ML) expertise, which is often also not available. Automated machine learning (AutoML) promises to build end-to-end ML pipelines automatically enabling domain experts without ML expertise to create their own models. This paper introduces AutoRUL, an AutoML-driven end-to-end approach for automatic RUL predictions. AutoRUL combines fine-tuned standard regression methods to an ensemble with high predictive power. By evaluating the proposed method on eight real-world and synthetic datasets against state-of-the-art hand-crafted models, we show that AutoML provides a viable alternative to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26159;&#23454;&#29616;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.12205</link><description>&lt;p&gt;
&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#36808;&#21521;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI. (arXiv:2306.12205v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26159;&#23454;&#29616;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#24494;&#35843;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#24403;&#27169;&#22411;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#24076;&#26395;&#20854;&#33021;&#33719;&#24471;&#38544;&#24335;&#30693;&#35782;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#27867;&#21270;&#21040;&#19981;&#21516;&#38750;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#20219;&#21153;&#65292;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20998;&#23618;&#25968;&#25454;&#25512;&#29702;&#21644;&#34507;&#30333;&#36136;&#25240;&#21472;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#22235;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;T5&#12289;BART&#12289;BERT &#21644; GPT-2 &#37117;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#32489;&#12290;&#23427;&#20204;&#30340;&#34920;&#29616;&#24456;&#30456;&#20284;&#65292;&#19988;&#23427;&#20204;&#30340;&#34920;&#29616;&#27604;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340; transformers &#35201;&#22909;&#24471;&#22810;&#12290;&#20363;&#22914;&#65292;&#22312; Listops &#25968;&#25454;&#38598;&#19978;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026; 58.7&#65285;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340; transformers &#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026; 29.0&#65285;&#12290;&#36328;&#19977;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#25152;&#23637;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26159;&#23454;&#29616;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26377;&#21069;&#36884;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have recently emerged as a powerful tool for fine-tuning a variety of language tasks. Ideally, when models are pre-trained on large amount of data, they are expected to gain implicit knowledge. In this paper, we investigate the ability of pre-trained language models to generalize to different non-language tasks. In particular, we test them on tasks from different domains such as computer vision, reasoning on hierarchical data, and protein fold prediction. The four pre-trained models that we used, T5, BART, BERT, and GPT-2 achieve outstanding results. They all have similar performance and they outperform transformers that are trained from scratch by a large margin. For instance, pre-trained language models perform better on the Listops dataset, with an average accuracy of 58.7\%, compared to transformers trained from scratch, which have an average accuracy of 29.0\%. The significant improvement demonstrated across three types of datasets suggests that pre-tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#20855;&#20307;&#20351;&#29992;&#32422;&#26463;&#31639;&#26415;&#38382;&#39064;&#25506;&#32034;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#25968;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20197;&#30053;&#24494;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20998;&#23618;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.12198</link><description>&lt;p&gt;
&#25581;&#24320;&#40657;&#21283;&#23376;&#65306;&#20998;&#26512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks. (arXiv:2306.12198v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#20855;&#20307;&#20351;&#29992;&#32422;&#26463;&#31639;&#26415;&#38382;&#39064;&#25506;&#32034;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#25968;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20197;&#30053;&#24494;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20998;&#23618;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22810;&#25968;&#20808;&#36827;&#27169;&#22411;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#29305;&#24615;&#65292;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#38543;&#30528;&#22522;&#20110;transformers&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#36827;&#23637;&#21450;&#20854;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#19981;&#26029;&#38598;&#25104;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;AI&#27169;&#22411;&#65292;&#24517;&#39035;&#29702;&#35299;&#28041;&#21450;&#30340;&#36807;&#31243;&#27493;&#39588;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#36827;&#34892;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#26131;&#25026;&#30340;&#38750;&#35821;&#35328;&#20219;&#21153;&#26469;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#36816;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#30340;&#32422;&#26463;&#31639;&#26415;&#38382;&#39064;&#65292;&#20197;&#20998;&#26512;&#20854;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#25968;&#21644;&#38544;&#34255;&#29366;&#24577;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#27169;&#22411;&#20197;&#30053;&#24494;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20998;&#23618;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating deep learning language models has always been a significant research area due to the ``black box" nature of most advanced models. With the recent advancements in pre-trained language models based on transformers and their increasing integration into daily life, addressing this issue has become more pressing. In order to achieve an explainable AI model, it is essential to comprehend the procedural steps involved and compare them with human thought processes. Thus, in this paper, we use simple, well-understood non-language tasks to explore these models' inner workings. Specifically, we apply a pre-trained language model to constrained arithmetic problems with hierarchical structure, to analyze their attention weight scores and hidden states. The investigation reveals promising results, with the model addressing hierarchical problems in a moderately structured manner, similar to human problem-solving strategies. Additionally, by inspecting the attention weights layer by laye
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21482;&#26377;&#19968;&#20010;&#35282;&#33394;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#38754;&#37096;&#34920;&#24773;&#25968;&#25454;&#38598;&#31232;&#23569;&#12289;&#26631;&#27880;&#22256;&#38590;&#31561;&#38382;&#39064;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#23558;&#38754;&#37096;&#22120;&#23448;&#30340;&#26631;&#24535;&#28857;&#20998;&#32452;&#32452;&#32455;&#65292;&#24182;&#23558;&#23427;&#20204;&#36830;&#25509;&#21040;&#30456;&#20851;&#30340;&#34920;&#24773;&#26435;&#37325;&#19978;&#12290;</title><link>http://arxiv.org/abs/2306.12188</link><description>&lt;p&gt;
&#21333;&#20010;&#35282;&#33394;&#30340;&#38754;&#37096;&#34920;&#24773;&#37325;&#23450;&#21521;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Facial Expression Re-targeting from a Single Character. (arXiv:2306.12188v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21482;&#26377;&#19968;&#20010;&#35282;&#33394;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#38754;&#37096;&#34920;&#24773;&#25968;&#25454;&#38598;&#31232;&#23569;&#12289;&#26631;&#27880;&#22256;&#38590;&#31561;&#38382;&#39064;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#23558;&#38754;&#37096;&#22120;&#23448;&#30340;&#26631;&#24535;&#28857;&#20998;&#32452;&#32452;&#32455;&#65292;&#24182;&#23558;&#23427;&#20204;&#36830;&#25509;&#21040;&#30456;&#20851;&#30340;&#34920;&#24773;&#26435;&#37325;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#38754;&#37096;&#21160;&#30011;&#20013;&#30340;&#35270;&#39057;&#37325;&#23450;&#21521;&#26159;&#34394;&#25311;&#29616;&#23454;&#12289;&#31038;&#20132;&#23186;&#20307;&#12289;&#28216;&#25103;&#12289;&#30005;&#24433;&#21644;&#35270;&#39057;&#20250;&#35758;&#20013;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#26681;&#25454;&#20154;&#33080;&#35270;&#39057;&#28608;&#27963;&#35282;&#33394;&#30340;&#38754;&#37096;&#34920;&#24773;&#12290;&#26412;&#25991;&#20013;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#38754;&#37096;&#34920;&#24773;&#25968;&#25454;&#38598;&#31232;&#23569;&#12289;&#26631;&#27880;&#22256;&#38590;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#21482;&#26377;&#19968;&#20010;&#35282;&#33394;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#23558;&#27599;&#24103;&#22270;&#20687;&#37325;&#26032;&#34920;&#31034;&#20026;&#38754;&#37096;&#26631;&#24535;&#28857;&#65292;&#24182;&#24320;&#21457;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#35813;&#26041;&#27861;&#20998;&#32452;&#32452;&#32455;&#27599;&#20010;&#38754;&#37096;&#22120;&#23448;&#30340;&#26631;&#24535;&#28857;&#65292;&#24182;&#23558;&#23427;&#20204;&#36830;&#25509;&#21040;&#30456;&#20851;&#30340;&#34920;&#24773;&#26435;&#37325;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#20854;&#20182;&#26041;&#27861;&#22686;&#24378;&#38754;&#37096;&#34920;&#24773;&#30340;&#34920;&#36798;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video retargeting for digital face animation is used in virtual reality, social media, gaming, movies, and video conference, aiming to animate avatars' facial expressions based on videos of human faces. The standard method to represent facial expressions for 3D characters is by blendshapes, a vector of weights representing the avatar's neutral shape and its variations under facial expressions, e.g., smile, puff, blinking. Datasets of paired frames with blendshape vectors are rare, and labeling can be laborious, time-consuming, and subjective. In this work, we developed an approach that handles the lack of appropriate datasets. Instead, we used a synthetic dataset of only one character. To generalize various characters, we re-represented each frame to face landmarks. We developed a unique deep-learning architecture that groups landmarks for each facial organ and connects them to relevant blendshape weights. Additionally, we incorporated complementary methods for facial expressions that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#20266;&#38543;&#26426;&#25237;&#24433;&#21040;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#22312;&#22810;&#20010;&#26377;&#19981;&#21516;&#20915;&#31574;&#36793;&#30028;&#30340;&#35757;&#32451;&#20998;&#31867;&#22120;&#20013;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#26469;&#27979;&#35797;&#36755;&#20837;&#65292;&#33021;&#22815;&#20013;&#21644;&#23545;&#25239;&#25915;&#20987;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#22120;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12161</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#38543;&#26426;&#21270;&#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks Neutralization via Data Set Randomization. (arXiv:2306.12161v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#20266;&#38543;&#26426;&#25237;&#24433;&#21040;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#22312;&#22810;&#20010;&#26377;&#19981;&#21516;&#20915;&#31574;&#36793;&#30028;&#30340;&#35757;&#32451;&#20998;&#31867;&#22120;&#20013;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#26469;&#27979;&#35797;&#36755;&#20837;&#65292;&#33021;&#22815;&#20013;&#21644;&#23545;&#25239;&#25915;&#20987;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#22120;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#23545;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#26500;&#25104;&#20102;&#20005;&#37325;&#30340;&#23041;&#32961;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#35201;&#20040;&#21482;&#38024;&#23545;&#26576;&#19968;&#29305;&#23450;&#31867;&#22411;&#30340;&#25915;&#20987;&#65292;&#35201;&#20040;&#23481;&#26131;&#21463;&#21040;&#22797;&#26434;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#34429;&#28982;&#30528;&#30524;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#22120;&#65292;&#20294;&#19982;&#24341;&#29992;&#31867;&#21035;&#30456;&#20851;&#30340;&#29305;&#24449;&#26159;&#19968;&#33324;&#30340;&#12290;&#23427;&#26893;&#26681;&#20110;&#36229;&#31354;&#38388;&#25237;&#24433;&#65292;&#25552;&#20379;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#20266;&#38543;&#26426;&#25237;&#24433;&#21040;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21019;&#24314;&#20102;&#21508;&#31181;&#25237;&#24433;&#25968;&#25454;&#38598;&#30340;&#19968;&#22871;&#29983;&#25104;&#22120;&#65292;&#27599;&#20010;&#29983;&#25104;&#22120;&#35757;&#32451;&#19968;&#20010;&#29305;&#23450;&#30340;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#24471;&#21040;&#20855;&#26377;&#19981;&#21516;&#20915;&#31574;&#36793;&#30028;&#30340;&#19981;&#21516;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#36873;&#25321;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#27979;&#35797;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20250;&#25439;&#23475;&#23545;&#20110;&#21512;&#27861;&#36755;&#20837;&#30340;&#20934;&#30830;&#24615;&#12290;&#38500;&#20102;&#35814;&#32454;&#38416;&#36848;&#21644;&#25552;&#20379;&#25105;&#20204;&#30340;&#38450;&#24481;&#26426;&#21046;&#30340;&#20840;&#38754;&#29305;&#24449;&#21270;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22235;&#20010;&#23454;&#39564;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks on deep-learning models pose a serious threat to their reliability and security. Existing defense mechanisms are narrow addressing a specific type of attack or being vulnerable to sophisticated attacks. We propose a new defense mechanism that, while being focused on image-based classifiers, is general with respect to the cited category. It is rooted on hyperspace projection. In particular, our solution provides a pseudo-random projection of the original dataset into a new dataset. The proposed defense mechanism creates a set of diverse projected datasets, where each projected dataset is used to train a specific classifier, resulting in different trained classifiers with different decision boundaries. During testing, it randomly selects a classifier to test the input. Our approach does not sacrifice accuracy over legitimate input. Other than detailing and providing a thorough characterization of our defense mechanism, we also provide a proof of concept of using four 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36895;&#24230;&#26356;&#24555;&#30340;&#26041;&#27861;&#26469;&#23436;&#25104;Segment Anything&#27169;&#22411;&#30340;&#20219;&#21153;&#65292;&#23558;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20998;&#27573;&#29983;&#25104;&#21644;&#25552;&#31034;&#65292;&#20351;&#29992;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#26469;&#35757;&#32451;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#65292;&#20197;&#27492;&#26469;&#36798;&#21040;&#19982;SAM&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12156</link><description>&lt;p&gt;
&#24555;&#36895;&#30340;&#21306;&#20998;&#20219;&#20309;&#19996;&#35199;
&lt;/p&gt;
&lt;p&gt;
Fast Segment Anything. (arXiv:2306.12156v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36895;&#24230;&#26356;&#24555;&#30340;&#26041;&#27861;&#26469;&#23436;&#25104;Segment Anything&#27169;&#22411;&#30340;&#20219;&#21153;&#65292;&#23558;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20998;&#27573;&#29983;&#25104;&#21644;&#25552;&#31034;&#65292;&#20351;&#29992;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#26469;&#35757;&#32451;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#65292;&#20197;&#27492;&#26469;&#36798;&#21040;&#19982;SAM&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#8220;segment anything model&#8221;&#65288;SAM&#65289;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#23427;&#27491;&#22312;&#25104;&#20026;&#35768;&#22810;&#39640;&#32423;&#20219;&#21153;&#30340;&#22522;&#30784;&#27493;&#39588;&#65292;&#22914;&#22270;&#20687;&#20998;&#21106;&#12289;&#22270;&#20687;&#35828;&#26126;&#21644;&#22270;&#20687;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#23427;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#38459;&#30861;&#20102;&#23427;&#22312;&#26356;&#24191;&#27867;&#30340;&#24037;&#19994;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20027;&#35201;&#30340;&#35745;&#31639;&#25104;&#26412;&#26469;&#33258;&#20110;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#30340;Transformer&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#27604;&#36739;&#24615;&#33021;&#30340;&#21152;&#36895;&#26367;&#20195;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20998;&#27573;&#29983;&#25104;&#21644;&#25552;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#23454;&#20363;&#20998;&#21106;&#20998;&#25903;&#30340;&#24120;&#35268;CNN&#26816;&#27979;&#22120;&#20063;&#21487;&#20197;&#24456;&#22909;&#22320;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27492;&#20219;&#21153;&#36716;&#25442;&#20026;&#28145;&#20837;&#30740;&#31350;&#30340;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#65292;&#24182;&#30452;&#25509;&#20351;&#29992;SAM&#20316;&#32773;&#21457;&#24067;&#30340;1/50 SA-1B&#25968;&#25454;&#38598;&#35757;&#32451;&#29616;&#26377;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;50&#20493;&#26356;&#39640;&#30340;&#36895;&#24230;&#19979;&#23454;&#29616;&#20102;&#19982;SAM&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;MRI&#20998;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#19988;CNN&#30340;&#35299;&#37322;&#33021;&#21147;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.12150</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#22240;&#32032;&#30740;&#31350;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#35299;&#37322;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Benchmark data to study the influence of pre-training on explanation performance in MR image classification. (arXiv:2306.12150v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;MRI&#20998;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#19988;CNN&#30340;&#35299;&#37322;&#33021;&#21147;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24120;&#24120;&#22312;&#21307;&#23398;&#39044;&#27979;&#20219;&#21153;&#20013;&#34987;&#25104;&#21151;&#22320;&#24212;&#29992;&#65292;&#36890;&#24120;&#19982;&#36801;&#31227;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#26102;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;CNN&#20135;&#29983;&#30340;&#27169;&#22411;&#39640;&#24230;&#22797;&#26434;&#19988;&#36890;&#24120;&#19981;&#25552;&#20379;&#20219;&#20309;&#26377;&#20851;&#20854;&#39044;&#27979;&#26426;&#21046;&#30340;&#20449;&#24687;&#65292;&#36825;&#20419;&#20351;&#20102;&#8220;&#21487;&#35299;&#37322;&#24615;&#8221;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;MRI&#20998;&#31867;&#20219;&#21153;&#20013;&#23450;&#37327;&#35780;&#20272;&#35299;&#37322;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#20102;&#35299;&#36801;&#31227;&#23398;&#20064;&#23545;&#35299;&#37322;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#20110;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;CNN&#30340;&#27969;&#34892;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;CNN&#25552;&#20379;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#33021;&#21147;&#20005;&#37325;&#20381;&#36182;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) are frequently and successfully used in medical prediction tasks. They are often used in combination with transfer learning, leading to improved performance when training data for the task are scarce. The resulting models are highly complex and typically do not provide any insight into their predictive mechanisms, motivating the field of 'explainable' artificial intelligence (XAI). However, previous studies have rarely quantitatively evaluated the 'explanation performance' of XAI methods against ground-truth data, and transfer learning and its influence on objective measures of explanation performance has not been investigated. Here, we propose a benchmark dataset that allows for quantifying explanation performance in a realistic magnetic resonance imaging (MRI) classification task. We employ this benchmark to understand the influence of transfer learning on the quality of explanations. Experimental results show that popular XAI methods applied to t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24518;&#38459;&#22120;&#30340;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#25512;&#29702;&#21644;&#35757;&#32451;&#26399;&#38388;&#23454;&#29616;&#20803;&#21487;&#22609;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.12142</link><description>&lt;p&gt;
&#22810;&#32423;&#24518;&#38459;&#22120;&#30340;&#31361;&#35302;&#20803;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Synaptic metaplasticity with multi-level memristive devices. (arXiv:2306.12142v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24518;&#38459;&#22120;&#30340;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#25512;&#29702;&#21644;&#35757;&#32451;&#26399;&#38388;&#23454;&#29616;&#20803;&#21487;&#22609;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26377;&#26102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#26366;&#32463;&#35757;&#32451;&#36807;&#30340;&#32593;&#32476;&#20250;&#24536;&#35760;&#21407;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#21487;&#22609;&#24615;&#30340;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#25193;&#23637;&#21040;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24518;&#38459;&#22120;&#30340;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#25512;&#29702;&#21644;&#35757;&#32451;&#26399;&#38388;&#23454;&#29616;&#20803;&#21487;&#22609;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#20214;&#26550;&#26500;&#65292;&#23558;&#32463;&#36807;&#37327;&#21270;&#30340;&#26435;&#37325;&#19982;&#20197;&#27169;&#25311;&#22810;&#32423;&#26041;&#24335;&#32534;&#31243;&#30340;&#24518;&#38459;&#22120;&#22120;&#20214;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#24182;&#19982;&#25968;&#23383;&#22788;&#29702;&#21333;&#20803;&#32467;&#21512;&#20351;&#29992;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20803;&#21487;&#22609;&#23384;&#20648;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#32452;&#21512;&#36719;&#20214;&#26694;&#26550;&#21644;&#22522;&#20110;&#24518;&#38459;&#22120;&#20132;&#21449;&#38453;&#21015;&#30340;&#20869;&#23384;&#35745;&#31639;&#65292;&#22312;130&#32435;&#31859;CMOS&#25216;&#26415;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#20004;&#23618;&#24863;&#30693;&#22120;&#21487;&#20197;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#27491;&#30830;&#22320;&#23398;&#20064;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has made remarkable progress in various tasks, surpassing human performance in some cases. However, one drawback of neural networks is catastrophic forgetting, where a network trained on one task forgets the solution when learning a new one. To address this issue, recent works have proposed solutions based on Binarized Neural Networks (BNNs) incorporating metaplasticity. In this work, we extend this solution to quantized neural networks (QNNs) and present a memristor-based hardware solution for implementing metaplasticity during both inference and training. We propose a hardware architecture that integrates quantized weights in memristor devices programmed in an analog multi-level fashion with a digital processing unit for high-precision metaplastic storage. We validated our approach using a combined software framework and memristor based crossbar array for in-memory computing fabricated in 130 nm CMOS technology. Our experimental results show that a two-layer perceptron 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26408;&#26495;&#32570;&#38519;&#26816;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#12289;ShuffleNetv2&#20027;&#24178;&#32593;&#32476;&#12289;&#24178;&#25200;&#22359;&#21644;&#20301;&#31227;&#22359;&#30340;&#20248;&#21270;&#20197;&#21450;&#20851;&#27880;&#26426;&#21046;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#26816;&#27979;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12113</link><description>&lt;p&gt;
&#34701;&#21512;&#20851;&#27880;&#26426;&#21046;&#21644;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;&#26408;&#26495;&#32570;&#38519;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lightweight wood panel defect detection method incorporating attention mechanism and feature fusion network. (arXiv:2306.12113v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12113
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26408;&#26495;&#32570;&#38519;&#26816;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#12289;ShuffleNetv2&#20027;&#24178;&#32593;&#32476;&#12289;&#24178;&#25200;&#22359;&#21644;&#20301;&#31227;&#22359;&#30340;&#20248;&#21270;&#20197;&#21450;&#20851;&#27880;&#26426;&#21046;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#26816;&#27979;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#26408;&#26495;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#20173;&#23384;&#22312;&#26816;&#27979;&#29575;&#19981;&#39640;&#12289;&#26816;&#27979;&#36895;&#24230;&#24930;&#20197;&#21450;&#38590;&#20197;&#22312;&#26408;&#26495;&#34920;&#38754;&#37096;&#32626;&#23884;&#20837;&#24335;&#35774;&#22791;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;YOLOv5-LW&#30340;&#36731;&#37327;&#32423;&#26408;&#26495;&#32570;&#38519;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#38598;&#25104;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#21452;&#21521;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;MBiFPN&#65289;&#20316;&#20026;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#26469;&#22686;&#24378;&#21487;&#25509;&#21463;&#32570;&#38519;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;ShuffleNetv2&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#20027;&#24178;&#32593;&#32476;&#26469;&#23454;&#29616;&#36731;&#37327;&#21270;&#35774;&#35745;&#65292;&#24341;&#20837;&#24178;&#25200;&#22359;&#21644;&#20301;&#31227;&#22359;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#26368;&#21518;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#25935;&#24863;&#24230;&#24182;&#20943;&#23569;&#38169;&#35823;&#26816;&#27979;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26408;&#26495;&#32570;&#38519;&#26816;&#27979;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#26816;&#27979;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has made significant progress in wood panel defect detection. However, there are still challenges such as low detection , slow detection speed, and difficulties in deploying embedded devices on wood panel surfaces. To overcome these issues, we propose a lightweight wood panel defect detection method called YOLOv5-LW, which incorporates attention mechanisms and a feature fusion network.Firstly, to enhance the detection capability of acceptable defects, we introduce the Multi-scale Bi-directional Feature Pyramid Network (MBiFPN) as a feature fusion network. The MBiFPN reduces feature loss, enriches local and detailed features, and improves the model's detection capability for acceptable defects.Secondly, to achieve a lightweight design, we reconstruct the ShuffleNetv2 network model as the backbone network. This reconstruction reduces the number of parameters and computational requirements while maintaining performance. We also introduce the Stem Block and S
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39640;&#32500;&#32463;&#39564;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#19981;&#21464;&#20998;&#35299;&#21644;&#65288;&#31354;&#38388;-&#65289;&#26102;&#38388;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#33258;&#21160;&#20998;&#31163;&#20986;&#19968;&#31181;&#23454;&#20363;&#29305;&#23450;&#30340;&#32534;&#30721;&#21644;&#19968;&#31181;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#65292;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#22312;&#20219;&#24847;&#36830;&#32493;&#26102;&#38388;&#25512;&#26029;&#31995;&#32479;&#34892;&#20026;&#65292;&#19982;&#26174;&#24335;&#30340;&#31070;&#32463;ODE&#20844;&#24335;&#19981;&#21516;&#65292;&#39640;&#25928;&#21487;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.12077</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21464;&#20998;&#35299;&#21644;&#65288;&#31354;&#38388;-&#65289;&#26102;&#38388;&#21464;&#25442;&#22120;&#23398;&#20064;&#28508;&#22312;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers. (arXiv:2306.12077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39640;&#32500;&#32463;&#39564;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#19981;&#21464;&#20998;&#35299;&#21644;&#65288;&#31354;&#38388;-&#65289;&#26102;&#38388;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#33258;&#21160;&#20998;&#31163;&#20986;&#19968;&#31181;&#23454;&#20363;&#29305;&#23450;&#30340;&#32534;&#30721;&#21644;&#19968;&#31181;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#65292;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#22312;&#20219;&#24847;&#36830;&#32493;&#26102;&#38388;&#25512;&#26029;&#31995;&#32479;&#34892;&#20026;&#65292;&#19982;&#26174;&#24335;&#30340;&#31070;&#32463;ODE&#20844;&#24335;&#19981;&#21516;&#65292;&#39640;&#25928;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#65288;&#31354;&#38388;-&#65289;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#35774;&#35745;&#26088;&#22312;&#24378;&#21046;&#23454;&#26045;&#26576;&#20123;&#31185;&#23398;&#21160;&#26426;&#30340;&#19981;&#21464;&#24615;&#30340;&#26694;&#26550;&#65292;&#20174;&#39640;&#32500;&#32463;&#39564;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36825;&#26679;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#26469;&#33258;&#19968;&#20010;&#31995;&#32479;&#30340;&#22810;&#20010;&#19981;&#21516;&#23454;&#20363;&#30340;&#25968;&#25454;&#21487;&#29992;&#65292;&#20854;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#22312;&#24320;&#22987;&#26102;&#23436;&#20840;&#19981;&#30693;&#36947;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#20998;&#31163;&#65292;&#21363;&#19968;&#31181;&#23454;&#20363;&#29305;&#23450;&#30340;&#32534;&#30721;&#65288;&#25429;&#33719;&#21021;&#22987;&#26465;&#20214;&#12289;&#24120;&#25968;&#31561;&#65289;&#21644;&#19968;&#31181;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26412;&#36523;&#22312;&#31995;&#32479;&#30340;&#25152;&#26377;&#23454;&#20363;/&#23454;&#29616;&#20013;&#37117;&#26159;&#36890;&#29992;&#30340;&#12290;&#36825;&#31181;&#20998;&#31163;&#26159;&#20197;&#33258;&#21160;&#21270;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23454;&#29616;&#30340;&#65292;&#21482;&#38656;&#35201;&#32463;&#39564;&#25968;&#25454;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#22312;&#20219;&#20309;&#36830;&#32493;&#26102;&#38388;&#26377;&#25928;&#22320;&#25512;&#26029;&#31995;&#32479;&#34892;&#20026;&#65292;&#20294;&#19981;&#38656;&#35201;&#26174;&#24335;&#30340;&#31070;&#32463;ODE&#20844;&#24335;&#65292;&#36825;&#20351;&#24471;&#23427;&#39640;&#25928;&#19988;&#39640;&#24230;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30740;&#31350;&#20102;&#36825;&#31181;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for learning dynamical systems from high-dimensional empirical data that combines variational autoencoders and (spatio-)temporal attention within a framework designed to enforce certain scientifically-motivated invariances. We focus on the setting in which data are available from multiple different instances of a system whose underlying dynamical model is entirely unknown at the outset. The approach rests on a separation into an instance-specific encoding (capturing initial conditions, constants etc.) and a latent dynamics model that is itself universal across all instances/realizations of the system. The separation is achieved in an automated, data-driven manner and only empirical data are required as inputs to the model. The approach allows effective inference of system behaviour at any continuous time but does not require an explicit neural ODE formulation, which makes it efficient and highly scalable. We study behaviour through simple theoretical analyses and ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#21367;&#31215;&#31867;&#22411;&#21644;&#26550;&#26500;&#25913;&#36827;&#65292;&#25193;&#23637;&#20102;&#31561;&#21464;Transformer&#21040;&#26356;&#39640;&#30340;&#31561;&#21464;&#34920;&#31034;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#37327;&#21644;&#21147;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#35745;&#31639;&#25928;&#29575;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.12059</link><description>&lt;p&gt;
EquiformerV2: &#25913;&#36827;&#30340;&#31561;&#21464;Transformer&#65292;&#29992;&#20110;&#25193;&#23637;&#21040;&#26356;&#39640;&#27425;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#21367;&#31215;&#31867;&#22411;&#21644;&#26550;&#26500;&#25913;&#36827;&#65292;&#25193;&#23637;&#20102;&#31561;&#21464;Transformer&#21040;&#26356;&#39640;&#30340;&#31561;&#21464;&#34920;&#31034;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#37327;&#21644;&#21147;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#35745;&#31639;&#25928;&#29575;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;Transformer&#65288;&#20363;&#22914;Equiformer&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;3D&#21407;&#23376;&#31995;&#32479;&#39046;&#22495;&#30340;&#21151;&#25928;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#20173;&#28982;&#23616;&#38480;&#20110;&#23567;&#25968;&#27425;&#31561;&#21464;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#26550;&#26500;&#26159;&#21542;&#33021;&#22815;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#26356;&#39640;&#30340;&#27425;&#25968;&#12290;&#20174;Equiformer&#24320;&#22987;&#65292;&#25105;&#20204;&#39318;&#20808;&#29992;eSCN&#21367;&#31215;&#26367;&#25442;&#20102;$SO(3)$&#21367;&#31215;&#65292;&#20197;&#26377;&#25928;&#22320;&#21512;&#24182;&#26356;&#39640;&#27425;&#30340;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#26356;&#39640;&#27425;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26550;&#26500;&#25913;&#36827;&#8212;&#8212;&#27880;&#24847;&#21147;&#37325;&#26631;&#20934;&#21270;&#12289;&#21487;&#20998;&#31163;&#30340;$S^2$&#28608;&#27963;&#21644;&#21487;&#20998;&#31163;&#23618;&#24402;&#19968;&#21270;&#12290;&#23558;&#36825;&#19968;&#20999;&#25918;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#22312;&#22823;&#22411;OC20&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#21147;&#19978;&#25552;&#39640;&#20102;&#26368;&#22810;$12\%$&#65292;&#33021;&#37327;&#19978;&#25552;&#39640;&#20102;$4\%$&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#36895;&#24230;-&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#21560;&#38468;&#33021;&#25152;&#38656;&#30340;DFT&#35745;&#31639;&#37327;&#26041;&#38754;&#32553;&#20943;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements -- attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to $12\%$ on forces, $4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$ reduction in DFT calculations needed for computing adsorption energies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#21738;&#20123;&#26679;&#26412;&#26368;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#25110;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#8220;&#26679;&#26412;&#25915;&#20987;&#33021;&#21147;/&#40065;&#26834;&#24615;&#8221;&#30340;&#23450;&#20041;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20986;&#19968;&#20010;&#30475;&#19981;&#35265;&#30340;&#30446;&#26631;&#27169;&#22411;&#20013;&#26368;&#26131;&#21463;&#25915;&#20987;&#21644;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.12043</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#26679;&#26412;&#25915;&#20987;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sample Attackability in Natural Language Adversarial Attacks. (arXiv:2306.12043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#21738;&#20123;&#26679;&#26412;&#26368;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#25110;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#8220;&#26679;&#26412;&#25915;&#20987;&#33021;&#21147;/&#40065;&#26834;&#24615;&#8221;&#30340;&#23450;&#20041;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20986;&#19968;&#20010;&#30475;&#19981;&#35265;&#30340;&#30446;&#26631;&#27169;&#22411;&#20013;&#26368;&#26131;&#21463;&#25915;&#20987;&#21644;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#23545;&#25239;&#25915;&#20987;&#30740;&#31350;&#24050;&#32463;&#22312;&#35774;&#35745;&#24378;&#22823;&#30340;&#25915;&#20987;&#26041;&#27861;&#21644;&#38450;&#24481;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#21162;&#21147;&#21435;&#30830;&#23450;&#21738;&#20123;&#28304;&#26679;&#26412;&#26159;&#26368;&#26131;&#21463;&#25915;&#20987;&#25110;&#26368;&#20855;&#40065;&#26834;&#24615;&#65292;&#21363;&#22312;&#19968;&#20010;&#30475;&#19981;&#35265;&#30340;&#30446;&#26631;&#27169;&#22411;&#19978;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#25991;&#27491;&#24335;&#25193;&#23637;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25915;&#20987;&#20013;&#26679;&#26412;&#25915;&#20987;&#33021;&#21147;/&#40065;&#26834;&#24615;&#30340;&#23450;&#20041;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#12289;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#22235;&#31181;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#19981;&#33021;&#20805;&#20998;&#25551;&#36848;&#25915;&#20987;&#24615;/&#40065;&#26834;&#24615;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20986;&#19968;&#20010;&#30475;&#19981;&#35265;&#30340;&#30446;&#26631;&#27169;&#22411;&#20013;&#26368;&#26131;&#21463;&#25915;&#20987;&#21644;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25915;&#20987;&#26041;&#27861;&#20013;&#65292;&#23545;&#20110;&#21738;&#20123;&#26679;&#26412;&#34987;&#35748;&#20026;&#26159;&#26368;&#26131;&#21463;&#25915;&#20987;/&#26368;&#20855;&#40065;&#26834;&#24615;&#65292;&#19981;&#23384;&#22312;&#26126;&#26174;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attack research in natural language processing (NLP) has made significant progress in designing powerful attack methods and defence approaches. However, few efforts have sought to identify which source samples are the most attackable or robust, i.e. can we determine for an unseen target model, which samples are the most vulnerable to an adversarial attack. This work formally extends the definition of sample attackability/robustness for NLP attacks. Experiments on two popular NLP datasets, four state of the art models and four different NLP adversarial attack methods, demonstrate that sample uncertainty is insufficient for describing characteristics of attackable/robust samples and hence a deep learning based detector can perform much better at identifying the most attackable and robust samples for an unseen target model. Nevertheless, further analysis finds that there is little agreement in which samples are considered the most attackable/robust across different NLP attack 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20381;&#36182;&#26680;&#38887;&#24615;&#30340;&#26032;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#33410;&#28857;&#22312;&#25554;&#20837;&#21644;&#21024;&#38500;&#36793;&#30340;&#24773;&#20917;&#19979;&#30340;&#26680;&#38887;&#24615;&#65292;&#20197;&#25429;&#25417;&#37051;&#23621;&#33410;&#28857;&#21644;&#26410;&#26469;&#37051;&#23621;&#33410;&#28857;&#23545;&#33410;&#28857;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#30495;&#23454;&#32593;&#32476;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12038</link><description>&lt;p&gt;
&#23450;&#37327;&#33410;&#28857;&#22522;&#26680;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Node-based Core Resilience. (arXiv:2306.12038v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20381;&#36182;&#26680;&#38887;&#24615;&#30340;&#26032;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#33410;&#28857;&#22312;&#25554;&#20837;&#21644;&#21024;&#38500;&#36793;&#30340;&#24773;&#20917;&#19979;&#30340;&#26680;&#38887;&#24615;&#65292;&#20197;&#25429;&#25417;&#37051;&#23621;&#33410;&#28857;&#21644;&#26410;&#26469;&#37051;&#23621;&#33410;&#28857;&#23545;&#33410;&#28857;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#30495;&#23454;&#32593;&#32476;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#20998;&#35299;&#26159;&#21508;&#31181;&#22270;&#20998;&#26512;&#20219;&#21153;&#65288;&#22914;&#23494;&#38598;&#23376;&#22270;&#30340;&#21457;&#29616;&#21644;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#65289;&#30340;&#39640;&#25928;&#26500;&#24314;&#22359;&#12290;&#20854;&#20851;&#38190;&#24369;&#28857;&#26159;&#23545;&#22270;&#30340;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65306;&#25554;&#20837;&#25110;&#21024;&#38500;&#20960;&#26465;&#36793;&#23601;&#21487;&#33021;&#26497;&#22823;&#22320;&#25913;&#21464;&#22270;&#30340;&#26680;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#22312;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#19978;&#34920;&#24449;&#12289;&#37327;&#21270;&#21644;&#65292;&#22914;&#26524;&#21487;&#33021;&#30340;&#35805;&#65292;&#25552;&#39640;&#32473;&#23450;&#22270;&#30340;&#26680;&#32467;&#26500;&#38887;&#24615;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#32771;&#34385;&#20102;&#25972;&#20010;&#22270;&#25110;&#20854;&#20013;&#37325;&#35201;&#23376;&#22270;&#30340;&#26680;&#38887;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#33410;&#28857;&#30340;&#26680;&#38887;&#24615;&#65292;&#36890;&#36807;&#25554;&#20837;&#21644;&#21024;&#38500;&#36793;&#26469;&#24230;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#20197;&#21069;&#25552;&#20986;&#30340;&#24230;&#37327;&#25351;&#26631;Core Strength&#19981;&#33021;&#27491;&#30830;&#22320;&#25429;&#25417;&#21040;&#33410;&#28857;&#22312;&#21024;&#38500;&#36793;&#30340;&#24773;&#20917;&#19979;&#30340;&#26680;&#38887;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20381;&#36182;&#22270;&#30340;&#27010;&#24565;&#65292;&#20197;&#25429;&#25417;&#37051;&#23621;&#33410;&#28857;&#65288;&#23545;&#20110;&#36793;&#21024;&#38500;&#65289;&#21644;&#21487;&#33021;&#30340;&#26410;&#26469;&#37051;&#23621;&#33410;&#28857;&#65288;&#23545;&#20110;&#36793;&#25554;&#20837;&#65289;&#23545;&#33410;&#28857;&#26680;&#38887;&#24615;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#20381;&#36182;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33410;&#28857;&#30340;&#38887;&#24615;&#24230;&#37327;&#65292;&#31216;&#20026;&#20381;&#36182;&#26680;&#38887;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#30495;&#23454;&#32593;&#32476;&#19978;&#35780;&#20272;&#20102;&#26032;&#24230;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#25429;&#25417;&#33410;&#28857;&#30495;&#27491;&#26680;&#38887;&#24615;&#26041;&#38754;&#20248;&#20110;Core Strength&#12290;
&lt;/p&gt;
&lt;p&gt;
Core decomposition is an efficient building block for various graph analysis tasks such as dense subgraph discovery and identifying influential nodes. One crucial weakness of the core decomposition is its sensitivity to changes in the graph: inserting or removing a few edges can drastically change the core structure of a graph. Hence, it is essential to characterize, quantify, and, if possible, improve the resilience of the core structure of a given graph in global and local levels. Previous works mostly considered the core resilience of the entire graph or important subgraphs in it. In this work, we study node-based core resilience measures upon edge removals and insertions. We first show that a previously proposed measure, Core Strength, does not correctly capture the core resilience of a node upon edge removals. Next, we introduce the concept of dependency graph to capture the impact of neighbor nodes (for edge removal) and probable future neighbor nodes (for edge insertion) on the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#22238;&#24402;&#30340;&#20381;&#23384;&#35299;&#26512;&#22120;&#65292;&#25104;&#21151;&#25429;&#33719;&#20102;&#33410;&#28857;&#21644;&#36793;&#32536;&#20043;&#38388;&#30340;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12018</link><description>&lt;p&gt;
&#19968;&#31181;&#21322;&#33258;&#22238;&#24402;&#22270;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20381;&#23384;&#20851;&#31995;&#22270;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Semi-Autoregressive Graph Generative Model for Dependency Graph Parsing. (arXiv:2306.12018v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12018
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#22238;&#24402;&#30340;&#20381;&#23384;&#35299;&#26512;&#22120;&#65292;&#25104;&#21151;&#25429;&#33719;&#20102;&#33410;&#28857;&#21644;&#36793;&#32536;&#20043;&#38388;&#30340;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#20381;&#23384;&#35299;&#26512;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#26681;&#25454;&#23545;&#22270;&#32852;&#21512;&#27010;&#29575;&#30340;&#19981;&#21516;&#20998;&#35299;&#26041;&#27861;&#65292;&#29616;&#26377;&#35299;&#26512;&#22120;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#26377;&#21521;&#36793;&#35270;&#20026;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20381;&#23384;&#22270;&#20013;&#23384;&#22312;&#29420;&#31435;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#32452;&#20214;&#28151;&#21512;&#65292;&#26631;&#24535;&#30528;&#20808;&#21069;&#30340;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#33410;&#28857;&#21644;&#36793;&#32536;&#20043;&#38388;&#30340;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#19968;&#29305;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21322;&#33258;&#22238;&#24402;&#30340;&#20381;&#23384;&#35299;&#26512;&#22120;&#65292;&#36890;&#36807;&#21152;&#20837;&#33410;&#28857;&#32452;&#21644;&#36793;&#32452;&#33258;&#22238;&#24402;&#22320;&#24182;&#23558;&#33410;&#28857;&#23884;&#20837;&#19968;&#27425;&#24615;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#20381;&#23384;&#22270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#25429;&#33719;&#20102;&#33410;&#28857;&#21644;&#36793;&#32536;&#20043;&#38388;&#30340;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the impressive progress in Neural Dependency Parsing. According to the different factorization approaches to the graph joint probabilities, existing parsers can be roughly divided into autoregressive and non-autoregressive patterns. The former means that the graph should be factorized into multiple sequentially dependent components, then it can be built up component by component. And the latter assumes these components to be independent so that they can be outputted in a one-shot manner. However, when treating the directed edge as an explicit dependency relationship, we discover that there is a mixture of independent and interdependent components in the dependency graph, signifying that both aforementioned models fail to precisely capture the explicit dependencies among nodes and edges. Based on this property, we design a Semi-Autoregressive Dependency Parser to generate dependency graphs via adding node groups and edge groups autoregressively while pouring 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#39062;&#25991;&#26412;&#26377;&#26465;&#20214;Tau PET&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#21487;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#21644;&#34987;&#35797;&#30340;MR&#22270;&#20687;&#29983;&#25104;&#36924;&#30495;&#30340;Tau PET&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;Tau PET&#25968;&#25454;&#38598;&#30340;&#20844;&#20849;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11984</link><description>&lt;p&gt;
TauPETGen&#65306;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#26377;&#26465;&#20214;Tau PET&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
TauPETGen: Text-Conditional Tau PET Image Synthesis Based on Latent Diffusion Models. (arXiv:2306.11984v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#39062;&#25991;&#26412;&#26377;&#26465;&#20214;Tau PET&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#21487;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#21644;&#34987;&#35797;&#30340;MR&#22270;&#20687;&#29983;&#25104;&#36924;&#30495;&#30340;Tau PET&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;Tau PET&#25968;&#25454;&#38598;&#30340;&#20844;&#20849;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#21512;&#25104;&#25216;&#26415;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#21644;&#34987;&#35797;&#30340;MR&#22270;&#20687;&#29983;&#25104;&#36924;&#30495;&#30340;Tau PET&#22270;&#20687;&#12290;&#29983;&#25104;&#30340;Tau PET&#22270;&#20687;&#26377;&#21161;&#20110;&#30740;&#31350;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;Tau PET&#25968;&#25454;&#38598;&#30340;&#20844;&#20849;&#21487;&#29992;&#24615;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#21644;&#34987;&#35797;&#30340;MR&#22270;&#20687;&#20026;&#26465;&#20214;&#36827;&#34892;&#22270;&#20687;&#29983;&#25104;&#12290;&#34987;&#35797;&#30340;MR&#22270;&#20687;&#21487;&#20197;&#25552;&#20379;&#35299;&#21078;&#32454;&#33410;&#65292;&#32780;&#25991;&#26412;&#25551;&#36848;&#65288;&#22914;&#24615;&#21035;&#12289;&#25195;&#25551;&#26102;&#38388;&#12289;&#35748;&#30693;&#27979;&#35797;&#20998;&#25968;&#21644;&#28096;&#31881;&#26679;&#34507;&#30333;&#29366;&#24577;&#65289;&#21017;&#21487;&#25552;&#20379;&#26377;&#20851;Tau&#31070;&#32463;&#32420;&#32500;&#32544;&#32467;&#21487;&#33021;&#27785;&#31215;&#30340;&#20301;&#32622;&#30340;&#36827;&#19968;&#27493;&#25351;&#23548;&#12290;&#22522;&#20110;&#20020;&#24202;[18F]MK-6240&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#19981;&#21516;&#20020;&#24202;&#38454;&#27573;&#30340;&#36924;&#30495;Tau PET&#22270;&#20687;&#26041;&#38754;&#20855;&#26377;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we developed a novel text-guided image synthesis technique which could generate realistic tau PET images from textual descriptions and the subject's MR image. The generated tau PET images have the potential to be used in examining relations between different measures and also increasing the public availability of tau PET datasets. The method was based on latent diffusion models. Both textual descriptions and the subject's MR prior image were utilized as conditions during image generation. The subject's MR image can provide anatomical details, while the text descriptions, such as gender, scan time, cognitive test scores, and amyloid status, can provide further guidance regarding where the tau neurofibrillary tangles might be deposited. Preliminary experimental results based on clinical [18F]MK-6240 datasets demonstrate the feasibility of the proposed method in generating realistic tau PET images at different clinical stages.
&lt;/p&gt;</description></item><item><title>AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11971</link><description>&lt;p&gt;
AdCraft&#65306;&#19968;&#31181;&#29992;&#20110;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20248;&#21270;&#30340;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11971
&lt;/p&gt;
&lt;p&gt;
AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#8212;&#8212; AdCraft&#65292;&#20854;&#20855;&#26377;&#38543;&#26426;&#21644;&#38750;&#38745;&#24577;&#29305;&#24615;&#12290;&#35813;&#29615;&#22659;&#27169;&#25311;&#20102;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20013;&#20986;&#20215;&#21644;&#39044;&#31639;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;SEM&#26159;&#19968;&#31181;&#21033;&#29992;&#20184;&#36153;&#24191;&#21578;&#26469;&#22686;&#21152;&#32593;&#31449;&#22312;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#19978;&#30340;&#21487;&#35265;&#24615;&#30340;&#25968;&#23383;&#33829;&#38144;&#25216;&#26415;&#12290;SEM&#24191;&#21578;&#27963;&#21160;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#65292;&#21253;&#25324;&#20851;&#38190;&#23383;&#36873;&#25321;&#12289;&#24191;&#21578;&#35774;&#35745;&#12289;&#20986;&#20215;&#31649;&#29702;&#12289;&#39044;&#31639;&#35843;&#25972;&#21644;&#34920;&#29616;&#30417;&#25511;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20248;&#21270;SEM&#24191;&#21578;&#25237;&#25918;&#27963;&#21160;&#30340;&#28508;&#22312;&#31574;&#30053;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#25110;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#21487;&#23450;&#21046;&#29615;&#22659;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#35780;&#20272;&#21644;&#25552;&#39640;&#19982;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20184;&#20986;&#36825;&#20123;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;AdCraft&#29615;&#22659;&#19979;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce \env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#26377;&#22122;&#38899;&#35745;&#31639;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#38750;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22411;&#19979;&#25152;&#26377;&#22235;&#20010;&#20989;&#25968;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#22823;&#22810;&#25968;&#19979;&#30028;&#19982;&#19978;&#30028;&#31526;&#21512;&#65292;&#24046;&#24322;&#19981;&#36229;&#36807;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2306.11951</link><description>&lt;p&gt;
&#35770;&#22914;&#20309;&#22788;&#29702;&#26377;&#22122;&#38899;&#30340;&#35745;&#31639;&#38382;&#39064;&#30340;&#26368;&#20248;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
On the Optimal Bounds for Noisy Computing. (arXiv:2306.11951v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#26377;&#22122;&#38899;&#35745;&#31639;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#38750;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22411;&#19979;&#25152;&#26377;&#22235;&#20010;&#20989;&#25968;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#22823;&#22810;&#25968;&#19979;&#30028;&#19982;&#19978;&#30028;&#31526;&#21512;&#65292;&#24046;&#24322;&#19981;&#36229;&#36807;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;Feige&#31561;&#20154;1994&#24180;&#32771;&#34385;&#30340;&#20351;&#29992;&#26377;&#22122;&#22768;&#30340;&#20449;&#24687;&#36827;&#34892;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;&#26377;&#22122;&#22768;&#30340;&#26597;&#35810;&#20013;&#35745;&#31639;&#24191;&#20041; OR &#20989;&#25968;&#65292;&#20197;&#21450;&#20174;&#26377;&#22122;&#22768;&#30340;&#25104;&#23545;&#27604;&#36739;&#20013;&#35745;&#31639; MAX&#12289;SEARCH &#21644; SORT &#20989;&#25968;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340; $K$ &#20010;&#20803;&#32032;&#65292;&#30446;&#26631;&#26159;&#22312;&#27599;&#20010;&#26597;&#35810;&#30340;&#32467;&#26524;&#20197;&#27010;&#29575; $p$ &#32763;&#36716;&#26102;&#65292;&#20197;&#33267;&#23569; $1-\delta$ &#30340;&#27010;&#29575;&#27491;&#30830;&#22320;&#24674;&#22797;&#25152;&#38656;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#33258;&#36866;&#24212;&#37319;&#26679;&#35774;&#32622;&#21644;&#38750;&#33258;&#36866;&#24212;&#37319;&#26679;&#35774;&#32622;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#22351;&#24773;&#20917;&#19979;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#32039;&#23494;&#36793;&#30028;&#65292;&#36825;&#20123;&#36793;&#30028;&#26159;&#20197; $K$ &#20381;&#36182;&#24615;&#20026;&#22522;&#30784;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#20381;&#36182;&#20110; $\delta$ &#21644; $p$ &#26041;&#38754;&#65292;&#19978;&#19979;&#30028;&#24182;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#26597;&#35810;&#27169;&#22411;&#19979;&#25152;&#26377;&#22235;&#20010;&#20989;&#25968;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#22823;&#22810;&#25968;&#19979;&#30028;&#19982;&#19978;&#30028;&#31526;&#21512;&#65292;&#24046;&#24322;&#19981;&#36229;&#36807;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the problem of computing with noisy information considered in Feige et al. 1994, which includes computing the OR function from noisy queries, and computing the MAX, SEARCH and SORT functions from noisy pairwise comparisons. For $K$ given elements, the goal is to correctly recover the desired function with probability at least $1-\delta$ when the outcome of each query is flipped with probability $p$. We consider both the adaptive sampling setting where each query can be adaptively designed based on past outcomes, and the non-adaptive sampling setting where the query cannot depend on past outcomes. The prior work provides tight bounds on the worst-case query complexity in terms of the dependence on $K$. However, the upper and lower bounds do not match in terms of the dependence on $\delta$ and $p$. We improve the lower bounds for all the four functions under both adaptive and non-adaptive query models. Most of our lower bounds match the upper bounds up to constant factors when
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#38598;&#25104; Q &#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35823;&#24046;&#21453;&#39304;&#26469;&#20943;&#23567;&#38598;&#25104;&#26041;&#27861;&#20013;&#20272;&#35745;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#32467;&#21512;&#27169;&#22411;&#35782;&#21035;&#33258;&#36866;&#24212;&#25511;&#21046;&#65288;MIAC&#65289;&#26469;&#23454;&#29616;&#38598;&#25104;&#22823;&#23567;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.11918</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38598;&#25104; Q &#23398;&#20064;&#65306;&#36890;&#36807;&#35823;&#24046;&#21453;&#39304;&#26469;&#20943;&#23567;&#20272;&#35745;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback. (arXiv:2306.11918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#38598;&#25104; Q &#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35823;&#24046;&#21453;&#39304;&#26469;&#20943;&#23567;&#38598;&#25104;&#26041;&#27861;&#20013;&#20272;&#35745;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#32467;&#21512;&#27169;&#22411;&#35782;&#21035;&#33258;&#36866;&#24212;&#25511;&#21046;&#65288;MIAC&#65289;&#26469;&#23454;&#29616;&#38598;&#25104;&#22823;&#23567;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#26159;&#32531;&#35299; Q-learning &#20013;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#22810;&#20010;&#20989;&#25968;&#36924;&#36817;&#22120;&#26469;&#20272;&#35745;&#21160;&#20316;&#20540;&#12290;&#20272;&#35745;&#20559;&#35823;&#20005;&#37325;&#20381;&#36182;&#20110;&#38598;&#25104;&#22823;&#23567;&#65288;&#21363;&#30446;&#26631;&#20013;&#20351;&#29992;&#30340; Q &#20989;&#25968;&#36924;&#36817;&#22120;&#25968;&#37327;&#65289;&#65292;&#22240;&#27492;&#20915;&#23450;&#8220;&#27491;&#30830;&#8221;&#30340;&#38598;&#25104;&#22823;&#23567;&#38750;&#24120;&#22256;&#38590;&#65292;&#22240;&#20026;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#20272;&#35745;&#20559;&#24046;&#23548;&#20986;&#20102;&#19978;&#38480;&#21644;&#19979;&#38480;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#30028;&#23545;&#38598;&#25104;&#22823;&#23567;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#20174;&#32780;&#23558;&#20559;&#24046;&#39537;&#21160;&#21040;&#25509;&#36817;&#38646;&#65292;&#22240;&#27492;&#21487;&#20197;&#30456;&#24212;&#22320;&#22788;&#29702;&#26102;&#38388;&#21464;&#21270;&#36924;&#36817;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#38598;&#25104;&#26041;&#27861;&#19982;&#27169;&#22411;&#35782;&#21035;&#33258;&#36866;&#24212;&#25511;&#21046;&#65288;MIAC&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#38598;&#25104; Q &#23398;&#20064;&#65288;AdaEQ&#65289;&#65292;&#26159;&#19968;&#31181;&#24191;&#20041;&#30340; Q &#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#33258;&#36866;&#24212;&#38598;&#25104;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ensemble method is a promising way to mitigate the overestimation issue in Q-learning, where multiple function approximators are used to estimate the action values. It is known that the estimation bias hinges heavily on the ensemble size (i.e., the number of Q-function approximators used in the target), and that determining the `right' ensemble size is highly nontrivial, because of the time-varying nature of the function approximation errors during the learning process. To tackle this challenge, we first derive an upper bound and a lower bound on the estimation bias, based on which the ensemble size is adapted to drive the bias to be nearly zero, thereby coping with the impact of the time-varying approximation errors accordingly. Motivated by the theoretic findings, we advocate that the ensemble method can be combined with Model Identification Adaptive Control (MIAC) for effective ensemble size adaptation. Specifically, we devise Adaptive Ensemble Q-learning (AdaEQ), a generalized 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#29983;&#25104;&#23545;&#24212;&#30340;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#39046;&#20808;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#40065;&#26834;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11915</link><description>&lt;p&gt;
&#22270;&#20998;&#31867;&#38382;&#39064;&#20013;&#32467;&#26500;&#24863;&#30693;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Structure-Aware Robustness Certificates for Graph Classification. (arXiv:2306.11915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11915
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#29983;&#25104;&#23545;&#24212;&#30340;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#39046;&#20808;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#40065;&#26834;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20445;&#35777;&#23433;&#20840;&#24615;&#30340;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#29992;&#20110;&#22270;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35777;&#26126;&#20445;&#35777;&#19982;&#33410;&#28857;&#23545;&#32763;&#36716;&#65288;&#28155;&#21152;&#25110;&#21024;&#38500;&#36793;&#32536;&#65289;&#30340;&#24635;&#25968;&#26377;&#20851;&#65292;&#36825;&#30456;&#24403;&#20110;&#20197;&#37051;&#25509;&#30697;&#38453;&#20026;&#20013;&#24515;&#30340;l0&#29699;&#12290;&#23613;&#31649;&#20174;&#29702;&#35770;&#19978;&#30475;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#36825;&#31181;&#21508;&#21521;&#21516;&#24615;&#30340;&#32467;&#26500;&#22122;&#22768;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#33021;&#36807;&#20110;&#20005;&#26684;&#65292;&#22240;&#20026;&#26377;&#20123;&#33410;&#28857;&#23545;&#20110;&#30830;&#23450;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#26356;&#20026;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35777;&#20070;&#32473;&#20986;&#20102;&#23545;&#22270;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24754;&#35266;&#25551;&#36848;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#23558;&#38750;&#21508;&#21521;&#21516;&#24615;&#30340;&#22122;&#22768;&#20998;&#24067;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36807;&#31243;&#20026;&#20998;&#31867;&#22120;&#29983;&#25104;&#20102;&#32467;&#26500;&#24863;&#30693;&#30340;&#35777;&#20070;&#65292;&#22240;&#27492;&#40065;&#26834;&#24615;&#35777;&#20070;&#30340;&#22823;&#23567;&#21487;&#20197;&#22312;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#20043;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certifying the robustness of a graph-based machine learning model poses a critical challenge for safety. Current robustness certificates for graph classifiers guarantee output invariance with respect to the total number of node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$ ball centred on the adjacency matrix. Although theoretically attractive, this type of isotropic structural noise can be too restrictive in practical scenarios where some node pairs are more critical than others in determining the classifier's output. The certificate, in this case, gives a pessimistic depiction of the robustness of the graph model. To tackle this issue, we develop a randomised smoothing method based on adding an anisotropic noise distribution to the input graph structure. We show that our process generates structural-aware certificates for our classifiers, whereby the magnitude of robustness certificates can vary across different pre-defined structures of the graph. We demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;ARDR&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#20197;&#23558;PCA&#23884;&#20837;&#23436;&#20840;&#24674;&#22797;&#65292;&#36890;&#36807;&#31245;&#21152;&#20462;&#25913;&#21487;&#20197;&#29992;LLE&#22797;&#29616;ARDR&#23884;&#20837;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#25104;&#31435;&#65292;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.11898</link><description>&lt;p&gt;
&#26080;&#27861;&#35299;&#37322;&#30340;&#35299;&#37322;&#65306;&#35299;&#37322;tSNE&#21644;UMAP&#23884;&#20837;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unexplainable Explanations: Towards Interpreting tSNE and UMAP Embeddings. (arXiv:2306.11898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;ARDR&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#20197;&#23558;PCA&#23884;&#20837;&#23436;&#20840;&#24674;&#22797;&#65292;&#36890;&#36807;&#31245;&#21152;&#20462;&#25913;&#21487;&#20197;&#29992;LLE&#22797;&#29616;ARDR&#23884;&#20837;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#25104;&#31435;&#65292;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#35299;&#37322;&#20026;&#21560;&#24341;/&#25490;&#26021;&#38477;&#32500;&#65288;ARDR&#65289;&#26041;&#27861;&#65288;&#22914;tSNE&#21644;UMAP&#65289;&#24050;&#25104;&#20026;&#26631;&#20934;&#12290;&#36825;&#21462;&#20915;&#20110;2D&#34920;&#31034;&#20013;&#30340;&#32467;&#26500;&#19982;&#27169;&#22411;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#19968;&#33268;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#26410;&#32463;&#35777;&#26126;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;ARDR&#31639;&#27861;&#26159;&#21542;&#26377;&#20219;&#20309;&#25910;&#25947;&#20445;&#35777;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#21560;&#24341;&#21644;&#25490;&#26021;&#26469;&#23436;&#20840;&#24674;&#22797;PCA&#23884;&#20837;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#26524;&#31245;&#21152;&#20462;&#25913;&#65292;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65288;LLE&#65289;&#21487;&#20197;&#22797;&#29616;ARDR&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#36825;&#20123;&#29468;&#24819;&#25104;&#31435;&#65292;&#23601;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has become standard to explain neural network latent spaces with attraction/repulsion dimensionality reduction (ARDR) methods like tSNE and UMAP. This relies on the premise that structure in the 2D representation is consistent with the structure in the model's latent space. However, this is an unproven assumption -- we are unaware of any convergence guarantees for ARDR algorithms. We work on closing this question by relating ARDR methods to classical dimensionality reduction techniques. Specifically, we show that one can fully recover a PCA embedding by applying attractions and repulsions onto a randomly initialized dataset. We also show that, with a small change, Locally Linear Embeddings (LLE) can reproduce ARDR embeddings. Finally, we formalize a series of conjectures that, if true, would allow one to attribute structure in the 2D embedding back to the input distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#26045;&#24037;&#26426;&#22120;&#20154;&#20219;&#21153;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20856;&#22411;&#26045;&#24037;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.11897</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#28082;&#21387;&#24314;&#31569;&#26426;&#22120;&#20154;&#36828;&#31243;&#25805;&#20316;&#34394;&#25311;&#35013;&#32622;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-based Virtual Fixtures for Teleoperation of Hydraulic Construction Machine. (arXiv:2306.11897v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#26045;&#24037;&#26426;&#22120;&#20154;&#20219;&#21153;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20856;&#22411;&#26045;&#24037;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#25805;&#32437;&#26045;&#24037;&#35774;&#22791;&#26159;&#24314;&#31569;&#34892;&#19994;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#33021;&#22815;&#23454;&#29616;&#36828;&#36317;&#31163;&#23433;&#20840;&#25805;&#32437;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25163;&#26564;&#36827;&#34892;&#20851;&#33410;&#32423;&#21035;&#30340;&#36828;&#31243;&#25805;&#20316;&#38656;&#35201;&#32463;&#36807;&#38271;&#26102;&#38388;&#30340;&#22521;&#35757;&#25165;&#33021;&#29087;&#32451;&#25484;&#25569;&#65292;&#21516;&#26102;&#65292;&#30001;&#20110;&#26426;&#22120;&#30340;&#22810;&#33258;&#30001;&#24230;&#29305;&#24615;&#65292;&#26426;&#22120;&#30340;&#36816;&#21160;&#21482;&#33021;&#22312;&#25191;&#34892;&#21518;&#36827;&#34892;&#39564;&#35777;&#65292;&#20351;&#24471;&#20248;&#21270;&#25511;&#21046;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#21153;&#24615;&#33021;&#12290;&#36890;&#36807;&#23398;&#20064;&#33719;&#24471;&#30340;&#25511;&#21046;&#31574;&#30053;&#29992;&#20110;&#25552;&#20379;&#20851;&#33410;&#30340;&#39640;&#25928;&#25511;&#21046;&#21644;&#21327;&#35843;&#30340;&#20219;&#21153;&#25351;&#20196;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;Brokk 170&#26045;&#24037;&#26426;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20854;&#22312;&#20856;&#22411;&#26045;&#24037;&#20219;&#21153;&#65288;&#23558;&#20991;&#23376;&#25554;&#20837;&#38075;&#23380;&#20013;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of teleoperation is a crucial aspect of the construction industry, as it enables operators to control machines safely from a distance. However, remote operation of these machines at a joint level using individual joysticks necessitates extensive training for operators to achieve proficiency due to their multiple degrees of freedom. Additionally, verifying the machine resulting motion is only possible after execution, making optimal control challenging. In addressing this issue, this study proposes a reinforcement learning-based approach to optimize task performance. The control policy acquired through learning is used to provide instructions on efficiently controlling and coordinating multiple joints. To evaluate the effectiveness of the proposed framework, a user study is conducted with a Brokk 170 construction machine by assessing its performance in a typical construction task involving inserting a chisel into a borehole. The effectiveness of the proposed framework is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#24178;&#39044;&#39118;&#26684;&#36716;&#31227;&#65288;IST&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#24178;&#39044;&#35757;&#32451;&#20998;&#24067;&#65292;&#20174;&#32780;&#26174;&#30528;&#25913;&#21892;&#20102;&#21333;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#22806;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11890</link><description>&lt;p&gt;
&#21333;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#65292;&#24178;&#39044;&#39118;&#26684;&#36716;&#31227;&#23454;&#29616;&#22806;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy. (arXiv:2306.11890v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#24178;&#39044;&#39118;&#26684;&#36716;&#31227;&#65288;IST&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#24178;&#39044;&#35757;&#32451;&#20998;&#24067;&#65292;&#20174;&#32780;&#26174;&#30528;&#25913;&#21892;&#20102;&#21333;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#22806;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#65292;&#21253;&#25324;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#21457;&#29616;&#36807;&#31243;&#65292;&#38656;&#35201;&#23545;&#19978;&#19979;&#25991;&#24178;&#25200;&#20855;&#26377;&#19981;&#21464;&#24615;&#21644;&#23545;&#26032;&#25968;&#25454;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#22240;&#26524;&#34920;&#24449;&#12290;&#21033;&#29992;&#20004;&#20010;&#26032;&#30340;&#21333;&#32454;&#32990;&#33639;&#20809;&#26174;&#24494;&#38236;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#22797;&#21046;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33324;&#36866;&#29992;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#20855;&#25361;&#25112;&#24615;&#30340;OOD&#27867;&#21270;&#27700;&#24179;&#19978;&#23398;&#20064;&#22240;&#26524;&#34920;&#24449;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#26681;&#25454;&#20854;&#20182;&#24050;&#24314;&#31435;&#30340;&#24230;&#37327;&#34913;&#37327;&#65292;&#26082;&#26377;&#30340;&#22825;&#30495;&#22522;&#32447;&#35774;&#35745;&#29992;&#20110;&#38450;&#27490;&#28151;&#28102;&#65292;&#20294;&#36825;&#20123;&#22522;&#32447;&#22312;&#36825;&#20123;&#27979;&#35797;&#20013;&#37117;&#20250;&#22833;&#25928;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24178;&#39044;&#39118;&#26684;&#36716;&#31227;&#65288;IST&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#24178;&#39044;&#35757;&#32451;&#20998;&#24067;&#65292;&#22312;&#20854;&#20013;&#20943;&#36731;&#29983;&#29289;&#21407;&#22240;&#21644;&#24178;&#25200;&#22240;&#32032;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26174;&#30528;&#25913;&#21892;&#20102;OOD&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world deployment of computer vision systems, including in the discovery processes of biomedical research, requires causal representations that are invariant to contextual nuisances and generalize to new data. Leveraging the internal replicate structure of two novel single-cell fluorescent microscopy datasets, we propose generally applicable tests to assess the extent to which models learn causal representations across increasingly challenging levels of OOD-generalization. We show that despite seemingly strong performance, as assessed by other established metrics, both naive and contemporary baselines designed to ward against confounding, collapse on these tests. We introduce a new method, Interventional Style Transfer (IST), that substantially improves OOD generalization by generating interventional training distributions in which spurious correlations between biological causes and nuisances are mitigated. We publish our code and datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#26469;&#36827;&#34892;&#22870;&#21169;&#22609;&#24418;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#25506;&#32034; - &#21033;&#29992;&#26435;&#34913;&#30340;&#20248;&#38597;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#38416;&#26126;&#20102;&#20449;&#24687;&#29109;&#12289;&#38543;&#26426;&#31995;&#32479;&#21160;&#21147;&#23398;&#20197;&#21450;&#23427;&#20204;&#23545;&#29109;&#20135;&#29983;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.11885</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#22870;&#21169;&#22609;&#24418;
&lt;/p&gt;
&lt;p&gt;
Reward Shaping via Diffusion Process in Reinforcement Learning. (arXiv:2306.11885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#26469;&#36827;&#34892;&#22870;&#21169;&#22609;&#24418;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#25506;&#32034; - &#21033;&#29992;&#26435;&#34913;&#30340;&#20248;&#38597;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#38416;&#26126;&#20102;&#20449;&#24687;&#29109;&#12289;&#38543;&#26426;&#31995;&#32479;&#21160;&#21147;&#23398;&#20197;&#21450;&#23427;&#20204;&#23545;&#29109;&#20135;&#29983;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#19981;&#26029;&#21457;&#23637;&#65292;&#20197;&#22312;&#19981;&#30830;&#23450;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#38543;&#26426;&#28909;&#21147;&#23398;&#21644;&#31995;&#32479;&#21160;&#21147;&#23398;&#21407;&#29702;&#65292;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#25506;&#32034;&#22870;&#21169;&#22609;&#24418;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#20248;&#38597;&#30340;&#26694;&#26550;&#26469;&#24605;&#32771;&#25506;&#32034; - &#21033;&#29992;&#26435;&#34913;&#12290;&#26412;&#25991;&#38416;&#26126;&#20102;&#20449;&#24687;&#29109;&#65292;&#38543;&#26426;&#31995;&#32479;&#21160;&#21147;&#23398;&#21450;&#20854;&#23545;&#29109;&#20135;&#29983;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#25506;&#32034;&#20351;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#21452;&#37325;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#27966;&#29983;&#26377;&#25928;&#31574;&#30053;&#30340;&#26368;&#22823;&#29109;&#31243;&#24207;&#65292;&#25110;&#32773;&#26159;&#35745;&#31639;&#20449;&#24687;&#25104;&#26412;&#21644;&#25910;&#30410;&#30340;&#20462;&#27491;&#25104;&#26412;&#20248;&#21270;&#31243;&#24207;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20449;&#24687;&#30340;&#29289;&#29702;&#26412;&#36136;&#21450;&#20854;&#23545;MDP&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#26032;&#35270;&#35282;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;RL&#20013;&#30340;&#20449;&#24687;&#21462;&#21521;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) models have continually evolved to navigate the exploration - exploitation trade-off in uncertain Markov Decision Processes (MDPs). In this study, I leverage the principles of stochastic thermodynamics and system dynamics to explore reward shaping via diffusion processes. This provides an elegant framework as a way to think about exploration-exploitation trade-off. This article sheds light on relationships between information entropy, stochastic system dynamics, and their influences on entropy production. This exploration allows us to construct a dual-pronged framework that can be interpreted as either a maximum entropy program for deriving efficient policies or a modified cost optimization program accounting for informational costs and benefits. This work presents a novel perspective on the physical nature of information and its implications for online learning in MDPs, consequently providing a better understanding of information-oriented formulations in RL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31215;&#26497;&#20154;&#26426;&#21327;&#20316;&#30340;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#24847;&#22270;&#39044;&#27979;&#21644;&#23433;&#20840;&#25511;&#21046;&#26469;&#25552;&#39640;&#21327;&#20316;&#25928;&#29575;&#21644;&#36991;&#20813;&#20132;&#20114;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.11862</link><description>&lt;p&gt;
&#31215;&#26497;&#21327;&#20316;&#30340;&#20154;&#26426;&#35013;&#37197;&#65306;&#21033;&#29992;&#20154;&#31867;&#24847;&#22270;&#39044;&#27979;&#21644;&#40065;&#26834;&#23433;&#20840;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Proactive Human-Robot Co-Assembly: Leveraging Human Intention Prediction and Robust Safe Control. (arXiv:2306.11862v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31215;&#26497;&#20154;&#26426;&#21327;&#20316;&#30340;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#24847;&#22270;&#39044;&#27979;&#21644;&#23433;&#20840;&#25511;&#21046;&#26469;&#25552;&#39640;&#21327;&#20316;&#25928;&#29575;&#21644;&#36991;&#20813;&#20132;&#20114;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#21327;&#20316;&#26159;&#23454;&#29616;&#28789;&#27963;&#21046;&#36896;&#20197;&#28385;&#36275;&#19981;&#21516;&#23458;&#25143;&#38656;&#27714;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#31181;&#25361;&#25112;&#65292;&#26500;&#24314;&#33021;&#22815;&#22312;&#23433;&#20840;&#39640;&#25928;&#22320;&#21327;&#21161;&#20154;&#31867;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31215;&#26497;&#20154;&#26426;&#21327;&#20316;&#30340;&#38598;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#24847;&#22270;&#39044;&#27979;&#27169;&#22359;&#21644;&#19968;&#20010;&#40065;&#26834;&#30340;&#23433;&#20840;&#25511;&#21046;&#27169;&#22359;&#65292;&#20998;&#21035;&#29992;&#20110;&#25552;&#39640;&#21327;&#20316;&#25928;&#29575;&#21644;&#36991;&#20813;&#20132;&#20114;&#23433;&#20840;&#39118;&#38505;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;Kinova Gen3&#26426;&#22120;&#20154;&#19978;&#25191;&#34892;&#21327;&#20316;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#29615;&#22659;&#21464;&#21270;&#21644;&#19981;&#21516;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-robot collaboration (HRC) is one key component to achieving flexible manufacturing to meet the different needs of customers. However, it is difficult to build intelligent robots that can proactively assist humans in a safe and efficient way due to several challenges.First, it is challenging to achieve efficient collaboration due to diverse human behaviors and data scarcity. Second, it is difficult to ensure interactive safety due to uncertainty in human behaviors. This paper presents an integrated framework for proactive HRC. A robust intention prediction module, which leverages prior task information and human-in-the-loop training, is learned to guide the robot for efficient collaboration. The proposed framework also uses robust safe control to ensure interactive safety under uncertainty. The developed framework is applied to a co-assembly task using a Kinova Gen3 robot. The experiment demonstrates that our solution is robust to environmental changes as well as different human p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#25042;&#24816;&#26234;&#33021;&#20307;&#36827;&#34892;&#24809;&#32602;&#20197;&#24110;&#21161;&#22242;&#38431;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#20272;&#35745;&#25913;&#21892;&#23545;&#26234;&#33021;&#20307;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;Amortized Causal Discovery&#33258;&#21160;&#26816;&#27979;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.11846</link><description>&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#19979;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20197;&#23454;&#29616;&#39640;&#25928;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Discovering Causality for Efficient Cooperation in Multi-Agent Environments. (arXiv:2306.11846v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#25042;&#24816;&#26234;&#33021;&#20307;&#36827;&#34892;&#24809;&#32602;&#20197;&#24110;&#21161;&#22242;&#38431;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#20272;&#35745;&#25913;&#21892;&#23545;&#26234;&#33021;&#20307;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;Amortized Causal Discovery&#33258;&#21160;&#26816;&#27979;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#38656;&#35201;&#20316;&#20026;&#19968;&#20010;&#22242;&#38431;&#23398;&#20064;&#34892;&#20026;&#20197;&#23454;&#29616;&#20849;&#21516;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#23398;&#20064;&#20219;&#21153;&#30340;&#36807;&#31243;&#20013;&#65292;&#19968;&#20123;&#26234;&#33021;&#20307;&#21487;&#33021;&#20250;&#23398;&#20064;&#21040;&#27425;&#20248;&#31574;&#30053;&#65292;&#20174;&#32780;&#26410;&#33021;&#23545;&#22242;&#38431;&#30446;&#26631;&#20570;&#20986;&#36129;&#29486;&#12290;&#36825;&#20123;&#26234;&#33021;&#20307;&#34987;&#31216;&#20026;&#8220;&#25042;&#24816;&#26234;&#33021;&#20307;&#8221;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#38750;&#21512;&#20316;&#24615;&#34892;&#20026;&#21487;&#33021;&#26469;&#33258;&#20110;&#26410;&#33021;&#29702;&#35299;&#26159;&#21542;&#23548;&#33268;&#22238;&#25253;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#21512;&#20316;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22914;&#20309;&#24212;&#29992;&#23427;&#26469;&#24809;&#32602;&#36825;&#20123;&#25042;&#24816;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22240;&#26524;&#20851;&#31995;&#20272;&#35745;&#21487;&#29992;&#20110;&#25913;&#36827;&#23545;&#26234;&#33021;&#20307;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#23427;&#26469;&#25913;&#36827;&#22810;&#26234;&#33021;&#20307;&#29420;&#31435;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20998;&#25674;&#22240;&#26524;&#21457;&#29616;&#26469;&#33258;&#21160;&#26816;&#27979;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cooperative Multi-Agent Reinforcement Learning (MARL) agents are required to learn behaviours as a team to achieve a common goal. However, while learning a task, some agents may end up learning sub-optimal policies, not contributing to the objective of the team. Such agents are called lazy agents due to their non-cooperative behaviours that may arise from failing to understand whether they caused the rewards. As a consequence, we observe that the emergence of cooperative behaviours is not necessarily a byproduct of being able to solve a task as a team. In this paper, we investigate the applications of causality in MARL and how it can be applied in MARL to penalise these lazy agents. We observe that causality estimations can be used to improve the credit assignment to the agents and show how it can be leveraged to improve independent learning in MARL. Furthermore, we investigate how Amortized Causal Discovery can be used to automate causality detection within MARL environments. The r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;Transformer&#27169;&#22411;&#26469;&#35299;&#20915;&#34920;&#26684;&#22686;&#24378;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#20197;&#37325;&#26500;&#21407;&#22987;&#20540;&#25110;&#26631;&#39064;&#65292;&#20197;&#20415;&#20943;&#36731;&#25968;&#25454;&#20998;&#26512;&#24072;&#22312;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.11843</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#30340;Transformer&#27169;&#22411;&#29992;&#20110;&#34920;&#26684;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Based Transformer for Table Augmentation. (arXiv:2306.11843v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;Transformer&#27169;&#22411;&#26469;&#35299;&#20915;&#34920;&#26684;&#22686;&#24378;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#20197;&#37325;&#26500;&#21407;&#22987;&#20540;&#25110;&#26631;&#39064;&#65292;&#20197;&#20415;&#20943;&#36731;&#25968;&#25454;&#20998;&#26512;&#24072;&#22312;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20934;&#22791;&#65288;&#20063;&#31216;&#25968;&#25454;&#25972;&#29702;&#65289;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#36827;&#34892;&#20998;&#26512;&#25110;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#26368;&#32791;&#36153;&#26102;&#38388;&#21644;&#31934;&#21147;&#30340;&#27493;&#39588;&#20043;&#19968;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35797;&#22270;&#20943;&#36731;&#26368;&#32456;&#29992;&#25143;&#65288;&#20363;&#22914;&#25968;&#25454;&#20998;&#26512;&#24072;&#65289;&#22312;&#20174;&#25968;&#25454;&#28246;&#20013;&#26500;&#24314;&#21160;&#24577;&#34920;&#26684;&#25968;&#25454;&#30340;&#36807;&#31243;&#20013;&#30340;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#34920;&#26684;&#22686;&#24378;&#20219;&#21153;&#65292;&#21253;&#25324;&#34892;/&#21015;&#22635;&#20805;&#21644;&#25968;&#25454;&#25554;&#34917;&#12290;&#32473;&#23450;&#19968;&#32452;&#34920;&#26684;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#33258;&#23398;&#20064;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#33258;&#23398;&#20064;&#31574;&#30053;&#26159;&#20174;&#35821;&#26009;&#24211;&#20013;&#38543;&#26426;&#21435;&#38500;&#34920;&#26684;&#65292;&#24182;&#35757;&#32451;&#26816;&#32034;&#27169;&#22411;&#20197;&#22312;&#32473;&#23450;&#37096;&#20998;&#34920;&#26684;&#20316;&#20026;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#37325;&#26500;&#21407;&#22987;&#20540;&#25110;&#26631;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#39318;&#20808;&#35757;&#32451;&#23494;&#38598;&#30340;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data preparation, also called data wrangling, is considered one of the most expensive and time-consuming steps when performing analytics or building machine learning models. Preparing data typically involves collecting and merging data from complex heterogeneous, and often large-scale data sources, such as data lakes. In this paper, we introduce a novel approach toward automatic data wrangling in an attempt to alleviate the effort of end-users, e.g. data analysts, in structuring dynamic views from data lakes in the form of tabular data. We aim to address table augmentation tasks, including row/column population and data imputation. Given a corpus of tables, we propose a retrieval augmented self-trained transformer model. Our self-learning strategy consists in randomly ablating tables from the corpus and training the retrieval-based model to reconstruct the original values or headers given the partial tables as input. We adopt this strategy to first train the dense neural retrieval mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20219;&#20309;&#28145;&#24230;&#30340;ReLU&#32593;&#32476;&#37117;&#21487;&#20197;&#34987;&#37325;&#20889;&#20026;&#19968;&#20010;&#20855;&#26377;&#36879;&#26126;&#24615;&#30340;&#27973;&#23618;&#32593;&#32476;&#12290;&#36825;&#19968;&#32467;&#35770;&#26377;&#21161;&#20110;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.11827</link><description>&lt;p&gt;
&#20219;&#20309;&#28145;&#24230;ReLU&#32593;&#32476;&#37117;&#26159;&#27973;&#23618;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Any Deep ReLU Network is Shallow. (arXiv:2306.11827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20219;&#20309;&#28145;&#24230;&#30340;ReLU&#32593;&#32476;&#37117;&#21487;&#20197;&#34987;&#37325;&#20889;&#20026;&#19968;&#20010;&#20855;&#26377;&#36879;&#26126;&#24615;&#30340;&#27973;&#23618;&#32593;&#32476;&#12290;&#36825;&#19968;&#32467;&#35770;&#26377;&#21161;&#20110;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#36896;&#24615;&#22320;&#35777;&#26126;&#20102;&#27599;&#20010;&#28145;&#24230;&#30340;ReLU&#32593;&#32476;&#21487;&#20197;&#34987;&#37325;&#20889;&#20026;&#19968;&#20010;&#20989;&#25968;&#19978;&#31561;&#20215;&#30340;&#19977;&#23618;&#32593;&#32476;&#65292;&#20854;&#20013;&#26435;&#37325;&#20540;&#20026;&#24310;&#36831;&#23454;&#25968;&#12290;&#22522;&#20110;&#27492;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#21487;&#20197;&#32473;&#20986;&#19968;&#20010;&#28145;&#24230;ReLU&#32593;&#32476;&#23545;&#24212;&#30340;&#26174;&#24335;&#26435;&#37325;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#27973;&#23618;&#32593;&#32476;&#26159;&#36879;&#26126;&#30340;&#65292;&#24182;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We constructively prove that every deep ReLU network can be rewritten as a functionally identical three-layer network with weights valued in the extended reals. Based on this proof, we provide an algorithm that, given a deep ReLU network, finds the explicit weights of the corresponding shallow network. The resulting shallow network is transparent and used to generate explanations of the model s behaviour.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.11816</link><description>&lt;p&gt;
&#23398;&#20250;&#29983;&#25104;&#27604;&#20320;&#30340;LMM&#26356;&#22909;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;(RL)&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#20363;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#26368;&#36817;&#30340;LLM&#65292;&#22914;ChatGPT&#21644;GPT - 4&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#24182;&#34701;&#21512;&#20102;RL&#21644;&#20154;&#31867;&#21453;&#39304;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#23398;&#20064;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#25506;&#32034;&#20102;&#36229;&#20986;&#36890;&#29992;RL&#31639;&#27861;&#22914;PPO&#20043;&#22806;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;RL&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21160;&#24577;&#40657;&#21283;&#23376;&#30340;&#25351;&#23548;LLM&#22914;GPT-3&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#24341;&#23548;&#21453;&#39304;&#30340;RL(RLGF)&#65292;&#36825;&#26159;&#19968;&#22871;&#29992;&#20110;LLM&#24494;&#35843;&#30340;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;GRUE&#22522;&#20934;&#27979;&#35797;&#30340;IMDB&#27491;&#21521;&#35780;&#35770;&#21644;CommonGen&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;RL&#31639;&#27861;&#27604;&#30417;&#30563;&#23398;&#20064;(SL)&#21644;&#40664;&#35748;PPO&#22522;&#32447;&#34920;&#29616;&#26356;&#39640;&#65292;&#35777;&#26126;&#20102;&#19982;&#25351;&#23548;LLM&#20114;&#21160;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for conditional text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users by incorporating RL and feedback from humans. Inspired by learning-to-search algorithms and capitalizing on key properties of text generation, we seek to investigate reinforcement learning algorithms beyond general purpose algorithms such as Proximal policy optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive review and CommonGen text generation task from the GRUE benchmark. We show that our RL algorithms achieve higher performance than supervised learning (SL) and default PPO baselines, demonstrating the benefit of interaction with the guide LLM. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#24615;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#24037;&#20214;&#30340;&#35774;&#35745;&#26041;&#27861;&#23398;&#65292;&#20197;&#35299;&#20915;&#20915;&#31574;&#32773;&#19981;&#24895;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#31243;&#24207;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23398;&#21253;&#25324;&#20116;&#20010;&#38454;&#27573;&#12290;&#36890;&#36807;&#35774;&#35745;&#24182;&#23454;&#26045;&#19968;&#20010;&#24037;&#20214;&#65292;&#39044;&#27979;&#35786;&#26029;&#20026;&#26032;&#20896;&#32954;&#28814;&#30340;&#24739;&#32773;&#26159;&#21542;&#23558;&#26469;&#20250;&#20986;&#29616;&#33457;&#31881;&#28909;&#30151;&#29366;&#26469;&#35777;&#26126;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11771</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#24037;&#20214;&#30340;&#35774;&#35745;&#65306;&#26041;&#27861;&#21644;&#23454;&#29992;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Designing Explainable Predictive Machine Learning Artifacts: Methodology and Practical Demonstration. (arXiv:2306.11771v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#24615;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#24037;&#20214;&#30340;&#35774;&#35745;&#26041;&#27861;&#23398;&#65292;&#20197;&#35299;&#20915;&#20915;&#31574;&#32773;&#19981;&#24895;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#31243;&#24207;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23398;&#21253;&#25324;&#20116;&#20010;&#38454;&#27573;&#12290;&#36890;&#36807;&#35774;&#35745;&#24182;&#23454;&#26045;&#19968;&#20010;&#24037;&#20214;&#65292;&#39044;&#27979;&#35786;&#26029;&#20026;&#26032;&#20896;&#32954;&#28814;&#30340;&#24739;&#32773;&#26159;&#21542;&#23558;&#26469;&#20250;&#20986;&#29616;&#33457;&#31881;&#28909;&#30151;&#29366;&#26469;&#35777;&#26126;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#32452;&#32455;&#20013;&#36234;&#26469;&#36234;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#37325;&#35201;&#30340;&#19994;&#21153;&#39046;&#22495;&#25512;&#21160;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#21508;&#34892;&#21508;&#19994;&#20844;&#21496;&#30340;&#20915;&#31574;&#32773;&#20173;&#28982;&#26497;&#20026;&#19981;&#24895;&#24847;&#38599;&#29992;&#22522;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#25226;&#36825;&#20010;&#38382;&#39064;&#24402;&#22240;&#20110;&#23545;&#20110;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26222;&#36941;&#25345;&#26377;&#30340;&#35266;&#28857;&#65292;&#21363;&#23427;&#20204;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65292;&#20854;&#22797;&#26434;&#24615;&#19981;&#20801;&#35768;&#25581;&#31034;&#39537;&#21160;&#30456;&#24212;&#31995;&#32479;&#36755;&#20986;&#30340;&#22240;&#32032;&#12290;&#20026;&#20102;&#26377;&#21161;&#20110;&#20811;&#26381;&#36825;&#31181;&#37319;&#29992;&#38556;&#30861;&#65292;&#25105;&#20204;&#35748;&#20026;&#20449;&#24687;&#31995;&#32479;&#30740;&#31350;&#24212;&#26356;&#22810;&#22320;&#20851;&#27880;&#35774;&#35745;&#33021;&#21521;&#20154;&#31867;&#20915;&#31574;&#32773;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#21407;&#22411;&#39044;&#27979;&#23548;&#21521;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#65288;&#21363;&#24037;&#20214;&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#24037;&#20855;&#26469;&#20419;&#36827;&#36825;&#31181;&#24037;&#20214;&#30340;&#24320;&#21457;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#23545;&#20854;&#24320;&#21457;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#30740;&#31350;&#31354;&#30333;&#26159;&#30001;&#20110;&#32570;&#20047;&#24050;&#24314;&#31435;&#30340;&#35774;&#35745;&#26041;&#27861;&#23398;&#65292;&#22240;&#27492;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#36825;&#26679;&#30340;&#26041;&#27861;&#23398;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#21253;&#25324;&#20116;&#20010;&#38454;&#27573;&#65292;&#24341;&#23548;&#24037;&#31243;&#24072;&#23436;&#25104;&#24037;&#20214;&#30340;&#35843;&#26597;&#12289;&#35268;&#33539;&#12289;&#35774;&#35745;&#12289;&#39564;&#35777;&#21644;&#23454;&#26045;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#24182;&#23454;&#26045;&#19968;&#20010;&#24037;&#20214;&#26469;&#35777;&#26126;&#20854;&#36866;&#29992;&#24615;&#65292;&#35813;&#24037;&#20214;&#39044;&#27979;&#35786;&#26029;&#20026;&#26032;&#20896;&#32954;&#28814;&#30340;&#24739;&#32773;&#26159;&#21542;&#23558;&#26469;&#20250;&#20986;&#29616;&#33457;&#31881;&#28909;&#30151;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction-oriented machine learning is becoming increasingly valuable to organizations, as it may drive applications in crucial business areas. However, decision-makers from companies across various industries are still largely reluctant to employ applications based on modern machine learning algorithms. We ascribe this issue to the widely held view on advanced machine learning algorithms as "black boxes" whose complexity does not allow for uncovering the factors that drive the output of a corresponding system. To contribute to overcome this adoption barrier, we argue that research in information systems should devote more attention to the design of prototypical prediction-oriented machine learning applications (i.e., artifacts) whose predictions can be explained to human decision-makers. However, despite the recent emergence of a variety of tools that facilitate the development of such artifacts, there has so far been little research on their development. We attribute this research g
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#24418;&#24314;&#27169;&#35821;&#35328;&#65292;&#20351;&#24471;&#22312;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#23545;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#34892;&#19968;&#33268;&#21644;&#21487;&#29702;&#35299;&#30340;&#24314;&#27169;&#65292;&#24182;&#23558;&#21508;&#20010;&#23376;&#39046;&#22495;&#20998;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#23376;&#31995;&#32479;&#65292;&#20174;&#32780;&#20943;&#23569;&#29616;&#26377;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.11767</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#22270;&#24418;&#24314;&#27169;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
A Graphical Modeling Language for Artificial Intelligence Applications in Automation Systems. (arXiv:2306.11767v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#24418;&#24314;&#27169;&#35821;&#35328;&#65292;&#20351;&#24471;&#22312;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#23545;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#34892;&#19968;&#33268;&#21644;&#21487;&#29702;&#35299;&#30340;&#24314;&#27169;&#65292;&#24182;&#23558;&#21508;&#20010;&#23376;&#39046;&#22495;&#20998;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#23376;&#31995;&#32479;&#65292;&#20174;&#32780;&#20943;&#23569;&#29616;&#26377;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36890;&#24120;&#26159;&#20998;&#24067;&#24335;&#31995;&#32479;&#65292;&#20854;&#24320;&#21457;&#21644;&#38598;&#25104;&#28041;&#21450;&#22810;&#20010;&#19987;&#23478;&#12290;&#27599;&#20010;&#19987;&#23478;&#20351;&#29992;&#33258;&#24049;&#30340;&#39046;&#22495;&#29305;&#23450;&#24314;&#27169;&#35821;&#35328;&#21644;&#24037;&#20855;&#26469;&#27169;&#25311;&#31995;&#32479;&#20803;&#32032;&#12290;&#30446;&#21069;&#23578;&#19981;&#23384;&#22312;&#19968;&#31181;&#36328;&#23398;&#31185;&#30340;&#22270;&#24418;&#24314;&#27169;&#35821;&#35328;&#65292;&#33021;&#22815;&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20316;&#20026;&#25972;&#20307;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#20351;&#25152;&#26377;&#23398;&#31185;&#37117;&#33021;&#22815;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#24418;&#24314;&#27169;&#35821;&#35328;&#65292;&#33021;&#22815;&#22312;&#31995;&#32479;&#32423;&#21035;&#19978;&#23545;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#34892;&#19968;&#33268;&#21644;&#21487;&#29702;&#35299;&#30340;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#23558;&#21508;&#20010;&#23376;&#39046;&#22495;&#20998;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#23376;&#31995;&#32479;&#65292;&#20174;&#32780;&#20943;&#23569;&#29616;&#26377;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) applications in automation systems are usually distributed systems whose development and integration involve several experts. Each expert uses its own domain-specific modeling language and tools to model the system elements. An interdisciplinary graphical modeling language that enables the modeling of an AI application as an overall system comprehensible to all disciplines does not yet exist. As a result, there is often a lack of interdisciplinary system understanding, leading to increased development, integration, and maintenance efforts. This paper therefore presents a graphical modeling language that enables consistent and understandable modeling of AI applications in automation systems at system level. This makes it possible to subdivide individual subareas into domain specific subsystems and thus reduce the existing efforts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23398;&#20064;&#21644;&#36827;&#21270;&#30456;&#32467;&#21512;&#26159;&#21542;&#33021;&#22815;&#25214;&#21040;&#27604;&#21333;&#29420;&#36827;&#21270;&#21457;&#29616;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#22312;&#23398;&#20064;&#21644;&#36873;&#25321;&#36807;&#31243;&#20013;&#24341;&#20837;&#22122;&#22768;&#31561;&#29305;&#23450;&#26465;&#20214;&#26102;&#65292;&#36825;&#31181;&#32452;&#21512;&#25165;&#33021;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2306.11761</link><description>&lt;p&gt;
&#23398;&#20064;&#19982;&#36827;&#21270;&#65306;&#24433;&#21709;&#26377;&#25928;&#32467;&#21512;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning and evolution: factors influencing an effective combination. (arXiv:2306.11761v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23398;&#20064;&#21644;&#36827;&#21270;&#30456;&#32467;&#21512;&#26159;&#21542;&#33021;&#22815;&#25214;&#21040;&#27604;&#21333;&#29420;&#36827;&#21270;&#21457;&#29616;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#22312;&#23398;&#20064;&#21644;&#36873;&#25321;&#36807;&#31243;&#20013;&#24341;&#20837;&#22122;&#22768;&#31561;&#29305;&#23450;&#26465;&#20214;&#26102;&#65292;&#36825;&#31181;&#32452;&#21512;&#25165;&#33021;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#31070;&#32463;&#36827;&#21270;&#31038;&#21306;&#20013;&#65292;&#36827;&#21270;&#21644;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#26159;&#19968;&#20010;&#26377;&#20105;&#35758;&#30340;&#35770;&#28857;&#12290;&#26412;&#25991;&#20316;&#32773;&#25506;&#35752;&#20102;&#23558;&#23398;&#20064;&#21644;&#36827;&#21270;&#30456;&#32467;&#21512;&#26159;&#21542;&#33021;&#22815;&#25214;&#21040;&#27604;&#21333;&#29420;&#36827;&#21270;&#21457;&#29616;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#31995;&#21015;&#23454;&#35777;&#30740;&#31350;&#65292;&#31361;&#20986;&#20102;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#65292;&#22914;&#22312;&#23398;&#20064;&#21644;&#36873;&#25321;&#36807;&#31243;&#20013;&#24341;&#20837;&#22122;&#22768;&#31561;&#65292;&#36825;&#20123;&#26465;&#20214;&#20915;&#23450;&#20102;&#36825;&#31181;&#32452;&#21512;&#30340;&#25104;&#21151;&#12290;&#32467;&#26524;&#22312;&#20004;&#20010;&#23450;&#24615;&#19981;&#21516;&#30340;&#39046;&#22495;&#20013;&#33719;&#24471;&#65292;&#20854;&#20013;&#20195;&#29702;/&#29615;&#22659;&#20132;&#20114;&#26159;&#26368;&#23567;&#25110;&#19981;&#23384;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mutual relationship between evolution and learning is a controversial argument among the artificial intelligence and neuro-evolution communities. After more than three decades, there is still no common agreement on the matter. In this paper the author investigates whether combining learning and evolution permits to find better solutions than those discovered by evolution alone. More specifically, the author presents a series of empirical studies that highlight some specific conditions determining the success of such a combination, like the introduction of noise during the learning and selection processes. Results are obtained in two qualitatively different domains, where agent/environment interactions are minimal or absent.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;DLA-in-loop&#21487;&#38752;&#24615;&#35780;&#20272;&#24179;&#21488;&#65292;&#29992;&#20110;&#22312;&#26089;&#26399;DLA&#35774;&#35745;&#38454;&#27573;&#35780;&#20272;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;DLA&#30340;&#24635;&#25925;&#38556;&#21069;&#34892;&#39542;&#36317;&#31163;&#31561;&#39640;&#32423;&#21487;&#38752;&#24615;&#25351;&#26631;&#65292;&#20174;&#32780;&#36991;&#20813;&#39640;&#26114;&#30340;&#35774;&#35745;&#36845;&#20195;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.11759</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#24490;&#29615;&#21487;&#38752;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Accelerator in Loop Reliability Evaluation for Autonomous Driving. (arXiv:2306.11759v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;DLA-in-loop&#21487;&#38752;&#24615;&#35780;&#20272;&#24179;&#21488;&#65292;&#29992;&#20110;&#22312;&#26089;&#26399;DLA&#35774;&#35745;&#38454;&#27573;&#35780;&#20272;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;DLA&#30340;&#24635;&#25925;&#38556;&#21069;&#34892;&#39542;&#36317;&#31163;&#31561;&#39640;&#32423;&#21487;&#38752;&#24615;&#25351;&#26631;&#65292;&#20174;&#32780;&#36991;&#20813;&#39640;&#26114;&#30340;&#35774;&#35745;&#36845;&#20195;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;(DLA)&#30340;&#21487;&#38752;&#24615;&#23545;&#31995;&#32479;&#23433;&#20840;&#26377;&#30528;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;DLA&#30340;&#21487;&#38752;&#24615;&#36890;&#24120;&#36890;&#36807;&#20302;&#32423;&#25351;&#26631;&#65288;&#22914;&#36755;&#20986;&#30340;&#22343;&#26041;&#35823;&#24046;&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#25351;&#26631;&#19982;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#24635;&#25925;&#38556;&#21069;&#34892;&#39542;&#36317;&#31163;&#31561;&#39640;&#32423;&#25351;&#26631;&#20173;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#22312;&#21518;&#30789;&#38454;&#27573;&#35780;&#20272;&#30340;&#39640;&#32423;&#21487;&#38752;&#24615;&#25351;&#26631;&#21487;&#33021;&#20173;&#20250;&#23548;&#33268;DLA&#35774;&#35745;&#20462;&#35746;&#65292;&#36827;&#32780;&#23548;&#33268;&#38024;&#23545;&#33258;&#20027;&#39550;&#39542;&#30340;&#26114;&#36149;&#21487;&#38752;&#30340;DLA&#35774;&#35745;&#36845;&#20195;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;DLA&#24490;&#29615;&#21487;&#38752;&#24615;&#35780;&#20272;&#24179;&#21488;&#65292;&#20197;&#20415;&#22312;&#26089;&#26399;DLA&#35774;&#35745;&#38454;&#27573;&#35780;&#20272;&#31995;&#32479;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliability of deep learning accelerators (DLAs) used in autonomous driving systems has significant impact on the system safety. However, the DLA reliability is usually evaluated with low-level metrics like mean square errors of the output which remains rather different from the high-level metrics like total distance traveled before failure in autonomous driving. As a result, the high-level reliability metrics evaluated at the post-silicon stage may still lead to DLA design revision and result in expensive reliable DLA design iterations targeting at autonomous driving. To address the problem, we proposed a DLA-in-loop reliability evaluation platform to enable system reliability evaluation at the early DLA design stage.
&lt;/p&gt;</description></item><item><title>MRFI&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#36827;&#34892;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.11758</link><description>&lt;p&gt;
MRFI&#65306;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#30340;&#24320;&#28304;&#22810;&#20998;&#36776;&#29575;&#25925;&#38556;&#27880;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing. (arXiv:2306.11758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11758
&lt;/p&gt;
&lt;p&gt;
MRFI&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#36827;&#34892;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#21363;&#20351;&#22312;&#19981;&#21487;&#38752;&#30340;&#30828;&#20214;&#19978;&#20063;&#33021;&#36827;&#34892;&#26377;&#24377;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#37096;&#32626;&#20043;&#21069;&#36827;&#34892;&#21508;&#31181;&#30828;&#20214;&#25925;&#38556;&#30340;&#20840;&#38754;&#21487;&#38752;&#24615;&#20998;&#26512;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#30340;&#38169;&#35823;&#27880;&#20837;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#20173;&#28982;&#23616;&#38480;&#20110;&#23545;&#31070;&#32463;&#20803;&#30340;&#22522;&#26412;&#25925;&#38556;&#27880;&#20837;&#65292;&#24182;&#26410;&#25552;&#20379;&#32454;&#31890;&#24230;&#28431;&#27934;&#20998;&#26512;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#20173;&#38656;&#35201;&#26356;&#25913;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20351;&#25925;&#38556;&#27880;&#20837;&#19982;&#27491;&#24120;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32039;&#23494;&#32806;&#21512;&#65292;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#30340;&#20351;&#29992;&#38590;&#24230;&#24182;&#20943;&#24930;&#20102;&#25925;&#38556;&#27169;&#25311;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22810;&#20998;&#36776;&#29575;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;MRFI&#12290;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#65292;&#32780;&#19981;&#26159;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#25925;&#38556;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure resilient neural network processing on even unreliable hardware, comprehensive reliability analysis against various hardware faults is generally required before the deep neural network models are deployed, and efficient error injection tools are highly demanded. However, most existing fault injection tools remain rather limited to basic fault injection to neurons and fail to provide fine-grained vulnerability analysis capability. In addition, many of the fault injection tools still need to change the neural network models and make the fault injection closely coupled with normal neural network processing, which further complicates the use of the fault injection tools and slows down the fault simulation. In this work, we propose MRFI, a highly configurable multi-resolution fault injection tool for deep neural networks. It enables users to modify an independent fault configuration file rather than neural network models for the fault injection and vulnerability analysis. Particul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26465;&#20214;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23436;&#22791;&#31639;&#27861;&#35777;&#26126;&#20102; Pearl &#30340; do-&#28436;&#31639;&#27861;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#26159;&#23436;&#22791;&#30340;&#12290;&#35813;&#24037;&#20316;&#23545;&#29616;&#26377;&#32467;&#26524;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26465;&#20214;&#22240;&#26524;&#25928;&#24212;&#21487;&#35782;&#21035;&#24615;&#30340;&#26356;&#23436;&#25972;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.11755</link><description>&lt;p&gt;
&#20851;&#20110;&#26465;&#20214;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Identifiability of Conditional Causal Effects. (arXiv:2306.11755v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26465;&#20214;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23436;&#22791;&#31639;&#27861;&#35777;&#26126;&#20102; Pearl &#30340; do-&#28436;&#31639;&#27861;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#26159;&#23436;&#22791;&#30340;&#12290;&#35813;&#24037;&#20316;&#23545;&#29616;&#26377;&#32467;&#26524;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26465;&#20214;&#22240;&#26524;&#25928;&#24212;&#21487;&#35782;&#21035;&#24615;&#30340;&#26356;&#23436;&#25972;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32473;&#23450;&#22240;&#26524;&#22270;&#21644;&#26576;&#20123;&#24418;&#24335;&#20026; $Q[S]:=P(S|do(V\setminus S))$ &#30340;&#35266;&#27979;&#21644;/&#25110;&#24178;&#39044;&#20998;&#24067;&#26102;&#65292;&#20219;&#24847;&#26465;&#20214;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#31216;&#27492;&#38382;&#39064;&#20026;&#26465;&#20214;&#24191;&#20041;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65288;c-gID&#65289;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379; c-gID &#38382;&#39064;&#30340;&#23436;&#22791;&#31639;&#27861;&#35777;&#26126;&#20102; Pearl &#30340; do-&#28436;&#31639;&#27861;&#23545;&#20110; c-gID &#38382;&#39064;&#30340;&#23436;&#22791;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#26126;&#30830;&#21152;&#20837;&#20102;&#21487;&#35782;&#21035;&#24615;&#25152;&#24517;&#38656;&#30340;&#27491;&#24615;&#20551;&#23450;&#65292;&#23545; Lee &#31561;&#20154;&#65288;2020&#65289;&#12289;Correa &#31561;&#20154;&#65288;2021&#65289;&#20013;&#30340; c-gID &#38382;&#39064;&#36827;&#34892;&#20102;&#37325;&#26032;&#35752;&#35770;&#12290;&#26412;&#25991;&#25193;&#23637;&#20102; Lee &#31561;&#20154;&#65288;2019&#65289;&#12289;Kivva &#31561;&#20154;&#65288;2022&#65289;&#30340;&#20851;&#20110;&#19968;&#33324;&#21487;&#35782;&#21035;&#24615;&#65288;gID&#65289;&#30340;&#30740;&#31350;&#65292;&#21518;&#32773;&#30740;&#31350;&#20102;&#26080;&#26465;&#20214;&#22240;&#26524;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102; Shpitser &#21644; Pearl&#65288;2006b&#65289;&#30340;&#30740;&#31350;&#65292;&#21518;&#32773;&#20165;&#32473;&#20986;&#35266;&#27979;&#20998;&#24067; $P(\mathbf{V})$ &#30340;&#26465;&#20214;&#19979;&#26465;&#20214;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of identifiability of an arbitrary conditional causal effect given both the causal graph and a set of any observational and/or interventional distributions of the form $Q[S]:=P(S|do(V\setminus S))$, where $V$ denotes the set of all observed variables and $S\subseteq V$. We call this problem conditional generalized identifiability (c-gID in short) and prove the completeness of Pearl's $do$-calculus for the c-gID problem by providing sound and complete algorithm for the c-gID problem. This work revisited the c-gID problem in Lee et al. [2020], Correa et al. [2021] by adding explicitly the positivity assumption which is crucial for identifiability. It extends the results of [Lee et al., 2019, Kivva et al., 2022] on general identifiability (gID) which studied the problem for unconditional causal effects and Shpitser and Pearl [2006b] on identifiability of conditional causal effects given merely the observational distribution $P(\mathbf{V})$ as our algorithm generaliz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#29366;&#20808;&#39564;&#30340;&#19977;&#32500;&#29289;&#20307;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#34920;&#31034;&#20013;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;3D&#37325;&#24314;&#22522;&#20934;&#27979;&#35797;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11739</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#24418;&#29366;&#20808;&#39564;&#30340;&#22810;&#35270;&#35282;&#19977;&#32500;&#29289;&#20307;&#37325;&#24314;&#19982;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-view 3D Object Reconstruction and Uncertainty Modelling with Neural Shape Prior. (arXiv:2306.11739v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#29366;&#20808;&#39564;&#30340;&#19977;&#32500;&#29289;&#20307;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#34920;&#31034;&#20013;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;3D&#37325;&#24314;&#22522;&#20934;&#27979;&#35797;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#29289;&#20307;&#37325;&#24314;&#22312;&#35821;&#20041;&#22330;&#26223;&#29702;&#35299;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#30001;&#20110;&#32570;&#20047;&#28145;&#24230;&#20449;&#24687;&#12289;&#36974;&#25377;&#21644;&#22122;&#22768;&#31561;&#38382;&#39064;&#65292;&#20174;&#21333;ocular&#22270;&#20687;&#30452;&#25509;&#37325;&#24314;&#35814;&#32454;&#30340;&#19977;&#32500;&#24418;&#29366;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#29983;&#25104;&#30830;&#23450;&#24615;&#30340;&#29289;&#20307;&#27169;&#22411;&#65292;&#24182;&#27809;&#26377;&#24847;&#35782;&#21040;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#23545;&#35937;&#34920;&#31034;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#34920;&#31034;&#20174;&#22823;&#22411;3D&#23545;&#35937;&#27169;&#22411;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#29289;&#20307;&#24418;&#29366;&#20998;&#24067;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#22312;&#34920;&#31034;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#32534;&#30721;&#22120;&#65292;&#30452;&#25509;&#20174;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#28508;&#22312;&#20195;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#23558;&#28508;&#22312;&#20195;&#30721;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#21040;SDF&#20540;&#20013;&#65292;&#24182;&#20026;&#27599;&#20010;&#32593;&#26684;&#32452;&#20214;&#29983;&#25104;&#24102;&#26377;&#23616;&#37096;&#19981;&#30830;&#23450;&#24615;&#30340;&#19977;&#32500;&#23545;&#35937;&#32593;&#26684;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#28176;&#36827;&#34701;&#21512;&#26041;&#27861;&#65292;&#23558;&#22810;&#35282;&#24230;&#23545;&#35937;&#30340;&#28508;&#22312;&#20195;&#30721;&#34701;&#21512;&#25104;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#23436;&#25972;3D&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;3D&#37325;&#24314;&#22522;&#20934;&#27979;&#35797;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#38752;&#19988;&#23450;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D object reconstruction is important for semantic scene understanding. It is challenging to reconstruct detailed 3D shapes from monocular images directly due to a lack of depth information, occlusion and noise. Most current methods generate deterministic object models without any awareness of the uncertainty of the reconstruction. We tackle this problem by leveraging a neural object representation which learns an object shape distribution from large dataset of 3d object models and maps it into a latent space. We propose a method to model uncertainty as part of the representation and define an uncertainty-aware encoder which generates latent codes with uncertainty directly from individual input images. Further, we propose a method to propagate the uncertainty in the latent code to SDF values and generate a 3d object mesh with local uncertainty for each mesh component. Finally, we propose an incremental fusion method under a Bayesian framework to fuse the latent codes from multi-view ob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36924;&#30495;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#30340;&#22522;&#20934;&#35780;&#20272;RM-PRT&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#27169;&#24577;&#25552;&#31034;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.11335</link><description>&lt;p&gt;
RM-PRT: &#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#21644;&#22522;&#20110;&#28176;&#36827;&#25512;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks. (arXiv:2306.11335v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36924;&#30495;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#30340;&#22522;&#20934;&#35780;&#20272;RM-PRT&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#27169;&#24577;&#25552;&#31034;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#30340;&#20986;&#29616;&#65292;&#26174;&#30528;&#25512;&#36827;&#20102;&#26426;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36825;&#19968;&#31361;&#30772;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36825;&#20123;&#24320;&#28304;LLM&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#32479;&#19968;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#20013;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#20934;&#30830;&#29702;&#35299;&#21644;&#25191;&#34892;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28176;&#36827;&#25512;&#29702;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;RM-PRT&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RM-PRT&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;Unreal Engine 5&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#20445;&#30495;&#25968;&#23383;&#21452;&#32990;&#32974;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;782&#20010;&#31867;&#21035;&#65292;2023&#20010;&#29289;&#20307;&#65292;&#24182;&#20351;&#29992;ChatGPT&#29983;&#25104;&#20102;15,000&#20010;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20197;&#35814;&#32454;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;RM-PRT&#22522;&#20934;&#35780;&#20272;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#25509;&#21463;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#33258;&#21160;&#36755;&#20986;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs action
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GTAGC&#30340;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#33258;&#32534;&#30721;&#22120;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11307</link><description>&lt;p&gt;
&#22686;&#24378;&#23646;&#24615;&#32858;&#31867;&#30340;&#22270;&#24418;&#21464;&#25442;&#26041;&#27861;&#65306;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method. (arXiv:2306.11307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GTAGC&#30340;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#33258;&#32534;&#30721;&#22120;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#24433;&#21709;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#26377;&#21161;&#20110;&#22270;&#32858;&#31867;&#65292;&#36825;&#26159;&#21508;&#20010;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#26368;&#36817;&#24050;&#32463;&#36827;&#20837;&#20102;&#22270;&#23398;&#20064;&#30340;&#39046;&#22495;&#65292;&#36825;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#30740;&#31350;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#24050;&#25104;&#20026;&#22270;&#32858;&#31867;&#20219;&#21153;&#20248;&#36873;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#23616;&#37096;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#29702;&#35299;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#22797;&#26434;&#20840;&#23616;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#30340;&#22270;&#24418;&#32858;&#31867;&#65288;GTAGC&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22270;&#33258;&#32534;&#30721;&#22120;&#19982;&#22270;&#24418;&#21464;&#25442;&#22120;&#34701;&#21512;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#33410;&#28857;&#20043;&#38388;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#34701;&#21512;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20351;&#24471;&#22270;&#24418;&#32858;&#31867;&#20219;&#21153;&#26377;&#20102;&#26174;&#30528;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning (GRL) is an influential methodology, enabling a more profound understanding of graph-structured data and aiding graph clustering, a critical task across various domains. The recent incursion of attention mechanisms, originally an artifact of Natural Language Processing (NLP), into the realm of graph learning has spearheaded a notable shift in research trends. Consequently, Graph Attention Networks (GATs) and Graph Attention Auto-Encoders have emerged as preferred tools for graph clustering tasks. Yet, these methods primarily employ a local attention mechanism, thereby curbing their capacity to apprehend the intricate global dependencies between nodes within graphs. Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph Transformer, GTAGC is adept at capturing global dependencies between nodes. This integration amplifi
&lt;/p&gt;</description></item><item><title>BayLing&#26159;&#19968;&#20010;&#25351;&#20196;&#36981;&#24490;&#30340;LLM&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#32763;&#35793;&#23558;&#35821;&#35328;&#29983;&#25104;&#21644;&#25351;&#20196;&#36981;&#24490;&#30340;&#33021;&#21147;&#20174;&#33521;&#35821;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#20351;&#24471;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#20063;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;LLMs&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#25968;&#25454;&#25110;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2306.10968</link><description>&lt;p&gt;
BayLing&#65306;&#36890;&#36807;&#20132;&#20114;&#24335;&#32763;&#35793;&#36830;&#25509;&#36328;&#35821;&#35328;&#23545;&#40784;&#21644;&#25351;&#20196;&#36861;&#36394;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models. (arXiv:2306.10968v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10968
&lt;/p&gt;
&lt;p&gt;
BayLing&#26159;&#19968;&#20010;&#25351;&#20196;&#36981;&#24490;&#30340;LLM&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#32763;&#35793;&#23558;&#35821;&#35328;&#29983;&#25104;&#21644;&#25351;&#20196;&#36981;&#24490;&#30340;&#33021;&#21147;&#20174;&#33521;&#35821;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#20351;&#24471;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#20063;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;LLMs&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#25968;&#25454;&#25110;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#20174;&#22522;&#30784;LLMs&#21457;&#23637;&#20026;&#25351;&#20196;&#36861;&#36394;LLMs&#65292;&#25351;&#20196;&#35843;&#20248;&#22312;&#23558;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#36890;&#24120;&#19987;&#27880;&#20110;&#33521;&#35821;&#65292;&#23548;&#33268;&#38750;&#33521;&#35821;&#35821;&#35328;&#34920;&#29616;&#36739;&#24046;&#12290;&#20026;&#20102;&#25552;&#39640;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#38656;&#35201;&#25910;&#38598;&#22522;&#30784;LLMs&#30340;&#35821;&#35328;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#35821;&#35328;&#29305;&#23450;&#30340;&#25351;&#20196;&#20197;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#65292;&#36825;&#20004;&#32773;&#37117;&#26159;&#27785;&#37325;&#30340;&#36127;&#25285;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#20154;&#31867;&#24037;&#20316;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20132;&#20114;&#24335;&#32763;&#35793;&#20219;&#21153;&#23558;&#35821;&#35328;&#29983;&#25104;&#21644;&#25351;&#20196;&#36981;&#24490;&#30340;&#33021;&#21147;&#20174;&#33521;&#35821;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;BayLing&#65292;&#19968;&#31181;&#25351;&#20196;&#36981;&#24490;LLM&#65292;&#21033;&#29992;LLaMA&#20316;&#20026;&#22522;&#30784;LLM&#65292;&#33258;&#21160;&#26500;&#24314;&#20132;&#20114;&#24335;&#32763;&#35793;&#25351;&#20196;&#20197;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#12290;&#23545;&#33521;&#35821;&#12289;&#20013;&#25991;&#21644;&#35199;&#29677;&#29273;&#35821;&#19977;&#31181;&#35821;&#35328;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;BayLing&#22312;&#19981;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#25968;&#25454;&#25110;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;LLMs&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable prowess in language understanding and generation. Advancing from foundation LLMs to instructionfollowing LLMs, instruction tuning plays a vital role in aligning LLMs to human preferences. However, the existing LLMs are usually focused on English, leading to inferior performance in non-English languages. In order to improve the performance for non-English languages, it is necessary to collect language-specific training data for foundation LLMs and construct language-specific instructions for instruction tuning, both of which are heavy loads. To minimize human workload, we propose to transfer the capabilities of language generation and instruction following from English to other languages through an interactive translation task. We have developed BayLing, an instruction-following LLM by utilizing LLaMA as the foundation LLM and automatically constructing interactive translation instructions for instructing tuning. Extensive assess
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.10548</link><description>&lt;p&gt;
MARBLE&#65306;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33402;&#26415;&#19982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20043;&#38388;&#20132;&#21449;&#30340;&#24191;&#27867;&#26102;&#20195;&#20013;&#65292;&#20363;&#22914;&#22270;&#20687;&#29983;&#25104;&#21644;&#34394;&#26500;&#20849;&#21019;&#65292;&#38899;&#20048;&#30340;AI&#20173;&#28982;&#30456;&#23545;&#21021;&#27493;&#65292;&#29305;&#21035;&#26159;&#22312;&#38899;&#20048;&#29702;&#35299;&#26041;&#38754;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#35780;&#20272;&#22522;&#20934;MARBLE&#65292;&#26088;&#22312;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#23450;&#20041;&#21253;&#25324;&#22768;&#23398;&#65292;&#28436;&#22863;&#65292;&#20048;&#35889;&#21644;&#39640;&#32423;&#25551;&#36848;&#22312;&#20869;&#30340;&#22235;&#20010;&#23618;&#27425;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;8&#20010;&#20844;&#20849;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;14&#39033;&#20219;&#21153;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21327;&#35758;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#22522;&#20110;&#38899;&#20048;&#24405;&#38899;&#24320;&#21457;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#24449;&#30340;&#20844;&#24179;&#21644;&#26631;&#20934;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;MARBLE&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#37325;&#29992;&#30340;&#24037;&#20855;&#24211;&#65292;&#20197;&#25903;&#25345;&#31038;&#21306;&#39537;&#21160;&#30340;&#23458;&#35266;&#22522;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and re
&lt;/p&gt;</description></item><item><title>&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#26159;&#35299;&#20915;GRL&#24403;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.10456</link><description>&lt;p&gt;
&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25512;&#36827;&#29983;&#29289;&#21307;&#23398;&#65306;&#26368;&#26032;&#36827;&#23637;&#65292;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advancing Biomedicine with Graph Representation Learning: Recent Progress, Challenges, and Future Directions. (arXiv:2306.10456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10456
&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#26159;&#35299;&#20915;GRL&#24403;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#21253;&#25324;&#29983;&#29289;&#21307;&#23398;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#37117;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#12290;&#26412;&#32508;&#36848;&#30340;&#30446;&#30340;&#26159;&#22238;&#39038;GRL&#26041;&#27861;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;GRL&#24403;&#21069;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning (GRL) has emerged as a pivotal field that has contributed significantly to breakthroughs in various fields, including biomedicine. The objective of this survey is to review the latest advancements in GRL methods and their applications in the biomedical field. We also highlight key challenges currently faced by GRL and outline potential directions for future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SAT&#27169;&#36136;&#32452;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#21512;&#35777;&#20070;&#23398;&#20064;(co-certificate learning)&#65292;&#29992;&#20110;&#35299;&#20915;&#28385;&#36275;&#32473;&#23450;co-NP&#23646;&#24615;&#30340;&#21516;&#26500;&#22270;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;SAT&#30340;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10427</link><description>&lt;p&gt;
&#22522;&#20110; SAT &#27169;&#36136;&#30340;&#21512;&#35777;&#20070;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Co-Certificate Learning with SAT Modulo Symmetries. (arXiv:2306.10427v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SAT&#27169;&#36136;&#32452;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#21512;&#35777;&#20070;&#23398;&#20064;(co-certificate learning)&#65292;&#29992;&#20110;&#35299;&#20915;&#28385;&#36275;&#32473;&#23450;co-NP&#23646;&#24615;&#30340;&#21516;&#26500;&#22270;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;SAT&#30340;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; SAT-based &#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#28385;&#36275;&#32473;&#23450; co-NP &#23646;&#24615;&#30340;&#25152;&#26377;&#21516;&#26500;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102; SAT Modulo Symmetry (SMS) &#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026; co-certificate learning &#30340;&#25216;&#26415;&#12290;&#22914;&#26524; SMS &#29983;&#25104;&#36829;&#21453;&#32473;&#23450; co-NP &#23646;&#24615;&#30340;&#20505;&#36873;&#22270;&#24418;&#65292;&#21017;&#25105;&#20204;&#33719;&#24471;&#36825;&#31181;&#36829;&#21453;&#30340;&#35777;&#20070;&#65292;&#21363; co-NP &#23646;&#24615;&#30340;&#8220;&#21512;&#35777;&#20070;&#8221;&#12290; &#21512;&#35777;&#20070;&#23548;&#33268; SAT &#27714;&#35299;&#22120;&#20316;&#20026; SMS &#30340;&#21518;&#31471;&#23398;&#20064;&#20854; CDCL &#31243;&#24207;&#30340;&#23376;&#21477;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; SMS &#21152;&#19978; co-certificate learning &#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892; Kochen-Specker &#30690;&#37327;&#31995;&#32479;&#22823;&#23567;&#19979;&#38480;&#30340;&#26368;&#20339;&#24050;&#30693;&#30028;&#38480;&#65292;&#36825;&#26159;&#37327;&#23376;&#21147;&#23398;&#22522;&#30784;&#30340;&#20013;&#24515;&#38382;&#39064;&#65292;&#24050;&#32463;&#30740;&#31350;&#20102;&#21322;&#20010;&#22810;&#19990;&#32426;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110; SAT &#30340;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new SAT-based method for generating all graphs up to isomorphism that satisfy a given co-NP property. Our method extends the SAT Modulo Symmetry (SMS) framework with a technique that we call co-certificate learning. If SMS generates a candidate graph that violates the given co-NP property, we obtain a certificate for this violation, i.e., `co-certificate' for the co-NP property. The co-certificate gives rise to a clause that the SAT solver, serving as SMS's backend, learns as part of its CDCL procedure. We demonstrate that SMS plus co-certificate learning is a powerful method that allows us to improve the best-known lower bound on the size of Kochen-Specker vector systems, a problem that is central to the foundations of quantum mechanics and has been studied for over half a century. Our approach is orders of magnitude faster and scales significantly better than a recently proposed SAT-based method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Enlighten-anything&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#20013;&#23558;&#20998;&#27573;&#27169;&#22411;&#19982;SAM&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#33391;&#22909;&#35270;&#35273;&#24863;&#30693;&#30340;&#34701;&#21512;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.10286</link><description>&lt;p&gt;
Enlighten-anything: &#24403;&#20998;&#27573;&#27169;&#22411;&#36935;&#35265;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enlighten-anything:When Segment Anything Model Meets Low-light Image Enhancement. (arXiv:2306.10286v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Enlighten-anything&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#20013;&#23558;&#20998;&#27573;&#27169;&#22411;&#19982;SAM&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#33391;&#22909;&#35270;&#35273;&#24863;&#30693;&#30340;&#34701;&#21512;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#26159;&#19968;&#39033;&#20302;&#32423;&#21035;&#35270;&#35273;&#20219;&#21153;&#65292;&#22823;&#22810;&#25968;CNN&#26041;&#27861;&#37117;&#26159;&#20316;&#20026;&#40657;&#30418;&#23376;&#35774;&#35745;&#30340;&#65292;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#22266;&#26377;&#32654;&#23398;&#12290;&#35768;&#22810;&#26080;&#30417;&#30563;&#26041;&#27861;&#24573;&#30053;&#20102;&#20302;&#20809;&#22330;&#26223;&#20013;&#21487;&#35265;&#20449;&#24687;&#30340;&#36864;&#21270;&#65292;&#36825;&#20250;&#20005;&#37325;&#24433;&#21709;&#34917;&#20805;&#20449;&#24687;&#30340;&#32858;&#21512;&#65292;&#24182;&#20351;&#34701;&#21512;&#31639;&#27861;&#26080;&#27861;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#34701;&#21512;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Enlighten-anything&#65292;&#33021;&#22815;&#23558;SAM&#20998;&#27573;&#30340;&#35821;&#20041;&#24847;&#22270;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#35270;&#35273;&#24863;&#30693;&#30340;&#34701;&#21512;&#22270;&#20687;&#12290;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#24471;&#21040;&#20102;&#26497;&#22823;&#25552;&#39640;&#65292;&#23545;LOL&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PSNR&#19978;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;3dB&#65292;&#22312;SSIM&#19978;&#25552;&#39640;&#20102;8&#12290; SAM&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#20026;&#26080;&#30417;&#30563;&#20302;&#20809;&#22686;&#24378;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24110;&#21161;&#12290;Enlighten-anything&#30340;&#28304;&#20195;&#30721;&#21487;&#20197;&#20174; https://github.com/zhangbaijin/enlighten-any &#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image restoration is a low-level visual task, and most CNN methods are designed as black boxes, lacking transparency and intrinsic aesthetics. Many unsupervised approaches ignore the degradation of visible information in low-light scenes, which will seriously affect the aggregation of complementary information and also make the fusion algorithm unable to produce satisfactory fusion results under extreme conditions. In this paper, we propose Enlighten-anything, which is able to enhance and fuse the semantic intent of SAM segmentation with low-light images to obtain fused images with good visual perception. The generalization ability of unsupervised learning is greatly improved, and experiments on LOL dataset are conducted to show that our method improves 3db in PSNR over baseline and 8 in SSIM. zero-shot learning of SAM introduces a powerful aid for unsupervised low-light enhancement. The source code of Enlighten-anything can be obtained from https://github.com/zhangbaijin/enlighten-any
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21152;&#36895;&#30697;&#38453;&#23545;&#35282;&#21270;&#65292;&#23558;&#36873;&#25321;&#26368;&#24555;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36807;&#31243;&#35270;&#20026;&#26827;&#30424;&#28216;&#25103;&#65292;&#20026;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.10075</link><description>&lt;p&gt;
&#23558;&#30697;&#38453;&#23545;&#35282;&#21270;&#20316;&#20026;&#19968;&#31181;&#26827;&#30424;&#28216;&#25103;: &#25945;&#25480;&#26412;&#24449;&#20540;&#27714;&#35299;&#22120;&#26368;&#24555;&#30340;&#35299;&#20915;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Matrix Diagonalization as a Board Game: Teaching an Eigensolver the Fastest Path to Solution. (arXiv:2306.10075v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21152;&#36895;&#30697;&#38453;&#23545;&#35282;&#21270;&#65292;&#23558;&#36873;&#25321;&#26368;&#24555;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36807;&#31243;&#35270;&#20026;&#26827;&#30424;&#28216;&#25103;&#65292;&#20026;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#23545;&#35282;&#21270;&#26159;&#31185;&#23398;&#35745;&#31639;&#20013;&#35768;&#22810;&#39046;&#22495;&#30340;&#22522;&#30784;&#12290;&#23545;&#35282;&#21270;&#30697;&#38453;&#20197;&#35299;&#20915;&#29305;&#24449;&#20540;&#38382;&#39064;&#38656;&#35201;&#19968;&#31995;&#21015;&#36845;&#20195;&#65292;&#26368;&#32456;&#36798;&#21040;&#25152;&#26377;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#20805;&#20998;&#25910;&#25947;&#21644;&#31934;&#30830;&#35299;&#20915;&#12290;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992; AlphaZero &#26694;&#26550;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#36873;&#25321;&#26368;&#24555;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36807;&#31243;&#35270;&#20026;&#26827;&#30424;&#28216;&#25103;&#65292;&#20197;&#21152;&#36895; Jacobi &#23545;&#35282;&#21270;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#23558; Jacobi &#23545;&#35282;&#21270;&#31639;&#27861;&#24212;&#29992;&#20110;&#20986;&#29616;&#22312;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#20013;&#30340;&#23545;&#31216;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#24120;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#21152;&#36895;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#25913;&#36827;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#24615;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix diagonalization is at the cornerstone of numerous fields of scientific computing. Diagonalizing a matrix to solve an eigenvalue problem requires a sequential path of iterations that eventually reaches a sufficiently converged and accurate solution for all the eigenvalues and eigenvectors. This typically translates into a high computational cost. Here we demonstrate how reinforcement learning, using the AlphaZero framework, can accelerate Jacobi matrix diagonalizations by viewing the selection of the fastest path to solution as a board game. To demonstrate the viability of our approach we apply the Jacobi diagonalization algorithm to symmetric Hamiltonian matrices that appear in quantum chemistry calculations. We find that a significant acceleration can often be achieved. Our findings highlight the opportunity to use machine learning as a promising tool to improve the performance of numerical linear algebra.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#35774;&#22791;&#20043;&#38388;&#36890;&#36807;&#21327;&#20316;&#23436;&#25104;&#20998;&#25955;&#20219;&#21153;&#12290;&#36890;&#36807;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#30340;&#21327;&#20316;&#22270;&#65292;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09595</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#22270;&#27169;&#22411;&#20808;&#39564;&#19979;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structured Cooperative Learning with Graphical Model Priors. (arXiv:2306.09595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#35774;&#22791;&#20043;&#38388;&#36890;&#36807;&#21327;&#20316;&#23436;&#25104;&#20998;&#25955;&#20219;&#21153;&#12290;&#36890;&#36807;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#30340;&#21327;&#20316;&#22270;&#65292;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#20998;&#25955;&#35774;&#22791;&#19978;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#20010;&#24615;&#21270;&#24314;&#27169;&#65292;&#36825;&#20123;&#35774;&#22791;&#30340;&#23616;&#37096;&#25968;&#25454;&#21463;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#65288;SCooL&#65289;&#8221;&#65292;&#20854;&#20013;&#19968;&#20010;&#36328;&#35774;&#22791;&#30340;&#21327;&#20316;&#22270;&#30001;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#65292;&#20197;&#33258;&#21160;&#21327;&#35843;&#35774;&#22791;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#12290;&#36890;&#36807;&#36873;&#25321;&#26045;&#21152;&#19981;&#21516;&#32467;&#26500;&#30340;&#22270;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#25512;&#23548;&#20986;&#19968;&#31867;&#20016;&#23500;&#30340;&#29616;&#26377;&#21644;&#26032;&#22411;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#31181; SCooL &#30340;&#31034;&#20363;&#65292;&#22312;&#20854;&#20013;&#20197; Dirac &#20998;&#24067;&#12289;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#21644;&#27880;&#24847;&#21147;&#20316;&#20026;&#29983;&#25104;&#21327;&#20316;&#22270;&#30340;&#20808;&#39564;&#12290;&#36825;&#20123; EM &#31867;&#22411;&#30340;&#31639;&#27861;&#36890;&#36807;&#26356;&#26032;&#21327;&#20316;&#22270;&#21644;&#21327;&#21516;&#26412;&#22320;&#27169;&#22411;&#23398;&#20064;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#65292;&#20165;&#36890;&#36807;&#30417;&#35270;&#27169;&#22411;&#26356;&#26032;&#26469;&#20248;&#21270;&#21327;&#20316;&#22270;&#65292;&#20174;&#32780;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to train personalized models for different tasks on decentralized devices with limited local data. We propose "Structured Cooperative Learning (SCooL)", in which a cooperation graph across devices is generated by a graphical model prior to automatically coordinate mutual learning between devices. By choosing graphical models enforcing different structures, we can derive a rich class of existing and novel decentralized learning algorithms via variational inference. In particular, we show three instantiations of SCooL that adopt Dirac distribution, stochastic block model (SBM), and attention as the prior generating cooperation graphs. These EM-type algorithms alternate between updating the cooperation graph and cooperative learning of local models. They can automatically capture the cross-task correlations among devices by only monitoring their model updating in order to optimize the cooperation graph. We evaluate SCooL and compare it with existing decentralized learning met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20256;&#25773;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35270;&#35273;&#20869;&#23481;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.08487</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#24378;&#21270;&#30693;&#35782;&#22270;&#35889;&#20256;&#25773;&#35782;&#21035;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
Recognizing Unseen Objects via Multimodal Intensive Knowledge Graph Propagation. (arXiv:2306.08487v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20256;&#25773;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35270;&#35273;&#20869;&#23481;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;Zero-Shot Learning&#65292;ZSL&#65289;&#26088;&#22312;&#33258;&#21160;&#35782;&#21035;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#65292;&#26159;&#26426;&#22120;&#25345;&#32493;&#29702;&#35299;&#26032;&#30340;&#29616;&#23454;&#19990;&#30028;&#30693;&#35782;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#30693;&#35782;&#22270;&#35889;&#65288;Knowledge Graph, KG&#65289;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#22788;&#29702;&#20855;&#26377;&#22823;&#35268;&#27169;&#21644;&#38750;&#23646;&#24615;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#26041;&#26696;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#23558;&#24050;&#32463;&#30693;&#36947;&#21644;&#26410;&#30693;&#30340;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#23884;&#20837;&#21040;&#24050;&#30693;&#30693;&#35782;&#22270;&#35889;&#30340;&#35270;&#35273;&#20449;&#24687;&#20013;&#65292;&#20197;&#25552;&#39640;&#26410;&#30693;&#25968;&#25454;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#23454;&#38469;&#19978;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#26159;&#30001;&#22810;&#27169;&#24577;&#20107;&#23454;&#33258;&#28982;&#24418;&#25104;&#30340;&#12290;&#19982;&#20165;&#20174;&#22270;&#24418;&#35282;&#24230;&#32771;&#34385;&#30340;&#26222;&#36890;&#32467;&#26500;&#30693;&#35782;&#30456;&#27604;&#65292;&#22810;&#27169;&#24577; KG &#21487;&#20197;&#20026;&#35748;&#30693;&#31995;&#32479;&#25552;&#20379;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;&#20363;&#22914;&#65292;&#25991;&#26412;&#25551;&#36848;&#21644;&#35270;&#35273;&#20869;&#23481;&#21487;&#20197;&#27604;&#20165;&#20381;&#36182;&#20110;&#30693;&#35782;&#19977;&#20803;&#32452;&#25551;&#36848;&#26356;&#22810;&#30340;&#20851;&#38190;&#32454;&#33410;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#20934;&#30340;&#29942;&#39048;&#65292;&#36825;&#31181;&#22810;&#27169;&#24577;&#32454;&#31890;&#24230;&#30340;&#30693;&#35782;&#20027;&#35201;&#26410;&#34987;&#24320;&#21457;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Learning (ZSL), which aims at automatically recognizing unseen objects, is a promising learning paradigm to understand new real-world knowledge for machines continuously. Recently, the Knowledge Graph (KG) has been proven as an effective scheme for handling the zero-shot task with large-scale and non-attribute data. Prior studies always embed relationships of seen and unseen objects into visual information from existing knowledge graphs to promote the cognitive ability of the unseen data. Actually, real-world knowledge is naturally formed by multimodal facts. Compared with ordinary structural knowledge from a graph perspective, multimodal KG can provide cognitive systems with fine-grained knowledge. For example, the text description and visual content can depict more critical details of a fact than only depending on knowledge triplets. Unfortunately, this multimodal fine-grained knowledge is largely unexploited due to the bottleneck of feature alignment between different moda
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04719</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#20320;&#30340;&#30524;&#30555;&#65306;&#20851;&#20110;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#65288;&#19981;&#65289;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#65311;&#29305;&#24449;&#21487;&#35270;&#21270;&#36890;&#36807;&#20248;&#21270;&#26469;&#21487;&#35270;&#21270;&#39640;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#35797;&#22270;&#22238;&#31572;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22914;&#20170;&#65292;&#21487;&#35270;&#21270;&#26041;&#27861;&#26500;&#25104;&#20102;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#30340;&#20102;&#35299;&#30340;&#22522;&#30784;&#65292;&#20316;&#20026;&#19968;&#31181;&#26426;&#26800;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#26377;&#22810;&#21487;&#38752;&#65311;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#32593;&#32476;&#30005;&#36335;&#26469;&#35784;&#39575;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20351;&#20854;&#26174;&#31034;&#23436;&#20840;&#19982;&#33258;&#28982;&#36755;&#20837;&#30340;&#27491;&#24120;&#32593;&#32476;&#34892;&#20026;&#27627;&#26080;&#32852;&#31995;&#30340;&#20219;&#24847;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#22312;&#26631;&#20934;&#65292;&#26410;&#25805;&#32437;&#32593;&#32476;&#20013;&#21457;&#29983;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#19982;&#26631;&#20934;&#36755;&#20837;&#22788;&#29702;&#38750;&#24120;&#19981;&#21516;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#25903;&#25745;&#36825;&#19968;&#32463;&#39564;&#21457;&#29616;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#21487;&#35270;&#21270;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#26497;&#20854;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03361</link><description>&lt;p&gt;
&#35774;&#35745;&#29992;&#25143;&#35282;&#33394;&#24863;&#30693;&#30340;&#23545;&#35805;&#20195;&#29702;&#36827;&#34892;&#26377;&#36259;&#30340;&#23545;&#35805;&#65306;$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground
&lt;/p&gt;
&lt;p&gt;
$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20197;&#35299;&#20915;&#21830;&#19994;&#35774;&#32622;&#20013;&#28041;&#21450;&#20010;&#24615;&#21270;&#23545;&#35805;&#21709;&#24212;&#19982;&#38750;&#27491;&#24335;&#21709;&#24212;&#20132;&#26367;&#30340;$\textit{WWH}$&#65288;$\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20197;&#24212;&#23545;&#20010;&#24615;&#21270;&#12289;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;$\textit{WWH}$&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#21709;&#24212;&#31867;&#22411;&#26631;&#31614;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#32452;&#21512;&#23548;&#33268;&#20102;&#26356;&#21152;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#20027;&#35266;&#20154;&#31867;&#35780;&#20272;&#21644;&#23458;&#35266;&#35780;&#20272;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for building a personalized open-domain dialogue system to address the $\textit{WWH}$ ($\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of $\textit{WWH}$ in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.
&lt;/p&gt;</description></item><item><title>SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01981</link><description>&lt;p&gt;
SGEM&#65306;&#36890;&#36807;&#24207;&#21015;&#32423;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#23454;&#29616;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization. (arXiv:2306.01981v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01981
&lt;/p&gt;
&lt;p&gt;
SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#32463;&#24120;&#26292;&#38706;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#26377;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#20197;&#36866;&#24212;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#23454;&#20363;&#12290;&#23613;&#31649;&#26377;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20165;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#36138;&#24515;&#35299;&#30721;&#65292;&#24182;&#22312;&#24103;&#32423;&#21035;&#19978;&#36328;&#36234;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#35843;&#25972;&#65292;&#36825;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#24207;&#21015;&#24615;&#36136;&#19979;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TTA&#26694;&#26550;&#65292;&#31216;&#20026;SGEM&#65292;&#29992;&#20110;&#19968;&#33324;ASR&#27169;&#22411;&#12290;&#20026;&#20102;&#22788;&#29702;&#24207;&#21015;&#36755;&#20986;&#65292;SGEM&#39318;&#20808;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#26469;&#25506;&#32034;&#20505;&#36873;&#36755;&#20986;&#26631;&#24535;&#65292;&#24182;&#36873;&#25321;&#26368;&#21487;&#20449;&#30340;&#26631;&#24535;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#21644;&#36127;&#25277;&#26679;&#20316;&#20026;&#26080;&#30417;&#30563;&#30446;&#26631;&#26469;&#36866;&#24212;&#27169;&#22411;&#12290;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#36716;&#21464;&#19979;&#65292;SGEM&#23454;&#29616;&#20102;&#19977;&#31181;&#20027;&#27969;ASR&#27169;&#22411;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20108;&#20998;&#22270;&#20013;&#29992;&#25143;&#21644;&#20869;&#23481;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01913</link><description>&lt;p&gt;
PDT: &#38754;&#21521;&#26102;&#38388;&#24863;&#30693;&#30340;&#21452;&#21521;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#29992;&#20110;&#20108;&#20998;&#22270;
&lt;/p&gt;
&lt;p&gt;
PDT: Pretrained Dual Transformers for Time-aware Bipartite Graphs. (arXiv:2306.01913v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01913
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20108;&#20998;&#22270;&#20013;&#29992;&#25143;&#21644;&#20869;&#23481;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#39044;&#20808;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#27491;&#22312;&#26222;&#21450;&#24182;&#28044;&#29616;&#65292;&#38543;&#30528;&#19981;&#26029;&#22686;&#38271;&#30340;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#12290;&#24050;&#32463;&#35748;&#35782;&#21040;&#65292;&#20174;&#25551;&#32472;&#29992;&#25143;&#20869;&#23481;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19978;&#19979;&#25991;&#30693;&#35782;&#23545;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#39044;&#35757;&#32451;&#26041;&#27861;&#23398;&#20064;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#20294;&#20026;&#36825;&#31181;&#20219;&#21153;&#25214;&#21040;&#26368;&#20339;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#34920;&#31034;&#29992;&#25143;-&#20869;&#23481;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#26377;&#20004;&#20010;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26041;&#38754;&#65292;&#21363;&#29992;&#25143;&#26041;&#38754;&#21644;&#20869;&#23481;&#26041;&#38754;&#12290;&#20026;&#20102;&#23398;&#20064;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#29992;&#25143;&#26041;&#38754;&#21644;&#20869;&#23481;&#26041;&#38754;&#20043;&#38388;&#30340;&#21452;&#21521;&#26144;&#23556;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#30446;&#26631;&#21046;&#23450;&#20026;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#37325;Transformer&#26550;&#26500;&#26469;&#32534;&#30721;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on large models is prevalent and emerging with the ever-growing user-generated content in many machine learning application categories. It has been recognized that learning contextual knowledge from the datasets depicting user-content interaction plays a vital role in downstream tasks. Despite several studies attempting to learn contextual knowledge via pre-training methods, finding an optimal training objective and strategy for this type of task remains a challenging problem. In this work, we contend that there are two distinct aspects of contextual knowledge, namely the user-side and the content-side, for datasets where user-content interaction can be represented as a bipartite graph. To learn contextual knowledge, we propose a pre-training method that learns a bi-directional mapping between the spaces of the user-side and the content-side. We formulate the training goal as a contrastive learning task and propose a dual-Transformer architecture to encode the contextual k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; FigGen&#65292;&#19968;&#31181;&#36890;&#36807;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;&#31185;&#23398;&#22270;&#24418;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#25506;&#32034;&#20102;&#25991;&#26412;&#21040;&#22270;&#24418;&#29983;&#25104;&#30340;&#39046;&#22495;&#65292;&#24182;&#35299;&#20915;&#20102;&#35813;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.00800</link><description>&lt;p&gt;
FigGen: &#25991;&#26412;&#21040;&#31185;&#23398;&#22270;&#24418;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FigGen: Text to Scientific Figure Generation. (arXiv:2306.00800v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; FigGen&#65292;&#19968;&#31181;&#36890;&#36807;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;&#31185;&#23398;&#22270;&#24418;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#25506;&#32034;&#20102;&#25991;&#26412;&#21040;&#22270;&#24418;&#29983;&#25104;&#30340;&#39046;&#22495;&#65292;&#24182;&#35299;&#20915;&#20102;&#35813;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#33258;&#28982;&#22270;&#20687;&#21644;&#33402;&#26415;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#25216;&#26415;&#22312;&#21019;&#36896;&#22797;&#26434;&#30340;&#35270;&#35273;&#32452;&#21512;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36924;&#30495;&#24230;&#21644;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19968;&#30452;&#19987;&#27880;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;&#29421;&#31364;&#39046;&#22495;&#65292;&#32780;&#20854;&#20182;&#20998;&#24067;&#21017;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25991;&#26412;&#21040;&#22270;&#24418;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#21363;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21019;&#36896;&#35770;&#25991;&#30340;&#31185;&#23398;&#22270;&#24418;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; FigGen&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#24418;&#25216;&#26415;&#65292;&#20063;&#20171;&#32461;&#20102;&#25152;&#25552;&#20986;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#22312; https://github.com/joanrod/figure-diffusion &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generative modeling landscape has experienced tremendous growth in recent years, particularly in generating natural images and art. Recent techniques have shown impressive potential in creating complex visual compositions while delivering impressive realism and quality. However, state-of-the-art methods have been focusing on the narrow domain of natural images, while other distributions remain unexplored. In this paper, we introduce the problem of text-to-figure generation, that is creating scientific figures of papers from text descriptions. We present FigGen, a diffusion-based approach for text-to-figure as well as the main challenges of the proposed task. Code and models are available at https://github.com/joanrod/figure-diffusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RL+&#27169;&#22411;&#25511;&#21046;&#26694;&#26550;&#20197;&#24320;&#21457;&#20986;&#21487;&#20197;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#20998;&#25955; RL &#36807;&#31243;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#22312;&#36275;&#24335; locomotion &#19978;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#65292;&#33021;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#24182;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.17842</link><description>&lt;p&gt;
RL+&#27169;&#22411;&#25511;&#21046;&#65306;&#20351;&#29992;&#25353;&#38656;&#26368;&#20248;&#25511;&#21046;&#23398;&#20064;&#22810;&#21151;&#33021;&#36275;&#24335; locomotion
&lt;/p&gt;
&lt;p&gt;
RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion. (arXiv:2305.17842v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RL+&#27169;&#22411;&#25511;&#21046;&#26694;&#26550;&#20197;&#24320;&#21457;&#20986;&#21487;&#20197;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#20998;&#25955; RL &#36807;&#31243;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#22312;&#36275;&#24335; locomotion &#19978;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#65292;&#33021;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#24182;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#30340;&#36275;&#24335; locomotion&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#26469;&#22686;&#24378; RL &#35757;&#32451;&#36807;&#31243;&#65292;&#35206;&#30422;&#20102;&#24191;&#27867;&#30340;&#36895;&#24230;&#21644;&#27493;&#24577;&#12290;&#36825;&#20123;&#21442;&#32771;&#36816;&#21160;&#20316;&#20026; RL &#31574;&#30053;&#27169;&#20223;&#30340;&#30446;&#26631;&#65292;&#23548;&#33268;&#24320;&#21457;&#20986;&#21487;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#20840;&#36523;&#21160;&#21147;&#23398;&#65292;RL &#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30828;&#20214;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; RL &#35757;&#32451;&#36807;&#31243;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#30340;&#24378;&#20581;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#21644;&#22788;&#29702;&#21487;&#33021;&#23545;&#31616;&#21270;&#27169;&#22411;&#26500;&#25104;&#25361;&#25112;&#30340;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#20102; RL &#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter presents a control framework that combines model-based optimal control and reinforcement learning (RL) to achieve versatile and robust legged locomotion. Our approach enhances the RL training process by incorporating on-demand reference motions generated through finite-horizon optimal control, covering a broad range of velocities and gaits. These reference motions serve as targets for the RL policy to imitate, resulting in the development of robust control policies that can be learned efficiently and reliably. Moreover, by considering whole-body dynamics, RL overcomes the inherent limitations of modelling simplifications. Through simulation and hardware experiments, we demonstrate the robustness and controllability of the RL training process within our framework. Furthermore, our method demonstrates the ability to generalize reference motions and handle more complex locomotion tasks that may pose challenges for the simplified model, leveraging the flexibility of RL.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#32418;&#22806;&#28909;&#20687;&#25216;&#26415;&#36827;&#34892;&#20083;&#33146;&#30284;&#20998;&#21106;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#65292;&#24182;&#22312;UNet&#32467;&#26500;&#20013;&#20351;&#29992;Grad-CAM&#20998;&#26512;&#20102;&#28508;&#22312;&#20559;&#24046;&#21644;&#24369;&#28857;&#21306;&#22495;&#65292;&#27604;&#29616;&#26377;&#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.14389</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20083;&#33146;&#30284;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#21450;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI. (arXiv:2305.14389v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#32418;&#22806;&#28909;&#20687;&#25216;&#26415;&#36827;&#34892;&#20083;&#33146;&#30284;&#20998;&#21106;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#65292;&#24182;&#22312;UNet&#32467;&#26500;&#20013;&#20351;&#29992;Grad-CAM&#20998;&#26512;&#20102;&#28508;&#22312;&#20559;&#24046;&#21644;&#24369;&#28857;&#21306;&#22495;&#65292;&#27604;&#29616;&#26377;&#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#20581;&#24247;&#23041;&#32961;&#65292;&#30446;&#21069;&#23578;&#26080;&#38271;&#26399;&#27835;&#24840;&#26041;&#27861;&#12290;&#26089;&#26399;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20083;&#33146;X&#32447;&#25668;&#24433;&#30340;&#35299;&#37322;&#21463;&#21040;&#22823;&#37327;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#12290;&#38543;&#30528;&#20083;&#33146;&#30284;&#20363;&#25968;&#26377;&#26395;&#36229;&#36807;&#32954;&#30284;&#65292;&#25913;&#36827;&#26089;&#26399;&#26816;&#27979;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#32418;&#22806;&#28909;&#20687;&#25216;&#26415;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20083;&#33146;&#30284;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#23545;&#22270;&#20687;&#36827;&#34892;&#22686;&#24378;&#21644;&#36827;&#34892;&#30284;&#30151;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21367;&#31215;&#32467;&#26500;&#65288;UNet&#65289;&#29992;&#20110;&#25925;&#38556;&#35782;&#21035;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;Grad-CAM&#65289;&#26469;&#20998;&#26512;&#22312;IRT&#22270;&#20687;&#20013;UNet&#32467;&#26500;&#30340;&#20559;&#24046;&#21644;&#24369;&#28857;&#21306;&#22495;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Breast cancer (BC) remains a significant health threat, with no long-term cure currently available. Early detection is crucial, yet mammography interpretation is hindered by high false positives and negatives. With BC incidence projected to surpass lung cancer, improving early detection methods is vital. Thermography, using high-resolution infrared cameras, offers promise, especially when combined with artificial intelligence (AI). This work presents an attention-based convolutional neural network for segmentation, providing increased speed and precision in BC detection and classification. The system enhances images and performs cancer segmentation with explainable AI. We propose a transformer-attention-based convolutional architecture (UNet) for fault identification and employ Gradient-weighted Class Activation Mapping (Grad-CAM) to analyze areas of bias and weakness in the UNet architecture with IRT images. The superiority of our proposed framework is confirmed when compared with exi
&lt;/p&gt;</description></item><item><title>GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13681</link><description>&lt;p&gt;
GUARD: &#19968;&#20010;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13681
&lt;/p&gt;
&lt;p&gt;
GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#38169;&#30340;&#24615;&#36136;&#65292;&#23558;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#29616;&#23454;&#24212;&#29992;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#20154;&#26426;&#20132;&#20114;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#65289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26368;&#36817;&#65292;&#23433;&#20840;RL&#65288;&#21363;&#32422;&#26463;RL&#65289;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#36805;&#36895;&#20986;&#29616;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#65292;&#25506;&#32034;&#29615;&#22659;&#12290;&#30001;&#20110;&#31639;&#27861;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#27604;&#36739;&#29616;&#26377;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GUARD&#65292;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;GUARD&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#20855;&#26377;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#12290;&#20854;&#27425;&#65292;GUARD&#20840;&#38754;&#28085;&#30422;&#20102;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#33258;&#21253;&#21547;&#30340;&#23454;&#29616;&#12290;&#31532;&#19977;&#65292;GUARD&#22312;&#20219;&#21153;&#21644;&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29366;&#24577;&#19979;&#29616;&#26377;&#26041;&#27861;&#22312;GUARD&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#23398;&#20064;&#36335;&#24452;&#23548;&#33322;&#65288;ALPN&#65289;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23558;&#20851;&#27880;&#30693;&#35782;&#36861;&#36394;&#65288;AKT&#65289;&#27169;&#22411;&#19982;&#22686;&#24378;&#29109;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;EPPO&#65289;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26469;&#20026;&#23398;&#29983;&#25552;&#20379;&#39640;&#24230;&#33258;&#36866;&#24212;&#30340;&#23398;&#20064;&#36335;&#24452;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04475</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#36861;&#36394;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#36335;&#24452;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Adaptive Learning Path Navigation Based on Knowledge Tracing and Reinforcement Learning. (arXiv:2305.04475v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#23398;&#20064;&#36335;&#24452;&#23548;&#33322;&#65288;ALPN&#65289;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23558;&#20851;&#27880;&#30693;&#35782;&#36861;&#36394;&#65288;AKT&#65289;&#27169;&#22411;&#19982;&#22686;&#24378;&#29109;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;EPPO&#65289;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26469;&#20026;&#23398;&#29983;&#25552;&#20379;&#39640;&#24230;&#33258;&#36866;&#24212;&#30340;&#23398;&#20064;&#36335;&#24452;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#23398;&#20064;&#36335;&#24452;&#23548;&#33322;&#65288;ALPN&#65289;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20026;&#23398;&#29983;&#25552;&#20379;&#39640;&#24230;&#33258;&#36866;&#24212;&#30340;&#23398;&#20064;&#36335;&#24452;&#26469;&#22686;&#24378;E-learning&#24179;&#21488;&#12290;ALPN&#31995;&#32479;&#23558;&#20851;&#27880;&#30693;&#35782;&#36861;&#36394;&#65288;AKT&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#35780;&#20272;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#65292;&#19982;&#25552;&#20986;&#30340;&#22686;&#24378;&#29109;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;EPPO&#65289;&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26032;&#31639;&#27861;&#21487;&#20197;&#20248;&#21270;&#23398;&#20064;&#26448;&#26009;&#30340;&#25512;&#33616;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#27169;&#22411;&#21327;&#35843;&#19968;&#33268;&#65292;ALPN&#31995;&#32479;&#23558;&#23398;&#20064;&#36335;&#24452;&#37327;&#36523;&#23450;&#21046;&#20026;&#23398;&#29983;&#30340;&#38656;&#27714;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ALPN&#31995;&#32479;&#22312;&#26368;&#22823;&#21270;&#23398;&#20064;&#32467;&#26524;&#26041;&#38754;&#27604;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#39640;&#20102;8.2&#65285;&#65292;&#32780;&#22312;&#29983;&#25104;&#23398;&#20064;&#36335;&#24452;&#26041;&#38754;&#25552;&#20379;&#20102;10.5&#65285;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#31995;&#32479;&#22312;&#33258;&#36866;&#24212;E-learning&#26041;&#38754;&#26631;&#24535;&#30528;&#37325;&#22823;&#36827;&#23637;&#65292;&#21487;&#33021;&#20250;&#25913;&#21464;&#25968;&#23383;&#21270;&#26102;&#20195;&#30340;&#25945;&#32946;&#26684;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Adaptive Learning Path Navigation (ALPN) system, a novel approach for enhancing E-learning platforms by providing highly adaptive learning paths for students. The ALPN system integrates the Attentive Knowledge Tracing (AKT) model, which assesses students' knowledge states, with the proposed Entropy-enhanced Proximal Policy Optimization (EPPO) algorithm. This new algorithm optimizes the recommendation of learning materials. By harmonizing these models, the ALPN system tailors the learning path to students' needs, significantly increasing learning effectiveness. Experimental results demonstrate that the ALPN system outperforms previous research by 8.2% in maximizing learning outcomes and provides a 10.5% higher diversity in generating learning paths. The proposed system marks a significant advancement in adaptive E-learning, potentially transforming the educational landscape in the digital era.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;SAM&#26469;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;SAM&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#26469;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.11332</link><description>&lt;p&gt;
&#21033;&#29992;SAM&#30340;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;: &#20197;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#20026;&#22522;&#30784;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model. (arXiv:2304.11332v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;SAM&#26469;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;SAM&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#26469;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#26159;&#19968;&#20010;&#26368;&#36817;&#21457;&#23637;&#30340;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;. SAM&#20351;&#29992;&#20102;&#36229;&#36807;1&#20159;&#20010;&#25513;&#27169;&#30340;1100&#19975;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20026;&#33258;&#28982;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#24191;&#27867;&#23545;&#35937;&#29983;&#25104;&#20998;&#21106;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#26679;&#19968;&#20010;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26469;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23613;&#31649;SAM&#24182;&#27809;&#26377;&#31435;&#21363;&#20026;&#21307;&#23398;&#22270;&#20687;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20998;&#21106;&#65292;&#20294;&#20854;&#29983;&#25104;&#30340;&#25513;&#27169;&#12289;&#29305;&#24449;&#21644;&#31283;&#23450;&#24615;&#20998;&#25968;&#23545;&#20110;&#26500;&#24314;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;SAM&#26469;&#22686;&#24378;&#32463;&#20856;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65288;&#22914;U-Net&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is a recently developed large model for general-purpose segmentation for computer vision tasks. SAM was trained using 11 million images with over 1 billion masks and can produce segmentation results for a wide range of objects in natural scene images. SAM can be viewed as a general perception model for segmentation (partitioning images into semantically meaningful regions). Thus, how to utilize such a large foundation model for medical image segmentation is an emerging research target. This paper shows that although SAM does not immediately give high-quality segmentation for medical images, its generated masks, features, and stability scores are useful for building and training better medical image segmentation models. In particular, we demonstrate how to use SAM to augment image inputs for a commonly-used medical image segmentation model (e.g., U-Net). Experiments on two datasets show the effectiveness of our proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22788;&#29702;Wikidata&#38480;&#23450;&#35789;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#38480;&#23450;&#35789;&#36827;&#34892;&#20998;&#31867;&#21644;&#20351;&#29992;&#22810;&#25490;&#24207;&#36923;&#36753;&#35821;&#35328;&#24418;&#24335;&#21270;Wikidata&#27169;&#22411;&#65292;&#20197;&#24212;&#29992;&#20110;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.03375</link><description>&lt;p&gt;
&#25512;&#29702;&#20013;&#22788;&#29702;Wikidata&#38480;&#23450;&#35789;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Handling Wikidata Qualifiers in Reasoning. (arXiv:2304.03375v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22788;&#29702;Wikidata&#38480;&#23450;&#35789;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#38480;&#23450;&#35789;&#36827;&#34892;&#20998;&#31867;&#21644;&#20351;&#29992;&#22810;&#25490;&#24207;&#36923;&#36753;&#35821;&#35328;&#24418;&#24335;&#21270;Wikidata&#27169;&#22411;&#65292;&#20197;&#24212;&#29992;&#20110;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wikidata&#26159;&#19968;&#20010;&#34987;&#35768;&#22810;&#31038;&#21306;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;Wikidata&#35821;&#21477;&#24102;&#26377;&#38480;&#23450;&#35789;&#20540;&#23545;&#65292;&#29992;&#20110;&#25551;&#36848;&#20449;&#24687;&#65292;&#20363;&#22914;&#35821;&#21477;&#30340;&#26377;&#25928;&#19978;&#19979;&#25991;&#65292;&#20854;&#22240;&#26524;&#20851;&#31995;&#65292;&#26469;&#28304;&#31561;&#12290;&#22312;&#25512;&#29702;&#20013;&#22788;&#29702;&#38480;&#23450;&#35789;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23545;&#38480;&#23450;&#31526;&#36827;&#34892;&#20998;&#31867;&#21644;&#23558;Wikidata&#27169;&#22411;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#22810;&#25490;&#24207;&#30340;&#36923;&#36753;&#35821;&#35328;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#35813;&#36923;&#36753;&#19982;&#35821;&#20041;Web&#35268;&#21017;&#35821;&#35328;&#65288;SWRL&#65289;&#30456;&#32467;&#21512;&#65292;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wikidata is a knowledge graph increasingly adopted by many communities for diverse applications. Wikidata statements are annotated with qualifier-value pairs that are used to depict information, such as the validity context of the statement, its causality, provenances, etc. Handling the qualifiers in reasoning is a challenging problem. When defining inference rules (in particular, rules on ontological properties (x subclass of y, z instance of x, etc.)), one must consider the qualifiers, as most of them participate in the semantics of the statements. This poses a complex problem because a) there is a massive number of qualifiers, and b) the qualifiers of the inferred statement are often a combination of the qualifiers in the rule condition. In this work, we propose to address this problem by a) defining a categorization of the qualifiers b) formalizing the Wikidata model with a many-sorted logical language; the sorts of this language are the qualifier categories. We couple this logic w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21521;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#21644;&#21033;&#29992;&#19981;&#24179;&#34913;&#31639;&#27861;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#25913;&#36827;&#21518;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#20998;&#31867;&#20934;&#30830;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#31867;&#65292;&#24615;&#33021;&#25552;&#21319;&#24456;&#22823;&#12290;</title><link>http://arxiv.org/abs/2304.01457</link><description>&lt;p&gt;
&#25506;&#32034;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Vision-Language Models for Imbalanced Learning. (arXiv:2304.01457v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21521;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#21644;&#21033;&#29992;&#19981;&#24179;&#34913;&#31639;&#27861;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#25913;&#36827;&#21518;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#20998;&#31867;&#20934;&#30830;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#31867;&#65292;&#24615;&#33021;&#25552;&#21319;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#31867;&#30340;&#20998;&#24067;&#20542;&#26012;&#65292;&#23548;&#33268;&#22312;&#39044;&#27979;&#23569;&#25968;&#31867;&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#21521;VLM&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#22823;&#37327;&#31867;&#21035;&#23548;&#33268;&#30340;&#20869;&#23384;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#25429;&#25417;&#23614;&#37096;&#31867;&#21035;&#30340;&#24494;&#22937;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#12289;&#24494;&#35843;&#20197;&#21450;&#21152;&#20837;&#19981;&#24179;&#34913;&#31639;&#27861;&#65288;&#20363;&#22914;Focal Loss&#12289;Balanced SoftMax&#21644;Distribution Alignment&#65289;&#26469;&#25913;&#36827;VLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#35299;&#30721;&#22120;&#21644;&#19981;&#24179;&#34913;&#26041;&#27861;&#26102;&#65292;VLM&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25913;&#36827;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#24179;&#22343;&#25552;&#39640;&#20102;6.58%&#12289;69.82%&#21644;10.43%&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid OOM (out of memory) problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24615;&#28508;&#22312;&#25193;&#25955;&#30340;&#20004;&#38454;&#27573;&#22330;&#26223;&#37325;&#24314;&#26694;&#26550;&#65292;&#31216;&#20316;&#8220;Brain-Diffuser&#8221;&#65292;&#33021;&#22815;&#20174;fMRI&#20449;&#21495;&#37325;&#24314;&#22797;&#26434;&#22330;&#26223;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.05334</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#24615;&#28508;&#22312;&#25193;&#25955;&#30340;fMRI&#20449;&#21495;&#33258;&#28982;&#22330;&#26223;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Natural scene reconstruction from fMRI signals using generative latent diffusion. (arXiv:2303.05334v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24615;&#28508;&#22312;&#25193;&#25955;&#30340;&#20004;&#38454;&#27573;&#22330;&#26223;&#37325;&#24314;&#26694;&#26550;&#65292;&#31216;&#20316;&#8220;Brain-Diffuser&#8221;&#65292;&#33021;&#22815;&#20174;fMRI&#20449;&#21495;&#37325;&#24314;&#22797;&#26434;&#22330;&#26223;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#35299;&#30721;&#30740;&#31350;&#20013;&#65292;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#35805;&#39064;&#20043;&#19968;&#26159;&#22522;&#20110;fMRI&#20449;&#21495;&#37325;&#24314;&#24863;&#30693;&#21040;&#30340;&#33258;&#28982;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25104;&#21151;&#22320;&#37325;&#26032;&#21019;&#24314;&#20102;&#35270;&#35273;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#20363;&#22914;&#20302;&#27700;&#24179;&#23646;&#24615;&#65288;&#24418;&#29366;&#12289;&#32441;&#29702;&#12289;&#24067;&#23616;&#65289;&#25110;&#39640;&#27700;&#24179;&#29305;&#24449;&#65288;&#23545;&#35937;&#31867;&#21035;&#12289;&#22330;&#26223;&#32454;&#33410;&#35821;&#20041;&#65289;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#21516;&#26102;&#20026;&#22797;&#26434;&#30340;&#22330;&#26223;&#22270;&#20687;&#37325;&#24314;&#36825;&#20123;&#23646;&#24615;&#12290;&#29983;&#25104;AI&#26368;&#36817;&#36890;&#36807;&#33021;&#22815;&#29983;&#25104;&#39640;&#22797;&#26434;&#24230;&#22270;&#20687;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#21019;&#26032;&#25216;&#26415;&#36827;&#34892;&#33041;&#35299;&#30721;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Brain-Diffuser&#8221;&#30340;&#20004;&#38454;&#27573;&#22330;&#26223;&#37325;&#24314;&#26694;&#26550;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#20174;fMRI&#20449;&#21495;&#24320;&#22987;&#65292;&#25105;&#20204;&#20351;&#29992;VDVAE&#65288;&#38750;&#24120;&#28145;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65289;&#27169;&#22411;&#37325;&#24314;&#22270;&#20687;&#65292;&#25429;&#25417;&#20302;&#27700;&#24179;&#23646;&#24615;&#21644;&#24635;&#20307;&#24067;&#23616;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#26694;&#26550;&#26469;&#25552;&#21462;&#26356;&#39640;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#37325;&#24314;&#22797;&#26434;&#30340;&#22330;&#26223;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In neural decoding research, one of the most intriguing topics is the reconstruction of perceived natural images based on fMRI signals. Previous studies have succeeded in re-creating different aspects of the visuals, such as low-level properties (shape, texture, layout) or high-level features (category of objects, descriptive semantics of scenes) but have typically failed to reconstruct these properties together for complex scene images. Generative AI has recently made a leap forward with latent diffusion models capable of generating high-complexity images. Here, we investigate how to take advantage of this innovative technology for brain decoding. We present a two-stage scene reconstruction framework called ``Brain-Diffuser''. In the first stage, starting from fMRI signals, we reconstruct images that capture low-level properties and overall layout using a VDVAE (Very Deep Variational Autoencoder) model. In the second stage, we use the image-to-image framework of a latent diffusion mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; CoHiClust &#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29983;&#25104;&#19982;&#25105;&#20204;&#30452;&#35273;&#21644;&#22270;&#20687;&#35821;&#20041;&#30456;&#31526;&#30340;&#21512;&#29702;&#32858;&#31867;&#32467;&#26500;&#65292;&#19988;&#22312;&#22823;&#37096;&#20998;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24179;&#38754;&#32858;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.03389</link><description>&lt;p&gt;
&#23545;&#27604;&#23618;&#27425;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Contrastive Hierarchical Clustering. (arXiv:2303.03389v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; CoHiClust &#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29983;&#25104;&#19982;&#25105;&#20204;&#30452;&#35273;&#21644;&#22270;&#20687;&#35821;&#20041;&#30456;&#31526;&#30340;&#21512;&#29702;&#32858;&#31867;&#32467;&#26500;&#65292;&#19988;&#22312;&#22823;&#37096;&#20998;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24179;&#38754;&#32858;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32858;&#31867;&#19968;&#30452;&#34987;&#24179;&#38754;&#27169;&#22411;&#25152;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25968;&#25454;&#38598;&#20998;&#25104;&#39044;&#23450;&#20041;&#25968;&#37327;&#30340;&#32452;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#22522;&#26412;&#20107;&#23454;&#30340;&#30456;&#20284;&#24230;&#38750;&#24120;&#39640;&#65292;&#20294;&#24179;&#38754;&#20998;&#21306;&#25552;&#20379;&#30340;&#20449;&#24687;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411; CoHiClust&#65292;&#21487;&#24212;&#29992;&#20110;&#20856;&#22411;&#22270;&#20687;&#25968;&#25454;&#12290;&#36890;&#36807;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;CoHiClust &#21487;&#20197;&#23558;&#22522;&#30784;&#32593;&#32476;&#33976;&#39311;&#20026;&#20108;&#21449;&#26641;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#23618;&#27425;&#32858;&#31867;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#32858;&#31867;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#34913;&#37327;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoHiClust &#29983;&#25104;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#31526;&#21512;&#25105;&#20204;&#30340;&#30452;&#35273;&#21644;&#22270;&#20687;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#23427;&#19982;&#26368;&#20808;&#36827;&#30340;&#24179;&#38754;&#32858;&#31867;&#30456;&#27604;&#65292;&#22312;&#22823;&#22810;&#25968;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep clustering has been dominated by flat models, which split a dataset into a predefined number of groups. Although recent methods achieve an extremely high similarity with the ground truth on popular benchmarks, the information contained in the flat partition is limited. In this paper, we introduce CoHiClust, a Contrastive Hierarchical Clustering model based on deep neural networks, which can be applied to typical image data. By employing a self-supervised learning approach, CoHiClust distills the base network into a binary tree without access to any labeled data. The hierarchical clustering structure can be used to analyze the relationship between clusters, as well as to measure the similarity between data points. Experiments demonstrate that CoHiClust generates a reasonable structure of clusters, which is consistent with our intuition and image semantics. Moreover, it obtains superior clustering accuracy on most of the image datasets compared to the state-of-the-art flat clusterin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.06403</link><description>&lt;p&gt;
&#29616;&#35937;&#24847;&#35782;&#29366;&#24577;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#30340;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
Sources of Richness and Ineffability for Phenomenally Conscious States. (arXiv:2302.06403v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In their framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing.
&lt;/p&gt;
&lt;p&gt;
&#24847;&#35782;&#29366;&#24577;&#65288;&#21363;&#26377;&#26576;&#31181;&#24863;&#21463;&#30340;&#29366;&#24577;&#65289;&#20284;&#20046;&#26082;&#20016;&#23500;&#21448;&#20805;&#28385;&#32454;&#33410;&#65292;&#21448;&#38590;&#20197;&#23436;&#20840;&#25551;&#36848;&#25110;&#22238;&#24518;&#12290;&#29305;&#21035;&#26159;&#38590;&#20197;&#35328;&#35828;&#24615;&#30340;&#38382;&#39064;&#26159;&#21746;&#23398;&#19978;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#37096;&#20998;&#28608;&#21457;&#20102;&#35299;&#37322;&#40511;&#27807;&#30340;&#20449;&#24565;&#65306;&#24847;&#35782;&#19981;&#33021;&#24402;&#32467;&#20026;&#22522;&#30784;&#29289;&#29702;&#36807;&#31243;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#24037;&#20316;&#35760;&#24518;&#20013;&#30340;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#22914;&#20309;&#23548;&#33268;&#25105;&#20204;&#21407;&#22987;&#20307;&#39564;&#30340;&#36139;&#20047;&#22238;&#24518;&#65292;&#35821;&#35328;&#30340;&#31163;&#25955;&#31526;&#21495;&#24615;&#36136;&#19981;&#36275;&#20197;&#25551;&#36848;&#20307;&#39564;&#30340;&#20016;&#23500;&#21644;&#39640;&#32500;&#32467;&#26500;&#65292;&#20197;&#21450;&#35748;&#30693;&#21151;&#33021;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#20307;&#39564;&#30340;&#20849;&#20139;&#21644;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conscious states (states that there is something it is like to be in) seem both rich or full of detail, and ineffable or hard to fully describe or recall. The problem of ineffability, in particular, is a longstanding issue in philosophy that partly motivates the explanatory gap: the belief that consciousness cannot be reduced to underlying physical processes. Here, we provide an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In our framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing. We describe how attractor dynamics in working memory would induce impoverished recollections of our original experiences, how the discrete symbolic nature of language is insufficient for describing the rich and high-dimensional structure of experiences, and how similarity in the cognitive function o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; PLay&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#25351;&#21335;&#22312;&#30690;&#37327;&#22270;&#31354;&#38388;&#20013;&#29983;&#25104;&#21442;&#25968;&#21270;&#26465;&#20214;&#24067;&#23616;&#12290;&#30456;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#20026;&#19987;&#19994;&#24067;&#23616;&#35774;&#35745;&#24102;&#26469;&#20102;&#26032;&#39062;&#21644;&#20114;&#21160;&#30340;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2301.11529</link><description>&lt;p&gt;
PLay&#65306;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#29983;&#25104;&#21442;&#25968;&#26465;&#20214;&#21270;&#30340;&#24067;&#23616;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
PLay: Parametrically Conditioned Layout Generation using Latent Diffusion. (arXiv:2301.11529v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; PLay&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#25351;&#21335;&#22312;&#30690;&#37327;&#22270;&#31354;&#38388;&#20013;&#29983;&#25104;&#21442;&#25968;&#21270;&#26465;&#20214;&#24067;&#23616;&#12290;&#30456;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#20026;&#19987;&#19994;&#24067;&#23616;&#35774;&#35745;&#24102;&#26469;&#20102;&#26032;&#39062;&#21644;&#20114;&#21160;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23616;&#35774;&#35745;&#26159;&#21508;&#31181;&#35774;&#35745;&#39046;&#22495;&#65288;&#21253;&#25324;&#29992;&#25143;&#30028;&#38754;&#12289;&#25991;&#26723;&#21644;&#22270;&#24418;&#35774;&#35745;&#65289;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#30001;&#20110;&#27492;&#20219;&#21153;&#38656;&#35201;&#35774;&#35745;&#24072;&#20184;&#20986;&#32321;&#29712;&#30340;&#25163;&#21160;&#21162;&#21147;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#23581;&#35797;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#30452;&#35266;&#30340;&#29992;&#25143;&#25511;&#21046;&#21644;&#23454;&#29616;&#35774;&#35745;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; PLay&#65292;&#23427;&#20174;&#29992;&#25143;&#25351;&#23450;&#30340;&#25351;&#21335;&#20013;&#29983;&#25104;&#30690;&#37327;&#22270;&#31354;&#38388;&#20013;&#30340;&#21442;&#25968;&#21270;&#26465;&#20214;&#24067;&#23616;&#12290;&#36825;&#20123;&#25351;&#21335;&#36890;&#24120;&#30001;&#35774;&#35745;&#24072;&#22312;&#24403;&#21069;&#23454;&#36341;&#20013;&#29992;&#20110;&#34920;&#31034;&#20182;&#20204;&#30340;&#35774;&#35745;&#24847;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324; FID &#21644; FD-VG &#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#29992;&#25143;&#30740;&#31350;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#23427;&#20026;&#19987;&#19994;&#24067;&#23616;&#35774;&#35745;&#27969;&#31243;&#24102;&#26469;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#20114;&#21160;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layout design is an important task in various design fields, including user interface, document, and graphic design. As this task requires tedious manual effort by designers, prior works have attempted to automate this process using generative models, but commonly fell short of providing intuitive user controls and achieving design objectives. In this paper, we build a conditional latent diffusion model, PLay, that generates parametrically conditioned layouts in vector graphic space from user-specified guidelines, which are commonly used by designers for representing their design intents in current practices. Our method outperforms prior works across three datasets on metrics including FID and FD-VG, and in user study. Moreover, it brings a novel and interactive experience to professional layout design processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRDU&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#26356;&#20840;&#38754;&#22320;&#21453;&#26144;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20016;&#23500;&#27169;&#24335;&#12289;&#22797;&#26434;&#27169;&#26495;&#21644;&#22810;&#26679;&#30340;&#24067;&#23616;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.15421</link><description>&lt;p&gt;
VRDU&#65306;&#38754;&#21521;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#29702;&#35299;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
VRDU: A Benchmark for Visually-rich Document Understanding. (arXiv:2211.15421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRDU&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#26356;&#20840;&#38754;&#22320;&#21453;&#26144;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20016;&#23500;&#27169;&#24335;&#12289;&#22797;&#26434;&#27169;&#26495;&#21644;&#22810;&#26679;&#30340;&#24067;&#23616;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20016;&#23500;&#35270;&#35273;&#21270;&#19994;&#21153;&#25991;&#26723;&#20197;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#33258;&#21160;&#21270;&#19994;&#21153;&#24037;&#20316;&#27969;&#31243;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#21463;&#21040;&#20851;&#27880;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#21453;&#26144;&#24037;&#19994;&#20013;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Visually Rich Document Understanding (VRDU)&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;VRDU&#21253;&#21547;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20195;&#34920;&#20102;&#22810;&#31181;&#25361;&#25112;&#65306;&#20016;&#23500;&#30340;&#27169;&#24335;&#65292;&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#20197;&#21450;&#20998;&#23618;&#23454;&#20307;; &#22797;&#26434;&#30340;&#27169;&#26495;&#65292;&#21253;&#25324;&#34920;&#26684;&#21644;&#22810;&#21015;&#24067;&#23616;; &#20197;&#21450;&#21333;&#20010;&#25991;&#26723;&#31867;&#22411;&#20013;&#19981;&#21516;&#24067;&#23616;&#65288;&#27169;&#26495;&#65289;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#23569;&#26679;&#26412;&#21644;&#24120;&#35268;&#23454;&#39564;&#35774;&#32622;&#65292;&#20197;&#21450;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#21305;&#37197;&#31639;&#27861;&#26469;&#35780;&#20272;&#25552;&#21462;&#32467;&#26524;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24378;&#22522;&#32447;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#35266;&#23519;&#32467;&#26524;&#65306;(1)&#36890;&#29992;n&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as hierarchical entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and offer three observations: (1) generalizing to n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DeepIPC&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#24863;&#30693;&#21644;&#25511;&#21046;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeepIPC&#20855;&#26377;&#26368;&#20339;&#30340;&#21487;&#39550;&#24615;&#21644;&#22810;&#20219;&#21153;&#24615;&#33021;&#65292;&#29978;&#33267;&#27604;&#20854;&#20182;&#27169;&#22411;&#25152;&#38656;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2207.09934</link><description>&lt;p&gt;
DeepIPC&#65306;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#28145;&#24230;&#24863;&#30693;&#21644;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DeepIPC: Deeply Integrated Perception and Control for an Autonomous Vehicle in Real Environments. (arXiv:2207.09934v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DeepIPC&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#24863;&#30693;&#21644;&#25511;&#21046;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeepIPC&#20855;&#26377;&#26368;&#20339;&#30340;&#21487;&#39550;&#24615;&#21644;&#22810;&#20219;&#21153;&#24615;&#33021;&#65292;&#29978;&#33267;&#27604;&#20854;&#20182;&#27169;&#22411;&#25152;&#38656;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;DeepIPC&#65292;&#19968;&#20010;&#22788;&#29702;&#33258;&#21160;&#39550;&#39542;&#20013;&#24863;&#30693;&#21644;&#25511;&#21046;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30001;&#24863;&#30693;&#21644;&#25511;&#21046;&#22120;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#32452;&#25104;&#12290;&#24863;&#30693;&#27169;&#22359;&#20351;&#29992;RGBD&#22270;&#20687;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#40479;&#30640;&#22270;&#35821;&#20041;&#26144;&#23556;&#65292;&#24182;&#25552;&#20379;&#23427;&#20204;&#30340;&#32534;&#30721;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25511;&#21046;&#22120;&#27169;&#22359;&#20351;&#29992;GNSS&#23450;&#20301;&#21644;&#35282;&#36895;&#24230;&#30340;&#27979;&#37327;&#26469;&#22788;&#29702;&#36825;&#20123;&#29305;&#24449;&#65292;&#20272;&#35745;&#19968;&#31995;&#21015;&#36335;&#24452;&#28857;&#21644;&#28508;&#22312;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#20195;&#29702;&#23558;&#36335;&#24452;&#28857;&#21644;&#28508;&#22312;&#29305;&#24449;&#36716;&#25442;&#20026;&#19968;&#32452;&#23548;&#33322;&#25511;&#21046;&#65292;&#20197;&#39537;&#21160;&#36710;&#36742;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#34892;&#36710;&#35760;&#24405;&#21644;&#22312;&#19981;&#21516;&#30495;&#23454;&#22330;&#26223;&#19979;&#36827;&#34892;&#33258;&#21160;&#39550;&#39542;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeepIPC&#21363;&#20351;&#20351;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#20063;&#33021;&#23454;&#29616;&#26368;&#20339;&#21487;&#39550;&#24615;&#21644;&#22810;&#20219;&#21153;&#24615;&#33021;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#30456;&#20851;&#20195;&#30721;&#21487;&#22312; https://github.com &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose DeepIPC, an end-to-end autonomous driving model that handles both perception and control tasks in driving a vehicle. The model consists of two main parts, perception and controller modules. The perception module takes an RGBD image to perform semantic segmentation and bird's eye view (BEV) semantic mapping along with providing their encoded features. Meanwhile, the controller module processes these features with the measurement of GNSS locations and angular speed to estimate waypoints that come with latent features. Then, two different agents are used to translate waypoints and latent features into a set of navigational controls to drive the vehicle. The model is evaluated by predicting driving records and performing automated driving under various conditions in real environments. The experimental results show that DeepIPC achieves the best drivability and multi-task performance even with fewer parameters compared to the other models. Codes are available at https://github.co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#21551;&#21160;&#20102;&#23545;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#19978;&#38543;&#26426;&#20248;&#21270;&#30340;&#31995;&#32479;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#24230;&#37327;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#19982;&#31181;&#32676;&#26799;&#24230;&#21644;&#32463;&#39564;&#26799;&#24230;&#20043;&#38388;&#30340;&#23450;&#37327;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2206.07082</link><description>&lt;p&gt;
&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#20013;&#38543;&#26426;&#20248;&#21270;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Stability and Generalization of Stochastic Optimization with Nonconvex and Nonsmooth Problems. (arXiv:2206.07082v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#21551;&#21160;&#20102;&#23545;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#19978;&#38543;&#26426;&#20248;&#21270;&#30340;&#31995;&#32479;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#24230;&#37327;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#19982;&#31181;&#32676;&#26799;&#24230;&#21644;&#32463;&#39564;&#26799;&#24230;&#20043;&#38388;&#30340;&#23450;&#37327;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20248;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#38750;&#24120;&#24191;&#27867;&#65292;&#36825;&#20419;&#20351;&#20102;&#35768;&#22810;&#29702;&#35770;&#30740;&#31350;&#65292;&#20197;&#29702;&#35299;&#20854;&#23454;&#38469;&#25104;&#21151;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20248;&#21270;&#35823;&#24046;&#30340;&#25910;&#25947;&#24615;&#19978;&#65292;&#32780;&#38543;&#26426;&#20248;&#21270;&#30340;&#27867;&#21270;&#20998;&#26512;&#36828;&#36828;&#28382;&#21518;&#12290;&#36825;&#23588;&#20854;&#36866;&#29992;&#20110;&#23454;&#36341;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#21551;&#21160;&#20102;&#23545;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#19978;&#38543;&#26426;&#20248;&#21270;&#30340;&#31995;&#32479;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#24230;&#37327;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#19982;&#31181;&#32676;&#26799;&#24230;&#21644;&#32463;&#39564;&#26799;&#24230;&#20043;&#38388;&#30340;&#23450;&#37327;&#36830;&#25509;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#23558;&#20854;&#25193;&#23637;&#21040;&#30740;&#31350;&#32463;&#39564;&#39118;&#38505;&#21644;&#32676;&#20307;&#39118;&#38505;&#30340;Moreau&#21253;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20123;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#23450;&#37327;&#32852;&#31995;&#65292;&#26080;&#35770;&#26159;&#22312;&#26799;&#24230;&#36824;&#26159;Moreau&#21253;&#32476;&#26041;&#38754;&#65292;&#37117;&#26159;&#25991;&#29486;&#20013;&#39318;&#27425;&#20986;&#29616;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#27867;&#21270;&#29702;&#35770;&#24212;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;(DLNN)&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#24037;&#20316;&#25552;&#20379;&#26356;&#32039;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimization has found wide applications in minimizing objective functions in machine learning, which motivates a lot of theoretical studies to understand its practical success. Most of existing studies focus on the convergence of optimization errors, while the generalization analysis of stochastic optimization is much lagging behind. This is especially the case for nonconvex and nonsmooth problems often encountered in practice. In this paper, we initialize a systematic stability and generalization analysis of stochastic optimization on nonconvex and nonsmooth problems. We introduce novel algorithmic stability measures and establish their quantitative connection on the gap between population gradients and empirical gradients, which is then further extended to study the gap between the Moreau envelope of the empirical risk and that of the population risk. To our knowledge, these quantitative connection between stability and generalization in terms of either gradients or Morea
&lt;/p&gt;</description></item><item><title>AIGenC &#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#36896;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#24565;&#31354;&#38388;&#21644;&#20998;&#23618;&#32467;&#26500;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#36890;&#29992;&#24615;&#21644;&#21019;&#26032;&#33021;&#21147;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.09738</link><description>&lt;p&gt;
AIGenC&#65306;&#36890;&#36807;&#21019;&#36896;&#21147;&#36827;&#34892; AI &#36890;&#29992;&#21270;
&lt;/p&gt;
&lt;p&gt;
AIGenC: AI generalisation via creativity. (arXiv:2205.09738v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09738
&lt;/p&gt;
&lt;p&gt;
AIGenC &#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#36896;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#24565;&#31354;&#38388;&#21644;&#20998;&#23618;&#32467;&#26500;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#36890;&#29992;&#24615;&#21644;&#21019;&#26032;&#33021;&#21147;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#21040;&#21019;&#36896;&#21147;&#30340;&#35748;&#30693;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65288;AIGenC&#65289;&#65292;&#26088;&#22312;&#25552;&#20379;&#24517;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#12289;&#20351;&#29992;&#21644;&#29983;&#25104;&#21487;&#36716;&#31227;&#30340;&#34920;&#24449;&#12290;&#19982;&#26426;&#22120;&#34920;&#24449;&#23398;&#20064;&#19981;&#21516;&#30340;&#26159;&#65292;&#29983;&#29289;&#34920;&#24449;&#21253;&#25324;&#20851;&#31995;&#21644;&#32852;&#24819;&#20449;&#24687;&#65292;&#23884;&#20837;&#20102;&#20016;&#23500;&#21644;&#32467;&#26500;&#21270;&#30340;&#27010;&#24565;&#31354;&#38388;&#12290;AIGenC &#27169;&#22411;&#37319;&#29992;&#20998;&#23618;&#30340;&#22270;&#24418;&#26550;&#26500;&#65292;&#20855;&#26377;&#19981;&#21516;&#32452;&#20214;&#33719;&#21462;&#30340;&#21508;&#31181;&#32423;&#21035;&#21644;&#31867;&#22411;&#30340;&#34920;&#24449;&#12290;&#31532;&#19968;&#20010;&#32452;&#20214;&#37096;&#20998;&#8212;&#8212;&#27010;&#24565;&#22788;&#29702;&#65292;&#20174;&#24863;&#23448;&#36755;&#20837;&#20013;&#25552;&#21462;&#23545;&#35937;&#21644;&#25903;&#37197;&#22240;&#32032;&#65292;&#24182;&#23558;&#23427;&#20204;&#32534;&#30721;&#25104;&#27010;&#24565;&#31354;&#38388;&#12290;&#29983;&#25104;&#30340;&#34920;&#24449;&#23384;&#20648;&#22312;&#21452;&#37325;&#35760;&#24518;&#31995;&#32479;&#20013;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30446;&#26631;&#23548;&#21521;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#20016;&#23500;&#24230;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#26356;&#39640;&#23618;&#27425;&#30340;&#25277;&#35937;&#12290;&#21478;&#22806;&#20004;&#20010;&#32452;&#20214;&#24182;&#34892;&#24037;&#20316;&#65292;&#26816;&#27979;&#21644;&#24674;&#22797;&#23384;&#20648;&#34920;&#24449;&#20013;&#30340;&#30456;&#20851;&#27010;&#24565;&#21644;&#21019;&#36896;&#36830;&#25509;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#29983;&#25104;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#24182;&#22312;&#26032;&#22330;&#26223;&#20013;&#24212;&#29992;&#23427;&#20204;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AIGenC &#22312;&#21508;&#31181;&#36890;&#29992;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by cognitive theories of creativity, this paper introduces a computational model (AIGenC) that lays down the necessary components to enable artificial agents to learn, use and generate transferable representations. Unlike machine representation learning, which relies exclusively on raw sensory data, biological representations incorporate relational and associative information that embeds rich and structured concept spaces. The AIGenC model poses a hierarchical graph architecture with various levels and types of representations procured by different components. The first component, Concept Processing, extracts objects and affordances from sensory input and encodes them into a concept space. The resulting representations are stored in a dual memory system and enriched with goal-directed and temporal information acquired through reinforcement learning, creating a higher-level of abstraction. Two additional components work in parallel to detect and recover relevant concepts and cr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;QHD&#31163;&#32447;&#31574;&#30053;&#65292;&#23427;&#22522;&#20110;&#36229;&#32500;&#24378;&#21270;&#23398;&#20064;&#19982;&#33041;&#21551;&#21457;&#35745;&#31639;&#65292;&#23454;&#29616;&#31283;&#20581;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;QHD&#30456;&#27604;&#20110;DQN&#20855;&#26377;&#26356;&#39640;&#25928;&#29575;&#19988;&#36866;&#29992;&#20110;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#26377;&#22312;&#32447;&#21644;&#23454;&#26102;&#23398;&#20064;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2205.06978</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing. (arXiv:2205.06978v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;QHD&#31163;&#32447;&#31574;&#30053;&#65292;&#23427;&#22522;&#20110;&#36229;&#32500;&#24378;&#21270;&#23398;&#20064;&#19982;&#33041;&#21551;&#21457;&#35745;&#31639;&#65292;&#23454;&#29616;&#31283;&#20581;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;QHD&#30456;&#27604;&#20110;DQN&#20855;&#26377;&#26356;&#39640;&#25928;&#29575;&#19988;&#36866;&#29992;&#20110;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#26377;&#22312;&#32447;&#21644;&#23454;&#26102;&#23398;&#20064;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#24320;&#21019;&#20102;&#22686;&#24378;&#29616;&#26377;&#26234;&#33021;&#31995;&#32479;&#30340;&#26032;&#26426;&#20250;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#21253;&#25324;&#22797;&#26434;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;RL&#31639;&#27861;&#65292;&#22914;Deep Q-Networks&#65288;DQN&#65289;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QHD&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#32500;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#32447;&#31574;&#30053;&#65292;&#27169;&#20223;&#22823;&#33041;&#30340;&#23646;&#24615;&#65292;&#23454;&#29616;&#31283;&#20581;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;QHD&#20381;&#38752;&#36731;&#37327;&#32423;&#30340;&#33041;&#21551;&#21457;&#27169;&#22411;&#65292;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26700;&#38754;&#21644;&#21151;&#29575;&#38480;&#21046;&#30340;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#65292;QHD&#30340;&#25972;&#20307;&#25928;&#29575;&#26174;&#33879;&#20248;&#20110;DQN&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#39640;&#25110;&#21487;&#27604;&#36739;&#30340;&#22238;&#25253;&#12290;QHD&#20063;&#36866;&#29992;&#20110;&#39640;&#24230;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#26377;&#26497;&#22823;&#30340;&#22312;&#32447;&#21644;&#23454;&#26102;&#23398;&#20064;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#25903;&#25345;&#23567;&#30340;&#32463;&#39564;&#37325;&#25918;&#25209;&#37327;&#65292;&#19982;DQN&#30456;&#27604;&#25552;&#20379;12.3&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#30830;&#20445;&#26368;&#23567;&#30340;&#36136;&#37327;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;QHD&#19982;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has opened up new opportunities to enhance existing smart systems that generally include a complex decision-making process. However, modern RL algorithms, e.g., Deep Q-Networks (DQN), are based on deep neural networks, resulting in high computational costs. In this paper, we propose QHD, an off-policy value-based Hyperdimensional Reinforcement Learning, that mimics brain properties toward robust and real-time learning. QHD relies on a lightweight brain-inspired model to learn an optimal policy in an unknown environment. On both desktop and power-limited embedded platforms, QHD achieves significantly better overall efficiency than DQN while providing higher or comparable rewards. QHD is also suitable for highly-efficient reinforcement learning with great potential for online and real-time learning. Our solution supports a small experience replay batch size that provides 12.3 times speedup compared to DQN while ensuring minimal quality loss. Our evaluation sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#26799;&#24230;&#21152;&#26435;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.01464</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#26799;&#24230;&#21152;&#26435;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Value Gradient weighted Model-Based Reinforcement Learning. (arXiv:2204.01464v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#26799;&#24230;&#21152;&#26435;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#26159;&#19968;&#31181;&#39640;&#25928;&#33719;&#21462;&#25511;&#21046;&#31574;&#30053;&#30340;&#25216;&#26415;&#65292;&#20294;&#27169;&#22411;&#35823;&#24046;&#24448;&#24448;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;MBRL&#20013;&#30340;&#27169;&#22411;&#36890;&#24120;&#20165;&#29992;&#20110;&#37325;&#24314;&#21160;&#24577;&#65292;&#29305;&#21035;&#26159;&#29366;&#24577;&#35266;&#23519;&#20540;&#65292;&#32780;&#27169;&#22411;&#35823;&#24046;&#23545;&#31574;&#30053;&#30340;&#24433;&#21709;&#19981;&#20250;&#34987;&#35757;&#32451;&#30446;&#26631;&#25429;&#25417;&#21040;&#12290;&#36825;&#23548;&#33268;MBRL&#30340;&#30446;&#26631;&#19982;&#23454;&#38469;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#24433;&#21709;&#31574;&#30053;&#21644;&#20215;&#20540;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#26799;&#24230;&#21152;&#26435;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;VaGraM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeepMVI&#65292;&#19968;&#31181;&#29992;&#20110;&#22635;&#20805;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#32570;&#22833;&#20540;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#31934;&#32454;&#21644;&#31895;&#31890;&#24230;&#27169;&#24335;&#65292;&#20197;&#21450;&#36328;&#20998;&#31867;&#32500;&#24230;&#30340;&#30456;&#20851;&#31995;&#21015;&#36235;&#21183;&#12290;&#32463;&#36807;&#25913;&#36827;&#65292;DeepMVI&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2103.01600</link><description>&lt;p&gt;
&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#30340;&#32570;&#22833;&#20540;&#22635;&#20805;&#26041;&#27861;DeepMVI&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Missing Value Imputation on Multidimensional Time Series. (arXiv:2103.01600v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.01600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeepMVI&#65292;&#19968;&#31181;&#29992;&#20110;&#22635;&#20805;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#32570;&#22833;&#20540;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#31934;&#32454;&#21644;&#31895;&#31890;&#24230;&#27169;&#24335;&#65292;&#20197;&#21450;&#36328;&#20998;&#31867;&#32500;&#24230;&#30340;&#30456;&#20851;&#31995;&#21015;&#36235;&#21183;&#12290;&#32463;&#36807;&#25913;&#36827;&#65292;DeepMVI&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeepMVI&#65292;&#19968;&#31181;&#29992;&#20110;&#22635;&#20805;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#32570;&#22833;&#20540;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#30001;&#20110;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#38271;&#26102;&#38388;&#25968;&#25454;&#32858;&#21512;&#20250;&#20135;&#29983;&#22823;&#37327;&#32570;&#22833;&#25968;&#25454;&#65292;&#22240;&#27492;&#23545;&#20110;&#21487;&#38752;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#38656;&#35201;&#20180;&#32454;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#31934;&#32454;&#21644;&#31895;&#31890;&#24230;&#27169;&#24335;&#65292;&#20197;&#21450;&#36328;&#20998;&#31867;&#32500;&#24230;&#30340;&#30456;&#20851;&#31995;&#21015;&#36235;&#21183;&#12290;&#22312;&#32463;&#36807;&#22810;&#27425;&#19981;&#25104;&#21151;&#30340;&#23581;&#35797;&#20043;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#24049;&#30340;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#20855;&#26377;&#26032;&#22411;&#21367;&#31215;&#31383;&#21475;&#29305;&#24449;&#21644;&#26680;&#22238;&#24402;&#30340;&#26102;&#38388;&#21464;&#24418;&#22120;&#12290;&#22312;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22635;&#20805;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27604;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DeepMVI, a deep learning method for missing value imputation in multidimensional time-series datasets. Missing values are commonplace in decision support platforms that aggregate data over long time stretches from disparate sources, and reliable data analytics calls for careful handling of missing data. One strategy is imputing the missing values, and a wide variety of algorithms exist spanning simple interpolation, matrix factorization methods like SVD, statistical models like Kalman filters, and recent deep learning methods. We show that often these provide worse results on aggregate analytics compared to just excluding the missing data. DeepMVI uses a neural network to combine fine-grained and coarse-grained patterns along a time series, and trends from related series across categorical dimensions. After failing with off-the-shelf neural architectures, we design our own network that includes a temporal transformer with a novel convolutional window feature, and kernel regr
&lt;/p&gt;</description></item></channel></rss>