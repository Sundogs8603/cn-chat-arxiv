<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Steered Diffusion&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36817;&#26399;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#29983;&#25104;&#25511;&#21046;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.00224</link><description>&lt;p&gt;
Steered Diffusion: &#19968;&#31181;&#24191;&#20041;&#30340;&#25554;&#20214;&#24335;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis. (arXiv:2310.00224v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00224
&lt;/p&gt;
&lt;p&gt;
Steered Diffusion&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36817;&#26399;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#29983;&#25104;&#25511;&#21046;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#35757;&#32451;&#38598;&#25165;&#33021;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#33021;&#22815;&#25191;&#34892;&#25554;&#20214;&#24335;&#21512;&#25104;&#30340;&#27169;&#22411;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#21363;&#20351;&#29992;&#39044;&#23450;&#20041;&#25110;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65288;&#20363;&#22914;&#20351;&#29992;&#35821;&#35328;&#65289;&#65292;&#32780;&#35813;&#27169;&#22411;&#24182;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#22312;&#29983;&#25104;&#20219;&#21153;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25351;&#23548;&#36890;&#24120;&#21482;&#23545;&#21512;&#25104;&#39640;&#32423;&#35821;&#20041;&#26377;&#29992;&#65292;&#32780;&#19981;&#26159;&#32534;&#36753;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#20013;&#30340;&#32454;&#31890;&#24230;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20511;&#21161;&#26368;&#36817;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#30340;&#24378;&#22823;&#32454;&#31890;&#24230;&#29983;&#25104;&#25511;&#21046;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Steered Diffusion&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#20026;&#26080;&#26465;&#20214;&#29983;&#25104;&#32780;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#36924;&#30495;&#30340;&#38646;&#26679;&#26412;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#35774;&#35745;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#36870;&#27169;&#22411;&#25439;&#22833;&#26469;&#22312;&#25512;&#29702;&#26102;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional generative models typically demand large annotated training sets to achieve high-quality synthesis. As a result, there has been significant interest in designing models that perform plug-and-play generation, i.e., to use a predefined or pretrained model, which is not explicitly trained on the generative task, to guide the generative process (e.g., using language). However, such guidance is typically useful only towards synthesizing high-level semantics rather than editing fine-grained details as in image-to-image translation tasks. To this end, and capitalizing on the powerful fine-grained generative control offered by the recent diffusion-based generative models, we introduce Steered Diffusion, a generalized framework for photorealistic zero-shot conditional image generation using a diffusion model trained for unconditional generation. The key idea is to steer the image generation of the diffusion model at inference time via designing a loss using a pre-trained inverse mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#38544;&#24615;&#24378;&#30423;&#35774;&#32622;&#21644;&#19981;&#21516;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#35780;&#20272;&#20102;&#38544;&#31169;&#21644;&#25512;&#33616;&#22120;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;&#23450;&#21046;&#38544;&#31169;&#25216;&#26415;&#30340;&#38656;&#27714;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20010;&#20307;&#29992;&#25143;&#30340;&#25968;&#25454;&#35760;&#24405;&#28155;&#21152;&#25289;&#26222;&#25289;&#26031;&#26426;&#21046;&#30340;&#22122;&#22768;&#26159;&#19981;&#21512;&#36866;&#30340;&#36873;&#25321;&#65292;&#23427;&#22312;&#20219;&#20309;&#22122;&#22768;&#27700;&#24179;&#19979;&#37117;&#20250;&#20135;&#29983;&#26368;&#22823;&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2310.00221</link><description>&lt;p&gt;
&#36229;&#36234;&#38543;&#26426;&#22122;&#22768;&#65306;&#36890;&#36807;&#38544;&#24615;&#24378;&#30423;&#30740;&#31350;&#27934;&#23519;&#21311;&#21517;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Beyond Random Noise: Insights on Anonymization Strategies from a Latent Bandit Study. (arXiv:2310.00221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#38544;&#24615;&#24378;&#30423;&#35774;&#32622;&#21644;&#19981;&#21516;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#35780;&#20272;&#20102;&#38544;&#31169;&#21644;&#25512;&#33616;&#22120;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;&#23450;&#21046;&#38544;&#31169;&#25216;&#26415;&#30340;&#38656;&#27714;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20010;&#20307;&#29992;&#25143;&#30340;&#25968;&#25454;&#35760;&#24405;&#28155;&#21152;&#25289;&#26222;&#25289;&#26031;&#26426;&#21046;&#30340;&#22122;&#22768;&#26159;&#19981;&#21512;&#36866;&#30340;&#36873;&#25321;&#65292;&#23427;&#22312;&#20219;&#20309;&#22122;&#22768;&#27700;&#24179;&#19979;&#37117;&#20250;&#20135;&#29983;&#26368;&#22823;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#20849;&#20139;&#30693;&#35782;&#36827;&#34892;&#25512;&#33616;&#20219;&#21153;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#22686;&#21152;&#20102;&#36129;&#29486;&#65292;&#24182;&#24378;&#35843;&#20102;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#27169;&#24335;&#32780;&#38750;&#20381;&#36182;&#19968;&#20992;&#20999;&#35299;&#20915;&#26041;&#26696;&#30340;&#23450;&#21046;&#38544;&#31169;&#25216;&#26415;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#38544;&#24615;&#24378;&#30423;&#35774;&#32622;&#26469;&#35780;&#20272;&#38544;&#31169;&#21644;&#25512;&#33616;&#22120;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#32858;&#21512;&#31574;&#30053;&#65292;&#22914;&#24179;&#22343;&#12289;&#26368;&#36817;&#37051;&#21644;&#32858;&#31867;&#32467;&#21512;&#22122;&#22768;&#27880;&#20837;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#25163;&#25910;&#38598;&#30340;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#36741;&#21161;&#20449;&#24687;&#36827;&#34892;&#38142;&#25509;&#25915;&#20987;&#30340;&#24773;&#26223;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24320;&#25918;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20010;&#20307;&#29992;&#25143;&#30340;&#25968;&#25454;&#35760;&#24405;&#28155;&#21152;&#25289;&#26222;&#25289;&#26031;&#26426;&#21046;&#30340;&#22122;&#22768;&#26159;&#19968;&#20010;&#31967;&#31957;&#30340;&#36873;&#25321;&#12290;&#23427;&#30456;&#23545;&#20110;&#21435;&#21311;&#21517;&#21270;&#27010;&#29575;&#21644;ADS&#24230;&#37327;&#26469;&#35828;&#65292;&#22312;&#20219;&#20309;&#22122;&#22768;&#27700;&#24179;&#19979;&#37117;&#25552;&#20379;&#20102;&#26368;&#22823;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the issue of privacy in a learning scenario where users share knowledge for a recommendation task. Our study contributes to the growing body of research on privacy-preserving machine learning and underscores the need for tailored privacy techniques that address specific attack patterns rather than relying on one-size-fits-all solutions. We use the latent bandit setting to evaluate the trade-off between privacy and recommender performance by employing various aggregation strategies, such as averaging, nearest neighbor, and clustering combined with noise injection. More specifically, we simulate a linkage attack scenario leveraging publicly available auxiliary information acquired by the adversary. Our results on three open real-world datasets reveal that adding noise using the Laplace mechanism to an individual user's data record is a poor choice. It provides the highest regret for any noise level, relative to de-anonymization probability and the ADS metric. Inst
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2310.00212</link><description>&lt;p&gt;
&#20004;&#20004;&#37051;&#36817;&#31574;&#30053;&#20248;&#21270;: &#21033;&#29992;&#30456;&#23545;&#21453;&#39304;&#36827;&#34892;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#39044;&#20808;&#35757;&#32451;&#26469;&#33719;&#21462;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25509;&#35302;&#21040;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;LLMs&#21487;&#33021;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#19981;&#19968;&#33268;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#24341;&#23548;LLMs&#26397;&#30528;&#26377;&#30410;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#30340;&#20027;&#23548;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;Proximal Policy Optimization&#65288;PPO&#65289;&#26159;&#40664;&#35748;&#30340;RL&#20248;&#21270;&#22120;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;PPO&#22312;&#20248;&#21270;&#22522;&#20110;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#26657;&#20934;&#22870;&#21169;&#23610;&#24230;&#65292;PPO&#23545;&#20110;&#21253;&#21547;&#30456;&#21516;&#20559;&#22909;&#20449;&#24687;&#30340;&#31561;&#20215;&#22870;&#21169;&#20989;&#25968;&#19981;&#20855;&#22791;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#36712;&#36857;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;PPO&#23545;&#20110;&#22522;&#20110;&#20196;&#29260;&#30340;&#26356;&#26032;&#30340;&#38656;&#27714;&#24341;&#20837;&#20102;&#20989;&#25968;&#36924;&#36817;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#30456;&#23545;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;Pairwise Proximal Policy Optimization&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#23454;&#29616;&#35268;&#21010;&#30340;&#33258;&#20027;&#21327;&#35843;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00194</link><description>&lt;p&gt;
&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models. (arXiv:2310.00194v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00194
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#23454;&#29616;&#35268;&#21010;&#30340;&#33258;&#20027;&#21327;&#35843;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20154;&#33041;&#20013;&#33719;&#21462;&#28789;&#24863;&#65292;&#21363;&#36890;&#36807;&#21069;&#39069;&#21494;&#30382;&#23618;&#65288;PFC&#65289;&#20013;&#19987;&#38376;&#27169;&#22359;&#30340;&#37325;&#22797;&#20132;&#20114;&#26469;&#23436;&#25104;&#35268;&#21010;&#12290;&#36825;&#20123;&#27169;&#22359;&#25191;&#34892;&#20914;&#31361;&#30417;&#27979;&#12289;&#29366;&#24577;&#39044;&#27979;&#12289;&#29366;&#24577;&#35780;&#20272;&#12289;&#20219;&#21153;&#20998;&#35299;&#21644;&#20219;&#21153;&#21327;&#35843;&#31561;&#21151;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#26377;&#26102;&#33021;&#22815;&#21333;&#29420;&#25191;&#34892;&#36825;&#20123;&#21151;&#33021;&#65292;&#20294;&#22312;&#26381;&#21153;&#20110;&#19968;&#20010;&#30446;&#26631;&#26102;&#24448;&#24448;&#38590;&#20197;&#33258;&#20027;&#21327;&#35843;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22810;&#20010;&#22522;&#20110;LLM&#65288;GPT-4&#65289;&#27169;&#22359;&#30340;&#40657;&#30418;&#26550;&#26500;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#19987;&#38376;&#30340;PFC&#21551;&#21457;&#27169;&#22359;&#30340;&#20132;&#20114;&#23558;&#19968;&#20010;&#26356;&#22823;&#30340;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#23545;LLM&#30340;&#31616;&#30701;&#33258;&#21160;&#35843;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#32452;&#21512;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. To address this, we take inspiration from the human brain, in which planning is accomplished via the recurrent interaction of specialized modules in the prefrontal cortex (PFC). These modules perform functions such as conflict monitoring, state prediction, state evaluation, task decomposition, and task coordination. We find that LLMs are sometimes capable of carrying out these functions in isolation, but struggle to autonomously coordinate them in the service of a goal. Therefore, we propose a black box architecture with multiple LLM-based (GPT-4) modules. The architecture improves planning through the interaction of specialized PFC-inspired modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate the combined architecture on two challenging planning tasks -
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20004;&#20010;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#22270;&#21367;&#31215;&#21487;&#20197;&#34987;&#35270;&#20026;Mixup&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#65292;&#23427;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#37117;&#34987;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.00183</link><description>&lt;p&gt;
&#22270;&#21367;&#31215;&#21644;Mixup&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Equivalence of Graph Convolution and Mixup. (arXiv:2310.00183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00183
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20004;&#20010;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#22270;&#21367;&#31215;&#21487;&#20197;&#34987;&#35270;&#20026;Mixup&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#65292;&#23427;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#37117;&#34987;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#21367;&#31215;&#21644;Mixup&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22270;&#21367;&#31215;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#26159;&#36890;&#36807;&#32858;&#21512;&#37051;&#23621;&#26679;&#26412;&#30340;&#29305;&#24449;&#26469;&#23398;&#20064;&#29305;&#23450;&#33410;&#28857;&#25110;&#26679;&#26412;&#30340;&#20195;&#34920;&#24615;&#29305;&#24449;&#12290;&#32780;Mixup&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#29420;&#28909;&#26631;&#31614;&#36827;&#34892;&#24179;&#22343;&#26469;&#29983;&#25104;&#26032;&#30340;&#31034;&#20363;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#20043;&#38388;&#30340;&#19968;&#20010;&#20849;&#21516;&#20043;&#22788;&#26159;&#23427;&#20204;&#21033;&#29992;&#20102;&#26469;&#33258;&#22810;&#20010;&#26679;&#26412;&#30340;&#20449;&#24687;&#26469;&#24471;&#20986;&#29305;&#24449;&#34920;&#31034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#22312;&#20004;&#20010;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#22270;&#21367;&#31215;&#21487;&#20197;&#34987;&#35270;&#20026;Mixup&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#65292;&#23427;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#37117;&#34987;&#24212;&#29992;&#12290;&#36825;&#20004;&#20010;&#26465;&#20214;&#26159;&#65306;1&#65289;\textit{&#21516;&#36136;&#25913;&#26631;} - &#23558;&#30446;&#26631;&#33410;&#28857;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#25152;&#26377;&#37051;&#23621;&#65292;&#20197;&#21450;2&#65289;\textit{&#27979;&#35797;&#26102;Mixup} - &#22312;&#27979;&#35797;&#26102;&#23545;&#29305;&#24449;&#36827;&#34892;Mixup&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20004;&#20010;&#26465;&#20214;&#30340;&#25968;&#23398;&#34920;&#36798;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20010;&#31561;&#20215;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \textit{Homophily Relabel} - assigning the target node's label to all its neighbors, and 2) \textit{Test-Time Mixup} - Mixup the feature during the test time. We establis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Motif&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20132;&#20114;&#26469;&#33719;&#24471;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20195;&#29702;&#31243;&#24207;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Motif&#30340;&#20869;&#22312;&#22870;&#21169;&#30456;&#27604;&#30452;&#25509;&#26368;&#22823;&#21270;&#24471;&#20998;&#30340;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#28216;&#25103;&#20013;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#28216;&#25103;&#24471;&#20998;&#65292;&#24182;&#22312;&#20043;&#21069;&#27809;&#26377;&#21462;&#24471;&#36827;&#23637;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.00166</link><description>&lt;p&gt;
Motif: &#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#30340;&#20869;&#22312;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Motif: Intrinsic Motivation from Artificial Intelligence Feedback. (arXiv:2310.00166v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Motif&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20132;&#20114;&#26469;&#33719;&#24471;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20195;&#29702;&#31243;&#24207;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Motif&#30340;&#20869;&#22312;&#22870;&#21169;&#30456;&#27604;&#30452;&#25509;&#26368;&#22823;&#21270;&#24471;&#20998;&#30340;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#28216;&#25103;&#20013;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#28216;&#25103;&#24471;&#20998;&#65292;&#24182;&#22312;&#20043;&#21069;&#27809;&#26377;&#21462;&#24471;&#36827;&#23637;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#25506;&#32034;&#20016;&#23500;&#30340;&#29615;&#22659;&#24182;&#35780;&#20272;&#33258;&#24049;&#30340;&#34892;&#21160;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Motif&#65292;&#19968;&#31181;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#20808;&#39564;&#30693;&#35782;&#19982;&#20195;&#29702;&#31243;&#24207;&#25509;&#21475;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;Motif&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#23558;LLMs&#29992;&#20110;&#20915;&#31574;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65306;&#23427;&#36890;&#36807;&#20174;LLM&#20013;&#20135;&#29983;&#23545;&#37197;&#23545;&#26631;&#39064;&#30340;&#20559;&#22909;&#26469;&#26500;&#24314;&#20869;&#22312;&#22870;&#21169;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#22870;&#21169;&#23545;&#20195;&#29702;&#31243;&#24207;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#12289;&#24320;&#25918;&#24615;&#21644;&#31243;&#24207;&#29983;&#25104;&#30340;NetHack&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;Motif&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20165;&#36890;&#36807;&#23398;&#20064;&#26368;&#22823;&#21270;&#20854;&#20869;&#22312;&#22870;&#21169;&#65292;Motif&#30340;&#28216;&#25103;&#24471;&#20998;&#27604;&#30452;&#25509;&#35757;&#32451;&#20197;&#26368;&#22823;&#21270;&#24471;&#20998;&#30340;&#31639;&#27861;&#26356;&#39640;&#12290;&#24403;&#23558;Motif&#30340;&#20869;&#22312;&#22870;&#21169;&#19982;&#29615;&#22659;&#22870;&#21169;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#20197;&#21069;&#20174;&#26410;&#21462;&#24471;&#36827;&#23637;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent. Motif is based on the idea of grounding LLMs for decision-making without requiring them to interact with the environment: it elicits preferences from an LLM over pairs of captions to construct an intrinsic reward, which is then used to train agents with reinforcement learning. We evaluate Motif's performance and behavior on the challenging, open-ended and procedurally-generated NetHack game. Surprisingly, by only learning to maximize its intrinsic reward, Motif achieves a higher game score than an algorithm directly trained to maximize the score itself. When combining Motif's intrinsic reward with the environment reward, our method significantly outperforms existing approaches and makes progress on tasks where no advancements have ever been made with
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#22312;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;40.4&#30340;&#25513;&#30721;AP$_r$&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00161</link><description>&lt;p&gt;
&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection-Oriented Image-Text Pretraining for Open-Vocabulary Detection. (arXiv:2310.00161v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#22312;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;40.4&#30340;&#25513;&#30721;AP$_r$&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#26032;&#30340;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#29992;&#26816;&#27979;&#22120;&#26550;&#26500;&#26367;&#20195;&#24120;&#29992;&#30340;&#20998;&#31867;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#26816;&#27979;&#22120;&#22836;&#37096;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#65292;&#26356;&#22909;&#22320;&#28385;&#36275;&#26816;&#27979;&#30340;&#21306;&#22495;&#32423;&#35782;&#21035;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#26631;&#20934;&#30340;&#23545;&#27604;&#25439;&#22833;&#32780;&#19981;&#20351;&#29992;&#20266;&#26631;&#31614;&#65292;&#26159;&#23545;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#27880;&#24847;&#21147;&#30340;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#26356;&#21152;&#40065;&#26834;&#12289;&#24179;&#31227;&#19981;&#21464;&#65292;&#24182;&#19988;&#19981;&#21463;&#31383;&#21475;&#27169;&#24335;&#30340;&#20559;&#24046;&#24433;&#21709;&#12290;&#22312;&#27969;&#34892;&#30340;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#24120;&#35265;&#30340;ViT-L&#20027;&#24178;&#32593;&#32476;&#21462;&#24471;&#20102;40.4&#30340;&#25513;&#30721;AP$_r$&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new open-vocabulary detection approach based on detection-oriented image-text pretraining to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we replace the commonly used classification architecture with the detector architecture, which better serves the region-level recognition needs of detection by enabling the detector heads to learn from noisy image-text pairs. Using only standard contrastive loss and no pseudo-labeling, our approach is a simple yet effective extension of the contrastive learning method to learn emergent object-semantic cues. In addition, we propose a shifted-window learning approach upon window attention to make the backbone representation more robust, translation-invariant, and less biased by the window pattern. On the popular LVIS open-vocabulary detection benchmark, our approach sets a new state of the art of 40.4 mask AP$_r$ using the common ViT-L backbone, significantly outperforming t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#29305;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00160</link><description>&lt;p&gt;
&#33258;&#25105;&#29305;&#21270;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Self-Specialization: Uncovering Latent Expertise within Large Language Models. (arXiv:2310.00160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00160
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#29305;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#25105;&#35843;&#25972;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#20154;&#31867;&#32534;&#20889;&#30340;&#31181;&#23376;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#25945;&#23398;&#25968;&#25454;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#23545;&#40784;&#20197;&#36981;&#24490;&#19968;&#33324;&#25351;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#20877;&#20851;&#27880;&#19968;&#33324;&#23545;&#40784;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#19987;&#23478;&#39046;&#22495;&#29305;&#21270;&#30340;&#33258;&#25105;&#23545;&#40784;&#65288;&#20363;&#22914;&#65292;&#29983;&#29289;&#21307;&#23398;&#65289;&#65292;&#21457;&#29616;&#23427;&#23545;&#20110;&#25552;&#39640;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#38750;&#24120;&#26377;&#25928;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29616;&#26377;&#23545;&#40784;&#27169;&#22411;&#22312;&#19987;&#19994;&#39046;&#22495;&#20869;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#25581;&#31034;&#20102;&#8220;&#36890;&#29992;&#8221;&#25351;&#31034;&#36319;&#38543;&#35757;&#32451;&#23545;&#19979;&#28216;&#19987;&#23478;&#39046;&#22495;&#24615;&#33021;&#30340;&#36793;&#38469;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#25105;&#29305;&#21270;&#65292;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#36807;&#31243;&#12290;&#24403;&#36890;&#36807;&#26816;&#32034;&#26469;&#20943;&#23569;&#20135;&#29983;&#24187;&#35273;&#24182;&#25552;&#39640;&#23545;&#40784;&#30340;&#24182;&#21457;&#24615;&#21518;&#65292;&#33258;&#25105;&#29305;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated the effectiveness of self-alignment in which a large language model is, by itself, aligned to follow general instructions through the automatic generation of instructional data using a handful of human-written seeds. Instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine), discovering it to be very effective for improving zero-shot and few-shot performance in target domains of interest. As a preliminary, we first present the benchmark results of existing aligned models within a specialized domain, which reveals the marginal effect that "generic" instruction-following training has on downstream expert domains' performance. To remedy this, we explore self-specialization that leverages domain-specific unlabelled data and a few labeled seeds for the self-alignment process. When augmented with retrieval to reduce hallucination and enhance concurrency of the alignment, self-specialization offer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#65292;&#23558;&#38745;&#24577;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#21253;&#21547;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00158</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#20998;&#31867;&#20013;&#30340;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Feedback-guided Data Synthesis for Imbalanced Classification. (arXiv:2310.00158v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#65292;&#23558;&#38745;&#24577;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#21253;&#21547;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29616;&#29366;&#26159;&#20351;&#29992;&#26469;&#33258;&#38271;&#23614;&#20998;&#24067;&#30340;&#30495;&#23454;&#22270;&#20687;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#36825;&#20123;&#38745;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#25253;&#21578;&#20102;&#36866;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#21463;&#21040;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#19981;&#36275;&#30340;&#38480;&#21046;&#65292;&#36825;&#23558;&#20419;&#36827;&#29983;&#25104;&#26679;&#26412;&#30340;&#26377;&#29992;&#24615;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#22686;&#24378;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#12290;&#20026;&#20102;&#20351;&#35813;&#26694;&#26550;&#26377;&#25928;&#65292;&#25105;&#20204;&#21457;&#29616;&#26679;&#26412;&#24517;&#39035;&#25509;&#36817;&#25163;&#22836;&#20219;&#21153;&#30340;&#30495;&#23454;&#25968;&#25454;&#25903;&#25345;&#65292;&#24182;&#19988;&#20855;&#26377;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38271;&#23614;&#25968;&#25454;&#38598;&#65288;ImageNe...&#19978;&#39564;&#35777;&#20102;&#19977;&#20010;&#21453;&#39304;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current status quo in machine learning is to use static datasets of real images for training, which often come from long-tailed distributions. With the recent advances in generative models, researchers have started augmenting these static datasets with synthetic data, reporting moderate performance improvements on classification tasks. We hypothesize that these performance gains are limited by the lack of feedback from the classifier to the generative model, which would promote the usefulness of the generated samples to improve the classifier's performance. In this work, we introduce a framework for augmenting static datasets with useful synthetic samples, which leverages one-shot feedback from the classifier to drive the sampling of the generative model. In order for the framework to be effective, we find that the samples must be close to the support of the real data of the task at hand, and be sufficiently diverse. We validate three feedback criteria on a long-tailed dataset (ImageNe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.00156</link><description>&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#23398;&#20064;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning Generalizable Tool-use Skills through Trajectory Generation. (arXiv:2310.00156v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00156
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21033;&#29992;&#24037;&#20855;&#30340;&#33258;&#20027;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#35768;&#22810;&#24120;&#35265;&#20219;&#21153;&#65292;&#22914;&#28921;&#39274;&#21644;&#28165;&#27905;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#31995;&#32479;&#22312;&#36866;&#24212;&#26032;&#24037;&#20855;&#26041;&#38754;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#22522;&#20110;&#21487;&#21450;&#24615;&#30340;&#20808;&#21069;&#24037;&#20316;&#36890;&#24120;&#23545;&#29615;&#22659;&#20570;&#20986;&#20102;&#24456;&#24378;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#20219;&#21153;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#20351;&#29992;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#26469;&#25805;&#32437;&#21487;&#21464;&#24418;&#29289;&#20307;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#20316;&#20026;&#19968;&#31995;&#21015;&#28857;&#20113;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#24037;&#20855;&#24418;&#29366;&#12290;&#23545;&#20110;&#20219;&#20309;&#26032;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#20808;&#29983;&#25104;&#19968;&#20010;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#28982;&#21518;&#20248;&#21270;&#24037;&#20855;&#23039;&#21183;&#24207;&#21015;&#20197;&#19982;&#29983;&#25104;&#30340;&#36712;&#36857;&#23545;&#40784;&#12290;&#25105;&#20204;&#20026;&#22235;&#31181;&#19981;&#21516;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#20219;&#21153;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#21333;&#20010;&#24037;&#20855;&#30340;&#31034;&#33539;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model for four different challenging deformable object manipulation tasks. Our model is trained with demonstration data from just a single tool for each task and is able to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#35299;&#20915;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#20998;&#26512;&#20219;&#21153;&#23618;&#38754;&#21644;&#26679;&#26412;&#23618;&#38754;&#30340;&#32422;&#26463;&#65292;&#22312;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#20013;&#20998;&#37197;&#36164;&#28304;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00154</link><description>&lt;p&gt;
&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#65306;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#23454;&#29616;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers. (arXiv:2310.00154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#35299;&#20915;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#20998;&#26512;&#20219;&#21153;&#23618;&#38754;&#21644;&#26679;&#26412;&#23618;&#38754;&#30340;&#32422;&#26463;&#65292;&#22312;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#20013;&#20998;&#37197;&#36164;&#28304;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22266;&#26377;&#22320;&#26159;&#19968;&#20010;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#22312;&#8220;&#26080;&#36951;&#24536;&#8221;&#35201;&#27714;&#19979;&#23398;&#20064;&#19968;&#20010;&#39044;&#27979;&#22120;&#12290;&#23613;&#31649;&#20043;&#21069;&#26377;&#20960;&#39033;&#30740;&#31350;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#26126;&#30830;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#26159;&#21487;&#34892;&#19988;&#26377;&#30410;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#38480;&#21046;&#24615;&#23398;&#20064;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21487;&#20197;&#23558;&#20808;&#21069;&#20219;&#21153;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#26679;&#26412;&#23384;&#20648;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#20004;&#20010;&#29256;&#26412;&#65306;&#19968;&#20010;&#22312;&#20219;&#21153;&#23618;&#38754;&#19978;&#26377;&#32422;&#26463;&#30340;&#31895;&#31961;&#26041;&#27861;&#21644;&#19968;&#20010;&#22312;&#26679;&#26412;&#23618;&#38754;&#19978;&#26377;&#32422;&#26463;&#30340;&#31934;&#32454;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20598;&#21464;&#37327;&#25351;&#31034;&#20102;&#26368;&#20248;&#20540;&#23545;&#20110;&#32422;&#26463;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#22312;&#31895;&#31961;&#26041;&#27861;&#20013;&#23545;&#32531;&#20914;&#21306;&#36827;&#34892;&#20102;&#21010;&#20998;&#65292;&#23558;&#26356;&#22810;&#36164;&#28304;&#20998;&#37197;&#32473;&#26356;&#38590;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is inherently a constrained learning problem. The goal is to learn a predictor under a \emph{no-forgetting} requirement. Although several prior studies formulate it as such, they do not solve the constrained problem explicitly. In this work, we show that it is both possible and beneficial to undertake the constrained optimization problem directly. To do this, we leverage recent results in constrained learning through Lagrangian duality. We focus on memory-based methods, where a small subset of samples from previous tasks can be stored in a replay buffer. In this setting, we analyze two versions of the continual learning problem: a coarse approach with constraints at the task level and a fine approach with constraints at the sample level. We show that dual variables indicate the sensitivity of the optimal value with respect to constraint perturbations. We then leverage this result to partition the buffer in the coarse approach, allocating more resources to harder task
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#36827;&#34892;3D&#37325;&#24314;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#27979;&#35268;&#21010;&#65292;&#21512;&#29702;&#36873;&#25321;&#30456;&#26426;&#20301;&#32622;&#24182;&#32771;&#34385;&#22122;&#22768;&#23545;&#37325;&#24314;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;3D&#37325;&#24314;&#32467;&#26524;&#30340;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.00145</link><description>&lt;p&gt;
&#22312;&#22024;&#26434;&#30340;&#20892;&#19994;&#29615;&#22659;&#20013;&#30340;3D&#37325;&#24314;&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#35270;&#35282;&#30340;&#35266;&#27979;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
3D Reconstruction in Noisy Agricultural Environments: A Bayesian Optimization Perspective for View Planning. (arXiv:2310.00145v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#36827;&#34892;3D&#37325;&#24314;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#27979;&#35268;&#21010;&#65292;&#21512;&#29702;&#36873;&#25321;&#30456;&#26426;&#20301;&#32622;&#24182;&#32771;&#34385;&#22122;&#22768;&#23545;&#37325;&#24314;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;3D&#37325;&#24314;&#32467;&#26524;&#30340;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#37325;&#24314;&#26159;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#22240;&#20854;&#22312;&#20892;&#19994;&#12289;&#27700;&#19979;&#21644;&#22478;&#24066;&#29615;&#22659;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#31181;&#37325;&#35201;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#21512;&#29702;&#23433;&#25918;&#30456;&#26426;&#26469;&#26368;&#22823;&#21270;&#35270;&#35273;&#20449;&#24687;&#65292;&#25552;&#39640;3D&#37325;&#24314;&#32467;&#26524;&#65292;&#31216;&#20026;&#35266;&#27979;&#35268;&#21010;&#12290;&#36890;&#36807;&#23558;&#20960;&#20309;&#26631;&#20934;&#24212;&#29992;&#20110;&#36873;&#25321;&#36739;&#23569;&#20294;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#30340;&#20219;&#24847;&#22270;&#20687;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;3D&#37325;&#24314;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#20013;&#32771;&#34385;&#21040;&#23384;&#22312;&#30340;&#22122;&#22768;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#27809;&#26377;&#25552;&#20379;&#26377;&#20851;&#22122;&#22768;&#30340;&#20808;&#39564;&#20449;&#24687;&#26102;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#20989;&#25968;&#65292;&#32771;&#34385;&#21040;&#29616;&#26377;&#30340;&#22122;&#22768;&#65292;&#20165;&#20381;&#38752;&#30456;&#23545;&#36739;&#23569;&#30340;&#22122;&#22768;&#23454;&#29616;&#26469;&#35745;&#31639;&#65292;&#32780;&#19981;&#38656;&#35201;&#20854;&#23553;&#38381;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D reconstruction is a fundamental task in robotics that gained attention due to its major impact in a wide variety of practical settings, including agriculture, underwater, and urban environments. An important approach for this task, known as view planning, is to judiciously place a number of cameras in positions that maximize the visual information improving the resulting 3D reconstruction. Circumventing the need for a large number of arbitrary images, geometric criteria can be applied to select fewer yet more informative images to markedly improve the 3D reconstruction performance. Nonetheless, incorporating the noise of the environment that exists in various real-world scenarios into these criteria may be challenging, particularly when prior information about the noise is not provided. To that end, this work advocates a novel geometric function that accounts for the existing noise, relying solely on a relatively small number of noise realizations without requiring its closed-form e
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#37319;&#26679;&#22686;&#24378;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#26102;&#24577;&#38543;&#26426;&#28216;&#36208;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#26356;&#31934;&#32454;&#30340;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#26816;&#27979;&#24322;&#24120;&#21644;&#20132;&#26131;&#31361;&#21457;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#20197;&#22826;&#22346;&#20132;&#26131;&#25968;&#25454;&#20013;&#26102;&#38388;&#32447;&#32034;&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#35813;&#26694;&#26550;&#36827;&#34892;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00144</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#37319;&#26679;&#22686;&#24378;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65306;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20197;&#22826;&#22346;&#32593;&#32476;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks. (arXiv:2310.00144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#37319;&#26679;&#22686;&#24378;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#26102;&#24577;&#38543;&#26426;&#28216;&#36208;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#26356;&#31934;&#32454;&#30340;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#26816;&#27979;&#24322;&#24120;&#21644;&#20132;&#26131;&#31361;&#21457;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#20197;&#22826;&#22346;&#20132;&#26131;&#25968;&#25454;&#20013;&#26102;&#38388;&#32447;&#32034;&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#35813;&#26694;&#26550;&#36827;&#34892;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#22826;&#22346;&#32593;&#32476;&#30340;&#24555;&#36895;&#28436;&#36827;&#38656;&#35201;&#20808;&#36827;&#30340;&#25216;&#26415;&#26469;&#30830;&#20445;&#20854;&#23545;&#28508;&#22312;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#24182;&#20445;&#25345;&#36879;&#26126;&#24230;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#27492;&#31867;&#24179;&#21488;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20808;&#23548;&#24615;&#25104;&#26524;&#65292;&#20294;&#25429;&#25417;&#31354;&#38388;&#21644;&#26102;&#38388;&#20107;&#21153;&#27169;&#24335;&#30340;&#22797;&#26434;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#19982;&#20351;&#29992;&#27010;&#29575;&#37319;&#26679;&#22686;&#24378;&#30340;&#26102;&#24577;&#38543;&#26426;&#28216;&#36208;&#65288;TRW&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#19982;&#20256;&#32479;&#30340;GCNs&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;TRW&#30340;&#20248;&#21183;&#26469;&#35782;&#21035;&#20197;&#22826;&#22346;&#20132;&#26131;&#20013;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#20837;&#24494;&#30340;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#26426;&#21046;&#12290;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;TRW-GCN&#26694;&#26550;&#22312;&#26816;&#27979;&#24322;&#24120;&#21644;&#20132;&#26131;&#31361;&#21457;&#30340;&#24615;&#33021;&#25351;&#26631;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#20256;&#32479;GCNs&#30340;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#24378;&#35843;&#20102;&#20197;&#22826;&#22346;&#20132;&#26131;&#25968;&#25454;&#20013;&#26102;&#38388;&#32447;&#32034;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20351;&#29992;&#27010;&#29575;&#37319;&#26679;&#22686;&#24378;&#30340;&#26102;&#31354;GCNs&#36827;&#34892;&#20132;&#26131;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid evolution of the Ethereum network necessitates sophisticated techniques to ensure its robustness against potential threats and to maintain transparency. While Graph Neural Networks (GNNs) have pioneered anomaly detection in such platforms, capturing the intricacies of both spatial and temporal transactional patterns has remained a challenge. This study presents a fusion of Graph Convolutional Networks (GCNs) with Temporal Random Walks (TRW) enhanced by probabilistic sampling to bridge this gap. Our approach, unlike traditional GCNs, leverages the strengths of TRW to discern complex temporal sequences in Ethereum transactions, thereby providing a more nuanced transaction anomaly detection mechanism. Preliminary evaluations demonstrate that our TRW-GCN framework substantially advances the performance metrics over conventional GCNs in detecting anomalies and transaction bursts. This research not only underscores the potential of temporal cues in Ethereum transactional data but a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#36890;&#29992;&#26041;&#27861;&#65288;GASS&#65289;&#65292;&#22312;&#26377;&#38480;&#20998;&#24067;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#22768;&#38899;&#20107;&#20214;&#21644;&#35821;&#38899;&#20998;&#31163;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31163;&#36229;&#20986;&#20998;&#24067;&#30340;&#30005;&#24433;&#21644;&#38899;&#20048;&#20869;&#23481;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.00140</link><description>&lt;p&gt;
GASS&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GASS: Generalizing Audio Source Separation with Large-scale Data. (arXiv:2310.00140v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#36890;&#29992;&#26041;&#27861;&#65288;GASS&#65289;&#65292;&#22312;&#26377;&#38480;&#20998;&#24067;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#22768;&#38899;&#20107;&#20214;&#21644;&#35821;&#38899;&#20998;&#31163;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31163;&#36229;&#20986;&#20998;&#24067;&#30340;&#30005;&#24433;&#21644;&#38899;&#20048;&#20869;&#23481;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#28304;&#20998;&#31163;&#30340;&#30446;&#26631;&#26159;&#20998;&#31163;&#20219;&#24847;&#28151;&#21512;&#38899;&#39057;&#20013;&#30340;&#38899;&#39057;&#28304;&#65292;&#28040;&#38500;&#20165;&#25805;&#20316;&#20110;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#35821;&#38899;&#25110;&#38899;&#20048;&#65289;&#30340;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#28304;&#20998;&#31163;&#30340;&#28508;&#21147;&#21463;&#38480;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#20855;&#26377;&#20027;&#35201;&#22768;&#38899;&#20107;&#20214;&#30340;&#28151;&#21512;&#20197;&#21450;&#23567;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20197;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#30340;&#21333;&#19968;&#36890;&#29992;&#38899;&#39057;&#28304;&#20998;&#31163;&#65288;GASS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20998;&#31163;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#22768;&#38899;&#20107;&#20214;&#12290;&#25105;&#20204;&#23545;GASS&#27169;&#22411;&#36827;&#34892;&#20102;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#24378;&#26377;&#21147;&#20998;&#24067;&#32467;&#26524;&#26174;&#31034;&#20102;GASS&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#22312;&#22768;&#38899;&#20107;&#20214;&#21644;&#35821;&#38899;&#20998;&#31163;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#21017;&#26174;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;GASS&#27169;&#22411;&#22312;&#20998;&#31163;&#36229;&#20986;&#20998;&#24067;&#30340;&#30005;&#24433;&#21644;&#38899;&#20048;&#20869;&#23481;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#23545;GASS&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#22987;&#32456;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Universal source separation targets at separating the audio sources of an arbitrary mix, removing the constraint to operate on a specific domain like speech or music. Yet, the potential of universal source separation is limited because most existing works focus on mixes with predominantly sound events, and small training datasets also limit its potential for supervised learning. Here, we study a single general audio source separation (GASS) model trained to separate speech, music, and sound events in a supervised fashion with a large-scale dataset. We assess GASS models on a diverse set of tasks. Our strong in-distribution results show the feasibility of GASS models, and the competitive out-of-distribution performance in sound event and speech separation shows its generalization abilities. Yet, it is challenging for GASS models to generalize for separating out-of-distribution cinematic and music content. We also fine-tune GASS models on each dataset and consistently outperform the ones
&lt;/p&gt;</description></item><item><title>ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.00117</link><description>&lt;p&gt;
ABScribe: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00117
&lt;/p&gt;
&lt;p&gt;
ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#20070;&#20889;&#25991;&#26412;&#26469;&#25506;&#32034;&#26367;&#20195;&#24819;&#27861;&#26159;&#20889;&#20316;&#36807;&#31243;&#30340;&#20851;&#38190;&#12290;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#31616;&#21270;&#20889;&#20316;&#21464;&#21270;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30028;&#38754;&#23384;&#22312;&#21516;&#26102;&#32771;&#34385;&#22810;&#31181;&#21464;&#21270;&#30340;&#25361;&#25112;&#65306;&#22312;&#19981;&#35206;&#30422;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#26032;&#30340;&#29256;&#26412;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#32780;&#25353;&#39034;&#24207;&#31896;&#36148;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;&#25991;&#26723;&#21464;&#24471;&#26434;&#20081;&#65292;&#22686;&#21152;&#24037;&#20316;&#37327;&#65292;&#24182;&#25171;&#26029;&#20316;&#32773;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ABScribe&#65292;&#19968;&#31181;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#19988;&#32467;&#26500;&#21270;&#22320;&#25506;&#32034;&#20889;&#20316;&#21464;&#21270;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;ABScribe&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;LLM&#25552;&#31034;&#24555;&#36895;&#20135;&#29983;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20250;&#33258;&#21160;&#36716;&#25442;&#25104;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#12290;&#21464;&#20307;&#22312;&#25991;&#26412;&#27573;&#33853;&#20013;&#34987;&#23384;&#20648;&#22312;&#30456;&#37051;&#20301;&#32622;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#19978;&#30340;&#40736;&#26631;&#24748;&#20572;&#20132;&#20114;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;12&#21517;&#25776;&#20889;&#20154;&#21592;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;ABScribe&#33021;&#26174;&#33879;&#20943;&#36731;&#20219;&#21153;&#36127;&#33655;&#65288;d = 1.20, p &lt; 0.001&#65289;&#65292;&#25552;&#39640;&#29992;&#25143;&#30340;&#35748;&#30693;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p &lt; 0.001), enhances user perceptions o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00116</link><description>&lt;p&gt;
&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization. (arXiv:2310.00116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20363;&#22914;&#35774;&#35745;&#20855;&#26377;&#26356;&#22909;&#40065;&#26834;&#24615;&#24615;&#36136;&#30340;&#26032;&#26550;&#26500;&#65288;&#20363;&#22914;&#65292;Lipschitz-capped&#32593;&#32476;&#65289;&#25110;&#20462;&#25913;&#35757;&#32451;&#36807;&#31243;&#26412;&#36523;&#65288;&#20363;&#22914;&#65292;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#65292;&#32422;&#26463;&#23398;&#20064;&#25110;&#27491;&#21017;&#21270;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22686;&#21152;&#36755;&#20837;&#65288;&#29305;&#24449;&#65289;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21487;&#33021;&#24182;&#19981;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#23545;&#24320;&#21457;&#33021;&#22815;&#30452;&#25509;&#25805;&#32437;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#35757;&#32451;&#36807;&#31243;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#35813;&#31867;&#21035;&#30340;&#26368;&#26032;&#21457;&#23637;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#40065;&#26834;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#36755;&#20986;&#65288;logit&#65289;&#31354;&#38388;&#20013;&#22686;&#21152;&#36793;&#30028;&#65292;&#24182;&#27839;&#30528;&#33030;&#24369;&#26041;&#21521;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20004;&#20010;&#30446;&#26631;&#21487;&#20197;&#30452;&#25509;&#20419;&#36827;&#36755;&#20837;&#31354;&#38388;&#20013;&#26356;&#22823;&#30340;&#36793;&#30028;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. As a result, there has been an increasing interest in developing training procedures that can directly manipulate the decision boundary in the input space. In this paper, we build upon recent developments in this category by developing a robust training algorithm whose objective is to increase the margin in the output (logit) space while regularizing the Lipschitz constant of the model along vulnerable directions. We show that these two objectives can directly promote larger margins in the input space. To this end, we develop a scalable method for calcula
&lt;/p&gt;</description></item><item><title>HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00113</link><description>&lt;p&gt;
HyperMask: &#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00113
&lt;/p&gt;
&lt;p&gt;
HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#39034;&#24207;&#35757;&#32451;&#26102;&#65292;&#24448;&#24448;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#26368;&#26377;&#25928;&#30340;&#20043;&#19968;&#26159;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36229;&#32593;&#32476;&#26681;&#25454;&#20219;&#21153;&#30340;&#29305;&#24449;&#29983;&#25104;&#30446;&#26631;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#20027;&#35201;&#38480;&#21046;&#26159;&#36229;&#32593;&#32476;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#21487;&#20197;&#20135;&#29983;&#23436;&#20840;&#19981;&#21516;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#22240;&#27492;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#21333;&#29420;&#35299;&#20915;&#30340;&#12290;&#27169;&#22411;&#22312;&#23398;&#20064;&#21518;&#32493;&#20219;&#21153;&#26102;&#19981;&#20351;&#29992;&#20043;&#21069;&#20219;&#21153;&#25152;&#20851;&#32852;&#30340;&#32593;&#32476;&#20449;&#24687;&#65292;&#24182;&#23454;&#38469;&#19978;&#20135;&#29983;&#20102;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#24425;&#31080;&#31080;&#35777;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#35748;&#20026;&#23384;&#22312;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#65288;&#21363;&#20013;&#22870;&#31080;&#65289;&#65292;&#21487;&#20197;&#20445;&#25345;&#23436;&#25972;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperMask&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#12290;&#36229;&#32593;&#32476;&#20135;&#29983;&#21322;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#20197;&#33719;&#21462;&#30446;&#26631;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FashionFlow&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20174;&#38745;&#24577;&#22270;&#20687;&#29983;&#25104;&#30701;&#35270;&#39057;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#24182;&#36830;&#25509;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#32452;&#20214;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#20266;3D&#21367;&#31215;&#23618;&#39640;&#25928;&#29983;&#25104;&#35270;&#39057;&#65292;&#24182;&#21033;&#29992;VAE&#21644;CLIP&#32534;&#30721;&#22120;&#25429;&#25417;&#20851;&#38190;&#29305;&#24449;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#25104;&#21151;&#21512;&#25104;&#26102;&#23578;&#35270;&#39057;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#23637;&#31034;&#26381;&#35013;&#30340;&#21512;&#36523;&#24230;&#21644;&#22806;&#35266;&#65292;&#20026;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#30340;&#36141;&#29289;&#20307;&#39564;&#25552;&#20379;&#25913;&#36827;&#21644;&#22686;&#24378;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.00106</link><description>&lt;p&gt;
FashionFlow: &#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20174;&#38745;&#24577;&#22270;&#20687;&#29983;&#25104;&#21160;&#24577;&#26102;&#23578;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery. (arXiv:2310.00106v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FashionFlow&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20174;&#38745;&#24577;&#22270;&#20687;&#29983;&#25104;&#30701;&#35270;&#39057;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#24182;&#36830;&#25509;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#32452;&#20214;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#20266;3D&#21367;&#31215;&#23618;&#39640;&#25928;&#29983;&#25104;&#35270;&#39057;&#65292;&#24182;&#21033;&#29992;VAE&#21644;CLIP&#32534;&#30721;&#22120;&#25429;&#25417;&#20851;&#38190;&#29305;&#24449;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#25104;&#21151;&#21512;&#25104;&#26102;&#23578;&#35270;&#39057;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#23637;&#31034;&#26381;&#35013;&#30340;&#21512;&#36523;&#24230;&#21644;&#22806;&#35266;&#65292;&#20026;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#30340;&#36141;&#29289;&#20307;&#39564;&#25552;&#20379;&#25913;&#36827;&#21644;&#22686;&#24378;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#29983;&#25104;&#22120;&#65292;&#31216;&#20026;FashionFlow&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#38745;&#24577;&#22270;&#20687;&#21019;&#24314;&#30701;&#35270;&#39057;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24320;&#21457;&#24182;&#36830;&#25509;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#32452;&#20214;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;&#20247;&#19981;&#21516;&#12290;&#36825;&#20123;&#32452;&#20214;&#21253;&#25324;&#20351;&#29992;&#20266;3D&#21367;&#31215;&#23618;&#39640;&#25928;&#29983;&#25104;&#35270;&#39057;&#12290;VAE&#21644;CLIP&#32534;&#30721;&#22120;&#20174;&#38745;&#24577;&#22270;&#20687;&#20013;&#25429;&#25417;&#21040;&#37325;&#35201;&#29305;&#24449;&#65292;&#20197;&#24433;&#21709;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#25104;&#21151;&#21512;&#25104;&#20855;&#26377;&#19981;&#21516;&#35282;&#24230;&#30340;&#27169;&#29305;&#19968;&#36793;&#25670;&#23039;&#21183;&#65292;&#23637;&#31034;&#26381;&#35013;&#30340;&#21512;&#36523;&#24230;&#21644;&#22806;&#35266;&#30340;&#26102;&#23578;&#35270;&#39057;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#20110;&#25913;&#36827;&#21644;&#25552;&#21319;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#30340;&#36141;&#29289;&#20307;&#39564;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our study introduces a new image-to-video generator called FashionFlow. By utilising a diffusion model, we are able to create short videos from still images. Our approach involves developing and connecting relevant components with the diffusion model, which sets our work apart. The components include the use of pseudo-3D convolutional layers to generate videos efficiently. VAE and CLIP encoders capture vital characteristics from still images to influence the diffusion model. Our research demonstrates a successful synthesis of fashion videos featuring models posing from various angles, showcasing the fit and appearance of the garment. Our findings hold great promise for improving and enhancing the shopping experience for the online fashion industry.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.00100</link><description>&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;--&#25688;&#35201;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65281;
&lt;/p&gt;
&lt;p&gt;
Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21360;&#35937;&#37096;&#20998;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#22312;&#21521;&#21307;&#29983;&#20256;&#36798;&#36825;&#20123;&#21457;&#29616;&#26102;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#26469;&#35828;&#65292;&#20934;&#22791;&#36825;&#20123;&#25688;&#35201;&#26082;&#32791;&#26102;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#33021;&#22815;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#24635;&#32467;&#36825;&#20123;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#33258;&#21160;&#21270;&#22320;&#29983;&#25104;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#25918;&#23556;&#23398;&#21360;&#35937;&#65292;&#20197;&#24635;&#32467;&#33521;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#24503;&#35821;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#12290;&#22312;&#19968;&#39033;&#30450;&#27979;&#20013;&#65292;&#20004;&#20301;&#26377;&#25191;&#19994;&#36164;&#26684;&#30340;&#25918;&#23556;&#31185;&#21307;&#29983;&#34920;&#31034;&#65292;&#23545;&#20110;&#33267;&#23569;70%&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#20854;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Voice2Action&#65292;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#34394;&#25311;&#29616;&#23454;&#20013;&#36827;&#34892;&#39640;&#25928;&#23454;&#26102;&#20132;&#20114;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23545;&#23450;&#21046;&#35821;&#38899;&#20449;&#21495;&#21644;&#25991;&#26412;&#21629;&#20196;&#36827;&#34892;&#20998;&#23618;&#20998;&#26512;&#65292;&#24182;&#23558;&#25191;&#34892;&#20219;&#21153;&#20998;&#25104;&#20132;&#20114;&#23376;&#38598;&#65292;Voice2Action&#33021;&#22815;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2310.00092</link><description>&lt;p&gt;
Voice2Action: &#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#34394;&#25311;&#29616;&#23454;&#20013;&#39640;&#25928;&#23454;&#26102;&#20132;&#20114;&#30340;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality. (arXiv:2310.00092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Voice2Action&#65292;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#34394;&#25311;&#29616;&#23454;&#20013;&#36827;&#34892;&#39640;&#25928;&#23454;&#26102;&#20132;&#20114;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23545;&#23450;&#21046;&#35821;&#38899;&#20449;&#21495;&#21644;&#25991;&#26412;&#21629;&#20196;&#36827;&#34892;&#20998;&#23618;&#20998;&#26512;&#65292;&#24182;&#23558;&#25191;&#34892;&#20219;&#21153;&#20998;&#25104;&#20132;&#20114;&#23376;&#38598;&#65292;Voice2Action&#33021;&#22815;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35757;&#32451;&#21644;&#35843;&#25972;&#20197;&#20165;&#20165;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#34987;&#25552;&#31034;&#20026;&#20219;&#21153;&#39537;&#21160;&#30340;&#33258;&#20027;&#20195;&#29702;&#20154;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#25191;&#34892;&#29615;&#22659;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#22312;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#20013;&#37096;&#32626;&#20195;&#29702;LLMs&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20854;&#21407;&#22240;&#26159;&#22312;&#32447;&#20132;&#20114;&#30340;&#25928;&#29575;&#20302;&#19979;&#20197;&#21450;3D&#29615;&#22659;&#20013;&#22797;&#26434;&#30340;&#25805;&#20316;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Voice2Action&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#20316;&#21644;&#23454;&#20307;&#25552;&#21462;&#26469;&#20998;&#23618;&#20998;&#26512;&#23450;&#21046;&#35821;&#38899;&#20449;&#21495;&#21644;&#25991;&#26412;&#21629;&#20196;&#65292;&#24182;&#23558;&#25191;&#34892;&#20219;&#21153;&#23454;&#26102;&#20998;&#25104;&#35268;&#33539;&#30340;&#20132;&#20114;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#26469;&#38450;&#27490;&#38169;&#35823;&#12290;&#22312;&#20855;&#26377;&#21512;&#25104;&#25351;&#20196;&#25968;&#25454;&#30340;&#22478;&#24066;&#24037;&#31243;VR&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Voice2Action&#33021;&#22815;&#27604;&#27809;&#26377;&#20248;&#21270;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SocREval&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;GPT-4&#21644;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22797;&#26434;&#25512;&#29702;&#27169;&#22411;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.00074</link><description>&lt;p&gt;
SocREval&#65306;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation. (arXiv:2310.00074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SocREval&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;GPT-4&#21644;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22797;&#26434;&#25512;&#29702;&#27169;&#22411;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#35780;&#20272;&#23427;&#20204;&#30340;&#36880;&#27493;&#25512;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#35780;&#20272;&#25351;&#26631;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#25512;&#29702;&#38142;&#26469;&#35780;&#20272;&#27169;&#22411;&#23548;&#20986;&#30340;&#25512;&#29702;&#38142;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#20154;&#24037;&#32534;&#20889;&#30340;&#25512;&#29702;&#38142;&#21487;&#33021;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#20854;&#33719;&#21462;&#36890;&#24120;&#26159;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#12290;&#29616;&#26377;&#30340;&#26080;&#21442;&#32771;&#25512;&#29702;&#25351;&#26631;&#28040;&#38500;&#20102;&#20154;&#24037;&#21046;&#20316;&#25512;&#29702;&#38142;&#30340;&#38656;&#27714;&#20316;&#20026;&#21442;&#32771;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22312;&#20855;&#26377;&#20154;&#24037;&#25512;&#29702;&#38142;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#22797;&#26434;&#21270;&#20102;&#27969;&#31243;&#24182;&#24341;&#21457;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#33258;&#21160;&#35780;&#20272;&#25512;&#29702;&#38142;&#36136;&#37327;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#21046;&#20316;&#21442;&#32771;&#30340;&#38656;&#27714;&#12290;&#21033;&#29992;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23450;&#21046;&#21270;&#25552;&#31034;&#26469;&#22686;&#24378;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#36825;&#23601;&#26159;&#25105;&#20204;&#31216;&#20043;&#20026;SocREval&#65288;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comprehensively assess the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains to assess the model-derived chains. However, such ``gold-standard'' human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning metrics eliminate the need for human-crafted reasoning chains as references, but they typically require fine-tuning on datasets with human-derived reasoning chains, which complicates the process and raises concerns regarding generalizability across diverse datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, obviating the need for human-crafted references. Leveraging the Socratic method, we devise tailored prompts to enhance reference-free reasoning evaluation, which we term SocREval (Socratic metho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#21548;&#20247;&#32918;&#20687;&#65288;ELP&#65289;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#31163;&#25955;&#35774;&#35745;&#65292;&#33021;&#26681;&#25454;&#23545;&#35805;&#20013;&#19981;&#21516;&#24773;&#32490;&#29983;&#25104;&#33258;&#28982;&#22810;&#26679;&#21448;&#21487;&#25511;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#38754;&#37096;&#34920;&#24773;&#29983;&#25104;&#20013;&#30340;&#38750;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00068</link><description>&lt;p&gt;
&#24773;&#24863;&#21548;&#20247;&#32918;&#20687;&#65306;&#30495;&#23454;&#30340;&#21548;&#20247;&#21160;&#20316;&#27169;&#25311;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Emotional Listener Portrait: Realistic Listener Motion Simulation in Conversation. (arXiv:2310.00068v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#21548;&#20247;&#32918;&#20687;&#65288;ELP&#65289;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#31163;&#25955;&#35774;&#35745;&#65292;&#33021;&#26681;&#25454;&#23545;&#35805;&#20013;&#19981;&#21516;&#24773;&#32490;&#29983;&#25104;&#33258;&#28982;&#22810;&#26679;&#21448;&#21487;&#25511;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#38754;&#37096;&#34920;&#24773;&#29983;&#25104;&#20013;&#30340;&#38750;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#32773;&#22836;&#37096;&#29983;&#25104;&#20027;&#35201;&#20851;&#27880;&#22312;&#26681;&#25454;&#35762;&#35805;&#32773;&#20256;&#36882;&#30340;&#20449;&#24687;&#29983;&#25104;&#21548;&#32773;&#30340;&#38750;&#35821;&#35328;&#34892;&#20026;&#65288;&#20363;&#22914;&#24494;&#31505;&#65289;&#12290;&#29983;&#25104;&#36825;&#26679;&#30340;&#21709;&#24212;&#26102;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#23545;&#35805;&#20013;&#31934;&#32454;&#38754;&#37096;&#34920;&#24773;&#30340;&#38750;&#30830;&#23450;&#24615;&#29305;&#24615;&#65292;&#36825;&#21462;&#20915;&#20110;&#35762;&#35805;&#32773;&#21644;&#21548;&#32773;&#30340;&#24773;&#32490;&#21644;&#24577;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24773;&#24863;&#21548;&#20247;&#32918;&#20687;&#65288;ELP&#65289;&#65292;&#23427;&#23558;&#27599;&#20010;&#32454;&#31890;&#24230;&#38754;&#37096;&#21160;&#20316;&#35270;&#20026;&#33509;&#24178;&#31163;&#25955;&#21160;&#20316;&#32534;&#30721;&#35789;&#30340;&#32452;&#21512;&#65292;&#24182;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#19981;&#21516;&#24773;&#24863;&#19979;&#21160;&#20316;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#8220;&#26174;&#24335;&#8221;&#21644;&#8220;&#31163;&#25955;&#8221;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#30340;ELP&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#20174;&#23398;&#20064;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#33258;&#21160;&#29983;&#25104;&#23545;&#32473;&#23450;&#35762;&#35805;&#32773;&#30340;&#33258;&#28982;&#22810;&#26679;&#30340;&#21709;&#24212;&#65292;&#36824;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#39044;&#20808;&#30830;&#23450;&#24577;&#24230;&#30340;&#21487;&#25511;&#21709;&#24212;&#12290;&#22312;&#20960;&#20010;&#23450;&#37327;&#24230;&#37327;&#25351;&#26631;&#19979;&#65292;&#25105;&#20204;&#30340;ELP&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Listener head generation centers on generating non-verbal behaviors (e.g., smile) of a listener in reference to the information delivered by a speaker. A significant challenge when generating such responses is the non-deterministic nature of fine-grained facial expressions during a conversation, which varies depending on the emotions and attitudes of both the speaker and the listener. To tackle this problem, we propose the Emotional Listener Portrait (ELP), which treats each fine-grained facial motion as a composition of several discrete motion-codewords and explicitly models the probability distribution of the motions under different emotion in conversation. Benefiting from the ``explicit'' and ``discrete'' design, our ELP model can not only automatically generate natural and diverse responses toward a given speaker via sampling from the learned distribution but also generate controllable responses with a predetermined attitude. Under several quantitative metrics, our ELP exhibits sig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;AI&#38598;&#25104;&#21516;&#26102;&#22788;&#29702;&#21452;&#37325;LIGO&#25506;&#27979;&#22120;&#21644;Virgo&#25506;&#27979;&#22120;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#25104;&#21151;&#35757;&#32451;&#20986;&#33021;&#22815;&#25506;&#27979;&#31209;&#24207;&#26356;&#39640;&#30340;&#24341;&#21147;&#27874;&#27169;&#24335;&#30340;AI&#20998;&#31867;&#22120;&#65292;&#24182;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#20272;&#35745;&#20102;&#28508;&#22312;&#20108;&#36827;&#21046;&#40657;&#27934;&#30340;&#24635;&#36136;&#37327;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#38598;&#25104;&#22312;&#22788;&#29702;&#22823;&#37327;&#20449;&#21495;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00052</link><description>&lt;p&gt;
AI&#38598;&#25104;&#29992;&#20110;&#25506;&#27979;&#31209;&#24207;&#26356;&#39640;&#30340;&#24341;&#21147;&#27874;&#27169;&#24335;&#65306;&#20934;&#22278;&#24418;&#65292;&#26059;&#36716;&#65292;&#38750;&#36827;&#21160;&#30340;&#20108;&#36827;&#21046;&#40657;&#27934;&#21512;&#24182;&#12290;(arXiv:2310.00052v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
AI ensemble for signal detection of higher order gravitational wave modes of quasi-circular, spinning, non-precessing binary black hole mergers. (arXiv:2310.00052v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;AI&#38598;&#25104;&#21516;&#26102;&#22788;&#29702;&#21452;&#37325;LIGO&#25506;&#27979;&#22120;&#21644;Virgo&#25506;&#27979;&#22120;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#25104;&#21151;&#35757;&#32451;&#20986;&#33021;&#22815;&#25506;&#27979;&#31209;&#24207;&#26356;&#39640;&#30340;&#24341;&#21147;&#27874;&#27169;&#24335;&#30340;AI&#20998;&#31867;&#22120;&#65292;&#24182;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#20272;&#35745;&#20102;&#28508;&#22312;&#20108;&#36827;&#21046;&#40657;&#27934;&#30340;&#24635;&#36136;&#37327;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#38598;&#25104;&#22312;&#22788;&#29702;&#22823;&#37327;&#20449;&#21495;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#31354;&#22270;&#27169;&#22411;&#65292;&#21516;&#26102;&#22788;&#29702;&#26469;&#33258;&#21452;&#37325;&#20808;&#36827;&#30340;LIGO&#25506;&#27979;&#22120;&#21644;&#20808;&#36827;&#30340;Virgo&#25506;&#27979;&#22120;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;240&#19975;&#20010;&#25551;&#36848;&#20934;&#22278;&#24418;&#65292;&#26059;&#36716;&#65292;&#38750;&#36827;&#21160;&#20108;&#36827;&#21046;&#40657;&#27934;&#21512;&#24182;&#30340;\texttt {IMRPhenomXPHM}&#27874;&#24418;&#26469;&#35757;&#32451;&#36825;&#20123;AI&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#32452;&#20998;&#36136;&#37327;$m_{\{1,2\}}\in[3M_\odot, 50 M_\odot]$&#65292;&#20010;&#20307;&#33258;&#26059;$s^z_{\{1,2\}}\in[-0.9, 0.9]$; &#24182;&#19988;&#21253;&#25324;$(\ell, |m|) = \{(2, 2), (2, 1), (3, 3), (3, 2), (4, 4)\}$&#27169;&#24335;&#20197;&#21450;$\ell = 3, |m| = 2$&#35856;&#27874;&#20013;&#30340;&#27169;&#24335;&#28151;&#21512;&#25928;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;Summit&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#30340;96&#20010;NVIDIA V100 GPU&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#22312;22&#23567;&#26102;&#20869;&#35757;&#32451;&#36825;&#20123;AI&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21019;&#24314;&#20102;AI&#39044;&#27979;&#22120;&#65292;&#29992;&#20110;&#20272;&#35745;&#25152;&#26377;AI&#20998;&#31867;&#22120;&#38598;&#21512;&#35782;&#21035;&#20986;&#30340;&#28508;&#22312;&#20108;&#36827;&#21046;&#40657;&#27934;&#30340;&#24635;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#36825;&#20010;&#38598;&#21512;&#12289;3&#20010;AI&#20998;&#31867;&#22120;&#21644;2&#20010;&#39044;&#27979;&#22120;&#26469;&#22788;&#29702;&#19968;&#20010;&#20026;&#26399;&#19968;&#24180;&#30340;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#27880;&#20837;&#20102;30&#19975;&#20010;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce spatiotemporal-graph models that concurrently process data from the twin advanced LIGO detectors and the advanced Virgo detector. We trained these AI classifiers with 2.4 million \texttt{IMRPhenomXPHM} waveforms that describe quasi-circular, spinning, non-precessing binary black hole mergers with component masses $m_{\{1,2\}}\in[3M_\odot, 50 M_\odot]$, and individual spins $s^z_{\{1,2\}}\in[-0.9, 0.9]$; and which include the $(\ell, |m|) = \{(2, 2), (2, 1), (3, 3), (3, 2), (4, 4)\}$ modes, and mode mixing effects in the $\ell = 3, |m| = 2$ harmonics. We trained these AI classifiers within 22 hours using distributed training over 96 NVIDIA V100 GPUs in the Summit supercomputer. We then used transfer learning to create AI predictors that estimate the total mass of potential binary black holes identified by all AI classifiers in the ensemble. We used this ensemble, 3 AI classifiers and 2 predictors, to process a year-long test set in which we injected 300,000 signals. This ye
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#24182;&#20855;&#26377;&#19982;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.00035</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;LoRA&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
LoRA ensembles for large language model fine-tuning. (arXiv:2310.00035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#24182;&#20855;&#26377;&#19982;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#34920;&#29616;&#20026;&#36807;&#20110;&#33258;&#20449;&#12289;&#26657;&#20934;&#19981;&#20339;&#20197;&#21450;&#23545;&#27979;&#35797;&#25968;&#25454;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#26679;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#36825;&#20123;&#20302;&#31209;&#36866;&#37197;&#22120;&#34920;&#31034;&#30340;&#21442;&#25968;&#25968;&#37327;&#38750;&#24120;&#23567;&#65292;&#27604;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#23567;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;LoRA&#36866;&#37197;&#22120;&#38598;&#25104;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples. One approach commonly used in vision for alleviating this issue is a deep ensemble, which constructs an ensemble by training the same model multiple times using different random initializations. However, there is a huge challenge to ensembling LLMs: the most effective LLMs are very, very large. Keeping a single LLM in memory is already challenging enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many settings. To address these issues, we propose an ensemble approach using Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique. Critically, these low-rank adapters represent a very small number of parameters, orders of magnitude less than the underlying pre-trained model. Thus, it is possible to construct large ensembles of LoRA adapters with almost the same computat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00034</link><description>&lt;p&gt;
PB-LLM: &#37096;&#20998;&#20108;&#20540;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PB-LLM: Partially Binarized Large Language Models. (arXiv:2310.00034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32593;&#32476;&#20108;&#20540;&#21270;&#65292;&#19968;&#31181;&#21387;&#32553;&#27169;&#22411;&#26435;&#37325;&#20026;&#21333;&#20010;&#27604;&#29305;&#30340;&#37327;&#21270;&#30340;&#28608;&#36827;&#24418;&#24335;&#65292;&#19987;&#38376;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21387;&#32553;&#12290;&#30001;&#20110;&#20043;&#21069;&#30340;&#20108;&#20540;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;LLMs&#23849;&#28291;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37096;&#20998;&#20108;&#20540;&#21270;LLM&#65288;PB-LLM&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#37327;&#21270;LLMs&#30340;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#25581;&#31034;&#20102;&#29616;&#26377;&#20108;&#20540;&#21270;&#31639;&#27861;&#30340;&#21407;&#29983;&#24212;&#29992;&#30340;&#26080;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#26174;&#33879;&#26435;&#37325;&#22312;&#23454;&#29616;&#20302;&#20301;&#37327;&#21270;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;PB-LLM&#22312;&#20108;&#36827;&#21046;&#21270;&#36807;&#31243;&#20013;&#36807;&#28388;&#20102;&#19968;&#23567;&#37096;&#20998;&#26174;&#33879;&#26435;&#37325;&#65292;&#23558;&#23427;&#20204;&#20998;&#37197;&#21040;&#39640;&#20301;&#23384;&#20648;&#20013;&#65292;&#21363;&#37096;&#20998;&#20108;&#20540;&#21270;&#12290;PB-LLM&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#30340;&#35282;&#24230;&#20998;&#26512;&#21518;&#65292;&#25193;&#23637;&#20102;&#24674;&#22797;&#37327;&#21270;LLMM&#23481;&#37327;&#30340;&#33021;&#21147;&#12290;&#22312;PTQ&#19979;&#65292;&#32467;&#21512;&#20102;GPTQ&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#37325;&#26500;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#26469;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26377;&#25928;&#24615;&#21644;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.00029</link><description>&lt;p&gt;
&#34701;&#20837;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#30340;&#23545;&#25239;&#39550;&#39542;&#34892;&#20026;&#29983;&#25104;&#25216;&#26415;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation. (arXiv:2310.00029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#26469;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26377;&#25928;&#24615;&#21644;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#20110;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#26292;&#38706;&#20986;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38754;&#23545;&#30340;&#26377;&#25928;&#21644;&#21512;&#29702;&#30340;&#39118;&#38505;&#20107;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#25163;&#34892;&#20026;&#65292;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#30340;&#39118;&#38505;&#35748;&#30693;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#29256;&#26412;&#30340;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#23545;&#25163;&#31574;&#30053;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#39640;&#20445;&#30495;&#30340;&#30828;&#20214;&#22312;&#29615;&#65288;HiL&#65289;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#22522;&#20110;&#24182;&#32447;&#24773;&#26223;&#30340;&#23545;&#27604;&#26696;&#20363;&#30740;&#31350;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#25163;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#34987;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;De-SaTE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#20197;&#21450;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#65292;&#20934;&#30830;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#25552;&#20379;&#20851;&#38190;&#25351;&#26631;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.00023</link><description>&lt;p&gt;
De-SaTE&#65306;&#29992;&#20110;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#39044;&#27979;&#30340;&#21435;&#22122;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics. (arXiv:2310.00023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;De-SaTE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#20197;&#21450;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#65292;&#20934;&#30830;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#25552;&#20379;&#20851;&#38190;&#25351;&#26631;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38146;&#31163;&#23376;&#30005;&#27744;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20174;&#20026;&#20415;&#25658;&#24335;&#30005;&#23376;&#35774;&#22791;&#20379;&#30005;&#21040;&#25512;&#21160;&#30005;&#21160;&#27773;&#36710;&#21644;&#25903;&#25345;&#33021;&#28304;&#23384;&#20648;&#31995;&#32479;&#12290;&#26377;&#25928;&#31649;&#29702;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#20934;&#30830;&#39044;&#27979;&#20854;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#36825;&#26159;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#30340;&#33021;&#37327;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#32463;&#36807;&#35757;&#32451;&#26469;&#22788;&#29702;&#30005;&#27744;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22122;&#22768;&#31867;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23567;&#27874;&#21435;&#22122;&#22120;&#26469;&#29983;&#25104;&#32534;&#30721;/&#20998;&#35299;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#36890;&#36807;&#19987;&#29992;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#36827;&#34892;&#22788;&#29702;&#12290;&#22312;NASA&#21644;CALCE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#22810;&#31181;&#22122;&#22768;&#27169;&#24335;&#19979;&#30340;&#24191;&#27867;&#20581;&#24247;&#25351;&#26631;&#20272;&#35745;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25253;&#21578;&#30340;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Lithium Ion (Li-ion) batteries have gained widespread popularity across various industries, from powering portable electronic devices to propelling electric vehicles and supporting energy storage systems. A central challenge in managing Li-ion batteries effectively is accurately predicting their Remaining Useful Life (RUL), which is a critical measure for proactive maintenance and predictive analytics. This study presents a novel approach that harnesses the power of multiple denoising modules, each trained to address specific types of noise commonly encountered in battery data. Specifically we use a denoising auto-encoder and a wavelet denoiser to generate encoded/decomposed representations, which are subsequently processed through dedicated self-attention transformer encoders. After extensive experimentation on the NASA and CALCE datasets, we are able to characterize a broad spectrum of health indicator estimations under a set of diverse noise patterns. We find that our reported error
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#20449;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;ACC-DA&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.00013</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#39550;&#39542;&#20013;&#22522;&#20110;&#39046;&#22495;&#21305;&#37197;&#30340;&#21327;&#21516;&#24863;&#30693;&#30340;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Adaptive Communications in Collaborative Perception with Domain Alignment for Autonomous Driving. (arXiv:2310.00013v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#20449;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;ACC-DA&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36890;&#20449;&#20801;&#35768;&#36710;&#36742;&#20132;&#25442;&#34917;&#20805;&#20449;&#24687;&#65292;&#22810;&#20010;&#36830;&#25509;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21327;&#21516;&#24863;&#30693;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#24863;&#30693;&#33021;&#21147;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36890;&#36947;&#21464;&#21270;&#21644;&#21327;&#21516;&#36710;&#36742;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACC-DA&#65292;&#19968;&#20010;&#36890;&#36947;&#24863;&#30693;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#24182;&#26368;&#23567;&#21270;&#24179;&#22343;&#20256;&#36755;&#24310;&#36831;&#65292;&#21516;&#26102;&#20943;&#36731;&#25968;&#25454;&#24322;&#26500;&#24615;&#24102;&#26469;&#30340;&#21103;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#28857;&#21253;&#25324;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#20256;&#36755;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#36890;&#36947;&#20449;&#24687;&#29366;&#24577;&#26500;&#24314;&#36890;&#20449;&#22270;&#24182;&#26368;&#23567;&#21270;&#20256;&#36755;&#24310;&#36831;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#65292;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#30721;&#29575;-&#30072;&#21464;&#25240;&#34935;&#20197;&#22686;&#24378;&#24863;&#30693;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#23427;&#26368;&#23567;&#21270;&#20102;&#26102;&#22495;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative perception among multiple connected and autonomous vehicles can greatly enhance perceptive capabilities by allowing vehicles to exchange supplementary information via communications. Despite advances in previous approaches, challenges still remain due to channel variations and data heterogeneity among collaborative vehicles. To address these issues, we propose ACC-DA, a channel-aware collaborative perception framework to dynamically adjust the communication graph and minimize the average transmission delay while mitigating the side effects from the data heterogeneity. Our novelties lie in three aspects. We first design a transmission delay minimization method, which can construct the communication graph and minimize the transmission delay according to different channel information state. We then propose an adaptive data reconstruction mechanism, which can dynamically adjust the rate-distortion trade-off to enhance perception efficiency. Moreover, it minimizes the temporal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#20934;&#21017;&#65292;&#24314;&#31435;&#36830;&#32493;&#30340;&#12289;&#21487;&#23548;&#30340;&#26680;&#20989;&#25968;&#65292;&#31616;&#21270;&#20102;&#22312;&#29699;&#38754;&#19978;&#31561;&#20998;&#28857;&#38598;&#30340;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38656;&#28041;&#21450;&#36816;&#31639;&#31526;&#30340;&#24773;&#20917;&#19979;&#23545;&#28508;&#22312;&#28857;&#31995;&#32479;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#24471;&#21040;&#20102;&#19982;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#27604;&#26356;&#39640;&#25928;&#30340;&#36817;&#20284;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00012</link><description>&lt;p&gt;
&#22312;&#29699;&#38754;&#19978;&#26080;&#36816;&#31639;&#31526;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Operator-free Equilibrium on the Sphere. (arXiv:2310.00012v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#20934;&#21017;&#65292;&#24314;&#31435;&#36830;&#32493;&#30340;&#12289;&#21487;&#23548;&#30340;&#26680;&#20989;&#25968;&#65292;&#31616;&#21270;&#20102;&#22312;&#29699;&#38754;&#19978;&#31561;&#20998;&#28857;&#38598;&#30340;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38656;&#28041;&#21450;&#36816;&#31639;&#31526;&#30340;&#24773;&#20917;&#19979;&#23545;&#28508;&#22312;&#28857;&#31995;&#32479;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#24471;&#21040;&#20102;&#19982;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#27604;&#26356;&#39640;&#25928;&#30340;&#36817;&#20284;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#26368;&#23567;&#24046;&#24322;&#24230;&#65292;&#23427;&#28304;&#20110;Legendre&#30340;ODE&#21644;&#29699;&#35856;&#20989;&#25968;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29699;&#38754;&#19978;&#31561;&#20998;&#28857;&#38598;&#30340;&#20934;&#21017;&#12290;&#24314;&#31435;&#20102;&#19968;&#20010;&#36830;&#32493;&#30340;&#12289;&#21487;&#23548;&#30340;&#26680;&#20989;&#25968;&#65292;&#20197;&#31616;&#21270;&#24191;&#20041;&#26368;&#23567;&#24046;&#24322;&#24230;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36890;&#36807;Pycke&#32479;&#35745;&#29983;&#25104;&#30340;&#30830;&#23450;&#24615;&#28857;&#26469;&#23545;&#29699;&#38754;&#19978;&#30340;Frank&#20989;&#25968;&#36827;&#34892;&#31215;&#20998;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#26680;&#20989;&#25968;&#23884;&#20837;&#30340;&#28857;&#31995;&#32479;&#30340;&#24046;&#24322;&#12290;&#36827;&#34892;&#20102;&#23450;&#37327;&#23454;&#39564;&#24182;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#25512;&#23548;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23548;&#25968;&#25506;&#32034;&#20855;&#26377;&#26368;&#23567;&#24046;&#24322;&#24230;&#30340;&#28508;&#22312;&#28857;&#31995;&#32479;&#65292;&#32780;&#26080;&#38656;&#28041;&#21450;&#20266;&#24494;&#20998;&#31639;&#23376;&#21644;Beltrami&#31639;&#23376;&#12290;&#19982;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#29983;&#25104;&#30340;&#38543;&#26426;&#28857;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#28857;&#21363;&#21487;&#22312;&#20219;&#24847;&#32500;&#24230;&#20013;&#36817;&#20284;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a generalized minimum discrepancy, which derives from Legendre's ODE and spherical harmonic theoretics to provide a new criterion of equidistributed pointsets on the sphere. A continuous and derivative kernel in terms of elementary functions is established to simplify the computation of the generalized minimum discrepancy. We consider the deterministic point generated from Pycke's statistics to integrate a Franke function for the sphere and investigate the discrepancies of points systems embedding with different kernels. Quantitive experiments are conducted and the results are analyzed. Our deduced model can explore latent point systems, that have the minimum discrepancy without the involvement of pseudodifferential operators and Beltrami operators, by the use of derivatives. Compared to the random point generated from the Monte Carlo method, only a few points generated by our method are required to approximate the target in arbitrary dimensions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#20849;&#24773;&#30340;&#20998;&#31867;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25351;&#20986;&#35757;&#32451;&#20154;&#24037;&#20849;&#24773;&#30340;&#26631;&#20934;&#27969;&#31243;&#21253;&#25324;&#24773;&#32490;&#35782;&#21035;&#12289;&#20998;&#26512;&#21644;&#21709;&#24212;&#21160;&#20316;&#12290;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#34394;&#25311;&#20195;&#29702;&#21644;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#26377;&#36739;&#39640;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.00010</link><description>&lt;p&gt;
&#20154;&#24037;&#20849;&#24773;&#20998;&#31867;&#65306;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Artificial Empathy Classification: A Survey of Deep Learning Techniques, Datasets, and Evaluation Scales. (arXiv:2310.00010v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00010
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#20849;&#24773;&#30340;&#20998;&#31867;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25351;&#20986;&#35757;&#32451;&#20154;&#24037;&#20849;&#24773;&#30340;&#26631;&#20934;&#27969;&#31243;&#21253;&#25324;&#24773;&#32490;&#35782;&#21035;&#12289;&#20998;&#26512;&#21644;&#21709;&#24212;&#21160;&#20316;&#12290;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#34394;&#25311;&#20195;&#29702;&#21644;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#26377;&#36739;&#39640;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#36741;&#21161;&#21457;&#23637;&#26426;&#22120;&#20154;&#23398;&#65288;ADR&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#20154;&#24037;&#20849;&#24773;&#65288;AE&#65289;&#20316;&#20026;&#21487;&#33021;&#30340;&#26410;&#26469;&#20154;&#26426;&#20132;&#20114;&#65288;HRI&#65289;&#33539;&#24335;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#20154;&#31867;&#20174;&#20986;&#29983;&#24320;&#22987;&#23601;&#23398;&#20250;&#20849;&#24773;&#65292;&#22240;&#27492;&#22312;&#26426;&#22120;&#20154;&#21644;&#26234;&#33021;&#26426;&#22120;&#20013;&#28748;&#36755;&#36825;&#31181;&#24863;&#35273;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#25968;&#25454;&#21644;&#26102;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#27169;&#20223;&#20849;&#24773;&#12290;&#20154;&#24037;&#20849;&#24773;&#30340;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;1&#65289;&#20351;&#29992;&#20174;&#35270;&#39057;&#25110;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#65288;ER&#65289;&#65292;2&#65289;&#20998;&#26512;&#24863;&#30693;&#30340;&#24773;&#32490;&#25110;&#20849;&#24773;&#31243;&#24230;&#20197;&#36873;&#25321;&#26368;&#20339;&#34892;&#21160;&#26041;&#26696;&#65292;3&#65289;&#25191;&#34892;&#21709;&#24212;&#21160;&#20316;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#20351;&#29992;&#34394;&#25311;&#20195;&#29702;&#25110;&#26426;&#22120;&#20154;&#30340;AE&#24120;&#24120;&#28041;&#21450;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
From the last decade, researchers in the field of machine learning (ML) and assistive developmental robotics (ADR) have taken an interest in artificial empathy (AE) as a possible future paradigm for human-robot interaction (HRI). Humans learn empathy since birth, therefore, it is challenging to instill this sense in robots and intelligent machines. Nevertheless, by training over a vast amount of data and time, imitating empathy, to a certain extent, can be possible for robots. Training techniques for AE, along with findings from the field of empathetic AI research, are ever-evolving. The standard workflow for artificial empathy consists of three stages: 1) Emotion Recognition (ER) using the retrieved features from video or textual data, 2) analyzing the perceived emotion or degree of empathy to choose the best course of action, and 3) carrying out a response action. Recent studies that show AE being used with virtual agents or robots often include Deep Learning (DL) techniques. For ins
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM-grounded Video Diffusion (LVD)&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#20877;&#36890;&#36807;&#36825;&#20123;&#24067;&#23616;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#21644;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#29983;&#25104;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.17444</link><description>&lt;p&gt;
LLM&#22522;&#20110;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM-grounded Video Diffusion Models. (arXiv:2309.17444v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17444
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM-grounded Video Diffusion (LVD)&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#20877;&#36890;&#36807;&#36825;&#20123;&#24067;&#23616;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#21644;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#29983;&#25104;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#31070;&#32463;&#35270;&#39057;&#29983;&#25104;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36890;&#24120;&#29983;&#25104;&#21463;&#38480;&#21046;&#25110;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#65288;&#20363;&#22914;&#65292;&#29978;&#33267;&#32570;&#20047;&#20174;&#24038;&#21521;&#21491;&#31227;&#21160;&#30340;&#29289;&#20307;&#30340;&#25552;&#31034;&#33021;&#21147;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLM&#22522;&#20110;&#35270;&#39057;&#25193;&#25955;&#65288;LVD&#65289;&#12290;LVD&#19981;&#30452;&#25509;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#29983;&#25104;&#35270;&#39057;&#65292;&#32780;&#26159;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26681;&#25454;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#24067;&#23616;&#26469;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#33021;&#22815;&#20174;&#21333;&#32431;&#30340;&#25991;&#26412;&#20013;&#29702;&#35299;&#22797;&#26434;&#30340;&#26102;&#31354;&#21160;&#24577;&#65292;&#24182;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#20013;&#36890;&#24120;&#35266;&#23519;&#21040;&#30340;&#25552;&#31034;&#21644;&#29289;&#20307;&#36816;&#21160;&#27169;&#24335;&#23494;&#20999;&#23545;&#40784;&#30340;&#24067;&#23616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#27880;&#24847;&#21147;&#22270;&#26469;&#25351;&#23548;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#19982;&#36825;&#20123;&#24067;&#23616;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion (e.g., even lacking the ability to be prompted for objects moving from left to right). To address these limitations, we introduce LLM-grounded Video Diffusion (LVD). Instead of directly generating videos from the text inputs, LVD first leverages a large language model (LLM) to generate dynamic scene layouts based on the text inputs and subsequently uses the generated layouts to guide a diffusion model for video generation. We show that LLMs are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. We then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STGNN&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#22312;&#21435;&#20013;&#24515;&#21270;&#32676;&#38598;&#25511;&#21046;&#20013;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#25193;&#23637;&#26469;&#26356;&#22909;&#22320;&#27169;&#25311;&#38598;&#20013;&#24335;&#25511;&#21046;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#25928;&#26524;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17437</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21435;&#20013;&#24515;&#21270;&#32676;&#38598;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Decentralized Flocking Controllers with Spatio-Temporal Graph Neural Network. (arXiv:2309.17437v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STGNN&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#22312;&#21435;&#20013;&#24515;&#21270;&#32676;&#38598;&#25511;&#21046;&#20013;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#25193;&#23637;&#26469;&#26356;&#22909;&#22320;&#27169;&#25311;&#38598;&#20013;&#24335;&#25511;&#21046;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#25928;&#26524;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#32676;&#38598;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#36827;&#34892;&#21435;&#20013;&#24515;&#21270;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#20165;&#20381;&#38752;&#30456;&#37051;&#29366;&#24577;&#26159;&#19981;&#36275;&#20197;&#27169;&#20223;&#38598;&#20013;&#24335;&#25511;&#21046;&#31574;&#30053;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;$L$-&#36339;&#24310;&#36831;&#29366;&#24577;&#32435;&#20837;&#35745;&#31639;&#20013;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#23427;&#21487;&#33021;&#23548;&#33268;&#36828;&#31163;&#30340;&#32676;&#20307;&#25104;&#21592;&#20043;&#38388;&#32570;&#20047;&#19968;&#33268;&#24615;&#65292;&#23567;&#38598;&#32676;&#30340;&#24418;&#25104;&#65292;&#20174;&#32780;&#23548;&#33268;&#36830;&#36143;&#30340;&#38598;&#32676;&#34892;&#20026;&#22833;&#36133;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26102;&#31354;GNN&#65292;&#21629;&#21517;&#20026;STGNN&#65292;&#23427;&#21253;&#25324;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#25193;&#23637;&#12290;&#31354;&#38388;&#25193;&#23637;&#25910;&#38598;&#26469;&#33258;&#36828;&#22788;&#39046;&#33322;&#32773;&#30340;&#24310;&#36831;&#29366;&#24577;&#65292;&#32780;&#26102;&#38388;&#25193;&#23637;&#21017;&#32435;&#20837;&#26469;&#33258;&#30456;&#37051;&#39046;&#33322;&#32773;&#30340;&#20808;&#21069;&#29366;&#24577;&#12290;&#36890;&#36807;&#20174;&#36825;&#20004;&#20010;&#25193;&#23637;&#20013;&#25910;&#38598;&#26356;&#24191;&#27867;&#12289;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Recently a line of researches has delved the use of graph neural networks (GNNs) for decentralized control in swarm robotics. However, it has been observed that relying solely on the states of immediate neighbors is insufficient to imitate a centralized control policy. To address this limitation, prior studies proposed incorporating $L$-hop delayed states into the computation. While this approach shows promise, it can lead to a lack of consensus among distant flock members and the formation of small clusters, consequently resulting in the failure of cohesive flocking behaviors. Instead, our approach leverages spatiotemporal GNN, named STGNN that encompasses both spatial and temporal expansions. The spatial expansion collects delayed states from distant neighbors, while the temporal expansion incorporates previous states from immediate neighbors. The broader and more comprehensive information gathered from both expansions results in more effective and accurate predictions. We develop an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.17425</link><description>&lt;p&gt;
&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Data Filtering Networks. (arXiv:2309.17425v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35757;&#32451;&#38598;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30707;&#65292;&#24182;&#20026;&#35821;&#35328;&#24314;&#27169;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#37319;&#38598;&#20173;&#28982;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#33539;&#24335;&#65292;&#20294;&#25968;&#25454;&#31574;&#21010;&#24448;&#24448;&#20173;&#28982;&#26159;&#20020;&#26102;&#30340;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#20174;&#32593;&#32476;&#19978;&#25910;&#38598;&#22823;&#37327;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#23558;&#27492;&#20505;&#36873;&#27744;&#31579;&#36873;&#21040;&#23454;&#38469;&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65288;DFN&#65289;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#29992;&#20110;&#31579;&#36873;&#30340;&#32593;&#32476;&#30340;&#36136;&#37327;&#19982;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26159;&#19981;&#21516;&#30340;&#65306;&#20363;&#22914;&#65292;&#19968;&#20010;&#22312;ImageNet&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#27604;&#19968;&#20010;&#22312;ImageNet&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#20294;&#22312;&#19968;&#23567;&#37096;&#20998;&#39640;&#36136;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#24046;&#30340;&#35757;&#32451;&#38598;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#25968;&#25454;&#38598;DFN-5B&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train 
&lt;/p&gt;</description></item><item><title>PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;</title><link>http://arxiv.org/abs/2309.17260</link><description>&lt;p&gt;
PlaceNav: &#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17260
&lt;/p&gt;
&lt;p&gt;
PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25299;&#25169;&#23548;&#33322;&#20998;&#20026;&#26426;&#22120;&#20154;&#26080;&#20851;&#21644;&#26426;&#22120;&#20154;&#29305;&#23450;&#30340;&#32452;&#20214;&#21487;&#20197;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#26426;&#22120;&#20154;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23548;&#33322;&#26041;&#27861;&#20173;&#21463;&#21040;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35745;&#31639;&#32553;&#25918;&#24615;&#24046;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PlaceNav&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#26469;&#36873;&#25321;&#25299;&#25169;&#23548;&#33322;&#27969;&#31243;&#20013;&#30340;&#23376;&#30446;&#26631;&#12290;&#36825;&#20351;&#24471;&#23376;&#30446;&#26631;&#36873;&#25321;&#26356;&#39640;&#25928;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22320;&#28857;&#35782;&#21035;&#20351;&#24471;&#36125;&#21494;&#26031;&#28388;&#27874;&#25104;&#20026;&#21487;&#33021;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#23376;&#30446;&#26631;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#19968;&#35774;&#35745;&#65292;&#24182;&#19988;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.16595</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#26377;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#22270;&#23398;&#20064;&#65306;&#20309;&#26102;&#20309;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#22270;&#25968;&#25454;&#65289;&#19978;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;LLM&#25991;&#29486;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#37325;&#35201;&#25968;&#25454;&#24418;&#24577;&#12290;&#25105;&#20204;&#26088;&#22312;&#20102;&#35299;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20309;&#26102;&#20309;&#22320;&#24341;&#20837;&#22270;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#8220;&#20309;&#26102;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#35774;&#32622;&#20013;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#20016;&#23500;&#25110;&#31232;&#32570;&#12290;&#23545;&#20110;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#24615;&#33021;&#30340;&#20004;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#25968;&#25454;&#27844;&#38706;&#21644;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65307;&#65288;ii&#65289;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#35777;&#25454;&#34920;&#26126;LLM&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#26377;&#26174;&#33879;&#30456;&#20851;&#65307;&#65288;iii&#65289;LLM&#22312;&#30446;&#26631;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#19982;&#27491;&#21521;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively relat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#28608;&#21457;&#21512;&#20316;&#30340;&#31574;&#30053;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#21512;&#20316;&#31574;&#30053;&#21644;&#24341;&#20837;&#40723;&#21169;&#22242;&#38431;&#22238;&#25253;&#30340;&#20462;&#25913;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#29616;&#23454;&#22256;&#22659;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#22343;&#20540;&#22330;&#21338;&#24328;&#29702;&#35770;&#65292;&#24314;&#31435;&#20102;&#26080;&#38480;&#22823;&#26234;&#33021;&#20307;&#38598;&#21512;&#20013;&#30340;&#24179;&#34913;&#35299;&#21644;&#22870;&#21169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16263</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#21160;&#21147;&#23398;&#65306;&#25506;&#32034;&#20855;&#26377;&#22343;&#20540;&#22330;&#22343;&#34913;&#30340;&#21338;&#24328;&#29702;&#35770;&#24773;&#26223;
&lt;/p&gt;
&lt;p&gt;
Cooperation Dynamics in Multi-Agent Systems: Exploring Game-Theoretic Scenarios with Mean-Field Equilibria. (arXiv:2309.16263v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#28608;&#21457;&#21512;&#20316;&#30340;&#31574;&#30053;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#21512;&#20316;&#31574;&#30053;&#21644;&#24341;&#20837;&#40723;&#21169;&#22242;&#38431;&#22238;&#25253;&#30340;&#20462;&#25913;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#29616;&#23454;&#22256;&#22659;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#22343;&#20540;&#22330;&#21338;&#24328;&#29702;&#35770;&#65292;&#24314;&#31435;&#20102;&#26080;&#38480;&#22823;&#26234;&#33021;&#20307;&#38598;&#21512;&#20013;&#30340;&#24179;&#34913;&#35299;&#21644;&#22870;&#21169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#26159;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#36890;&#24120;&#35201;&#27714;&#26234;&#33021;&#20307;&#22312;&#20010;&#20307;&#25910;&#30410;&#21644;&#38598;&#20307;&#22238;&#25253;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22312;&#21338;&#24328;&#29702;&#35770;&#24773;&#26223;&#20013;&#28608;&#21457;&#21512;&#20316;&#30340;&#31574;&#30053;&#65292;&#20363;&#22914;&#36845;&#20195;&#22234;&#24466;&#22256;&#22659;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#24517;&#39035;&#20248;&#21270;&#20010;&#20307;&#21644;&#22242;&#38431;&#30340;&#32467;&#26524;&#12290;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#21512;&#20316;&#31574;&#30053;&#23545;&#20110;&#20419;&#36827;&#37325;&#22797;&#21338;&#24328;&#20013;&#22242;&#38431;&#23548;&#21521;&#34892;&#20026;&#30340;&#26377;&#25928;&#24615;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#65292;&#21363;&#40723;&#21169;&#22242;&#38431;&#22238;&#25253;&#20063;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#20010;&#20307;&#25910;&#30410;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#29616;&#23454;&#22256;&#22659;&#12290;&#30740;&#31350;&#36824;&#25193;&#23637;&#21040;&#26234;&#33021;&#20307;&#20154;&#21475;&#25351;&#25968;&#22686;&#38271;&#30340;&#24773;&#26223;&#65288;$N \longrightarrow +\infty$&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#35745;&#31639;&#21644;&#24179;&#34913;&#30830;&#23450;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21033;&#29992;&#22343;&#20540;&#22330;&#21338;&#24328;&#29702;&#35770;&#65292;&#24314;&#31435;&#20102;&#26080;&#38480;&#22823;&#26234;&#33021;&#20307;&#38598;&#21512;&#20013;&#30340;&#24179;&#34913;&#35299;&#21644;&#22870;&#21169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperation is fundamental in Multi-Agent Systems (MAS) and Multi-Agent Reinforcement Learning (MARL), often requiring agents to balance individual gains with collective rewards. In this regard, this paper aims to investigate strategies to invoke cooperation in game-theoretic scenarios, namely the Iterated Prisoner's Dilemma, where agents must optimize both individual and group outcomes. Existing cooperative strategies are analyzed for their effectiveness in promoting group-oriented behavior in repeated games. Modifications are proposed where encouraging group rewards will also result in a higher individual gain, addressing real-world dilemmas seen in distributed systems. The study extends to scenarios with exponentially growing agent populations ($N \longrightarrow +\infty$), where traditional computation and equilibrium determination are challenging. Leveraging mean-field game theory, equilibrium solutions and reward structures are established for infinitely large agent sets in repea
&lt;/p&gt;</description></item><item><title>Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15806</link><description>&lt;p&gt;
Lyra: &#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#30340;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15806
&lt;/p&gt;
&lt;p&gt;
Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#25506;&#32034;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#24187;&#35273;&#30340;&#20943;&#36731;&#21644;&#36890;&#36807;&#35777;&#26126;&#22120;&#38169;&#35823;&#28040;&#24687;&#30340;&#32454;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Lyra&#65292;&#19968;&#31181;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#20462;&#27491;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65306;&#24037;&#20855;&#20462;&#27491;&#65288;TC&#65289;&#21644;&#29468;&#24819;&#20462;&#27491;&#65288;CC&#65289;&#12290;&#20026;&#20102;&#22312;&#24418;&#24335;&#35777;&#26126;&#30340;&#21518;&#22788;&#29702;&#20013;&#23454;&#29616;&#24037;&#20855;&#20462;&#27491;&#65292;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#35777;&#26126;&#24037;&#20855;&#65288;&#22914;Sledgehammer&#65289;&#26469;&#25351;&#23548;&#26367;&#25442;&#19981;&#27491;&#30830;&#30340;&#24037;&#20855;&#12290;&#24037;&#20855;&#20462;&#27491;&#26174;&#33879;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29468;&#24819;&#20462;&#27491;&#65292;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#26426;&#21046;&#65292;&#26088;&#22312;&#19982;&#35777;&#26126;&#22120;&#20114;&#21160;&#65292;&#36890;&#36807;&#35777;&#26126;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#36827;&#19968;&#27493;&#23436;&#21892;&#24418;&#24335;&#35777;&#26126;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15048</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#26029;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#36890;&#36807;&#39034;&#24207;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#30001;&#19968;&#32452;&#21807;&#19968;&#30340;&#31867;&#32452;&#25104;&#12290;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102;&#19981;&#25552;&#20379;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#26631;&#35782;&#31526;&#65288;&#25110;&#20219;&#21153;ID&#65289;&#12290;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#39044;&#27979;&#20219;&#21153;ID&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20849;&#20139;&#32593;&#32476;&#20013;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#27599;&#20010;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#36951;&#24536;&#12290;&#35813;&#26041;&#27861;&#20013;&#27599;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#38750;&#24120;&#35268;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#20256;&#32479;&#20998;&#31867;&#22120;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#12290;&#31163;&#32676;&#26816;&#27979;&#22120;&#21487;&#20197;&#23545;&#20219;&#21153;&#20869;&#65288;&#20998;&#24067;&#20869;&#65288;IND&#65289;&#65289;&#30340;&#31867;&#36827;&#34892;&#39044;&#27979;&#21644;&#35782;&#21035;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#25512;&#26029;&#26399;&#38388;&#65292;&#31163;&#32676;&#26816;&#27979;&#33021;&#21147;&#26159;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;ID&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#35748;&#20026;&#20351;&#29992;&#20256;&#32479;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;ID&#39044;&#27979;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14681</link><description>&lt;p&gt;
&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26377;&#24517;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#33391;&#22909;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#26631;&#20934;&#33539;&#24335;&#20013;&#23384;&#22312;&#20197;&#19979;&#24330;&#31471;&#65306;&#26131;&#21463;&#36873;&#23450;&#28436;&#31034;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;&#36825;&#20123;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;ICL&#65292;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20154;&#24037;&#28436;&#31034;&#30340;&#33539;&#20363;&#12290;SEC&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#65292;&#19981;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#20316;&#20026;ICL&#20013;&#30340;&#28436;&#31034;&#65292;&#32780;&#26159;&#35201;&#27714;LLMs&#39318;&#20808;&#33258;&#34892;&#21019;&#24314;&#28436;&#31034;&#65292;&#28982;&#21518;&#29983;&#25104;&#26368;&#32456;&#36755;&#20986;&#12290;SEC&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#36866;&#24212;&#21407;&#22987;ICL&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#65292;&#24182;&#19988;&#26356;&#21152;&#20415;&#25463;&#65306;&#22240;&#20026;&#21487;&#20197;&#33410;&#30465;&#31034;&#20363;&#21644;&#29702;&#30001;&#30340;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#31639;&#26415;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understandin
&lt;/p&gt;</description></item><item><title>LTU-AS&#26159;&#19968;&#20010;&#20855;&#26377;&#26222;&#36866;&#38899;&#39057;&#24863;&#30693;&#21644;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#21644;&#32852;&#21512;&#29702;&#35299;&#21475;&#35821;&#25991;&#26412;&#12289;&#35821;&#38899;&#22768;&#38899;&#23398;&#21644;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.14405</link><description>&lt;p&gt;
&#32852;&#21512;&#38899;&#39057;&#21644;&#35821;&#38899;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Joint Audio and Speech Understanding. (arXiv:2309.14405v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14405
&lt;/p&gt;
&lt;p&gt;
LTU-AS&#26159;&#19968;&#20010;&#20855;&#26377;&#26222;&#36866;&#38899;&#39057;&#24863;&#30693;&#21644;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#21644;&#32852;&#21512;&#29702;&#35299;&#21475;&#35821;&#25991;&#26412;&#12289;&#35821;&#38899;&#22768;&#38899;&#23398;&#21644;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21608;&#22260;&#20805;&#26021;&#30528;&#21253;&#25324;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#22768;&#38899;&#22312;&#20869;&#30340;&#38899;&#39057;&#20449;&#21495;&#12290;&#23545;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214;&#30340;&#35782;&#21035;&#21644;&#29702;&#35299;&#65292;&#20197;&#21450;&#23545;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#28145;&#21051;&#29702;&#35299;&#65292;&#26500;&#25104;&#20102;&#22522;&#26412;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#27425;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;LTU-AS&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#26222;&#36941;&#38899;&#39057;&#24863;&#30693;&#21644;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23558;Whisper&#20316;&#20026;&#24863;&#30693;&#27169;&#22359;&#21644;LLaMA&#20316;&#20026;&#25512;&#29702;&#27169;&#22359;&#36827;&#34892;&#38598;&#25104;&#65292;LTU-AS&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#21644;&#32852;&#21512;&#29702;&#35299;&#21475;&#35821;&#25991;&#26412;&#12289;&#35821;&#38899;&#22768;&#38899;&#23398;&#20197;&#21450;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214; - &#20960;&#20046;&#21487;&#20197;&#20174;&#38899;&#39057;&#20449;&#21495;&#20013;&#24863;&#30693;&#21040;&#30340;&#19968;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper as a perception module and LLaMA as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.14402</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;3.2&#37096;&#20998;&#65292;&#30693;&#35782;&#25805;&#25511;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 3.2, Knowledge Manipulation. (arXiv:2309.14402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23384;&#20648;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#22312;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#20854;&#23384;&#20648;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#22235;&#31181;&#25805;&#25511;&#31867;&#22411;&#65306;&#26816;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#20160;&#20040;&#8221;&#65289;&#12289;&#20998;&#31867;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#22855;&#25968;&#36824;&#26159;&#20598;&#25968;&#8221;&#65289;&#12289;&#27604;&#36739;&#65288;&#20363;&#22914;&#65292;&#8220;&#22312;&#23646;&#24615;X&#20013;A&#26159;&#21542;&#22823;&#20110;B&#8221;&#65289;&#21644;&#36870;&#21521;&#25628;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;&#21738;&#20010;&#20154;&#30340;&#23646;&#24615;X&#31561;&#20110;T&#8221;&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20687;GPT2/3/4&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#25110;&#27604;&#36739;&#20219;&#21153;&#20013;&#24456;&#38590;&#32988;&#20219;&#65292;&#38500;&#38750;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;Chain of Thoughts&#65288;CoTs&#65289;&#12290;&#26080;&#35770;&#25552;&#31034;&#26159;&#20160;&#20040;&#65292;&#23427;&#20204;&#22312;&#36870;&#21521;&#30693;&#35782;&#25628;&#32034;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#20026;&#25511;&#21046;&#23454;&#39564;&#32780;&#35774;&#35745;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35777;&#23454;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models can store vast amounts of factual knowledge, but their ability to use this knowledge for logical reasoning remains questionable. This paper explores a language model's ability to manipulate its stored knowledge during inference. We focus on four manipulation types: retrieval (e.g., "What is person A's attribute X"), classification (e.g., "Is A's attribute X even or odd?"), comparison (e.g., "Is A greater than B in attribute X?") and inverse search (e.g., "Which person's attribute X equals T?")  We observe that pre-trained language models like GPT2/3/4 excel in knowledge retrieval but struggle with simple classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. They also perform poorly in inverse knowledge search, irrespective of the prompts. Our primary contribution is a synthetic dataset for a controlled experiment that confirms these inherent weaknesses: a language model cannot efficiently manipulate knowledge
&lt;/p&gt;</description></item><item><title>LinGCN&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#21644;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#21644;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.14331</link><description>&lt;p&gt;
LinGCN: &#32467;&#26500;&#21270;&#30340;&#32447;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21516;&#24577;&#21152;&#23494;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference. (arXiv:2309.14331v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14331
&lt;/p&gt;
&lt;p&gt;
LinGCN&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#21644;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#21644;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#38271;&#24050;&#32463;&#22312;&#20010;&#20154;&#21307;&#30103;&#21644;&#37329;&#34701;&#31995;&#32479;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#30340;&#38761;&#21629;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20113;&#31471;&#37096;&#32626;GCN&#24341;&#21457;&#20102;&#23545;&#23458;&#25143;&#25968;&#25454;&#21487;&#33021;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#65292;&#37319;&#29992;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#21487;&#20197;&#30830;&#20445;&#25935;&#24863;&#23458;&#25143;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#24341;&#20837;&#20102;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LinGCN&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#20056;&#27861;&#28145;&#24230;&#24182;&#20248;&#21270;HE&#22522;&#20110;GCN&#25512;&#26029;&#24615;&#33021;&#30340;&#26694;&#26550;&#12290;LinGCN&#22260;&#32469;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#23637;&#24320;&#65306;&#65288;1&#65289;&#21487;&#24494;&#30340;&#32467;&#26500;&#21270;&#32447;&#24615;&#21270;&#31639;&#27861;&#65292;&#25645;&#37197;&#21442;&#25968;&#21270;&#30340;&#31163;&#25955;&#25351;&#31034;&#20989;&#25968;&#65292;&#36890;&#36807;&#19982;&#27169;&#22411;&#26435;&#37325;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#20197;&#28385;&#36275;&#20248;&#21270;&#30446;&#26631;&#12290;&#36825;&#31181;&#31574;&#30053;&#20419;&#36827;&#20102;&#32454;&#31890;&#24230;&#30340;&#33410;&#28857;&#32423;&#38750;&#32447;&#24615;&#20301;&#32622;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20174;&#20132;&#20114;&#24335;&#21465;&#20107;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35282;&#24230;&#23545;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#28216;&#25103;&#20027;&#25345;&#36827;&#34892;&#24314;&#27169;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#27979;&#35797;&#31867;&#21035;&#26469;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.13702</link><description>&lt;p&gt;
&#25216;&#33021;&#26816;&#27979;&#65306;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#28216;&#25103;&#20027;&#25345;&#27169;&#22411;&#30340;&#19968;&#20123;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Skill Check: Some Considerations on the Evaluation of Gamemastering Models for Role-playing Games. (arXiv:2309.13702v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20174;&#20132;&#20114;&#24335;&#21465;&#20107;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35282;&#24230;&#23545;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#28216;&#25103;&#20027;&#25345;&#36827;&#34892;&#24314;&#27169;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#27979;&#35797;&#31867;&#21035;&#26469;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#65292;&#28216;&#25103;&#20027;&#25345;&#65288;GM&#65289;&#26159;&#36127;&#36131;&#28216;&#25103;&#30340;&#29609;&#23478;&#65292;&#24517;&#39035;&#35774;&#35745;&#29609;&#23478;&#38754;&#20020;&#30340;&#25361;&#25112;&#24182;&#35762;&#36848;&#20182;&#20204;&#34892;&#21160;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20174;&#20132;&#20114;&#24335;&#21465;&#20107;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35282;&#24230;&#35752;&#35770;&#20102;&#23545;GM&#36827;&#34892;&#24314;&#27169;&#30340;&#25361;&#25112;&#12290;&#22312;&#35752;&#35770;&#36825;&#20123;&#25361;&#25112;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#27979;&#35797;&#31867;&#21035;&#26469;&#35780;&#20272;&#36825;&#20123;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#20351;&#29992;ChatGPT&#12289;Bard&#21644;OpenAssistant&#20316;&#20026;&#24320;&#31665;&#21363;&#29992;&#30340;GM&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In role-playing games a Game Master (GM) is the player in charge of the game, who must design the challenges the players face and narrate the outcomes of their actions. In this work we discuss some challenges to model GMs from an Interactive Storytelling and Natural Language Processing perspective. Following those challenges we propose three test categories to evaluate such dialogue systems, and we use them to test ChatGPT, Bard and OpenAssistant as out-of-the-box GMs.
&lt;/p&gt;</description></item><item><title>MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13459</link><description>&lt;p&gt;
&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25972;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Model-Agnostic Graph Neural Network for Integrating Local and Global Information. (arXiv:2309.13459v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13459
&lt;/p&gt;
&lt;p&gt;
MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#20197;&#22270;&#20026;&#37325;&#28857;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;GNN&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#30001;&#20110;&#40657;&#30418;&#29305;&#24615;&#65292;&#32467;&#26524;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65307;&#26080;&#27861;&#23398;&#20064;&#19981;&#21516;&#39034;&#24207;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MaGNet&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#20174;&#39640;&#38454;&#37051;&#23621;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;MaGNet&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#22270;&#25299;&#25169;&#19979;&#22797;&#26434;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#20272;&#35745;&#27169;&#22411;&#21644;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37325;&#35201;&#33410;&#28857;&#29305;&#24449;&#30340;&#35299;&#37322;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;Rademacher&#22797;&#26434;&#24230;&#24314;&#31435;&#20102;MaGNet&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, existing GNNs suffer from two significant limitations: a lack of interpretability in results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to sequentially integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and important node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and showcase its pow
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;&#20854;&#21152;&#24378;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#33021;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13414</link><description>&lt;p&gt;
&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#24102;&#26377;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#20840;&#33021;&#36924;&#36817;&#22120;
&lt;/p&gt;
&lt;p&gt;
State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory. (arXiv:2309.13414v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;&#20854;&#21152;&#24378;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#33021;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#31616;&#21333;&#26377;&#25928;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#24207;&#21015;&#24314;&#27169;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#27839;&#26102;&#38388;&#26041;&#21521;&#32570;&#20047;&#38750;&#32447;&#24615;&#28608;&#27963;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#28155;&#21152;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21487;&#20197;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30475;&#21040;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;&#29702;&#35770;&#32467;&#26524;&#32463;&#36807;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the exponential decaying memory issue. Theoretical results are justified by numerical verifications.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#23376;&#35789;&#25286;&#20998;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.12491</link><description>&lt;p&gt;
&#25506;&#32034;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation. (arXiv:2309.12491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#23376;&#35789;&#25286;&#20998;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26631;&#35760;&#21270;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#34987;&#22823;&#22810;&#25968;&#20154;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#32844;&#19994;&#21517;&#31216;&#30340;&#39057;&#29575;&#12289;&#23427;&#20204;&#22312;&#23376;&#35789;&#26631;&#35760;&#22120;&#35789;&#27719;&#34920;&#20013;&#30340;&#34920;&#31034;&#20197;&#21450;&#24615;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22899;&#24615;&#21644;&#38750;&#21051;&#26495;&#21360;&#35937;&#30340;&#24615;&#21035;&#32844;&#19994;&#21517;&#31216;&#30340;&#21464;&#24418;&#65288;&#20363;&#22914;&#65292;&#35199;&#29677;&#29273;&#35821;&#20013;&#30340;"doctora"&#34920;&#31034;"&#22899;&#21307;&#29983;"&#65289;&#24448;&#24448;&#34987;&#25286;&#20998;&#25104;&#22810;&#20010;&#23376;&#35789;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#20854;&#24433;&#21709;&#22823;&#20110;&#23376;&#35789;&#25286;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#35821;&#26009;&#24211;&#19981;&#20844;&#24320;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the effect of tokenization on gender bias in machine translation, an aspect that has been largely overlooked in previous works. Specifically, we focus on the interactions between the frequency of gendered profession names in training data, their representation in the subword tokenizer's vocabulary, and gender bias. We observe that female and non-stereotypical gender inflections of profession names (e.g., Spanish "doctora" for "female doctor") tend to be split into multiple subword tokens. Our results indicate that the imbalance of gender forms in the model's training corpus is a major factor contributing to gender bias and has a greater impact than subword splitting. We show that analyzing subword splits provides good estimates of gender-form imbalance in the training data and can be used even when the corpus is not publicly available. We also demonstrate that fine-tuning just the token embedding layer can decrease the gap in gender prediction accuracy between female and male 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#23433;&#20840;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#65292;&#21152;&#36895;&#39564;&#35777;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#22312;&#27979;&#35797;&#20013;&#23481;&#26131;&#24341;&#21457;&#25925;&#38556;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#21644;&#33021;&#22815;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#27169;&#25311;&#30340;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#26159;&#21542;&#26377;&#21161;&#20110;&#23545;&#26032;&#22330;&#26223;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#36827;&#34892;&#26356;&#24555;&#30340;&#23398;&#20064;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12474</link><description>&lt;p&gt;
SAVME: &#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#33258;&#21160;&#31995;&#32479;&#30340;&#39640;&#25928;&#23433;&#20840;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
SAVME: Efficient Safety Validation for Autonomous Systems Using Meta-Learning. (arXiv:2309.12474v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#23433;&#20840;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#65292;&#21152;&#36895;&#39564;&#35777;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#22312;&#27979;&#35797;&#20013;&#23481;&#26131;&#24341;&#21457;&#25925;&#38556;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#21644;&#33021;&#22815;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#27169;&#25311;&#30340;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#26159;&#21542;&#26377;&#21161;&#20110;&#23545;&#26032;&#22330;&#26223;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#36827;&#34892;&#26356;&#24555;&#30340;&#23398;&#20064;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#21069;&#65292;&#21457;&#29616;&#33258;&#21160;&#31995;&#32479;&#30340;&#28508;&#22312;&#25925;&#38556;&#38750;&#24120;&#37325;&#35201;&#12290;&#34394;&#26500;&#27861;&#24120;&#34987;&#29992;&#26469;&#35780;&#20272;&#27492;&#31867;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#20294;&#36816;&#34892;&#20934;&#30830;&#27169;&#25311;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23558;&#20803;&#23398;&#20064;&#31574;&#30053;&#19982;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#21152;&#36895;&#39564;&#35777;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23398;&#20064;&#22312;&#27979;&#35797;&#20013;&#23481;&#26131;&#24341;&#21457;&#25925;&#38556;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#65292;&#20197;&#21450;&#33021;&#22815;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#27169;&#25311;&#30340;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#12290;&#22312;&#20803;&#23398;&#20064;&#30340;&#31934;&#31070;&#19979;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#26159;&#21542;&#26377;&#21161;&#20110;&#23545;&#26032;&#22330;&#26223;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#36827;&#34892;&#26356;&#24555;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;3D&#39550;&#39542;&#27169;&#25311;&#22120;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#25972;&#21512;&#20102;16&#31181;&#20445;&#30495;&#24230;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering potential failures of an autonomous system is important prior to deployment. Falsification-based methods are often used to assess the safety of such systems, but the cost of running many accurate simulation can be high. The validation can be accelerated by identifying critical failure scenarios for the system under test and by reducing the simulation runtime. We propose a Bayesian approach that integrates meta-learning strategies with a multi-armed bandit framework. Our method involves learning distributions over scenario parameters that are prone to triggering failures in the system under test, as well as a distribution over fidelity settings that enable fast and accurate simulations. In the spirit of meta-learning, we also assess whether the learned fidelity settings distribution facilitates faster learning of the scenario parameter distributions for new scenarios. We showcase our methodology using a cutting-edge 3D driving simulator, incorporating 16 fidelity settings fo
&lt;/p&gt;</description></item><item><title>LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11998</link><description>&lt;p&gt;
LMSYS-Chat-1M&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11998
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19982;&#20854;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LMSYS-Chat-1M&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#19982;25&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30340;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#25105;&#20204;&#30340;Vicuna&#28436;&#31034;&#21644;Chatbot Arena&#32593;&#31449;&#19978;&#30340;21&#19975;&#20010;&#29420;&#31435;IP&#22320;&#22336;&#20013;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#20869;&#23481;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#31574;&#21010;&#36807;&#31243;&#12289;&#22522;&#26412;&#32479;&#35745;&#25968;&#25454;&#21644;&#20027;&#39064;&#20998;&#24067;&#65292;&#24378;&#35843;&#20854;&#22810;&#26679;&#24615;&#12289;&#29420;&#29305;&#24615;&#21644;&#35268;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#29992;&#20363;&#23637;&#31034;&#20102;&#23427;&#30340;&#22810;&#26679;&#24615;&#65306;&#24320;&#21457;&#19982;GPT-4&#34920;&#29616;&#30456;&#20284;&#30340;&#20869;&#23481;&#36807;&#28388;&#27169;&#22411;&#12289;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#22522;&#20934;&#12289;&#35757;&#32451;&#19982;Vicuna&#34920;&#29616;&#30456;&#20284;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12289;&#21019;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25105;&#20204;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is pub
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.11054</link><description>&lt;p&gt;
&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#24605;&#36335;&#38142;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#23545;&#35774;&#35745;&#24605;&#36335;&#38142;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#27604;&#36739;&#20102;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#21508;&#31181;&#31243;&#24207;&#24605;&#36335;&#38142;&#65292;&#21253;&#25324;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#12289;&#27880;&#37322;&#25551;&#36848;&#31243;&#24207;&#21644;&#38750;&#25551;&#36848;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#32534;&#31243;&#35821;&#35328;&#23545;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#24433;&#21709;&#65292;&#27604;&#36739;&#20102;Python&#21644;Wolfram&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;GSM8K&#12289;MATHQA&#21644;SVAMP&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#36890;&#24120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20855;&#26377;30B&#21442;&#25968;&#30340;&#26368;&#20339;&#32452;&#21512;&#26126;&#26174;&#36229;&#36807;&#20102;GPT-3.5-turbo&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#22810;&#26679;&#24615;&#65292;&#22240;&#27492;&#36890;&#24120;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#26356;&#22909;&#36873;&#25321;&#27604;Wolfram&#35821;&#35328;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#32771;&#34385;&#22240;&#32032;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into acco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#35775;&#35848;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;LLMs&#22312;&#26412;&#31185;&#24037;&#31243;&#25945;&#32946;&#20013;&#30340;&#20351;&#29992;&#21644;&#35266;&#28857;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#23398;&#29983;&#21644;&#25945;&#24072;&#23545;LLMs&#30340;&#37319;&#29992;&#25552;&#20379;&#20102;&#27934;&#35265;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.10694</link><description>&lt;p&gt;
"&#20276;&#38543;&#30528;&#20255;&#22823;&#30340;&#21147;&#37327;&#32780;&#26469;&#30340;&#26159;&#20255;&#22823;&#30340;&#36131;&#20219;&#65281;": &#23398;&#29983;&#21644;&#25945;&#24072;&#23545;LLMs&#23545;&#26412;&#31185;&#24037;&#31243;&#25945;&#32946;&#24433;&#21709;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
"With Great Power Comes Great Responsibility!": Student and Instructor Perspectives on the influence of LLMs on Undergraduate Engineering Education. (arXiv:2309.10694v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#35775;&#35848;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;LLMs&#22312;&#26412;&#31185;&#24037;&#31243;&#25945;&#32946;&#20013;&#30340;&#20351;&#29992;&#21644;&#35266;&#28857;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#23398;&#29983;&#21644;&#25945;&#24072;&#23545;LLMs&#30340;&#37319;&#29992;&#25552;&#20379;&#20102;&#27934;&#35265;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27969;&#34892;&#24341;&#21457;&#20102;&#23398;&#26415;&#30028;&#30340;&#35752;&#35770;&#65292;&#23398;&#29983;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;LLMs&#30340;&#35838;&#31243;&#26597;&#35810;&#24037;&#20855;&#65292;&#25945;&#24072;&#20204;&#21017;&#25506;&#32034;&#20102;&#22522;&#20110;LLMs&#30340;&#25945;&#23398;&#21644;&#30740;&#31350;&#12290;&#23613;&#31649;&#27491;&#22312;&#21162;&#21147;&#24320;&#21457;&#19987;&#20026;&#23398;&#29983;&#21644;&#25945;&#24072;&#23450;&#21046;&#30340;LLMs&#24037;&#20855;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#30340;&#29992;&#25143;&#30740;&#31350;&#26469;&#25429;&#25417;&#23398;&#29983;&#21644;&#25945;&#24072;&#23545;LLMs&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#21360;&#24230;&#30340;&#26412;&#31185;&#24037;&#31243;&#38498;&#26657;&#36827;&#34892;&#35843;&#26597;&#21644;&#35775;&#35848;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#26412;&#25991;&#20351;&#29992;&#20102;1306&#20221;&#23398;&#29983;&#35843;&#26597;&#22238;&#31572;&#12289;112&#20221;&#23398;&#29983;&#35775;&#35848;&#21644;27&#20221;&#25945;&#24072;&#35775;&#35848;&#65292;&#25506;&#35752;&#20102;ChatGPT&#65288;&#19968;&#31181;&#27969;&#34892;&#30340;LLM&#65289;&#22312;&#23398;&#26415;&#19978;&#30340;&#20351;&#29992;&#24773;&#20917;&#12289;&#24863;&#30693;&#21040;&#30340;&#22909;&#22788;&#12289;&#23041;&#32961;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#24378;&#23398;&#29983;&#21644;&#25945;&#24072;&#37319;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;&#36825;&#20123;&#27934;&#35265;&#36827;&#19968;&#27493;&#29992;&#20110;&#35752;&#35770;LLMs&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise in popularity of Large Language Models (LLMs) has prompted discussions in academic circles, with students exploring LLM-based tools for coursework inquiries and instructors exploring them for teaching and research. Even though a lot of work is underway to create LLM-based tools tailored for students and instructors, there is a lack of comprehensive user studies that capture the perspectives of students and instructors regarding LLMs. This paper addresses this gap by conducting surveys and interviews within undergraduate engineering universities in India. Using 1306 survey responses among students, 112 student interviews, and 27 instructor interviews around the academic usage of ChatGPT (a popular LLM), this paper offers insights into the current usage patterns, perceived benefits, threats, and challenges, as well as recommendations for enhancing the adoption of LLMs among students and instructors. These insights are further utilized to discuss the practical implications of LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#22521;&#20859;&#21512;&#20316;&#19982;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#29983;&#24577;&#31995;&#32479;&#24320;&#21457;&#20102;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.10007</link><description>&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#38388;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#19982;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem. (arXiv:2309.10007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#22521;&#20859;&#21512;&#20316;&#19982;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#29983;&#24577;&#31995;&#32479;&#24320;&#21457;&#20102;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#22521;&#20859;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#24037;&#20855;&#65292;&#24320;&#21457;&#20986;&#19982;&#30495;&#23454;&#30340;Nigel&#21644;F1TENTH&#20004;&#31181;&#27604;&#20363;&#33258;&#20027;&#36710;&#36742;&#24179;&#21488;&#20855;&#26377;&#29420;&#29305;&#29305;&#24615;&#21644;&#33021;&#21147;&#30340;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#29983;&#24577;&#31995;&#32479;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#20132;&#21449;&#36335;&#21475;&#31359;&#36234;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#32452;&#21512;&#20316;&#36710;&#36742;&#65288;Nigel&#65289;&#22312;&#21333;&#20010;&#25110;&#22810;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#20849;&#20139;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#65292;&#37319;&#29992;&#19968;&#31181;&#20844;&#20849;&#31574;&#30053;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#30340;&#22836;&#23545;&#22836;&#33258;&#20027;&#36187;&#36710;&#38382;&#39064;&#65292;&#20351;&#29992;&#21478;&#19968;&#32452;&#36710;&#36742;&#65288;F1TENTH&#65289;&#22312;&#22810;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#37319;&#29992;&#20010;&#20307;&#31574;&#30053;&#26041;&#27861;&#12290;&#22312;&#20219;&#20309;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#37117;&#37319;&#29992;&#20102;&#20998;&#25955;&#23398;&#20064;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles. We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies. We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#28216;&#25103;&#35268;&#21017;&#29983;&#25104;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#30340;&#35268;&#21017;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#65292;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.09476</link><description>&lt;p&gt;
&#26426;&#26800;&#21270;&#29983;&#25104;&#22120;2.0: &#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#30340;&#28216;&#25103;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules. (arXiv:2309.09476v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#28216;&#25103;&#35268;&#21017;&#29983;&#25104;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#30340;&#35268;&#21017;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#65292;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#28216;&#25103;&#35774;&#35745;&#65288;AGD&#65289;&#26159;&#30740;&#31350;&#33258;&#21160;&#29983;&#25104;&#28216;&#25103;&#35268;&#21017;&#30340;&#25216;&#26415;&#28216;&#25103;&#30740;&#31350;&#30340;&#19968;&#20010;&#38271;&#26399;&#35838;&#39064;&#12290; AGD&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#20154;&#31867;&#29609;&#23478;&#28216;&#25103;&#30340;&#36817;&#20284;&#65292;&#21487;&#20197;&#26159;&#23458;&#35266;&#20989;&#25968;&#25110;AI&#20195;&#29702;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#37096;&#20998;&#36825;&#20123;&#36817;&#20284;&#22120;&#26159;&#38745;&#24577;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#23427;&#20204;&#19981;&#33021;&#21453;&#26144;&#20154;&#31867;&#29609;&#23478;&#22312;&#28216;&#25103;&#20013;&#30340;&#23398;&#20064;&#21644;&#25552;&#39640;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24212;&#29992;&#20110;&#29983;&#25104;&#35268;&#21017;&#30340;&#20154;&#31867;&#28216;&#25103;&#35780;&#20272;&#20013;&#12290;&#25105;&#20204;&#22312;Unity&#20013;&#37325;&#26032;&#21019;&#24314;&#20102;&#32463;&#20856;&#30340;AGD&#29615;&#22659;Mechanic Maker&#20316;&#20026;&#19968;&#20010;&#20840;&#26032;&#30340;&#24320;&#28304;&#29983;&#25104;&#35268;&#21017;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;RL&#19982;A*&#20195;&#29702;&#22522;&#32447;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#35268;&#21017;&#38598;&#65292;&#36825;&#20123;&#35268;&#21017;&#21487;&#33021;&#26356;&#36866;&#21512;&#20154;&#31867;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated game design (AGD), the study of automatically generating game rules, has a long history in technical games research. AGD approaches generally rely on approximations of human play, either objective functions or AI agents. Despite this, the majority of these approximators are static, meaning they do not reflect human player's ability to learn and improve in a game. In this paper, we investigate the application of Reinforcement Learning (RL) as an approximator for human play for rule generation. We recreate the classic AGD environment Mechanic Maker in Unity as a new, open-source rule generation framework. Our results demonstrate that RL produces distinct sets of rules from an A* agent baseline, which may be more usable by humans.
&lt;/p&gt;</description></item><item><title>Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07936</link><description>&lt;p&gt;
Landscape-Sketch-Step: &#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems. (arXiv:2309.07936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07936
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25104;&#26412;&#20989;&#25968;&#30340;&#35780;&#20272;&#38750;&#24120;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#29978;&#33267;&#31105;&#27490;&#30340;&#22330;&#26223;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;Landscape-Sketch-Step&#65288;LSS&#65289;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#20808;&#21069;&#37319;&#26679;&#28857;&#30340;&#21382;&#21490;&#20449;&#24687;&#65292;&#20197;&#26126;&#26234;&#22320;&#36873;&#25321;&#24212;&#35780;&#20272;&#25104;&#26412;&#20989;&#25968;&#30340;&#21442;&#25968;&#20540;&#12290;&#19982;&#22797;&#21046;&#20132;&#25442;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#25152;&#38656;&#30340;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#19982;&#27169;&#25311;&#36864;&#28779;&#26041;&#27861;&#30456;&#24403;&#65292;&#36825;&#22312;&#39640;&#36890;&#37327;&#35745;&#31639;&#25110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#31561;&#29615;&#22659;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#35780;&#20272;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#35201;&#20040;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#25165;&#33021;&#23436;&#25104;&#12290;&#35813;&#26041;&#27861;&#19982;&#26631;&#20934;&#30340;&#20195;&#29702;&#20248;&#21270;&#25216;&#26415;&#20063;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#19981;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new heuristics for global optimization in scenarios where extensive evaluations of the cost function are expensive, inaccessible, or even prohibitive. The method, which we call Landscape-Sketch-and-Step (LSS), combines Machine Learning, Stochastic Optimization, and Reinforcement Learning techniques, relying on historical information from previously sampled points to make judicious choices of parameter values where the cost function should be evaluated at. Unlike optimization by Replica Exchange Monte Carlo methods, the number of evaluations of the cost function required in this approach is comparable to that used by Simulated Annealing, quality that is especially important in contexts like high-throughput computing or high-performance computing tasks, where evaluations are either computationally expensive or take a long time to be performed. The method also differs from standard Surrogate Optimization techniques, for it does not construct a surrogate model
&lt;/p&gt;</description></item><item><title>MMICL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.07915</link><description>&lt;p&gt;
MMICL&#65306;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. (arXiv:2309.07915v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07915
&lt;/p&gt;
&lt;p&gt;
MMICL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28145;&#24230;&#23398;&#20064;&#30340;&#22797;&#33487;&#24320;&#22987;&#65292;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLM&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#20219;&#21153;&#20449;&#24687;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22823;&#22810;&#25968;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;&#21253;&#21547;&#22810;&#20010;&#22270;&#20687;&#65289;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36861;&#28335;&#21040;VLM&#30340;&#26550;&#26500;&#35774;&#35745;&#25110;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#21069;&#30340;VLM&#20027;&#35201;&#24378;&#35843;&#21033;&#29992;&#24102;&#26377;&#21333;&#20010;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#24102;&#26377;&#20132;&#38169;&#22810;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#23613;&#31649;&#19968;&#20123;&#26032;&#25552;&#20986;&#30340;VLM&#21487;&#20197;&#22788;&#29702;&#24102;&#26377;&#22810;&#20010;&#22270;&#20687;&#30340;&#29992;&#25143;&#25552;&#31034;&#65292;&#20294;&#39044;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#25552;&#20379;&#27604;&#20174;Web&#25235;&#21462;&#26102;&#20132;&#38169;&#22270;&#20687;&#21644;&#25991;&#26412;&#26356;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MMICL&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Starting from the resurgence of deep learning, vision-language models (VLMs) benefiting from large language models (LLMs) have never been so popular. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images. The issue can traced back to the architectural design of VLMs or pre-training data. Specifically, the current VLMs primarily emphasize utilizing multi-modal data with a single image some, rather than multi-modal prompts with interleaved multiple images and text. Even though some newly proposed VLMs could handle user prompts with multiple images, pre-training data does not provide more sophisticated multi-modal prompts than interleaved image and text crawled from the web. We propose MMICL to address the issue by considering both the model and data perspectives. We introduce a well-designed architecture capable of seamlessly integrating vis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#21270;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.06687</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#20154;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#25105;&#25913;&#36827;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#21160;&#21270;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#24072;
&lt;/p&gt;
&lt;p&gt;
Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics. (arXiv:2309.06687v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#21270;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20247;&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#35774;&#35745;&#39640;&#24615;&#33021;&#30340;&#22870;&#21169;&#20989;&#25968;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#36755;&#20837;&#12290;&#26368;&#36817;&#65292;&#24191;&#27867;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#38656;&#35201;&#28145;&#20837;&#24120;&#35782;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#22914;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#24847;&#35782;&#21040;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#19982;&#36825;&#31181;&#30693;&#35782;&#26412;&#36136;&#19978;&#26159;&#30456;&#20851;&#30340;&#65292;LLM&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#25552;&#20379;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#26694;&#26550;&#65292;&#20855;&#26377;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#12290;&#35813;&#26694;&#26550;&#20197;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20026;&#22522;&#30784;&#65292;&#30001;LLM&#21046;&#23450;&#19968;&#20010;&#21021;&#22987;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#32467;&#26524;&#21576;&#29616;&#32473;LLM&#20197;&#25351;&#23548;&#20854;&#33258;&#25105;&#25913;&#36827;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#22810;&#31181;&#36830;&#32493;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep Reinforcement Learning (DRL) has achieved notable success in numerous robotic applications, designing a high-performing reward function remains a challenging task that often requires substantial manual input. Recently, Large Language Models (LLMs) have been extensively adopted to address tasks demanding in-depth common-sense knowledge, such as reasoning and planning. Recognizing that reward function design is also inherently linked to such knowledge, LLM offers a promising potential in this context. Motivated by this, we propose in this work a novel LLM framework with a self-refinement mechanism for automated reward function design. The framework commences with the LLM formulating an initial reward function based on natural language inputs. Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process. We examine the performance of our proposed framework through a variety of continuous robot
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#39046;&#22495;&#30340;&#26032;&#25216;&#26415;&#36827;&#34892;&#38646;&#26679;&#26412;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#36716;&#21270;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#35821;&#20041;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#24179;&#31283;&#20869;&#23481;&#30340;&#25512;&#33616;&#12290;&#22312;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#26263;&#31034;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.01026</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#26263;&#31034;&#30340;&#38646;&#26679;&#26412;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging. (arXiv:2309.01026v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01026
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#39046;&#22495;&#30340;&#26032;&#25216;&#26415;&#36827;&#34892;&#38646;&#26679;&#26412;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#36716;&#21270;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#35821;&#20041;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#24179;&#31283;&#20869;&#23481;&#30340;&#25512;&#33616;&#12290;&#22312;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#26263;&#31034;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26368;&#26032;&#36827;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25512;&#33616;&#22810;&#27169;&#24577;&#38750;&#24179;&#31283;&#20869;&#23481;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#36755;&#20837;&#28210;&#26579;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#35745;&#31639;&#35821;&#20041;&#23884;&#20837;&#33719;&#21462;&#23427;&#20204;&#30340;&#25968;&#20540;&#34920;&#31034;&#12290;&#19968;&#26086;&#33719;&#24471;&#25152;&#26377;&#20869;&#23481;&#39033;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36866;&#24403;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#36827;&#34892;&#25512;&#33616;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#26263;&#31034;&#29615;&#22659;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36755;&#20837;&#21253;&#25324;&#34920;&#26684;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for zero-shot recommendation of multimodal non-stationary content that leverages recent advancements in the field of generative AI. We propose rendering inputs of different modalities as textual descriptions and to utilize pre-trained LLMs to obtain their numerical representations by computing semantic embeddings. Once unified representations of all content items are obtained, the recommendation can be performed by computing an appropriate similarity metric between them without any additional learning. We demonstrate our approach on a synthetic multimodal nudging environment, where the inputs consist of tabular, textual, and visual data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00079</link><description>&lt;p&gt;
&#20851;&#20110;Adam&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21518;&#21521;&#35823;&#24046;&#20998;&#26512;&#34987;&#29992;&#26469;&#25214;&#21040;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#21457;&#29616;&#26377;&#38480;&#27493;&#38271;&#20250;&#38544;&#24335;&#22320;&#35268;&#33539;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20986;&#29616;&#22312;ODE&#20013;&#30340;&#39033;&#20250;&#24809;&#32602;&#25439;&#22833;&#26799;&#24230;&#30340;&#20108;&#33539;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#20013;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#20294;&#28041;&#21450;&#30340;&#8220;&#33539;&#25968;&#8221;&#19981;&#21516;&#65306;&#23545;&#24212;&#30340;ODE&#39033;&#35201;&#20040;&#24809;&#32602;&#65288;&#25200;&#21160;&#30340;&#65289;&#25439;&#22833;&#26799;&#24230;&#30340;&#19968;&#33539;&#25968;&#65292;&#35201;&#20040;&#30456;&#21453;&#22320;&#38459;&#27490;&#20854;&#20943;&#23567;&#65288;&#21518;&#19968;&#31181;&#24773;&#20917;&#26159;&#20856;&#22411;&#30340;&#65289;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13498</link><description>&lt;p&gt;
&#36867;&#31163;&#26679;&#26412;&#38519;&#38449;&#65306;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators. (arXiv:2308.13498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#65288;PaiDEs&#65289;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#21033;&#29992;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#37197;&#23545;&#36317;&#31163;&#26469;&#24314;&#31435;&#29109;&#30340;&#36793;&#30028;&#65292;&#24182;&#23558;&#36825;&#20123;&#36793;&#30028;&#20316;&#20026;&#22522;&#20110;&#20449;&#24687;&#20934;&#21017;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#26368;&#36817;&#22522;&#20110;&#26679;&#26412;&#30340;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#29992;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;PaiDEs&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#65288;&#26368;&#22810;100&#20493;&#65289;&#19978;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#65288;&#26368;&#22810;100&#20493;&#65289;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#26356;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#35780;&#20272;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#39564;&#65306;&#19968;&#32500;&#27491;&#24358;&#25968;&#25454;&#65292;&#25670;&#21160;&#29289;&#20307;&#65288;Pendulum-v0&#65289;&#65292;&#36339;&#36291;&#26426;&#22120;&#20154;&#65288;Hopper-v2&#65289;&#65292;&#34434;&#34433;&#26426;&#22120;&#20154;&#65288;Ant-v2&#65289;&#21644;&#20154;&#24418;&#26426;&#22120;&#20154;&#65288;Humanoid-v2&#65289;&#12290;&#23545;&#20110;&#27599;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#26469;&#23637;&#31034;PaiDEs&#22312;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.12030</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#19982;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT-4&#22240;&#20854;&#24778;&#20154;&#30340;&#25913;&#36827;&#21644;&#24615;&#33021;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#25104;&#20026;LLM&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#35805;&#39064;&#65292;&#23427;&#36824;&#20351;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#22312;&#26356;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#29983;&#25104;&#25152;&#38656;&#38271;&#24230;&#30340;&#21512;&#36866;&#31572;&#26696;&#25110;&#25991;&#31456;&#12290;&#27492;&#22806;&#65292;LLM&#20013;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#38750;&#24120;&#32791;&#26102;&#65292;&#32780;&#25511;&#21046;&#29983;&#25104;&#38271;&#24230;&#30340;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#38271;&#24230;&#20219;&#24847;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#65292;&#20174;&#32780;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#26469;&#23454;&#29616;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#31867;&#20284;GPT&#30340;LLM&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22870;&#21169;&#27169;&#22411;&#25552;&#20379;&#22870;&#21169;&#20449;&#21495;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#23545;&#39044;&#23450;&#20041;&#30446;&#26631;&#38271;&#24230;&#36827;&#34892;&#22870;&#21169;&#26469;&#24433;&#21709;LLM&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#20282;&#26381;&#65292;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#19988;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#36827;&#34892;&#27169;&#22411;&#25913;&#36827;&#65292;&#21487;&#20197;&#22788;&#29702;100&#31859;&#39640;&#24230;&#30340;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.11471</link><description>&lt;p&gt;
&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#65288;DOVESEI&#65289;
&lt;/p&gt;
&lt;p&gt;
Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI). (arXiv:2308.11471v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#20282;&#26381;&#65292;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#19988;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#36827;&#34892;&#27169;&#22411;&#25913;&#36827;&#65292;&#21487;&#20197;&#22788;&#29702;100&#31859;&#39640;&#24230;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22478;&#24066;&#31354;&#20013;&#26426;&#22120;&#20154;&#30340;&#22522;&#30784;&#27493;&#39588;&#20043;&#19968;&#65292;&#21363;&#23433;&#20840;&#30528;&#38470;&#12290;&#25105;&#20204;&#20851;&#27880;&#23433;&#20840;&#30528;&#38470;&#24863;&#30693;&#22534;&#26632;&#20013;&#26368;&#20851;&#38190;&#30340;&#26041;&#38754;&#20043;&#19968;&#65292;&#21363;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#21453;&#24212;&#24335;&#26080;&#20154;&#26426;&#31995;&#32479;&#65292;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#35270;&#35273;&#20282;&#26381;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#20854;&#24320;&#25918;&#35789;&#27719;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#35843;&#25972;&#38656;&#27714;&#65292;&#32469;&#36807;&#23545;&#20869;&#37096;&#27169;&#22411;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#20197;&#36827;&#34892;&#25913;&#36827;&#30340;&#24517;&#35201;&#24615;&#12290;&#32771;&#34385;&#21040;&#24403;&#22320;&#24403;&#23616;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#20174;100&#31859;&#39640;&#24230;&#36215;&#39134;&#30340;&#25805;&#20316;&#12290;&#36825;&#20010;&#36873;&#25321;&#26159;&#26377;&#24847;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#20043;&#21069;&#30340;&#24037;&#20316;&#22788;&#29702;&#30340;&#39640;&#24230;&#20165;&#38480;&#20110;30&#31859;&#65292;&#19982;&#23567;&#22411;&#31435;&#20307;&#30456;&#26426;&#30340;&#33021;&#21147;&#30456;&#21563;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20256;&#32479;&#30340;&#19977;&#32500;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#26469;&#23548;&#33322;&#21097;&#19979;&#30340;20&#31859;&#12290;&#21033;&#29992;&#21333;&#30446;&#30456;&#26426;&#21644;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
This work targets what we consider to be the foundational step for urban airborne robots, a safe landing. Our attention is directed toward what we deem the most crucial aspect of the safe landing perception stack: segmentation. We present a streamlined reactive UAV system that employs visual servoing by harnessing the capabilities of open vocabulary image segmentation. This approach can adapt to various scenarios with minimal adjustments, bypassing the necessity for extensive data accumulation for refining internal models, thanks to its open vocabulary methodology. Given the limitations imposed by local authorities, our primary focus centers on operations originating from altitudes of 100 meters. This choice is deliberate, as numerous preceding works have dealt with altitudes up to 30 meters, aligning with the capabilities of small stereo cameras. Consequently, we leave the remaining 20m to be navigated using conventional 3D path planning methods. Utilizing monocular cameras and image 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#27169;&#22411;&#21463;&#21040;&#27880;&#20837;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#21644;&#27880;&#20837;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10819</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25552;&#31034;&#27880;&#20837;&#30340;&#25351;&#20196;&#36319;&#38543;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection. (arXiv:2308.10819v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#27169;&#22411;&#21463;&#21040;&#27880;&#20837;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#21644;&#27880;&#20837;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36981;&#24490;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#22312;&#38754;&#21521;&#23458;&#25143;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20986;&#33394;&#33021;&#21147;&#20063;&#24341;&#21457;&#20102;&#23545;&#30001;&#31532;&#19977;&#26041;&#25915;&#20987;&#32773;&#27880;&#20837;&#27169;&#22411;&#36755;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#39118;&#38505;&#25918;&#22823;&#30340;&#25285;&#24551;&#65292;&#36825;&#20123;&#25351;&#20196;&#21487;&#33021;&#25805;&#32437;LLM&#30340;&#21407;&#22987;&#25351;&#20196;&#24182;&#23548;&#33268;&#24847;&#22806;&#30340;&#34892;&#20026;&#21644;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;LLM&#20934;&#30830;&#36776;&#21035;&#35201;&#36981;&#24490;&#30340;&#25351;&#20196;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#23545;LLM&#25351;&#20196;&#36319;&#38543;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#35813;&#22522;&#20934;&#30340;&#30446;&#26631;&#26159;&#37327;&#21270;LLM&#21463;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#36825;&#20123;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#21644;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable proficiency in following instructions, making them valuable in customer-facing applications. However, their impressive capabilities also raise concerns about the amplification of risks posed by adversarial instructions, which can be injected into the model input by third-party attackers to manipulate LLMs' original instructions and prompt unintended actions and content. Therefore, it is crucial to understand LLMs' ability to accurately discern which instructions to follow to ensure their safe deployment in real-world scenarios. In this paper, we propose a pioneering benchmark for automatically evaluating the robustness of instruction-following LLMs against adversarial instructions injected in the prompt. The objective of this benchmark is to quantify the extent to which LLMs are influenced by injected adversarial instructions and assess their ability to differentiate between these injected adversarial instructions and original user ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26080;&#20154;&#26426;&#22312;&#30416;&#30719;&#21208;&#27979;&#20219;&#21153;&#20013;&#38754;&#20020;&#30340;&#22240;&#26524;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#22240;&#26524;&#26694;&#26550;&#65292;&#21253;&#25324;&#22240;&#26524;&#35268;&#21010;&#12289;&#22312;&#32447;&#36866;&#24212;&#21644;&#20107;&#21518;&#35299;&#37322;&#65292;&#20197;&#35299;&#20915;&#28151;&#28102;&#21464;&#37327;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#24314;&#27169;&#22256;&#38590;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.10047</link><description>&lt;p&gt;
&#38754;&#21521;&#30719;&#23665;&#21208;&#27979;&#20219;&#21153;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#30340;&#27010;&#29575;&#22240;&#26524;&#21457;&#29616;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards Probabilistic Causal Discovery, Inference &amp; Explanations for Autonomous Drones in Mine Surveying Tasks. (arXiv:2308.10047v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26080;&#20154;&#26426;&#22312;&#30416;&#30719;&#21208;&#27979;&#20219;&#21153;&#20013;&#38754;&#20020;&#30340;&#22240;&#26524;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#22240;&#26524;&#26694;&#26550;&#65292;&#21253;&#25324;&#22240;&#26524;&#35268;&#21010;&#12289;&#22312;&#32447;&#36866;&#24212;&#21644;&#20107;&#21518;&#35299;&#37322;&#65292;&#20197;&#35299;&#20915;&#28151;&#28102;&#21464;&#37327;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#24314;&#27169;&#22256;&#38590;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#24314;&#27169;&#20026;&#33258;&#20027;&#20195;&#29702;&#25552;&#20379;&#20102;&#35299;&#26512;&#20854;&#19982;&#19990;&#30028;&#20114;&#21160;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#20102;&#27491;&#24335;&#30340;&#30693;&#35782;&#21644;&#27010;&#29575;&#24615;&#30340;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#36935;&#21040;&#26159;&#24120;&#35265;&#30340;&#12290;&#22240;&#27492;&#65292;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#24110;&#21161;&#33258;&#20027;&#20195;&#29702;&#36827;&#34892;&#20915;&#31574;&#21644;&#35299;&#37322;&#32467;&#26524;&#65292;&#20294;&#26159;&#20197;&#36825;&#31181;&#26041;&#24335;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#20250;&#24341;&#20837;&#26032;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#22312;&#30416;&#30719;&#20013;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#29615;&#22659;&#20013;&#65292;&#35782;&#21035;&#20102;&#19982;&#22240;&#26524;&#20851;&#31995;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#36825;&#26679;&#30340;&#29615;&#22659;&#23545;&#33258;&#20027;&#20195;&#29702;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23384;&#22312;&#28151;&#28102;&#21464;&#37327;&#12289;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#19988;&#38590;&#20197;&#25552;&#21069;&#26500;&#24314;&#23436;&#25972;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#22240;&#26524;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;&#22522;&#20110;&#22240;&#26524;&#30340;POMDP&#35268;&#21010;&#12289;&#22312;&#32447;SCM&#36866;&#24212;&#21644;&#20107;&#21518;&#22240;&#26524;&#25512;&#26029;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#35745;&#21010;&#20013;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal modelling offers great potential to provide autonomous agents the ability to understand the data-generation process that governs their interactions with the world. Such models capture formal knowledge as well as probabilistic representations of noise and uncertainty typically encountered by autonomous robots in real-world environments. Thus, causality can aid autonomous agents in making decisions and explaining outcomes, but deploying causality in such a manner introduces new challenges. Here we identify challenges relating to causality in the context of a drone system operating in a salt mine. Such environments are challenging for autonomous agents because of the presence of confounders, non-stationarity, and a difficulty in building complete causal models ahead of time. To address these issues, we propose a probabilistic causal framework consisting of: causally-informed POMDP planning, online SCM adaptation, and post-hoc counterfactual explanations. Further, we outline planned
&lt;/p&gt;</description></item><item><title>Ada-QPacknet&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#30340;&#39640;&#25928;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#37327;&#21270;&#25216;&#26415;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#65292;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07939</link><description>&lt;p&gt;
Ada-QPacknet -- &#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20250;&#36951;&#24536;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting. (arXiv:2308.07939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07939
&lt;/p&gt;
&lt;p&gt;
Ada-QPacknet&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#30340;&#39640;&#25928;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#37327;&#21270;&#25216;&#26415;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#65292;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32487;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#20010;&#36807;&#31243;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#20173;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#12290;&#26368;&#36817;&#35774;&#35745;&#20102;&#35768;&#22810;CL&#31639;&#27861;&#65292;&#22823;&#37096;&#20998;&#37117;&#23384;&#22312;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#26550;&#26500;&#30340;&#26041;&#27861;Ada-QPacknet&#12290;&#23427;&#36890;&#36807;&#21098;&#26525;&#25552;&#21462;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#12290;&#22522;&#20110;&#26550;&#26500;&#30340;CL&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#23481;&#37327;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37327;&#21270;&#26041;&#27861;&#20943;&#23567;&#20102;&#27169;&#22411;&#30340;&#35268;&#27169;&#12290;&#35813;&#26041;&#27861;&#20943;&#23567;&#20102;&#26435;&#37325;&#26684;&#24335;&#30340;&#20301;&#23485;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#28151;&#21512;8&#20301;&#21644;4&#20301;&#37327;&#21270;&#22312;&#33879;&#21517;&#30340;CL&#22330;&#26223;&#19978;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#21098;&#26525;&#21644;&#37327;&#21270;&#36825;&#20004;&#31181;&#21387;&#32553;&#25216;&#26415;&#24212;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#30340;CL&#31574;&#30053;&#12290;&#35813;&#31639;&#27861;&#22312;&#33879;&#21517;&#30340;&#24773;&#33410;&#32452;&#21512;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combination
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#25805;&#32437;&#22120;&#30340;&#21407;&#22987;&#20219;&#21153;&#36716;&#25442;&#20026;&#26426;&#22120;&#20154;&#30340;&#20302;&#23618;&#21160;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#31867;&#20284;&#31243;&#24207;&#20989;&#25968;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#20301;&#32622;/&#21147;&#30340;&#35774;&#23450;&#28857;&#30340;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#28151;&#21512;&#25511;&#21046;</title><link>http://arxiv.org/abs/2308.06810</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#22320;&#38754;&#25805;&#32437;&#22120;&#30340;&#21407;&#22987;&#20219;&#21153;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Ground Manipulator Primitive Tasks to Executable Actions using Large Language Models. (arXiv:2308.06810v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#25805;&#32437;&#22120;&#30340;&#21407;&#22987;&#20219;&#21153;&#36716;&#25442;&#20026;&#26426;&#22120;&#20154;&#30340;&#20302;&#23618;&#21160;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#31867;&#20284;&#31243;&#24207;&#20989;&#25968;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#20301;&#32622;/&#21147;&#30340;&#35774;&#23450;&#28857;&#30340;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#28151;&#21512;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32467;&#26500;&#22312;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#26159;&#22823;&#22810;&#25968;&#26426;&#22120;&#20154;&#31995;&#32479;&#22312;&#35268;&#21010;&#21644;&#25191;&#34892;&#21151;&#33021;&#20043;&#38388;&#32570;&#20047;&#30452;&#25509;&#30340;&#26041;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#25805;&#32437;&#22120;&#30340;&#21407;&#22987;&#20219;&#21153;&#36716;&#25442;&#20026;&#26426;&#22120;&#20154;&#30340;&#20302;&#23618;&#21160;&#20316;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#31243;&#24207;&#20989;&#25968;&#30340;&#25552;&#31034;&#65292;&#22522;&#20110;&#20219;&#21153;&#26694;&#26550;&#24418;&#24335;&#20027;&#20041;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#20351;&#24471;LLMs&#33021;&#22815;&#29983;&#25104;&#20301;&#32622;/&#21147;&#30340;&#35774;&#23450;&#28857;&#36827;&#34892;&#28151;&#21512;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layered architectures have been widely used in robot systems. The majority of them implement planning and execution functions in separate layers. However, there still lacks a straightforward way to transit high-level tasks in the planning layer to the low-level motor commands in the execution layer. In order to tackle this challenge, we propose a novel approach to ground the manipulator primitive tasks to robot low-level actions using large language models (LLMs). We designed a program-function-like prompt based on the task frame formalism. In this way, we enable LLMs to generate position/force set-points for hybrid control. Evaluations over several state-of-the-art LLMs are provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Adv-Inpainting&#30340;&#21019;&#26032;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#29305;&#24449;&#34701;&#21512;&#29983;&#25104;&#33258;&#28982;&#19988;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#22270;&#26696;&#21644;&#36793;&#30028;&#26041;&#38754;&#26356;&#21152;&#33258;&#28982;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05320</link><description>&lt;p&gt;
Adv-Inpainting:&#36890;&#36807;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#29305;&#24449;&#34701;&#21512;&#29983;&#25104;&#33258;&#28982;&#19988;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;
&lt;/p&gt;
&lt;p&gt;
Adv-Inpainting: Generating Natural and Transferable Adversarial Patch via Attention-guided Feature Fusion. (arXiv:2308.05320v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Adv-Inpainting&#30340;&#21019;&#26032;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#29305;&#24449;&#34701;&#21512;&#29983;&#25104;&#33258;&#28982;&#19988;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#22270;&#26696;&#21644;&#36793;&#30028;&#26041;&#38754;&#26356;&#21152;&#33258;&#28982;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#21021;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21033;&#29992;&#21152;&#24615;&#22122;&#22768;&#25915;&#20987;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25805;&#20316;&#25972;&#20010;&#33080;&#37096;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20154;&#33080;&#35782;&#21035;&#25915;&#20987;&#37117;&#22522;&#20110;&#23545;&#25239;&#24615;&#36148;&#32440;&#65292;&#23558;&#25200;&#21160;&#38480;&#21046;&#22312;&#19968;&#20010;&#36739;&#23567;&#30340;&#21306;&#22495;&#20869;&#12290;&#20808;&#21069;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#25915;&#20987;&#24120;&#24120;&#23548;&#33268;&#19981;&#33258;&#28982;&#30340;&#22270;&#26696;&#21644;&#26126;&#26174;&#30340;&#36793;&#30028;&#65292;&#23481;&#26131;&#34987;&#23519;&#35273;&#12290;&#25105;&#20204;&#35748;&#20026;&#29983;&#25104;&#24102;&#26377;&#21512;&#29702;&#20869;&#23481;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#20250;&#27604;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#25110;&#30452;&#25509;&#20174;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#26356;&#20855;&#26377;&#26356;&#24378;&#30340;&#36801;&#31227;&#24615;&#12290;&#20026;&#20102;&#29983;&#25104;&#33258;&#28982;&#19988;&#39640;&#24230;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20004;&#38454;&#27573;&#31895;&#21040;&#31934;&#30340;&#25915;&#20987;&#26694;&#26550;&#65292;&#31216;&#20026;Adv-Inpainting&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;StyleGAN&#65288;Att-StyleGAN&#65289;&#65292;&#26681;&#25454;&#27880;&#24847;&#21147;&#22270;&#33258;&#36866;&#24212;&#22320;&#32467;&#21512;&#32441;&#29702;&#21644;&#36523;&#20221;&#29305;&#24449;&#65292;&#29983;&#25104;&#39640;&#24230;&#21487;&#36801;&#31227;&#21644;&#33258;&#28982;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rudimentary adversarial attacks utilize additive noise to attack facial recognition (FR) models. However, because manipulating the total face is impractical in the physical setting, most real-world FR attacks are based on adversarial patches, which limit perturbations to a small area. Previous adversarial patch attacks often resulted in unnatural patterns and clear boundaries that were easily noticeable. In this paper, we argue that generating adversarial patches with plausible content can result in stronger transferability than using additive noise or directly sampling from the latent space. To generate natural-looking and highly transferable adversarial patches, we propose an innovative two-stage coarse-to-fine attack framework called Adv-Inpainting. In the first stage, we propose an attention-guided StyleGAN (Att-StyleGAN) that adaptively combines texture and identity features based on the attention map to generate high-transferable and natural adversarial patches. In the second
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;FLIPS&#36890;&#36807;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#21644;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#26469;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FLIPS&#30456;&#27604;&#38543;&#26426;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03901</link><description>&lt;p&gt;
FLIPS: &#20351;&#29992;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLIPS: Federated Learning using Intelligent Participant Selection. (arXiv:2308.03901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;FLIPS&#36890;&#36807;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#21644;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#26469;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FLIPS&#30456;&#27604;&#38543;&#26426;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#19982;&#32773;&#36873;&#25321;&#20013;&#30340;&#22909;&#22788;&#12290;FLIPS&#26681;&#25454;&#25968;&#25454;&#30340;&#26631;&#31614;&#20998;&#24067;&#39044;&#20808;&#23545;&#21442;&#19982;FL&#35757;&#32451;&#20316;&#19994;&#30340;&#21508;&#26041;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#22312;FL&#35757;&#32451;&#26399;&#38388;&#30830;&#20445;&#27599;&#20010;&#32858;&#31867;&#22312;&#34987;&#36873;&#20013;&#30340;&#21442;&#19982;&#32773;&#20013;&#20844;&#24179;&#22320;&#34920;&#31034;&#12290;FLIPS&#21487;&#20197;&#25903;&#25345;&#26368;&#24120;&#35265;&#30340;FL&#31639;&#27861;&#65292;&#21253;&#25324;FedAvg&#65292;FedProx&#65292;FedDyn&#65292;FedOpt&#21644;FedYogi&#12290;&#20026;&#20102;&#31649;&#29702;&#24179;&#21488;&#30340;&#24322;&#26500;&#24615;&#21644;&#21160;&#24577;&#36164;&#28304;&#21487;&#29992;&#24615;&#65292;FLIPS&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#22788;&#29702;&#20998;&#24067;&#24335;&#26234;&#33021;&#31038;&#21306;&#24212;&#29992;&#20013;&#23481;&#37327;&#21464;&#21270;&#30340;&#25302;&#32047;&#31649;&#29702;&#26426;&#21046;&#12290;&#26631;&#31614;&#20998;&#24067;&#12289;&#32858;&#31867;&#21644;&#21442;&#19982;&#32773;&#36873;&#25321;&#30340;&#38544;&#31169;&#36890;&#36807;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;(TEE)&#26469;&#30830;&#20445;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#23558;FLIPS&#19982;&#38543;&#26426;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the design and implementation of FLIPS, a middleware system to manage data and participant heterogeneity in federated learning (FL) training workloads. In particular, we examine the benefits of label distribution clustering on participant selection in federated learning. FLIPS clusters parties involved in an FL training job based on the label distribution of their data apriori, and during FL training, ensures that each cluster is equitably represented in the participants selected. FLIPS can support the most common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To manage platform heterogeneity and dynamic resource availability, FLIPS incorporates a straggler management mechanism to handle changing capacities in distributed, smart community applications. Privacy of label distributions, clustering and participant selection is ensured through a trusted execution environment (TEE). Our comprehensive empirical evaluation compares FLIPS with random p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.12856</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#35268;&#21010;&#12289;&#38271;&#26399;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#31243;&#24207;&#21512;&#25104;&#33021;&#21147;&#30340;&#29616;&#23454;&#19990;&#30028;WebAgent
&lt;/p&gt;
&lt;p&gt;
A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#20027;Web&#33258;&#21160;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#32593;&#31449;&#19978;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#19977;&#20010;&#26041;&#38754;&#30340;&#38480;&#21046;&#65306;&#24320;&#25918;&#39046;&#22495;&#24615;&#12289;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#23545;HTML&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;WebAgent&#36890;&#36807;&#23558;&#25351;&#20196;&#20998;&#35299;&#20026;&#35268;&#33539;&#30340;&#23376;&#25351;&#20196;&#65292;&#23558;&#38271;HTML&#25991;&#26723;&#24635;&#32467;&#20026;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#20174;&#20013;&#29983;&#25104;&#30340;Python&#31243;&#24207;&#23545;&#32593;&#31449;&#36827;&#34892;&#25805;&#20316;&#26469;&#25552;&#21069;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-U-PaLM&#35774;&#35745;&#20102;WebAgent&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#26681;&#20195;&#30721;&#65292;&#24182;&#20351;&#29992;HTML-T5&#36827;&#34892;&#39044;&#35757;&#32451;LLMs&#65292;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#20197;&#21450;&#28151;&#21512;&#38271;&#36328;&#24230;&#21435;&#22122;&#30446;&#26631;&#26469;&#36827;&#34892;&#35268;&#21010;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25216;&#26415;&#21644;&#21382;&#21490;&#25968;&#25454;&#65292;&#20026;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#25552;&#20379;&#31185;&#23398;&#20915;&#31574;&#24037;&#20855;&#21644;&#35777;&#25454;&#65292;&#20197;&#35299;&#20915;&#36947;&#36335;&#32500;&#25252;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10085</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#36947;&#36335;&#27573;&#25512;&#33616;&#32500;&#25252;&#30340;&#20915;&#31574;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A decision making framework for recommended maintenance of road segments. (arXiv:2307.10085v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10085
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25216;&#26415;&#21644;&#21382;&#21490;&#25968;&#25454;&#65292;&#20026;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#25552;&#20379;&#31185;&#23398;&#20915;&#31574;&#24037;&#20855;&#21644;&#35777;&#25454;&#65292;&#20197;&#35299;&#20915;&#36947;&#36335;&#32500;&#25252;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#36947;&#36335;&#20132;&#36890;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21508;&#22269;&#24050;&#23436;&#25104;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#24314;&#35774;&#12290;&#28982;&#32780;&#65292;&#38543;&#20043;&#32780;&#26469;&#30340;&#25361;&#25112;&#22312;&#20110;&#29616;&#26377;&#36947;&#36335;&#30340;&#32500;&#25252;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#21508;&#22269;&#22312;&#36947;&#36335;&#32500;&#25252;&#39033;&#30446;&#19978;&#30340;&#39044;&#31639;&#26377;&#38480;&#65292;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#22312;&#36827;&#34892;&#31185;&#23398;&#20915;&#31574;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#23558;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25216;&#26415;&#19982;&#21382;&#21490;&#32500;&#25252;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#36866;&#24212;&#36947;&#36335;&#32500;&#25252;&#31185;&#23398;&#20915;&#31574;&#30340;&#32972;&#26223;&#65292;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#25972;&#21512;&#26088;&#22312;&#20026;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#25552;&#20379;&#26356;&#31185;&#23398;&#30340;&#24037;&#20855;&#21644;&#35777;&#25454;&#65292;&#20197;&#36827;&#34892;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26694;&#26550;&#20027;&#35201;&#35299;&#20915;&#20197;&#19979;&#22235;&#20010;&#38382;&#39064;&#65306;1&#65289;&#39044;&#27979;&#21508;&#36335;&#32447;&#30340;&#36335;&#38754;&#24615;&#33021;&#65292;2&#65289;&#30830;&#23450;&#32500;&#25252;&#36335;&#32447;&#30340;&#20248;&#20808;&#32423;&#65292;3&#65289;&#22522;&#20110;&#35780;&#20272;&#26631;&#20934;&#21046;&#23450;&#32500;&#25252;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of global road transportation, countries worldwide have completed the construction of road networks. However, the ensuing challenge lies in the maintenance of existing roads. It is well-known that countries allocate limited budgets to road maintenance projects, and road management departments face difficulties in making scientifically informed maintenance decisions. Therefore, integrating various artificial intelligence decision-making techniques to thoroughly explore historical maintenance data and adapt them to the context of road maintenance scientific decision-making has become an urgent issue. This integration aims to provide road management departments with more scientific tools and evidence for decision-making. The framework proposed in this paper primarily addresses the following four issues: 1) predicting the pavement performance of various routes, 2) determining the prioritization of maintenance routes, 3) making maintenance decisions based on the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.09882</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20294;&#26080;&#27861;&#25552;&#20379;&#26679;&#26412;&#21608;&#22260;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#27880;&#24847;&#21040;&#22312;&#33021;&#37327;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#21487;&#20197;&#23548;&#33268;&#21028;&#21035;&#22120;&#25552;&#20379;&#38750;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#65288;&#36890;&#24120;&#31216;&#20026;&#33021;&#37327;&#65289;&#30340;&#23545;&#25239;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#32467;&#21512;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#19979;&#20869;&#23481;&#65306;1&#65289;Wasserstein GAN&#23545;&#20998;&#21306;&#20989;&#25968;&#36827;&#34892;&#20102;&#26377;&#20559;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26080;&#20559;&#20272;&#35745;&#26041;&#27861;&#65307;2&#65289;&#22312;&#26368;&#20248;&#21270;&#20284;&#28982;&#26102;&#65292;&#24517;&#39035;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#12290;&#36825;&#34987;&#20551;&#35774;&#20250;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#24335;&#35206;&#30422;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#26126;&#30830;&#35745;&#31639;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#12290;&#36825;&#26159;&#35774;&#35745;&#26080;&#20559;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#29983;&#25104;&#22120;&#29109;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#29983;&#25104;&#23494;&#24230;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#30340;&#27969;&#32593;&#32476;&#26469;&#33719;&#24471;&#30340;&#65292;&#31216;&#20026;&#21333;&#21521;&#27969;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#26679;&#26412;&#19978;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#26354;&#29575;&#30340;&#26679;&#26412;&#36890;&#24120;&#26159;&#20855;&#26377;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#30340;&#38271;&#23614;&#26679;&#26412;&#65292;&#24182;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#26679;&#26412;&#36827;&#34892;&#38543;&#26426;&#26631;&#31614;&#38169;&#35823;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20986;&#36825;&#20123;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.05831</link><description>&lt;p&gt;
&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#35270;&#35282;&#25581;&#31034;&#35760;&#24518;&#21270;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Memorization Through the Lens of Curvature of Loss Function Around Samples. (arXiv:2307.05831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#26679;&#26412;&#19978;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#26354;&#29575;&#30340;&#26679;&#26412;&#36890;&#24120;&#26159;&#20855;&#26377;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#30340;&#38271;&#23614;&#26679;&#26412;&#65292;&#24182;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#26679;&#26412;&#36827;&#34892;&#38543;&#26426;&#26631;&#31614;&#38169;&#35823;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20986;&#36825;&#20123;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#36807;&#22810;&#65292;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#21487;&#20197;&#23436;&#20840;&#35760;&#24518;&#35757;&#32451;&#38598;&#65292;&#21363;&#20351;&#26631;&#31614;&#26159;&#38543;&#26426;&#30340;&#12290;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#35757;&#32451;&#26679;&#26412;&#21608;&#22260;&#30340;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#20316;&#20026;&#35760;&#24518;&#21270;&#31243;&#24230;&#30340;&#24230;&#37327;&#65292;&#23545;&#25152;&#26377;&#35757;&#32451;&#36718;&#27425;&#36827;&#34892;&#24179;&#22343;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26469;&#30740;&#31350;&#24120;&#35265;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#26679;&#26412;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20855;&#26377;&#26368;&#39640;&#25439;&#22833;&#26354;&#29575;&#30340;&#26679;&#26412;&#65292;&#21457;&#29616;&#23427;&#20204;&#36890;&#24120;&#26159;&#38271;&#23614;&#26679;&#26412;&#12289;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#26679;&#26412;&#12290;&#36825;&#31181;&#20998;&#26512;&#24110;&#21161;&#25105;&#20204;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#37325;&#22797;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#38543;&#26426;&#38169;&#35823;&#21270;&#23569;&#37327;&#26679;&#26412;&#30340;&#26631;&#31614;&#26469;&#20154;&#20026;&#22320;&#32473;&#25968;&#25454;&#38598;&#24341;&#20837;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#23637;&#31034;&#20102;&#25353;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#20986;&#26631;&#31614;&#38169;&#35823;&#26679;&#26412;&#30340;&#39640;AUROC&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are overparametrized and easily overfit the datasets they train on. In the extreme case, it is shown that they can memorize a training set with fully randomized labels. We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs. We use this to study the generalization versus memorization properties of different samples in popular image datasets. We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples. This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels. We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#26694;&#26550;&#65292;&#38024;&#23545;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#21644;&#24615;&#33021;&#35780;&#20272;&#38382;&#39064;&#65292;&#25104;&#21151;&#32553;&#23567;&#20102;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.01597</link><description>&lt;p&gt;
&#22312;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20013;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;: Seq2Peak&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework. (arXiv:2307.01597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#26694;&#26550;&#65292;&#38024;&#23545;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#21644;&#24615;&#33021;&#35780;&#20272;&#38382;&#39064;&#65292;&#25104;&#21151;&#32553;&#23567;&#20102;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#65288;PHSF&#65289;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;PHSF&#20013;&#21364;&#38590;&#20197;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#20013;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#30340;&#25361;&#25112;&#65292;&#20351;&#24471;&#30452;&#25509;&#39044;&#27979;&#27604;&#26631;&#20934;&#30340;TSF&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25163;&#21160;&#20174;&#24120;&#35268;&#39044;&#27979;&#32467;&#26524;&#20013;&#25552;&#21462;&#26368;&#22823;&#20540;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#27169;&#22411;&#20250;&#26368;&#23567;&#21270;&#24179;&#22343;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#65292;&#19968;&#20010;&#19987;&#20026;PHSF&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#22312;TSF&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;Seq2Peak&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;CyclicNorm&#27969;&#31243;&#26469;&#20943;&#36731;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#33258;&#30001;&#23792;&#20540;&#23567;&#26102;&#35299;&#30721;&#22120;&#65292;&#37319;&#29992;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#21033;&#29992;&#21407;&#22987;&#24207;&#21015;&#21644;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in various domains. While state-of-the-art deep learning models excel in regular Time Series Forecasting (TSF), they struggle to achieve comparable results in PHSF. This can be attributed to the challenges posed by the high degree of non-stationarity in peak-hour series, which makes direct forecasting more difficult than standard TSF. Additionally, manually extracting the maximum value from regular forecasting results leads to suboptimal performance due to models minimizing the mean deficit. To address these issues, this paper presents Seq2Peak, a novel framework designed specifically for PHSF tasks, bridging the performance gap observed in TSF models. Seq2Peak offers two key components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid loss function that utilizes both the original series and peak-hour series as superv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;RefSAM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#20174;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#22810;&#35270;&#22270;&#20449;&#24687;&#20013;&#21152;&#20837;SAM&#30340;&#28508;&#21147;&#65292;&#25506;&#32034;&#20854;&#22312;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#65288;RVOS&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;MLP&#21644;&#20998;&#23618;&#31264;&#23494;&#27880;&#24847;&#27169;&#22359;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24418;&#24577;&#30340;&#31934;&#30830;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.00997</link><description>&lt;p&gt;
RefSAM&#65306;&#39640;&#25928;&#36866;&#24212;&#20219;&#20309;&#27169;&#22411;&#30340;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation. (arXiv:2307.00997v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RefSAM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#20174;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#22810;&#35270;&#22270;&#20449;&#24687;&#20013;&#21152;&#20837;SAM&#30340;&#28508;&#21147;&#65292;&#25506;&#32034;&#20854;&#22312;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#65288;RVOS&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;MLP&#21644;&#20998;&#23618;&#31264;&#23494;&#27880;&#24847;&#27169;&#22359;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24418;&#24577;&#30340;&#31934;&#30830;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#22240;&#20854;&#22312;&#22270;&#20687;&#20998;&#21106;&#20013;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#65288;RVOS&#65289;&#26041;&#38754;&#65292;&#30001;&#20110;&#38656;&#35201;&#31934;&#30830;&#30340;&#29992;&#25143;&#20132;&#20114;&#25552;&#31034;&#20197;&#21450;&#23545;&#35821;&#35328;&#21644;&#35270;&#35273;&#31561;&#19981;&#21516;&#24418;&#24577;&#30340;&#26377;&#38480;&#29702;&#35299;&#33021;&#21147;&#65292;SAM&#32570;&#20047;&#29087;&#32451;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RefSAM&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#20174;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#22810;&#35270;&#22270;&#20449;&#24687;&#20013;&#21152;&#20837;SAM&#30340;&#28508;&#21147;&#65292;&#25506;&#32034;&#20854;&#22312;RVOS&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21407;&#22987;SAM&#27169;&#22411;&#36827;&#34892;&#20102;&#36866;&#24212;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#36328;&#27169;&#24577;MLP&#23558;&#25351;&#20195;&#34920;&#36798;&#30340;&#25991;&#26412;&#23884;&#20837;&#25237;&#24433;&#20026;&#31232;&#30095;&#21644;&#23494;&#38598;&#23884;&#20837;&#65292;&#20316;&#20026;&#29992;&#25143;&#20132;&#20114;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#36328;&#27169;&#24577;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20998;&#23618;&#31264;&#23494;&#27880;&#24847;&#27169;&#22359;&#65292;&#20197;&#23558;&#20998;&#23618;&#35270;&#35273;&#35821;&#20041;&#20449;&#24687;&#19982;&#31232;&#30095;&#23884;&#20837;&#34701;&#21512;&#65292;&#20197;&#33719;&#24471;&#32454;&#31890;&#24230;&#30340;&#23494;&#38598;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has gained significant attention for its impressive performance in image segmentation. However, it lacks proficiency in referring video object segmentation (RVOS) due to the need for precise user-interactive prompts and a limited understanding of different modalities, such as language and vision. This paper presents the RefSAM model, which explores the potential of SAM for RVOS by incorporating multi-view information from diverse modalities and successive frames at different timestamps in an online manner. Our proposed approach adapts the original SAM model to enhance cross-modality learning by employing a lightweight Cross-Modal MLP that projects the text embedding of the referring expression into sparse and dense embeddings, serving as user-interactive prompts. Additionally, we have introduced the hierarchical dense attention module to fuse hierarchical visual semantic information with sparse embeddings in order to obtain fine-grained dense embeddings
&lt;/p&gt;</description></item><item><title>DoReMi&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#20197;&#21457;&#29616;&#19981;&#19968;&#33268;&#65292;&#24182;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35268;&#21010;&#20197;&#23454;&#29616;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2307.00329</link><description>&lt;p&gt;
DoReMi: &#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#25191;&#34892;&#19981;&#19968;&#33268;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment. (arXiv:2307.00329v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00329
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#20197;&#21457;&#29616;&#19981;&#19968;&#33268;&#65292;&#24182;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35268;&#21010;&#20197;&#23454;&#29616;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#22823;&#37327;&#30340;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#20855;&#22791;&#20986;&#33394;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#20154;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#20197;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#22312;&#36923;&#36753;&#19978;&#27491;&#30830;&#19988;&#21487;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#25200;&#21160;&#25110;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#19981;&#23436;&#21892;&#65292;&#24213;&#23618;&#25191;&#34892;&#21487;&#33021;&#20250;&#20559;&#31163;&#39640;&#32423;&#35745;&#21010;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoReMi&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#21450;&#26102;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#29983;&#25104;&#35745;&#21010;&#27493;&#39588;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#25351;&#31034;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#22312;&#20302;&#23618;&#25216;&#33021;&#25191;&#34892;&#36807;&#31243;&#20013;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#12290;&#22914;&#26524;&#21457;&#29983;&#29305;&#23450;&#30340;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35268;&#21010;&#20197;&#20174;&#20013;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. Previous research has explored how to ground language models in robotic tasks to ensure that the sequences generated by the language model are both logically correct and practically executable. However, low-level execution may deviate from the high-level plan due to environmental perturbations or imperfect controller design. In this paper, we propose DoReMi, a novel language model grounding framework that enables immediate Detection and Recovery from Misalignments between plan and execution. Specifically, LLMs are leveraged for both planning and generating constraints for planned steps. These constraints can indicate plan-execution misalignments and we use a vision question answering (VQA) model to check constraints during low-level skill execution. If certain misalignment occurs, our method will call the language model to re-plan in order to recover from mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#33394;&#24425;&#31070;&#32463;&#34920;&#31034;&#27861;&#65288;Polyner&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;CT&#25104;&#20687;&#20013;&#23384;&#22312;&#37329;&#23646;&#20266;&#24433;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;Polyner&#36890;&#36807;&#24314;&#27169;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#20934;&#30830;&#27169;&#25311;CT&#37319;&#38598;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24674;&#22797;&#21407;&#22987;&#29289;&#20307;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;Polyner&#22312;&#37329;&#23646;&#20266;&#24433;&#20943;&#23569;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15203</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22810;&#33394;&#24425;&#31070;&#32463;&#34920;&#31034;&#27861;&#29992;&#20110;CT&#37329;&#23646;&#20266;&#24433;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Polychromatic Neural Representation for CT Metal Artifact Reduction. (arXiv:2306.15203v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#33394;&#24425;&#31070;&#32463;&#34920;&#31034;&#27861;&#65288;Polyner&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;CT&#25104;&#20687;&#20013;&#23384;&#22312;&#37329;&#23646;&#20266;&#24433;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;Polyner&#36890;&#36807;&#24314;&#27169;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#20934;&#30830;&#27169;&#25311;CT&#37319;&#38598;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24674;&#22797;&#21407;&#22987;&#29289;&#20307;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;Polyner&#22312;&#37329;&#23646;&#20266;&#24433;&#20943;&#23569;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22522;&#20110;&#23618;&#26512;&#26415;&#30340;&#31070;&#32463;&#37325;&#24314;&#25216;&#26415;&#65288;&#22914;NeRF&#65292;NeAT&#21644;NeRP&#65289;&#22312;&#21307;&#23398;&#25104;&#20687;&#26041;&#38754;&#24050;&#32463;&#23637;&#31034;&#20986;&#29420;&#29305;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#33394;&#24425;&#31070;&#32463;&#34920;&#31034;&#27861;&#65288;Polyner&#65289;&#26469;&#35299;&#20915;CT&#25104;&#20687;&#20013;&#23384;&#22312;&#20154;&#20307;&#37329;&#23646;&#26893;&#20837;&#29289;&#26102;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#37329;&#23646;&#20266;&#24433;&#26159;&#30001;&#20110;X&#23556;&#32447;&#33021;&#35889;&#19981;&#21516;&#33021;&#37327;&#32423;&#37329;&#23646;&#30340;&#34928;&#20943;&#31995;&#25968;&#21095;&#28872;&#21464;&#21270;&#32780;&#20135;&#29983;&#30340;&#65292;&#23548;&#33268;CT&#27979;&#37327;&#20013;&#30340;&#38750;&#32447;&#24615;&#37329;&#23646;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#20174;&#21463;&#37329;&#23646;&#24433;&#21709;&#30340;&#27979;&#37327;&#20013;&#37325;&#24314;CT&#22270;&#20687;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#37329;&#23646;&#20266;&#24433;&#20943;&#23569;&#65288;MAR&#65289;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#32463;&#39564;&#27169;&#22411;&#23548;&#33268;&#20449;&#21495;&#25439;&#22833;&#21644;&#24378;&#28872;&#30340;&#28151;&#21472;&#37325;&#24314;&#12290;Polyner&#20174;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#35282;&#24230;&#23545;MAR&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#22810;&#33394;&#24425;&#27491;&#28436;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#27169;&#25311;&#38750;&#32447;&#24615;CT&#37319;&#38598;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25454;&#27492;&#35774;&#35745;&#19968;&#20010;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#37329;&#23646;&#20266;&#24433;&#30340;CT&#25237;&#24433;&#22270;&#20013;&#24674;&#22797;&#20986;&#21407;&#22987;&#30340;&#29289;&#20307;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#23454;&#38469;CT&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;Polyner&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging neural reconstruction techniques based on tomography (e.g., NeRF, NeAT, and NeRP) have started showing unique capabilities in medical imaging. In this work, we present a novel Polychromatic neural representation (Polyner) to tackle the challenging problem of CT imaging when metallic implants exist within the human body. The artifacts arise from the drastic variation of metal's attenuation coefficients at various energy levels of the X-ray spectrum, leading to a nonlinear metal effect in CT measurements. Reconstructing CT images from metal-affected measurements hence poses a complicated nonlinear inverse problem where empirical models adopted in previous metal artifact reduction (MAR) approaches lead to signal loss and strongly aliased reconstructions. Polyner instead models the MAR problem from a nonlinear inverse problem perspective. Specifically, we first derive a polychromatic forward model to accurately simulate the nonlinear CT acquisition process. Then, we incorporate ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#35780;&#20272;&#32473;&#23450;&#36335;&#24452;&#35268;&#21010;&#20013;&#29305;&#23450;&#26102;&#38388;&#28857;&#19978;&#30340;&#21333;&#20010;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;(LTL)&#32422;&#26463;&#30340;&#30456;&#20851;&#24615;&#21644;&#29366;&#24577;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#31163;&#25955;&#26102;&#38388;&#12289;&#31163;&#25955;&#31354;&#38388;&#20013;&#25191;&#34892;&#26377;&#38480;&#35745;&#21010;&#30340;&#20195;&#29702;&#20219;&#21153;&#20013;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26102;&#38388;&#28857;&#35299;&#37322;&#21644;&#35268;&#21017;&#21442;&#25968;&#29366;&#24577;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13956</link><description>&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26102;&#38388;&#28857;&#35299;&#37322;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Pointwise-in-Time Explanation for Linear Temporal Logic Rules. (arXiv:2306.13956v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#35780;&#20272;&#32473;&#23450;&#36335;&#24452;&#35268;&#21010;&#20013;&#29305;&#23450;&#26102;&#38388;&#28857;&#19978;&#30340;&#21333;&#20010;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;(LTL)&#32422;&#26463;&#30340;&#30456;&#20851;&#24615;&#21644;&#29366;&#24577;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#31163;&#25955;&#26102;&#38388;&#12289;&#31163;&#25955;&#31354;&#38388;&#20013;&#25191;&#34892;&#26377;&#38480;&#35745;&#21010;&#30340;&#20195;&#29702;&#20219;&#21153;&#20013;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26102;&#38388;&#28857;&#35299;&#37322;&#21644;&#35268;&#21017;&#21442;&#25968;&#29366;&#24577;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#32473;&#23450;&#36335;&#24452;&#35268;&#21010;&#20013;&#29305;&#23450;&#26102;&#38388;&#28857;&#19978;&#30340;&#21333;&#20010;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;(LTL)&#32422;&#26463;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#20010;&#20219;&#21153;&#34987;&#25105;&#20204;&#31216;&#20026;&#8220;&#26102;&#38388;&#28857;&#35299;&#37322;&#8221;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#29366;&#24577;&#35780;&#20272;&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22312;Kripke&#32467;&#26500;&#21487;&#34920;&#36798;&#30340;&#31163;&#25955;&#26102;&#38388;&#12289;&#31163;&#25955;&#31354;&#38388;&#20013;&#25191;&#34892;&#26377;&#38480;&#35745;&#21010;&#30340;&#20195;&#29702;&#12290;&#22312;&#32473;&#23450;&#30340;&#32467;&#26500;&#19978;&#21644;&#24050;&#30693;&#32422;&#26463;&#20195;&#29702;&#30340;&#19968;&#32452;LTL&#35268;&#21017;&#30340;&#35745;&#21010;&#20013;&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#21709;&#24212;&#22320;&#29983;&#25104;&#35299;&#37322;&#12290;&#23545;&#20110;&#25152;&#36873;&#30340;&#26597;&#35810;&#26102;&#38388;&#65292;&#35299;&#37322;&#35782;&#21035;&#21738;&#20123;&#35268;&#21017;&#26159;&#27963;&#21160;&#30340;&#65292;&#21738;&#20123;&#35268;&#21017;&#21018;&#21018;&#34987;&#28385;&#36275;&#65292;&#21738;&#20123;&#35268;&#21017;&#26159;&#19981;&#27963;&#21160;&#30340;&#65292;&#20854;&#20013;&#26694;&#26550;&#29366;&#24577;&#26631;&#20934;&#26159;&#27491;&#24335;&#21644;&#30452;&#35266;&#22320;&#23450;&#20041;&#30340;&#12290;&#35299;&#37322;&#36824;&#21487;&#20197;&#21253;&#25324;&#21333;&#20010;&#35268;&#21017;&#21442;&#25968;&#30340;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#36827;&#19968;&#27493;&#30340;&#27934;&#23519;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#36825;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#20854;&#23454;&#29616;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a framework to assess the relevance of individual linear temporal logic (LTL) constraints at specific times in a given path plan, a task we refer to as "pointwise-in-time" explanation. We develop this framework, featuring a status assessment algorithm, for agents which execute finite plans in a discrete-time, discrete-space setting expressible via a Kripke structure. Given a plan on this structure and a set of LTL rules which are known to constrain the agent, the algorithm responds to two types of user queries to produce explanation. For the selected query time, explanations identify which rules are active, which have just been satisfied, and which are inactive, where the framework status criteria are formally and intuitively defined. Explanations may also include the status of individual rule arguments to provide further insight. In this paper, we systematically present this novel framework and provide an example of its implementation.
&lt;/p&gt;</description></item><item><title>SPRINT &#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#37325;&#26631;&#35760;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#20154;&#21147;&#65292;&#21516;&#26102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.11886</link><description>&lt;p&gt;
SPRINT&#65306;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196; relabeling &#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#31574;&#30053;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling. (arXiv:2306.11886v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11886
&lt;/p&gt;
&lt;p&gt;
SPRINT &#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#37325;&#26631;&#35760;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#20154;&#21147;&#65292;&#21516;&#26102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#26426;&#22120;&#20154;&#31574;&#30053;&#24182;&#36171;&#20104;&#20016;&#23500;&#30340;&#25216;&#33021;&#38598;&#21512;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23450;&#20041;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20294;&#36825;&#38656;&#35201;&#20154;&#20026;&#22320;&#27880;&#37322;&#25968;&#21313;&#19975;&#20010;&#25351;&#20196;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SPRINT&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#22823;&#22823;&#20943;&#23569;&#39044;&#35757;&#32451;&#22810;&#26679;&#30340;&#25216;&#33021;&#25152;&#38656;&#30340;&#20154;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#26680;&#24515;&#24819;&#27861;&#26469;&#33258;&#21160;&#25193;&#23637;&#22522;&#30784;&#39044;&#35757;&#32451;&#20219;&#21153;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#25351;&#20196;&#37325;&#26631;&#35760;&#21644;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20132;&#21449;&#36712;&#36857;&#25216;&#33021;&#38142;&#25509;&#12290;&#22240;&#27492;&#65292;SPRINT &#39044;&#35757;&#32451;&#21487;&#20197;&#20026;&#26426;&#22120;&#20154;&#35013;&#22791;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#12290;&#22312;&#23478;&#24237;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#21416;&#25151;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPRINT &#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training robot policies with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining through offline reinforcement learning. As a result, SPRINT pre-training equips robots with a much richer repertoire of skills. Experimental results in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/spr
&lt;/p&gt;</description></item><item><title>phi-1&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31934;&#24515;&#35757;&#32451;&#21644;&#20248;&#21270;&#65292;&#23613;&#31649;&#35268;&#27169;&#30456;&#23545;&#36739;&#23567;&#65292;&#20294;&#22312;&#20934;&#30830;&#29575;&#21644;&#26032;&#30340;&#24615;&#36136;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11644</link><description>&lt;p&gt;
&#25945;&#31185;&#20070;&#26159;&#20320;&#38656;&#35201;&#30340;&#20840;&#37096;&#12290; (arXiv:2306.11644v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Textbooks Are All You Need. (arXiv:2306.11644v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11644
&lt;/p&gt;
&lt;p&gt;
phi-1&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31934;&#24515;&#35757;&#32451;&#21644;&#20248;&#21270;&#65292;&#23613;&#31649;&#35268;&#27169;&#30456;&#23545;&#36739;&#23567;&#65292;&#20294;&#22312;&#20934;&#30830;&#29575;&#21644;&#26032;&#30340;&#24615;&#36136;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;phi-1&#65292;&#20854;&#20307;&#31215;&#26126;&#26174;&#23567;&#20110;&#31454;&#20105;&#27169;&#22411;&#65306;phi-1&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#25317;&#26377;13&#20159;&#20010;&#21442;&#25968;&#65292;&#22312;8&#20010;A100&#19978;&#36827;&#34892;&#20102;4&#22825;&#30340;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#32593;&#32476;&#30340;&#8220;&#25945;&#31185;&#20070;&#36136;&#37327;&#8221;&#25968;&#25454;&#65288;60&#20159;&#20010;&#26631;&#35760;&#65289;&#21644;&#20351;&#29992;GPT-3.5&#21512;&#25104;&#29983;&#25104;&#30340;&#25945;&#31185;&#20070;&#21644;&#32451;&#20064;&#65288;10&#20159;&#20010;&#26631;&#35760;&#65289;&#12290;&#23613;&#31649;&#35268;&#27169;&#23567;&#65292;phi-1&#22312;HumanEval&#19978;&#30340;pass@1&#20934;&#30830;&#29575;&#20026;50.6&#65285;&#65292;&#22312;MBPP&#19978;&#20026;55.5&#65285;&#12290;&#19982;&#25105;&#20204;&#22312;&#32534;&#30721;&#32451;&#20064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#30340;&#27169;&#22411; phi-1-base &#21644;&#20855;&#26377;&#30456;&#21516;&#27969;&#31243;&#30340;350M&#21442;&#25968;&#30340;&#36739;&#23567;&#27169;&#22411; phi-1-small &#30456;&#27604;&#65292;&#23427;&#36824;&#23637;&#29616;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#26032;&#30340;&#24615;&#36136;&#65292;phi-1-small &#22312; HumanEval &#19978;&#20173;&#36798;&#21040;45&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#36816;&#29992;&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#65292;&#20197;&#25552;&#21319;&#22495;&#36716;&#31227;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11180</link><description>&lt;p&gt;
&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#22312;&#22495;&#36716;&#31227;&#19979;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Active Learning for Semantic Segmentation under Domain Shift. (arXiv:2306.11180v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11180
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#36816;&#29992;&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#65292;&#20197;&#25552;&#21319;&#22495;&#36716;&#31227;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22495;&#36716;&#31227;&#19979;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#22522;&#20110;&#22270;&#20687;&#21306;&#22495;&#21644;&#20266;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#33719;&#21462;&#31574;&#30053;&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;&#22312;&#21306;&#22495;&#20869;&#23384;&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#35782;&#21035;&#20986;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#20687;&#32032;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20027;&#21160;&#23398;&#20064;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#38480;&#21046;&#65292;&#20266;&#26631;&#31614;&#30340;&#21464;&#21270;&#20165;&#38480;&#20110;&#36873;&#25321;&#31867;&#21035;&#30340;&#36718;&#24275;&#65292;&#38480;&#21046;&#20102;&#26368;&#32456;&#30340;&#20027;&#21160;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#20351;&#29992;&#36229;bolic&#26041;&#27861;&#26469;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#12290;&#36825;&#28304;&#20110;&#19968;&#31181;&#26080;&#23618;&#27425;&#32422;&#26463;&#35757;&#32451;&#30340;&#36229;bolic&#31354;&#38388;&#30340;&#26032;&#39062;&#20960;&#20309;&#29305;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#31867;&#21035;&#34987;&#26144;&#23556;&#21040;&#20855;&#26377;&#30456;&#24403;&#20869;&#31867;&#21322;&#24452;&#26041;&#24046;&#30340;&#32039;&#20945;&#36229;bolic&#21306;&#22495;&#65292;&#22240;&#20026;&#27169;&#22411;&#23558;&#38590;&#20197;&#35299;&#37322;&#30340;&#31867;&#21035;&#25918;&#32622;&#22312;&#26356;&#23494;&#38598;&#30340;&#36229;bolic&#21306;&#22495;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the task of semantic segmentation (SS) under domain shift, active learning (AL) acquisition strategies based on image regions and pseudo labels are state-of-the-art (SoA). The presence of diverse pseudo-labels within a region identifies pixels between different classes, which is a labeling efficient active learning data acquisition strategy. However, by design, pseudo-label variations are limited to only select the contours of classes, limiting the final AL performance. We approach AL for SS in the Poincar\'e hyperbolic ball model for the first time and leverage the variations of the radii of pixel embeddings within regions as a novel data acquisition strategy. This stems from a novel geometric property of a hyperbolic space trained without enforced hierarchies, which we experimentally prove. Namely, classes are mapped into compact hyperbolic areas with a comparable intra-class radii variance, as the model places classes of increasing explainable difficulty at denser hyperbolic are
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#36731;&#37327;&#32423;&#25506;&#38024;QUAG&#21644;&#26367;&#20195;&#26041;&#27861;QUAG-attention&#65292;&#21457;&#29616;&#35270;&#39057;&#38382;&#31572;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#24187;&#35937;&#65292;&#21363;&#20351;&#22312;&#22810;&#27169;&#24577;&#25439;&#20260;&#19979;&#20173;&#33021;&#20445;&#25345;&#39640;&#24615;&#33021;&#65292;&#19988;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#23454;&#29616;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08889</link><description>&lt;p&gt;
&#25581;&#31034;&#35270;&#39057;&#38382;&#31572;&#27169;&#22411;&#20013;&#32852;&#21512;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#24187;&#35937;
&lt;/p&gt;
&lt;p&gt;
Revealing the Illusion of Joint Multimodal Understanding in VideoQA Models. (arXiv:2306.08889v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08889
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#36731;&#37327;&#32423;&#25506;&#38024;QUAG&#21644;&#26367;&#20195;&#26041;&#27861;QUAG-attention&#65292;&#21457;&#29616;&#35270;&#39057;&#38382;&#31572;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#24187;&#35937;&#65292;&#21363;&#20351;&#22312;&#22810;&#27169;&#24577;&#25439;&#20260;&#19979;&#20173;&#33021;&#20445;&#25345;&#39640;&#24615;&#33021;&#65292;&#19988;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#23454;&#29616;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;Transformer&#27169;&#22411;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#20294;&#20854;&#25104;&#21151;&#21407;&#22240;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20849;&#21516;&#25429;&#25417;&#21644;&#21033;&#29992;&#35270;&#39057;&#21644;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#32467;&#26500;&#21644;&#21160;&#24577;&#24615;&#65311;&#25110;&#32773;&#23427;&#20204;&#20165;&#20165;&#26159;&#21033;&#29992;&#20102;&#25463;&#24452;&#26469;&#33719;&#24471;&#39640;&#20998;&#65311;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#19988;&#38750;&#21442;&#25968;&#21270;&#30340;&#25506;&#38024;&#8220;QUAG&#8221;&#65288;QUadrant AveraGe&#65289;&#65292;&#20197;&#23545;&#22810;&#27169;&#24577;&#34920;&#31034;&#36827;&#34892;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;QUAG&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#31995;&#32479;&#22320;&#28040;&#38500;&#27169;&#22411;&#30340;&#32806;&#21512;&#22810;&#27169;&#24577;&#29702;&#35299;&#26469;&#20419;&#36827;&#32852;&#21512;&#25968;&#25454;&#38598;-&#27169;&#22411;&#30740;&#31350;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#34920;&#26126;&#21363;&#20351;&#22312;&#22810;&#27169;&#24577;&#25439;&#20260;&#19979;&#65292;&#27169;&#22411;&#20173;&#33021;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;QUAG&#25193;&#23637;&#20026;&#8220;QUAG-attention&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21270;&#19988;&#34920;&#36798;&#33021;&#21147;&#36739;&#24369;&#30340;&#33258;&#27880;&#24847;&#21147;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24102;&#26377;QUAG-attention&#30340;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36798;&#21040;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#35745;&#31639;&#37327;&#26174;&#33879;&#20943;&#23569;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#24403;&#21069;&#30340;VideoQA&#27169;&#22411;&#22312;&#29702;&#35299;&#22810;&#27169;&#24577;&#20449;&#24687;&#26102;&#23384;&#22312;&#19968;&#23450;&#30340;&#24187;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
While VideoQA Transformer models demonstrate competitive performance on standard benchmarks, the reasons behind their success are not fully understood. Do these models jointly capture and leverage the rich multimodal structures and dynamics from video and text? Or are they merely exploiting shortcuts to achieve high scores? Hence, we design $\textit{QUAG}$ (QUadrant AveraGe), a lightweight and non-parametric probe, to critically analyze multimodal representations. QUAG facilitates combined dataset-model study by systematic ablation of model's coupled multimodal understanding during inference. Surprisingly, it demonstrates that the models manage to maintain high performance even under multimodal impairment. We extend QUAG to design "QUAG-attention", a simplistic and less-expressive replacement of self-attention. We find that the models with QUAG-attention achieve similar performance with significantly less mulops without any finetuning. These findings indicate that the current VideoQA b
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>Push&#26159;&#19968;&#20010;&#24182;&#21457;&#27010;&#29575;&#32534;&#31243;&#24211;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#65292;&#21487;&#20197;&#22312;&#22810;GPU&#30828;&#20214;&#19978;&#25191;&#34892;BDL&#25512;&#29702;&#31639;&#27861;&#12290;&#35813;&#24211;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#31890;&#23376;&#65292;&#24182;&#20801;&#35768;&#31890;&#23376;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#21644;&#21508;&#31181;&#21442;&#25968;&#26356;&#26032;&#65292;&#31616;&#21270;&#20102;BDL&#23454;&#39564;&#21644;&#25193;&#23637;&#31890;&#23376;&#25805;&#20316;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.06528</link><description>&lt;p&gt;
Push: &#24182;&#21457;&#27010;&#29575;&#32534;&#31243;&#29992;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Push: Concurrent Probabilistic Programming for Bayesian Deep Learning. (arXiv:2306.06528v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06528
&lt;/p&gt;
&lt;p&gt;
Push&#26159;&#19968;&#20010;&#24182;&#21457;&#27010;&#29575;&#32534;&#31243;&#24211;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#65292;&#21487;&#20197;&#22312;&#22810;GPU&#30828;&#20214;&#19978;&#25191;&#34892;BDL&#25512;&#29702;&#31639;&#27861;&#12290;&#35813;&#24211;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#31890;&#23376;&#65292;&#24182;&#20801;&#35768;&#31890;&#23376;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#21644;&#21508;&#31181;&#21442;&#25968;&#26356;&#26032;&#65292;&#31616;&#21270;&#20102;BDL&#23454;&#39564;&#21644;&#25193;&#23637;&#31890;&#23376;&#25805;&#20316;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Push&#30340;&#24211;&#65292;&#37319;&#29992;&#27010;&#29575;&#32534;&#31243;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#12290;&#35813;&#24211;&#21487;&#22312;&#22810;GPU&#30828;&#20214;&#19978;&#24182;&#21457;&#25191;&#34892;BDL&#25512;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;Push&#24341;&#20837;&#20102;&#19968;&#31181;&#25277;&#35937;&#65292;&#23558;&#36755;&#20837;NN&#34920;&#31034;&#20026;&#19968;&#20010;&#31890;&#23376;&#12290;Push&#20351;&#24471;&#21019;&#24314;&#31890;&#23376;&#21464;&#24471;&#23481;&#26131;&#65292;&#20197;&#20415;&#20110;&#22797;&#21046;&#21644;&#31890;&#23376;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#21508;&#31181;&#21442;&#25968;&#26356;&#26032;&#65292;&#21253;&#25324;&#24120;&#35265;&#30340;BDL&#31639;&#27861;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;Push&#38477;&#20302;&#36827;&#34892;BDL&#23454;&#39564;&#30340;&#38376;&#27099;&#65292;&#36890;&#36807;&#31616;&#21270;&#22312;&#22810;GPU&#19978;&#25193;&#23637;&#31890;&#23376;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21033;&#29992;&#21333;&#33410;&#28857;&#22810;GPU&#35774;&#22791;&#36827;&#34892;&#35270;&#35273;&#21644;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20219;&#21153;&#26102;&#30340;&#31890;&#23376;&#25193;&#23637;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a library called Push that takes a probabilistic programming approach to Bayesian deep learning (BDL). This library enables concurrent execution of BDL inference algorithms on multi-GPU hardware for neural network (NN) models. To accomplish this, Push introduces an abstraction that represents an input NN as a particle. Push enables easy creation of particles so that an input NN can be replicated and particles can communicate asynchronously so that a variety of parameter updates can be expressed, including common BDL algorithms. Our hope is that Push lowers the barrier to experimenting with BDL by streamlining the scaling of particles across GPUs. We evaluate the scaling behavior of particles on single-node multi-GPU devices on vision and scientific machine learning (SciML) tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01631</link><description>&lt;p&gt;
Gode -- &#23558;&#29983;&#29289;&#21270;&#23398;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21040;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20419;&#36827;&#21019;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#21457;&#23637;&#21644;&#29702;&#35299;&#21270;&#23398;&#29289;&#36136;&#21644;&#29983;&#29289;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20010;&#20998;&#23376;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889; (KG) &#30340;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#36827;&#34892;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#20998;&#23376;&#32423;&#21644; KG &#32423;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312; 11 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#22810;&#31383;&#21475;&#26412;&#22320;-&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MW-MAE&#65289;&#22312;&#38899;&#39057;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36890;&#29992;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00561</link><description>&lt;p&gt;
&#22810;&#31383;&#21475;&#26412;&#22320;-&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26159;&#26356;&#22909;&#30340;&#38899;&#39057;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners. (arXiv:2306.00561v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00561
&lt;/p&gt;
&lt;p&gt;
&#22810;&#31383;&#21475;&#26412;&#22320;-&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MW-MAE&#65289;&#22312;&#38899;&#39057;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36890;&#29992;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37197;&#22791;&#20102;&#26032;&#22411;&#22810;&#31383;&#21475;&#22810;&#22836;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#22810;&#31383;&#21475;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MW-MAE&#65289;&#65292;&#36890;&#36807;&#20960;&#20010;&#19981;&#21516;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#31383;&#21475;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26377;&#21161;&#20110;&#22312;&#27599;&#20010;&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#22359;&#20013;&#23545;&#23616;&#37096;-&#20840;&#23616;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#21313;&#20010;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;MW-MAEs&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#22987;&#32456;&#20248;&#20110;&#26631;&#20934;MAEs&#65292;&#24182;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;&#65292;&#21516;&#26102;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#36890;&#36807;&#30740;&#31350;&#27880;&#24847;&#36317;&#31163;&#21644;&#29109;&#65292;&#21457;&#29616;MW-MAE&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#20855;&#26377;&#26356;&#23485;&#24191;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#22836;&#37096;&#12290;&#36890;&#36807;&#25237;&#24433;&#21152;&#26435;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;PWCCA&#65289;&#20998;&#26512;&#27880;&#24847;&#22836;&#29305;&#24449;&#34920;&#31034;&#65292;&#26174;&#31034;MW-MAE&#30340;&#35299;&#30721;&#22120;&#23618;&#20013;&#20855;&#26377;&#30456;&#21516;&#31383;&#21475;&#22823;&#23567;&#30340;&#27880;&#24847;&#21147;&#22836;&#23398;&#20064;&#21040;&#30456;&#20851;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#36825;&#20351;&#24471;&#27599;&#20010;&#22359;&#33021;&#22815;&#29420;&#31435;&#25429;&#25417;&#21040;&#26412;&#22320;&#21644;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations, along with demonstrating considerably better scaling characteristics. Investigating attention distances and entropies reveals that MW-MAE encoders learn heads with broader local and global attention. Analyzing attention head feature representations through Projection Weighted Canonical Correlation Analysis (PWCCA) shows that attention heads with the same window sizes across the decoder layers of the MW-MAE learn correlated feature representations which enables each block to independently capture local and glo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19358</link><description>&lt;p&gt;
&#31283;&#20581;&#30340;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#23646;&#24615;&#24050;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25991;&#29486;&#26222;&#36941;&#35748;&#20026;LLMs&#34920;&#31034;&#30001;&#23569;&#25968;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#21644;&#24133;&#24230;&#30340;&#8220;&#24322;&#24120;&#32500;&#24230;&#8221;&#20027;&#23548;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#20943;&#36731;&#36825;&#20123;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#36843;&#20351;LLMs&#25104;&#20026;&#21508;&#21521;&#21516;&#24615;&#65288;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25152;&#26377;&#32500;&#24230;&#20855;&#26377;&#22343;&#21248;&#26041;&#24046;&#65289;&#30340;&#12290;&#21508;&#21521;&#21516;&#24615;&#34987;&#35748;&#20026;&#26159;LLMs&#30340;&#19968;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#26356;&#21152;&#36148;&#36817;&#20154;&#31867;&#30452;&#35273;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;NLP&#20013;&#21508;&#21521;&#21516;&#24615;&#30340;&#35768;&#22810;&#35266;&#28857;&#37117;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#26377;&#32570;&#38519;&#30340;&#21508;&#21521;&#21516;&#24615;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-STAR&#65306;&#22522;&#20110;IsoScore$^{\star}$&#30340;&#31283;&#23450;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#21453;&#39304;&#30340;RLHF&#12290;</title><link>http://arxiv.org/abs/2305.18505</link><description>&lt;p&gt;
&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Query Human Feedback Efficiently in RL?. (arXiv:2305.18505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#21453;&#39304;&#30340;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#33539;&#20363;&#65292;&#22312;&#27492;&#33539;&#20363;&#19979;&#65292;RL&#20195;&#29702;&#23398;&#20064;&#20351;&#29992;&#23545;&#36712;&#36857;&#30340;&#25104;&#23545;&#20248;&#20808;&#32423;&#21453;&#39304;&#26469;&#26368;&#20248;&#21270;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#26126;&#30830;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#23613;&#31649;RLHF&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#29992;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#23454;&#35777;&#30740;&#31350;&#24182;&#26410;&#35299;&#20915;&#22914;&#20309;&#39640;&#25928;&#37319;&#26679;&#36712;&#36857;&#23545;&#20197;&#26597;&#35810;&#20154;&#31867;&#21453;&#39304;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#25506;&#32034;&#24615;&#36712;&#36857;&#65292;&#22312;&#25910;&#38598;&#20219;&#20309;&#20154;&#31867;&#21453;&#39304;&#20043;&#21069;&#65292;&#20351;&#23398;&#20064;&#38544;&#34255;&#30340;&#22870;&#21169;&#20989;&#25968;&#26356;&#21152;&#20934;&#30830;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#25991;&#29486;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#22522;&#20110;&#20559;&#22909;&#27169;&#22411;&#19979;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#20154;&#31867;&#21453;&#39304;&#26356;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#32435;&#20837;&#32447;&#24615;&#21644;&#20302;&#31209;MDPs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#30340;&#21453;&#39304;&#30340;RLHF&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#20248;&#21270;&#20855;&#26377;&#26377;&#38480;&#21453;&#39304;&#30340;&#20219;&#21153;&#26102;&#33719;&#24471;&#25506;&#32034;&#24615;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While RLHF has demonstrated practical success in fine-tuning language models, existing empirical work does not address the challenge of how to efficiently sample trajectory pairs for querying human feedback. In this study, we propose an efficient sampling approach to acquiring exploratory trajectories that enable accurate learning of hidden reward functions before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing literature. Specifically, our framework can incorporate linear and low-rank MDPs. Additionally, we investigate RLHF with action-based comparison feedback and introduce an
&lt;/p&gt;</description></item><item><title>C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2305.16209</link><description>&lt;p&gt;
C-MCTS: &#23433;&#20840;&#35268;&#21010;&#19982;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
C-MCTS: Safe Planning with Monte Carlo Tree Search. (arXiv:2305.16209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16209
&lt;/p&gt;
&lt;p&gt;
C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#21487;&#20197;&#35299;&#20915;&#21463;&#32422;&#26463;&#30340;&#23433;&#20840;&#20915;&#31574;&#38382;&#39064;&#12290;&#23613;&#31649;CMDP&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;MCTS&#31561;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;CMDP&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#22312;&#25104;&#26412;&#26041;&#38754;&#20445;&#23432;&#34892;&#20107;&#65292;&#36890;&#36807;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#25104;&#26412;&#20272;&#35745;&#26469;&#36991;&#20813;&#36829;&#21453;&#32422;&#26463;&#65292;&#20294;&#36825;&#31181;&#20272;&#35745;&#23384;&#22312;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;MCTS&#65288;C-MCTS&#65289;&#65292;&#23427;&#20351;&#29992;&#20808;&#21069;&#22312;&#20195;&#29702;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#35757;&#32451;&#30340;&#23433;&#20840;&#35780;&#21028;&#22120;&#26469;&#20272;&#35745;&#25104;&#26412;&#12290;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#35780;&#21028;&#22120;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#12290;C-MCTS&#28385;&#36275;&#25104;&#26412;&#32422;&#26463;&#65292;&#20294;&#25805;&#20316;&#25509;&#36817;&#32422;&#26463;&#36793;&#30028;&#65292;&#27604;&#20197;&#24448;&#30340;&#24037;&#20316;&#33719;&#24471;&#26356;&#39640;&#30340;&#22870;&#21169;&#12290;&#20316;&#20026;&#19968;&#20010;&#24456;&#22909;&#30340;&#21103;&#20135;&#21697;&#65292;&#36825;&#20010;&#35268;&#21010;&#22120;&#22312;&#35268;&#21010;&#27493;&#39588;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#27169;&#22411;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
The Constrained Markov Decision Process (CMDP) formulation allows to solve safety-critical decision making tasks that are subject to constraints. While CMDPs have been extensively studied in the Reinforcement Learning literature, little attention has been given to sampling-based planning algorithms such as MCTS for solving them. Previous approaches perform conservatively with respect to costs as they avoid constraint violations by using Monte Carlo cost estimates that suffer from high variance. We propose Constrained MCTS (C-MCTS), which estimates cost using a safety critic that is trained with Temporal Difference learning in an offline phase prior to agent deployment. The critic limits exploration by pruning unsafe trajectories within MCTS during deployment. C-MCTS satisfies cost constraints but operates closer to the constraint boundary, achieving higher rewards than previous work. As a nice byproduct, the planner is more efficient w.r.t. planning steps. Most importantly, under model
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34987;&#21160;&#23398;&#20064;&#65292;&#22312;&#26234;&#33021;&#20307;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#33324;&#21270;&#30340;&#20027;&#21160;&#22240;&#26524;&#31574;&#30053;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#25512;&#26029;&#21644;&#20351;&#29992;&#20174;&#26410;&#20986;&#29616;&#30340;&#22240;&#26524;&#38142;&#25509;&#65292;&#24182;&#23558;&#23454;&#39564;&#31574;&#30053;&#25512;&#24191;&#21040;&#20174;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#21464;&#37327;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.16183</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#20307;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#21160;&#23398;&#20064;&#20027;&#21160;&#22240;&#26524;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Passive learning of active causal strategies in agents and language models. (arXiv:2305.16183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16183
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34987;&#21160;&#23398;&#20064;&#65292;&#22312;&#26234;&#33021;&#20307;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#33324;&#21270;&#30340;&#20027;&#21160;&#22240;&#26524;&#31574;&#30053;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#25512;&#26029;&#21644;&#20351;&#29992;&#20174;&#26410;&#20986;&#29616;&#30340;&#22240;&#26524;&#38142;&#25509;&#65292;&#24182;&#23558;&#23454;&#39564;&#31574;&#30053;&#25512;&#24191;&#21040;&#20174;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#21464;&#37327;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34987;&#21160;&#25968;&#25454;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#21040;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#21644;&#23454;&#39564;&#30340;&#20160;&#20040;&#20449;&#24687;&#65311;&#37492;&#20110;&#34987;&#21160;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#20351;&#29992;&#31561;&#20132;&#20114;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#21151;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#24456;&#37325;&#35201;&#12290;&#34987;&#21160;&#23398;&#20064;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#34987;&#21160;&#23398;&#20064;&#23454;&#38469;&#19978;&#33021;&#22815;&#35753;&#26234;&#33021;&#20307;&#23398;&#20064;&#21040;&#19968;&#33324;&#21270;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#30830;&#23450;&#21644;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#32467;&#26500;&#65292;&#21482;&#35201;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#24178;&#39044;&#12290;&#25105;&#20204;&#22312;&#24418;&#24335;&#19978;&#35828;&#26126;&#20102;&#39318;&#20808;&#36827;&#34892;&#23454;&#39564;&#65292;&#28982;&#21518;&#23547;&#27714;&#30446;&#26631;&#30340;&#31574;&#30053;&#33021;&#22815;&#21407;&#21017;&#19978;&#20351;&#34987;&#21160;&#23398;&#20064;&#23454;&#29616;&#19968;&#33324;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#22312;&#27979;&#35797;&#26102;&#33021;&#22815;&#25512;&#26029;&#21644;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#20174;&#26410;&#20986;&#29616;&#30340;&#22240;&#26524;&#38142;&#25509;&#65307;&#36825;&#20123;&#26234;&#33021;&#20307;&#36824;&#33021;&#22815;&#23558;&#23454;&#39564;&#31574;&#30053;&#25512;&#24191;&#21040;&#20174;&#26410;&#22312;&#35757;&#32451;&#20013;&#35266;&#23519;&#21040;&#30340;&#26032;&#21464;&#37327;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#19968;&#33324;&#21270;&#22240;&#26524;&#24178;&#39044;&#21644;&#21033;&#29992;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.15852</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#65306;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#24819;&#30340;&#25991;&#26412;&#12290;&#33258;&#30456;&#30683;&#30462;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24187;&#35273;&#24418;&#24335;&#65292;&#25351;&#30340;&#26159;&#35821;&#35328;&#27169;&#22411;&#22312;&#21516;&#19968;&#35821;&#22659;&#20013;&#29983;&#25104;&#20004;&#20010;&#30683;&#30462;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#38024;&#23545;&#26368;&#20808;&#36827;&#12289;&#32463;&#36807;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#33258;&#30456;&#30683;&#30462;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12289;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#33879;&#21517;&#30340;&#36824;&#26159;&#19981;&#22826;&#20986;&#21517;&#30340;&#35805;&#39064;&#65292;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#30456;&#30683;&#30462;&#37117;&#32463;&#24120;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15611</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#35282;&#24230;&#21078;&#26512;&#29983;&#29289;&#25968;&#25454;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#65306;&#35266;&#28857;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#26159;&#21542;&#20855;&#26377;&#20174;&#23567;&#22270;&#20013;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#25512;&#24191;&#21040;&#21516;&#19968;&#39046;&#22495;&#30340;&#22823;&#22270;&#20013;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23588;&#20854;&#26159;&#24230;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#20013;&#65292;&#24230;&#25968;&#26159;&#26377;&#30028;&#30340;&#65292;&#22240;&#27492;&#24230;&#20998;&#24067;&#30340;&#20559;&#31227;&#24456;&#23567;&#12290;&#21363;&#20351;&#24230;&#20998;&#24067;&#20559;&#31227;&#24456;&#23567;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GNNs&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#26263;&#31034;&#26377;&#20854;&#20182;&#21407;&#22240;&#12290;&#20107;&#23454;&#19978;&#65292;&#20197;&#24448;&#23545;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#21508;&#31181;&#22270;&#23610;&#23544;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#25506;&#32034;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#20998;&#26512;&#22823;&#22810;&#38598;&#20013;&#22312;&#31354;&#38388;&#39046;&#22495;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#35889;&#35282;&#24230;&#21435;&#30740;&#31350;GNNs&#22312;&#29983;&#29289;&#22270;&#25968;&#25454;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#27169;&#25311;&#21508;&#31181;&#31867;&#22411;&#30340;&#24230;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#27979;&#35797;GNNs &#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38500;&#20102;&#24230;&#20998;&#24067;&#20559;&#31227;&#22806;&#65292;GNNs &#36824;&#23545;&#22270;&#22823;&#23567;&#21464;&#21270;&#24341;&#36215;&#30340;&#35889;&#20998;&#24067;&#20559;&#31227;&#24456;&#25935;&#24863;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;GNN&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#20855;&#26377;&#23610;&#23544;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20851;&#20110;GNNs&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#30340;&#26032;&#35266;&#28857;&#21644;&#23454;&#36341;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
&lt;/p&gt;</description></item><item><title>STAR&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.15090</link><description>&lt;p&gt;
STAR: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32467;&#26500;&#21040;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
STAR: Improving Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models. (arXiv:2305.15090v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15090
&lt;/p&gt;
&lt;p&gt;
STAR&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#20107;&#20214;&#25277;&#21462;&#65292;&#38656;&#35201;&#23545;&#36755;&#20986;&#32467;&#26500;&#21644;&#23376;&#20219;&#21153;&#20381;&#36182;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#20026;&#20102;&#33719;&#24471;&#21512;&#29702;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#20197;&#65288;&#27573;&#33853;&#65292;&#30446;&#26631;&#32467;&#26500;&#65289;&#23545;&#30340;&#24418;&#24335;&#30340;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#26159;&#26114;&#36149;&#30340;&#65292;&#22240;&#27492;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#26041;&#27861;&#12290;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#35201;&#20040;&#20173;&#28982;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#35201;&#20040;&#30001;&#20110;&#24615;&#33021;&#24046;&#32780;&#26080;&#27861;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAR&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#26377;&#38480;&#30340;&#31181;&#23376;&#31034;&#20363;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#65292;&#20174;&#32780;&#25552;&#39640;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction tasks such as event extraction require an in-depth understanding of the output structure and sub-task dependencies. They heavily rely on task-specific training data in the form of (passage, target structure) pairs to obtain reasonable performance. However, obtaining such data through human annotation is costly, leading to a pressing need for low-resource information extraction approaches that require minimal human labeling for real-world applications. Fine-tuning supervised models with synthesized training data would be a generalizable method, but the existing data generation methods either still rely on large-scale ground-truth data or cannot be applied to complicated IE tasks due to their poor performance. To address these challenges, we propose STAR, a data generation method that leverages Large Language Models (LLMs) to synthesize data instances given limited seed demonstrations, thereby boosting low-resource information extraction performance. Our approach i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13301</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Diffusion Models with Reinforcement Learning. (arXiv:2305.13301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#28789;&#27963;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#30340;&#36817;&#20284;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#24182;&#19981;&#20851;&#27880;&#20284;&#28982;&#65292;&#32780;&#26159;&#20851;&#27880;&#20154;&#31867;&#24863;&#30693;&#30340;&#22270;&#20687;&#36136;&#37327;&#25110;&#33647;&#29289;&#25928;&#21147;&#31561;&#19979;&#28216;&#30446;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#27492;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;&#21435;&#22122;&#35270;&#20026;&#22810;&#27493;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#19968;&#31867;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#26367;&#20195;&#30340;&#22870;&#21169;&#21152;&#26435;&#20284;&#28982;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;DDPO&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;DDPO&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;&#21453;&#39304;&#30340;&#25552;&#31034;-&#22270;&#20687;&#23545;&#40784;&#26041;&#24335;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCare&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12788</link><description>&lt;p&gt;
GraphCare: &#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#25552;&#21319;&#21307;&#30103;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. (arXiv:2305.12788v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCare&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#65292;&#20294;&#23558;&#21307;&#23398;&#30693;&#35782;&#25972;&#21512;&#21040;&#39044;&#27979;&#21644;&#20915;&#31574;&#20013;&#20197;&#25552;&#39640;&#25928;&#26524;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#20010;&#24615;&#21270;&#39044;&#27979;&#38656;&#35201;&#20010;&#24615;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#65292;&#32780;&#20174;&#24739;&#32773;EHR&#25968;&#25454;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\textsc{GraphCare}&#30340;&#24320;&#25918;&#24335;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;EHR&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#22806;&#37096;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#26500;&#24314;&#20010;&#20307;&#21270;&#30340;&#24739;&#32773;&#30693;&#35782;&#22270;&#35889;&#65292;&#28982;&#21518;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;Bi-attention AugmenTed (BAT)&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#36827;&#34892;&#21307;&#30103;&#39044;&#27979;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;MIMIC-III&#21644;MIMIC-IV&#19978;&#65292;\textsc{GraphCare}&#22312;&#22235;&#20010;&#20851;&#38190;&#30340;&#21307;&#30103;&#39044;&#27979;&#20219;&#21153;&#19978;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#32447;&#65306;&#27515;&#20129;&#29575;&#12289;&#20877;&#20837;&#38498;&#29575;&#12289;&#20303;&#38498;&#22825;&#25968;&#21644;&#33647;&#29289;&#25512;&#33616;&#12290;&#22312;MIMIC-III&#19978;&#65292;&#23427;&#23558;AUROC&#25552;&#39640;&#20102;17.6%&#21644;6.6%&#65292;&#23558;F1&#24471;&#20998;&#25552;&#39640;&#20102;7.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose \textsc{GraphCare}, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, \textsc{GraphCare} surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by 7.9\% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11854</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577; Web &#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027; Web &#23548;&#33322;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20381;&#36182;&#25968;&#21313;&#20159;&#27425;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#24615;&#20132;&#20114;&#21644;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#26469;&#33258;&#20016;&#23500;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33073;&#26426;&#35757;&#32451;&#65292;&#29992;&#20110;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340; Web &#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;&#65292; WebGUM&#65292;&#23427;&#35266;&#23519;&#20102;&#32593;&#39029;&#25130;&#22270;&#21644; HTML &#39029;&#38754;&#65292;&#24182;&#36755;&#20986; Web &#23548;&#33322;&#25805;&#20316;&#65292;&#22914;&#21333;&#20987;&#21644;&#36755;&#20837;&#12290;WebGUM &#26159;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#22823;&#37327;&#30340;&#28436;&#31034;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312; MiniWoB &#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#36229;&#36807;&#20043;&#21069;&#26368;&#20339;&#33073;&#26426;&#26041;&#27861; 31.9% &#20197;&#19978;&#65292;&#25509;&#36817;&#23454;&#29616;&#22312;&#32447;&#20132;&#20114;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11738</link><description>&lt;p&gt;
CRITIC&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24037;&#20855;&#20132;&#20114;&#25209;&#35780;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#65292;&#20363;&#22914;&#20986;&#29616;&#24187;&#35273;&#20107;&#23454;&#65292;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#25110;&#21019;&#24314;&#20882;&#29359;&#21644;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#19982;&#36825;&#20123;&#27169;&#22411;&#19981;&#21516;&#65292;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#20132;&#21449;&#26816;&#26597;&#21644;&#31934;&#28860;&#20182;&#20204;&#30340;&#21021;&#27493;&#20869;&#23481;&#65292;&#20363;&#22914;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#25110;&#20351;&#29992;&#20195;&#30721;&#35299;&#37322;&#22120;&#36827;&#34892;&#35843;&#35797;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#65288;&#23454;&#36136;&#19978;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#20197;&#31867;&#20284;&#20110;&#20154;&#31867;&#19982;&#24037;&#20855;&#20132;&#20114;&#30340;&#26041;&#24335;&#39564;&#35777;&#21644;&#36880;&#27493;&#20462;&#27491;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20174;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#65292;CRITIC&#19982;&#36866;&#24403;&#30340;&#24037;&#20855;&#20132;&#20114;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#28982;&#21518;&#26681;&#25454;&#22312;&#27492;&#39564;&#35777;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#20462;&#25913;&#36755;&#20986;&#12290;&#28041;&#21450;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#12289;&#25968;&#23398;&#31243;&#24207;&#32508;&#21512;&#21644;&#27602;&#24615;&#26816;&#27979;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;LLMs&#33021;&#22815;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#24182;&#32416;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.10924</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#26500;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Structural Pruning for Diffusion Models. (arXiv:2305.10924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPM&#65289;&#30340;&#36716;&#22411;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#36890;&#24120;&#28041;&#21450;&#21040;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#37117;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-Pruning&#65292;&#19968;&#31181;&#19987;&#20026;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;Diff-Pruning&#30340;&#26412;&#36136;&#26159;&#36890;&#36807;&#21098;&#26525;&#26102;&#38388;&#27493;&#38271;&#30340;Taylor&#23637;&#24320;&#65292;&#22312;&#36807;&#28388;&#25481;&#26080;&#36129;&#29486;&#25193;&#25955;&#27493;&#39588;&#21644;&#25972;&#21512;&#26377;&#20449;&#24687;&#30340;&#26799;&#24230;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65306;1&#65289;&#25928;&#29575;&#65306;&#23427;&#21487;&#20197;&#20197;&#21407;&#22987;&#35757;&#32451;&#25237;&#20837;&#30340;&#20165;10&#65285;&#21040;20&#65285;&#30340;&#20195;&#20215;&#23454;&#29616;&#32422;50&#65285;&#30340;FLOPs&#20943;&#23569;; 2&#65289;&#19968;&#33268;&#24615;: &#21098;&#26525;&#21518;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#25928;&#26524;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#65292;&#19981;&#20250;&#24433;&#21709;&#29983;&#25104;&#24314;&#27169;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across four diverse datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) Consistency: the pruned diffusio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10865</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22411;MARL&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#30528;&#37325;&#20110;&#36866;&#24403;&#30340;&#20449;&#29992;&#20998;&#37197;&#12290;&#33258;&#21160;&#23376;&#30446;&#26631;&#29983;&#25104;&#65288;ASG&#65289;&#26159;&#26368;&#36817;&#20986;&#29616;&#30340;&#19968;&#31181;&#21487;&#34892;&#30340;MARL&#26041;&#27861;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#22312;&#20869;&#22312;&#39537;&#21160;&#30340;&#22686;&#24378;&#23398;&#20064;&#20013;&#21033;&#29992;&#23376;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20174;&#31232;&#30095;&#22870;&#21169;&#20013;&#36827;&#34892;&#22797;&#26434;&#20219;&#21153;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26080;&#30097;&#38656;&#35201;&#22823;&#37327;&#30340;&#22521;&#35757;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#35299;&#32806;"&#20915;&#31574;&#26041;&#27861;&#65292;&#21363;&#22312;MARL&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;&#65288;SAMA&#65289;&#65292;&#21463;&#21040;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the "over-representation" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel "disentangled" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thou
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2305.03731</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Assessing Working Memory Capacity of ChatGPT. (arXiv:2305.03731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#26159;&#20154;&#31867;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20316;&#20026;&#20449;&#24687;&#20020;&#26102;&#23384;&#20648;&#21644;&#25805;&#20316;&#30340;&#24037;&#20316;&#31354;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;ChatGPT&#22312;N-back&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35843;&#26597;&#20102;&#36825;&#19968;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#24037;&#20316;&#35760;&#24518;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#25509;&#30528;&#20171;&#32461;&#20102;&#35780;&#20272;ChatGPT&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#22312;&#35328;&#35821;&#21644;&#31354;&#38388;N- back&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#25991;&#29486;&#25253;&#36947;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24403;&#21069;&#36827;&#23637;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#65292;&#24182;&#20026;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29702;&#35299;&#20154;&#31867;&#24037;&#20316;&#35760;&#24518;&#30340;&#26410;&#26469;&#21162;&#21147;&#25552;&#20379;&#20102;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory is a critical aspect of both human intelligence and artificial intelligence (AI), serving as a workspace for the temporary storage and manipulation of information. This paper investigates working memory capacity of ChatGPT, a state-of-the-art language model, by examining its performance on N-back tasks. We begin by discussing the importance of working memory to humans and AI, followed by the methods employed to assess working memory capacity of ChatGPT. Our study compares behavioral performance of ChatGPT on verbal and spatial N-back tasks to that of human participants reported in the literature, revealing notable similarities. Our findings offer crucial insights into the current progress in designing AI systems with human-level cognitive abilities and hold promise for informing future endeavors aimed at enhancing AI working memory and understanding human working memory through AI models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02614</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#21450;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling. (arXiv:2305.02614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#23547;&#25214;&#40657;&#31665;&#20989;&#25968;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#34429;&#28982;&#40657;&#31665;&#20989;&#25968;&#30340;&#35780;&#20272;&#25104;&#26412;&#24448;&#24448;&#24456;&#39640;&#65292;&#20294;&#20943;&#23569;&#26114;&#36149;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#26159;&#29702;&#24819;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;BO&#29615;&#22659;&#19979;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#20854;&#20013;&#65292;&#20851;&#38190;&#22312;&#20110;&#36873;&#25321;&#39564;&#35777;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20197;&#25552;&#39640;BO&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#37319;&#26679;&#65292;&#25105;&#20204;&#37319;&#29992;&#40657;&#31665;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#65292;&#23558;&#20854;&#20248;&#21270;&#20026;&#25152;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#12290;&#26356;&#36827;&#19968;&#27493;&#65292;&#36890;&#36807;&#20174;&#21160;&#24577;&#36866;&#24212;&#30340;&#26497;&#20540;&#20998;&#24067;&#20013;&#36873;&#25321;&#26410;&#26631;&#31614;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;BO&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;BO&#26041;&#27861;&#22312;&#23398;&#20064;&#21518;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#20351;&#20854;&#21487;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a powerful tool for seeking the global optimum of black-box functions. While evaluations of the black-box functions can be highly costly, it is desirable to reduce the use of expensive labeled data. For the first time, we introduce a teacher-student model to exploit semi-supervised learning that can make use of large amounts of unlabelled data under the context of BO. Importantly, we show that the selection of the validation and unlabeled data is key to the performance of BO. To optimize the sampling of unlabeled data, we employ a black-box parameterized sampling distribution optimized as part of the employed bi-level optimization framework. Taking one step further, we demonstrate that the performance of BO can be further improved by selecting unlabeled data from a dynamically fitted extreme value distribution. Our BO method operates in a learned latent space with reduced dimensionality, making it scalable to high-dimensional problems. The proposed approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DP-ICL&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#20445;&#35777;&#19979;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;&#32463;&#36807;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.01639</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private In-Context Learning. (arXiv:2305.01639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DP-ICL&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#20445;&#35777;&#19979;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;&#32463;&#36807;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#22686;&#24378;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"DP-ICL"&#26469;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#20445;&#35777;&#12290;DP-ICL&#36890;&#36807;&#20351;&#29992;"report-noisy-max"&#26426;&#21046;&#22312;&#31034;&#20363;&#38598;&#21512;&#19978;&#24314;&#31435;&#22024;&#26434;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#31169;&#26377;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;DP-ICL&#65292;&#21457;&#29616;&#20854;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;(&lt;2%&#38477;&#32423;)&#12290;
&lt;/p&gt;
&lt;p&gt;
An important question in deploying large language models (LLMs) is how to augment LLMs with private data. We propose Differentially Private In-context Learning (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy guarantees. DP-ICL performs private inference by establishing noisy consensus over an ensemble of exemplars using the Report-Noisy-Max mechanism. We evaluate DP-ICL on four benchmarks and find that it achieves comparable performance (&lt;2\% degradation) with non-private ICL.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#38024;&#23545;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#24471;&#21040;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;Presto&#65289;&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14065</link><description>&lt;p&gt;
&#38754;&#21521;&#36965;&#24863;&#26102;&#24207;&#25968;&#25454;&#30340;&#36731;&#37327;&#32423;&#39044;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
Lightweight, Pre-trained Transformers for Remote Sensing Timeseries. (arXiv:2304.14065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14065
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#38024;&#23545;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#24471;&#21040;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;Transformer&#65288;Presto&#65289;&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#20256;&#24863;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#31038;&#20250;&#30456;&#20851;&#24212;&#29992;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#31639;&#27861;&#30340;&#26631;&#31614;&#21487;&#33021;&#24456;&#38590;&#25110;&#19981;&#21487;&#33021;&#33719;&#24471;&#12290;&#36825;&#20010;&#25361;&#25112;&#24050;&#32463;&#25512;&#21160;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#36890;&#36807;&#36965;&#24863;&#25968;&#25454;&#35299;&#38145;&#22312;&#26631;&#35760;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#22320;&#29702;&#20301;&#32622;&#25110;&#24212;&#29992;&#39046;&#22495;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20026;&#36965;&#24863;&#25968;&#25454;&#35774;&#35745;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#35757;&#32451;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#26356;&#23567;&#12289;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Remote Sensing Transformer&#65288;Presto&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#23545;&#36965;&#24863;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#21487;&#27604;&#27169;&#22411;&#30456;&#27604;&#65292;Presto&#22312;&#20960;&#20010;&#36965;&#24863;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38656;&#35201;&#25968;&#37327;&#32423;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms for parsing remote sensing data have a wide range of societally relevant applications, but labels used to train these algorithms can be difficult or impossible to acquire. This challenge has spurred research into self-supervised learning for remote sensing data aiming to unlock the use of machine learning in geographies or application domains where labelled datasets are small. Current self-supervised learning approaches for remote sensing data draw significant inspiration from techniques applied to natural images. However, remote sensing data has important differences from natural images -- for example, the temporal dimension is critical for many tasks and data is collected from many complementary sensors. We show that designing models and self-supervised training techniques specifically for remote sensing data results in both smaller and more performant models. We introduce the Pretrained Remote Sensing Transformer (Presto), a transformer-based model pre-tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21463;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36825;&#20010;&#26041;&#27861;&#37319;&#29992;&#33258;&#21160;&#24494;&#20998;&#30340;ODE&#34920;&#36798;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#29615;&#22659;&#65292;&#22312;&#38376;&#25511;&#21046;&#21644;&#27721;&#23494;&#23572;&#39039;&#21442;&#25968;&#30340;&#23398;&#20064;&#20013;&#36890;&#36807;&#31995;&#32479;&#20132;&#20114;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#27604;&#26631;&#20934;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.09718</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26412;&#25928;&#29575;&#30340;&#27169;&#22411;&#39537;&#21160;&#37327;&#23376;&#25511;&#21046;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-efficient Model-based Reinforcement Learning for Quantum Control. (arXiv:2304.09718v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21463;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36825;&#20010;&#26041;&#27861;&#37319;&#29992;&#33258;&#21160;&#24494;&#20998;&#30340;ODE&#34920;&#36798;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#29615;&#22659;&#65292;&#22312;&#38376;&#25511;&#21046;&#21644;&#27721;&#23494;&#23572;&#39039;&#21442;&#25968;&#30340;&#23398;&#20064;&#20013;&#36890;&#36807;&#31995;&#32479;&#20132;&#20114;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#27604;&#26631;&#20934;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20248;&#20110;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;&#25511;&#21046;&#22120;&#19982;&#29289;&#29702;&#31995;&#32479;&#20132;&#20114;&#30340;&#27425;&#25968;&#12290;&#20511;&#21161;&#19968;&#20010;&#24402;&#32435;&#20559;&#32622;&#65292;&#21463;&#26368;&#36817;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#24494;&#30340;ODE&#65292;&#20854;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#36817;&#20284;&#29615;&#22659;&#65292;&#20854;&#26102;&#21464;&#37096;&#20998;&#65288;&#21253;&#25324;&#25511;&#21046;&#65289;&#23436;&#20840;&#24050;&#30693;&#12290;&#25511;&#21046;&#22120;&#21644;&#36830;&#32493;&#26102;&#22495;&#29420;&#31435;&#21442;&#25968;&#30340;&#27721;&#23494;&#23572;&#39039;&#23398;&#20064;&#26159;&#36890;&#36807;&#19982;&#31995;&#32479;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#30340;&#12290;&#22312;&#30495;&#23454;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#22312;&#20934;&#22791;&#19968;&#20123;&#26631;&#20934;&#21333;&#37327;&#23376;&#38376;&#30340;&#38381;&#21512;&#21644;&#24320;&#25918;&#31995;&#32479;&#21160;&#24577;&#26102;&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#19982;&#26631;&#20934;&#27169;&#22411;&#33258;&#30001;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36825;&#21253;&#25324;&#21333;&#27425;&#27979;&#37327;&#12289;&#20219;&#24847;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25130;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model-based reinforcement learning (RL) approach for noisy time-dependent gate optimization with improved sample complexity over model-free RL. Sample complexity is the number of controller interactions with the physical system. Leveraging an inductive bias, inspired by recent advances in neural ordinary differential equations (ODEs), we use an auto-differentiable ODE parametrised by a learnable Hamiltonian ansatz to represent the model approximating the environment whose time-dependent part, including the control, is fully known. Control alongside Hamiltonian learning of continuous time-independent parameters is addressed through interactions with the system. We demonstrate an order of magnitude advantage in the sample complexity of our method over standard model-free RL in preparing some standard unitary gates with closed and open system dynamics, in realistic numerical experiments incorporating single shot measurements, arbitrary Hilbert space truncations and uncertaint
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#30524;&#37096;&#22270;&#20687;&#20013;&#20934;&#30830;&#23450;&#20301;&#35282;&#33180;&#21453;&#23556;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#23545;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#20165;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#19988;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.05673</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#30524;&#37096;&#22270;&#20687;&#20013;&#20934;&#30830;&#23450;&#20301;&#35282;&#33180;&#21453;&#23556;
&lt;/p&gt;
&lt;p&gt;
Precise localization of corneal reflections in eye images using deep learning trained on synthetic data. (arXiv:2304.05673v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05673
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#30524;&#37096;&#22270;&#20687;&#20013;&#20934;&#30830;&#23450;&#20301;&#35282;&#33180;&#21453;&#23556;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#23545;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#20165;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#19988;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#22320;&#23450;&#20301;&#21333;&#20010;&#30524;&#37096;&#22270;&#20687;&#20013;&#35282;&#33180;&#21453;&#23556;&#30340;&#20013;&#24515;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#32431;&#31929;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#20351;&#29992;&#21482;&#26377;&#27169;&#25311;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#22909;&#22788;&#26159;&#23436;&#20840;&#36991;&#24320;&#20102;&#38656;&#35201;&#23545;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#30340;&#32321;&#29712;&#27880;&#37322;&#36807;&#31243;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#25918;&#32622;&#22312;&#19981;&#21516;&#32972;&#26223;&#20013;&#21644;&#23884;&#20837;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20174;&#30495;&#23454;&#30524;&#30555;&#20013;&#25293;&#25668;&#30340;&#39640;&#36136;&#37327;&#35270;&#39057;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#31354;&#38388;&#31934;&#24230;&#26041;&#38754;&#38477;&#20302;&#20102;35&#65285;&#65292;&#24182;&#22312;&#27169;&#25311;&#22270;&#20687;&#26041;&#38754;&#20197;&#31354;&#38388;&#20934;&#30830;&#24615;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#35282;&#33180;&#21453;&#23556;&#20013;&#24515;&#23450;&#20301;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning method for accurately localizing the center of a single corneal reflection (CR) in an eye image. Unlike previous approaches, we use a convolutional neural network (CNN) that was trained solely using simulated data. Using only simulated data has the benefit of completely sidestepping the time-consuming process of manual annotation that is required for supervised training on real eye images. To systematically evaluate the accuracy of our method, we first tested it on images with simulated CRs placed on different backgrounds and embedded in varying levels of noise. Second, we tested the method on high-quality videos captured from real eyes. Our method outperformed state-of-the-art algorithmic methods on real eye images with a 35% reduction in terms of spatial precision, and performed on par with state-of-the-art on simulated images in terms of spatial accuracy.We conclude that our method provides a precise method for CR center localization and provides a solutio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.02858</link><description>&lt;p&gt;
&#38754;&#21521;&#31867;&#21035;&#19981;&#22343;&#38382;&#39064;&#30340;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#32508;&#36848;&#65306;&#32452;&#21512;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65288;CI&#65289;&#26159;&#25351;&#23646;&#20110;&#19968;&#20010;&#31867;&#30340;&#35266;&#27979;&#20540;&#25968;&#37327;&#20302;&#20110;&#20854;&#20182;&#31867;&#30340;&#25968;&#37327;&#12290;&#38598;&#25104;&#23398;&#20064;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#19968;&#20123;&#31574;&#30053;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22686;&#24378;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#26412;&#25991;&#23545;&#29992;&#20110;&#35299;&#20915;&#22522;&#20934;CI&#38382;&#39064;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35745;&#31639;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;CI&#38382;&#39064;&#30340;10&#20010;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;10&#20010;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#25552;&#39640;&#20998;&#31867;&#25928;&#26524;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other classes. Ensemble learning that combines multiple models to obtain a robust model has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, but the true rank of different combinations would require a computational review. In this paper, we present a computational review to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We propose a general framework that evaluates 10 data augmentation and 10 ensemble learning methods for CI problems. Our objective was to identify the most effective combination for improving classificat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#25968;&#25454;&#36755;&#20837;&#24466;&#24351;&#27169;&#22411;&#65292;&#23398;&#20064;&#24182;&#25512;&#26029;&#20986;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#33258;&#23450;&#20041;&#22270;&#20687;&#65292;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2304.00186</link><description>&lt;p&gt;
&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Subject-driven Text-to-Image Generation via Apprenticeship Learning. (arXiv:2304.00186v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#25968;&#25454;&#36755;&#20837;&#24466;&#24351;&#27169;&#22411;&#65292;&#23398;&#20064;&#24182;&#25512;&#26029;&#20986;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#33258;&#23450;&#20041;&#22270;&#20687;&#65292;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;DreamBooth&#65289;&#22312;&#36890;&#36807;&#38024;&#23545;&#30446;&#26631;&#20027;&#39064;&#24494;&#35843;&#8220;&#19987;&#23478;&#27169;&#22411;&#8221;&#65292;&#29983;&#25104;&#39640;&#24230;&#33258;&#23450;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#24456;&#26114;&#36149;&#65292;&#22240;&#20026;&#27599;&#20010;&#20027;&#39064;&#37117;&#24517;&#39035;&#23398;&#20064;&#19968;&#20010;&#26032;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26367;&#20195;&#20027;&#39064;&#29305;&#23450;&#24494;&#35843;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#12290;&#32473;&#23450;&#19968;&#20010;&#26032;&#20027;&#39064;&#30340;&#23569;&#37327;&#28436;&#31034;&#65292;SuTI&#21487;&#20197;&#21363;&#26102;&#29983;&#25104;&#19981;&#21516;&#22330;&#26223;&#20013;&#20027;&#39064;&#30340;&#26032;&#29256;&#26412;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#20027;&#39064;&#29305;&#23450;&#30340;&#20248;&#21270;&#12290;SuTI&#30001;&#8220;&#24466;&#24351;&#23398;&#20064;&#8221;&#39537;&#21160;&#65292;&#20854;&#20013;&#20174;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21333;&#20010;&#30340;&#24466;&#24351;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20114;&#32852;&#32593;&#25366;&#25496;&#20102;&#25968;&#30334;&#19975;&#20010;&#22270;&#20687;&#31751;&#65292;&#27599;&#20010;&#22270;&#20687;&#31751;&#37117;&#32858;&#28966;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#35270;&#35273;&#20027;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#36825;&#20123;&#31751;&#26469;&#35757;&#32451;&#22823;&#37327;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#35270;&#35273;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#24466;&#24351;&#27169;&#22411;&#36890;&#36807;&#25512;&#26029;&#22522;&#20110;&#20854;&#25991;&#26412;&#25551;&#36848;&#30340;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#24182;&#29983;&#25104;&#22270;&#20687;&#26469;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SuTI&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#19981;&#21516;&#20027;&#39064;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#27604;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with \emph{in-context} learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by {\em apprenticeship learning}, where a single apprentice model is learned from data generated by massive amount of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train massive amount of expert models specialized on diff
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;(AI)&#19987;&#23478;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#20915;&#31574;&#25152;&#38656;&#25216;&#33021;&#24182;&#19981;&#20102;&#35299;&#30340;&#38382;&#39064;&#12290; &#34429;&#28982;&#24403;&#21069;&#30340;&#26426;&#22120;&#21487;&#20197;&#27169;&#25311;&#29305;&#23450;&#30340;&#20154;&#31867;&#23646;&#24615;&#65292;&#20294; AGI &#26159;&#22312;&#36825;&#26679;&#30340;&#21069;&#25552;&#19979;&#24320;&#21457;&#20986;&#26469;&#30340;&#65306;&#36825;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#19968;&#33324;&#26234;&#33021;&#27700;&#24179;&#12290;&#36825;&#20250;&#20998;&#25955;&#24403;&#21069;&#30740;&#31350;&#30340;&#27880;&#24847;&#21147;&#65292;&#36828;&#31163;&#30456;&#20851;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00002</link><description>&lt;p&gt;
&#20998;&#26512;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#35821;&#22659;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Contextual Shortcomings of Artificial General Intelligence. (arXiv:2304.00002v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00002
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;(AI)&#19987;&#23478;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#20915;&#31574;&#25152;&#38656;&#25216;&#33021;&#24182;&#19981;&#20102;&#35299;&#30340;&#38382;&#39064;&#12290; &#34429;&#28982;&#24403;&#21069;&#30340;&#26426;&#22120;&#21487;&#20197;&#27169;&#25311;&#29305;&#23450;&#30340;&#20154;&#31867;&#23646;&#24615;&#65292;&#20294; AGI &#26159;&#22312;&#36825;&#26679;&#30340;&#21069;&#25552;&#19979;&#24320;&#21457;&#20986;&#26469;&#30340;&#65306;&#36825;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#19968;&#33324;&#26234;&#33021;&#27700;&#24179;&#12290;&#36825;&#20250;&#20998;&#25955;&#24403;&#21069;&#30740;&#31350;&#30340;&#27880;&#24847;&#21147;&#65292;&#36828;&#31163;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#26159;&#26368;&#23574;&#31471;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#39033;&#30446;&#65292;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#20063;&#21313;&#20998;&#26126;&#26174;&#12290;&#23613;&#31649;&#36825;&#31181;&#24046;&#24322;&#26412;&#36136;&#19978;&#23558;&#27599;&#20010;&#20154;&#30340;&#33021;&#21147;&#21010;&#20998;&#24320;&#26469;&#65292;&#20294;&#20154;&#31867;&#32423;&#21035;&#30340;&#26234;&#33021;(HLI)&#24050;&#32463;&#26159;AGI&#20960;&#21313;&#24180;&#26469;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#21453;&#23545;&#22270;&#28789;&#27979;&#35797;&#30340;&#20108;&#20803;&#35770;&#65292;&#21363;&#23558;&#20854;&#20316;&#20026;&#28508;&#22312;&#26234;&#33021;&#26426;&#22120;&#30340;&#22522;&#30784;&#21644;&#21407;&#22987;&#24314;&#31435;&#30340;&#24847;&#22270;&#12290;&#23427;&#35752;&#35770;&#20102;AI&#19987;&#23478;&#22914;&#20309;&#35823;&#35299;&#27169;&#20223;&#28216;&#25103;&#20316;&#20026;&#23545;&#35745;&#31639;&#26426;&#31995;&#32479;&#36827;&#34892;&#25311;&#20154;&#21270;&#30340;&#25163;&#27573;&#65292;&#24182;&#26029;&#35328;HLI&#26159;&#19968;&#20010;&#36716;&#31227;&#27880;&#24847;&#21147;&#12289;&#20351;&#24403;&#21069;&#30740;&#31350;&#36828;&#31163;&#30456;&#20851;&#38382;&#39064;&#30340;&#38169;&#35823;&#26041;&#21521;&#12290;&#23613;&#31649;&#23545;AGI&#24212;&#29992;&#30340;&#28508;&#22312;&#35774;&#35745;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#21364;&#24456;&#23569;&#32771;&#34385;&#36825;&#26679;&#19968;&#20010;&#31995;&#32479;&#22914;&#20309;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#27700;&#24179;&#35775;&#38382;&#21644;&#25668;&#21462;&#25968;&#25454;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#26426;&#22120;&#21487;&#33021;&#27169;&#25311;&#29305;&#23450;&#30340;&#20154;&#31867;&#23646;&#24615;&#65292;&#20294;AGI&#26159;&#22312;&#36825;&#26679;&#30340;&#21069;&#25552;&#19979;&#24320;&#21457;&#20986;&#26469;&#30340;&#65306;&#36825;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#19968;&#33324;&#26234;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even in the most cutting-edge Artificial General Intelligence (AGI) endeavors, the disparity between humans and artificial systems is extremely apparent. Although this difference fundamentally divides the capabilities of each, human-level intelligence (HLI) has remained the aim of AGI for decades. This paper opposes the binarity of the Turing Test, the foundation of this intention and original establishment of a potentially intelligent machine. It discusses how AI experts misinterpreted the Imitation Game as a means to anthropomorphize computer systems and asserts that HLI is a red herring that distracts current research from relevant problems. Despite the extensive research on the potential design of an AGI application, there has been little consideration of how such a system will access and ingest data at a human-like level. Although current machines may emulate specific human attributes, AGI is developed under the pretense that this can be easily scaled up to a general intelligence 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#23548;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;IL&#36807;&#31243;&#65292;&#24182;&#22312;&#23398;&#20064;&#36866;&#24212;&#24615;&#20301;&#32622;&#21644;&#23039;&#24577;&#25511;&#21046;&#31574;&#30053;&#26041;&#38754;&#36827;&#34892;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15688</link><description>&lt;p&gt;
&#20351;&#29992;Tube MPC&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#39640;&#25928;&#23398;&#20064;&#40065;&#26834;&#24615;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;arXiv:2303.15688v1 [cs.RO]&#65289;
&lt;/p&gt;
&lt;p&gt;
Efficient Deep Learning of Robust, Adaptive Policies using Tube MPC-Guided Data Augmentation. (arXiv:2303.15688v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#23548;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;IL&#36807;&#31243;&#65292;&#24182;&#22312;&#23398;&#20064;&#36866;&#24212;&#24615;&#20301;&#32622;&#21644;&#23039;&#24577;&#25511;&#21046;&#31574;&#30053;&#26041;&#38754;&#36827;&#34892;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#37096;&#32626;&#25935;&#25463;&#33258;&#20027;&#31995;&#32479;&#38656;&#35201;&#36866;&#24212;&#33021;&#21147;&#21644;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;&#29616;&#26377;&#30340;&#40065;&#26834;&#21644;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#65292;&#22914;&#22522;&#20110;MPC&#30340;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#22312;&#22312;&#32447;&#36816;&#34892;&#35745;&#31639;&#37327;&#24222;&#22823;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20986;&#29616;&#20102;&#26377;&#25928;&#22320;&#20174;MPC&#23398;&#20064;&#40065;&#26834;&#19988;&#21487;&#22312;&#26426;&#36733;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#31574;&#30053;&#30340;&#31574;&#30053;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#22522;&#26412;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#29616;&#26377;&#30340;&#39640;&#25928;IL&#31639;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#24615;&#31574;&#30053;&#20174;MPC&#23398;&#20064;&#65292;&#20855;&#26377;&#23398;&#20064;&#36866;&#24212;&#20855;&#26377;&#25361;&#25112;&#24615;&#27169;&#22411;/&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#30340;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23398;&#20064;&#30340;&#20302;&#32500;&#27169;&#22411;/&#29615;&#22659;&#34920;&#31034;&#19978;&#23545;&#31574;&#30053;&#36827;&#34892;&#35843;&#25972;&#65292;&#20174;&#32780;&#20462;&#25913;IL&#36807;&#31243;&#65292;&#36825;&#21487;&#20197;&#22312;&#22312;&#32447;&#29366;&#24577;&#19979;&#39640;&#25928;&#22320;&#20272;&#35745;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23450;&#21046;&#20026;&#23398;&#20064;&#33258;&#36866;&#24212;&#20301;&#32622;&#21644;&#23039;&#24577;&#25511;&#21046;&#31574;&#30053;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24178;&#25200;&#19979;&#36319;&#36394;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of agile autonomous systems in challenging, unstructured environments requires adaptation capabilities and robustness to uncertainties. Existing robust and adaptive controllers, such as the ones based on MPC, can achieve impressive performance at the cost of heavy online onboard computations. Strategies that efficiently learn robust and onboard-deployable policies from MPC have emerged, but they still lack fundamental adaptation capabilities. In this work, we extend an existing efficient IL algorithm for robust policy learning from MPC with the ability to learn policies that adapt to challenging model/environment uncertainties. The key idea of our approach consists in modifying the IL procedure by conditioning the policy on a learned lower-dimensional model/environment representation that can be efficiently estimated online. We tailor our approach to the task of learning an adaptive position and attitude control policy to track trajectories under challenging disturbances
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12206</link><description>&lt;p&gt;
&#34892;&#20026;&#20581;&#24247;&#20010;&#24615;&#21270;&#20171;&#20837;&#30340;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization for Personalized Interventions in Behavioral Health. (arXiv:2303.12206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12206
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#65292;&#36890;&#36807;&#25945;&#32946;&#65292;&#28608;&#21169;&#65292;&#25552;&#37266;&#21644;&#22806;&#23637;&#65292;&#26377;&#26395;&#26174;&#30528;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20171;&#20837;&#20855;&#26377;&#25104;&#26412;&#21644;&#33021;&#21147;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#24739;&#32773;&#20010;&#24615;&#21270;&#20171;&#20837;&#20197;&#26368;&#22823;&#21270;&#26576;&#31181;&#38271;&#26399;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#26041;&#27861;/&#32467;&#26524;&#65306;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26469;&#33258;&#22686;&#24378;&#23398;&#20064;&#25991;&#29486;&#30340;&#36890;&#29992;&#26080;&#27169;&#22411;&#26041;&#27861;&#23545;&#20110;&#21307;&#30103;&#24212;&#29992;&#26469;&#35828;&#36807;&#20110;&#25968;&#25454;&#23494;&#38598;&#65292;&#32780;&#26356;&#31616;&#21333;&#30340;&#36172;&#33218;&#38382;&#39064;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24573;&#30053;&#20102;&#38271;&#26399;&#24739;&#32773;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;DecompPI&#65292;&#23427;&#36817;&#20284;&#20110;&#19968;&#27493;&#25919;&#31574;&#36845;&#20195;&#12290;&#23454;&#29616;DecompPI&#21482;&#38656;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#31181;&#33258;&#28982;&#30340;&#32467;&#26500;&#20551;&#35774;&#19979;&#65292;DecompPI&#21487;&#20197;&#33719;&#24471;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;
Problem definition: Behavioral health interventions, delivered through digital platforms, have the potential to significantly improve health outcomes, through education, motivation, reminders, and outreach. We study the problem of optimizing personalized interventions for patients to maximize some long-term outcome, in a setting where interventions are costly and capacity-constrained.  Methodology/results: This paper provides a model-free approach to solving this problem. We find that generic model-free approaches from the reinforcement learning literature are too data intensive for healthcare applications, while simpler bandit approaches make progress at the expense of ignoring long-term patient dynamics. We present a new algorithm we dub DecompPI that approximates one step of policy iteration. Implementing DecompPI simply consists of a prediction task from offline data, alleviating the need for online experimentation. Theoretically, we show that under a natural set of structural assu
&lt;/p&gt;</description></item><item><title>IFAN&#26159;&#19968;&#20010;&#38754;&#21521;&#20154;&#31867;&#21644;NLP&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20132;&#20114;&#26694;&#26550;&#65292;&#36890;&#36807;&#29992;&#25143;&#30340;&#23454;&#26102;&#21453;&#39304;&#21644;&#36866;&#37197;&#22120;&#23618;&#30340;&#23545;&#40784;&#65292;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#20559;&#35265;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.03124</link><description>&lt;p&gt;
IFAN&#65306;&#38754;&#21521;&#20154;&#31867;&#21644;NLP&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20132;&#20114;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
IFAN: An Explainability-Focused Interaction Framework for Humans and NLP Models. (arXiv:2303.03124v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03124
&lt;/p&gt;
&lt;p&gt;
IFAN&#26159;&#19968;&#20010;&#38754;&#21521;&#20154;&#31867;&#21644;NLP&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20132;&#20114;&#26694;&#26550;&#65292;&#36890;&#36807;&#29992;&#25143;&#30340;&#23454;&#26102;&#21453;&#39304;&#21644;&#36866;&#37197;&#22120;&#23618;&#30340;&#23545;&#40784;&#65292;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#20559;&#35265;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#30417;&#30563;&#26159;&#23558;&#22797;&#26434;NLP&#27169;&#22411;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#22522;&#26412;&#25903;&#26609;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#35299;&#37322;&#24615;&#21644;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#38656;&#35201;&#25216;&#26415;&#29087;&#32451;&#12290;&#23613;&#31649;&#23384;&#22312;&#29992;&#20110;&#27169;&#22411;&#29702;&#35299;&#21644;&#20998;&#26512;&#30340;&#24037;&#20855;&#21253;&#65292;&#20294;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#30340;&#36873;&#39033;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IFAN&#65292;&#19968;&#31181;&#29992;&#20110;&#19982;NLP&#27169;&#22411;&#36827;&#34892;&#23454;&#26102;&#22522;&#20110;&#35299;&#37322;&#30340;&#20132;&#20114;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;IFAN&#30340;&#30028;&#38754;&#65292;&#29992;&#25143;&#21487;&#20197;&#23545;&#36873;&#25321;&#30340;&#27169;&#22411;&#35299;&#37322;&#25552;&#20379;&#21453;&#39304;&#65292;&#28982;&#21518;&#36890;&#36807;&#36866;&#37197;&#22120;&#23618;&#23558;&#20854;&#19982;&#20154;&#31867;&#30340;&#29702;&#24615;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#22312;&#26368;&#23567;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20943;&#36731;&#20559;&#35265;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#21313;&#20998;&#26377;&#25928;&#12290;IFAN&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#30340;&#31649;&#29702;&#31995;&#32479;&#21644;API&#65292;&#29992;&#20110;&#31649;&#29702;&#27169;&#22411;&#65288;&#21644;&#25968;&#25454;&#38598;&#65289;&#20197;&#21450;&#25511;&#21046;&#35775;&#38382;&#26435;&#38480;&#12290;&#28436;&#31034;&#22320;&#22336;&#65306;https://ifan.ml&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability and human oversight are fundamental pillars of deploying complex NLP models into real-world applications. However, applying explainability and human-in-the-loop methods requires technical proficiency. Despite existing toolkits for model understanding and analysis, options to integrate human feedback are still limited. We propose IFAN, a framework for real-time explanation-based interaction with NLP models. Through IFAN's interface, users can provide feedback to selected model explanations, which is then integrated through adapter layers to align the model with human rationale. We show the system to be effective in debiasing a hate speech classifier with minimal impact on performance. IFAN also offers a visual admin system and API to manage models (and datasets) as well as control access rights. A demo is live at https://ifan.ml.
&lt;/p&gt;</description></item><item><title>EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.14838</link><description>&lt;p&gt;
EvoPrompting: &#36866;&#29992;&#20110;&#20195;&#30721;&#32423;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14838
&lt;/p&gt;
&lt;p&gt;
EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#23601;&#65292;&#25105;&#20204;&#25506;&#32034;&#23558;LM&#20316;&#20026;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#30340;&#20351;&#29992;&#12290;&#23613;&#31649;NAS&#20173;&#28982;&#36807;&#20110;&#22256;&#38590;&#65292;&#20197;&#33267;&#20110;&#20165;&#20165;&#36890;&#36807;&#25552;&#31034;&#23601;&#38590;&#20197;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36827;&#21270;&#25552;&#31034;&#24037;&#31243;&#19982;&#36719;&#25552;&#31034;&#35843;&#25972;&#30340;&#32452;&#21512;&#65292;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;EvoPrompting&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#21270;&#19988;&#24615;&#33021;&#39640;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;EvoPrompting&#22312;MNIST-1D&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;EvoPrompting&#20135;&#29983;&#30340;&#21367;&#31215;&#26550;&#26500;&#21464;&#20307;&#22312;&#20934;&#30830;&#29575;&#21644;&#27169;&#22411;&#22823;&#23567;&#26041;&#38754;&#22343;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#35774;&#35745;&#30340;&#26550;&#26500;&#21644;&#22825;&#30495;&#30340;&#23569;&#25968;&#20808;&#23548;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#25628;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;EvoPrompting&#33021;&#22815;&#35774;&#35745;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26356;&#22909;&#30340;&#26032;&#39062;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 ou
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#26500;&#36896;&#24207;&#21015;&#25968;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20844;&#24335;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.13186</link><description>&lt;p&gt;
&#26500;&#36896;&#25968;&#65306;&#22914;&#20309;&#24314;&#31435;&#19968;&#20010;&#22270;&#24418;&#65311;
&lt;/p&gt;
&lt;p&gt;
Construction numbers: How to build a graph?. (arXiv:2302.13186v2 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13186
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#26500;&#36896;&#24207;&#21015;&#25968;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20844;&#24335;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;50&#24180;&#21069;&#65292;&#26031;&#22374;&#21033;&#32771;&#34385;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#12290;&#23545;&#20110;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25105;&#20204;&#31216;&#36825;&#26679;&#30340;&#32447;&#24615;&#25193;&#23637;&#20026;&#22270;&#24418;&#30340;&#8220;&#26500;&#36896;&#24207;&#21015;&#8221;&#65292;&#22240;&#20026;&#27599;&#20010;&#36793;&#37117;&#36981;&#24490;&#20854;&#20004;&#20010;&#31471;&#28857;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#27492;&#31867;&#24207;&#21015;&#25968;&#37327;&#12290;&#23545;&#20110;&#36335;&#24452;&#65292;&#25105;&#20204;&#35748;&#21516;&#26031;&#22374;&#21033;&#30340;&#24819;&#27861;&#65288;&#20999;&#32447;&#25968;&#65289;&#65292;&#24182;&#24471;&#21040;&#20102;&#20854;&#20182;&#31867;&#22411;&#30340;&#20844;&#24335;&#12290;&#27492;&#22806;&#36824;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counting the number of linear extensions of a partial order was considered by Stanley about 50 years ago. For the partial order on the vertices and edges of a graph determined by inclusion, we call such linear extensions {\it construction sequences} for the graph as each edge follows both of its endpoints. The number of such sequences for paths, cycles, stars, double-stars, and complete graphs is found. For paths, we agree with Stanley (the Tangent numbers) and get formulas for the other classes. Structure and applications are also studied.
&lt;/p&gt;</description></item><item><title>HUST&#36724;&#25215;&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;&#29699;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;90&#20010;&#24102;&#26377;6&#31181;&#25925;&#38556;&#31867;&#22411;&#65288;&#20869;&#37096;&#35010;&#32441;&#12289;&#22806;&#37096;&#35010;&#32441;&#12289;&#29699;&#20307;&#35010;&#32441;&#21644;&#23427;&#20204;&#30340;2&#31181;&#32452;&#21512;&#65289;&#30340;5&#31181;&#19981;&#21516;&#31867;&#22411;&#36724;&#25215;&#30340;&#25391;&#21160;&#25968;&#25454;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#20197;&#21450;&#20808;&#36827;&#30340;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#23545;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#20934;&#30830;&#29575;&#21487;&#36798;&#21040;100%&#65292;&#22312;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#20934;&#30830;&#29575;&#20026;60-80%&#12290;</title><link>http://arxiv.org/abs/2302.12533</link><description>&lt;p&gt;
HUST&#36724;&#25215;&#65306;&#19968;&#20010;&#23454;&#29992;&#30340;&#29699;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HUST bearing: a practical dataset for ball bearing fault diagnosis. (arXiv:2302.12533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12533
&lt;/p&gt;
&lt;p&gt;
HUST&#36724;&#25215;&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;&#29699;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;90&#20010;&#24102;&#26377;6&#31181;&#25925;&#38556;&#31867;&#22411;&#65288;&#20869;&#37096;&#35010;&#32441;&#12289;&#22806;&#37096;&#35010;&#32441;&#12289;&#29699;&#20307;&#35010;&#32441;&#21644;&#23427;&#20204;&#30340;2&#31181;&#32452;&#21512;&#65289;&#30340;5&#31181;&#19981;&#21516;&#31867;&#22411;&#36724;&#25215;&#30340;&#25391;&#21160;&#25968;&#25454;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#20197;&#21450;&#20808;&#36827;&#30340;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#23545;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#20934;&#30830;&#29575;&#21487;&#36798;&#21040;100%&#65292;&#22312;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#20934;&#30830;&#29575;&#20026;60-80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HUST&#36724;&#25215;&#30340;&#23454;&#29992;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#32452;&#19981;&#21516;&#29699;&#36724;&#25215;&#30340;&#25391;&#21160;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;5&#31181;&#31867;&#22411;&#36724;&#25215;&#30340;90&#20010;&#21407;&#22987;&#25391;&#21160;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#20869;&#37096;&#35010;&#32441;&#12289;&#22806;&#37096;&#35010;&#32441;&#12289;&#29699;&#20307;&#35010;&#32441;&#20197;&#21450;&#23427;&#20204;&#30340;2&#31181;&#32452;&#21512;&#22312;&#20869;&#30340;6&#31181;&#32570;&#38519;&#31867;&#22411;&#65292;&#20197;&#21450;3&#20010;&#24037;&#20316;&#26465;&#20214;&#19979;&#30340;&#37319;&#26679;&#29575;&#20026;51,200&#27425;/&#31186;&#12290;&#25105;&#20204;&#22312;&#24341;&#20837;&#30340;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#21253;&#32476;&#20998;&#26512;&#21644;&#38454;&#27425;&#36319;&#36394;&#20998;&#26512;&#65292;&#20197;&#36827;&#34892;&#25968;&#25454;&#30340;&#21021;&#27493;&#35780;&#20272;&#12290;&#20351;&#29992;&#19981;&#21516;&#22495;&#20013;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#26469;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#20856;&#22411;&#30340;&#20808;&#36827;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35266;&#23519;&#25968;&#25454;&#38598;&#21508;&#37096;&#20998;&#20043;&#38388;&#30340;&#30693;&#35782;&#21487;&#36801;&#31227;&#24615;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#32463;&#36807;&#23454;&#39564;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;&#36798;&#21040;100%&#30340;&#19981;&#21516;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#38750;&#30417;&#30563;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;60-80%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a practical dataset named HUST bearing, that provides a large set of vibration data on different ball bearings. This dataset contains 90 raw vibration data of 6 types of defects (inner crack, outer crack, ball crack, and their 2-combinations) on 5 types of bearing at 3 working conditions with the sample rate of 51,200 samples per second. We established the envelope analysis and order tracking analysis on the introduced dataset to allow an initial evaluation of the data. A number of classical machine learning classification methods are used to identify bearing faults of the dataset using features in different domains. The typical advanced unsupervised transfer learning algorithms also perform to observe the transferability of knowledge among parts of the dataset. The experimental results of examined methods on the dataset gain divergent accuracy up to 100% on classification task and 60-80% on unsupervised transfer learning task.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26102;&#38388;&#23610;&#24230;&#28201;&#24230;&#32553;&#25918;&#65288;LHTS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#28201;&#24230;&#32553;&#25918;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;LHTS&#21487;&#20197;&#20248;&#21270;&#26679;&#26412;&#30340;&#38271;&#26102;&#38388;&#23610;&#24230;&#20284;&#28982;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21644;&#23383;&#31526;/&#35821;&#35328;&#33258;&#22238;&#24402;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.03686</link><description>&lt;p&gt;
&#38271;&#26102;&#38388;&#23610;&#24230;&#28201;&#24230;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Long Horizon Temperature Scaling. (arXiv:2302.03686v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03686
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26102;&#38388;&#23610;&#24230;&#28201;&#24230;&#32553;&#25918;&#65288;LHTS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#28201;&#24230;&#32553;&#25918;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;LHTS&#21487;&#20197;&#20248;&#21270;&#26679;&#26412;&#30340;&#38271;&#26102;&#38388;&#23610;&#24230;&#20284;&#28982;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21644;&#23383;&#31526;/&#35821;&#35328;&#33258;&#22238;&#24402;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28201;&#24230;&#32553;&#25918;&#26159;&#19968;&#31181;&#35843;&#33410;&#27169;&#22411;&#20998;&#24067;&#38160;&#24230;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#23427;&#24191;&#27867;&#24212;&#29992;&#20110;&#37319;&#26679;&#21487;&#33021;&#30340;&#29983;&#25104;&#29289;&#21644;&#26657;&#20934;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#29978;&#33267;&#22312;&#35768;&#22810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37096;&#32626;&#20013;&#20316;&#20026;&#21487;&#25511;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#20381;&#36182;&#20110;&#36138;&#23146;&#22320;&#20248;&#21270;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#30701;&#35270;&#28201;&#24230;&#32553;&#25918;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#38271;&#26102;&#38388;&#23610;&#24230;&#28201;&#24230;&#32553;&#25918;&#65288;LHTS&#65289;&#65292;&#29992;&#20110;&#20174;&#28201;&#24230;&#32553;&#25918;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;LHTS&#19982;&#25152;&#26377;&#22522;&#20110;&#20284;&#28982;&#30340;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20248;&#21270;&#26679;&#26412;&#30340;&#38271;&#26102;&#38388;&#23610;&#24230;&#20284;&#28982;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#28201;&#24230;&#30456;&#20851;&#30340;LHTS&#30446;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#31995;&#21015;&#28201;&#24230;&#19978;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;&#21487;&#25511;&#38271;&#26102;&#38388;&#23610;&#24230;&#28201;&#24230;&#21442;&#25968;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21644;&#23383;&#31526;/&#35821;&#35328;&#33258;&#22238;&#24402;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;LHTS&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#30456;&#27604;&#20110;&#30701;&#35270;&#28201;&#24230;&#32553;&#25918;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temperature scaling is a popular technique for tuning the sharpness of a model distribution. It is used extensively for sampling likely generations and calibrating model uncertainty, and even features as a controllable parameter to many large language models in deployment. However, autoregressive models rely on myopic temperature scaling that greedily optimizes the next token. To address this, we propose Long Horizon Temperature Scaling (LHTS), a novel approach for sampling from temperature-scaled joint distributions. LHTS is compatible with all likelihood-based models, and optimizes for the long horizon likelihood of samples. We derive a temperature-dependent LHTS objective, and show that finetuning a model on a range of temperatures produces a single model capable of generation with a controllable long horizon temperature parameter. We experiment with LHTS on image diffusion models and character/language autoregressive models, demonstrating advantages over myopic temperature scaling 
&lt;/p&gt;</description></item><item><title>MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11259</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#29983;&#25104;&#19982;&#33258;&#25105;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11259
&lt;/p&gt;
&lt;p&gt;
MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#29983;&#25104;&#24050;&#32463;&#21463;&#21040;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#20854;&#38761;&#26032;&#20102;&#31185;&#23398;&#23478;&#35774;&#35745;&#20998;&#23376;&#32467;&#26500;&#30340;&#26041;&#24335;&#65292;&#24182;&#20026;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20998;&#23376;&#29983;&#25104;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#29983;&#25104;&#35821;&#27861;&#25110;&#21270;&#23398;&#23384;&#22312;&#32570;&#38519;&#30340;&#20998;&#23376;&#65292;&#29421;&#31364;&#30340;&#39046;&#22495;&#19987;&#27880;&#20197;&#21450;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#25110;&#22806;&#37096;&#20998;&#23376;&#25968;&#25454;&#24211;&#32780;&#38480;&#21046;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MolGen&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;MolGen&#36890;&#36807;&#37325;&#26500;&#19968;&#20159;&#22810;&#20010;&#20998;&#23376;SELFIES&#33719;&#24471;&#20102;&#22266;&#26377;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21453;&#39304;&#33539;&#24335;&#65292;&#21551;&#21457;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#26368;&#32456;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#26356;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MolGen&#22312;&#21270;&#23398;&#26377;&#25928;&#24615;&#65292;&#22810;&#26679;&#24615;&#65292;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.05599</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#30701;SSVEP&#25968;&#25454;&#25193;&#23637;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Short-length SSVEP data extension by a novel generative adversarial networks based framework. (arXiv:2301.05599v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;SSVEP&#30340;&#33041;&#26426;&#25509;&#21475;&#22240;&#20854;&#39640;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#21644;&#30446;&#26631;&#25968;&#37327;&#21487;&#29992;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#29992;&#25143;&#26657;&#20934;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#25968;&#25454;&#38271;&#24230;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#21019;&#24314;&#21512;&#25104;&#30340;&#33041;&#30005;&#25968;&#25454;&#65292;&#26377;&#26395;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GANs&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#29992;&#20110;&#25968;&#25454;&#38271;&#24230;&#25193;&#23637;&#12290;TEGAN&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#26032;&#39062;&#30340;U&#22411;&#29983;&#25104;&#22120;&#26550;&#26500;&#21644;&#19968;&#20010;&#36741;&#21161;&#20998;&#31867;&#22120;&#21152;&#20837;&#21040;&#32593;&#32476;&#32467;&#26500;&#20013;&#65292;TEGAN&#21487;&#20197;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#20135;&#29983;&#26377;&#26465;&#20214;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;TEGAN&#29983;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#25152;&#38656;&#30340;&#26657;&#20934;&#26102;&#38388;&#24182;&#25913;&#21892;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steady-state visual evoked potentials (SSVEPs) based brain-computer interface (BCI) has received considerable attention due to its high information transfer rate (ITR) and available quantity of targets. However, the performance of frequency identification methods heavily hinges on the amount of user calibration data and data length, which hinders the deployment in real-world applications. Recently, generative adversarial networks (GANs)-based data generation methods have been widely adopted to create synthetic electroencephalography (EEG) data, holds promise to address these issues. In this paper, we proposed a GAN-based end-to-end signal transformation network for data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP signals into long-length artificial SSVEP signals. By incorporating a novel U-Net generator architecture and an auxiliary classifier into the network architecture, the TEGAN could produce conditioned features in the synthetic data. Additionally, we i
&lt;/p&gt;</description></item><item><title>&#40664;&#40664;&#26432;&#25163;&#26159;&#19968;&#31181;&#38544;&#34109;&#30340;&#12289;&#26080;&#26631;&#31614;&#30340;&#12289;&#40657;&#30418;&#23376;&#21518;&#38376;&#25915;&#20987;&#65292;&#23427;&#20351;&#29992;&#20102;&#38544;&#34109;&#30340;&#27602;&#29289;&#21644;&#35302;&#21457;&#22120;&#65292;&#22312;&#26080;&#26631;&#31614;&#25915;&#20987;&#20013;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#36890;&#36807;&#28176;&#21464;&#23545;&#40784;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;MNIST&#12289;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.02615</link><description>&lt;p&gt;
&#40664;&#40664;&#26432;&#25163;: &#19968;&#31181;&#38544;&#34109;&#30340;&#12289;&#26080;&#26631;&#31614;&#30340;&#12289;&#40657;&#30418;&#23376;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack. (arXiv:2301.02615v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02615
&lt;/p&gt;
&lt;p&gt;
&#40664;&#40664;&#26432;&#25163;&#26159;&#19968;&#31181;&#38544;&#34109;&#30340;&#12289;&#26080;&#26631;&#31614;&#30340;&#12289;&#40657;&#30418;&#23376;&#21518;&#38376;&#25915;&#20987;&#65292;&#23427;&#20351;&#29992;&#20102;&#38544;&#34109;&#30340;&#27602;&#29289;&#21644;&#35302;&#21457;&#22120;&#65292;&#22312;&#26080;&#26631;&#31614;&#25915;&#20987;&#20013;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#36890;&#36807;&#28176;&#21464;&#23545;&#40784;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;MNIST&#12289;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#27745;&#26579;&#25915;&#20987;&#23545;&#31070;&#32463;&#32593;&#32476;&#26500;&#25104;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#23485;&#26494;&#30340;&#23041;&#32961;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#40664;&#40664;&#26432;&#25163;&#30340;&#26032;&#22411;&#25915;&#20987;&#65292;&#22312;&#26080;&#26631;&#31614;&#30340;&#40657;&#30418;&#23376;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#20351;&#29992;&#38544;&#34109;&#30340;&#27602;&#29289;&#21644;&#35302;&#21457;&#22120;&#65292;&#24182;&#19988;&#32988;&#36807;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#26631;&#31614;&#25915;&#20987;&#20013;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#20316;&#20026;&#35302;&#21457;&#22120;&#30340;&#26041;&#27861;&#65292;&#22312;&#27602;&#26631;&#31614;&#35774;&#32622;&#19979;&#30340;&#25104;&#21151;&#26696;&#20363;&#20043;&#21518;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#22825;&#30495;&#30340;&#36866;&#24212;&#26041;&#27861;&#30340;&#25104;&#21151;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#38656;&#35201;&#28176;&#21464;&#23545;&#40784;&#20197;&#30830;&#20445;&#39640;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#23545;MNIST&#12289;CIFAR10&#21644;&#19968;&#20010;&#32553;&#23567;&#29256;&#30340;ImageNet&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor poisoning attacks pose a well-known risk to neural networks. However, most studies have focused on lenient threat models. We introduce Silent Killer, a novel attack that operates in clean-label, black-box settings, uses a stealthy poison and trigger and outperforms existing methods. We investigate the use of universal adversarial perturbations as triggers in clean-label attacks, following the success of such approaches under poison-label settings. We analyze the success of a naive adaptation and find that gradient alignment for crafting the poison is required to ensure high success rates. We conduct thorough experiments on MNIST, CIFAR10, and a reduced version of ImageNet and achieve state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#36807;&#31243;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#21462;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#36890;&#29992;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#36739;&#20026;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.01913</link><description>&lt;p&gt;
&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#23398;&#20064;&#36890;&#29992;&#30340;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver. (arXiv:2301.01913v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#36807;&#31243;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#21462;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#36890;&#29992;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#36739;&#20026;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#35268;&#21010;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#12290;&#27714;&#35299;&#22120;&#20013;&#30340;&#37325;&#35201;&#35774;&#35745;&#36873;&#25321;&#26159;&#20998;&#25903;&#21551;&#21457;&#24335;&#65292;&#23427;&#20204;&#26088;&#22312;&#22312;&#26368;&#30701;&#30340;&#26102;&#38388;&#20869;&#23547;&#25214;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#20123;&#21551;&#21457;&#24335;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#24182;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#28608;&#21457;&#20102;&#35768;&#22810;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#23398;&#20064;&#39640;&#25928;&#21551;&#21457;&#24335;&#30340;&#21162;&#21147;&#65292;&#32780;&#26080;&#38656;&#19987;&#23478;&#24178;&#39044;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#26377;&#20960;&#31181;&#36890;&#29992;&#30340;&#21464;&#37327;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#36890;&#29992;&#30340;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#36873;&#25321;&#21364;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#36807;&#31243;&#21487;&#20197;&#29992;&#20110;&#22312;&#32422;&#26463;&#35268;&#21010;&#27714;&#35299;&#22120;&#20869;&#33719;&#24471;&#19968;&#20010;&#20540;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#36825;&#24471;&#30410;&#20110;&#28145;&#24230;Q&#23398;&#20064;&#31639;&#27861;&#21644;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Constraint programming is known for being an efficient approach for solving combinatorial problems. Important design choices in a solver are the branching heuristics, which are designed to lead the search to the best solutions in a minimum amount of time. However, developing these heuristics is a time-consuming process that requires problem-specific expertise. This observation has motivated many efforts to use machine learning to automatically learn efficient heuristics without expert intervention. To the best of our knowledge, it is still an open research question. Although several generic variable-selection heuristics are available in the literature, the options for a generic value-selection heuristic are more scarce. In this paper, we propose to tackle this issue by introducing a generic learning procedure that can be used to obtain a value-selection heuristic inside a constraint programming solver. This has been achieved thanks to the combination of a deep Q-learning algorithm, a t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#27491;&#30830;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#36991;&#20813;&#27425;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.13723</link><description>&lt;p&gt;
&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#26469;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Multi-task Learning via Seeking Task-based Flat Regions. (arXiv:2211.13723v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13723
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#27491;&#30830;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#36991;&#20813;&#27425;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#19988;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#39592;&#24178;&#23398;&#20064;&#22810;&#20010;&#30446;&#26631;&#12290;&#19982;&#21333;&#29420;&#35757;&#32451;&#20219;&#21153;&#30456;&#27604;&#65292;MTL&#26174;&#30528;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#26469;&#28508;&#22312;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#65292;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#12290;&#20854;&#20013;&#65292;MTL&#30340;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#38598;&#20013;&#22312;&#25805;&#32437;&#20219;&#21153;&#26799;&#24230;&#20197;&#25512;&#23548;&#20986;&#23545;&#25152;&#26377;&#20219;&#21153;&#26377;&#30410;&#30340;&#26368;&#32456;&#26799;&#24230;&#19979;&#38477;&#26041;&#21521;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#38382;&#39064;&#19978;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#32780;&#19981;&#20351;&#29992;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#26631;&#20934;&#35757;&#32451;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#26368;&#23567;&#21270;&#32463;&#39564;&#25439;&#22833;&#65292;&#24456;&#23481;&#26131;&#36973;&#21463;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning (MTL) is a widely-used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions on real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;L-MAE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#23436;&#25972;&#30340;&#35821;&#20041;&#20998;&#21106;&#26631;&#31614;&#12290;&#35813;&#26041;&#27861;&#39318;&#27425;&#23558;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#20102;&#26631;&#31614;&#21644;&#22270;&#20687;&#30340;&#34701;&#21512;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#22823;&#22411;&#27169;&#22411;&#21644;&#19987;&#19994;&#39046;&#22495;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#26631;&#27880;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.11242</link><description>&lt;p&gt;
L-MAE&#65306;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
L-MAE: Masked Autoencoders are Semantic Segmentation Datasets Augmenter. (arXiv:2211.11242v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;L-MAE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#23436;&#25972;&#30340;&#35821;&#20041;&#20998;&#21106;&#26631;&#31614;&#12290;&#35813;&#26041;&#27861;&#39318;&#27425;&#23558;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#20102;&#26631;&#31614;&#21644;&#22270;&#20687;&#30340;&#34701;&#21512;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#22823;&#22411;&#27169;&#22411;&#21644;&#19987;&#19994;&#39046;&#22495;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#26631;&#27880;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#22411;&#27169;&#22411;&#25110;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#24433;&#20687;&#25110;&#36965;&#24863;&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#32791;&#26102;&#36153;&#21147;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22823;&#22411;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#32780;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#38656;&#35201;&#39046;&#22495;&#19987;&#23478;&#30340;&#21442;&#19982;&#12290;&#36825;&#20004;&#31181;&#24773;&#20917;&#37117;&#23481;&#26131;&#23548;&#33268;&#25968;&#25454;&#26631;&#27880;&#30340;&#19981;&#20934;&#30830;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#20687;&#32032;&#32423;&#26631;&#31614;&#23436;&#25104;&#26041;&#27861;&#65292;&#21363;L-MAE&#65288;&#26631;&#31614;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#23427;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#20013;&#30340;&#29616;&#26377;&#20449;&#24687;&#29983;&#25104;&#23436;&#25972;&#30340;&#26631;&#31614;&#12290;&#35813;&#25991;&#39318;&#27425;&#23558;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;L-MAE&#37319;&#29992;&#20102;&#22534;&#21472;&#26631;&#31614;&#21644;&#30456;&#24212;&#22270;&#20687;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#21363;&#34701;&#21512;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#36974;&#32617;&#34701;&#21512;&#26144;&#23556;&#26102;&#20250;&#20002;&#22833;&#19968;&#20123;&#22270;&#20687;&#20449;&#24687;&#65292;&#22240;&#27492;&#35813;&#26041;&#27861;&#30452;&#25509;&#36827;&#34892;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating semantic segmentation datasets has consistently been laborious and time-consuming, particularly in the context of large models or specialized domains(i.e. Medical Imaging or Remote Sensing). Specifically, large models necessitate a substantial volume of data, while datasets in professional domains frequently require the involvement of domain experts. Both scenarios are susceptible to inaccurate data labeling, which can significantly affect the ultimate performance of the trained model. This paper proposes a simple and effective label pixel-level completion method, \textbf{Label Mask AutoEncoder} (L-MAE), which fully uses the existing information in the label to generate the complete label. The proposed model are the first to apply the Mask Auto-Encoder to downstream tasks. In detail, L-MAE adopts the fusion strategy that stacks the label and the corresponding image, namely fuse map. Moreover, since some of the image information is lost when masking the fuse map, direct recon
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;&#65288;HELM&#65289;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#22330;&#26223;&#21644;&#24230;&#37327;&#36827;&#34892;&#20998;&#31867;&#24182;&#37319;&#29992;&#22810;&#24230;&#37327;&#26041;&#27861;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.09110</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Holistic Evaluation of Language Models. (arXiv:2211.09110v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09110
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;&#65288;HELM&#65289;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#22330;&#26223;&#21644;&#24230;&#37327;&#36827;&#34892;&#20998;&#31867;&#24182;&#37319;&#29992;&#22810;&#24230;&#37327;&#26041;&#27861;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#27491;&#22312;&#25104;&#20026;&#20960;&#20046;&#25152;&#26377;&#20027;&#35201;&#35821;&#35328;&#25216;&#26415;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#38480;&#21046;&#21644;&#39118;&#38505;&#24182;&#19981;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;&#65288;HELM&#65289;&#65292;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#24863;&#20852;&#36259;&#30340;&#28508;&#22312;&#22330;&#26223;&#65288;&#21363;&#29992;&#20363;&#65289;&#21644;&#24230;&#37327;&#65288;&#21363;&#26399;&#26395;&#65289;&#30340;&#24191;&#38420;&#31354;&#38388;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#23485;&#27867;&#30340;&#23376;&#38598;&#65292;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#21644;&#21487;&#34892;&#24615;&#65292;&#27880;&#24847;&#21040;&#20102;&#32570;&#22833;&#25110;&#26410;&#20805;&#20998;&#20195;&#34920;&#30340;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#20026;&#34987;&#24573;&#35270;&#30340;&#33521;&#35821;&#26041;&#35328;&#36827;&#34892;&#38382;&#31572;&#65292;&#29992;&#20110;&#21487;&#20449;&#24230;&#30340;&#24230;&#37327;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#24230;&#37327;&#26041;&#27861;&#65306;&#25105;&#20204;&#20998;&#21035;&#38024;&#23545;&#27599;&#20010;&#26680;&#24515;&#22330;&#26223;&#27979;&#37327;&#20102;&#20934;&#30830;&#24230;&#12289;&#26657;&#20934;&#24230;&#12289;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#26377;&#27602;&#24615;&#21644;&#25928;&#29575;&#36825;7&#20010;&#24230;&#37327;&#25351;&#26631;&#65288;&#22312;87.5%&#30340;&#26102;&#38388;&#20869;&#65289;&#12290;&#36825;&#30830;&#20445;&#20102;&#20934;&#30830;&#24230;&#20197;&#22806;&#30340;&#24230;&#37327;&#19981;&#20250;&#34987;&#24573;&#35270;&#65292;&#24182;&#19988;&#26435;&#34913;&#28165;&#26224;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;7&#20010;&#38024;&#23545;&#24615;&#35780;&#20272;&#65292;&#22522;&#20110;26&#20010;&#38024;&#23545;&#24615;&#22330;&#26223;&#65292;&#20197;&#20998;&#26512;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze speci
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#26368;&#36817;&#21457;&#24067;&#30340;&#33016;&#37096;X&#20809;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#20559;&#20506;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#22312;&#29983;&#29289;&#24615;&#21035;&#21644;&#31181;&#26063;&#20043;&#38388;&#23384;&#22312;&#20122;&#32452;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2209.02965</link><description>&lt;p&gt;
&#33016;&#37096;X&#20809;&#28145;&#24230;&#23398;&#20064;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#20559;&#20506;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Risk of Bias in Chest Radiography Deep Learning Foundation Models. (arXiv:2209.02965v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#26368;&#36817;&#21457;&#24067;&#30340;&#33016;&#37096;X&#20809;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#20559;&#20506;&#39118;&#38505;&#65292;&#24182;&#21457;&#29616;&#22312;&#29983;&#29289;&#24615;&#21035;&#21644;&#31181;&#26063;&#20043;&#38388;&#23384;&#22312;&#20122;&#32452;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#20998;&#26512;&#26368;&#36817;&#21457;&#24067;&#30340;&#33016;&#37096;X&#20809;&#22522;&#30784;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#20559;&#20506;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#29983;&#29289;&#24615;&#21035;&#21644;&#31181;&#26063;&#20043;&#38388;&#23384;&#22312;&#20122;&#32452;&#24615;&#33021;&#24046;&#36317;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#26412;&#22238;&#39038;&#24615;&#30740;&#31350;&#20351;&#29992;CheXpert&#25968;&#25454;&#38598;&#20013;&#33258;2002&#24180;10&#26376;&#33267;2017&#24180;7&#26376;&#26399;&#38388;&#25910;&#38598;&#30340;42,884&#21517;&#24739;&#32773;&#65288;&#24180;&#40836;&#24179;&#22343;&#20026;63&#23681;&#65292;&#26631;&#20934;&#20559;&#24046;&#20026;17&#23681;&#65307;&#30007;&#24615;23,623&#20154;&#65292;&#22899;&#24615;19,261&#20154;&#65289;&#30340;127,118&#24352;&#33016;&#37096;X&#20809;&#12290;&#20351;&#29992;&#38477;&#32500;&#26041;&#27861;&#21644;&#20004;&#26679;&#26412;Kolmogorov-Smirnov&#26816;&#39564;&#26816;&#27979;&#33016;&#37096;X&#20809;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#30784;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#30340;&#29305;&#24449;&#20013;&#30340;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#30830;&#23450;&#26159;&#21542;&#23384;&#22312;&#20559;&#20506;&#12290;&#28982;&#21518;&#36827;&#34892;&#20840;&#38754;&#30340;&#30142;&#30149;&#26816;&#27979;&#24615;&#33021;&#20998;&#26512;&#65292;&#23558;&#29305;&#24449;&#20013;&#30340;&#20219;&#20309;&#20559;&#20506;&#19982;&#24739;&#32773;&#20122;&#32452;&#30340;&#20998;&#31867;&#24615;&#33021;&#24046;&#24322;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To analyze a recently published chest radiography foundation model for the presence of biases that could lead to subgroup performance disparities across biological sex and race.  Materials and Methods: This retrospective study used 127,118 chest radiographs from 42,884 patients (mean age, 63 [SD] 17 years; 23,623 male, 19,261 female) from the CheXpert dataset collected between October 2002 and July 2017. To determine the presence of bias in features generated by a chest radiography foundation model and baseline deep learning model, dimensionality reduction methods together with two-sample Kolmogorov-Smirnov tests were used to detect distribution shifts across sex and race. A comprehensive disease detection performance analysis was then performed to associate any biases in the features to specific disparities in classification performance across patient subgroups.  Results: Ten out of twelve pairwise comparisons across biological sex and race showed statistically significant di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#27169;&#24335;&#33021;&#22815;&#35299;&#20915;&#20197;&#33258;&#21033;&#20026;&#23548;&#21521;&#30340;&#32456;&#36523;&#23398;&#20064;&#20010;&#20307;&#20013;&#30340;&#20998;&#24037;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.05568</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#65292;&#20998;&#24037;&#30340;&#24418;&#25104;
&lt;/p&gt;
&lt;p&gt;
The emergence of division of labor through decentralized social sanctioning. (arXiv:2208.05568v4 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#27169;&#24335;&#33021;&#22815;&#35299;&#20915;&#20197;&#33258;&#21033;&#20026;&#23548;&#21521;&#30340;&#32456;&#36523;&#23398;&#20064;&#20010;&#20307;&#20013;&#30340;&#20998;&#24037;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29983;&#24577;&#25104;&#21151;&#20381;&#36182;&#20110;&#25105;&#20204;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21363;&#28789;&#27963;&#33258;&#32452;&#32455;&#25104;&#21512;&#20316;&#31038;&#20250;&#32676;&#20307;&#65292;&#20854;&#20013;&#26368;&#25104;&#21151;&#30340;&#32676;&#20307;&#37319;&#29992;&#20102;&#22823;&#37327;&#30340;&#19987;&#19994;&#21270;&#21644;&#20998;&#24037;&#12290;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#21160;&#29289;&#19981;&#21516;&#65292;&#20154;&#31867;&#36890;&#36807;&#19968;&#29983;&#30340;&#35797;&#38169;&#20013;&#23398;&#20064;&#33258;&#24049;&#35201;&#25198;&#28436;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#20851;&#38190;&#35282;&#33394;&#27604;&#20854;&#20182;&#35282;&#33394;&#26356;&#20855;&#21560;&#24341;&#21147;&#65292;&#24182;&#19988;&#20010;&#20307;&#26159;&#33258;&#21033;&#30340;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#31038;&#20250;&#22256;&#22659;&#65306;&#27599;&#20010;&#20010;&#20307;&#37117;&#24076;&#26395;&#20854;&#20182;&#20154;&#25198;&#28436;&#20851;&#38190;&#20294;&#26080;&#25253;&#37228;&#30340;&#35282;&#33394;&#65292;&#36825;&#26679;&#20182;&#20204;&#21487;&#20197;&#33258;&#30001;&#36873;&#25321;&#19968;&#20010;&#25253;&#37228;&#26356;&#39640;&#30340;&#35282;&#33394;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#27599;&#20010;&#20154;&#37117;&#36825;&#26679;&#34892;&#20107;&#65292;&#19988;&#19968;&#20010;&#20851;&#38190;&#35282;&#33394;&#32570;&#20047;&#22635;&#34917;&#65292;&#23601;&#20250;&#21457;&#29983;&#28798;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#26368;&#20339;&#35282;&#33394;&#20998;&#37197;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65306;&#22914;&#20309;&#22312;&#19968;&#32676;&#20197;&#33258;&#21033;&#20026;&#23548;&#21521;&#30340;&#32456;&#36523;&#23398;&#20064;&#20010;&#20307;&#20013;&#24418;&#25104;&#20998;&#24037;&#21602;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#27169;&#22411;&#65288;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#27169;&#24335;&#65289;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human ecological success relies on our characteristic ability to flexibly self-organize into cooperative social groups, the most successful of which employ substantial specialization and division of labor. Unlike most other animals, humans learn by trial and error during their lives what role to take on. However, when some critical roles are more attractive than others, and individuals are self-interested, then there is a social dilemma: each individual would prefer others take on the critical-but-unremunerative roles so they may remain free to take one that pays better. But disaster occurs if all act thusly and a critical role goes unfilled. In such situations learning an optimum role distribution may not be possible. Consequently, a fundamental question is: how can division of labor emerge in groups of self-interested lifetime-learning individuals? Here we show that by introducing a model of social norms, which we regard as emerging patterns of decentralized social sanctioning, it be
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#31181;&#32676;&#35268;&#27169;&#36275;&#22815;&#22823;&#26102;&#65292;NSGA-II&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#23436;&#25972;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#20294;&#24403;&#31181;&#32676;&#35268;&#27169;&#36739;&#23567;&#26102;&#65292;NSGA-II&#23545;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36924;&#36817;&#25928;&#26524;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#23384;&#22312;&#36739;&#22823;&#30340;&#38388;&#38553;&#12290;&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.02693</link><description>&lt;p&gt;
NSGA-II&#30340;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;II&#30340;&#36817;&#20284;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Approximation Guarantees for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II). (arXiv:2203.02693v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02693
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#31181;&#32676;&#35268;&#27169;&#36275;&#22815;&#22823;&#26102;&#65292;NSGA-II&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#23436;&#25972;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#20294;&#24403;&#31181;&#32676;&#35268;&#27169;&#36739;&#23567;&#26102;&#65292;NSGA-II&#23545;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36924;&#36817;&#25928;&#26524;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#23384;&#22312;&#36739;&#22823;&#30340;&#38388;&#38553;&#12290;&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#31181;&#32676;&#35268;&#27169;&#36275;&#22815;&#22823;&#26102;&#65292;NSGA-II&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#23436;&#25972;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#31181;&#32676;&#35268;&#27169;&#36739;&#23567;&#26102;&#65292;&#23427;&#23545;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36924;&#36817;&#31243;&#24230;&#12290;&#23545;&#20110;OneMinMax&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#25351;&#20986;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29238;&#20195;&#21644;&#21518;&#20195;&#24456;&#22909;&#22320;&#35206;&#30422;&#20102;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#20294;&#26159;&#19979;&#19968;&#20195;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#23384;&#22312;&#36739;&#22823;&#30340;&#38388;&#38553;&#12290;&#25105;&#20204;&#30340;&#25968;&#23398;&#35777;&#26126;&#34920;&#26126;&#65292;&#36896;&#25104;&#36825;&#31181;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#30340;&#21407;&#22240;&#26159;NSGA-II&#22312;&#36873;&#25321;&#38454;&#27573;&#21482;&#35745;&#31639;&#20102;&#25317;&#25380;&#36317;&#31163;&#19968;&#27425;&#65292;&#28982;&#21518;&#21024;&#38500;&#20855;&#26377;&#26368;&#23567;&#25317;&#25380;&#36317;&#31163;&#30340;&#20010;&#20307;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#21024;&#38500;&#20250;&#22686;&#21152;&#26576;&#20123;&#20010;&#20307;&#30340;&#25317;&#25380;&#36317;&#31163;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#19981;&#23481;&#26131;&#20986;&#29616;&#36825;&#20010;&#38382;&#39064;&#30340;&#21464;&#20307;&#12290;&#23545;&#20110;&#22312;&#27599;&#27425;&#21024;&#38500;&#21518;&#26356;&#26032;&#25317;&#25380;&#36317;&#31163;&#30340;NSGA-II&#65288;Kukkonen and Deb&#65288;2006&#65289;&#65289;&#21644;&#31283;&#24577;NSGA-II&#65288;Nebro and Durillo&#65288;2009&#65289;&#65289;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#38388;&#38553;&#27704;&#36828;&#19981;&#20250;&#36229;&#36807;&#19968;&#20010;&#24456;&#23567;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical works have shown that the NSGA-II efficiently computes the full Pareto front when the population size is large enough. In this work, we study how well it approximates the Pareto front when the population size is smaller.  For the OneMinMax benchmark, we point out situations in which the parents and offspring cover well the Pareto front, but the next population has large gaps on the Pareto front. Our mathematical proofs suggest as reason for this undesirable behavior that the NSGA-II in the selection stage computes the crowding distance once and then removes individuals with smallest crowding distance without considering that a removal increases the crowding distance of some individuals.  We then analyze two variants not prone to this problem. For the NSGA-II that updates the crowding distance after each removal (Kukkonen and Deb (2006)) and the steady-state NSGA-II (Nebro and Durillo (2009)), we prove that the gaps in the Pareto front are never more than a small cons
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#24418;&#29366;&#20248;&#21270;&#65292;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24037;&#19994;&#35774;&#35745;&#24615;&#33021;&#26102;&#65292;&#35299;&#20915;&#20102;&#24418;&#29366;&#20559;&#31163;&#35757;&#32451;&#38598;&#26102;&#39044;&#27979;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#39640;&#20102;&#32467;&#26524;&#24418;&#29366;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2109.13337</link><description>&lt;p&gt;
DEBOSH: &#28145;&#24230;&#36125;&#21494;&#26031;&#24418;&#29366;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DEBOSH: Deep Bayesian Shape Optimization. (arXiv:2109.13337v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.13337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#24418;&#29366;&#20248;&#21270;&#65292;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24037;&#19994;&#35774;&#35745;&#24615;&#33021;&#26102;&#65292;&#35299;&#20915;&#20102;&#24418;&#29366;&#20559;&#31163;&#35757;&#32451;&#38598;&#26102;&#39044;&#27979;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#39640;&#20102;&#32467;&#26524;&#24418;&#29366;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21487;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#39044;&#27979;&#24037;&#19994;&#35774;&#35745;&#30340;&#24615;&#33021;&#65292;&#24182;&#29992;&#20110;&#26377;&#25928;&#20248;&#21270;&#20854;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20805;&#20998;&#25506;&#32034;&#24418;&#29366;&#31354;&#38388;&#65292;&#36890;&#24120;&#38656;&#35201;&#32771;&#34385;&#19982;&#35757;&#32451;&#38598;&#26126;&#26174;&#20559;&#31163;&#30340;&#24418;&#29366;&#12290;&#23545;&#20110;&#36825;&#20123;&#24773;&#20917;&#65292;GNN&#30340;&#39044;&#27979;&#21464;&#24471;&#19981;&#21487;&#38752;&#65292;&#20294;&#36825;&#36890;&#24120;&#34987;&#24573;&#35270;&#12290;&#38024;&#23545;&#20381;&#36182;&#39640;&#26031;&#36807;&#31243;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36890;&#36807;&#21033;&#29992;&#20854;&#35780;&#20272;&#33258;&#36523;&#31934;&#24230;&#30340;&#33021;&#21147;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20272;&#35745;&#20854;&#19981;&#30830;&#23450;&#24615;&#30340;&#26631;&#20934;&#26041;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#35745;&#31639;&#37327;&#22823;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24418;&#29366;&#20248;&#21270;&#30340;&#26032;&#39062;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#23427;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;BO&#65292;&#24182;&#25552;&#39640;&#20102;&#32467;&#26524;&#24418;&#29366;&#30340;&#36136;&#37327;&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) can predict the performance of an industrial design quickly and accurately and be used to optimize its shape effectively. However, to fully explore the shape space, one must often consider shapes deviating significantly from the training set. For these, GNN predictions become unreliable, something that is often ignored. For optimization techniques relying on Gaussian Processes, Bayesian Optimization (BO) addresses this issue by exploiting their ability to assess their own accuracy. Unfortunately, this is harder to do when using neural networks because standard approaches to estimating their uncertainty can entail high computational loads and reduced model accuracy. Hence, we propose a novel uncertainty-based method tailored to shape optimization. It enables effective BO and increases the quality of the resulting shapes beyond that of state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#20174;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#25552;&#28860;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#65292;&#26088;&#22312;&#24341;&#39046;&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#24212;&#29992;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2109.09658</link><description>&lt;p&gt;
FUTURE-AI:&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#20849;&#35782;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. (arXiv:2109.09658v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.09658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#20174;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#25552;&#28860;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#65292;&#26088;&#22312;&#24341;&#39046;&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#24212;&#29992;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#20020;&#24202;&#31995;&#32479;&#29983;&#25104;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#25512;&#21160;&#20102;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#25972;&#20010;&#20215;&#20540;&#38142;&#19978;&#30340;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#22270;&#20687;&#37325;&#24314;&#12289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#12290;&#23613;&#31649;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#25104;&#21151;&#24182;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#35768;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#25285;&#24515;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#35748;&#20026;&#20854;&#22797;&#26434;&#12289;&#19981;&#36879;&#26126;&#12289;&#38590;&#20197;&#29702;&#35299;&#12289;&#38590;&#20197;&#24212;&#29992;&#21644;&#38590;&#20197;&#22312;&#20851;&#38190;&#20020;&#24202;&#24212;&#29992;&#20013;&#24314;&#31435;&#20449;&#20219;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#25285;&#24551;&#21644;&#39118;&#38505;&#65292;&#20294;&#30446;&#21069;&#23578;&#27809;&#26377;&#20855;&#20307;&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#26469;&#24341;&#23548;&#26410;&#26469;&#21307;&#23398;&#24433;&#20687;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20197;&#22686;&#21152;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#37319;&#29992;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#31215;&#32047;&#30340;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#31934;&#36873;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today's clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2106.02626</link><description>&lt;p&gt;
&#32422;&#26463;&#36164;&#28304;&#19979;&#31070;&#32463;&#27169;&#22359;&#19987;&#19994;&#21270;&#30340;&#21160;&#21147;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#33041;&#22312;&#32467;&#26500;&#21644;&#21151;&#33021;&#19978;&#39640;&#24230;&#27169;&#22359;&#21270;&#65292;&#20294;&#26368;&#36817;&#30340;&#35777;&#25454;&#20351;&#19968;&#20123;&#20154;&#23545;&#20004;&#31181;&#27169;&#22359;&#21270;&#30340;&#31243;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#27979;&#35797;&#32467;&#26500;&#27169;&#22359;&#21270;&#26159;&#21542;&#36275;&#20197;&#20445;&#35777;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#21457;&#29616;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#65292;&#38500;&#38750;&#22312;&#26497;&#31471;&#27700;&#24179;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#29615;&#22659;&#21644;&#32593;&#32476;&#30340;&#21738;&#20123;&#29305;&#24449;&#20250;&#23548;&#33268;&#19987;&#19994;&#21270;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29609;&#20855;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#32593;&#32476;&#65292;&#20197;&#31934;&#30830;&#25511;&#21046;&#26465;&#20214;&#65292;&#24182;&#34920;&#26126;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20960;&#20010;&#19981;&#21516;&#30340;&#19987;&#19994;&#21270;&#24230;&#37327;&#25351;&#26631;&#32473;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19987;&#19994;&#21270;&#21482;&#33021;&#22312;&#29615;&#22659;&#20013;&#37027;&#20123;&#21487;&#20197;&#26126;&#30830;&#20998;&#31163;&#30340;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;2&#65289;&#19987;&#19994;&#21270;&#26356;&#23481;&#26131;&#22312;&#32593;&#32476;&#36164;&#28304;&#21463;&#21040;&#24378;&#28872;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;3&#65289;&#36825;&#20123;&#21457;&#29616;&#22312; qualitatively &#19978;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold except at extreme levels. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar ac
&lt;/p&gt;</description></item><item><title>iCORPP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#21516;&#26102;&#25512;&#29702;&#19990;&#30028;&#29366;&#24577;&#12289;&#21160;&#24577;&#21644;&#26500;&#24314;&#20219;&#21153;&#23548;&#21521;&#30340;&#25511;&#21046;&#22120;&#12290;</title><link>http://arxiv.org/abs/2004.08672</link><description>&lt;p&gt;
iCORPP: &#26426;&#22120;&#20154;&#19978;&#30340;&#20132;&#26367;&#24120;&#35782;&#25512;&#29702;&#19982;&#27010;&#29575;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
iCORPP: Interleaved Commonsense Reasoning and Probabilistic Planning on Robots. (arXiv:2004.08672v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.08672
&lt;/p&gt;
&lt;p&gt;
iCORPP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#21516;&#26102;&#25512;&#29702;&#19990;&#30028;&#29366;&#24577;&#12289;&#21160;&#24577;&#21644;&#26500;&#24314;&#20219;&#21153;&#23548;&#21521;&#30340;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#26426;&#22120;&#20154;&#30340;&#39034;&#24207;&#20915;&#31574;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#26426;&#22120;&#20154;&#21516;&#26102;&#25512;&#29702;&#24403;&#21069;&#19990;&#30028;&#29366;&#24577;&#21644;&#21160;&#24577;&#65292;&#21516;&#26102;&#35268;&#21010;&#34892;&#21160;&#20197;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#19968;&#26041;&#38754;&#65292;&#22768;&#26126;&#24615;&#35821;&#35328;&#21644;&#25512;&#29702;&#31639;&#27861;&#33021;&#22815;&#33391;&#22909;&#22320;&#25903;&#25345;&#34920;&#31034;&#21644;&#22788;&#29702;&#24120;&#35782;&#30693;&#35782;&#12290;&#20294;&#26159;&#36825;&#20123;&#31639;&#27861;&#22312;&#35268;&#21010;&#26410;&#25351;&#23450;&#26102;&#38388;&#36328;&#24230;&#19979;&#30340;&#26368;&#22823;&#32047;&#31215;&#22870;&#21169;&#30340;&#34892;&#21160;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#27010;&#29575;&#35268;&#21010;&#26694;&#26550;&#65288;&#22914;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#21040;&#30340;MDPs&#65288;POMDPs&#65289;&#65289;&#33021;&#22815;&#24456;&#22909;&#22320;&#25903;&#25345;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#35268;&#21010;&#23454;&#29616;&#38271;&#26399;&#30446;&#26631;&#12290;&#20294;&#26159;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#25110;&#25512;&#29702;&#19982;&#34892;&#21160;&#26080;&#30452;&#25509;&#20851;&#32852;&#30340;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#31216;&#20026;iCORPP&#65292;&#20197;&#21516;&#26102;&#20272;&#35745;&#24403;&#21069;&#19990;&#30028;&#29366;&#24577;&#65292;&#25512;&#29702;&#19990;&#30028;&#21160;&#24577;&#21644;&#26500;&#24314;&#20219;&#21153;&#23548;&#21521;&#30340;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot sequential decision-making in the real world is a challenge because it requires the robots to simultaneously reason about the current world state and dynamics, while planning actions to accomplish complex tasks. On the one hand, declarative languages and reasoning algorithms well support representing and reasoning with commonsense knowledge. But these algorithms are not good at planning actions toward maximizing cumulative reward over a long, unspecified horizon. On the other hand, probabilistic planning frameworks, such as Markov decision processes (MDPs) and partially observable MDPs (POMDPs), well support planning to achieve long-term goals under uncertainty. But they are ill-equipped to represent or reason about knowledge that is not directly related to actions.  In this article, we present a novel algorithm, called iCORPP, to simultaneously estimate the current world state, reason about world dynamics, and construct task-oriented controllers. In this process, robot decision-
&lt;/p&gt;</description></item></channel></rss>