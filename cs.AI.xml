<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>AnyLoc&#23454;&#29616;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;(VPR)&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#21644;&#26080;&#30417;&#30563;&#29305;&#24449;&#32858;&#21512;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.00688</link><description>&lt;p&gt;
AnyLoc:&#26397;&#30528;&#36890;&#29992;&#30340;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
AnyLoc: Towards Universal Visual Place Recognition. (arXiv:2308.00688v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00688
&lt;/p&gt;
&lt;p&gt;
AnyLoc&#23454;&#29616;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;(VPR)&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#21644;&#26080;&#30417;&#30563;&#29305;&#24449;&#32858;&#21512;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;(VPR)&#23545;&#20110;&#26426;&#22120;&#20154;&#23450;&#20301;&#33267;&#20851;&#37325;&#35201;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#26368;&#26377;&#25928;&#30340;VPR&#26041;&#27861;&#26159;&#38024;&#23545;&#29305;&#23450;&#29615;&#22659;&#21644;&#20219;&#21153;&#35774;&#35745;&#30340;&#65306;&#34429;&#28982;&#23427;&#20204;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#65288;&#20027;&#35201;&#26159;&#22478;&#24066;&#39550;&#39542;&#65289;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#26041;&#27861;&#26080;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#31283;&#20581;&#37096;&#32626;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;VPR&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#19968;&#31181;&#22312;&#21508;&#31181;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#65288;&#22478;&#24066;&#12289;&#25143;&#22806;&#12289;&#23460;&#20869;&#12289;&#31354;&#20013;&#12289;&#27700;&#19979;&#21644;&#22320;&#19979;&#29615;&#22659;&#65289;&#20013;&#22343;&#21487;&#36816;&#34892;&#30340;&#25216;&#26415;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;&#29616;&#25104;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#26159;&#26500;&#24314;&#36825;&#31181;&#36890;&#29992;VPR&#35299;&#20915;&#26041;&#26696;&#30340;&#29702;&#24819;&#22522;&#30784;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#27966;&#29983;&#29305;&#24449;&#19982;&#26080;&#30417;&#30563;&#29305;&#24449;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22871;&#20214;AnyLoc&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;4&#20493;&#30340;&#26174;&#33879;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;
Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are environment- and task-specific: while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a universal solution to VPR -- a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or fine-tuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models with no VPR-specific training are the right substrate upon which to build such a universal VPR solution. Combining these derived features with unsupervised feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X significantly higher 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#35270;&#35273;&#32534;&#30721;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#35268;&#24459;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#21644;&#35270;&#35273;&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00678</link><description>&lt;p&gt;
&#35270;&#35273;&#32534;&#30721;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#35268;&#24459;&#36866;&#29992;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Applicability of scaling laws to vision encoding models. (arXiv:2308.00678v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#35270;&#35273;&#32534;&#30721;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#35268;&#24459;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#21644;&#35270;&#35273;&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#22914;&#20309;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#35270;&#35273;&#32534;&#30721;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#22823;&#33041;&#27963;&#21160;&#65292;&#24182;&#21442;&#19982;Algonauts Project 2023 Challenge&#12290;&#35813;&#25361;&#25112;&#25552;&#20379;&#20102;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#35760;&#24405;&#30340;&#22823;&#33041;&#27963;&#21160;&#25968;&#25454;&#65292;&#21442;&#19982;&#32773;&#35266;&#30475;&#22270;&#20687;&#26102;&#20135;&#29983;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21442;&#25968;&#35268;&#27169;&#20174;86M&#21040;4.3B&#30340;&#22810;&#31181;&#35270;&#35273;&#27169;&#22411;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#24230;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#37325;&#28857;&#20998;&#26512;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65306;&#65288;1&#65289;fMRI&#35757;&#32451;&#38598;&#30340;&#26679;&#26412;&#22823;&#23567;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#20934;&#30830;&#24615;&#65311;&#65288;2&#65289;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#23545;&#35270;&#35273;&#30382;&#36136;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26159;&#22914;&#20309;&#21464;&#21270;&#30340;&#65311;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#30528;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#26679;&#26412;&#25968;&#37327;&#22686;&#21152;&#65292;&#39044;&#27979;&#20934;&#30830;&#24615;&#25353;&#29031;&#25193;&#23637;&#35268;&#24459;&#24471;&#21040;&#25913;&#21892;&#12290;&#21516;&#26679;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;&#35270;&#35273;&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#39044;&#27979;&#20934;&#30830;&#24615;&#20063;&#25353;&#29031;&#25193;&#23637;&#35268;&#24459;&#24471;&#21040;&#25913;&#21892;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#21644;&#35270;&#35273;&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigated how to build a high-performance vision encoding model to predict brain activity as part of our participation in the Algonauts Project 2023 Challenge. The challenge provided brain activity recorded by functional MRI (fMRI) while participants viewed images. Several vision models with parameter sizes ranging from 86M to 4.3B were used to build predictive models. To build highly accurate models, we focused our analysis on two main aspects: (1) How does the sample size of the fMRI training set change the prediction accuracy? (2) How does the prediction accuracy across the visual cortex vary with the parameter size of the vision models? The results show that as the sample size used during training increases, the prediction accuracy improves according to the scaling law. Similarly, we found that as the parameter size of the vision models increases, the prediction accuracy improves according to the scaling law. These results suggest that increasing the sample siz
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#20316;&#20026;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#26032;&#24037;&#20855;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;&#20165;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#30340;&#38646;-shot&#25552;&#31034;&#36275;&#20197;&#23454;&#29616;&#27491;&#30830;&#30340;&#24037;&#20855;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00675</link><description>&lt;p&gt;
&#24037;&#20855;&#25991;&#26723;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models. (arXiv:2308.00675v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00675
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#20316;&#20026;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#26032;&#24037;&#20855;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;&#20165;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#30340;&#38646;-shot&#25552;&#31034;&#36275;&#20197;&#23454;&#29616;&#27491;&#30830;&#30340;&#24037;&#20855;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#24037;&#20855;&#20351;&#29992;&#30340;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#26032;&#24037;&#20855;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28436;&#31034;&#24456;&#38590;&#33719;&#24471;&#65292;&#24182;&#19988;&#22914;&#26524;&#36873;&#25321;&#20102;&#38169;&#35823;&#30340;&#28436;&#31034;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33391;&#30340;&#20559;&#35265;&#20351;&#29992;&#12290;&#21363;&#20351;&#22312;&#32597;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#28436;&#31034;&#26159;readily available&#30340;&#65292;&#20063;&#27809;&#26377;&#21407;&#21017;&#24615;&#30340;&#36873;&#25321;&#21327;&#35758;&#26469;&#30830;&#23450;&#25552;&#20379;&#22810;&#23569;&#20010;&#21644;&#21738;&#20123;&#28436;&#31034;&#12290;&#38543;&#30528;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#36873;&#25321;&#25628;&#32034;&#32452;&#21512;&#25968;&#30340;&#22686;&#38271;&#25104;&#20026;&#19981;&#21487;&#22788;&#29702;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#28436;&#31034;&#30340;&#26041;&#27861;&#65306;&#24037;&#20855;&#25991;&#26723;&#12290;&#25105;&#20204;&#20027;&#24352;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#26469;&#25551;&#36848;&#21508;&#20010;&#24037;&#20855;&#30340;&#20351;&#29992;&#65292;&#32780;&#19981;&#26159;&#28436;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#36328;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;6&#20010;&#20219;&#21153;&#19978;&#30340;&#19977;&#20010;&#20027;&#35201;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20027;&#24352;&#12290;&#39318;&#20808;&#65292;&#22312;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#20165;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#30340;&#38646;-shot&#25552;&#31034;&#36275;&#20197;&#24341;&#20986;&#27491;&#30830;&#30340;&#24037;&#20855;&#20351;&#29992;&#65292;&#36798;&#21040;&#20102;few-shot&#25552;&#31034;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Secon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.00629</link><description>&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems - &#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20248;&#21270;&#20915;&#31574;&#31995;&#32479;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26799;&#24230;&#26041;&#27861;&#65292;&#38656;&#35201;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21453;&#39304;&#31232;&#32570;&#25110;&#32773;&#26080;&#20449;&#24687;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26080;&#23548;&#25968;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#26799;&#24230;&#21453;&#39304;&#36136;&#37327;&#30340;&#20381;&#36182;&#65292;&#20294;&#22312;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#24448;&#24448;&#38590;&#20197;&#25193;&#23637;&#12290;&#22914;&#26524;&#31995;&#32479;&#38656;&#35201;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21152;&#21095;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#35282;&#33394;&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#39640;&#25928;&#22320;&#20248;&#21270;&#30001;&#22823;&#37327;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;(HA-GP-UCB)&#22312;&#25928;&#26524;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;Human-M3&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00628</link><description>&lt;p&gt;
&#20154;&#31867;-M3&#65306;&#19968;&#20010;&#29992;&#20110;&#23460;&#22806;&#22330;&#26223;&#20013;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#22810;&#35270;&#35282;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes. (arXiv:2308.00628v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00628
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;Human-M3&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#23460;&#22806;&#29615;&#22659;&#20013;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23460;&#22806;&#22330;&#26223;3D&#20154;&#20307;&#23039;&#21183;&#25968;&#25454;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#21482;&#20351;&#29992;&#19968;&#31181;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#25110;&#28857;&#20113;&#65289;&#65292;&#24182;&#19988;&#22330;&#26223;&#20013;&#36890;&#24120;&#21482;&#26377;&#19968;&#20010;&#20154;&#12290;&#25968;&#25454;&#38598;&#22522;&#30784;&#30340;&#26377;&#38480;&#33539;&#22260;&#20005;&#37325;&#38459;&#30861;&#20102;&#21487;&#29992;&#25968;&#25454;&#30340;&#21464;&#21270;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Human-M3&#65292;&#36825;&#26159;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#23460;&#22806;&#22330;&#26223;&#30340;&#22810;&#35270;&#35282;RGB&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#28857;&#20113;&#25968;&#25454;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#40065;&#26834;&#30340;&#28857;&#20113;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#23460;&#22806;&#22330;&#26223;&#20013;&#22810;&#20010;&#20154;&#30340;&#22810;&#35270;&#35282;RGB&#35270;&#39057;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#20934;&#30830;&#20154;&#20307;&#23450;&#20301;&#21644;&#21305;&#37197;&#27169;&#31946;&#38382;&#39064;&#65292;&#29983;&#25104;&#20102;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D human pose estimation in outdoor environments has garnered increasing attention recently. However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene. This limited scope of dataset infrastructure considerably hinders the variability of available data. In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds. In order to obtain accurate human poses, we propose an algorithm based on multi-modal data input to generate ground truth annotation. This benefits from robust pointcloud detection and tracking, which solves the problem of inaccurate human localization and matching ambiguity that may exist in previous multi-view RGB videos in outdoor multi-person scenes, and generates rel
&lt;/p&gt;</description></item><item><title>JIANG&#26159;&#19968;&#20010;&#19987;&#20026;&#20013;&#25991;&#35774;&#35745;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#30340;&#20013;&#25991;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#32467;&#26500;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;&#20013;&#25991;&#20013;&#21457;&#25381;&#20854;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00624</link><description>&lt;p&gt;
JIANG: &#20013;&#22269;&#24320;&#25918;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
JIANG: Chinese Open Foundation Language Model. (arXiv:2308.00624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00624
&lt;/p&gt;
&lt;p&gt;
JIANG&#26159;&#19968;&#20010;&#19987;&#20026;&#20013;&#25991;&#35774;&#35745;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#30340;&#20013;&#25991;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#32467;&#26500;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;&#20013;&#25991;&#20013;&#21457;&#25381;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#23427;&#23637;&#31034;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#19968;&#25104;&#23601;&#24341;&#36215;&#20102;&#20844;&#21496;&#21644;&#31185;&#30740;&#26426;&#26500;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#23548;&#33268;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#36827;&#34892;&#20102;&#22823;&#37327;&#25237;&#36164;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#26102;&#26399;&#20986;&#29616;&#20102;&#35768;&#22810;&#22823;&#22411;&#27169;&#22411;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#20027;&#35201;&#26159;&#22522;&#20110;&#33521;&#25991;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#20854;&#20182;&#35821;&#35328;&#65288;&#22914;&#20013;&#25991;&#65289;&#20013;&#34920;&#29616;&#20986;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35789;&#27719;&#35774;&#35745;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#31561;&#22240;&#32032;&#65292;&#20854;&#28508;&#21147;&#20173;&#28982;&#21463;&#38480;&#65292;&#26080;&#27861;&#23436;&#20840;&#21457;&#25381;&#22312;&#20013;&#25991;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21517;&#20026;JIANG&#65288;&#23004;&#30340;&#25340;&#38899;&#65289;&#30340;&#19987;&#38376;&#38024;&#23545;&#20013;&#25991;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#20013;&#25991;&#35821;&#26009;&#24211;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#32467;&#26500;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
With the advancements in large language model technology, it has showcased capabilities that come close to those of human beings across various tasks. This achievement has garnered significant interest from companies and scientific research institutions, leading to substantial investments in the research and development of these models. While numerous large models have emerged during this period, the majority of them have been trained primarily on English data. Although they exhibit decent performance in other languages, such as Chinese, their potential remains limited due to factors like vocabulary design and training corpus. Consequently, their ability to fully express their capabilities in Chinese falls short. To address this issue, we introduce the model named JIANG (Chinese pinyin of ginger) specifically designed for the Chinese language. We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure. The extensive experimental res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#26412;&#20307;&#21644;&#35821;&#20041;&#30693;&#35782;&#21453;&#26144;&#21040;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.00607</link><description>&lt;p&gt;
&#36229;&#36234;One-Hot-Encoding: &#27880;&#20837;&#35821;&#20041;&#39537;&#21160;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers. (arXiv:2308.00607v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#26412;&#20307;&#21644;&#35821;&#20041;&#30693;&#35782;&#21453;&#26144;&#21040;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#21253;&#21547;&#20102;&#19982;&#29616;&#23454;&#19990;&#30028;&#26412;&#20307;&#35770;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#65306;&#29399;&#30340;&#21697;&#31181;&#20855;&#26377;&#21754;&#20083;&#21160;&#29289;&#30340;&#30456;&#20284;&#24615;&#65292;&#39135;&#29289;&#30340;&#22270;&#29255;&#36890;&#24120;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#25551;&#36848;&#65292;&#31561;&#31561;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#22270;&#20687;&#20998;&#31867;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26102;&#65292;&#23545;&#35937;&#31867;&#20043;&#38388;&#30340;&#30456;&#23545;&#30456;&#20284;&#24615;&#24120;&#24120;&#19982;One-Hot-Encoding&#26631;&#31614;&#37197;&#23545;&#12290;&#26681;&#25454;&#36825;&#31181;&#36923;&#36753;&#65292;&#22914;&#26524;&#19968;&#20010;&#22270;&#20687;&#34987;&#26631;&#35760;&#20026;&#8220;&#21242;&#23376;&#8221;&#65292;&#37027;&#20040;&#8220;&#33590;&#21242;&#8221;&#21644;&#8220;&#40104;&#40060;&#8221;&#22312;&#35757;&#32451;&#25439;&#22833;&#26041;&#38754;&#26159;&#21516;&#26679;&#38169;&#35823;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25972;&#21512;&#21453;&#26144;&#26412;&#20307;&#21644;&#35821;&#20041;&#30693;&#35782;&#30340;&#39069;&#22806;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#19982;&#20998;&#31867;&#26631;&#31614;&#30456;&#20851;&#30340;&#20219;&#20309;&#31867;&#22411;&#30340;&#35821;&#20041;&#20449;&#24687;&#23548;&#20986;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26412;&#20307;&#21644;&#35789;&#23884;&#20837;&#65292;&#24182;&#35752;&#35770;&#20102;&#30001;&#27492;&#24471;&#21040;&#30340;&#20449;&#24687;&#22914;&#20309;&#39537;&#21160;&#21463;&#30417;&#30563;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20854;&#27425;&#65292;
&lt;/p&gt;
&lt;p&gt;
Images are loaded with semantic information that pertains to real-world ontologies: dog breeds share mammalian similarities, food pictures are often depicted in domestic environments, and so on. However, when training machine learning models for image classification, the relative similarities amongst object classes are commonly paired with one-hot-encoded labels. According to this logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark' are equally wrong in terms of training loss. To overcome this limitation, we explore the integration of additional goals that reflect ontological and semantic knowledge, improving model interpretability and trustworthiness. We suggest a generic approach that allows to derive an additional loss term starting from any kind of semantic information about the classification label. First, we show how to apply our approach to ontologies and word embeddings, and discuss how the resulting information can drive a supervised learning process. Second
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexPredict&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00566</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#20301;&#32622;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Predicting masked tokens in stochastic locations improves masked image modeling. (arXiv:2308.00566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexPredict&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#26500;&#24314;&#38656;&#35201;&#23398;&#20064;&#26377;&#29992;&#34920;&#31034;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21487;&#20197;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20027;&#35201;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26159;&#25513;&#30422;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#65292;&#32780;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23384;&#22312;&#30456;&#24212;&#30340;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#12290;&#28982;&#32780;&#65292;MIM&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22312;&#20934;&#30830;&#20301;&#32622;&#19978;&#39044;&#27979;&#35821;&#20041;&#20869;&#23481;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#24352;&#19981;&#23436;&#25972;&#30340;&#29399;&#30340;&#22270;&#29255;&#65292;&#25105;&#20204;&#21487;&#20197;&#29468;&#27979;&#26377;&#19968;&#20010;&#23614;&#24052;&#65292;&#20294;&#25105;&#20204;&#26080;&#27861;&#30830;&#23450;&#23427;&#30340;&#30830;&#20999;&#20301;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlexPredict&#65292;&#36825;&#26159;&#19968;&#20010;&#32771;&#34385;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#26465;&#20214;&#21270;&#21040;&#38543;&#26426;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#19978;&#65292;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#26356;&#21152;&#40065;&#26834;&#23545;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#20363;&#22914;&#19982;MIM&#22522;&#20934;&#30456;&#27604;&#65292;FlexPredict&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is a promising paradigm in deep learning that enables learning from unlabeled data by constructing pretext tasks that require learning useful representations. In natural language processing, the dominant pretext task has been masked language modeling (MLM), while in computer vision there exists an equivalent called Masked Image Modeling (MIM). However, MIM is challenging because it requires predicting semantic content in accurate locations. E.g, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose FlexPredict, a stochastic model that addresses this challenge by incorporating location uncertainty into the model. Specifically, we condition the model on stochastic masked token positions to guide the model toward learning features that are more robust to location uncertainties. Our approach improves downstream performance on a range of tasks, e.g, compared to MIM baselines, Fle
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#33258;&#22238;&#24402;TSP&#27714;&#35299;&#22120;NAR4TSP&#20351;&#29992;&#29305;&#21035;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#24182;&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12289;&#25512;&#29702;&#24310;&#36831;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.00560</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#33258;&#22238;&#24402;&#27714;&#35299;&#22120;&#29992;&#20110;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-based Non-Autoregressive Solver for Traveling Salesman Problems. (arXiv:2308.00560v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00560
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#33258;&#22238;&#24402;TSP&#27714;&#35299;&#22120;NAR4TSP&#20351;&#29992;&#29305;&#21035;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#24182;&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12289;&#25512;&#29702;&#24310;&#36831;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;TSP&#65289;&#26159;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;TSP&#27714;&#35299;&#22120;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26102;&#38754;&#20020;&#20302;&#24310;&#36831;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NAR4TSP&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20197;&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#26041;&#24335;&#29983;&#25104;TSP&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;NAR4TSP&#20351;&#29992;&#22686;&#24378;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#28040;&#38500;&#20102;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#22522;&#20110;NAR&#27169;&#22411;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#26114;&#36149;&#26631;&#31614;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;NAR4TSP&#26159;&#31532;&#19968;&#20010;&#25104;&#21151;&#32467;&#21512;&#20102;RL&#21644;NAR&#35299;&#30721;&#30340;TSP&#27714;&#35299;&#22120;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;TSP&#23454;&#20363;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NAR4TSP&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12289;&#25512;&#29702;&#24310;&#36831;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NAR4TSP&#35299;&#30721;&#36807;&#31243;&#30340;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Traveling Salesman Problem (TSP) is a well-known problem in combinatorial optimization with applications in various domains. However, existing TSP solvers face challenges in producing high-quality solutions with low latency. To address this issue, we propose NAR4TSP, which produces TSP solutions in a Non-Autoregressive (NAR) manner using a specially designed Graph Neural Network (GNN), achieving faster inference speed. Moreover, NAR4TSP is trained using an enhanced Reinforcement Learning (RL) strategy, eliminating the dependency on costly labels used to train conventional supervised learning-based NAR models. To the best of our knowledge, NAR4TSP is the first TSP solver that successfully combines RL and NAR decoding. The experimental results on both synthetic and real-world TSP instances demonstrate that NAR4TSP outperforms four state-of-the-art models in terms of solution quality, inference latency, and generalization ability. Lastly, we present visualizations of NAR4TSP's decodin
&lt;/p&gt;</description></item><item><title>&#22312;&#23454;&#20363;&#32423;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Copula&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#26377;&#24847;&#20041;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00549</link><description>&lt;p&gt;
Copula&#29992;&#20110;&#23454;&#20363;&#32423;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Copula for Instance-wise Feature Selection and Ranking. (arXiv:2308.00549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00549
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#20363;&#32423;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Copula&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#26377;&#24847;&#20041;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#32972;&#26223;&#19979;&#65292;&#23454;&#20363;&#32423;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#26041;&#27861;&#21487;&#20197;&#20026;&#27599;&#20010;&#26679;&#26412;&#23454;&#29616;&#33391;&#22909;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20551;&#35774;&#29305;&#24449;&#23376;&#38598;&#29420;&#31435;&#65292;&#23545;&#20110;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#23384;&#22312;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#39640;&#26031;copula&#65292;&#19968;&#31181;&#25429;&#25417;&#21464;&#37327;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#24378;&#22823;&#25968;&#23398;&#25216;&#26415;&#65292;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#26356;&#25913;&#23601;&#21487;&#20197;&#23558;&#20854;&#32435;&#20837;&#24403;&#21069;&#30340;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#22312;&#24615;&#33021;&#27604;&#36739;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#26377;&#24847;&#20041;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance-wise feature selection and ranking methods can achieve a good selection of task-friendly features for each sample in the context of neural networks. However, existing approaches that assume feature subsets to be independent are imperfect when considering the dependency between features. To address this limitation, we propose to incorporate the Gaussian copula, a powerful mathematical technique for capturing correlations between variables, into the current feature selection framework with no additional changes needed. Experimental results on both synthetic and real datasets, in terms of performance comparison and interpretability, demonstrate that our method is capable of capturing meaningful correlations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#31215;&#26497;&#20581;&#24247;&#32769;&#40836;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#26089;&#26399;&#36864;&#23398;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#26500;&#24314;&#20998;&#31867;&#27169;&#22411;&#20197;&#21450;&#37319;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#31896;&#38468;&#24230;&#39044;&#27979;&#65292;&#24182;&#22312;&#31185;&#23398;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.00539</link><description>&lt;p&gt;
&#39044;&#27979;&#19968;&#20010;&#31215;&#26497;&#20581;&#24247;&#32769;&#40836;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#26089;&#26399;&#36864;&#23398;
&lt;/p&gt;
&lt;p&gt;
Predicting Early Dropouts of an Active and Healthy Ageing App. (arXiv:2308.00539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#31215;&#26497;&#20581;&#24247;&#32769;&#40836;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#26089;&#26399;&#36864;&#23398;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#26500;&#24314;&#20998;&#31867;&#27169;&#22411;&#20197;&#21450;&#37319;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#31896;&#38468;&#24230;&#39044;&#27979;&#65292;&#24182;&#22312;&#31185;&#23398;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#31215;&#26497;&#20581;&#24247;&#32769;&#40836;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#26089;&#26399;&#36864;&#23398;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#24050;&#25552;&#20132;&#32473;2022&#24180;IFMBE&#31185;&#23398;&#25361;&#25112;&#36187;&#65292;&#26159;IUPESM WC 2022&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#22788;&#29702;&#20102;&#32473;&#23450;&#30340;&#25968;&#25454;&#24211;&#24182;&#29983;&#25104;&#20102;&#19971;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#22788;&#29702;&#25216;&#26415;&#26500;&#24314;&#20998;&#31867;&#27169;&#22411;&#65292;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#39044;&#27979;&#29992;&#25143;&#30340;&#31896;&#38468;&#24230;&#12290;&#25105;&#20204;&#25552;&#20132;&#20102;11&#27425;&#23448;&#26041;&#36816;&#34892;&#65292;&#32467;&#26524;&#26174;&#31034;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#31896;&#38468;&#24230;&#39044;&#27979;&#12290;&#26681;&#25454;&#32467;&#26524;&#65292;&#21160;&#24577;&#29305;&#24449;&#23545;&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#24615;&#36136;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36807;&#37319;&#26679;&#26041;&#27861;&#65292;&#22914;SMOTE&#21644;ADASYN&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#36807;&#37319;&#26679;&#26041;&#27861;&#20351;&#32467;&#26524;&#33719;&#24471;&#20102;10%&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;2022&#24180;IFMBE&#31185;&#23398;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a machine learning approach for predicting early dropouts of an active and healthy ageing app. The presented algorithms have been submitted to the IFMBE Scientific Challenge 2022, part of IUPESM WC 2022. We have processed the given database and generated seven datasets. We used pre-processing techniques to construct classification models that predict the adherence of users using dynamic and static features. We submitted 11 official runs and our results show that machine learning algorithms can provide high-quality adherence predictions. Based on the results, the dynamic features positively influence a model's classification performance. Due to the imbalanced nature of the dataset, we employed oversampling methods such as SMOTE and ADASYN to improve the classification performance. The oversampling approaches led to a remarkable improvement of 10\%. Our methods won first place in the IFMBE Scientific Challenge 2022.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;PressureTransferNet&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#23558;&#20154;&#31867;&#23646;&#24615;&#36716;&#25442;&#20026;&#22320;&#38754;&#21387;&#21147;&#20998;&#24067;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00538</link><description>&lt;p&gt;
PressureTransferNet: &#20351;&#29992;3D&#27169;&#25311;&#30340;&#21387;&#21147;&#22270;&#36827;&#34892;&#20197;&#20154;&#31867;&#23646;&#24615;&#20026;&#25351;&#23548;&#30340;&#21160;&#24577;&#22320;&#38754;&#21387;&#21147;&#20998;&#24067;&#36716;&#25442;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps. (arXiv:2308.00538v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00538
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;PressureTransferNet&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#23558;&#20154;&#31867;&#23646;&#24615;&#36716;&#25442;&#20026;&#22320;&#38754;&#21387;&#21147;&#20998;&#24067;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PressureTransferNet&#65292;&#19968;&#31181;&#20351;&#29992;&#22320;&#38754;&#21387;&#21147;&#20449;&#24687;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#20010;&#20307;&#30340;&#24050;&#26377;&#21387;&#21147;&#25968;&#25454;&#29983;&#25104;&#29305;&#23450;&#27963;&#21160;&#30340;&#29305;&#23450;&#20010;&#20307;&#30340;&#21160;&#24577;&#22320;&#38754;&#21387;&#21147;&#20998;&#24067;&#22270;&#12290;PressureTransferNet&#26159;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20197;&#28304;&#21387;&#21147;&#22270;&#21644;&#30446;&#26631;&#20154;&#31867;&#23646;&#24615;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#21453;&#26144;&#30446;&#26631;&#23646;&#24615;&#30340;&#26032;&#21387;&#21147;&#22270;&#12290;&#20026;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#20256;&#24863;&#22120;&#27169;&#25311;&#26469;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#20154;&#31867;&#23646;&#24615;&#21644;&#21387;&#21147;&#20998;&#24067;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#26174;&#31034;&#20986;&#23427;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#20934;&#30830;&#22320;&#23558;&#20154;&#31867;&#23646;&#24615;&#36716;&#25442;&#20026;&#22320;&#38754;&#21387;&#21147;&#20998;&#24067;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#21512;&#25104;&#21387;&#21147;&#24418;&#29366;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#20102;&#35270;&#35273;&#39564;&#35777;&#65292;&#24182;&#22312;&#25509;&#35302;&#22320;&#38754;&#30340;&#21306;&#22495;&#19978;&#23454;&#29616;&#20102;0.79&#30340;&#20108;&#27425;R-square&#20540;&#12290;&#36890;&#36807;F1&#24471;&#20998;&#65288;0.911&#177;0.015&#65289;&#36827;&#34892;&#20998;&#31867;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose PressureTransferNet, a novel method for Human Activity Recognition (HAR) using ground pressure information. Our approach generates body-specific dynamic ground pressure profiles for specific activities by leveraging existing pressure data from different individuals. PressureTransferNet is an encoder-decoder model taking a source pressure map and a target human attribute vector as inputs, producing a new pressure map reflecting the target attribute. To train the model, we use a sensor simulation to create a diverse dataset with various human attributes and pressure profiles. Evaluation on a real-world dataset shows its effectiveness in accurately transferring human attributes to ground pressure profiles across different scenarios. We visually confirm the fidelity of the synthesized pressure shapes using a physics-based deep learning model and achieve a binary R-square value of 0.79 on areas with ground contact. Validation through classification with F1 score (0.911$\pm$0.015)
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23884;&#20837;&#21160;&#24577;&#29305;&#24449;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;GEDF-SCL&#27169;&#22411;&#65292;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;GEDF&#26469;&#39044;&#27979;&#30005;&#21147;&#31995;&#32479;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;&#65292;&#24182;&#32771;&#34385;&#20102;&#25299;&#25169;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00537</link><description>&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#19979;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#30005;&#21147;&#31995;&#32479;&#30636;&#24577;&#31283;&#23450;&#24615;&#30417;&#27979;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Embedding Dynamic Feature-based Supervised Contrastive Learning of Transient Stability for Changing Power Grid Topologies. (arXiv:2308.00537v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00537
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23884;&#20837;&#21160;&#24577;&#29305;&#24449;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;GEDF-SCL&#27169;&#22411;&#65292;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;GEDF&#26469;&#39044;&#27979;&#30005;&#21147;&#31995;&#32479;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;&#65292;&#24182;&#32771;&#34385;&#20102;&#25299;&#25169;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#65292;&#20934;&#30830;&#30340;&#22312;&#32447;&#30636;&#24577;&#31283;&#23450;&#24615;&#39044;&#27979;&#23545;&#20110;&#30830;&#20445;&#30005;&#21147;&#31995;&#32479;&#30340;&#31283;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;&#20998;&#26512;&#20381;&#36182;&#20110;&#26102;&#38388;&#22495;&#20223;&#30495;&#65292;&#19981;&#33021;&#24555;&#36895;&#36866;&#24212;&#30005;&#21147;&#32593;&#26684;&#25299;&#25169;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#23558;&#39640;&#32500;&#30005;&#21147;&#31995;&#32479;&#25299;&#25169;&#32467;&#26500;&#20449;&#24687;&#21521;&#37327;&#21270;&#20026;&#20302;&#32500;&#33410;&#28857;&#23884;&#20837;&#27969;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#23884;&#20837;&#21160;&#24577;&#29305;&#24449;&#65288;GEDF&#65289;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;GEDF-&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;GEDF-SCL&#65289;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;GEDF&#26469;&#39044;&#27979;&#30636;&#24577;&#31283;&#23450;&#24615;&#65292;&#32771;&#34385;&#20102;&#30005;&#21147;&#31995;&#32479;&#30340;&#25299;&#25169;&#20449;&#24687;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;GEDF-SCL&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22522;&#20110;IEEE 39&#33410;&#28857;&#31995;&#32479;&#27169;&#22411;&#29983;&#25104;&#20102;&#25299;&#25169;&#32467;&#26500;&#19981;&#21516;&#30340;&#30005;&#21147;&#32593;&#26684;&#12290;&#36890;&#36807;&#22312;&#36825;&#20123;&#29983;&#25104;&#30340;&#30005;&#21147;&#31995;&#32479;&#25299;&#25169;&#19978;&#27169;&#25311;N-1&#21644;N-$\bm{m}$-1&#25925;&#38556;&#65292;&#33719;&#21462;&#20102;&#30636;&#24577;&#36816;&#34892;&#25968;&#25454;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;GEDF-SCL&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate online transient stability prediction is critical for ensuring power system stability when facing disturbances. While traditional transient stablity analysis replies on the time domain simulations can not be quickly adapted to the power grid toplogy change. In order to vectorize high-dimensional power grid topological structure information into low-dimensional node-based graph embedding streaming data, graph embedding dynamic feature (GEDF) has been proposed. The transient stability GEDF-based supervised contrastive learning (GEDF-SCL) model uses supervised contrastive learning to predict transient stability with GEDFs, considering power grid topology information. To evaluate the performance of the proposed GEDF-SCL model, power grids of varying topologies were generated based on the IEEE 39-bus system model. Transient operational data was obtained by simulating N-1 and N-$\bm{m}$-1 contingencies on these generated power system topologies. Test result demonstrated that the GED
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#23481;&#24525;&#27880;&#37322;&#22122;&#22768;&#23454;&#29616;&#20102;&#23494;&#38598;&#23545;&#35937;&#35745;&#25968;&#30340;&#28857;&#27880;&#37322;&#27010;&#29575;&#22270;&#65288;PAPM&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#39640;&#26031;&#20998;&#24067;&#26469;&#24418;&#25104;&#23398;&#20064;&#30446;&#26631;PAPM&#65292;&#22312;&#25317;&#25380;&#22330;&#26223;&#20013;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;PAPM&#26041;&#27861;&#65288;HD-PAPM&#65289;&#65292;&#21478;&#19968;&#31181;&#26159;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;PAPM&#26041;&#27861;&#65288;AL-PAPM&#65289;&#12290;</title><link>http://arxiv.org/abs/2308.00530</link><description>&lt;p&gt;
&#28857;&#27880;&#37322;&#27010;&#29575;&#22270;: &#36890;&#36807;&#23481;&#24525;&#27880;&#37322;&#22122;&#22768;&#23454;&#29616;&#23494;&#38598;&#23545;&#35937;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Point Annotation Probability Map: Towards Dense Object Counting by Tolerating Annotation Noise. (arXiv:2308.00530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#23481;&#24525;&#27880;&#37322;&#22122;&#22768;&#23454;&#29616;&#20102;&#23494;&#38598;&#23545;&#35937;&#35745;&#25968;&#30340;&#28857;&#27880;&#37322;&#27010;&#29575;&#22270;&#65288;PAPM&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#39640;&#26031;&#20998;&#24067;&#26469;&#24418;&#25104;&#23398;&#20064;&#30446;&#26631;PAPM&#65292;&#22312;&#25317;&#25380;&#22330;&#26223;&#20013;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;PAPM&#26041;&#27861;&#65288;HD-PAPM&#65289;&#65292;&#21478;&#19968;&#31181;&#26159;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;PAPM&#26041;&#27861;&#65288;AL-PAPM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#22330;&#26223;&#20013;&#35745;&#25968;&#23545;&#35937;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#39640;&#26031;&#23494;&#24230;&#22238;&#24402;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#31181;&#26292;&#21147;&#22238;&#24402;&#26041;&#27861;&#25928;&#26524;&#19981;&#38169;&#65292;&#20294;&#21487;&#33021;&#27809;&#26377;&#24456;&#22909;&#22320;&#32771;&#34385;&#21040;&#30001;&#20154;&#24037;&#27880;&#37322;&#36807;&#31243;&#24341;&#36215;&#30340;&#27880;&#37322;&#22122;&#22768;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#22312;&#23494;&#38598;&#23545;&#35937;&#35745;&#25968;&#20219;&#21153;&#20013;&#32771;&#34385;&#27880;&#37322;&#22122;&#22768;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#23545;&#27880;&#37322;&#22122;&#22768;&#30340;&#24378;&#40065;&#26834;&#24615;&#65292;&#21033;&#29992;&#20855;&#26377;&#21487;&#35843;&#24102;&#23485;&#21644;&#24418;&#29366;&#21442;&#25968;&#30340;&#24191;&#20041;&#39640;&#26031;&#20998;&#24067;&#65288;GGD&#65289;&#20989;&#25968;&#26469;&#24418;&#25104;&#23398;&#20064;&#30446;&#26631;&#28857;&#27880;&#37322;&#27010;&#29575;&#22270;&#65288;PAPM&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;PAPM&#26041;&#27861;&#65288;HD-PAPM&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;GGD&#30340;&#20989;&#25968;&#26469;&#23481;&#24525;&#27880;&#37322;&#22122;&#22768;&#12290;&#23545;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#22522;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;PAPM&#21487;&#33021;&#23545;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#26469;&#35828;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;PAPM&#26041;&#27861;&#65288;AL-PAPM&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
Counting objects in crowded scenes remains a challenge to computer vision. The current deep learning based approach often formulate it as a Gaussian density regression problem. Such a brute-force regression, though effective, may not consider the annotation noise properly which arises from the human annotation process and may lead to different distributions. We conjecture that it would be beneficial to consider the annotation noise in the dense object counting task. To obtain strong robustness against annotation noise, generalized Gaussian distribution (GGD) function with a tunable bandwidth and shape parameter is exploited to form the learning target point annotation probability map, PAPM. Specifically, we first present a hand-designed PAPM method (HD-PAPM), in which we design a function based on GGD to tolerate the annotation noise. For end-to-end training, the hand-designed PAPM may not be optimal for the particular network and dataset. An adaptively learned PAPM method (AL-PAPM) is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#31227;&#38598;&#25104;&#23398;&#20064;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#36716;&#25442;&#20026;&#22266;&#23450;&#38271;&#24230;&#30340;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00525</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#31227;&#38598;&#25104;&#23398;&#20064;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transfer-Ensemble Learning based Deep Convolutional Neural Networks for Diabetic Retinopathy Classification. (arXiv:2308.00525v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#31227;&#38598;&#25104;&#23398;&#20064;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#36716;&#25442;&#20026;&#22266;&#23450;&#38271;&#24230;&#30340;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#20004;&#31181;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;VGG16&#21644;Inception V3&#65289;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#23558;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#30142;&#30149;&#20998;&#31867;&#20026;&#20116;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#26088;&#22312;&#21033;&#29992;&#20004;&#20010;&#21333;&#29420;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#20197;&#25552;&#39640;&#23545;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#38598;&#25104;&#27169;&#22411;&#26550;&#26500;&#28041;&#21450;&#20923;&#32467;&#27599;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#23618;&#20197;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#30340;&#23398;&#20064;&#34920;&#31034;&#12290;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#23618;&#34987;&#28155;&#21152;&#20197;&#23558;&#36755;&#20986;&#29305;&#24449;&#22270;&#36716;&#25442;&#20026;&#22266;&#23450;&#38271;&#24230;&#30340;&#21521;&#37327;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#21521;&#37327;&#36830;&#25509;&#36215;&#26469;&#24418;&#25104;&#36755;&#20837;&#22270;&#20687;&#30340;&#38598;&#21512;&#34920;&#31034;&#12290;&#38598;&#25104;&#27169;&#22411;&#20351;&#29992;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#22270;&#29255;&#25968;&#25454;&#38598;&#65288;APTOS&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20998;&#20026;&#35757;&#32451;&#38598;&#21644;&#39564;&#35777;&#38598;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#23398;&#20064;&#23558;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#31867;&#20026;&#30456;&#24212;&#30340;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#31867;&#21035;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#32467;&#26524;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article aims to classify diabetic retinopathy (DR) disease into five different classes using an ensemble approach based on two popular pre-trained convolutional neural networks: VGG16 and Inception V3. The proposed model aims to leverage the strengths of the two individual nets to enhance the classification performance for diabetic retinopathy. The ensemble model architecture involves freezing a portion of the layers in each pre-trained model to utilize their learned representations effectively. Global average pooling layers are added to transform the output feature maps into fixed-length vectors. These vectors are then concatenated to form a consolidated representation of the input image. The ensemble model is trained using a dataset of diabetic retinopathy images (APTOS), divided into training and validation sets. During the training process, the model learns to classify the retinal images into the corresponding diabetic retinopathy classes. Experimental results on the test set 
&lt;/p&gt;</description></item><item><title>SurveyLM&#26159;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#20013;&#26032;&#20852;&#20215;&#20540;&#35266;&#30340;&#24179;&#21488;&#65292;&#36890;&#36807;&#35843;&#26597;&#21644;&#23454;&#39564;&#26041;&#27861;&#31995;&#32479;&#35780;&#20272;&#20102;ALMs&#30340;&#23545;&#40784;&#21644;&#26032;&#20852;&#34892;&#20026;&#65292;&#24182;&#21033;&#29992;ALMs&#30340;&#21453;&#39304;&#26469;&#22686;&#24378;&#35843;&#26597;&#21644;&#23454;&#39564;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.00521</link><description>&lt;p&gt;
SurveyLM: &#19968;&#31181;&#25506;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#20013;&#26032;&#20852;&#20215;&#20540;&#35266;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
SurveyLM: A platform to explore emerging value perspectives in augmented language models' behaviors. (arXiv:2308.00521v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00521
&lt;/p&gt;
&lt;p&gt;
SurveyLM&#26159;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#20013;&#26032;&#20852;&#20215;&#20540;&#35266;&#30340;&#24179;&#21488;&#65292;&#36890;&#36807;&#35843;&#26597;&#21644;&#23454;&#39564;&#26041;&#27861;&#31995;&#32479;&#35780;&#20272;&#20102;ALMs&#30340;&#23545;&#40784;&#21644;&#26032;&#20852;&#34892;&#20026;&#65292;&#24182;&#21033;&#29992;ALMs&#30340;&#21453;&#39304;&#26469;&#22686;&#24378;&#35843;&#26597;&#21644;&#23454;&#39564;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30333;&#30382;&#20070;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SurveyLM&#19978;&#30340;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;(ALM)&#22312;&#22797;&#26434;&#31038;&#20250;&#29615;&#22659;&#20013;&#36890;&#36807;&#21160;&#24577;&#28436;&#21464;&#30340;&#24577;&#24230;&#21644;&#20215;&#20540;&#35266;&#23637;&#29616;&#20986;&#30340;&#32039;&#23494;&#23545;&#40784;&#34892;&#20026;&#30340;&#24179;&#21488;&#12290;&#20687;ALM&#36825;&#26679;&#30340;&#31038;&#20132;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36890;&#24120;&#22312;&#32454;&#24494;&#30340;&#31038;&#20132;&#22330;&#26223;&#20013;&#36816;&#20316;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#27491;&#30830;&#22238;&#31572;&#65292;&#25110;&#32773;&#31572;&#26696;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#32972;&#26223;&#22240;&#32032;&#65292;&#22240;&#27492;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#23427;&#20204;&#30340;&#23545;&#40784;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#29992;&#20256;&#32479;&#31038;&#20250;&#34892;&#20026;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#35843;&#26597;&#21644;&#23454;&#39564;&#26041;&#27861;&#31995;&#32479;&#22320;&#35780;&#20272;ALM&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#20854;&#23545;&#40784;&#21644;&#26032;&#20852;&#34892;&#20026;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;SurveyLM&#24179;&#21488;&#21033;&#29992;ALM&#33258;&#36523;&#30340;&#21453;&#39304;&#26469;&#22686;&#24378;&#35843;&#26597;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#21033;&#29992;&#20102;ALM&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#26041;&#38754;&#65292;&#21152;&#36895;&#20102;&#39640;&#36136;&#37327;&#35843;&#26597;&#26694;&#26550;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This white paper presents our work on SurveyLM, a platform for analyzing augmented language models' (ALMs) emergent alignment behaviors through their dynamically evolving attitude and value perspectives in complex social contexts. Social Artificial Intelligence (AI) systems, like ALMs, often function within nuanced social scenarios where there is no singular correct response, or where an answer is heavily dependent on contextual factors, thus necessitating an in-depth understanding of their alignment dynamics. To address this, we apply survey and experimental methodologies, traditionally used in studying social behaviors, to evaluate ALMs systematically, thus providing unprecedented insights into their alignment and emergent behaviors. Moreover, the SurveyLM platform leverages the ALMs' own feedback to enhance survey and experiment designs, exploiting an underutilized aspect of ALMs, which accelerates the development and testing of high-quality survey frameworks while conserving resour
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23450;&#24615;&#19987;&#23478;&#30693;&#35782;&#30340;&#37327;&#21270;&#20195;&#29702;&#27169;&#22411;&#24320;&#21457;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#23450;&#24615;&#25968;&#25454;&#32763;&#35793;&#25104;&#23450;&#37327;&#35268;&#21017;&#65292;&#20026;&#27169;&#22411;&#26500;&#24314;&#32773;&#21644;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21644;&#36879;&#26126;&#30340;&#24314;&#27169;&#36807;&#31243;&#12290;&#20197;&#19968;&#20010;&#26377;&#32452;&#32455;&#29359;&#32618;&#30340;&#24212;&#29992;&#26696;&#20363;&#20026;&#20363;&#65292;&#28436;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00505</link><description>&lt;p&gt;
&#22522;&#20110;&#23450;&#24615;&#19987;&#23478;&#30693;&#35782;&#30340;&#37327;&#21270;&#20195;&#29702;&#27169;&#22411;&#24320;&#21457;&#26694;&#26550;&#65306;&#19968;&#20010;&#26377;&#32452;&#32455;&#29359;&#32618;&#30340;&#24212;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Framework for developing quantitative agent based models based on qualitative expert knowledge: an organised crime use-case. (arXiv:2308.00505v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23450;&#24615;&#19987;&#23478;&#30693;&#35782;&#30340;&#37327;&#21270;&#20195;&#29702;&#27169;&#22411;&#24320;&#21457;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#23450;&#24615;&#25968;&#25454;&#32763;&#35793;&#25104;&#23450;&#37327;&#35268;&#21017;&#65292;&#20026;&#27169;&#22411;&#26500;&#24314;&#32773;&#21644;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21644;&#36879;&#26126;&#30340;&#24314;&#27169;&#36807;&#31243;&#12290;&#20197;&#19968;&#20010;&#26377;&#32452;&#32455;&#29359;&#32618;&#30340;&#24212;&#29992;&#26696;&#20363;&#20026;&#20363;&#65292;&#28436;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#25191;&#27861;&#30446;&#30340;&#24314;&#27169;&#29359;&#32618;&#32593;&#32476;&#65292;&#38656;&#35201;&#23558;&#26377;&#38480;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#32463;&#36807;&#39564;&#35777;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#12290;&#24403;&#21069;&#21009;&#20107;&#23398;&#24314;&#27169;&#20013;&#32570;&#23569;&#19968;&#20010;&#20026;&#27169;&#22411;&#26500;&#24314;&#32773;&#21644;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#31995;&#32479;&#21644;&#36879;&#26126;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#35745;&#31639;&#29359;&#32618;&#24314;&#27169;&#30340;&#24314;&#27169;&#36807;&#31243;&#65292;&#21253;&#25324;&#23558;&#23450;&#24615;&#25968;&#25454;&#36716;&#21270;&#20026;&#23450;&#37327;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FREIDA&#65288;&#22522;&#20110;&#19987;&#23478;&#30693;&#35782;&#39537;&#21160;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#26694;&#26550;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#29359;&#32618;&#21487;&#21345;&#22240;&#26367;&#20195;&#27169;&#22411;&#65288;CCRM&#65289;&#23558;&#20316;&#20026;&#31034;&#20363;&#26696;&#20363;&#65292;&#20197;&#28436;&#31034;FREIDA&#26041;&#27861;&#12290;&#23545;&#20110;CCRM&#65292;&#27491;&#22312;&#24314;&#27169;&#33655;&#20848;&#30340;&#19968;&#20010;&#26377;&#32452;&#32455;&#21487;&#21345;&#22240;&#32593;&#32476;&#65292;&#35797;&#22270;&#36890;&#36807;&#31227;&#38500;&#39318;&#33041;&#33410;&#28857;&#65292;&#20351;&#21097;&#20313;&#20195;&#29702;&#37325;&#26032;&#32452;&#32455;&#65292;&#24182;&#23558;&#32593;&#32476;&#24674;&#22797;&#21040;&#31283;&#23450;&#29366;&#24577;&#12290;&#23450;&#24615;&#25968;&#25454;&#28304;&#65292;&#20363;&#22914;&#26696;&#20214;&#25991;&#20214;&#65292;&#25991;&#29486;&#21644;&#37319;&#35775;&#65292;&#34987;&#36716;&#21270;&#20026;&#32463;&#39564;&#27861;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to model criminal networks for law enforcement purposes, a limited supply of data needs to be translated into validated agent-based models. What is missing in current criminological modelling is a systematic and transparent framework for modelers and domain experts that establishes a modelling procedure for computational criminal modelling that includes translating qualitative data into quantitative rules. For this, we propose FREIDA (Framework for Expert-Informed Data-driven Agent-based models). Throughout the paper, the criminal cocaine replacement model (CCRM) will be used as an example case to demonstrate the FREIDA methodology. For the CCRM, a criminal cocaine network in the Netherlands is being modelled where the kingpin node is being removed, the goal being for the remaining agents to reorganize after the disruption and return the network into a stable state. Qualitative data sources such as case files, literature and interviews are translated into empirical laws, and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#25991;&#26723;&#30340;&#22270;&#35889;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23637;&#31034;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#23884;&#20837;&#12289;K&#23884;&#20837;&#21644;&#35789;&#21521;&#37327;&#31354;&#38388;&#23884;&#20837;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#26500;&#24314;&#20102;&#25991;&#26412;&#20869;&#23481;&#21644;&#32858;&#31867;&#32467;&#26524;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;</title><link>http://arxiv.org/abs/2308.00504</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#25991;&#26723;&#30340;&#22270;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Explainable Graph Spectral Clustering of Text Documents. (arXiv:2308.00504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#25991;&#26723;&#30340;&#22270;&#35889;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23637;&#31034;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#23884;&#20837;&#12289;K&#23884;&#20837;&#21644;&#35789;&#21521;&#37327;&#31354;&#38388;&#23884;&#20837;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#26500;&#24314;&#20102;&#25991;&#26412;&#20869;&#23481;&#21644;&#32858;&#31867;&#32467;&#26524;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#32858;&#31867;&#26041;&#27861;&#20197;&#20854;&#33021;&#22815;&#34920;&#31034;&#19981;&#21516;&#24418;&#29366;&#12289;&#23494;&#24230;&#31561;&#30340;&#32858;&#31867;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#25991;&#26412;&#25991;&#26723;&#26102;&#65292;&#20854;&#32467;&#26524;&#24456;&#38590;&#21521;&#29992;&#25143;&#35299;&#37322;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#22312;&#20809;&#35889;&#31354;&#38388;&#20013;&#30340;&#23884;&#20837;&#19982;&#25991;&#26723;&#20869;&#23481;&#27809;&#26377;&#26126;&#26174;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#30740;&#31350;&#35299;&#37322;&#32858;&#31867;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#27492;&#30446;&#26631;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#37322;&#22522;&#20110;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#30340;&#22270;&#35889;&#32858;&#31867;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23637;&#31034;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#23884;&#20837;&#12289;K&#23884;&#20837;&#65288;&#26412;&#25991;&#25552;&#20986;&#65289;&#21644;&#35789;&#21521;&#37327;&#31354;&#38388;&#23884;&#20837;&#30340;&#65288;&#36817;&#20284;&#65289;&#31561;&#20215;&#24615;&#12290;&#20174;&#32780;&#26500;&#24314;&#20102;&#25991;&#26412;&#20869;&#23481;&#21644;&#32858;&#31867;&#32467;&#26524;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#20026;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#32972;&#26223;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#21033;&#26465;&#20214;&#19979;&#65292;K&#23884;&#20837;&#24456;&#22909;&#22320;&#36817;&#20284;&#20102;&#25289;&#26222;&#25289;&#26031;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral clustering methods are known for their ability to represent clusters of diverse shapes, densities etc. However, results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Therefore there is an urgent need to elaborate methods for explaining the outcome of the clustering. This paper presents a contribution towards this goal. We present a proposal of explanation of results of combinatorial Laplacian based graph spectral clustering. It is based on showing (approximate) equivalence of combinatorial Laplacian embedding, $K$-embedding (proposed in this paper) and term vector space embedding. Hence a bridge is constructed between the textual contents and the clustering results. We provide theoretical background for this approach. We performed experimental study showing that $K$-embedding approximates well Laplacian embedding under favourable blo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#21307;&#23398;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#23558;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#24211;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24187;&#35273;&#21644;&#26377;&#23475;&#31572;&#26696;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#34920;&#24615;&#21521;&#37327;&#30340;&#25277;&#21462;&#24615;&#21644;&#25277;&#35937;&#24615;&#25688;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00479</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#25945;&#32946;&#20013;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#20195;&#34920;&#24615;&#21521;&#37327;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education. (arXiv:2308.00479v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#21307;&#23398;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#23558;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#24211;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24187;&#35273;&#21644;&#26377;&#23475;&#31572;&#26696;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#34920;&#24615;&#21521;&#37327;&#30340;&#25277;&#21462;&#24615;&#21644;&#25277;&#35937;&#24615;&#25688;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20869;&#23481;&#29983;&#25104;&#21644;&#20316;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#26102;&#65292;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#35843;&#25972;&#20197;&#20943;&#36731;&#20135;&#29983;&#24187;&#35273;&#21644;&#26377;&#23475;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20801;&#35768;&#36731;&#26494;&#22320;&#36830;&#25509;&#21644;&#25805;&#20316;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#24211;&#21040;LLMs&#19978;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;RAG&#22312;&#21307;&#23398;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#34920;&#24615;&#21521;&#37327;&#30340;&#22823;&#35268;&#27169;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#30340;&#25277;&#21462;&#24615;&#21644;&#25277;&#35937;&#24615;&#25688;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are increasingly being used for various tasks including content generation and as chatbots. Despite their impressive performances in general tasks, LLMs need to be aligned when applying for domain specific tasks to mitigate the problems of hallucination and producing harmful answers. Retrieval Augmented Generation (RAG) allows to easily attach and manipulate a non-parametric knowledgebases to LLMs. Applications of RAG in the field of medical education are discussed in this paper. A combined extractive and abstractive summarization method for large unstructured textual data using representative vectors is proposed.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#20102;&#32654;&#22269;&#26368;&#22810;&#20154;&#21475;&#30340;100&#20010;&#22478;&#24066;&#20197;&#21450;&#30456;&#24212;&#30340;&#20154;&#21475;&#26222;&#26597;&#21306;&#22359;&#32676;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2014&#24180;&#33267;2023&#24180;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#30740;&#31350;&#32654;&#22269;&#22478;&#24066;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#19982;&#36139;&#22256;&#12289;&#20581;&#24247;&#30456;&#20851;&#30340;SDG&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.00465</link><description>&lt;p&gt;
&#32654;&#22269;&#22478;&#24066;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Satellite Imagery Dataset for Long-Term Sustainable Development in United States Cities. (arXiv:2308.00465v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00465
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#20102;&#32654;&#22269;&#26368;&#22810;&#20154;&#21475;&#30340;100&#20010;&#22478;&#24066;&#20197;&#21450;&#30456;&#24212;&#30340;&#20154;&#21475;&#26222;&#26597;&#21306;&#22359;&#32676;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2014&#24180;&#33267;2023&#24180;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#30740;&#31350;&#32654;&#22269;&#22478;&#24066;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#19982;&#36139;&#22256;&#12289;&#20581;&#24247;&#30456;&#20851;&#30340;SDG&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#22312;&#23454;&#29616;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDG&#65289;&#21644;&#20419;&#36827;&#32463;&#27982;&#22686;&#38271;&#12289;&#28385;&#36275;&#31038;&#20250;&#38656;&#27714;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23588;&#20854;&#26159;&#21355;&#26143;&#22270;&#20687;&#26159;&#30740;&#31350;&#21487;&#25345;&#32493;&#22478;&#24066;&#21457;&#23637;&#30340;&#28508;&#22312;&#25968;&#25454;&#28304;&#12290;&#28982;&#32780;&#65292;&#22312;&#32654;&#22269;&#32570;&#20047;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#22478;&#24066;&#12289;&#22810;&#24180;&#20221;&#12289;&#22810;&#23610;&#24230;&#21644;&#22810;&#20010;SDG&#25351;&#26631;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30417;&#27979;SDG&#12290;&#20026;&#20102;&#25903;&#25345;&#23545;&#32654;&#22269;&#22478;&#24066;SDG&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;25&#20010;&#21487;&#25345;&#32493;&#21457;&#23637;&#25351;&#26631;&#30340;&#20116;&#20010;SDG&#12290;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#32654;&#22269;&#20154;&#21475;&#26368;&#22810;&#30340;100&#20010;&#22478;&#24066;&#21644;&#30456;&#24212;&#30340;&#20154;&#21475;&#26222;&#26597;&#21306;&#22359;&#32676;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2014&#24180;&#33267;2023&#24180;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25910;&#38598;&#21355;&#26143;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#35782;&#21035;&#22478;&#24066;&#30340;&#40479;&#30640;&#22270;&#12290;&#25105;&#20204;&#36824;&#25910;&#38598;&#20102;&#20154;&#21475;&#12289;&#22812;&#38388;&#20809;&#32447;&#12289;&#35843;&#26597;&#21644;&#24314;&#31569;&#29615;&#22659;&#25968;&#25454;&#65292;&#25551;&#36848;&#20102;&#19982;&#36139;&#22256;&#12289;&#20581;&#24247;&#30456;&#20851;&#30340;SDG&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cities play an important role in achieving sustainable development goals (SDGs) to promote economic growth and meet social needs. Especially satellite imagery is a potential data source for studying sustainable urban development. However, a comprehensive dataset in the United States (U.S.) covering multiple cities, multiple years, multiple scales, and multiple indicators for SDG monitoring is lacking. To support the research on SDGs in U.S. cities, we develop a satellite imagery dataset using deep learning models for five SDGs containing 25 sustainable development indicators. The proposed dataset covers the 100 most populated U.S. cities and corresponding Census Block Groups from 2014 to 2023. Specifically, we collect satellite imagery and identify objects with state-of-the-art object detection and semantic segmentation models to observe cities' bird's-eye view. We further gather population, nighttime light, survey, and built environment data to depict SDGs regarding poverty, health, e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DMFC-GraspNet&#65292;&#22312;&#22810;&#25351;&#26426;&#22120;&#20154;&#25235;&#21462;&#29983;&#25104;&#39046;&#22495;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#19968;&#26159;&#25552;&#20986;&#20102;&#21487;&#24494;&#30340;&#22810;&#25351;&#25235;&#21462;&#35268;&#21010;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#21644;&#31264;&#23494;&#30340;&#25235;&#21462;&#39044;&#27979;&#65307;&#20108;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#31264;&#23494;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#24471;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19982;&#30495;&#23454;&#25235;&#21462;&#23494;&#20999;&#20851;&#32852;&#12290;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00456</link><description>&lt;p&gt;
DMFC-GraspNet: &#22810;&#25351;&#26426;&#22120;&#20154;&#22312;&#26434;&#20081;&#22330;&#26223;&#20013;&#21487;&#24494;&#30340;&#25235;&#21462;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DMFC-GraspNet: Differentiable Multi-Fingered Robotic Grasp Generation in Cluttered Scenes. (arXiv:2308.00456v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DMFC-GraspNet&#65292;&#22312;&#22810;&#25351;&#26426;&#22120;&#20154;&#25235;&#21462;&#29983;&#25104;&#39046;&#22495;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#19968;&#26159;&#25552;&#20986;&#20102;&#21487;&#24494;&#30340;&#22810;&#25351;&#25235;&#21462;&#35268;&#21010;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#21644;&#31264;&#23494;&#30340;&#25235;&#21462;&#39044;&#27979;&#65307;&#20108;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#31264;&#23494;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#24471;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19982;&#30495;&#23454;&#25235;&#21462;&#23494;&#20999;&#20851;&#32852;&#12290;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25235;&#21462;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24517;&#22791;&#30340;&#22522;&#26412;&#25216;&#33021;&#12290;&#27169;&#20223;&#20154;&#25163;&#32467;&#26500;&#30340;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#21487;&#20197;&#36827;&#34892;&#22797;&#26434;&#30340;&#29289;&#20307;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22810;&#25351;&#26426;&#22120;&#20154;&#25235;&#21462;&#25216;&#26415;&#36890;&#24120;&#22312;&#27599;&#27425;&#25512;&#29702;&#20013;&#21482;&#33021;&#39044;&#27979;&#19968;&#27425;&#25235;&#21462;&#65292;&#38480;&#21046;&#20102;&#20854;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#22810;&#25351;&#25235;&#21462;&#29983;&#25104;&#32593;&#32476;&#65288;DMFC-GraspNet&#65289;&#65292;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25235;&#21462;&#35268;&#21010;&#22120;&#65292;&#39044;&#27979;&#20102;&#19968;&#31181;&#26032;&#30340;&#25235;&#21462;&#34920;&#31034;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#32780;&#31264;&#23494;&#30340;&#25235;&#21462;&#39044;&#27979;&#12290;&#20854;&#27425;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22330;&#26223;&#21019;&#24314;&#21644;&#26631;&#31614;&#26144;&#23556;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#30340;&#31264;&#23494;&#26631;&#27880;&#65292;&#23454;&#29616;&#20102;&#19982;&#30495;&#23454;&#25235;&#21462;&#30340;&#23494;&#20999;&#20851;&#32852;&#12290;&#36890;&#36807;&#20223;&#30495;&#30740;&#31350;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic grasping is a fundamental skill required for object manipulation in robotics. Multi-fingered robotic hands, which mimic the structure of the human hand, can potentially perform complex object manipulations. Nevertheless, current techniques for multi-fingered robotic grasping frequently predict only a single grasp for each inference time, limiting their versatility and efficiency. This paper proposes a differentiable multi-fingered grasp generation network (DMFC-GraspNet) with two main contributions to address this challenge. Firstly, a novel neural grasp planner is proposed, which predicts a new grasp representation to enable versatile and dense grasp predictions. Secondly, a scene creation and label mapping method is developed for dense labeling of multi-fingered robotic hands, which allows a dense association of ground truth grasps. The proposed approach is evaluated through simulation studies and compared to existing approaches. The results demonstrate the effectiveness of t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#22522;&#20110;&#22270;&#30340;&#20132;&#20114;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#23548;&#19982;LLM&#38598;&#25104;&#22823;&#37327;&#22806;&#37096;&#24037;&#20855;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.00447</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#32467;&#26500;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Structural Embeddings of Tools for Large Language Models. (arXiv:2308.00447v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#22522;&#20110;&#22270;&#30340;&#20132;&#20114;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#23548;&#19982;LLM&#38598;&#25104;&#22823;&#37327;&#22806;&#37096;&#24037;&#20855;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#32780;&#26131;&#35265;&#65292;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29366;&#24577;&#38656;&#35201;&#24341;&#20837;&#22806;&#37096;&#24037;&#20855;&#12290;&#24050;&#32463;&#26377;&#22823;&#37327;&#25991;&#29486;&#35760;&#24405;&#20102;&#20854;&#32570;&#20047;&#30452;&#25509;&#30340;&#20195;&#25968;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#20801;&#35768;LLMs&#36890;&#36807;&#22806;&#37096;&#24037;&#20855;&#36816;&#34892;&#30340;&#26694;&#26550;&#12290;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#20855;&#21033;&#29992;&#30340;&#26412;&#20307;&#24615;&#36136;&#21487;&#20197;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#24456;&#22909;&#22320;&#25551;&#36848;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#30446;&#26631;&#26159;&#31361;&#20986;&#24378;&#35843;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#23545;LLM-&#24037;&#20855;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31034;&#33539;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#25351;&#23548;&#25351;&#25968;&#32423;&#22686;&#21152;&#30340;&#22806;&#37096;&#24037;&#20855;&#19982;LLMs&#30340;&#32534;&#25490;&#65292;&#20854;&#20013;&#24037;&#20855;&#30340;&#30446;&#26631;&#21644;&#21151;&#33021;&#20197;&#22270;&#24418;&#26041;&#24335;&#36827;&#34892;&#23618;&#27425;&#32467;&#26500;&#32534;&#30721;&#12290;&#20551;&#35774;&#20316;&#20026;&#23450;&#20041;&#22312;&#36825;&#37324;&#30340;&#24037;&#20855;&#65292;&#24605;&#32500;&#38142;(CoT)&#30340;&#25991;&#26412;&#29255;&#27573;&#21487;&#20197;&#34987;&#24819;&#35937;&#20026;&#19968;&#31181;&#24037;&#20855;&#65292;&#37027;&#20040;&#22522;&#20110;&#22270;&#30340;&#26694;&#26550;&#20063;&#21487;&#20197;&#22312;&#36825;&#20010;&#29305;&#23450;&#26041;&#21521;&#19978;&#24320;&#36767;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is evident that the current state of Large Language Models (LLMs) necessitates the incorporation of external tools. The lack of straightforward algebraic and logical reasoning is well documented and prompted researchers to develop frameworks which allow LLMs to operate via external tools. The ontological nature of tool utilization for a specific task can be well formulated with a Directed Acyclic Graph (DAG). The central aim of the paper is to highlight the importance of graph based approaches to LLM-tool interaction in near future. We propose an exemplary framework to guide the orchestration of exponentially increasing numbers of external tools with LLMs,where objectives and functionalities of tools are graph encoded hierarchically. Assuming that textual segments of a Chain-of-Thought (CoT) can be imagined as a tool as defined here, the graph based framework can pave new avenues in that particular direction as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00436</link><description>&lt;p&gt;
SelfCheck: &#20351;&#29992;LLMs&#33258;&#26816;&#20854;&#36880;&#27493;&#25512;&#29702;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#30340;&#21457;&#26126;&#65292;&#20351;&#24471;&#35299;&#20915;&#25512;&#29702;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#24378;&#22823;&#30340;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#38750;&#32447;&#24615;&#24605;&#32500;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#20855;&#26377;&#35782;&#21035;&#33258;&#24049;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#36880;&#27493;&#25512;&#29702;&#20013;&#30340;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#20197;&#35782;&#21035;&#27492;&#31867;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#39564;&#35777;&#26041;&#26696;&#26469;&#25913;&#36827;&#38382;&#31572;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#25237;&#31080;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;-GSM8K&#65292;MathQA&#21644;MATH&#19978;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#32780;&#25552;&#39640;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;Patch AE&#65289;&#26694;&#26550;&#26469;&#22686;&#24378;&#33258;&#32534;&#30721;&#22120;&#23545;&#24322;&#24120;&#30340;&#37325;&#26500;&#33021;&#21147;&#65292;&#24182;&#22312;Mvtec AD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20855;&#26377;&#22312;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00429</link><description>&lt;p&gt;
&#22522;&#20110;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#30340;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Patch-wise Auto-Encoder for Visual Anomaly Detection. (arXiv:2308.00429v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;Patch AE&#65289;&#26694;&#26550;&#26469;&#22686;&#24378;&#33258;&#32534;&#30721;&#22120;&#23545;&#24322;&#24120;&#30340;&#37325;&#26500;&#33021;&#21147;&#65292;&#24182;&#22312;Mvtec AD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20855;&#26377;&#22312;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#24322;&#24120;&#20808;&#39564;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#65292;&#20256;&#32479;&#30340;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#22312;&#20165;&#36890;&#36807;&#27491;&#24120;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#26102;&#20542;&#21521;&#20110;&#22833;&#36133;&#65292;&#22240;&#20026;&#27169;&#22411;&#23558;&#26080;&#27861;&#27491;&#30830;&#37325;&#26500;&#24322;&#24120;&#22270;&#20687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;Patch AE&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;AE&#23545;&#24322;&#24120;&#30340;&#37325;&#26500;&#33021;&#21147;&#32780;&#19981;&#26159;&#21066;&#24369;&#23427;&#12290;&#22270;&#20687;&#30340;&#27599;&#20010;&#34917;&#19969;&#37117;&#36890;&#36807;&#30456;&#24212;&#30340;&#31354;&#38388;&#20998;&#24067;&#29305;&#24449;&#21521;&#37327;&#30340;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#37325;&#26500;&#65292;&#21363;&#34917;&#19969;&#21270;&#37325;&#26500;&#65292;&#36825;&#30830;&#20445;&#20102;AE&#23545;&#24322;&#24120;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#39640;&#25928;&#12290;&#23427;&#22312;Mvtec AD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#22312;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection without priors of the anomalies is challenging. In the field of unsupervised anomaly detection, traditional auto-encoder (AE) tends to fail based on the assumption that by training only on normal images, the model will not be able to reconstruct abnormal images correctly. On the contrary, we propose a novel patch-wise auto-encoder (Patch AE) framework, which aims at enhancing the reconstruction ability of AE to anomalies instead of weakening it. Each patch of image is reconstructed by corresponding spatially distributed feature vector of the learned feature representation, i.e., patch-wise reconstruction, which ensures anomaly-sensitivity of AE. Our method is simple and efficient. It advances the state-of-the-art performances on Mvtec AD benchmark, which proves the effectiveness of our model. It shows great potential in practical industrial application scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#22768;&#22330;&#20808;&#39564;&#30693;&#35782;&#20934;&#30830;&#37325;&#24314;&#22768;&#22330;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#31934;&#24230;&#21644;&#33021;&#37327;&#20445;&#30041;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#39640;&#39057;&#33539;&#22260;&#21644;&#36229;&#20986;&#27979;&#37327;&#21306;&#22495;&#30340;&#22806;&#25512;&#24773;&#20917;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#25968;&#37327;&#21644;&#37197;&#32622;&#30340;&#27979;&#37327;&#20301;&#32622;&#65292;&#20026;&#22768;&#22330;&#37325;&#24314;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00426</link><description>&lt;p&gt;
&#20855;&#26377;&#29289;&#29702;&#22768;&#22330;&#20808;&#39564;&#30693;&#35782;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks with physical sound field priors. (arXiv:2308.00426v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#22768;&#22330;&#20808;&#39564;&#30693;&#35782;&#20934;&#30830;&#37325;&#24314;&#22768;&#22330;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#31934;&#24230;&#21644;&#33021;&#37327;&#20445;&#30041;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#39640;&#39057;&#33539;&#22260;&#21644;&#36229;&#20986;&#27979;&#37327;&#21306;&#22495;&#30340;&#22806;&#25512;&#24773;&#20917;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#25968;&#37327;&#21644;&#37197;&#32622;&#30340;&#27979;&#37327;&#20301;&#32622;&#65292;&#20026;&#22768;&#22330;&#37325;&#24314;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22768;&#22330;&#30340;&#26102;&#31354;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24179;&#38754;&#27874;&#22522;&#20989;&#25968;&#65292;&#24182;&#23398;&#20064;&#20102;&#25151;&#38388;&#20869;&#21387;&#21147;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#27979;&#37327;&#20013;&#37325;&#24314;&#22768;&#22330;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#24050;&#24314;&#31435;&#30340;&#25968;&#25454;&#38598;&#23545;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#31934;&#24230;&#21644;&#33021;&#37327;&#20445;&#30041;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#37325;&#24314;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39057;&#33539;&#22260;&#21644;&#36229;&#20986;&#27979;&#37327;&#21306;&#22495;&#30340;&#22806;&#25512;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#25968;&#37327;&#21644;&#37197;&#32622;&#30340;&#27979;&#37327;&#20301;&#32622;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#22768;&#22330;&#37325;&#24314;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#24341;&#20837;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning-based approach for the spatio-temporal reconstruction of sound fields using Generative Adversarial Networks (GANs). The method utilises a plane wave basis and learns the underlying statistical distributions of pressure in rooms to accurately reconstruct sound fields from a limited number of measurements. The performance of the method is evaluated using two established datasets and compared to state-of-the-art methods. The results show that the model is able to achieve an improved reconstruction performance in terms of accuracy and energy retention, particularly in the high-frequency range and when extrapolating beyond the measurement region. Furthermore, the proposed method can handle a varying number of measurement positions and configurations without sacrificing performance. The results suggest that this approach provides a promising approach to sound field reconstruction using generative models that allow for a physically informed prior to acousti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35805;&#35821;&#24863;&#30693;&#30340;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#20013;&#25286;&#20998;&#21644;&#37325;&#26032;&#34920;&#36798;&#22797;&#26434;&#30340;&#33521;&#35821;&#21477;&#23376;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21477;&#27861;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#30340;&#20445;&#23432;&#24615;&#21644;&#24573;&#30053;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.00425</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#21477;&#23376;&#21040;&#38142;&#25509;&#21629;&#39064;&#30340;&#35805;&#35821;&#24863;&#30693;&#25991;&#26412;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Discourse-Aware Text Simplification: From Complex Sentences to Linked Propositions. (arXiv:2308.00425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35805;&#35821;&#24863;&#30693;&#30340;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#20013;&#25286;&#20998;&#21644;&#37325;&#26032;&#34920;&#36798;&#22797;&#26434;&#30340;&#33521;&#35821;&#21477;&#23376;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21477;&#27861;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#30340;&#20445;&#23432;&#24615;&#21644;&#24573;&#30053;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26469;&#35828;&#65292;&#22797;&#26434;&#21477;&#23376;&#26159;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#65292;&#21477;&#23376;&#30340;&#38271;&#24230;&#21644;&#22797;&#26434;&#24615;&#20250;&#23548;&#33268;&#39044;&#27979;&#36136;&#37327;&#19979;&#38477;&#12290;&#25991;&#26412;&#31616;&#21270;&#30340;&#20219;&#21153;&#23601;&#26159;&#20026;&#20102;&#20351;&#21477;&#23376;&#26356;&#23481;&#26131;&#22788;&#29702;&#32780;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#30340;&#37325;&#20889;&#25805;&#20316;&#65292;&#27604;&#22914;&#37325;&#26032;&#25490;&#24207;&#12289;&#21024;&#38500;&#25110;&#25286;&#20998;&#12290;&#29616;&#26377;&#30340;&#21477;&#27861;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#37319;&#21462;&#20102;&#38750;&#24120;&#20445;&#23432;&#30340;&#26041;&#27861;&#65292;&#20542;&#21521;&#20110;&#20445;&#30041;&#36755;&#20837;&#32780;&#19981;&#36827;&#34892;&#36716;&#25442;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#24573;&#30053;&#20102;&#25991;&#26412;&#30340;&#36830;&#36143;&#24615;&#65292;&#38656;&#35201;&#36328;&#36234;&#20174;&#21477;&#25110;&#21477;&#23376;&#30340;&#19978;&#19979;&#25991;&#26469;&#25512;&#26029;&#35821;&#21477;&#30340;&#30495;&#27491;&#21547;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35805;&#35821;&#24863;&#30693;&#30340;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#65292;&#22312;&#21477;&#23376;&#25152;&#22788;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#20013;&#25286;&#20998;&#21644;&#37325;&#26032;&#34920;&#36798;&#22797;&#26434;&#30340;&#33521;&#35821;&#21477;&#23376;&#12290;&#22522;&#20110;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#36716;&#25442;&#38454;&#27573;&#65292;&#35813;&#38454;&#27573;&#21033;&#29992;&#20174;&#21477;&#21644;&#21629;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23436;&#25104;&#31616;&#21270;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentences that present a complex syntax act as a major stumbling block for downstream Natural Language Processing applications whose predictive quality deteriorates with sentence length and complexity. The task of Text Simplification (TS) may remedy this situation. It aims to modify sentences in order to make them easier to process, using a set of rewriting operations, such as reordering, deletion, or splitting. State-of-the-art syntactic TS approaches suffer from two major drawbacks: first, they follow a very conservative approach in that they tend to retain the input rather than transforming it, and second, they ignore the cohesive nature of texts, where context spread across clauses or sentences is needed to infer the true meaning of a statement. To address these problems, we present a discourse-aware TS approach that splits and rephrases complex English sentences within the semantic context in which they occur. Based on a linguistically grounded transformation stage that uses claus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25361;&#25112;&#22270;&#21327;&#21516;&#36807;&#28388;&#30340;&#31070;&#35805;&#65292;&#36890;&#36807;&#20851;&#27880;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20845;&#20010;&#27969;&#34892;&#30340;&#22270;&#25512;&#33616;&#27169;&#22411;&#22312;&#20960;&#20010;&#24120;&#35265;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.00404</link><description>&lt;p&gt;
&#25361;&#25112;&#22270;&#21327;&#21516;&#36807;&#28388;&#30340;&#31070;&#35805;&#65306;&#19968;&#39033;&#22522;&#20110;&#25512;&#29702;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Challenging the Myth of Graph Collaborative Filtering: a Reasoned and Reproducibility-driven Analysis. (arXiv:2308.00404v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25361;&#25112;&#22270;&#21327;&#21516;&#36807;&#28388;&#30340;&#31070;&#35805;&#65292;&#36890;&#36807;&#20851;&#27880;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20845;&#20010;&#27969;&#34892;&#30340;&#22270;&#25512;&#33616;&#27169;&#22411;&#22312;&#20960;&#20010;&#24120;&#35265;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#26174;&#33879;&#25512;&#21160;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#21644;&#29289;&#21697;&#26377;&#25928;&#22320;&#24314;&#27169;&#20026;&#19968;&#20010;&#20108;&#20998;&#22270;&#21644;&#26080;&#21521;&#22270;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21407;&#22987;&#30340;&#22522;&#20110;&#22270;&#30340;&#20316;&#21697;&#36890;&#24120;&#22312;&#26410;&#39564;&#35777;&#20854;&#22312;&#20855;&#20307;&#37197;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#30340;&#24773;&#20917;&#19979;&#37319;&#29992;&#22522;&#32447;&#35770;&#25991;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#30528;&#37325;&#20851;&#27880;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#21151;&#22797;&#21046;&#20102;&#20845;&#20010;&#27969;&#34892;&#19988;&#26368;&#26032;&#30340;&#22270;&#25512;&#33616;&#27169;&#22411;&#65288;NGCF&#12289;DGCF&#12289;LightGCN&#12289;SGL&#12289;UltraGCN&#21644;GFCF&#65289;&#22312;&#19977;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;Gowalla&#12289;Yelp 2018&#21644;&#20122;&#39532;&#36874;&#22270;&#20070;&#65289;&#19978;&#30340;&#32467;&#26524;&#30340;&#20195;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#22270;&#27169;&#22411;&#19982;&#22312;&#31163;&#32447;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#23545;&#20004;&#20010;&#32570;&#20047;&#29616;&#26377;&#25991;&#29486;&#20013;&#24050;&#24314;&#31435;&#35774;&#32622;&#30340;&#26032;&#25968;&#25454;&#38598;&#65288;Allrecipes&#21644;BookCrossing&#65289;&#30340;&#30740;&#31350;&#12290;&#30001;&#20110;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#19982;&#20197;&#21069;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#20351;&#24471;&#25105;&#20204;&#23545;&#22270;&#25512;&#33616;&#27169;&#22411;&#24615;&#33021;&#30340;&#35780;&#20272;&#32467;&#26524;&#26356;&#21152;&#28145;&#20837;&#21644;&#20840;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of graph neural network-based models (GNNs) has significantly advanced recommender systems by effectively modeling users and items as a bipartite, undirected graph. However, many original graph-based works often adopt results from baseline papers without verifying their validity for the specific configuration under analysis. Our work addresses this issue by focusing on the replicability of results. We present a code that successfully replicates results from six popular and recent graph recommendation models (NGCF, DGCF, LightGCN, SGL, UltraGCN, and GFCF) on three common benchmark datasets (Gowalla, Yelp 2018, and Amazon Book). Additionally, we compare these graph models with traditional collaborative filtering models that historically performed well in offline evaluations. Furthermore, we extend our study to two new datasets (Allrecipes and BookCrossing) that lack established setups in existing literature. As the performance on these datasets differs from the previous bench
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#21453;&#20107;&#23454;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;CGT&#65289;&#65292;&#36890;&#36807;&#23454;&#20363;&#32423;&#35299;&#37322;&#22120;&#21644;&#25200;&#21160;&#25513;&#30721;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#37325;&#35201;&#23376;&#22270;&#12290;</title><link>http://arxiv.org/abs/2308.00391</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#21453;&#20107;&#23454;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Graph Transformer for Traffic Flow Prediction. (arXiv:2308.00391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#21453;&#20107;&#23454;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;CGT&#65289;&#65292;&#36890;&#36807;&#23454;&#20363;&#32423;&#35299;&#37322;&#22120;&#21644;&#25200;&#21160;&#25513;&#30721;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#37325;&#35201;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#65288;TFP&#65289;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#23427;&#27169;&#25311;&#20102;&#20132;&#36890;&#27969;&#37327;&#30340;&#28508;&#22312;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#29992;&#20110;&#28508;&#22312;&#25317;&#22581;&#39044;&#27979;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#22810;&#31181;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#24448;&#24448;&#20250;&#20174;&#25968;&#25454;&#38598;&#20013;&#32487;&#25215;&#20559;&#24046;&#27169;&#24335;&#65292;&#24182;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;TFP&#35774;&#35745;&#30340;&#21453;&#20107;&#23454;&#22270;&#21464;&#21387;&#22120;&#65288;CGT&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#23454;&#20363;&#32423;&#35299;&#37322;&#22120;&#65288;&#20363;&#22914;&#65292;&#23547;&#25214;&#37325;&#35201;&#23376;&#22270;&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#36755;&#20837;&#20256;&#24863;&#22120;&#29305;&#24449;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#21644;&#22270;&#21464;&#21387;&#22120;&#27169;&#22359;&#19978;&#30340;&#22270;&#32467;&#26500;&#36827;&#34892;&#25200;&#21160;&#30340;&#29983;&#25104;&#22120;&#65292;&#20197;&#33719;&#21462;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36890;&#36807;&#25628;&#32034;&#36755;&#20837;&#25968;&#25454;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#19978;&#30340;&#26368;&#20339;&#25200;&#21160;&#25513;&#30721;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#21462;&#31616;&#27905;&#32780;&#20027;&#23548;&#30340;&#25968;&#25454;&#25110;&#22270;&#36793;&#38142;&#25509;&#65292;&#20379;&#21518;&#32493;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic flow prediction (TFP) is a fundamental problem of the Intelligent Transportation System (ITS), as it models the latent spatial-temporal dependency of traffic flow for potential congestion prediction. Recent graph-based models with multiple kinds of attention mechanisms have achieved promising performance. However, existing methods for traffic flow prediction tend to inherit the bias pattern from the dataset and lack interpretability. To this end, we propose a Counterfactual Graph Transformer (CGT) model with an instance-level explainer (e.g., finding the important subgraphs) specifically designed for TFP. We design a perturbation mask generator over input sensor features at the time dimension and the graph structure on the graph transformer module to obtain spatial and temporal counterfactual explanations. By searching the optimal perturbation masks on the input data feature and graph structures, we can obtain the concise and dominant data or graph edge links for the subsequent
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#19977;&#30456;&#31227;&#20301;&#35843;&#21046;&#25216;&#26415;&#65292;&#29992;&#20110;&#21452;&#26377;&#28304;&#26725;&#21464;&#25442;&#22120;&#65292;&#20197;&#26368;&#23567;&#21270;&#30005;&#27969;&#24212;&#21147;&#12290;&#36825;&#31181;&#35843;&#21046;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;DAB&#21464;&#25442;&#22120;&#30340;&#21151;&#29575;&#25928;&#29575;&#21644;&#21151;&#29575;&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.00382</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21452;&#26377;&#28304;&#26725;&#21464;&#25442;&#22120;&#30340;&#19977;&#30456;&#31227;&#20301;&#35843;&#21046;&#65292;&#20197;&#26368;&#23567;&#21270;&#30005;&#27969;&#24212;&#21147;
&lt;/p&gt;
&lt;p&gt;
Artificial-Intelligence-Based Triple Phase Shift Modulation for Dual Active Bridge Converter with Minimized Current Stress. (arXiv:2308.00382v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#19977;&#30456;&#31227;&#20301;&#35843;&#21046;&#25216;&#26415;&#65292;&#29992;&#20110;&#21452;&#26377;&#28304;&#26725;&#21464;&#25442;&#22120;&#65292;&#20197;&#26368;&#23567;&#21270;&#30005;&#27969;&#24212;&#21147;&#12290;&#36825;&#31181;&#35843;&#21046;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;DAB&#21464;&#25442;&#22120;&#30340;&#21151;&#29575;&#25928;&#29575;&#21644;&#21151;&#29575;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21452;&#26377;&#28304;&#26725;&#65288;DAB&#65289;&#21464;&#25442;&#22120;&#22240;&#20854;&#20986;&#33394;&#30340;&#21151;&#29575;&#23494;&#24230;&#21644;&#21452;&#21521;&#21151;&#29575;&#20256;&#36755;&#33021;&#21147;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#19977;&#30456;&#31227;&#20301;&#65288;TPS&#65289;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;DAB&#21464;&#25442;&#22120;&#26368;&#20808;&#36827;&#30340;&#35843;&#21046;&#25216;&#26415;&#20043;&#19968;&#12290;&#23427;&#21487;&#20197;&#25193;&#22823;&#38646;&#30005;&#21387;&#24320;&#20851;&#33539;&#22260;&#24182;&#26174;&#33879;&#25552;&#39640;&#21151;&#29575;&#25928;&#29575;&#12290;&#24403;&#21069;&#65292;&#24403;DAB&#21464;&#25442;&#22120;&#22312;TPS&#35843;&#21046;&#19979;&#26102;&#65292;&#30005;&#27969;&#24212;&#21147;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#35201;&#22312;DAB&#21464;&#25442;&#22120;&#22312;TPS&#35843;&#21046;&#19979;&#26368;&#23567;&#21270;&#30005;&#27969;&#24212;&#21147;&#65292;&#20998;&#26512;&#36807;&#31243;&#21644;&#23454;&#29616;&#36807;&#31243;&#20013;&#23384;&#22312;&#20004;&#20010;&#22256;&#38590;&#12290;&#39318;&#20808;&#65292;TPS&#35843;&#21046;&#20013;&#30340;&#19977;&#20010;&#35843;&#21046;&#21464;&#37327;&#23545;&#19981;&#21516;&#25805;&#20316;&#27169;&#24335;&#19979;&#30340;&#30005;&#27969;&#24212;&#21147;&#20998;&#26512;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#36825;&#20010;&#20998;&#26512;&#21644;&#25512;&#23548;&#36807;&#31243;&#23548;&#33268;&#20102;&#37325;&#35745;&#31639;&#36127;&#33655;&#65292;&#24182;&#19988;&#21463;&#21040;&#20302;&#31934;&#24230;&#30340;&#22256;&#25200;&#12290;&#20854;&#27425;&#65292;&#35201;&#23454;&#29616;TPS&#35843;&#21046;&#65292;&#22914;&#26524;&#20351;&#29992;&#26597;&#25214;&#34920;&#65292;&#21017;&#38656;&#35201;&#36739;&#22823;&#30340;&#23384;&#20648;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dual active bridge (DAB) converter has been popular in many applications for its outstanding power density and bidirectional power transfer capacity. Up to now, triple phase shift (TPS) can be considered as one of the most advanced modulation techniques for DAB converter. It can widen zero voltage switching range and improve power efficiency significantly. Currently, current stress of the DAB converter has been an important performance indicator when TPS modulation is applied for smaller size and higher efficiency. However, to minimize the current stress when the DAB converter is under TPS modulation, two difficulties exist in analysis process and realization process, respectively. Firstly, three degrees of modulation variables in TPS modulation bring challenges to the analysis of current stress in different operating modes. This analysis and deduction process leads to heavy computational burden and also suffers from low accuracy. Secondly, to realize TPS modulation, if a lookup ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#28151;&#21512;&#25193;&#23637;&#30456;&#31227;&#35843;&#21046;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#21452;&#26377;&#28304;&#26725;&#21464;&#25442;&#22120;&#30340;&#20840;&#38646;&#30005;&#21387;&#24320;&#20851;&#33539;&#22260;&#21644;&#26368;&#20248;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.00381</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#28151;&#21512;&#25193;&#23637;&#30456;&#31227;&#35843;&#21046;&#23545;&#21452;&#26377;&#28304;&#26725;&#21464;&#25442;&#22120;&#30340;&#20840;&#38646;&#30005;&#21387;&#24320;&#20851;&#33539;&#22260;&#21644;&#26368;&#20248;&#25928;&#29575;&#36827;&#34892;&#21327;&#21516;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Artificial-Intelligence-Based Hybrid Extended Phase Shift Modulation for the Dual Active Bridge Converter with Full ZVS Range and Optimal Efficiency. (arXiv:2308.00381v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#28151;&#21512;&#25193;&#23637;&#30456;&#31227;&#35843;&#21046;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#21452;&#26377;&#28304;&#26725;&#21464;&#25442;&#22120;&#30340;&#20840;&#38646;&#30005;&#21387;&#24320;&#20851;&#33539;&#22260;&#21644;&#26368;&#20248;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#26377;&#28304;&#26725;&#65288;DAB&#65289;&#21464;&#25442;&#22120;&#26159;&#35768;&#22810;&#27969;&#34892;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#26080;&#32447;&#20805;&#30005;&#12289;&#30005;&#21160;&#27773;&#36710;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#65289;&#30340;&#20851;&#38190;&#25903;&#25345;&#25216;&#26415;&#12290;DAB&#21464;&#25442;&#22120;&#30340;&#38646;&#30005;&#21387;&#24320;&#20851;&#33539;&#22260;&#21644;&#25928;&#29575;&#26159;&#20004;&#20010;&#37325;&#35201;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#20026;&#20102;&#36798;&#21040;&#25152;&#38656;&#30340;&#38646;&#30005;&#21387;&#24320;&#20851;&#33539;&#22260;&#21644;&#25928;&#29575;&#65292;&#35843;&#21046;&#24517;&#39035;&#34987;&#31934;&#24515;&#35774;&#35745;&#12290;&#28151;&#21512;&#35843;&#21046;&#32771;&#34385;&#20102;&#22810;&#31181;&#21333;&#19968;&#35843;&#21046;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#32508;&#21512;&#24615;&#33021;&#12290;&#20256;&#32479;&#19978;&#65292;&#20026;&#20102;&#35774;&#35745;&#28151;&#21512;&#35843;&#21046;&#65292;&#20351;&#29992;&#35856;&#27874;&#26041;&#27861;&#25110;&#20998;&#27573;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#37117;&#23384;&#22312;&#27169;&#22411;&#24314;&#31435;&#36807;&#31243;&#32791;&#26102;&#21644;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#28151;&#21512;&#25193;&#23637;&#30456;&#31227;&#65288;HEPS&#65289;&#35843;&#21046;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;HEPS&#35843;&#21046;&#26159;&#20197;&#33258;&#21160;&#21270;&#26041;&#24335;&#24320;&#21457;&#30340;&#65292;&#23427;&#20943;&#36731;&#20102;&#32321;&#29712;&#30340;&#27169;&#22411;&#24314;&#31435;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;&#22312;HEPS&#35843;&#21046;&#20013;&#65292;&#32771;&#34385;&#20102;&#20004;&#31181;EPS&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#25972;&#20010;&#24037;&#20316;&#33539;&#22260;&#20869;&#30340;&#20840;&#38646;&#30005;&#21387;&#24320;&#20851;&#25805;&#20316;&#21644;&#26368;&#20248;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dual active bridge (DAB) converter is the key enabler in many popular applications such as wireless charging, electric vehicle and renewable energy. ZVS range and efficiency are two significant performance indicators for DAB converter. To obtain the desired ZVS and efficiency performance, modulation should be carefully designed. Hybrid modulation considers several single modulation strategies to achieve good comprehensive performance. Conventionally, to design a hybrid modulation, harmonic approach or piecewise approach is used, but they suffer from time-consuming model building process and inaccuracy. Therefore, an artificial-intelligence-based hybrid extended phase shift (HEPS) modulation is proposed. Generally, the HEPS modulation is developed in an automated fashion, which alleviates cumbersome model building process while keeping high model accuracy. In HEPS modulation, two EPS strategies are considered to realize optimal efficiency with full ZVS operation over entire operating ra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.00377</link><description>&lt;p&gt;
&#24102;&#26377;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#30340;&#24418;&#29366;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Shape Completion with Prediction of Uncertain Regions. (arXiv:2308.00377v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#23436;&#25104;&#65292;&#21363;&#20174;&#37096;&#20998;&#35266;&#27979;&#39044;&#27979;&#29289;&#20307;&#30340;&#23436;&#25972;&#20960;&#20309;&#24418;&#29366;&#65292;&#23545;&#20110;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#24403;&#22522;&#20110;&#29289;&#20307;&#24418;&#29366;&#37325;&#24314;&#36827;&#34892;&#35268;&#21010;&#25110;&#23454;&#38469;&#25235;&#21462;&#30340;&#39044;&#27979;&#26102;&#65292;&#25351;&#31034;&#20005;&#37325;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#27169;&#31946;&#30340;&#29289;&#20307;&#35270;&#22270;&#26102;&#65292;&#22312;&#25972;&#20010;&#29289;&#20307;&#37096;&#20998;&#23384;&#22312; irreducible uncertainty &#30340;&#25193;&#23637;&#21306;&#22495;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#37325;&#35201;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#39044;&#27979;&#36825;&#20123;&#19981;&#30830;&#23450;&#21306;&#22495;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#20316;&#20026;&#39044;&#27979;&#23616;&#37096;&#31354;&#38388;&#21344;&#29992;&#30340;&#20219;&#20309;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#20004;&#31181;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#19968;&#20010;&#22522;&#20110;ShapeNet&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#30495;&#23454;&#28210;&#26579;&#30340;&#29289;&#20307;&#35270;&#22270;&#28145;&#24230;&#22270;&#20687;&#21450;&#20854;&#24102;&#26377;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet, of realistically rendered depth images of object views with ground-truth annot
&lt;/p&gt;</description></item><item><title>Fountain&#26159;&#19968;&#20010;&#26234;&#33021;&#19978;&#19979;&#25991;&#21161;&#25163;&#65292;&#23558;&#30693;&#35782;&#34920;&#31034;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#29992;&#20110;&#21046;&#36896;&#39118;&#38505;&#35782;&#21035;&#12290;&#23427;&#36890;&#36807;&#25551;&#36848;&#29616;&#26377;&#35774;&#35745;&#21644;&#27969;&#31243;&#20934;&#21017;&#20197;&#21450;&#25552;&#20986;&#30340;&#20559;&#24046;&#26469;&#24110;&#21161;&#35782;&#21035;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.00364</link><description>&lt;p&gt;
Fountain --&#19968;&#20010;&#26234;&#33021;&#19978;&#19979;&#25991;&#21161;&#25163;&#65292;&#32467;&#21512;&#30693;&#35782;&#34920;&#31034;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#21046;&#36896;&#39118;&#38505;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fountain -- an intelligent contextual assistant combining knowledge representation and language models for manufacturing risk identification. (arXiv:2308.00364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00364
&lt;/p&gt;
&lt;p&gt;
Fountain&#26159;&#19968;&#20010;&#26234;&#33021;&#19978;&#19979;&#25991;&#21161;&#25163;&#65292;&#23558;&#30693;&#35782;&#34920;&#31034;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#29992;&#20110;&#21046;&#36896;&#39118;&#38505;&#35782;&#21035;&#12290;&#23427;&#36890;&#36807;&#25551;&#36848;&#29616;&#26377;&#35774;&#35745;&#21644;&#27969;&#31243;&#20934;&#21017;&#20197;&#21450;&#25552;&#20986;&#30340;&#20559;&#24046;&#26469;&#24110;&#21161;&#35782;&#21035;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#29983;&#20135;&#36807;&#31243;&#20013;&#65292;&#19982;&#25209;&#20934;&#30340;&#35774;&#35745;&#25110;&#27969;&#31243;&#20559;&#31163;&#20250;&#23548;&#33268;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21464;&#21270;&#26377;&#26102;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#20135;&#21697;&#35774;&#35745;&#29305;&#24449;&#25110;&#21046;&#36896;&#36807;&#31243;&#30340;&#36866;&#24212;&#24615;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#24037;&#20316;&#27969;&#31243;&#30340;&#26089;&#26399;&#38454;&#27573;&#35782;&#21035;&#36825;&#20123;&#39118;&#38505;&#65292;&#20197;&#36991;&#20813;&#23548;&#33268;&#20445;&#20462;&#32034;&#36180;&#30340;&#25925;&#38556;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;Fountain&#20316;&#20026;&#19968;&#20010;&#19978;&#19979;&#25991;&#21161;&#25163;&#65292;&#38598;&#25104;&#22312;&#20559;&#24046;&#31649;&#29702;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#35774;&#35745;&#21644;&#27969;&#31243;&#20934;&#21017;&#20197;&#21450;&#25552;&#20986;&#30340;&#20559;&#24046;&#30340;&#25551;&#36848;&#26469;&#24110;&#21161;&#35782;&#21035;&#39118;&#38505;&#12290;&#22312;&#21046;&#36896;&#29615;&#22659;&#20013;&#65292;&#35813;&#21161;&#25163;&#25552;&#20379;&#30340;&#24314;&#35758;&#24517;&#39035;&#26159;&#21487;&#35299;&#37322;&#21644;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#20004;&#20010;&#32452;&#20214;&#30340;&#32467;&#21512;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#65306;1&#65289;&#20026;&#29305;&#23450;&#39046;&#22495;&#35821;&#20041;&#30456;&#20284;&#24615;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21644;2&#65289;&#20197;&#29289;&#26009;&#28165;&#21333;&#12289;&#22833;&#25928;&#27169;&#24335;&#21644;&#25928;&#24212;&#20998;&#26512;&#20026;&#22522;&#30784;&#30340;&#23646;&#24615;&#22270;&#30340;&#30693;&#35782;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deviations from the approved design or processes during mass production can lead to unforeseen risks. However, these changes are sometimes necessary due to changes in the product design characteristics or an adaptation in the manufacturing process. A major challenge is to identify these risks early in the workflow so that failures leading to warranty claims can be avoided. We developed Fountain as a contextual assistant integrated in the deviation management workflow that helps in identifying the risks based on the description of the existing design and process criteria and the proposed deviation. In the manufacturing context, it is important that the assistant provides recommendations that are explainable and consistent. We achieve this through a combination of the following two components 1) language models finetuned for domain specific semantic similarity and, 2) knowledge representation in the form of a property graph derived from the bill of materials, Failure Modes and Effect Ana
&lt;/p&gt;</description></item><item><title>MetaGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#21327;&#20316;&#20013;&#12290;&#23427;&#37319;&#29992;&#20803;&#32534;&#31243;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#65292;&#24182;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36825;&#31181;&#26694;&#26550;&#21033;&#29992;&#20102;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.00352</link><description>&lt;p&gt;
MetaGPT: &#20803;&#32534;&#31243;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. (arXiv:2308.00352v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00352
&lt;/p&gt;
&lt;p&gt;
MetaGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#21327;&#20316;&#20013;&#12290;&#23427;&#37319;&#29992;&#20803;&#32534;&#31243;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#65292;&#24182;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36825;&#31181;&#26694;&#26550;&#21033;&#29992;&#20102;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#65292;&#33258;&#21160;&#20219;&#21153;&#35299;&#20915;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#20219;&#21153;&#19978;&#65292;&#32570;&#20047;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#25506;&#32034;&#21644;&#30740;&#31350;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#31181;&#24187;&#35273;&#22312;&#22810;&#20010;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#26102;&#34987;&#26080;&#38480;&#25918;&#22823;&#65292;&#23548;&#33268;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MetaGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#22312;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#37319;&#29992;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#20316;&#20026;&#20803;&#32534;&#31243;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaGPT&#39318;&#20808;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#65288;SOPs&#65289;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#12290;&#28982;&#21518;&#65292;&#23427;&#36827;&#19968;&#27493;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#19982;&#20154;&#31867;&#19987;&#19994;&#20154;&#21592;&#24179;&#34892;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;MetaGPT&#21033;&#29992;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.Therefore, we introduce MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors. In this way, MetaGPT leverages the assembly line work model to assig
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#23398;&#20064;Green&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#21644;&#33945;&#29305;&#21345;&#27931;&#26679;&#26412;&#36827;&#34892;&#20998;&#21035;&#23398;&#20064;&#21644;&#31215;&#20998;&#36924;&#36817;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#24182;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00350</link><description>&lt;p&gt;
&#20351;&#29992;&#20302;&#31209;&#36817;&#20284;&#26377;&#25928;&#22320;&#23398;&#20064;Green&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning Green's Function Efficiently Using Low-Rank Approximations. (arXiv:2308.00350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#23398;&#20064;Green&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#21644;&#33945;&#29305;&#21345;&#27931;&#26679;&#26412;&#36827;&#34892;&#20998;&#21035;&#23398;&#20064;&#21644;&#31215;&#20998;&#36924;&#36817;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#24182;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;Green&#20989;&#25968;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23398;&#20064;Green&#20989;&#25968;&#30340;&#19968;&#20010;&#23454;&#38469;&#38480;&#21046;&#26159;&#37325;&#22797;&#30340;&#35745;&#31639;&#23494;&#38598;&#30340;&#33945;&#29305;&#21345;&#27931;&#31215;&#20998;&#36924;&#36817;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#20302;&#31209;&#20998;&#35299;&#23398;&#20064;Green&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#21644;&#33945;&#29305;&#21345;&#27931;&#26679;&#26412;&#36827;&#34892;&#31215;&#20998;&#36924;&#36817;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#20887;&#20313;&#35745;&#31639;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;MOD-Net&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;PINNs&#21644;MOD-Net&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the Green's function using deep learning models enables to solve different classes of partial differential equations. A practical limitation of using deep learning for the Green's function is the repeated computationally expensive Monte-Carlo integral approximations. We propose to learn the Green's function by low-rank decomposition, which results in a novel architecture to remove redundant computations by separate learning with domain data for evaluation and Monte-Carlo samples for integral approximation. Using experiments we show that the proposed method improves computational time compared to MOD-Net while achieving comparable accuracy compared to both PINNs and MOD-Net.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#25913;&#21892;&#27169;&#22411;&#30340;&#38450;&#24481;&#20027;&#21160;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;Dirichlet&#20998;&#24067;&#20316;&#20026;&#23376;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#30340;&#20808;&#39564;&#65292;&#24182;&#22312;&#36731;&#37327;&#32423;&#23376;&#27169;&#22411;&#19979;&#24341;&#20837;&#21442;&#25968;&#31354;&#38388;&#30340;&#22810;&#26679;&#24615;&#32422;&#26463;&#65292;&#26500;&#24314;&#22791;&#36873;&#30340;&#38598;&#25104;&#27169;&#22411;&#31354;&#38388;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#21160;&#24577;&#36873;&#21462;&#29305;&#23450;&#30340;&#23376;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00346</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#29992;&#20110;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness. (arXiv:2308.00346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#25913;&#21892;&#27169;&#22411;&#30340;&#38450;&#24481;&#20027;&#21160;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;Dirichlet&#20998;&#24067;&#20316;&#20026;&#23376;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#30340;&#20808;&#39564;&#65292;&#24182;&#22312;&#36731;&#37327;&#32423;&#23376;&#27169;&#22411;&#19979;&#24341;&#20837;&#21442;&#25968;&#31354;&#38388;&#30340;&#22810;&#26679;&#24615;&#32422;&#26463;&#65292;&#26500;&#24314;&#22791;&#36873;&#30340;&#38598;&#25104;&#27169;&#22411;&#31354;&#38388;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#21160;&#24577;&#36873;&#21462;&#29305;&#23450;&#30340;&#23376;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38754;&#20020;&#30528;&#22823;&#37327;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#30340;&#35782;&#21035;&#40065;&#26834;&#24615;&#34180;&#24369;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#26159;&#30001;&#29615;&#22659;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#22122;&#22768;&#20197;&#21450;&#21487;&#33021;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#24341;&#36215;&#30340;&#12290;&#21160;&#24577;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#23545;&#25239;&#24615;&#31034;&#20363;&#25915;&#38450;&#31454;&#36187;&#20013;&#30340;&#38450;&#24481;&#20027;&#21160;&#24615;&#12290;&#19982;&#20197;&#21069;&#20381;&#36182;&#20110;&#36755;&#20837;&#25110;&#20915;&#31574;&#30340;&#21160;&#24577;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#25506;&#32034;&#21160;&#24577;&#23646;&#24615;&#65292;&#20197;&#36827;&#19968;&#27493;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#30333;&#30418;&#25915;&#20987;&#24182;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#23558;Dirichlet&#20998;&#24067;&#20316;&#20026;&#23376;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#30340;&#20808;&#39564;&#65292;&#24182;&#22312;&#36731;&#37327;&#32423;&#23376;&#27169;&#22411;&#19979;&#24341;&#20837;&#21442;&#25968;&#31354;&#38388;&#30340;&#22810;&#26679;&#24615;&#32422;&#26463;&#65292;&#20197;&#26500;&#24314;&#22791;&#36873;&#38598;&#25104;&#27169;&#22411;&#31354;&#38388;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#29305;&#23450;&#30340;&#23376;&#27169;&#22411;&#34987;&#21160;&#24577;&#36873;&#21462;
&lt;/p&gt;
&lt;p&gt;
The deep neural network has attained significant efficiency in image recognition. However, it has vulnerable recognition robustness under extensive data uncertainty in practical applications. The uncertainty is attributed to the inevitable ambient noise and, more importantly, the possible adversarial attack. Dynamic methods can effectively improve the defense initiative in the arms race of attack and defense of adversarial examples. Different from the previous dynamic method depend on input or decision, this work explore the dynamic attributes in model level through dynamic ensemble selection technology to further protect the model from white-box attacks and improve the robustness. Specifically, in training phase the Dirichlet distribution is apply as prior of sub-models' predictive distribution, and the diversity constraint in parameter space is introduced under the lightweight sub-models to construct alternative ensembel model spaces. In test phase, the certain sub-models are dynamic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#21033;&#29992;&#20248;&#21270;&#30340;&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#26469;&#32465;&#26550;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#26059;&#32764;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23545;&#25239;&#36148;&#29255;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#33391;&#22909;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00344</link><description>&lt;p&gt;
&#21033;&#29992;&#20248;&#21270;&#30340;&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#32465;&#26550;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#26059;&#32764;
&lt;/p&gt;
&lt;p&gt;
Kidnapping Deep Learning-based Multirotors using Optimized Flying Adversarial Patches. (arXiv:2308.00344v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#21033;&#29992;&#20248;&#21270;&#30340;&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#26469;&#32465;&#26550;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#26059;&#32764;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23545;&#25239;&#36148;&#29255;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#33391;&#22909;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39134;&#34892;&#26426;&#22120;&#20154;&#65292;&#20363;&#22914;&#22810;&#26059;&#32764;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#22522;&#20110;&#30456;&#26426;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20363;&#22914;&#23039;&#24577;&#20272;&#35745;&#12290;&#22914;&#26524;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#32467;&#26524;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#32570;&#28857;&#65292;&#20363;&#22914;&#36890;&#36807;&#35745;&#31639;&#23567;&#22270;&#29255;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#36148;&#29255;&#65292;&#22312;&#29615;&#22659;&#20013;&#25918;&#32622;&#20197;&#25805;&#32437;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#22270;&#29255;&#23433;&#35013;&#22312;&#33267;&#23569;&#19968;&#20010;&#20854;&#20182;&#39134;&#34892;&#26426;&#22120;&#20154;&#19978;&#65292;&#22240;&#27492;&#21487;&#20197;&#25918;&#32622;&#22312;&#21463;&#23475;&#22810;&#26059;&#32764;&#30340;&#35270;&#37326;&#33539;&#22260;&#20869;&#30340;&#20219;&#20309;&#20301;&#32622;&#12290;&#36890;&#36807;&#24341;&#20837;&#25915;&#20987;&#32773;&#26426;&#22120;&#20154;&#65292;&#25105;&#20204;&#23558;&#31995;&#32479;&#25193;&#23637;&#20026;&#23545;&#25239;&#24615;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#26045;&#26377;&#25928;&#30340;&#25915;&#20987;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#23545;&#25239;&#36148;&#29255;&#21450;&#20854;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#20301;&#32622;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#25239;&#36148;&#29255;&#25968;&#37327;&#22686;&#21152;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Autonomous flying robots, such as multirotors, often rely on deep learning models that makes predictions based on a camera image, e.g. for pose estimation. These models can predict surprising results if applied to input images outside the training domain. This fault can be exploited by adversarial attacks, for example, by computing small images, so-called adversarial patches, that can be placed in the environment to manipulate the neural network's prediction. We introduce flying adversarial patches, where multiple images are mounted on at least one other flying robot and therefore can be placed anywhere in the field of view of a victim multirotor. By introducing the attacker robots, the system is extended to an adversarial multi-robot system. For an effective attack, we compare three methods that simultaneously optimize multiple adversarial patches and their position in the input image. We show that our methods scale well with the number of adversarial patches. Moreover, we demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#31639;&#27861;&#20844;&#24179;&#24615;&#30417;&#25511;&#25193;&#23637;&#21040;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#30417;&#25511;&#21253;&#21547;&#23545;&#20107;&#20214;&#24207;&#21015;&#19978;&#25968;&#20540;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#30340;&#31639;&#26415;&#34920;&#36798;&#24335;&#30340;&#20844;&#24179;&#24615;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00341</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#35266;&#27979;&#26465;&#20214;&#19979;&#30417;&#25511;&#31639;&#27861;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monitoring Algorithmic Fairness under Partial Observations. (arXiv:2308.00341v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#31639;&#27861;&#20844;&#24179;&#24615;&#30417;&#25511;&#25193;&#23637;&#21040;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#30417;&#25511;&#21253;&#21547;&#23545;&#20107;&#20214;&#24207;&#21015;&#19978;&#25968;&#20540;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#30340;&#31639;&#26415;&#34920;&#36798;&#24335;&#30340;&#20844;&#24179;&#24615;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#22312;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#22320;&#24433;&#21709;&#20154;&#31867;&#65292;&#23427;&#20204;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#20445;&#25345;&#20844;&#24179;&#21644;&#26080;&#20559;&#30340;&#35201;&#27714;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#34917;&#20805;&#35774;&#35745;&#26102;&#30340;&#20559;&#24046;&#32531;&#35299;&#25514;&#26045;&#65292;&#36817;&#26399;&#24341;&#20837;&#20102;&#19968;&#20123;&#36816;&#34892;&#26102;&#39564;&#35777;&#25216;&#26415;&#26469;&#30417;&#25511;&#37096;&#32626;&#31995;&#32479;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#20043;&#21069;&#30340;&#30417;&#25511;&#25216;&#26415;&#20551;&#35774;&#23545;&#65288;&#26410;&#30693;&#30340;&#65289;&#34987;&#30417;&#25511;&#31995;&#32479;&#30340;&#29366;&#24577;&#20855;&#26377;&#23436;&#20840;&#21487;&#35266;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21482;&#33021;&#30417;&#25511;&#34987;&#25351;&#23450;&#20026;&#19981;&#21516;&#20107;&#20214;&#30340;&#27010;&#29575;&#30340;&#31639;&#26415;&#34920;&#36798;&#24335;&#30340;&#20844;&#24179;&#24615;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20844;&#24179;&#24615;&#30417;&#25511;&#25193;&#23637;&#21040;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;POMC&#65289;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#38024;&#23545;&#21253;&#21547;&#23545;&#20107;&#20214;&#24207;&#21015;&#19978;&#30340;&#25968;&#20540;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#30340;&#31639;&#26415;&#34920;&#36798;&#24335;&#30340;&#35268;&#33539;&#12290;&#25105;&#20204;&#20165;&#20570;&#20986;&#30340;&#20551;&#35774;&#26159;&#22522;&#30784;POMC&#26159;&#38750;&#21608;&#26399;&#24615;&#30340;&#24182;&#19988;&#36215;&#22987;&#20110;&#31283;&#23450;&#20998;&#24067;&#65292;&#19988;&#23545;&#20110;&#20854;&#28151;&#21512;&#26102;&#38388;&#26377;&#19968;&#20010;&#24050;&#30693;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI and machine-learned software are used increasingly for making decisions that affect humans, it is imperative that they remain fair and unbiased in their decisions. To complement design-time bias mitigation measures, runtime verification techniques have been introduced recently to monitor the algorithmic fairness of deployed systems. Previous monitoring techniques assume full observability of the states of the (unknown) monitored system. Moreover, they can monitor only fairness properties that are specified as arithmetic expressions over the probabilities of different events. In this work, we extend fairness monitoring to systems modeled as partially observed Markov chains (POMC), and to specifications containing arithmetic expressions over the expected values of numerical functions on event sequences. The only assumptions we make are that the underlying POMC is aperiodic and starts in the stationary distribution, with a bound on its mixing time being known. These assumptions enab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#24322;&#26500;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#20986;&#30340;&#31574;&#30053;&#33021;&#22815;&#22312;&#36855;&#23467;&#33324;&#30340;&#30719;&#20117;&#29615;&#22659;&#20013;&#25628;&#32034;&#24182;&#23548;&#33322;&#21040;&#26410;&#30693;&#30446;&#26631;&#20301;&#32622;&#12290;&#24341;&#20837;&#22810;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#22909;&#22855;&#27169;&#22359;&#21152;&#36895;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.00331</link><description>&lt;p&gt;
&#24322;&#26500;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#30446;&#26631;&#25628;&#32034;&#21644;&#23548;&#33322;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Target Search and Navigation in Heterogeneous Robot Systems with Deep Reinforcement Learning. (arXiv:2308.00331v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#24322;&#26500;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#20986;&#30340;&#31574;&#30053;&#33021;&#22815;&#22312;&#36855;&#23467;&#33324;&#30340;&#30719;&#20117;&#29615;&#22659;&#20013;&#25628;&#32034;&#24182;&#23548;&#33322;&#21040;&#26410;&#30693;&#30446;&#26631;&#20301;&#32622;&#12290;&#24341;&#20837;&#22810;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#22909;&#22855;&#27169;&#22359;&#21152;&#36895;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#30340;&#24322;&#26500;&#26426;&#22120;&#20154;&#31995;&#32479;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#30446;&#26631;&#25628;&#32034;&#21644;&#23548;&#33322;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#26080;&#20154;&#26426;&#21644;&#26080;&#20154;&#36710;&#32452;&#25104;&#30340;&#24322;&#26500;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#25628;&#32034;&#21644;&#25937;&#25588;&#20219;&#21153;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#20986;&#30340;&#31574;&#30053;&#22312;&#31867;&#20284;&#36855;&#23467;&#30340;&#30719;&#20117;&#29615;&#22659;&#20013;&#25628;&#32034;&#30446;&#26631;&#24182;&#23548;&#33322;&#21040;&#30446;&#26631;&#20301;&#32622;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22914;&#26524;&#21516;&#26102;&#35757;&#32451;&#20004;&#20010;&#26426;&#22120;&#20154;&#65292;&#21487;&#33021;&#26080;&#27861;&#27491;&#30830;&#33719;&#24471;&#19982;&#21327;&#20316;&#30456;&#20851;&#30340;&#22870;&#21169;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#19968;&#20010;&#22909;&#22855;&#27169;&#22359;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#25506;&#32034;&#26410;&#35775;&#38382;&#30340;&#29615;&#22659;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35757;&#32451;&#20986;&#24322;&#26500;&#26426;&#22120;&#20154;&#31995;&#32479;&#23454;&#29616;&#26410;&#30693;&#30446;&#26631;&#20301;&#32622;&#30340;&#25628;&#32034;&#21644;&#23548;&#33322;&#65292;&#32780;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20570;&#21040;&#65292;&#24182;&#19988;&#21152;&#36895;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative heterogeneous robot systems can greatly improve the efficiency of target search and navigation tasks. In this paper, we design a heterogeneous robot system consisting of a UAV and a UGV for search and rescue missions in unknown environments. The system is able to search for targets and navigate to them in a maze-like mine environment with the policies learned through deep reinforcement learning algorithms. During the training process, if two robots are trained simultaneously, the rewards related to their collaboration may not be properly obtained. Hence, we introduce a multi-stage reinforcement learning framework and a curiosity module to encourage agents to explore unvisited environments. Experiments in simulation environments show that our framework can train the heterogeneous robot system to achieve the search and navigation with unknown target locations while existing baselines may not, and accelerate the training speed.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38408;&#20540;&#24863;&#30693;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21464;&#37327;&#20998;&#37197;&#29575;&#26469;&#25552;&#39640;&#21487;&#34892;&#35299;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.00327</link><description>&lt;p&gt;
&#38024;&#23545;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#29983;&#25104;&#21487;&#34892;&#35299;&#30340;&#38408;&#20540;&#24863;&#30693;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Threshold-aware Learning to Generate Feasible Solutions for Mixed Integer Programs. (arXiv:2308.00327v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00327
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38408;&#20540;&#24863;&#30693;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21464;&#37327;&#20998;&#37197;&#29575;&#26469;&#25552;&#39640;&#21487;&#34892;&#35299;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20854;&#20855;&#26377;&#31163;&#25955;&#30340;&#29305;&#24615;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34987;&#29992;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#31070;&#32463;&#32593;&#32476;&#19979;&#28508; (Neural diving&#65292;ND) &#26159;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#37096;&#20998;&#31163;&#25955;&#21464;&#37327;&#36171;&#20540;&#12290;&#28982;&#32780;&#65292;ND&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#65292;&#21363;&#21464;&#37327;&#20540;&#20998;&#31867;&#20934;&#30830;&#24230;&#19982;&#21407;&#22987;&#35299;&#30340;&#30028;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29305;&#23450;&#33539;&#22260;&#30340;&#21464;&#37327;&#20998;&#37197;&#29575;&#65288;&#35206;&#30422;&#29575;&#65289;&#20250;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21487;&#34892;&#35299;&#65292;&#25105;&#20204;&#24314;&#35758;&#20248;&#21270;&#35206;&#30422;&#29575;&#26469;&#24357;&#34917;&#23398;&#20064;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#35206;&#30422;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#24605;&#24819;&#26159;&#20849;&#21516;&#23398;&#20064;&#38480;&#21046;&#35206;&#30422;&#29575;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a high-quality feasible solution to a combinatorial optimization (CO) problem in a limited time is challenging due to its discrete nature. Recently, there has been an increasing number of machine learning (ML) methods for addressing CO problems. Neural diving (ND) is one of the learning-based approaches to generating partial discrete variable assignments in Mixed Integer Programs (MIP), a framework for modeling CO problems. However, a major drawback of ND is a large discrepancy between the ML and MIP objectives, i.e., variable value classification accuracy over primal bound. Our study investigates that a specific range of variable assignment rates (coverage) yields high-quality feasible solutions, where we suggest optimizing the coverage bridges the gap between the learning and MIP objectives. Consequently, we introduce a post-hoc method and a learning-based approach for optimizing the coverage. A key idea of our approach is to jointly learn to restrict the coverage search spac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#30340;DQN&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#65292;&#35813;&#27169;&#22411;&#30340;&#24179;&#22343;&#22238;&#21512;&#22870;&#21169;&#20026;46.16&#65292;&#22312;20,000&#27425;&#22238;&#21512;&#20869;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.00318</link><description>&lt;p&gt;
&#20687;&#32032;&#21040;&#31574;&#30053;&#65306;&#29992;&#20110;&#20869;&#37096;&#21644;&#36328;&#28216;&#25103;&#24378;&#21270;&#23398;&#20064;&#30340;DQN&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pixel to policy: DQN Encoders for within &amp; cross-game reinforcement learning. (arXiv:2308.00318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#30340;DQN&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#65292;&#35813;&#27169;&#22411;&#30340;&#24179;&#22343;&#22238;&#21512;&#22870;&#21169;&#20026;46.16&#65292;&#22312;20,000&#27425;&#22238;&#21512;&#20869;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#29615;&#22659;&#12290;&#35768;&#22810;&#36825;&#20123;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#30340;&#20849;&#20139;&#32467;&#26500;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#32467;&#26500;&#25552;&#39640;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;RL&#24615;&#33021;&#12290;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#20849;&#20139;&#32467;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#20043;&#38388;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#25913;&#36827;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#24182;&#27604;&#36739;&#20102;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;RL&#27169;&#22411;&#21644;&#36801;&#31227;&#23398;&#20064;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#22312;&#22810;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#24320;&#21457;&#36890;&#29992;&#30340;&#28216;&#25103;&#20195;&#29702;&#20197;&#21450;&#20351;&#29992;DQN&#23545;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#22312;&#30456;&#21516;&#25110;&#19981;&#21516;&#28216;&#25103;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;DQN&#27169;&#22411;&#22312;&#24179;&#22343;&#22238;&#21512;&#22870;&#21169;&#19978;&#36798;&#21040;&#20102;46.16&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#34920;&#29616;&#65292;&#20165;&#20165;&#20351;&#29992;&#20102;20,000&#27425;&#22238;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning can be applied to various tasks, and environments. Many of these environments have a similar shared structure, which can be exploited to improve RL performance on other tasks. Transfer learning can be used to take advantage of this shared structure, by learning policies that are transferable across different tasks and environments and can lead to more efficient learning as well as improved performance on a wide range of tasks. This work explores as well as compares the performance between RL models being trained from the scratch and on different approaches of transfer learning. Additionally, the study explores the performance of a model trained on multiple game environments, with the goal of developing a universal game-playing agent as well as transfer learning a pre-trained encoder using DQN, and training it on the same game or a different game. Our DQN model achieves a mean episode reward of 46.16 which even beats the human-level performance with merely 20k epi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#25991;&#26412;VQA&#20013;&#23545;&#35270;&#35273;&#29305;&#24449;&#29702;&#35299;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;TextVQA&#21644;VQA&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00295</link><description>&lt;p&gt;
&#35753;Text-VQA&#20013;&#30340;V&#21464;&#24471;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Making the V in Text-VQA Matter. (arXiv:2308.00295v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#25991;&#26412;VQA&#20013;&#23545;&#35270;&#35273;&#29305;&#24449;&#29702;&#35299;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;TextVQA&#21644;VQA&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;VQA&#26088;&#22312;&#36890;&#36807;&#38405;&#35835;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#19982;VQA&#20219;&#21153;&#30456;&#27604;&#65292;&#23427;&#38656;&#35201;&#22823;&#37327;&#30340;&#22330;&#26223;-&#25991;&#26412;&#20851;&#31995;&#29702;&#35299;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#26356;&#20851;&#27880;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#65292;&#32780;&#23545;&#20110;&#35270;&#35273;&#29305;&#24449;&#21017;&#32473;&#20104;&#36739;&#23569;&#37325;&#35270;&#65292;&#32780;&#19988;&#26377;&#20123;&#38382;&#39064;&#19981;&#38656;&#35201;&#29702;&#35299;&#22270;&#20687;&#12290;&#30001;&#20110;&#32570;&#20047;&#23545;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20250;&#39044;&#27979;&#20986;&#26377;&#20559;&#24046;&#30340;&#31572;&#26696;&#12290;&#20363;&#22914;&#65292;&#22312;&#31867;&#20284;&#8220;&#26631;&#29260;&#19978;&#20889;&#30528;&#20160;&#20040;&#65311;&#8221;&#30340;&#38382;&#39064;&#20013;&#65292;&#27169;&#22411;&#39044;&#27979;&#30340;&#31572;&#26696;&#24635;&#26159;&#8220;STOP&#8221;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#24573;&#30053;&#20102;&#22270;&#20687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQA&#25968;&#25454;&#38598;&#20316;&#20026;Text-VQA&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#65288;&#35753;V&#22312;Text-VQA&#20013;&#21464;&#24471;&#37325;&#35201;&#65289;&#65292;&#20197;&#21450;OCR&#29305;&#24449;&#21644;&#38382;&#39064;&#29305;&#24449;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;TextVQA&#25968;&#25454;&#38598;&#21644;VQA&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#24182;&#65292;&#24182;&#22312;&#36825;&#20010;&#21512;&#24182;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based VQA aims at answering questions by reading the text present in the images. It requires a large amount of scene-text relationship understanding compared to the VQA task. Recent studies have shown that the question-answer pairs in the dataset are more focused on the text present in the image but less importance is given to visual features and some questions do not require understanding the image. The models trained on this dataset predict biased answers due to the lack of understanding of visual context. For example, in questions like "What is written on the signboard?", the answer predicted by the model is always "STOP" which makes the model to ignore the image. To address these issues, we propose a method to learn visual features (making V matter in TextVQA) along with the OCR features and question features using VQA dataset as external knowledge for Text-based VQA. Specifically, we combine the TextVQA dataset and VQA dataset and train the model on this combined dataset. Suc
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#35270;&#35273;Transformer&#30340;&#26089;&#26399;&#36864;&#20986;&#26694;&#26550;LGViT&#65292;&#36890;&#36807;&#34701;&#21512;&#23616;&#37096;&#24863;&#30693;&#22836;&#37096;&#21644;&#20840;&#23616;&#32858;&#21512;&#22836;&#37096;&#65292;&#20197;&#35299;&#20915;ViTs&#20013;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#24212;&#29992;&#24102;&#26469;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.00255</link><description>&lt;p&gt;
LGViT: &#21152;&#36895;&#35270;&#35273;Transformer&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;
&lt;/p&gt;
&lt;p&gt;
LGViT: Dynamic Early Exiting for Accelerating Vision Transformer. (arXiv:2308.00255v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00255
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#35270;&#35273;Transformer&#30340;&#26089;&#26399;&#36864;&#20986;&#26694;&#26550;LGViT&#65292;&#36890;&#36807;&#34701;&#21512;&#23616;&#37096;&#24863;&#30693;&#22836;&#37096;&#21644;&#20840;&#23616;&#32858;&#21512;&#22836;&#37096;&#65292;&#20197;&#35299;&#20915;ViTs&#20013;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#24212;&#29992;&#24102;&#26469;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#21152;&#36895;&#24378;&#22823;&#30340;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#20197;&#25552;&#20379;&#22810;&#23186;&#20307;&#26381;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#24050;&#25104;&#20026;&#21560;&#24341;&#20154;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#26089;&#26399;&#36864;&#20986;&#26159;&#21152;&#36895;&#25512;&#29702;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22823;&#37096;&#20998;&#30740;&#31350;&#19987;&#27880;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;Transformer&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#23558;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#24212;&#29992;&#20110;ViTs&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;ViTs&#20013;&#26089;&#26399;&#36864;&#20986;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#27973;&#23618;&#20869;&#37096;&#20998;&#31867;&#22120;&#20013;&#19981;&#36275;&#30340;&#29305;&#24449;&#34920;&#31034;&#21644;&#28145;&#23618;&#20869;&#37096;&#20998;&#31867;&#22120;&#20013;&#25429;&#33719;&#30446;&#26631;&#35821;&#20041;&#20449;&#24687;&#30340;&#33021;&#21147;&#26377;&#38480;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;ViTs&#30340;&#26089;&#26399;&#36864;&#20986;&#26694;&#26550;&#65292;&#31216;&#20026;LGViT&#65292;&#23427;&#34701;&#21512;&#20102;&#24322;&#26500;&#30340;&#36864;&#20986;&#22836;&#37096;&#65292;&#21363;&#23616;&#37096;&#24863;&#30693;&#22836;&#37096;&#21644;&#20840;&#23616;&#32858;&#21512;&#22836;&#37096;&#65292;&#20197;&#26356;&#22909;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the efficient deployment and acceleration of powerful vision transformers (ViTs) on resource-limited edge devices for providing multimedia services have become attractive tasks. Although early exiting is a feasible solution for accelerating inference, most works focus on convolutional neural networks (CNNs) and transformer models in natural language processing (NLP).Moreover, the direct application of early exiting methods to ViTs may result in substantial performance degradation. To tackle this challenge, we systematically investigate the efficacy of early exiting in ViTs and point out that the insufficient feature representations in shallow internal classifiers and the limited ability to capture target semantic information in deep internal classifiers restrict the performance of these methods. We then propose an early exiting framework for general ViTs termed LGViT, which incorporates heterogeneous exiting heads, namely, local perception head and global aggregation head, to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EEG&#30340;&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#29305;&#24449;&#25513;&#34109;&#33258;&#32534;&#30721;&#21644;&#24773;&#32490;&#36801;&#31227;&#23398;&#20064;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.00246</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#25513;&#34109;&#33258;&#32534;&#30721;&#21644;&#24773;&#32490;&#36801;&#31227;&#23398;&#20064;&#30340;&#22522;&#20110;EEG&#30340;&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning. (arXiv:2308.00246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EEG&#30340;&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#29305;&#24449;&#25513;&#34109;&#33258;&#32534;&#30721;&#21644;&#24773;&#32490;&#36801;&#31227;&#23398;&#20064;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#36127;&#33655;&#26159;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#24515;&#29702;&#21162;&#21147;&#37327;&#65292;&#22312;&#24615;&#33021;&#21644;&#20915;&#31574;&#32467;&#26524;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#22312;&#21508;&#31181;&#25935;&#24863;&#39046;&#22495;&#20013;&#20998;&#31867;&#21644;&#20998;&#26512;&#35748;&#30693;&#36127;&#33655;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20998;&#31867;&#35748;&#30693;&#36127;&#33655;&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#21033;&#29992;&#24773;&#32490;&#21644;&#35748;&#30693;&#36127;&#33655;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#30417;&#30563;&#25513;&#34109;&#33258;&#32534;&#30721;&#22312;&#24773;&#32490;&#30456;&#20851;&#30340;EEG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20923;&#32467;&#26435;&#37325;&#21644;&#24494;&#35843;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#36827;&#34892;&#19979;&#28216;&#30340;&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21033;&#29992;&#20102;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20110;EEG&#30340;&#24773;&#32490;&#25968;&#25454;&#38598;&#65292;&#21363;SEED&#21644;SEED-IV&#65292;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21516;&#26102;&#25105;&#20204;&#20351;&#29992;CL-Drive&#25968;&#25454;&#38598;&#36827;&#34892;&#19979;&#28216;&#30340;&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive load, the amount of mental effort required for task completion, plays an important role in performance and decision-making outcomes, making its classification and analysis essential in various sensitive domains. In this paper, we present a new solution for the classification of cognitive load using electroencephalogram (EEG). Our model uses a transformer architecture employing transfer learning between emotions and cognitive load. We pre-train our model using self-supervised masked autoencoding on emotion-related EEG datasets and use transfer learning with both frozen weights and fine-tuning to perform downstream cognitive load classification. To evaluate our method, we carry out a series of experiments utilizing two publicly available EEG-based emotion datasets, namely SEED and SEED-IV, for pre-training, while we use the CL-Drive dataset for downstream cognitive load classification. The results of our experiments show that our proposed approach achieves strong results and ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#20197;&#20351;&#29992;&#21069;&#21021;&#22987;&#21270;&#38169;&#35823;&#20026;&#26696;&#20363;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#38745;&#24577;&#20998;&#26512;&#30340;&#24320;&#25918;&#31354;&#38388;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#20195;&#29702;&#31243;&#24207;LLift&#65292;&#35813;&#31243;&#24207;&#33021;&#22815;&#20811;&#26381;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#38169;&#35823;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2308.00245</link><description>&lt;p&gt;
&#31243;&#24207;&#20998;&#26512;&#25351;&#21335;&#65306;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20849;&#36827;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models. (arXiv:2308.00245v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#20197;&#20351;&#29992;&#21069;&#21021;&#22987;&#21270;&#38169;&#35823;&#20026;&#26696;&#20363;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#38745;&#24577;&#20998;&#26512;&#30340;&#24320;&#25918;&#31354;&#38388;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#20195;&#29702;&#31243;&#24207;LLift&#65292;&#35813;&#31243;&#24207;&#33021;&#22815;&#20811;&#26381;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#38169;&#35823;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24577;&#20998;&#26512;&#26159;&#36719;&#20214;&#24037;&#31243;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20943;&#36731;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#22312;&#31934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#23454;&#29616;&#24494;&#22937;&#24179;&#34913;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#30001;&#20110;&#26368;&#36817;&#30340;&#36827;&#23637;&#22312;&#29702;&#35299;&#12289;&#29983;&#25104;&#29978;&#33267;&#35843;&#35797;&#20195;&#30721;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38169;&#35823;&#30340;&#36923;&#36753;&#21487;&#33021;&#24456;&#22797;&#26434;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#25512;&#29702;&#21644;&#36328;&#22810;&#20010;&#20989;&#25968;&#30340;&#22823;&#33539;&#22260;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#19968;&#28857;&#19978;&#65292;LLM&#26356;&#36866;&#21512;&#22312;&#36741;&#21161;&#35282;&#33394;&#20013;&#19982;&#38745;&#24577;&#20998;&#26512;&#30456;&#34917;&#20805;&#20351;&#29992;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#20197;&#20351;&#29992;&#21069;&#21021;&#22987;&#21270;&#65288;UBI&#65289;&#38169;&#35823;&#20026;&#26696;&#20363;&#30740;&#31350;&#30340;LLM&#36741;&#21161;&#38745;&#24577;&#20998;&#26512;&#30340;&#24320;&#25918;&#31354;&#38388;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;LLift&#65292;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#21487;&#20197;&#19982;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#21644;LLM&#36827;&#34892;&#20132;&#20114;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#20195;&#29702;&#31243;&#24207;&#21644;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#35768;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#38024;&#23545;&#20855;&#20307;&#38169;&#35823;&#30340;&#24314;&#27169;&#65292;
&lt;/p&gt;
&lt;p&gt;
Static analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Models (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated agent that interfaces with both a static analysis tool and an LLM. By carefully designing the agent and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, 
&lt;/p&gt;</description></item><item><title>Capsa&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#25193;&#23637;&#20855;&#26377;&#39118;&#38505;&#24863;&#30693;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#23427;&#33021;&#22815;&#37327;&#21270;&#22810;&#31181;&#24418;&#24335;&#30340;&#39118;&#38505;&#65292;&#24182;&#23558;&#19981;&#21516;&#31639;&#27861;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#24182;&#34892;&#37327;&#21270;&#19981;&#21516;&#30340;&#39118;&#38505;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;&#22797;&#26434;&#24863;&#30693;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#24182;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;capsa&#30340;&#26377;&#25928;&#24615;&#12290;capsa&#33021;&#22815;&#36731;&#26494;&#32452;&#21512;aleatoric&#19981;&#30830;&#23450;&#24615;&#12289;epistemic&#19981;&#30830;&#23450;&#24615;&#21644;&#20559;&#35265;&#20272;&#35745;&#33021;&#21147;</title><link>http://arxiv.org/abs/2308.00231</link><description>&lt;p&gt;
Capsa: &#29992;&#20110;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39118;&#38505;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks. (arXiv:2308.00231v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00231
&lt;/p&gt;
&lt;p&gt;
Capsa&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#25193;&#23637;&#20855;&#26377;&#39118;&#38505;&#24863;&#30693;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#23427;&#33021;&#22815;&#37327;&#21270;&#22810;&#31181;&#24418;&#24335;&#30340;&#39118;&#38505;&#65292;&#24182;&#23558;&#19981;&#21516;&#31639;&#27861;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#24182;&#34892;&#37327;&#21270;&#19981;&#21516;&#30340;&#39118;&#38505;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;&#22797;&#26434;&#24863;&#30693;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#24182;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;capsa&#30340;&#26377;&#25928;&#24615;&#12290;capsa&#33021;&#22815;&#36731;&#26494;&#32452;&#21512;aleatoric&#19981;&#30830;&#23450;&#24615;&#12289;epistemic&#19981;&#30830;&#23450;&#24615;&#21644;&#20559;&#35265;&#20272;&#35745;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20063;&#24120;&#24120;&#20986;&#29616;&#31361;&#28982;&#12289;&#24847;&#22806;&#19988;&#28798;&#38590;&#24615;&#30340;&#22833;&#36133;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;&#29616;&#26377;&#30340;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#39118;&#38505;&#24863;&#30693;&#30340;&#31639;&#27861;&#22797;&#26434;&#32780;&#20020;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#37325;&#22823;&#30340;&#24037;&#31243;&#25913;&#21464;&#65292;&#36890;&#24120;&#21482;&#38024;&#23545;&#29305;&#23450;&#35774;&#32622;&#36827;&#34892;&#24320;&#21457;&#65292;&#24182;&#19988;&#24456;&#38590;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;capsa&#65292;&#19968;&#20010;&#29992;&#20110;&#25193;&#23637;&#20855;&#26377;&#39118;&#38505;&#24863;&#30693;&#30340;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;Capsa&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#22810;&#31181;&#24418;&#24335;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#19981;&#21516;&#30340;&#31639;&#27861;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#24182;&#34892;&#37327;&#21270;&#19981;&#21516;&#30340;&#39118;&#38505;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;capsa&#26694;&#26550;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#24182;&#22312;&#22797;&#26434;&#24863;&#30693;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;capsa&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;capsa&#36731;&#26494;&#32452;&#21512;aleatoric&#19981;&#30830;&#23450;&#24615;&#12289;epistemic&#19981;&#30830;&#23450;&#24615;&#21644;&#20559;&#35265;&#20272;&#35745;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The modern pervasiveness of large-scale deep neural networks (NNs) is driven by their extraordinary performance on complex problems but is also plagued by their sudden, unexpected, and often catastrophic failures, particularly on challenging scenarios. Existing algorithms that provide risk-awareness to NNs are complex and ad-hoc. Specifically, these methods require significant engineering changes, are often developed only for particular settings, and are not easily composable. Here we present capsa, a framework for extending models with risk-awareness. Capsa provides a methodology for quantifying multiple forms of risk and composing different algorithms together to quantify different risk metrics in parallel. We validate capsa by implementing state-of-the-art uncertainty estimation algorithms within the capsa framework and benchmarking them on complex perception datasets. We demonstrate capsa's ability to easily compose aleatoric uncertainty, epistemic uncertainty, and bias estimation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21644;&#21442;&#25968;&#21270;&#24314;&#27169;&#19982;BIM&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#24314;&#31569;&#35774;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20419;&#36827;&#24314;&#31569;&#24072;&#19982;AI&#30340;&#21512;&#20316;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#35774;&#35745;&#24605;&#36335;&#21644;&#29983;&#25104;&#21019;&#36896;&#24615;&#35774;&#35745;&#26469;&#25552;&#21319;&#35774;&#35745;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21327;&#20316;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00227</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;AI&#39537;&#21160;&#30340;&#21442;&#25968;&#21270;&#24314;&#27169;&#21644;BIM&#36827;&#34892;&#24314;&#31569;&#35774;&#35745;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Experiments on Generative AI-Powered Parametric Modeling and BIM for Architectural Design. (arXiv:2308.00227v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21644;&#21442;&#25968;&#21270;&#24314;&#27169;&#19982;BIM&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#24314;&#31569;&#35774;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20419;&#36827;&#24314;&#31569;&#24072;&#19982;AI&#30340;&#21512;&#20316;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#35774;&#35745;&#24605;&#36335;&#21644;&#29983;&#25104;&#21019;&#36896;&#24615;&#35774;&#35745;&#26469;&#25552;&#21319;&#35774;&#35745;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21327;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24314;&#31569;&#35774;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;ChatGPT&#21644;Veras&#31561;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#19982;&#21442;&#25968;&#21270;&#24314;&#27169;&#21644;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#65288;BIM&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#21319;&#35774;&#35745;&#36807;&#31243;&#12290;&#30740;&#31350;&#23454;&#39564;&#20102;ChatGPT&#21644;&#29983;&#25104;&#24335;AI&#22312;&#19977;&#32500;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#28508;&#21147;&#65292;&#36229;&#36234;&#20102;&#20854;&#22312;&#25991;&#26412;&#21644;&#20108;&#32500;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#24314;&#31569;&#24072;&#19982;AI&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#20415;&#20110;&#24555;&#36895;&#25506;&#32034;&#35774;&#35745;&#24605;&#36335;&#65292;&#24182;&#20135;&#29983;&#19982;&#29615;&#22659;&#30456;&#20851;&#30340;&#21019;&#36896;&#24615;&#35774;&#35745;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;ChatGPT&#29992;&#20110;&#33050;&#26412;&#32534;&#20889;&#21644;Veras&#29992;&#20110;&#29983;&#25104;&#35774;&#35745;&#24605;&#36335;&#65292;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#21270;&#24314;&#27169;&#21644;BIM&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#35813;&#26694;&#26550;&#20026;&#24314;&#31569;&#24072;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#26469;&#20256;&#36798;&#35774;&#35745;&#24847;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#12289;&#21019;&#36896;&#24615;&#21644;&#21327;&#20316;&#24615;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new architectural design framework that utilizes generative AI tools including ChatGPT and Veras with parametric modeling and Building Information Modeling (BIM) to enhance the design process. The study experiments with the potential of ChatGPT and generative AI in 3D architectural design, extending beyond its use in text and 2D image generation. The proposed framework promotes collaboration between architects and AI, facilitating a quick exploration of design ideas and producing context-sensitive, creative design generation. By integrating ChatGPT for scripting and Veras for generating design ideas with widely used parametric modeling and BIM tools, the framework provides architects with an intuitive and powerful method to convey design intent, leading to more efficient, creative, and collaborative design processes.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.00225</link><description>&lt;p&gt;
&#34987;&#25351;&#23548;&#30340;&#20559;&#35265;&#65306;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. (arXiv:2308.00225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00225
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25351;&#23548;&#35843;&#20248;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36825;&#20123;&#35843;&#20248;&#26041;&#27861;&#21487;&#20197;&#20351;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#20294;&#25105;&#20204;&#25512;&#27979;&#36825;&#20123;&#32463;&#36807;&#35843;&#20248;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#22810;&#38544;&#21547;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#20123;&#32463;&#36807;&#35843;&#20248;&#30340;&#27169;&#22411;&#21576;&#29616;&#20986;&#20808;&#21069;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#25110;&#36739;&#19981;&#26126;&#26174;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#35748;&#30693;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21253;&#25324;&#30683;&#30462;&#25928;&#24212;&#12289;&#30830;&#23450;&#24615;&#25928;&#24212;&#21644;&#20449;&#24565;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#24050;&#34987;&#35777;&#23454;&#23545;&#20154;&#31867;&#30340;&#20915;&#31574;&#21644;&#25512;&#29702;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#36825;&#20123;&#20559;&#35265;&#22312;&#21508;&#31181;&#27169;&#22411;&#20013;&#30340;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#27169;&#22411;&#65292;&#22914;Flan-T5&#12289;GPT3.5&#21644;GPT4&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#29702;&#35299;&#25351;&#23548;&#35843;&#20248;&#30340;LMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#26377;&#21161;&#20110;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically. While these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. Our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. We examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. This research constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#8212;&#8212;COLOR&#65292;&#21487;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#31561;&#21151;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#26174;&#31034;&#25104;&#21151;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#25928;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#30340;&#21453;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.00221</link><description>&lt;p&gt;
&#36229;&#36234;&#35782;&#21035;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Advancing Beyond Identification: Multi-bit Watermark for Language Models. (arXiv:2308.00221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#8212;&#8212;COLOR&#65292;&#21487;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#31561;&#21151;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#26174;&#31034;&#25104;&#21151;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#25928;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#30340;&#21453;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#31215;&#26497;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#28389;&#29992;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#26816;&#27979;&#65292;&#20294;&#26576;&#20123;&#24694;&#24847;&#28389;&#29992;&#38656;&#35201;&#36319;&#36394;&#23545;&#25163;&#29992;&#25143;&#20197;&#36827;&#34892;&#21453;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22810;&#20301;&#27700;&#21360;&#36890;&#36807;&#39068;&#33394;&#32534;&#30721;&#8221;&#65288;COLOR&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#12290;&#21033;&#29992;&#38646;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#20248;&#21183;&#65288;Kirchenbauer&#31561;&#65292;2023a&#65289;&#65292;COLOR&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#65288;&#32422;500&#20010;&#26631;&#35760;&#65289;&#20013;&#25104;&#21151;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#25928;&#22320;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#36827;&#34892;&#21453;&#21046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to proactively tackle misuse of large language models beyond identification of machine-generated text. While existing methods focus on detection, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose "Multi-bit Watermark through Color-listing" (COLOR), embedding traceable multi-bit information during language model generation. Leveraging the benefits of zero-bit watermarking (Kirchenbauer et al., 2023a), COLOR enables extraction without model access, on-the-fly embedding, and maintains text quality, while allowing zero-bit detection all at the same time. Preliminary experiments demonstrates successful embedding of 32-bit messages with 91.9% accuracy in moderate-length texts ($\sim$500 tokens). This work advances strategies to counter language model misuse effectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#27744;&#35843;&#25511;&#20998;&#23618;V2G&#21327;&#35843;&#31574;&#30053;&#65292;&#26088;&#22312;&#20419;&#36827;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#21644;&#30005;&#32593;&#31283;&#23450;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#22810;&#26041;&#21033;&#30410;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#30005;&#32593;&#12289;&#30005;&#21160;&#36710;&#32858;&#21512;&#22120;&#21644;&#29992;&#25143;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.00218</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#27744;&#35843;&#25511;&#20998;&#23618;V2G&#21327;&#35843;&#31574;&#30053;&#20197;&#23454;&#29616;&#22810;&#26041;&#21033;&#30410;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-Based Battery Conditioning Hierarchical V2G Coordination for Multi-Stakeholder Benefits. (arXiv:2308.00218v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#27744;&#35843;&#25511;&#20998;&#23618;V2G&#21327;&#35843;&#31574;&#30053;&#65292;&#26088;&#22312;&#20419;&#36827;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#21644;&#30005;&#32593;&#31283;&#23450;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#22810;&#26041;&#21033;&#30410;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#30005;&#32593;&#12289;&#30005;&#21160;&#36710;&#32858;&#21512;&#22120;&#21644;&#29992;&#25143;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#21160;&#36710;&#65288;EV&#65289;&#30340;&#26222;&#21450;&#21644;EV&#30005;&#23376;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36710;&#36742;&#23545;&#30005;&#32593;&#65288;V2G&#65289;&#25216;&#26415;&#21644;&#22823;&#35268;&#27169;&#35843;&#24230;&#31574;&#30053;&#24050;&#32463;&#20986;&#29616;&#65292;&#20197;&#20419;&#36827;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#21644;&#30005;&#32593;&#31283;&#23450;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;&#26435;&#30410;&#35777;&#26126;&#31639;&#27861;&#30340;&#22810;&#26041;&#21442;&#19982;&#32773;&#20998;&#23618;V2G&#21327;&#35843;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#22810;&#26041;&#21442;&#19982;&#32773;&#21253;&#25324;&#30005;&#32593;&#12289;&#30005;&#21160;&#36710;&#32858;&#21512;&#22120;&#65288;EVAs&#65289;&#21644;&#29992;&#25143;&#65292;&#24182;&#19988;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#22810;&#26041;&#21033;&#30410;&#12290;&#22312;&#30005;&#32593;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#36127;&#33655;&#27874;&#21160;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#28040;&#32791;&#65292;&#32780;&#22312;EVA&#26041;&#38754;&#65292;&#32771;&#34385;&#20102;&#33021;&#28304;&#38480;&#21046;&#21644;&#20805;&#30005;&#25104;&#26412;&#12290;&#29992;&#25143;&#26041;&#38754;&#32771;&#34385;&#20102;&#30005;&#27744;SOX&#30340;&#19977;&#20010;&#20851;&#38190;&#35843;&#33410;&#21442;&#25968;&#65292;&#21253;&#25324;&#30005;&#33655;&#29366;&#24577;&#12289;&#21151;&#29575;&#29366;&#24577;&#21644;&#20581;&#24247;&#29366;&#24577;&#12290;&#19982;&#22235;&#31181;&#20856;&#22411;&#22522;&#32447;&#30456;&#27604;&#65292;&#22810;&#26041;&#21442;&#19982;&#32773;&#20998;&#23618;&#21327;&#35843;&#31574;&#30053;&#21487;&#20197;&#22686;&#24378;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing prevalence of electric vehicles (EVs) and advancements in EV electronics, vehicle-to-grid (V2G) techniques and large-scale scheduling strategies have emerged to promote renewable energy utilization and power grid stability. This study proposes a multi-stakeholder hierarchical V2G coordination based on deep reinforcement learning (DRL) and the Proof of Stake algorithm. Furthermore, the multi-stakeholders include the power grid, EV aggregators (EVAs), and users, and the proposed strategy can achieve multi-stakeholder benefits. On the grid side, load fluctuations and renewable energy consumption are considered, while on the EVA side, energy constraints and charging costs are considered. The three critical battery conditioning parameters of battery SOX are considered on the user side, including state of charge, state of power, and state of health. Compared with four typical baselines, the multi-stakeholder hierarchical coordination strategy can enhance renewable energy con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#26799;&#24230;&#32047;&#31215;&#20248;&#21270;&#25216;&#26415;&#30340;Swin&#35270;&#35273;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#21457;&#29616;&#24212;&#29992;&#35813;&#25216;&#26415;&#20250;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#19979;&#38477;&#24182;&#22686;&#21152;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.00197</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#32047;&#31215;&#20248;&#21270;&#25216;&#26415;&#35780;&#20272;Swin&#35270;&#35273;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique. (arXiv:2308.00197v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#26799;&#24230;&#32047;&#31215;&#20248;&#21270;&#25216;&#26415;&#30340;Swin&#35270;&#35273;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#21457;&#29616;&#24212;&#29992;&#35813;&#25216;&#26415;&#20250;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#19979;&#38477;&#24182;&#22686;&#21152;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#30340;&#20248;&#21183;&#65292;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#39046;&#22495;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;Swin Transformer&#30001;&#20110;&#20854;&#20998;&#23618;&#35774;&#35745;&#21644;&#26377;&#25928;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#35270;&#35273;&#29305;&#24449;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#26412;&#25991;&#20351;&#29992;&#26799;&#24230;&#32047;&#31215;&#20248;&#21270;&#65288;GAO&#65289;&#25216;&#26415;&#35780;&#20272;&#20102;Swin ViT&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26799;&#24230;&#32047;&#31215;&#20248;&#21270;&#25216;&#26415;&#23545;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24212;&#29992;GAO&#25216;&#26415;&#20250;&#23548;&#33268;Swin ViT&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#65292;&#19982;&#26631;&#20934;&#30340;Swin Transformer&#27169;&#22411;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#24403;&#24212;&#29992;GAO&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;Swin ViT&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#22686;&#21152;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#24212;&#29992;GAO&#25216;&#26415;&#21487;&#33021;&#19981;&#36866;&#21512;Swin ViT&#27169;&#22411;&#65292;&#23384;&#22312;&#19968;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have emerged as a promising approach for visual recognition tasks, revolutionizing the field by leveraging the power of transformer-based architectures. Among the various ViT models, Swin Transformers have gained considerable attention due to their hierarchical design and ability to capture both local and global visual features effectively. This paper evaluates the performance of Swin ViT model using gradient accumulation optimization (GAO) technique. We investigate the impact of gradient accumulation optimization technique on the model's accuracy and training time. Our experiments show that applying the GAO technique leads to a significant decrease in the accuracy of the Swin ViT model, compared to the standard Swin Transformer model. Moreover, we detect a significant increase in the training time of the Swin ViT model when GAO model is applied. These findings suggest that applying the GAO technique may not be suitable for the Swin ViT model, and concern sho
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65292;&#23427;&#20204;&#33021;&#22815;&#23436;&#25104;&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#38656;&#35201;&#34987;&#35299;&#37322;&#21644;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#23545;&#20854;&#34892;&#20026;&#30340;&#25511;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.00189</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65306;&#22914;&#20309;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65311;
&lt;/p&gt;
&lt;p&gt;
Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?. (arXiv:2308.00189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00189
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65292;&#23427;&#20204;&#33021;&#22815;&#23436;&#25104;&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#38656;&#35201;&#34987;&#35299;&#37322;&#21644;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#23545;&#20854;&#34892;&#20026;&#30340;&#25511;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#36991;&#20813;&#19981;&#33391;&#34892;&#20026;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24182;&#27491;&#22312;&#37325;&#26032;&#22609;&#36896;&#25105;&#20204;&#19982;&#35745;&#31639;&#26426;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;&#26366;&#32463;&#26159;&#19968;&#20010;&#31185;&#23398;&#24037;&#31243;&#23398;&#31185;&#65292;&#23558;&#26500;&#24314;&#27169;&#22359;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#29616;&#22312;&#21487;&#20197;&#35828;&#24050;&#32463;&#26159;&#19968;&#20010;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65292;&#20854;&#20013;&#23547;&#27714;&#20986;&#29616;&#30340;&#34892;&#20026;&#20197;&#25903;&#25345;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#29992;&#20363;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#34913;&#37327;&#20219;&#21153;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#36825;&#20123;&#20219;&#21153;&#23436;&#25104;&#30340;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#21162;&#21147;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#20998;&#35299;&#20026;&#35299;&#37322;&#36328;&#20219;&#21153;&#24615;&#33021;&#30340;&#31867;&#21035;&#65292;&#20197;&#25351;&#23548;&#26426;&#26800;&#35299;&#37322;&#24182;&#24110;&#21161;&#26410;&#26469;&#20998;&#26512;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coaxing out desired behavior from pretrained models, while avoiding undesirable ones, has redefined NLP and is reshaping how we interact with computers. What was once a scientific engineering discipline-in which building blocks are stacked one on top of the other-is arguably already a complex systems science, in which emergent behaviors are sought out to support previously unimagined use cases.  Despite the ever increasing number of benchmarks that measure task performance, we lack explanations of what behaviors language models exhibit that allow them to complete these tasks in the first place. We argue for a systematic effort to decompose language model behavior into categories that explain cross-task performance, to guide mechanistic explanations and help future-proof analytic research.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#31649;&#29702;&#21644;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30740;&#31350;&#25551;&#36848;&#20102;&#20351;&#29992;&#23454;&#38469;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#37322;&#25968;&#25454;&#24211;&#26597;&#35810;&#31572;&#26696;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#26524;&#30340;&#36131;&#20219;&#35780;&#20998;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#20197;&#21450;&#19982;&#25968;&#25454;&#24211;&#20462;&#22797;&#21644;Shap-score&#35745;&#31639;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.00184</link><description>&lt;p&gt;
&#25968;&#25454;&#31649;&#29702;&#21644;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24402;&#22240;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Attribution-Scores in Data Management and Explainable Machine Learning. (arXiv:2308.00184v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00184
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31649;&#29702;&#21644;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30740;&#31350;&#25551;&#36848;&#20102;&#20351;&#29992;&#23454;&#38469;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#37322;&#25968;&#25454;&#24211;&#26597;&#35810;&#31572;&#26696;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#26524;&#30340;&#36131;&#20219;&#35780;&#20998;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#20197;&#21450;&#19982;&#25968;&#25454;&#24211;&#20462;&#22797;&#21644;Shap-score&#35745;&#31639;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#26368;&#36817;&#20851;&#20110;&#22312;&#25968;&#25454;&#24211;&#20013;&#20351;&#29992;&#23454;&#38469;&#22240;&#26524;&#20851;&#31995;&#26469;&#23450;&#20041;&#35299;&#37322;&#26597;&#35810;&#31572;&#26696;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#32467;&#26524;&#30340;&#36131;&#20219;&#35780;&#20998;&#30340;&#30740;&#31350;&#12290;&#22312;&#25968;&#25454;&#24211;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#24182;&#21033;&#29992;&#25968;&#25454;&#24211;&#20462;&#22797;&#19982;&#26377;&#29992;&#30340;&#36830;&#25509;&#12290;&#20462;&#22797;&#36824;&#29992;&#20110;&#32473;&#20986;&#25968;&#25454;&#24211;&#30340;&#19968;&#33268;&#24615;&#30340;&#37327;&#21270;&#24230;&#37327;&#12290;&#23545;&#20110;&#20998;&#31867;&#27169;&#22411;&#65292;&#36131;&#20219;&#35780;&#20998;&#24471;&#21040;&#20102;&#36866;&#24403;&#30340;&#25193;&#23637;&#21644;&#35828;&#26126;&#12290;&#36824;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;Shap-score&#30340;&#39640;&#25928;&#35745;&#31639;&#12290;&#37325;&#28857;&#25918;&#22312;&#20316;&#32773;&#21644;&#21512;&#20316;&#32773;&#30340;&#24037;&#20316;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe recent research on the use of actual causality in the definition of responsibility scores as explanations for query answers in databases, and for outcomes from classification models in machine learning. In the case of databases, useful connections with database repairs are illustrated and exploited. Repairs are also used to give a quantitative measure of the consistency of a database. For classification models, the responsibility score is properly extended and illustrated. The efficient computation of Shap-score is also analyzed and discussed. The emphasis is placed on work done by the author and collaborators.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.00177</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#20013;&#32988;&#36807;GBDTs
&lt;/p&gt;
&lt;p&gt;
Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#39046;&#22495;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#34920;&#26684;&#24418;&#24335;&#30340;Learning-To-Rank&#38382;&#39064;&#19978;&#23578;&#26410;&#19968;&#33268;&#22320;&#32988;&#36807;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDTs)&#12290;&#36817;&#26399;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20219;&#21153;&#19978;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#27604;&#26377;&#26631;&#31614;&#25968;&#25454;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#36824;&#26410;&#24212;&#29992;&#20110;Learning-To-Rank&#38382;&#39064;&#65292;&#32780;&#35813;&#38382;&#39064;&#36890;&#24120;&#20135;&#29983;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#21542;&#33021;&#25552;&#39640;LTR&#24615;&#33021;&#65292;&#19982;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#35774;&#35745;&#36873;&#25321;(&#21253;&#25324;SimCLR-Rank&#65292;&#36825;&#26159;&#25105;&#20204;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#20462;&#25913;&#30340;SimCLR&#26041;&#27861;)&#65292;&#25105;&#20204;&#20135;&#29983;&#20102;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26377;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#19988;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20248;&#20110;GBDTs(&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;)&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#31070;&#32463;&#27861;&#24459;&#21028;&#26029;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#26089;&#26399;&#23384;&#22312;&#30340;&#31995;&#32479;&#36827;&#34892;&#23454;&#39564;&#21457;&#29616;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#32463;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27861;&#24459;&#21028;&#26029;&#39044;&#27979;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.00165</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#31070;&#32463;&#27861;&#24459;&#21028;&#26029;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adversarially Robust Neural Legal Judgement Systems. (arXiv:2308.00165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#31070;&#32463;&#27861;&#24459;&#21028;&#26029;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#26089;&#26399;&#23384;&#22312;&#30340;&#31995;&#32479;&#36827;&#34892;&#23454;&#39564;&#21457;&#29616;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#32463;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27861;&#24459;&#21028;&#26029;&#39044;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#21028;&#26029;&#39044;&#27979;&#26159;&#26681;&#25454;&#26696;&#24773;&#25551;&#36848;&#26469;&#39044;&#27979;&#27861;&#38498;&#26696;&#20214;&#32467;&#26524;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#22522;&#20110;&#20107;&#23454;&#39044;&#27979;&#27861;&#24459;&#21028;&#26029;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24050;&#32463;&#22686;&#21152;&#20102;&#19982;&#27861;&#24459;&#21028;&#26029;&#39044;&#27979;&#31995;&#32479;&#30456;&#20851;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#20351;&#36825;&#31181;&#31995;&#32479;&#22312;&#23454;&#36341;&#20013;&#26377;&#25152;&#24110;&#21161;&#65292;&#23427;&#20204;&#24212;&#35813;&#33021;&#22815;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26500;&#24314;&#31070;&#32463;&#27861;&#24459;&#21028;&#26029;&#31995;&#32479;&#19978;&#65292;&#20294;&#23545;&#20110;&#21019;&#24314;&#40065;&#26834;&#30340;&#27861;&#24459;&#21028;&#26029;&#39044;&#27979;&#31995;&#32479;&#65288;LJP&#65289;&#20960;&#20046;&#27809;&#26377;&#32473;&#20104;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#23545;&#26089;&#26399;&#23384;&#22312;&#30340;LJP&#31995;&#32479;&#36827;&#34892;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#26080;&#27861;&#22788;&#29702;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#40065;&#26834;LJP&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#22312;&#19977;&#20010;&#27861;&#24459;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;LJP&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal judgment prediction is the task of predicting the outcome of court cases on a given text description of facts of cases. These tasks apply Natural Language Processing (NLP) techniques to predict legal judgment results based on facts. Recently, large-scale public datasets and NLP models have increased research in areas related to legal judgment prediction systems. For such systems to be practically helpful, they should be robust from adversarial attacks. Previous works mainly focus on making a neural legal judgement system; however, significantly less or no attention has been given to creating a robust Legal Judgement Prediction(LJP) system. We implemented adversarial attacks on early existing LJP systems and found that none of them could handle attacks. In this work, we proposed an approach for making robust LJP systems. Extensive experiments on three legal datasets show significant improvements in our approach over the state-of-the-art LJP system in handling adversarial attacks. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.00158</link><description>&lt;p&gt;
&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#20013;&#30340;&#23436;&#32654;&#36136;&#37327;&#27573;&#33853;&#65306;&#26159;&#21542;&#21487;&#20197;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#32534;&#36753;&#36317;&#31163;&#27169;&#24335;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#65288;TQE&#65289;&#26159;&#23558;&#36755;&#20986;&#32763;&#35793;&#37096;&#32626;&#21040;&#20351;&#29992;&#20013;&#20043;&#21069;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290; TQE&#23545;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#20154;&#24037;&#32763;&#35793;&#65288;HT&#65289;&#30340;&#36136;&#37327;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#26597;&#30475;&#21442;&#32771;&#32763;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20026;TQE&#20219;&#21153;&#21644;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;Fine-Tune&#12290;&#25105;&#20204;&#20197;ChatGPT&#20026;&#20363;&#65292;&#23558;TQE&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;&#33521;&#24847;&#21644;&#33521;&#24503;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;ChatGPT&#30340;API Fine-Tuned&#21487;&#20197;&#22312;&#39044;&#27979;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#33719;&#24471;&#30456;&#23545;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#21363;&#26159;&#21542;&#38656;&#35201;&#32534;&#36753;&#32763;&#35793;&#65292;&#20294;&#32943;&#23450;&#26377;&#25913;&#36827;&#20934;&#30830;&#24615;&#30340;&#31354;&#38388;&#12290;&#33521;&#24847;&#21452;&#35821;&#25688;&#35201;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.00143</link><description>&lt;p&gt;
&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20869;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#30340;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;DNNs&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#36825;&#20351;&#24471;&#35299;&#37322;&#21644;&#35777;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#23545;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#36825;&#20123;&#25216;&#26415;&#33021;&#22815;&#25214;&#20986;&#23548;&#33268;DNN&#34892;&#20026;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;XAI&#25216;&#26415;&#36890;&#24120;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;(i)&#23427;&#20204;&#26159;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#35299;&#37322;&#27491;&#30830;&#24615;&#30340;&#27491;&#24335;&#20445;&#35777;&#65307;(ii)&#23427;&#20204;&#36890;&#24120;&#36866;&#29992;&#20110;&#8220;&#19968;&#27425;&#24615;&#8221;&#31995;&#32479;(&#21363;DNN&#29420;&#31435;&#20110;&#36807;&#21435;&#30340;&#35843;&#29992;)&#65292;&#32780;&#19981;&#26159;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#22987;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#29992;&#20110;&#25512;&#29702;&#22810;&#27493;&#39588;&#30340;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20943;&#23569;&#24213;&#23618;&#39564;&#35777;&#22120;&#25152;&#25506;&#32034;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did.  Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems (where the DNN is invoked independently of past invocations), as opposed to reactive systems.  Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. W
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#26032;&#30340;&#20844;&#24179;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#34920;&#26684;&#20998;&#31867;&#20013;&#23454;&#39564;&#35780;&#20272;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#20026;&#26410;&#26469;&#20844;&#24179;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#26356;&#20005;&#35880;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.00133</link><description>&lt;p&gt;
&#19968;&#22871;&#29992;&#20110;&#34920;&#26684;&#20998;&#31867;&#20844;&#24179;&#24615;&#30340;&#20844;&#24179;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Suite of Fairness Datasets for Tabular Classification. (arXiv:2308.00133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00133
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#26032;&#30340;&#20844;&#24179;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#34920;&#26684;&#20998;&#31867;&#20013;&#23454;&#39564;&#35780;&#20272;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#20026;&#26410;&#26469;&#20844;&#24179;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#26356;&#20005;&#35880;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#26377;&#35768;&#22810;&#20851;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#20844;&#24179;&#24615;&#30340;&#31639;&#27861;&#30340;&#35770;&#25991;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#35770;&#25991;&#21482;&#20351;&#29992;&#20102;&#38750;&#24120;&#23569;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#21462;20&#20010;&#20844;&#27491;&#25968;&#25454;&#38598;&#24182;&#25552;&#20379;&#30456;&#20851;&#30340;&#20844;&#24179;&#20803;&#25968;&#25454;&#12290;&#24076;&#26395;&#36825;&#20123;&#25968;&#25454;&#38598;&#33021;&#22815;&#20419;&#36827;&#26410;&#26469;&#20844;&#24179;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26356;&#20005;&#35880;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been many papers with algorithms for improving fairness of machine-learning classifiers for tabular data. Unfortunately, most use only very few datasets for their experimental evaluation. We introduce a suite of functions for fetching 20 fairness datasets and providing associated fairness metadata. Hopefully, these will lead to more rigorous experimental evaluations in future fairness-aware machine learning research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#38899;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20027;&#35201;&#20851;&#27880;&#22810;&#31181;&#35774;&#32622;&#21644;&#22810;&#31181;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#21644;&#24369;&#26631;&#31614;&#25968;&#25454;&#20197;&#21450;&#38468;&#21152;&#25968;&#25454;&#27169;&#24577;&#65292;&#25913;&#36827;&#19979;&#28216;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.00129</link><description>&lt;p&gt;
&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65306;&#20351;&#29992;&#21333;&#35270;&#22270;&#12289;&#22810;&#35270;&#22270;&#21644;&#22810;&#20219;&#21153;&#26041;&#27861;&#23398;&#20064;&#21452;&#21521;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods. (arXiv:2308.00129v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#38899;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20027;&#35201;&#20851;&#27880;&#22810;&#31181;&#35774;&#32622;&#21644;&#22810;&#31181;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#21644;&#24369;&#26631;&#31614;&#25968;&#25454;&#20197;&#21450;&#38468;&#21152;&#25968;&#25454;&#27169;&#24577;&#65292;&#25913;&#36827;&#19979;&#28216;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38598;&#20013;&#30740;&#31350;&#20102;&#38024;&#23545;&#26102;&#38388;&#25110;&#31354;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#25913;&#36827;&#19979;&#28216;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#26377;&#30417;&#30563;&#23398;&#20064;&#19968;&#30452;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#33391;&#22909;&#39034;&#24207;&#34920;&#31034;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25193;&#23637;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#20010;&#38480;&#21046;&#22240;&#32032;&#26159;&#32570;&#20047;&#36275;&#22815;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#21463;&#21040;&#36825;&#19968;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25506;&#32034;&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#21644;&#24369;&#26631;&#31614;&#25968;&#25454;&#20197;&#21450;&#38468;&#21152;&#25968;&#25454;&#27169;&#24577;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#23545;&#35821;&#38899;&#25968;&#25454;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#19982;&#22823;&#22810;&#25968;&#20851;&#27880;&#21333;&#19968;&#23398;&#20064;&#35774;&#32622;&#30340;&#20854;&#20182;&#20316;&#21697;&#19981;&#21516;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#31181;&#35774;&#32622;&#65306;&#24102;&#36741;&#21161;&#25439;&#22833;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22810;&#35270;&#22270;&#23398;&#20064;&#12290;&#38500;&#20102;&#19981;&#21516;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#22810;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Tho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.00121</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#65306;AI&#20316;&#20026;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23433;&#20840;&#27979;&#35797;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28183;&#36879;&#27979;&#35797;&#26159;&#19968;&#39033;&#38656;&#35201;&#39640;&#27700;&#24179;&#19987;&#19994;&#30693;&#35782;&#30340;&#27963;&#21160;&#65292;&#24182;&#28041;&#21450;&#35768;&#22810;&#25163;&#21160;&#27979;&#35797;&#21644;&#20998;&#26512;&#27493;&#39588;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29992;&#20363;&#65306;&#29992;&#20110;&#23433;&#20840;&#27979;&#35797;&#20219;&#21153;&#30340;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#22312;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#20013;&#36827;&#34892;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#38381;&#29615;&#21453;&#39304;&#65292;&#23558;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20302;&#32423;&#25805;&#20316;&#19982;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#65288;&#36890;&#36807;SSH&#36830;&#25509;&#65289;&#30456;&#36830;&#65292;&#24182;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#34394;&#25311;&#26426;&#29366;&#24577;&#20197;&#23547;&#25214;&#28431;&#27934;&#65292;&#24182;&#25552;&#20379;&#20855;&#20307;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#36884;&#24452;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#27169;&#22359;&#21270;MODS&#26412;&#20307;&#35770;&#65288;MMODS-O&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20803;&#25968;&#25454;&#23545;&#35937;&#25551;&#36848;&#27169;&#24335;&#65288;MODS&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#29615;&#22659;&#19979;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#26412;&#20307;&#35770;&#35774;&#35745;&#26041;&#27861;&#23398;&#65288;MOMo&#65289;&#23454;&#29616;&#20102;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.00116</link><description>&lt;p&gt;
MODS&#30340;&#27169;&#22359;&#21270;&#26412;&#20307;&#35770; - &#20803;&#25968;&#25454;&#23545;&#35937;&#25551;&#36848;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
A Modular Ontology for MODS -- Metadata Object Description Schema. (arXiv:2308.00116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#27169;&#22359;&#21270;MODS&#26412;&#20307;&#35770;&#65288;MMODS-O&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20803;&#25968;&#25454;&#23545;&#35937;&#25551;&#36848;&#27169;&#24335;&#65288;MODS&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#29615;&#22659;&#19979;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#26412;&#20307;&#35770;&#35774;&#35745;&#26041;&#27861;&#23398;&#65288;MOMo&#65289;&#23454;&#29616;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#25968;&#25454;&#23545;&#35937;&#25551;&#36848;&#27169;&#24335;&#65288;MODS&#65289;&#26159;&#29992;&#20110;&#25551;&#36848;&#22270;&#20070;&#27010;&#24565;&#21644;&#20803;&#25968;&#25454;&#30340;&#65292;&#24182;&#30001;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#32500;&#25252;&#12290;MODS&#30340;&#26631;&#20934;&#29256;&#26412;&#26159;&#22522;&#20110;XML&#24605;&#32500;&#30340;XML&#27169;&#24335;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#22312;&#30693;&#35782;&#22270;&#35889;&#29615;&#22659;&#19979;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#27169;&#22359;&#21270;MODS&#26412;&#20307;&#35770;&#65288;MMODS-O&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;MODS&#30340;&#25152;&#26377;&#20803;&#32032;&#21644;&#23646;&#24615;&#12290;&#22312;&#35774;&#35745;&#26412;&#20307;&#35770;&#26102;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#36817;&#30340;&#27169;&#22359;&#21270;&#26412;&#20307;&#35770;&#35774;&#35745;&#26041;&#27861;&#23398;&#65288;MOMo&#65289;&#65292;&#26088;&#22312;&#22312;&#20445;&#23432;&#22320;&#19982;MODS&#21521;&#21518;&#20860;&#23481;&#30340;&#21516;&#26102;&#65292;&#22312;&#27169;&#22359;&#21270;&#21644;&#39640;&#36136;&#37327;&#26412;&#20307;&#35770;&#35774;&#35745;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Metadata Object Description Schema (MODS) was developed to describe bibliographic concepts and metadata and is maintained by the Library of Congress. Its authoritative version is given as an XML schema based on an XML mindset which means that it has significant limitations for use in a knowledge graphs context. We have therefore developed the Modular MODS Ontology (MMODS-O) which incorporates all elements and attributes of the MODS XML schema. In designing the ontology, we adopt the recent Modular Ontology Design Methodology (MOMo) with the intention to strike a balance between modularity and quality ontology design on the one hand, and conservative backward compatibility with MODS on the other.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#30340;&#26041;&#27861;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#25552;&#20379;&#31283;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#21644;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.00113</link><description>&lt;p&gt;
&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#26041;&#27861;&#24041;&#22266;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Three Bricks to Consolidate Watermarks for Large Language Models. (arXiv:2308.00113v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#30340;&#26041;&#27861;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#25552;&#20379;&#31283;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#21644;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21028;&#26029;&#29983;&#25104;&#25991;&#26412;&#21644;&#33258;&#28982;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32972;&#26223;&#19979;&#65292;&#27700;&#21360;&#25216;&#26415;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#23558;&#29983;&#25104;&#25991;&#26412;&#24402;&#23646;&#20110;&#29305;&#23450;&#27169;&#22411;&#30340;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;&#23427;&#25913;&#21464;&#20102;&#37319;&#26679;&#29983;&#25104;&#36807;&#31243;&#65292;&#30041;&#19979;&#20102;&#26080;&#24418;&#30340;&#30165;&#36857;&#22312;&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#65292;&#20197;&#20415;&#20110;&#21518;&#32493;&#30340;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#19977;&#20010;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#29282;&#22266;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#65288;&#23567;&#20110;10^(-6)&#65289;&#65292;&#36825;&#20123;&#20445;&#35777;&#20381;&#28982;&#26377;&#25928;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20351;&#29992;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#23545;&#27604;&#20102;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#20851;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#34892;&#24615;&#30340;&#35265;&#35299;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#21487;&#20197;&#35775;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#26223;&#20197;&#21450;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing generated text to a specific model. It alters the sampling generation process so as to leave an invisible trace in the generated output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10$^{\text{-6}}$). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.
&lt;/p&gt;</description></item><item><title>DPBERT&#26159;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;BERT&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#37096;&#20998;transformer&#23618;&#26469;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2308.00108</link><description>&lt;p&gt;
DPBERT: &#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;BERT&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
DPBERT: Efficient Inference for BERT based on Dynamic Planning. (arXiv:2308.00108v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00108
&lt;/p&gt;
&lt;p&gt;
DPBERT&#26159;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;BERT&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#37096;&#20998;transformer&#23618;&#26469;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#38590;&#20197;&#24212;&#29992;&#20110;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#33258;&#36866;&#24212;&#36755;&#20837;&#25512;&#29702;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#36825;&#20123;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;BERT&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DPBERT&#65292;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;BERT&#30340;&#19968;&#37096;&#20998;transformer&#23618;&#20316;&#20026;&#35745;&#31639;&#36335;&#24452;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21152;&#36895;BERT&#30340;&#25512;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21407;&#22987;BERT&#27169;&#22411;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#35268;&#21010;&#27169;&#22359;&#65292;&#29992;&#20110;&#30830;&#23450;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#21542;&#21253;&#21547;&#25110;&#32469;&#36807;&#26576;&#20010;&#23618;&#12290;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;98%&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24310;&#36831;&#38477;&#20302;&#21040;75%&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#33258;&#36866;&#24212;&#36755;&#20837;&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;&#21644;&#36895;&#24230;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained language models such as BERT have contributed significantly to the development of NLP. However, those models require large computational resources, making it difficult to be applied to mobile devices where computing power is limited. In this paper we aim to address the weakness of existing input-adaptive inference methods which fail to take full advantage of the structure of BERT. We propose Dynamic Planning in BERT, a novel fine-tuning strategy that can accelerate the inference process of BERT through selecting a subsequence of transformer layers list of backbone as a computational path for an input sample. To do this, our approach adds a planning module to the original BERT model to determine whether a layer is included or bypassed during inference. Experimental results on the GLUE benchmark exhibit that our method reduces latency to 75\% while maintaining 98\% accuracy, yielding a better accuracy-speed trade-off compared to state-of-the-art input-adaptive met
&lt;/p&gt;</description></item><item><title>&#39564;&#35777;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#30340;&#25968;&#25454;&#25277;&#35937;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#24037;&#20855;&#22312;&#25552;&#21462;&#20449;&#24687;&#30340;&#36895;&#24230;&#19978;&#20248;&#20110;&#21307;&#24072;&#20154;&#24037;&#25277;&#35937;&#32773;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#38750;&#21155;&#12290;</title><link>http://arxiv.org/abs/2308.00107</link><description>&lt;p&gt;
&#39564;&#35777;&#19968;&#31181;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#21307;&#30103;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#30340;&#25968;&#25454;&#25277;&#35937;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data. (arXiv:2308.00107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00107
&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#30340;&#25968;&#25454;&#25277;&#35937;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#24037;&#20855;&#22312;&#25552;&#21462;&#20449;&#24687;&#30340;&#36895;&#24230;&#19978;&#20248;&#20110;&#21307;&#24072;&#20154;&#24037;&#25277;&#35937;&#32773;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#38750;&#21155;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#25551;&#36848;&#19968;&#31181;&#29992;&#20110;&#20174;PDF&#25991;&#26723;&#20013;&#25552;&#21462;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#30340;&#24320;&#21457;&#21644;&#39564;&#35777;&#36807;&#31243;&#65292;&#20363;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#25991;&#26412;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#22522;&#20110;OpenAI&#30340;GPT-3.5&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#25277;&#35937;&#24037;&#20855;&#65292;&#24182;&#19982;&#19977;&#20301;&#21307;&#24072;&#20154;&#24037;&#25277;&#35937;&#32773;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35780;&#20272;&#22312;&#25552;&#21462;199&#20221;&#21435;&#26631;&#35782;&#21270;&#30340;&#28608;&#32032;&#20999;&#38500;&#26415;&#30149;&#29702;&#25253;&#21578;&#20013;&#30340;14&#20010;&#29420;&#29305;&#21464;&#37327;&#30340;&#25968;&#25454;&#26102;&#65292;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#25253;&#21578;&#20197;&#21521;&#37327;&#21270;&#21644;&#25195;&#25551;&#26684;&#24335;&#36827;&#34892;&#20102;&#22788;&#29702;&#65292;&#20197;&#30830;&#23450;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#23545;&#25968;&#25454;&#25277;&#35937;&#30340;&#24433;&#21709;&#12290;&#35813;&#24037;&#20855;&#22312;&#25968;&#25454;&#25277;&#35937;&#36895;&#24230;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#20934;&#30830;&#24615;&#30340;&#38750;&#21155;&#24615;&#12290;&#32467;&#26524;&#65306;&#20154;&#24037;&#25277;&#35937;&#32773;&#27599;&#20221;&#25253;&#21578;&#38656;&#35201;&#24179;&#22343;101&#31186;&#36827;&#34892;&#25968;&#25454;&#25277;&#35937;&#65292;&#26102;&#38388;&#33539;&#22260;&#20026;15&#31186;&#33267;284&#31186;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36719;&#20214;&#24037;&#20855;&#38656;&#27714;&#30340;&#24179;&#22343;&#26102;&#38388;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Objectives: To describe the development and validation of a zero-shot learning natural language processing (NLP) tool for abstracting data from unstructured text contained within PDF documents, such as those found within electronic health records. Materials and Methods: A data abstraction tool based on the GPT-3.5 model from OpenAI was developed and compared to three physician human abstractors in terms of time to task completion and accuracy for abstracting data on 14 unique variables from a set of 199 de-identified radical prostatectomy pathology reports. The reports were processed by the software tool in vectorized and scanned formats to establish the impact of optical character recognition on data abstraction. The tool was assessed for superiority for data abstraction speed and non-inferiority for accuracy. Results: The human abstractors required a mean of 101s per report for data abstraction, with times varying from 15 to 284 s. In comparison, the software tool required a mean of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#20010;&#20154;&#26159;&#21542;&#33021;&#22815;&#30417;&#30563;&#19968;&#20010;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#24322;&#26500;&#26426;&#22120;&#20154;&#32676;&#20307;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#24037;&#20316;&#36127;&#33655;&#31639;&#27861;&#25903;&#25345;&#19979;&#65292;&#19968;&#20010;&#20154;&#21487;&#20197;&#26377;&#25928;&#22320;&#25351;&#25381;&#21644;&#25511;&#21046;&#36825;&#26679;&#30340;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2308.00102</link><description>&lt;p&gt;
&#19968;&#20010;&#20154;&#33021;&#22815;&#30417;&#30563;100&#20010;&#24322;&#26500;&#26426;&#22120;&#20154;&#30340;&#32676;&#20307;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can A Single Human Supervise A Swarm of 100 Heterogeneous Robots?. (arXiv:2308.00102v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00102
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#20010;&#20154;&#26159;&#21542;&#33021;&#22815;&#30417;&#30563;&#19968;&#20010;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#24322;&#26500;&#26426;&#22120;&#20154;&#32676;&#20307;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#24037;&#20316;&#36127;&#33655;&#31639;&#27861;&#25903;&#25345;&#19979;&#65292;&#19968;&#20010;&#20154;&#21487;&#20197;&#26377;&#25928;&#22320;&#25351;&#25381;&#21644;&#25511;&#21046;&#36825;&#26679;&#30340;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65292;&#19968;&#20010;&#20154;&#26159;&#21542;&#33021;&#22815;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30417;&#30563;&#19968;&#20010;&#24322;&#26500;&#26426;&#22120;&#20154;&#32676;&#20307;&#23436;&#25104;&#20219;&#21153;&#12290;&#19968;&#20010;&#26222;&#36941;&#30340;&#25285;&#24551;&#26159;&#20154;&#31867;&#30340;&#24037;&#20316;&#36127;&#33655;&#26159;&#21542;&#20250;&#21040;&#36798;&#26497;&#38480;&#12290;&#32654;&#22269;&#22269;&#38450;&#39640;&#32423;&#30740;&#31350;&#35745;&#21010;&#23616;&#30340;OFFsensive Swarm-Enabled Tactics&#35745;&#21010;&#22312;&#32654;&#22269;&#38470;&#20891;&#22478;&#24066;&#35757;&#32451;&#22330;&#22320;&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20891;&#20107;&#28436;&#20064;&#65292;&#20026;&#25105;&#20204;&#20102;&#35299;&#23454;&#29616;&#36825;&#31181;&#32676;&#20307;&#37096;&#32626;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25351;&#25381;&#21644;&#25511;&#21046;&#32676;&#20307;&#25112;&#26415;&#25972;&#21512;&#22242;&#38431;&#30340;&#32676;&#20307;&#25351;&#25381;&#23448;&#20351;&#29992;&#24322;&#26500;&#26426;&#22120;&#20154;&#32676;&#20307;&#25191;&#34892;&#30456;&#20851;&#20219;&#21153;&#12290;&#22312;&#26368;&#32456;&#30340;OFFSET&#35745;&#21010;&#29616;&#22330;&#28436;&#20064;&#20013;&#65292;&#22242;&#38431;&#25910;&#38598;&#20102;&#19982;&#32676;&#20307;&#25351;&#25381;&#23448;&#30340;&#20154;&#31867;&#34920;&#29616;&#30456;&#20851;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#25351;&#26631;&#12290;&#20351;&#29992;&#19968;&#20010;&#22810;&#32500;&#30340;&#24037;&#20316;&#36127;&#33655;&#31639;&#27861;&#65292;&#26681;&#25454;&#24037;&#20316;&#36127;&#33655;&#30340;&#20116;&#20010;&#32452;&#25104;&#37096;&#20998;&#20272;&#35745;&#25972;&#20307;&#24037;&#20316;&#36127;&#33655;&#65292;&#24182;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#34429;&#28982;&#32676;&#20307;&#25351;&#25381;&#23448;&#30340;&#24037;&#20316;&#36127;&#33655;&#20272;&#35745;&#30830;&#23454;&#36229;&#36807;&#20102;&#36127;&#33655;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
An open research question has been whether a single human can supervise a true heterogeneous swarm of robots completing tasks in real world environments. A general concern is whether or not the human's workload will be taxed to the breaking point. The Defense Advanced Research Projects Agency's OFFsensive Swarm-Enabled Tactics program's field exercises that occurred at U.S. Army urban training sites provided the opportunity to understand the impact of achieving such swarm deployments. The Command and Control of Aggregate Swarm Tactics integrator team's swarm commander users the heterogeneous robot swarm to conduct relevant missions. During the final OFFSET program field exercise, the team collected objective and subjective metrics related to teh swarm commander's human performance. A multi-dimensional workload algorithm that estimates overall workload based on five components of workload was used to analyze the results. While the swarm commander's workload estimate did cross the overlo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#35282;&#24230;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24120;&#35782;&#30693;&#35782;&#25552;&#21319;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00085</link><description>&lt;p&gt;
&#20808;&#24605;&#32771;&#20877;&#22238;&#24212;&#65306;&#20026;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#38598;&#25104;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation. (arXiv:2308.00085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#35282;&#24230;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24120;&#35782;&#30693;&#35782;&#25552;&#21319;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26041;&#27861;&#35797;&#22270;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#25110;&#23545;&#24773;&#32490;&#21407;&#22240;&#30340;&#25512;&#29702;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#30340;&#32463;&#21382;&#21644;&#24863;&#21463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20174;&#29992;&#25143;&#30340;&#35282;&#24230;&#29702;&#35299;&#19978;&#19979;&#25991;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#31995;&#32479;&#30340;&#35282;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#30340;&#35282;&#24230;&#65288;&#29992;&#25143;&#30340;&#27442;&#26395;&#21644;&#21453;&#24212;&#65289;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65288;&#31995;&#32479;&#30340;&#24847;&#22270;&#21644;&#21453;&#24212;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#24120;&#35782;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#35282;&#24230;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#19982;ChatGPT&#21644;&#22522;&#20110;T5&#27169;&#22411;&#30340;&#26041;&#27861;&#36827;&#34892;&#25972;&#21512;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#19978;&#20248;&#20110;&#20854;&#20182;&#21487;&#27604;&#36739;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches to empathetic response generation try to incorporate commonsense knowledge or reasoning about the causes of emotions to better understand the user's experiences and feelings. However, these approaches mainly focus on understanding the causalities of context from the user's perspective, ignoring the system's perspective. In this paper, we propose a commonsense-based causality explanation approach for diverse empathetic response generation that considers both the user's perspective (user's desires and reactions) and the system's perspective (system's intentions and reactions). We enhance ChatGPT's ability to reason for the system's perspective by integrating in-context learning with commonsense knowledge. Then, we integrate the commonsense-based causality explanation with both ChatGPT and a T5-based model. Experimental evaluations demonstrate that our method outperforms other comparable methods on both automatic and human evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.00081</link><description>&lt;p&gt;
&#20026;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26500;&#24314;&#35821;&#20041;&#20016;&#23500;&#30340;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#31639;&#27861;&#23558;&#30693;&#35782;&#22270;&#35889;&#35270;&#20026;&#19968;&#20010;&#22810;&#21521;&#26631;&#35760;&#22270;&#65292;&#32570;&#20047;&#25429;&#25417;&#24213;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25429;&#33719;&#20102;&#22823;&#37327;&#20449;&#24687;&#65292;&#36825;&#19968;&#25429;&#33719;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#20174;LLMs&#20013;&#21463;&#30410;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#19981;&#21516;&#29983;&#25104;&#23884;&#20837;&#27169;&#22411;&#21464;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#21508;&#31181;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#65292;&#22914;&#36716;&#23548;&#21644;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20197;&#21450;&#23454;&#20307;&#31867;&#22411;&#39044;&#27979;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#31867;&#22411;&#20449;&#24687;&#12289;LLMs&#20197;&#21450;&#25429;&#25417;&#19981;&#21516;&#25551;&#36848;&#36923;&#36753;&#20844;&#29702;&#20013;&#30340;&#35821;&#20041;&#30340;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#31639;&#27861;&#30340;&#20851;&#38190;&#21453;&#24605;&#23545;&#35770;&#25991;&#36827;&#34892;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding based Knowledge Graph (KG) Completion has gained much attention over the past few years. Most of the current algorithms consider a KG as a multidirectional labeled graph and lack the ability to capture the semantics underlying the schematic information. In a separate development, a vast amount of information has been captured within the Large Language Models (LLMs) which has revolutionized the field of Artificial Intelligence. KGs could benefit from these LLMs and vice versa. This vision paper discusses the existing algorithms for KG completion based on the variations for generating KG embeddings. It starts with discussing various KG completion algorithms such as transductive and inductive link prediction and entity type prediction algorithms. It then moves on to the algorithms utilizing type information within the KGs, LLMs, and finally to algorithms capturing the semantics represented in different description logic axioms. We conclude the paper with a critical reflection on
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#26045;&#24378;&#22823;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#22914;FGSM&#12289;JSMA&#12289;PGD&#21644;C&amp;W&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#20316;&#20026;&#38450;&#24481;&#26041;&#27861;&#26469;&#22686;&#24378;NIDS&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00077</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Novel Deep Learning based Model to Defend Network Intrusion Detection System against Adversarial Attacks. (arXiv:2308.00077v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#26045;&#24378;&#22823;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#22914;FGSM&#12289;JSMA&#12289;PGD&#21644;C&amp;W&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#20316;&#20026;&#38450;&#24481;&#26041;&#27861;&#26469;&#22686;&#24378;NIDS&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#26159;&#20445;&#25252;&#32593;&#32476;&#31354;&#38388;&#20813;&#21463;&#21508;&#31181;&#23433;&#20840;&#39118;&#38505;&#21644;&#26410;&#30693;&#32593;&#32476;&#25915;&#20987;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#24050;&#32463;&#23454;&#26045;&#20102;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;NIDS&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#37117;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24694;&#24847;&#34892;&#20026;&#32773;&#36890;&#36807;&#21521;&#31995;&#32479;&#20013;&#27880;&#20837;&#23545;&#25239;&#25200;&#21160;&#26679;&#26412;&#26469;&#35797;&#22270;&#22238;&#36991;&#25110;&#27450;&#39575;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30740;&#31350;&#24378;&#22823;&#30340;&#23545;&#25239;&#25915;&#20987;&#31639;&#27861;&#21450;&#20854;&#23545;DL-based NIDS&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#26041;&#27861;&#65288;FGSM&#65289;&#65292;Jacobian&#26174;&#33879;&#24615;&#22270;&#25915;&#20987;&#65288;JSMA&#65289;&#65292;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#21644;Carlini&#65286;Wagner&#65288;C&#65286;W&#65289;&#26159;&#22235;&#31181;&#23545;NIDS&#23454;&#26045;&#30340;&#24378;&#22823;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;&#20316;&#20026;&#38450;&#24481;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#26469;&#22686;&#24378;NIDS&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#26524;&#21487;&#20197;&#24635;&#32467;&#20026;&#19977;&#20010;&#38454;&#27573;&#65292;&#21363;1&#65289;&#23545;&#25239;&#25915;&#20987;&#21069;&#65292;2&#65289;&#23545;&#25239;&#25915;&#20987;&#21518;&#65292;3&#65289;&#23545;&#25239;&#25915;&#20987;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network Intrusion Detection System (NIDS) is an essential tool in securing cyberspace from a variety of security risks and unknown cyberattacks. A number of solutions have been implemented for Machine Learning (ML), and Deep Learning (DL) based NIDS. However, all these solutions are vulnerable to adversarial attacks, in which the malicious actor tries to evade or fool the model by injecting adversarial perturbed examples into the system. The main aim of this research work is to study powerful adversarial attack algorithms and their defence method on DL-based NIDS. Fast Gradient Sign Method (FGSM), Jacobian Saliency Map Attack (JSMA), Projected Gradient Descent (PGD) and Carlini &amp; Wagner (C&amp;W) are four powerful adversarial attack methods implemented against the NIDS. As a defence method, Adversarial Training is used to increase the robustness of the NIDS model. The results are summarized in three phases, i.e., 1) before the adversarial attack, 2) after the adversarial attack, and 3) aft
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#25552;&#21319;&#20154;&#32676;&#31649;&#29702;&#30340;&#35268;&#21010;&#21644;&#25805;&#20316;&#38454;&#27573;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21019;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;3D&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36827;&#34892;&#39118;&#38505;&#35782;&#21035;&#65292;&#24182;&#24341;&#20837;&#20102;&#34676;&#34678;&#32467;&#27169;&#22411;&#26469;&#35780;&#20272;&#21644;&#39044;&#27979;&#39118;&#38505;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.00076</link><description>&lt;p&gt;
&#20154;&#32676;&#23433;&#20840;&#31649;&#29702;&#31995;&#32479;&#65306;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27963;&#21160;&#20915;&#31574;&#25903;&#25345;&#30340;&#35268;&#21010;&#21644;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Crowd Safety Manager: Towards Data-Driven Active Decision Support for Planning and Control of Crowd Events. (arXiv:2308.00076v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00076
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#25552;&#21319;&#20154;&#32676;&#31649;&#29702;&#30340;&#35268;&#21010;&#21644;&#25805;&#20316;&#38454;&#27573;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21019;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;3D&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36827;&#34892;&#39118;&#38505;&#35782;&#21035;&#65292;&#24182;&#24341;&#20837;&#20102;&#34676;&#34678;&#32467;&#27169;&#22411;&#26469;&#35780;&#20272;&#21644;&#39044;&#27979;&#39118;&#38505;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#35268;&#21010;&#21644;&#25805;&#20316;&#38454;&#27573;&#30340;&#20154;&#32676;&#31649;&#29702;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#21019;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;&#21487;&#35270;&#21270;&#65292;&#20351;&#29992;3D&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#65292;&#24182;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36827;&#34892;&#39118;&#38505;&#35782;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#34676;&#34678;&#32467;&#8221;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#39044;&#27979;&#39118;&#38505;&#27700;&#24179;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#23458;&#35266;&#20272;&#35745;&#21644;&#39044;&#27979;&#65292;&#22914;&#20132;&#36890;&#27969;&#37327;&#36816;&#33829;&#21644;&#25317;&#25380;&#31243;&#24230;&#65292;&#20197;&#21450;&#21508;&#31181;&#24694;&#21270;&#22240;&#32032;&#65292;&#22914;&#22825;&#27668;&#26465;&#20214;&#12289;&#24773;&#32490;&#21644;&#28216;&#23458;&#30340;&#30446;&#30340;&#65292;&#20197;&#35780;&#20272;&#28508;&#22312;&#20107;&#20214;&#39118;&#38505;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;Scheveningen&#30340;&#20154;&#32676;&#23433;&#20840;&#31649;&#29702;&#39033;&#30446;&#65292;&#20854;&#20013;DigiTwin&#22522;&#20110;&#20016;&#23500;&#30340;&#23454;&#26102;&#25968;&#25454;&#26469;&#28304;&#36827;&#34892;&#24320;&#21457;&#12290;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#25968;&#25454;&#26469;&#28304;&#26159;Resono&#65292;&#25552;&#20379;&#35775;&#23458;&#25968;&#37327;&#21644;&#21160;&#21521;&#30340;&#35265;&#35299;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#19968;&#32452;
&lt;/p&gt;
&lt;p&gt;
This paper presents novel technology and methodology aimed at enhancing crowd management in both the planning and operational phases. The approach encompasses innovative data collection techniques, data integration, and visualization using a 3D Digital Twin, along with the incorporation of artificial intelligence (AI) tools for risk identification. The paper introduces the Bowtie model, a comprehensive framework designed to assess and predict risk levels. The model combines objective estimations and predictions, such as traffic flow operations and crowdedness levels, with various aggravating factors like weather conditions, sentiments, and the purpose of visitors, to evaluate the expected risk of incidents. The proposed framework is applied to the Crowd Safety Manager project in Scheveningen, where the DigiTwin is developed based on a wealth of real-time data sources. One noteworthy data source is Resono, offering insights into the number of visitors and their movements, leveraging a m
&lt;/p&gt;</description></item><item><title>&#22312;&#20197;&#33394;&#21015;-&#24052;&#21202;&#26031;&#22374;&#21644;&#22303;&#32819;&#20854;-&#24211;&#23572;&#24503;&#20914;&#31361;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#25143;&#35821;&#35328;&#23545;ChatGPT&#20013;&#20914;&#31361;&#27515;&#20129;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#25915;&#20987;&#32773;&#30340;&#35821;&#35328;&#36827;&#34892;&#26597;&#35810;&#26102;&#65292;GPT-3.5&#25552;&#20379;&#30340;&#20272;&#35745;&#36739;&#20351;&#29992;&#34987;&#25915;&#20987;&#32676;&#20307;&#30340;&#35821;&#35328;&#26597;&#35810;&#26102;&#20302;27&#177;11&#65285;&#12290;&#27492;&#22806;&#65292;&#21542;&#35748;&#23384;&#22312;&#27492;&#31867;&#34989;&#20987;&#30340;&#22238;&#31572;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#36825;&#31181;&#24046;&#24322;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#26426;&#21046;&#65292;&#21487;&#33021;&#21152;&#22823;&#29616;&#26377;&#30340;&#23186;&#20307;&#20559;&#35265;&#24182;&#21152;&#21095;&#20449;&#24687;&#23396;&#31435;&#12290;</title><link>http://arxiv.org/abs/2308.00072</link><description>&lt;p&gt;
&#29992;&#25143;&#35821;&#35328;&#23545;ChatGPT&#20013;&#20914;&#31361;&#27515;&#20129;&#20272;&#35745;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
How User Language Affects Conflict Fatality Estimates in ChatGPT. (arXiv:2308.00072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00072
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#33394;&#21015;-&#24052;&#21202;&#26031;&#22374;&#21644;&#22303;&#32819;&#20854;-&#24211;&#23572;&#24503;&#20914;&#31361;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#25143;&#35821;&#35328;&#23545;ChatGPT&#20013;&#20914;&#31361;&#27515;&#20129;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#25915;&#20987;&#32773;&#30340;&#35821;&#35328;&#36827;&#34892;&#26597;&#35810;&#26102;&#65292;GPT-3.5&#25552;&#20379;&#30340;&#20272;&#35745;&#36739;&#20351;&#29992;&#34987;&#25915;&#20987;&#32676;&#20307;&#30340;&#35821;&#35328;&#26597;&#35810;&#26102;&#20302;27&#177;11&#65285;&#12290;&#27492;&#22806;&#65292;&#21542;&#35748;&#23384;&#22312;&#27492;&#31867;&#34989;&#20987;&#30340;&#22238;&#31572;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#36825;&#31181;&#24046;&#24322;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#26426;&#21046;&#65292;&#21487;&#33021;&#21152;&#22823;&#29616;&#26377;&#30340;&#23186;&#20307;&#20559;&#35265;&#24182;&#21152;&#21095;&#20449;&#24687;&#23396;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI&#30340;ChatGPT&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#24378;&#22823;&#30340;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#21644;&#20449;&#24687;&#26816;&#32034;&#33021;&#21147;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35813;&#27169;&#22411;&#26159;&#21542;&#20250;&#22797;&#21046;&#35821;&#35328;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#30340;&#25285;&#24551;&#20063;&#19981;&#26029;&#20986;&#29616;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20197;&#33394;&#21015;-&#24052;&#21202;&#26031;&#22374;&#21644;&#22303;&#32819;&#20854;-&#24211;&#23572;&#24503;&#20914;&#31361;&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20351;&#29992;GPT-3.5&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;&#26597;&#35810;&#36807;&#31243;&#65292;&#20197;&#24076;&#20271;&#26469;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#26597;&#35810;&#20851;&#20110;&#29305;&#23450;&#31354;&#34989;&#20013;&#30340;&#20260;&#20129;&#20154;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#24403;&#20351;&#29992;&#25915;&#20987;&#32773;&#30340;&#35821;&#35328;&#36827;&#34892;&#26597;&#35810;&#26102;&#65292;GPT-3.5&#25552;&#20379;&#30340;&#27515;&#20129;&#20272;&#35745;&#36739;&#20351;&#29992;&#34987;&#25915;&#20987;&#32676;&#20307;&#30340;&#35821;&#35328;&#26597;&#35810;&#26102;&#20302;27&#177;11&#65285;&#12290;&#21542;&#35748;&#23384;&#22312;&#27492;&#31867;&#34989;&#20987;&#30340;&#22238;&#31572;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#36825;&#31181;&#24046;&#24322;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;&#24120;&#35268;&#25628;&#32034;&#24341;&#25806;&#20013;&#19981;&#23384;&#22312;&#30340;&#26032;&#30340;&#20559;&#35265;&#26426;&#21046;&#12290;&#36825;&#31181;&#35821;&#35328;&#20559;&#35265;&#26377;&#21487;&#33021;&#25918;&#22823;&#29616;&#26377;&#30340;&#23186;&#20307;&#20559;&#35265;&#65292;&#24182;&#21152;&#21095;&#20449;&#24687;&#23396;&#31435;&#65292;&#26368;&#32456;&#21152;&#37325;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI's ChatGPT language model has gained popularity as a powerful tool for complex problem-solving and information retrieval. However, concerns arise about the reproduction of biases present in the language-specific training data. In this study, we address this issue in the context of the Israeli-Palestinian and Turkish-Kurdish conflicts. Using GPT-3.5, we employed an automated query procedure to inquire about casualties in specific airstrikes, in both Hebrew and Arabic for the former conflict and Turkish and Kurdish for the latter. Our analysis reveals that GPT-3.5 provides 27$\pm$11 percent lower fatality estimates when queried in the language of the attacker than in the language of the targeted group. Evasive answers denying the existence of such attacks further increase the discrepancy, creating a novel bias mechanism not present in regular search engines. This language bias has the potential to amplify existing media biases and contribute to information bubbles, ultimately reinf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00071</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#26041;&#27861;&#29992;&#20110;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20351;&#29992;&#20102;&#21253;&#21547;&#22266;&#26377;&#20559;&#35265;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#25345;&#32493;&#31995;&#32479;&#24615;&#27495;&#35270;&#65292;&#22240;&#27492;&#65292;&#23457;&#26597;&#21644;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23558;&#20844;&#24179;&#24615;&#25972;&#21512;&#21040;&#23427;&#20204;&#30340;&#21457;&#23637;&#20013;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20844;&#27491;&#21644;&#26080;&#20559;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Vicuna-13B-v1.3&#30340;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;13B&#21040;33B&#30340;&#35268;&#27169;&#25193;&#23637;&#20250;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#29702;&#21487;&#33021;&#26159;&#20351;LLMs&#22312;&#21051;&#26495;&#21360;&#35937;&#31561;&#39046;&#22495;&#20219;&#21153;&#19978;&#36229;&#36234;&#35268;&#27169;&#23450;&#24459;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#36873;&#23450;&#30340;&#25512;&#29702;&#36861;&#36394;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#31361;&#20986;&#26174;&#31034;&#20102;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.00031</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00031
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#36817;&#21313;&#24180;&#26469;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#20043;&#19968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#25104;&#20026;&#38750;&#24120;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;RL&#24212;&#29992;&#20110;&#29983;&#25104;AI&#20013;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#65292;&#20197;&#21450;&#20316;&#20026;&#19968;&#31181;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#36731;&#26494;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#35843;&#26597;&#32467;&#26524;&#20013;&#23545;&#36825;&#20010;&#36855;&#20154;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20154;&#26426;&#20132;&#20114;&#30340;&#26032;&#22411; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102; Alpha-GPT&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102; Alpha-GPT &#22312;&#37327;&#21270;&#25237;&#36164;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.00016</link><description>&lt;p&gt;
Alpha-GPT&#65306;&#20154;&#26426;&#20132;&#20114;&#24335; Alpha &#25366;&#25496;&#22312;&#37327;&#21270;&#25237;&#36164;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment. (arXiv:2308.00016v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20154;&#26426;&#20132;&#20114;&#30340;&#26032;&#22411; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102; Alpha-GPT&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102; Alpha-GPT &#22312;&#37327;&#21270;&#25237;&#36164;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#25237;&#36164;&#30740;&#31350;&#20013;&#65292;&#25366;&#25496;&#26032;&#30340; alpha&#65288;&#26377;&#25928;&#30340;&#20132;&#26131;&#20449;&#21495;&#25110;&#22240;&#23376;&#65289;&#26159;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20256;&#32479;&#30340; alpha &#25366;&#25496;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#25163;&#24037;&#21512;&#25104;&#22240;&#23376;&#36824;&#26159;&#31639;&#27861;&#25366;&#25496;&#22240;&#23376;&#65288;&#22914;&#36951;&#20256;&#32534;&#31243;&#25628;&#32034;&#65289;&#65292;&#37117;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#22312;&#23454;&#26045;&#37327;&#21270;&#20998;&#26512;&#24072;&#30340;&#24819;&#27861;&#26041;&#38754;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24341;&#20837;&#20102;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#20010;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102; Alpha-GPT&#65292;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335; alpha &#25366;&#25496;&#31995;&#32479;&#26694;&#26550;&#65292;&#20197;&#19968;&#31181;&#21551;&#21457;&#24335;&#30340;&#26041;&#24335;&#8220;&#29702;&#35299;&#8221;&#37327;&#21270;&#30740;&#31350;&#20154;&#21592;&#30340;&#24819;&#27861;&#65292;&#24182;&#36755;&#20986;&#20855;&#26377;&#21019;&#36896;&#24615;&#12289;&#28145;&#20837;&#27934;&#23519;&#21147;&#21644;&#26377;&#25928;&#24615;&#30340; alpha&#12290;&#36890;&#36807;&#22810;&#20010; alpha &#25366;&#25496;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; Alpha-GPT &#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important tasks in quantitative investment research is mining new alphas (effective trading signals or factors). Traditional alpha mining methods, either hand-crafted factor synthesizing or algorithmic factor mining (e.g., search with genetic programming), have inherent limitations, especially in implementing the ideas of quants. In this work, we propose a new alpha mining paradigm by introducing human-AI interaction, and a novel prompt engineering algorithmic framework to implement this paradigm by leveraging the power of large language models. Moreover, we develop Alpha-GPT, a new interactive alpha mining system framework that provides a heuristic way to ``understand'' the ideas of quant researchers and outputs creative, insightful, and effective alphas. We demonstrate the effectiveness and advantage of Alpha-GPT via a number of alpha mining experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00002</link><description>&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#19982;&#33719;&#21462;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#26159;&#25351;&#29702;&#35299;&#30701;&#35821;&#12289;&#21160;&#20316;&#21644;&#20107;&#20214;&#30340;&#20856;&#22411;&#26102;&#38388;&#32972;&#26223;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38656;&#35201;&#36825;&#31181;&#30693;&#35782;&#30340;&#38382;&#39064;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33021;&#24212;&#29992;&#20110;&#26102;&#38388;&#32447;&#25688;&#35201;&#12289;&#26102;&#38388;&#38382;&#31572;&#21644;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#31561;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#21892;&#20110;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#21644;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#37319;&#21462;&#25463;&#24452;&#65292;&#24182;&#38519;&#20837;&#31616;&#21333;&#30340;&#35821;&#35328;&#38519;&#38449;&#12290;&#26412;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#36890;&#36807;&#21508;&#31181;&#22686;&#24378;&#26041;&#24335;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#23545;&#36234;&#26469;&#36234;&#22810;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#33258;&#25105;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#24341;&#20837;&#20102;&#20004;&#20010;&#19981;&#21516;&#27169;&#24577;&#26469;&#25429;&#25417;"De Re"&#21644;"De Dicto"&#30693;&#35782;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#27169;&#24577;&#19981;&#33021;&#20114;&#30456;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.00001</link><description>&lt;p&gt;
&#22312;&#33258;&#25105;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#30340;"De Re"&#21644;"De Dicto"&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
De Re and De Dicto Knowledge in Egocentric Setting. (arXiv:2308.00001v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#33258;&#25105;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#24341;&#20837;&#20102;&#20004;&#20010;&#19981;&#21516;&#27169;&#24577;&#26469;&#25429;&#25417;"De Re"&#21644;"De Dicto"&#30693;&#35782;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#27169;&#24577;&#19981;&#33021;&#20114;&#30456;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102; "&#33258;&#25105;&#20013;&#24515;" &#36825;&#19968;&#26415;&#35821;&#65292;&#29992;&#20110;&#30740;&#31350;&#20195;&#29702;&#22120;&#30340;&#23646;&#24615;&#32780;&#19981;&#26159;&#21487;&#33021;&#19990;&#30028;&#30340;&#23646;&#24615;&#30340;&#36923;&#36753;&#31995;&#32479;&#12290;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#25429;&#25417;"De Re"&#21644;"De Dicto"&#30693;&#35782;&#30340;&#19981;&#21516;&#27169;&#24577;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#27169;&#24577;&#19981;&#33021;&#36890;&#36807;&#24444;&#27492;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior proposes the term "egocentric" for logical systems that study properties of agents rather than properties of possible worlds. In such a setting, the paper introduces two different modalities capturing de re and de dicto knowledge and proves that these two modalities are not definable through each other.
&lt;/p&gt;</description></item><item><title>L3DMC&#26159;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#26354;&#29575;&#31354;&#38388;&#36827;&#34892;&#32456;&#36523;&#23398;&#20064;&#30340;&#33976;&#39311;&#31574;&#30053;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#32500;&#25252;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#26469;&#20445;&#30041;&#24050;&#32463;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.16459</link><description>&lt;p&gt;
L3DMC: &#20351;&#29992;&#28151;&#21512;&#26354;&#29575;&#31354;&#38388;&#30340;&#33976;&#39311;&#36827;&#34892;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space. (arXiv:2307.16459v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16459
&lt;/p&gt;
&lt;p&gt;
L3DMC&#26159;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#26354;&#29575;&#31354;&#38388;&#36827;&#34892;&#32456;&#36523;&#23398;&#20064;&#30340;&#33976;&#39311;&#31574;&#30053;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#32500;&#25252;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#26469;&#20445;&#30041;&#24050;&#32463;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32456;&#36523;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#22240;&#20026;&#22312;&#39034;&#24207;&#23398;&#20064;&#26032;&#27010;&#24565;&#26102;&#23884;&#20837;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#29616;&#26377;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#22312;&#22266;&#23450;&#26354;&#29575;&#65288;&#20363;&#22914;&#38646;&#26354;&#29575;&#30340;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65289;&#19978;&#36816;&#34892;&#65292;&#36825;&#24182;&#19981;&#36866;&#21512;&#24314;&#27169;&#22797;&#26434;&#30340;&#25968;&#25454;&#20960;&#20309;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#33976;&#39311;&#31574;&#30053;&#30452;&#25509;&#24212;&#29992;&#20110;&#20302;&#32500;&#23884;&#20837;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#39640;&#24230;&#31283;&#23450;&#26469;&#38459;&#30861;&#32456;&#36523;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#26032;&#27010;&#24565;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L3DMC&#30340;&#33976;&#39311;&#31574;&#30053;&#65292;&#23427;&#22312;&#28151;&#21512;&#26354;&#29575;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#32500;&#25252;&#22797;&#26434;&#30340;&#20960;&#20309;&#32467;&#26500;&#26469;&#20445;&#30041;&#24050;&#32463;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#27491;&#23450;&#30340;&#37325;&#26500;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#23558;&#22266;&#23450;&#26354;&#29575;&#31354;&#38388;&#65288;&#27431;&#20960;&#37324;&#24503;&#21644;&#21452;&#26354;&#65289;&#30340;&#25237;&#24433;&#20302;&#32500;&#23884;&#20837;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of a lifelong learning (L3) model degrades when it is trained on a series of tasks, as the geometrical formation of the embedding space changes while learning novel concepts sequentially. The majority of existing L3 approaches operate on a fixed-curvature (e.g., zero-curvature Euclidean) space that is not necessarily suitable for modeling the complex geometric structure of data. Furthermore, the distillation strategies apply constraints directly on low-dimensional embeddings, discouraging the L3 model from learning new concepts by making the model highly stable. To address the problem, we propose a distillation strategy named L3DMC that operates on mixed-curvature spaces to preserve the already-learned knowledge by modeling and maintaining complex geometrical structures. We propose to embed the projected low dimensional embedding of fixed-curvature spaces (Euclidean and hyperbolic) to higher-dimensional Reproducing Kernel Hilbert Space (RKHS) using a positive-definite k
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#36890;&#36807;&#36817;&#20284;&#25968;&#25454;&#27969;&#24418;&#65292;&#24182;&#29992;&#32447;&#24615;&#23376;&#31354;&#38388;&#24314;&#27169;&#32467;&#26500;&#65292;&#26469;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.16419</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#30340;&#23376;&#31354;&#38388;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Subspace Distillation for Continual Learning. (arXiv:2307.16419v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16419
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#36890;&#36807;&#36817;&#20284;&#25968;&#25454;&#27969;&#24418;&#65292;&#24182;&#29992;&#32447;&#24615;&#23376;&#31354;&#38388;&#24314;&#27169;&#32467;&#26500;&#65292;&#26469;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#26368;&#32456;&#30340;&#30446;&#26631;&#26159;&#20445;&#30041;&#22312;&#21069;&#38754;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#36951;&#24536;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#32771;&#34385;&#20102;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;/&#36755;&#20986;&#31354;&#38388;&#30340;&#27969;&#24418;&#32467;&#26500;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#20013;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#25968;&#25454;&#27969;&#24418;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#36890;&#36807;&#32447;&#24615;&#23376;&#31354;&#38388;&#26469;&#24314;&#27169;&#32467;&#26500;&#24182;&#20445;&#25345;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26032;&#27010;&#24565;&#26102;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#23376;&#31354;&#38388;&#24314;&#27169;&#20855;&#26377;&#19968;&#20123;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#36866;&#24212;&#20998;&#31867;&#21644;&#20998;&#21106;&#38382;&#39064;&#12290;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ultimate objective in continual learning is to preserve knowledge learned in preceding tasks while learning new tasks. To mitigate forgetting prior knowledge, we propose a novel knowledge distillation technique that takes into the account the manifold structure of the latent/output space of a neural network in learning novel tasks. To achieve this, we propose to approximate the data manifold up-to its first order, hence benefiting from linear subspaces to model the structure and maintain the knowledge of a neural network while learning novel concepts. We demonstrate that the modeling with subspaces provides several intriguing properties, including robustness to noise and therefore effective for mitigating Catastrophic Forgetting in continual learning. We also discuss and show how our proposed method can be adopted to address both classification and segmentation problems. Empirically, we observe that our proposed method outperforms various continual learning methods on several challe
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#35270;&#35273;&#22270;&#20687;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#27169;&#31946;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#23436;&#25972;&#24615;&#26102;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16210</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#32570;&#22833;&#21644;&#27169;&#31946;&#30340;&#35270;&#35273;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment. (arXiv:2307.16210v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16210
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#35270;&#35273;&#22270;&#20687;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#27169;&#31946;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#23436;&#25972;&#24615;&#26102;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#37325;&#35201;&#25193;&#23637;&#65292;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#26469;&#35782;&#21035;&#36328;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20043;&#38388;&#30340;&#30456;&#21516;&#23454;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MMEA&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#29305;&#24449;&#30340;&#34701;&#21512;&#33539;&#24335;&#19978;&#65292;&#32780;&#24573;&#35270;&#20102;&#32570;&#22833;&#21644;&#20869;&#22312;&#27169;&#31946;&#24615;&#30340;&#35270;&#35273;&#22270;&#20687;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#35270;&#35273;&#27169;&#24577;&#19981;&#23436;&#25972;&#24615;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#20998;&#26512;&#65292;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;MMEA-UMVM&#25968;&#25454;&#38598;&#19978;&#23545;&#26368;&#26032;&#30340;MMEA&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#28085;&#30422;&#21452;&#35821;&#21644;&#21333;&#35821;&#23545;&#40784;KGs&#30340;&#31867;&#22411;&#65292;&#24182;&#37319;&#29992;&#26631;&#20934;&#65288;&#38750;&#36845;&#20195;&#65289;&#21644;&#36845;&#20195;&#35757;&#32451;&#33539;&#24335;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38754;&#23545;&#27169;&#24577;&#19981;&#23436;&#25972;&#24615;&#26102;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#27169;&#24577;&#22122;&#22768;&#65292;&#24182;&#22312;&#39640;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#24615;&#33021;&#25391;&#33633;&#25110;&#19979;&#38477;&#12290;&#36825;&#35777;&#26126;&#20102;&#22686;&#21152;&#35270;&#35273;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;PyTorch&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#22797;&#21046;&#30740;&#31350;&#65292;&#36890;&#36807;&#35843;&#26597;&#21644;&#35780;&#20272;&#38169;&#35823;&#30340;&#21407;&#22240;&#21644;&#30151;&#29366;&#65292;&#25552;&#20379;&#20102;&#23545;&#38169;&#35823;&#35782;&#21035;&#21644;&#20462;&#22797;&#36807;&#31243;&#30340;&#20102;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13777</link><description>&lt;p&gt;
PyTorch&#20869;&#37096;&#30340;&#38169;&#35823;&#65306;&#19968;&#20010;&#22797;&#21046;&#30740;&#31350;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Bugs Inside PyTorch: A Replication Study. (arXiv:2307.13777v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;PyTorch&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#22797;&#21046;&#30740;&#31350;&#65292;&#36890;&#36807;&#35843;&#26597;&#21644;&#35780;&#20272;&#38169;&#35823;&#30340;&#21407;&#22240;&#21644;&#30151;&#29366;&#65292;&#25552;&#20379;&#20102;&#23545;&#38169;&#35823;&#35782;&#21035;&#21644;&#20462;&#22797;&#36807;&#31243;&#30340;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#35782;&#21035;&#22797;&#26434;&#25968;&#25454;&#27169;&#24335;&#21644;&#23454;&#29616;&#26234;&#33021;&#34892;&#20026;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#36719;&#20214;&#31995;&#32479;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#12290;&#36825;&#31181;&#36719;&#20214;&#24320;&#21457;&#21464;&#38761;&#30340;&#26680;&#24515;&#25512;&#21160;&#32773;&#26159;&#26131;&#20110;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#21487;&#29992;&#24615;&#12290;&#20687;PyTorch&#21644;TensorFlow&#36825;&#26679;&#30340;&#24211;&#36171;&#20104;&#21508;&#31181;&#26234;&#33021;&#31995;&#32479;&#20197;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#31639;&#27861;&#21644;&#37197;&#32622;&#36873;&#39033;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21463;&#27426;&#36814;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#30340;&#38169;&#35823;&#20063;&#21487;&#33021;&#23545;&#20854;&#25152;&#25903;&#25345;&#30340;&#31995;&#32479;&#30340;&#36136;&#37327;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#65292;&#22240;&#27492;&#20102;&#35299;&#22914;&#20309;&#22312;&#36825;&#20123;&#24211;&#20013;&#35782;&#21035;&#21644;&#20462;&#22797;&#38169;&#35823;&#38750;&#24120;&#37325;&#35201;&#12290;&#21463;Jia&#31561;&#20154;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;TensorFlow&#20013;&#38169;&#35823;&#30340;&#35782;&#21035;&#21644;&#20462;&#22797;&#36807;&#31243;&#65292;&#25105;&#20204;&#23545;&#38750;&#24120;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;PyTorch&#20013;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;PyTorch&#24320;&#21457;&#36807;&#31243;&#20013;&#21457;&#29616;&#30340;&#38169;&#35823;&#30340;&#21407;&#22240;&#21644;&#30151;&#29366;&#65292;&#24182;&#35780;&#20272;&#20102;&#20462;&#22797;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software systems are increasingly relying on deep learning components, due to their remarkable capability of identifying complex data patterns and powering intelligent behaviour. A core enabler of this change in software development is the availability of easy-to-use deep learning libraries. Libraries like PyTorch and TensorFlow empower a large variety of intelligent systems, offering a multitude of algorithms and configuration options, applicable to numerous domains of systems. However, bugs in those popular deep learning libraries also may have dire consequences for the quality of systems they enable; thus, it is important to understand how bugs are identified and fixed in those libraries.  Inspired by a study of Jia et al., which investigates the bug identification and fixing process at TensorFlow, we characterize bugs in the PyTorch library, a very popular deep learning framework. We investigate the causes and symptoms of bugs identified during PyTorch's development, and assess the
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11224</link><description>&lt;p&gt;
Jina Embeddings:&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11224
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#30001;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#32452;&#25104;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#25991;&#26412;&#36755;&#20837;&#36716;&#21270;&#20026;&#25968;&#20540;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#22312;&#23494;&#38598;&#26816;&#32034;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20174;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#21644;&#19977;&#20803;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#25454;&#28165;&#29702;&#22312;&#25968;&#25454;&#38598;&#20934;&#22791;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#26368;&#21518;&#21033;&#29992;Massive Textual Embedding Benchmark&#65288;MTEB&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#36827;&#34892;&#20102;&#20934;&#30830;&#24314;&#27169;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.09866</link><description>&lt;p&gt;
&#26816;&#27979;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network. (arXiv:2307.09866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#36827;&#34892;&#20102;&#20934;&#30830;&#24314;&#27169;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#25551;&#36848;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30340;&#33030;&#24369;&#24615;&#23545;&#25105;&#20204;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#36825;&#20123;&#22522;&#30784;&#35774;&#26045;&#26159;&#22478;&#24066;&#27491;&#24120;&#36816;&#34892;&#25152;&#24517;&#38656;&#30340;&#24037;&#31243;&#35774;&#26045;&#65292;&#20197;&#32593;&#32476;&#30340;&#24418;&#24335;&#33258;&#28982;&#23384;&#22312;&#12290;&#28508;&#22312;&#30340;&#24212;&#29992;&#21253;&#25324;&#20445;&#25252;&#33030;&#24369;&#35774;&#26045;&#21644;&#35774;&#35745;&#31283;&#20581;&#30340;&#25299;&#25169;&#32467;&#26500;&#31561;&#12290;&#30001;&#20110;&#19981;&#21516;&#25299;&#25169;&#29305;&#24615;&#21644;&#22522;&#30784;&#35774;&#26045;&#33030;&#24369;&#24615;&#20197;&#21450;&#20854;&#22797;&#26434;&#30340;&#28436;&#21270;&#26426;&#21046;&#20043;&#38388;&#30340;&#24378;&#20851;&#32852;&#65292;&#19968;&#20123;&#21551;&#21457;&#24335;&#20998;&#26512;&#21644;&#26426;&#22120;&#36741;&#21161;&#20998;&#26512;&#22312;&#35299;&#20915;&#36825;&#31181;&#22330;&#26223;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#24314;&#27169;&#20026;&#24322;&#26500;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20934;&#30830;&#22320;&#25551;&#36848;&#22478;&#24066;&#31995;&#32479;&#30340;&#33030;&#24369;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#24322;&#26500;&#22270;&#65292;&#20174;&#32780;&#33021;&#22815;&#25429;&#25417;&#32423;&#32852;&#22833;&#36133;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09009</link><description>&lt;p&gt;
ChatGPT&#30340;&#34892;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3.5&#21644;GPT-4&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#26356;&#26032;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#30340;2023&#24180;3&#26376;&#21644;2023&#24180;6&#26376;&#29256;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28041;&#21450;&#22235;&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;1&#65289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;2&#65289;&#22238;&#31572;&#25935;&#24863;/&#21361;&#38505;&#38382;&#39064;&#65292;3&#65289;&#29983;&#25104;&#20195;&#30721;&#21644;4&#65289;&#35270;&#35273;&#25512;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#21644;GPT-4&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#22312;&#26102;&#38388;&#19978;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;GPT-4&#65288;2023&#24180;3&#26376;&#65289;&#22312;&#35782;&#21035;&#36136;&#25968;&#26041;&#38754;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65288;&#20934;&#30830;&#29575;&#20026;97.6%&#65289;&#65292;&#20294;GPT-4&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#30456;&#21516;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#38750;&#24120;&#24046;&#65288;&#20934;&#30830;&#29575;&#20026;2.4%&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;GPT-3.5&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#27604;GPT-3.5&#65288;2023&#24180;3&#26376;&#65289;&#35201;&#22909;&#24471;&#22810;&#12290;GPT-4&#22312;6&#26376;&#20221;&#23545;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#30340;&#24847;&#24895;&#36739;3&#26376;&#20221;&#35201;&#20302;&#65292;&#32780;&#26080;&#35770;&#26159;GPT-4&#36824;&#26159;GPT-3.5&#22312;6&#26376;&#20221;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#37117;&#26377;&#26356;&#22810;&#30340;&#26684;&#24335;&#38169;&#35823;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#30456;&#21516;LLM&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#30495;&#23454;&#23545;&#24212;&#30340;&#20851;&#38190;&#28857;&#23545;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#24378;&#21046;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.08930</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Graph Matching Based on Cycle Consistency. (arXiv:2307.08930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#30495;&#23454;&#23545;&#24212;&#30340;&#20851;&#38190;&#28857;&#23545;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#24378;&#21046;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#31232;&#30095;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#20013;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24212;&#29992;&#20110;&#22270;&#20687;&#20013;&#30340;&#20851;&#38190;&#28857;&#21305;&#37197;&#12290;&#19982;&#26631;&#20934;&#30340;&#8220;&#30417;&#30563;&#8221;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20851;&#38190;&#28857;&#23545;&#20043;&#38388;&#30340;&#30495;&#23454;&#23545;&#24212;&#12290;&#30456;&#21453;&#65292;&#23427;&#36890;&#36807;&#24378;&#21046;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#12290;&#30001;&#20110;&#21305;&#37197;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#26159;&#31163;&#25955;&#30340;&#65292;&#23427;&#20204;&#30340;&#23548;&#25968;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#32452;&#21512;&#27714;&#35299;&#22120;&#30340;&#40657;&#30418;&#24494;&#20998;&#30340;&#26368;&#26032;&#32467;&#26524;&#22522;&#30784;&#19978;&#26500;&#24314;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#19982;&#20219;&#24847;&#32593;&#32476;&#26550;&#26500;&#21644;&#32452;&#21512;&#27714;&#35299;&#22120;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We contribute to the sparsely populated area of unsupervised deep graph matching with application to keypoint matching in images. Contrary to the standard \emph{supervised} approach, our method does not require ground truth correspondences between keypoint pairs. Instead, it is self-supervised by enforcing consistency of matchings between images of the same object category. As the matching and the consistency loss are discrete, their derivatives cannot be straightforwardly used for learning. We address this issue in a principled way by building our method upon the recent results on black-box differentiation of combinatorial solvers. This makes our method exceptionally flexible, as it is compatible with arbitrary network architectures and combinatorial solvers. Our experimental evaluation suggests that our technique sets a new state-of-the-art for unsupervised graph matching.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pair-Net&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37197;&#23545;&#25552;&#26696;&#32593;&#32476;&#65288;PPN&#65289;&#26469;&#23398;&#20064;&#21644;&#36807;&#28388;&#20027;&#20307;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#31232;&#30095;&#37197;&#23545;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#20013;&#24573;&#35270;&#30340;&#23545;&#35937;&#38388;&#37197;&#23545;&#22238;&#24518;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.08699</link><description>&lt;p&gt;
&#32858;&#28966;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#37197;&#23545;-&#20851;&#31995;&#65306;&#22522;&#20110;&#37197;&#23545;&#32593;&#32476;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pair then Relation: Pair-Net for Panoptic Scene Graph Generation. (arXiv:2307.08699v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pair-Net&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37197;&#23545;&#25552;&#26696;&#32593;&#32476;&#65288;PPN&#65289;&#26469;&#23398;&#20064;&#21644;&#36807;&#28388;&#20027;&#20307;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#31232;&#30095;&#37197;&#23545;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#20013;&#24573;&#35270;&#30340;&#23545;&#35937;&#38388;&#37197;&#23545;&#22238;&#24518;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26159;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#20854;&#26088;&#22312;&#20351;&#29992;&#20840;&#26223;&#20998;&#21106;&#20195;&#26367;&#36793;&#30028;&#26694;&#26469;&#21019;&#24314;&#26356;&#20840;&#38754;&#30340;&#22330;&#26223;&#22270;&#34920;&#31034;&#12290;&#19982;&#22330;&#26223;&#22270;&#29983;&#25104;&#30456;&#27604;&#65292;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#20855;&#26377;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65306;&#20687;&#32032;&#32423;&#20998;&#21106;&#36755;&#20986;&#21644;&#23436;&#20840;&#20851;&#31995;&#25506;&#32034;&#65288;&#23427;&#36824;&#32771;&#34385;&#20102;&#29289;&#20307;&#21644;&#29289;&#36136;&#20043;&#38388;&#30340;&#20851;&#31995;&#65289;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#19979;&#28216;&#20219;&#21153;&#25110;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#26412;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#19988;&#24378;&#22823;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#22522;&#20934;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#24403;&#21069;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#29942;&#39048;&#65292;&#21457;&#29616;&#23545;&#35937;&#38388;&#37197;&#23545;&#30340;&#22238;&#24518;&#29575;&#26159;&#20808;&#21069;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#25152;&#24573;&#35270;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#21644;&#26368;&#36817;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65306;&#37197;&#23545;-&#20851;&#31995;&#65288;Pair-Net&#65289;&#65292;&#23427;&#20351;&#29992;&#37197;&#23545;&#25552;&#26696;&#32593;&#32476;&#65288;PPN&#65289;&#26469;&#23398;&#20064;&#21644;&#36807;&#28388;&#20027;&#20307;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#31232;&#30095;&#37197;&#23545;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation (SGG) that aims to create a more comprehensive scene graph representation using panoptic segmentation instead of boxes. Compared to SGG, PSG has several challenging problems: pixel-level segment outputs and full relationship exploration (It also considers thing and stuff relation). Thus, current PSG methods have limited performance, which hinders downstream tasks or applications. The goal of this work aims to design a novel and strong baseline for PSG. To achieve that, we first conduct an in-depth analysis to identify the bottleneck of the current PSG models, finding that inter-object pair-wise recall is a crucial factor that was ignored by previous PSG methods. Based on this and the recent query-based frameworks, we present a novel framework: Pair then Relation (Pair-Net), which uses a Pair Proposal Network (PPN) to learn and filter sparse pair-wise relationships between subjects and objects. Moreover, we also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#20107;&#23454;&#36335;&#24452;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#36890;&#36807;&#30830;&#23450;&#26367;&#20195;&#36335;&#24452;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#35266;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#24335;&#65292;&#24182;&#24110;&#21161;&#35782;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.07764</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI with counterfactual paths. (arXiv:2307.07764v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#20107;&#23454;&#36335;&#24452;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#36890;&#36807;&#30830;&#23450;&#26367;&#20195;&#36335;&#24452;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#35266;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#24335;&#65292;&#24182;&#24110;&#21161;&#35782;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#21407;&#21017;&#19978;&#26088;&#22312;&#20351;&#40657;&#30418;&#27169;&#22411;&#36879;&#26126;&#21487;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26465;&#20214;&#32622;&#25442;&#29983;&#25104;&#20102;&#21453;&#20107;&#23454;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#30830;&#23450;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#32467;&#26524;&#30340;&#26367;&#20195;&#36335;&#24452;&#26469;&#25552;&#20379;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#35299;&#37322;&#30340;&#29983;&#25104;&#12290;&#36890;&#36807;&#26816;&#26597;&#30693;&#35782;&#22270;&#35889;&#20013;&#36755;&#20837;&#25968;&#25454;&#30340;&#20551;&#35774;&#24615;&#21464;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#31995;&#32479;&#22320;&#39564;&#35777;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#26816;&#26597;&#23545;&#27169;&#22411;&#39044;&#27979;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#25110;&#29305;&#24449;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#30340;&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#26356;&#30452;&#35266;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#24335;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) is an increasingly important area of research in machine learning, which in principle aims to make black-box models transparent and interpretable. In this paper, we propose a novel approach to XAI that uses counterfactual paths generated by conditional permutations. Our method provides counterfactual explanations by identifying alternative paths that could have led to different outcomes. The proposed method is particularly suitable for generating explanations based on counterfactual paths in knowledge graphs. By examining hypothetical changes to the input data in the knowledge graph, we can systematically validate the behaviour of the model and examine the features or combination of features that are most important to the model's predictions. Our approach provides a more intuitive and interpretable explanation for the model's behaviour than traditional feature weighting methods and can help identify and mitigate biases in the model.
&lt;/p&gt;</description></item><item><title>SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;</title><link>http://arxiv.org/abs/2307.04192</link><description>&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#65306;&#33258;&#36866;&#24212;&#37319;&#26679;&#29992;&#20110;&#39640;&#25928;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04192
&lt;/p&gt;
&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#26159;&#35270;&#39057;&#29702;&#35299;&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#37197;&#22791;&#20102;&#35270;&#39057;&#21464;&#25442;&#22120;(Video Transformers)&#65292;&#23454;&#29616;&#20102;&#26102;&#38388;&#24314;&#27169;&#24182;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#22330;&#26223;&#20013;&#36807;&#20110;&#26114;&#36149;&#12290;&#19968;&#31181;&#32463;&#27982;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#21482;&#23545;&#35270;&#39057;&#30340;&#19968;&#23567;&#37096;&#20998;&#24103;&#36827;&#34892;&#37319;&#26679;&#65292;&#26469;&#20195;&#34920;&#35270;&#39057;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#24182;&#22312;&#36825;&#20123;&#37319;&#26679;&#24103;&#19978;&#35843;&#25972;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#36890;&#24120;&#38543;&#26426;&#37319;&#26679;&#19968;&#32452;&#24103;&#25110;&#29255;&#27573;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#37096;&#20851;&#32852;&#24615;&#21644;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26080;&#30446;&#26631;&#30340;&#37319;&#26679;&#21487;&#33021;&#20250;&#36951;&#28431;&#21487;&#20197;&#25512;&#23548;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#20851;&#38190;&#24103;&#65292;&#22312;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#22686;&#21152;&#26102;&#65292;&#24773;&#20917;&#20250;&#21464;&#24471;&#26356;&#31967;&#65292;&#32780;&#38543;&#30528;&#35270;&#39057;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24103;&#37319;&#26679;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#19968;&#23450;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#24448;&#24448;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#36807;&#31243;&#65292;&#36825;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#35299;&#37322;&#21644;&#29702;&#35299;&#26377;&#30528;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.02477</link><description>&lt;p&gt;
&#25512;&#29702;&#36824;&#26159;&#32972;&#35829;&#65311;&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. (arXiv:2307.02477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02477
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#19968;&#23450;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#24448;&#24448;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#36807;&#31243;&#65292;&#36825;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#35299;&#37322;&#21644;&#29702;&#35299;&#26377;&#30528;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#34920;&#26126;&#23427;&#20204;&#20855;&#22791;&#19968;&#23450;&#31243;&#24230;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#26159;&#36890;&#29992;&#19988;&#21487;&#36716;&#31227;&#30340;&#65292;&#36824;&#26159;&#19987;&#38376;&#38024;&#23545;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#65311;&#20026;&#20102;&#20998;&#24320;&#36825;&#20123;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22522;&#20110;&#8220;&#21453;&#20107;&#23454;&#8221;&#20219;&#21153;&#21464;&#31181;&#65292;&#36825;&#20123;&#21464;&#31181;&#19982;&#25903;&#25745;&#26631;&#20934;&#20219;&#21153;&#30340;&#40664;&#35748;&#20551;&#35774;&#26377;&#25152;&#20559;&#31163;&#12290;&#22312;&#19968;&#22871;&#21253;&#21547;11&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21453;&#20107;&#23454;&#21464;&#31181;&#30340;&#38750;&#24179;&#20961;&#24615;&#33021;&#65292;&#20294;&#19982;&#40664;&#35748;&#26465;&#20214;&#30456;&#27604;&#65292;&#24615;&#33021;&#26174;&#33879;&#32780;&#25345;&#32493;&#22320;&#19979;&#38477;&#12290;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20855;&#22791;&#25277;&#35937;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#26356;&#21152;&#35880;&#24910;&#30340;&#35299;&#37322;&#65292;&#20197;&#21306;&#20998;&#36825;&#20123;&#34892;&#20026;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;GPT-4&#20248;&#20110;ChatGPT&#65292;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;</title><link>http://arxiv.org/abs/2306.17156</link><description>&lt;p&gt;
&#32534;&#31243;&#25945;&#32946;&#30340;&#29983;&#25104;AI&#65306;&#27604;&#36739;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;ChatGPT&#12289;GPT-4&#21644;&#20154;&#31867;&#23548;&#24072;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;GPT-4&#20248;&#20110;ChatGPT&#65292;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#39640;&#35745;&#31639;&#26426;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20026;&#21021;&#32423;&#32534;&#31243;&#25552;&#20379;&#19979;&#19968;&#20195;&#25945;&#32946;&#25216;&#26415;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19982;&#32534;&#31243;&#25945;&#32946;&#30456;&#20851;&#30340;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#30001;&#20110;&#22810;&#31181;&#21407;&#22240;&#32780;&#21463;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#32771;&#34385;&#30340;&#26159;&#24050;&#32463;&#36807;&#26102;&#30340;&#27169;&#22411;&#25110;&#20165;&#20165;&#29305;&#23450;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#30740;&#31350;&#26469;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;ChatGPT&#65288;&#22522;&#20110;GPT-3.5&#65289;&#21644;GPT-4&#65292;&#24182;&#23558;&#20854;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#19982;&#20154;&#31867;&#23548;&#24072;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;&#21021;&#32423;Python&#32534;&#31243;&#38382;&#39064;&#21644;&#26469;&#33258;&#22312;&#32447;&#24179;&#21488;&#30340;&#30495;&#23454;&#38169;&#35823;&#31243;&#24207;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#19987;&#23478;&#35780;&#27880;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#26126;&#26174;&#20248;&#20110;ChatGPT&#65288;&#22522;&#20110;GPT-3.5&#65289;&#65292;&#24182;&#19988;&#25509;&#36817;&#20110;&#20154;&#31867;&#23548;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to hu
&lt;/p&gt;</description></item><item><title>ChiPFormer&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#24067;&#23616;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#33455;&#29255;&#24067;&#23616;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#20943;&#23569;&#24067;&#23616;&#26102;&#38388;&#30340;&#21516;&#26102;&#22686;&#24378;&#20102;&#23545;&#26410;&#30693;&#33455;&#29255;&#30005;&#36335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.14744</link><description>&lt;p&gt;
ChiPFormer: &#36890;&#36807;&#31163;&#32447;&#20915;&#31574;&#21464;&#25442;&#22120;&#23454;&#29616;&#21487;&#36716;&#31227;&#33455;&#29255;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
ChiPFormer: Transferable Chip Placement via Offline Decision Transformer. (arXiv:2306.14744v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14744
&lt;/p&gt;
&lt;p&gt;
ChiPFormer&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#24067;&#23616;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#33455;&#29255;&#24067;&#23616;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#20943;&#23569;&#24067;&#23616;&#26102;&#38388;&#30340;&#21516;&#26102;&#22686;&#24378;&#20102;&#23545;&#26410;&#30693;&#33455;&#29255;&#30005;&#36335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#33455;&#29255;&#35774;&#35745;&#20013;&#65292;&#24067;&#23616;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#26088;&#22312;&#30830;&#23450;&#33455;&#29255;&#30011;&#24067;&#19978;&#30005;&#36335;&#27169;&#22359;&#30340;&#20301;&#32622;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#33455;&#29255;&#24067;&#23616;&#20013;&#30340;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#38388;&#38271;&#19988;&#22312;&#26410;&#30693;&#30340;&#33455;&#29255;&#30005;&#36335;&#20013;&#20855;&#26377;&#36739;&#20302;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#33455;&#29255;&#24067;&#23616;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;ChiPFormer&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22266;&#23450;&#30340;&#31163;&#32447;&#25968;&#25454;&#23398;&#20064;&#21040;&#21487;&#36716;&#31227;&#30340;&#24067;&#23616;&#31574;&#30053;&#12290;ChiPFormer&#20855;&#26377;&#19968;&#20123;&#20808;&#21069;&#30340;&#30740;&#31350;&#25152;&#27809;&#26377;&#30340;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;ChiPFormer&#33021;&#22815;&#21033;&#29992;&#31163;&#32447;&#24067;&#23616;&#35774;&#35745;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#31574;&#30053;&#12290;&#20854;&#27425;&#65292;ChiPFormer&#33021;&#22815;&#20419;&#36827;&#23545;&#26410;&#30693;&#33455;&#29255;&#30005;&#36335;&#30340;&#26377;&#25928;&#24494;&#35843;&#65292;&#23558;&#24067;&#23616;&#36816;&#34892;&#26102;&#38388;&#20174;&#20960;&#23567;&#26102;&#32553;&#30701;&#21040;&#20960;&#20998;&#38047;&#12290;&#31532;&#19977;&#65292;&#23545;32&#20010;&#33455;&#29255;&#30005;&#36335;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;ChiPFormer&#22312;&#20943;&#23569;&#24067;&#23616;&#26102;&#38388;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#24067;&#23616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Placement is a critical step in modern chip design, aiming to determine the positions of circuit modules on the chip canvas. Recent works have shown that reinforcement learning (RL) can improve human performance in chip placement. However, such an RL-based approach suffers from long training time and low transfer ability in unseen chip circuits. To resolve these challenges, we cast the chip placement as an offline RL formulation and present ChiPFormer that enables learning a transferable placement policy from fixed offline data. ChiPFormer has several advantages that prior arts do not have. First, ChiPFormer can exploit offline placement designs to learn transferable policies more efficiently in a multi-task setting. Second, ChiPFormer can promote effective finetuning for unseen chip circuits, reducing the placement runtime from hours to minutes. Third, extensive experiments on 32 chip circuits demonstrate that ChiPFormer achieves significantly better placement quality while reducing t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21457;&#29616;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24494;&#35843;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#65292;&#21482;&#38656;&#23569;&#37327;&#33258;&#30001;&#21442;&#25968;&#21363;&#21487;&#26377;&#25928;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#19988;&#26576;&#20123;&#32500;&#24230;&#23545;&#20110;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.17446</link><description>&lt;p&gt;
&#24494;&#23567;&#23376;&#31354;&#38388;&#20013;&#21457;&#29983;&#24494;&#35843;: &#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models. (arXiv:2305.17446v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21457;&#29616;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24494;&#35843;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#65292;&#21482;&#38656;&#23569;&#37327;&#33258;&#30001;&#21442;&#25968;&#21363;&#21487;&#26377;&#25928;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#19988;&#26576;&#20123;&#32500;&#24230;&#23545;&#20110;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36807;&#24230;&#21442;&#25968;&#21270;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20887;&#20313;&#65292;&#34920;&#26126;PLMs&#30340;&#33258;&#30001;&#24230;&#36739;&#23567;&#12290;&#26412;&#25991;&#20174;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24494;&#35843;PLMs&#30340;&#38382;&#39064;&#65306;&#21457;&#29616;&#20869;&#22312;&#30340;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#21033;&#29992;&#32473;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#36807;&#31243;&#30340;&#21160;&#24577;&#65292;&#23398;&#20064;&#20102;&#21442;&#25968;&#20248;&#21270;&#36712;&#36857;&#20197;&#25581;&#31034;&#20854;&#20869;&#22312;&#30340;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#12290;&#19968;&#20010;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#22312;&#23376;&#31354;&#38388;&#20013;&#65292;PLMs&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#30340;&#33258;&#30001;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#30340;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23376;&#31354;&#38388;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#20986;&#29616;&#20102;&#19968;&#20123;&#24322;&#24120;&#32500;&#24230;&#12290;&#31105;&#29992;&#36825;&#20123;&#32500;&#24230;&#20250;&#20005;&#37325;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#32500;&#24230;&#23545;&#20110;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21040;&#19979;&#28216;&#20219;&#21153;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26426;&#21046;&#30340;&#31639;&#27861;&#65292;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2305.17372</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#20351;&#29992;&#22870;&#21169;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning With Reward Machines in Stochastic Games. (arXiv:2305.17372v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26426;&#21046;&#30340;&#31639;&#27861;&#65292;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;&#22870;&#21169;&#26426;&#21046;&#26469;&#25972;&#21512;&#39640;&#23618;&#27425;&#30340;&#22797;&#26434;&#20219;&#21153;&#30693;&#35782;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;QRM-SG&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#12290;&#22312;QRM-SG&#20013;&#65292;&#25105;&#20204;&#22312;&#22686;&#24191;&#29366;&#24577;&#31354;&#38388;&#20013;&#23450;&#20041;&#20102;&#32435;&#20160;&#22343;&#34913;&#19979;&#30340;Q&#20989;&#25968;&#12290;&#22686;&#24191;&#29366;&#24577;&#31354;&#38388;&#25972;&#21512;&#20102;&#38543;&#26426;&#21338;&#24328;&#30340;&#29366;&#24577;&#21644;&#22870;&#21169;&#26426;&#21046;&#30340;&#29366;&#24577;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#20102;&#31995;&#32479;&#20013;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;Q&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;QRM-SG&#20013;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;&#25105;&#20204;&#20351;&#29992;Lemke-Howson&#26041;&#27861;&#26469;&#24471;&#20986;&#32473;&#23450;&#24403;&#21069;Q&#20989;&#25968;&#26102;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate multi-agent reinforcement learning for stochastic games with complex tasks, where the reward functions are non-Markovian. We utilize reward machines to incorporate high-level knowledge of complex tasks. We develop an algorithm called Q-learning with reward machines for stochastic games (QRM-SG), to learn the best-response strategy at Nash equilibrium for each agent. In QRM-SG, we define the Q-function at a Nash equilibrium in augmented state space. The augmented state space integrates the state of the stochastic game and the state of reward machines. Each agent learns the Q-functions of all agents in the system. We prove that Q-functions learned in QRM-SG converge to the Q-functions at a Nash equilibrium if the stage game at each time step during learning has a global optimum point or a saddle point, and the agents update Q-functions based on the best-response strategy at this point. We use the Lemke-Howson method to derive the best-response strategy given current Q-func
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14387</link><description>&lt;p&gt;
AlpacaFarm: &#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#33391;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#24320;&#21457;&#36825;&#20123;LLMs&#38656;&#35201;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#30340;&#22797;&#26434;&#19988;&#23578;&#19981;&#26126;&#30830;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23558;&#27492;&#25351;&#20196;&#36319;&#38543;&#36807;&#31243;&#22797;&#21046;&#21644;&#29702;&#35299;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#65306; &#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;AlpacaFarm&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#29992;&#20110;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#20854;&#25104;&#26412;&#27604;&#20247;&#21253;&#24037;&#20316;&#32773;&#20415;&#23452;45&#20493;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#39564;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#20960;&#31181;&#20174;&#37197;&#23545;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PPO&#65292;best-of-n&#65292;expert iteration&#31561;&#65289;&#25552;&#20379;&#20102;&#21442;&#32771;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12073</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;GELU&#28608;&#27963;&#20989;&#25968;&#65306;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance. (arXiv:2305.12073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#28608;&#27963;&#20989;&#25968;&#26159;&#24433;&#21709;&#20854;&#23398;&#20064;&#33021;&#21147;&#12289;&#31283;&#23450;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36817;&#24180;&#26469;&#65292;&#39640;&#26031;&#35823;&#24046;&#32447;&#24615;&#21333;&#20803;&#65288;GELU&#65289;&#28608;&#27963;&#20989;&#25968;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20027;&#27969;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#12290;&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#35814;&#32454;&#25506;&#35752;&#20102;&#20854;&#21487;&#24494;&#24615;&#12289;&#26377;&#30028;&#24615;&#12289;&#24179;&#31283;&#24615;&#21644;&#20809;&#28369;&#24615;&#31561;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;GELU&#20989;&#25968;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#21033;&#29992;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;STL-10&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27531;&#24046;&#21367;&#31215;&#32593;&#32476;&#20316;&#20026;&#23454;&#35777;&#27979;&#35797;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;GELU&#30456;&#23545;&#20110;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#30830;&#31435;&#20102;&#23427;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting the most suitable activation function is a critical factor in the effectiveness of deep learning models, as it influences their learning capacity, stability, and computational efficiency. In recent years, the Gaussian Error Linear Unit (GELU) activation function has emerged as a dominant method, surpassing traditional functions such as the Rectified Linear Unit (ReLU) in various applications. This study presents a rigorous mathematical investigation of the GELU activation function, exploring its differentiability, boundedness, stationarity, and smoothness properties in detail. Additionally, we conduct an extensive experimental comparison of the GELU function against a broad range of alternative activation functions, utilizing a residual convolutional network trained on the CIFAR-10, CIFAR-100, and STL-10 datasets as the empirical testbed. Our results demonstrate the superior performance of GELU compared to other activation functions, establishing its suitability for a wide ra
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08698</link><description>&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08698
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#65288;MKGC&#65289;&#28041;&#21450;&#20351;&#29992;&#22810;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#21019;&#24314;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MKGC&#27169;&#22411;&#22312;&#22788;&#29702;&#21160;&#24577;&#29616;&#23454;&#22330;&#26223;&#20013;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#36830;&#32493;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#35774;&#32622;&#20027;&#35201;&#20851;&#27880;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#22810;&#27169;&#24577;&#28304;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#25506;&#32034;&#36830;&#32493;MKGC&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#30830;&#20445;&#20445;&#30041;&#20174;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#36807;&#21435;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#24320;&#21457;&#32456;&#36523;MKGC&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#36825;&#20010;&#22797;&#26434;&#30340;&#20027;&#39064;&#12290;&#22522;&#20110;&#32463;&#39564;&#21457;&#29616;&#65292;&#24403;&#22810;&#23186;&#20307;&#25968;&#25454;&#35757;&#32451;&#26102;&#65292;&#19968;&#20123;&#20856;&#22411;&#30340;MKGC&#27169;&#22411;&#21487;&#33021;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#24847;&#22806;&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#37027;&#20123;&#20165;&#21033;&#29992;&#25991;&#26412;&#36164;&#28304;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#25105;&#20204;&#20197;&#23454;&#39564;&#35777;&#25454;&#20026;&#22522;&#30784;&#65292;&#24635;&#32467;&#20986;&#20197;&#19979;&#35770;&#28857;&#65306;&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#25968;&#25454;&#28304;&#21464;&#21270;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Knowledge Graph Construction (MKGC) involves creating structured representations of entities and relations using multiple modalities, such as text and images. However, existing MKGC models face challenges in handling the addition of new entities and relations in dynamic real-world scenarios. The current continual setting for knowledge graph construction mainly focuses on entity and relation extraction from text data, overlooking other multimodal sources. Therefore, there arises the need to explore the challenge of continual MKGC to address the phenomenon of catastrophic forgetting and ensure the retention of past knowledge extracted from different forms of data. This research focuses on investigating this complex topic by developing lifelong MKGC benchmark datasets. Based on the empirical findings that several typical MKGC models, when trained on multimedia data, might unexpectedly underperform compared to those solely utilizing textual resources in a continual setting, we p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05832</link><description>&lt;p&gt;
&#22240;&#26524;&#20449;&#24687;&#20998;&#31163;&#65306;&#20026;&#25239;&#20998;&#24067;&#36716;&#31227;&#35774;&#35745;&#20195;&#29702;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#19982;&#26368;&#32456;&#20351;&#29992;&#24773;&#20917;&#19981;&#21516;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#39044;&#27979;&#20998;&#24067;&#36716;&#31227;&#65292;&#26377;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#30340;&#22240;&#26524;&#21644;&#21453;&#22240;&#26524;&#21464;&#37327;&#37117;&#26159;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#12290;&#21033;&#29992;&#20449;&#24687;&#35770;&#65292;&#25105;&#20204;&#20026;&#19979;&#28216;&#35266;&#27979;&#21464;&#37327;&#24320;&#21457;&#20102;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#36825;&#20123;&#21464;&#37327;&#20805;&#24403;&#20195;&#29702;&#12290;&#25105;&#20204;&#36873;&#25321;&#26377;&#21161;&#20110;&#24314;&#31435;&#31283;&#23450;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#35757;&#32451;&#20219;&#21153;&#20174;&#20195;&#29702;&#20013;&#25552;&#21462;&#22686;&#24378;&#31283;&#23450;&#24615;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical prediction models are often trained on data that is drawn from different probability distributions than their eventual use cases. One approach to proactively prepare for these shifts harnesses the intuition that causal mechanisms should remain invariant between environments. Here we focus on a challenging setting in which the causal and anticausal variables of the target are unobserved. Leaning on information theory, we develop feature selection and engineering techniques for the observed downstream variables that act as proxies. We identify proxies that help to build stable models and moreover utilize auxiliary training tasks to extract stability-enhancing information from proxies. We demonstrate the effectiveness of our techniques on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.05230</link><description>&lt;p&gt;
FedNoRo: &#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity. (arXiv:2305.05230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;(FNLL)&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#22810;&#28304;&#20998;&#25955;&#23398;&#20064;&#24037;&#20855;&#12290;&#29616;&#26377;&#30740;&#31350;&#22522;&#20110;&#20840;&#23616;&#25968;&#25454;&#31867;&#21035;&#24179;&#34913;&#30340;&#20551;&#35774;&#65292;&#21487;&#33021;&#26080;&#27861;&#24314;&#27169;&#22797;&#26434;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#30495;&#23454;&#30340;&#32852;&#37030;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#20854;&#20013;&#20840;&#23616;&#25968;&#25454;&#26159;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#65292;&#24182;&#19988;&#26631;&#31614;&#22122;&#22768;&#26159;&#24322;&#36136;&#30340;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312; FedNoRo &#30340;&#31532;&#19968;&#38454;&#27573;&#65292;&#37319;&#29992;&#27599;&#31867;&#25439;&#22833;&#25351;&#26631;&#20043;&#21518;&#36319;&#38543;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22024;&#26434;&#23458;&#25143;&#31471;&#35782;&#21035;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21516;&#26102;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#36317;&#31163;&#24863;&#30693;&#32858;&#21512;&#20989;&#25968;&#36827;&#34892;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#27169;&#22411;&#26356;&#26032;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedNoRo &#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340; FNLL &#26041;&#27861;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated noisy label learning (FNLL) is emerging as a promising tool for privacy-preserving multi-source decentralized learning. Existing research, relying on the assumption of class-balanced global data, might be incapable to model complicated label noise, especially in medical scenarios. In this paper, we first formulate a new and more realistic federated label noise problem where global data is class-imbalanced and label noise is heterogeneous, and then propose a two-stage framework named FedNoRo for noise-robust federated learning. Specifically, in the first stage of FedNoRo, per-class loss indicators followed by Gaussian Mixture Model are deployed for noisy client identification. In the second stage, knowledge distillation and a distance-aware aggregation function are jointly adopted for noise-robust federated model updating. Experimental results on the widely-used ICH and ISIC2019 datasets demonstrate the superiority of FedNoRo against the state-of-the-art FNLL methods for addre
&lt;/p&gt;</description></item><item><title>&#25688;&#35201;&#29983;&#25104;&#39046;&#22495;&#30446;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#28857;&#22312;&#20110;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#35780;&#20272;&#25688;&#35201;&#29983;&#25104;&#31995;&#32479;&#30340;&#25361;&#25112;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.04853</link><description>&lt;p&gt;
&#25688;&#35201;&#29983;&#25104;&#30340;&#24403;&#21069;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Current State of Summarization. (arXiv:2305.04853v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04853
&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#29983;&#25104;&#39046;&#22495;&#30446;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#28857;&#22312;&#20110;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#35780;&#20272;&#25688;&#35201;&#29983;&#25104;&#31995;&#32479;&#30340;&#25361;&#25112;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#20449;&#24687;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#25688;&#35201;&#29983;&#25104;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#31616;&#26126;&#25212;&#35201;&#22320;&#20171;&#32461;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#29983;&#25104;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#20316;&#20026;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21521;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36716;&#21464;&#30340;&#29616;&#26377;&#33539;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#35780;&#20272;&#25688;&#35201;&#29983;&#25104;&#31995;&#32479;&#30340;&#25361;&#25112;&#20197;&#21450;&#22522;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#30446;&#21069;&#23558;&#25688;&#35201;&#29983;&#25104;&#31995;&#32479;&#25972;&#21512;&#21040;&#21830;&#19994;&#24212;&#29992;&#20013;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosive growth of textual information, summarization systems have become increasingly important. This work aims to concisely indicate the current state of the art in abstractive text summarization. As part of this, we outline the current paradigm shifts towards pre-trained encoder-decoder models and large autoregressive language models. Additionally, we delve further into the challenges of evaluating summarization systems and the potential of instruction-tuned models for zero-shot summarization. Finally, we provide a brief overview of how summarization systems are currently being integrated into commercial applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11082</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19982;&#20154;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#26159;&#23545;&#40784;&#20854;&#34892;&#20026;&#65292;&#20351;&#20854;&#23545;&#20854;&#20154;&#31867;&#29992;&#25143;&#26377;&#29992;&#19988;&#26080;&#23475;&#12290;&#36825;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#65292;&#20197;&#22686;&#24378;&#25152;&#38656;&#30340;&#34892;&#20026;&#24182;&#25233;&#21046;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;(BEB)&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20010;&#20869;&#22312;&#29305;&#24449;&#21644;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#34987;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#26377;&#38480;&#27010;&#29575;&#30340;&#34892;&#20026;&#65292;&#37117;&#23384;&#22312;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#36755;&#20986;&#27492;&#34892;&#20026;&#30340;&#25552;&#31034;&#65292;&#20854;&#27010;&#29575;&#38543;&#25552;&#31034;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#20943;&#24369;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#20294;&#26410;&#23558;&#20854;&#23436;&#20840;&#28040;&#38500;&#30340;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#25269;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#31034;&#20102;&#39046;&#20808;&#30340;
&lt;/p&gt;
&lt;p&gt;
An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#21160;&#24577;&#20840;&#23616;&#28388;&#27874;&#22120;&#30340;&#21452;&#27969;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35828;&#35805;&#20154;&#39564;&#35777;&#39046;&#22495;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11020</link><description>&lt;p&gt;
&#20855;&#26377;&#21160;&#24577;&#20840;&#23616;&#28388;&#27874;&#22120;&#30340;&#21452;&#27969;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35828;&#35805;&#20154;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Dual-stream Time-Delay Neural Network with Dynamic Global Filter for Speaker Verification. (arXiv:2303.11020v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11020
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#21160;&#24577;&#20840;&#23616;&#28388;&#27874;&#22120;&#30340;&#21452;&#27969;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35828;&#35805;&#20154;&#39564;&#35777;&#39046;&#22495;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;(TDNN)&#26159;&#25991;&#26412;&#26080;&#20851;&#35828;&#35805;&#20154;&#39564;&#35777;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20256;&#32479;&#30340;TDNN&#26469;&#35828;&#65292;&#25429;&#25417;&#34987;&#35777;&#26126;&#23545;&#20110;&#40065;&#26834;&#35828;&#35805;&#20154;&#34920;&#31034;&#21644;&#38271;&#26102;&#38388;&#35828;&#35805;&#20154;&#39564;&#35777;&#33267;&#20851;&#37325;&#35201;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;(&#20363;&#22914;&#33258;&#25105;&#20851;&#27880;)&#23545;&#20110;&#36755;&#20837;&#20196;&#29260;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#24403;&#24212;&#29992;&#20110;TDNN&#20013;&#20855;&#26377;&#22823;&#23610;&#23544;&#29305;&#24449;&#26144;&#23556;&#26102;&#65292;&#36825;&#20351;&#23427;&#20204;&#22312;&#35745;&#31639;&#19978;&#26080;&#27861;&#25215;&#21463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TDNN&#30340;&#20840;&#23616;&#28388;&#27874;&#22120;(GFTDNN)&#65292;&#23427;&#24212;&#29992;&#23545;&#25968;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;FFT/IFFT&#21644;&#19968;&#32452;&#21487;&#24494;&#39057;&#22495;&#28388;&#27874;&#22120;&#26469;&#39640;&#25928;&#22320;&#24314;&#27169;&#35821;&#38899;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#20026;&#22686;&#24378;&#20840;&#23616;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#24182;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#65292;&#29305;&#21035;&#35774;&#35745;&#20102;&#21160;&#24577;&#28388;&#27874;&#31574;&#30053;&#21644;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#27969;TDNN(DS-TDNN)&#65292;&#23558;&#22522;&#26412;&#36890;&#36947;&#20998;&#25104;&#20004;&#20010;&#24182;&#34892;&#36890;&#36947;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#39057;&#22495;&#28388;&#27874;&#22120;&#35757;&#32451;&#27599;&#20010;&#36890;&#36947;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22823;&#35268;&#27169;&#35828;&#35805;&#20154;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26631;&#20934;TDNN&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The time-delay neural network (TDNN) is one of the state-of-the-art models for text-independent speaker verification. However, it is difficult for conventional TDNN to capture global context that has been proven critical for robust speaker representations and long-duration speaker verification in many recent works. Besides, the common solutions, e.g., self-attention, have quadratic complexity for input tokens, which makes them computationally unaffordable when applied to the feature maps with large sizes in TDNN. To address these issues, we propose the Global Filter for TDNN, which applies log-linear complexity FFT/IFFT and a set of differentiable frequency-domain filters to efficiently model the long-term dependencies in speech. Besides, a dynamic filtering strategy, and a sparse regularization method are specially designed to enhance the performance of the global filter and prevent it from overfitting. Furthermore, we construct a dual-stream TDNN (DS-TDNN), which splits the basic cha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09901</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;3&#19978;&#30340;mCPT&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26694;&#26550;&#26816;&#27979;&#30340;&#22810;&#35821;&#35328;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38646;&#26679;&#26412;&#30340;&#35199;&#29677;&#29273;&#35821;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#33719;&#32988;&#31995;&#32479;&#65292;&#24182;&#22312;&#21478;&#22806;&#20843;&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#22312;&#20110;&#22312;&#21482;&#26377;&#23569;&#37327;&#25110;&#38646;&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#19968;&#32452;14&#20010;&#26694;&#26550;&#65292;&#21363;&#22810;&#35821;&#35328;&#22810;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#38500;&#20102;&#25551;&#36848;&#31995;&#32479;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23884;&#20837;&#31354;&#38388;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#22914;&#20309;&#25903;&#25345;&#26694;&#26550;&#26816;&#27979;&#20197;&#25512;&#36827;&#35745;&#31639;&#26694;&#26550;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25968;&#23383;&#30165;&#36857;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#25233;&#37057;&#30151;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#30693;&#35782;&#24863;&#30693;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#25581;&#31034;&#20102;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05389</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20351;&#29992;&#25968;&#23383;&#30165;&#36857;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;&#65306;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Depression Detection Using Digital Traces on Social Media: A Knowledge-aware Deep Learning Approach. (arXiv:2303.05389v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25968;&#23383;&#30165;&#36857;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#25233;&#37057;&#30151;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#30693;&#35782;&#24863;&#30693;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#25581;&#31034;&#20102;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#20840;&#29699;&#24120;&#35265;&#30340;&#30142;&#30149;&#12290;&#23427;&#24456;&#38590;&#35786;&#26029;&#65292;&#24182;&#19988;&#25345;&#32493;&#34987;&#20302;&#20272;&#12290;&#30001;&#20110;&#25233;&#37057;&#30151;&#24739;&#32773;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#19981;&#26029;&#20998;&#20139;&#20182;&#20204;&#30340;&#30151;&#29366;&#12289;&#37325;&#22823;&#29983;&#27963;&#20107;&#20214;&#21644;&#27835;&#30103;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#19978;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#23383;&#30165;&#36857;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#25239;&#20987;&#25233;&#37057;&#30151;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20419;&#36827;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#25239;&#25233;&#37057;&#30151;&#24182;&#20943;&#36731;&#20854;&#31038;&#20250;&#21644;&#32463;&#27982;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#26377;&#25928;&#30340;&#25163;&#27573;&#23558;&#24050;&#24314;&#31435;&#30340;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#25233;&#37057;&#30151;&#26816;&#27979;&#20013;&#65292;&#25110;&#32773;&#38754;&#20020;&#29305;&#24449;&#25552;&#21462;&#22256;&#38590;&#32780;&#24433;&#21709;&#24615;&#33021;&#12290;&#22312;&#35774;&#35745;&#31185;&#23398;&#30740;&#31350;&#33539;&#24335;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#30693;&#35782;&#24863;&#30693;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979; (DKDD) &#26694;&#26550;&#65292;&#20197;&#20934;&#30830;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25233;&#37057;&#39118;&#38505;&#65292;&#24182;&#35299;&#37322;&#23545;&#36825;&#31181;&#26816;&#27979;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#22240;&#32032;&#12290;&#36890;&#36807;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a common disease worldwide. It is difficult to diagnose and continues to be underdiagnosed. Because depressed patients constantly share their symptoms, major life events, and treatments on social media, researchers are turning to user-generated digital traces on social media for depression detection. Such methods have distinct advantages in combating depression because they can facilitate innovative approaches to fight depression and alleviate its social and economic burden. However, most existing studies lack effective means to incorporate established medical domain knowledge in depression detection or suffer from feature extraction difficulties that impede greater performance. Following the design science research paradigm, we propose a Deep Knowledge-aware Depression Detection (DKDD) framework to accurately detect social media users at risk of depression and explain the critical factors that contribute to such detection. Extensive empirical studies with real-world data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#21457;&#29616;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#23398;&#29983;&#32593;&#32476;&#23545;&#25945;&#24072;&#32593;&#32476;&#30340;&#27010;&#29575;&#20559;&#31163;&#26159;&#31995;&#32479;&#24615;&#22840;&#22823;&#30340;&#65292;&#21516;&#26102;&#20063;&#24471;&#21040;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.12923</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#23398;&#29983;-&#25945;&#24072;&#20559;&#24046;&#65306;&#36829;&#21453;&#35268;&#21017;&#26159;&#21542;&#26377;&#30410;&#65311;
&lt;/p&gt;
&lt;p&gt;
On student-teacher deviations in distillation: does it pay to disobey?. (arXiv:2301.12923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12923
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#21457;&#29616;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#23398;&#29983;&#32593;&#32476;&#23545;&#25945;&#24072;&#32593;&#32476;&#30340;&#27010;&#29575;&#20559;&#31163;&#26159;&#31995;&#32479;&#24615;&#22840;&#22823;&#30340;&#65292;&#21516;&#26102;&#20063;&#24471;&#21040;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#20223;&#32463;&#36807;&#35757;&#32451;&#30340;&#8220;&#25945;&#24072;&#8221;&#32593;&#32476;&#30340;&#36719;&#27010;&#29575;&#26469;&#25552;&#39640;&#8220;&#23398;&#29983;&#8221;&#32593;&#32476;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#35757;&#32451;&#25104;&#36866;&#24212;&#25945;&#24072;&#30340;&#27010;&#29575;&#65292;&#23398;&#29983;&#19981;&#20165;&#26126;&#26174;&#20559;&#31163;&#36825;&#20123;&#27010;&#29575;&#65292;&#32780;&#19988;&#34920;&#29616;&#27604;&#25945;&#24072;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#30830;&#23450;&#23398;&#29983;-&#25945;&#24072;&#20559;&#24046;&#30340;&#30830;&#20999;&#24615;&#36136;&#65292;&#24182;&#35770;&#35777;&#23427;&#20204;&#19982;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#22914;&#20309;&#20849;&#23384;&#26469;&#35299;&#20915;&#36825;&#19968;&#30475;&#20284;&#30683;&#30462;&#30340;&#35266;&#23519;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#36825;&#20123;&#20559;&#24046;&#23545;&#24212;&#20110;&#23398;&#29983;&#31995;&#32479;&#24615;&#22320;&#22840;&#22823;&#25945;&#24072;&#30340;&#33258;&#20449;&#27700;&#24179;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#24314;&#31435;&#20102;KD&#22312;&#25910;&#25947;&#26356;&#24555;&#30340;&#36807;&#31243;&#20013;&#22840;&#22823;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#30340;&#35777;&#25454;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) has been widely-used to improve the test accuracy of a ``student'' network by training the student to mimic soft probabilities of a trained "teacher" network. Yet, it has been shown in recent work that, despite being trained to fit the teacher's probabilities, the student not only significantly deviates from these probabilities, but also performs even better than the teacher. Our work aims to reconcile this seemingly paradoxical observation by characterizing the precise nature of the student-teacher deviations, and by arguing how they can co-occur with better generalization. First, through experiments on image and language data, we identify that these deviations correspond to the student systematically exaggerating the confidence levels of the teacher. Next, we theoretically and empirically establish in some simple settings that KD also exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections of the data. Finally, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#21017;&#21270;&#22343;&#34913;&#65292;&#21487;&#20197;&#23558;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#25277;&#35937;&#20986;&#26469;&#24182;&#20316;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2301.09159</link><description>&lt;p&gt;
&#20174;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25277;&#35937;&#20986;&#19981;&#23436;&#32654;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Abstracting Imperfect Information Away from Two-Player Zero-Sum Games. (arXiv:2301.09159v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09159
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#22343;&#34913;&#65292;&#21487;&#20197;&#23558;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#25277;&#35937;&#20986;&#26469;&#24182;&#20316;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Nayyar&#31561;&#20154;&#22312;&#20854;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#20013;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#28216;&#25103;&#36807;&#31243;&#20013;&#35753;&#29609;&#23478;&#20844;&#24320;&#23459;&#24067;&#20854;&#31574;&#30053;&#65292;&#19981;&#23436;&#32654;&#20449;&#24687;&#21487;&#20197;&#34987;&#20174;&#20849;&#21516;&#25928;&#30410;&#28216;&#25103;&#20013;&#25277;&#35937;&#20986;&#26469;&#12290;&#36825;&#20010;&#35265;&#35299;&#26159;&#25903;&#25745;&#20849;&#21516;&#25928;&#30410;&#28216;&#25103;&#21512;&#29702;&#30340;&#27714;&#35299;&#22120;&#21644;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23558;&#21516;&#26679;&#30340;&#35265;&#35299;&#31616;&#21333;&#24212;&#29992;&#20110;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#20855;&#26377;&#20844;&#24320;&#31574;&#30053;&#23459;&#24067;&#30340;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#21487;&#33021;&#19982;&#21407;&#22987;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#19981;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#21512;&#29702;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#38656;&#35201;&#22797;&#26434;&#30340;&#39069;&#22806;&#26426;&#21046;&#65292;&#20854;&#20855;&#26377;&#19981;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23637;&#31034;&#26576;&#20123;&#27491;&#21017;&#21270;&#22343;&#34913;&#19981;&#20855;&#26377;&#19978;&#36848;&#30340;&#19981;&#23545;&#24212;&#38382;&#39064;&#65292;&#22240;&#27492;&#65292;&#35745;&#31639;&#23427;&#20204;&#21487;&#20197;&#34987;&#35270;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#12290;&#22240;&#20026;&#36825;&#20123;&#27491;&#21017;&#21270;&#22343;&#34913;&#21487;&#20197;&#34987;&#26080;&#38480;&#25509;&#36817;&#32435;&#20160;&#22343;&#34913;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In their seminal work, Nayyar et al. (2013) showed that imperfect information can be abstracted away from common-payoff games by having players publicly announce their policies as they play. This insight underpins sound solvers and decision-time planning algorithms for common-payoff games. Unfortunately, a naive application of the same insight to two-player zero-sum games fails because Nash equilibria of the game with public policy announcements may not correspond to Nash equilibria of the original game. As a consequence, existing sound decision-time planning algorithms require complicated additional mechanisms that have unappealing properties. The main contribution of this work is showing that certain regularized equilibria do not possess the aforementioned non-correspondence problem -- thus, computing them can be treated as perfect-information problems. Because these regularized equilibria can be made arbitrarily close to Nash equilibria, our result opens the door to a new perspectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#28789;&#27963;&#30721;&#29575;&#30340;Reed-Muller&#23376;&#30721;&#30340;&#39640;&#25928;&#35299;&#30721;&#38382;&#39064;&#65292;&#36890;&#36807;&#25193;&#23637;&#36882;&#24402;&#25237;&#24433;&#32858;&#21512;&#65288;RPA&#65289;&#35793;&#30721;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;subRPA&#21644;soft-subRPA&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32500;&#25345;&#36739;&#20302;&#22797;&#26434;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#35793;&#30721;&#24615;&#33021;&#24182;&#23454;&#29616;&#21487;&#24494;&#20998;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.06251</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#19979;&#30340;Reed-Muller&#23376;&#30721;&#39640;&#25928;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Aided Efficient Decoding of Reed-Muller Subcodes. (arXiv:2301.06251v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#28789;&#27963;&#30721;&#29575;&#30340;Reed-Muller&#23376;&#30721;&#30340;&#39640;&#25928;&#35299;&#30721;&#38382;&#39064;&#65292;&#36890;&#36807;&#25193;&#23637;&#36882;&#24402;&#25237;&#24433;&#32858;&#21512;&#65288;RPA&#65289;&#35793;&#30721;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;subRPA&#21644;soft-subRPA&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32500;&#25345;&#36739;&#20302;&#22797;&#26434;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#35793;&#30721;&#24615;&#33021;&#24182;&#23454;&#29616;&#21487;&#24494;&#20998;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Reed-Muller&#65288;RM&#65289;&#30721;&#22312;&#19968;&#33324;&#30340;&#20108;&#36827;&#21046;&#36755;&#20837;&#26080;&#35760;&#24518;&#23545;&#31216;&#20449;&#36947;&#19978;&#36798;&#21040;&#23481;&#37327;&#65292;&#24182;&#19988;&#25454;&#25512;&#27979;&#65292;&#22312;&#27604;&#20363;&#23450;&#24459;&#26041;&#38754;&#30340;&#24615;&#33021;&#19982;&#38543;&#26426;&#30721;&#30456;&#24403;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#32467;&#26524;&#26159;&#24314;&#31435;&#22312;&#23545;&#19968;&#33324;&#30721;&#21442;&#25968;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#35793;&#30721;&#22120;&#30340;&#24773;&#20917;&#19979;&#30340;&#12290;&#27492;&#22806;&#65292;RM&#30721;&#21482;&#33021;&#25509;&#21463;&#26377;&#38480;&#30340;&#30721;&#29575;&#38598;&#12290;&#23545;&#20110;&#26377;&#38480;&#38271;&#24230;&#30340;RM&#30721;&#65292;&#24050;&#32463;&#26377;&#35832;&#22914;&#36830;&#32493;&#21462;&#28040;&#21015;&#34920;&#65288;SCL&#65289;&#35793;&#30721;&#22120;&#21644;&#26368;&#36817;&#24341;&#20837;&#30340;&#36882;&#24402;&#25237;&#24433;&#32858;&#21512;&#65288;RPA&#65289;&#35793;&#30721;&#22120;&#31561;&#39640;&#25928;&#35793;&#30721;&#22120;&#21487;&#29992;&#12290;&#26412;&#25991;&#25105;&#20204;&#30740;&#31350;&#20855;&#26377;&#28789;&#27963;&#30721;&#29575;&#30340;RM&#30721;&#23376;&#30721;&#12290;&#39318;&#20808;&#25105;&#20204;&#23558;RPA&#35793;&#30721;&#31639;&#27861;&#25193;&#23637;&#21040;RM&#23376;&#30721;&#19978;&#12290;&#20026;&#20102;&#38477;&#20302;&#25105;&#20204;&#30340;&#35793;&#30721;&#31639;&#27861;&#65288;&#31216;&#20026;subRPA&#65289;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25237;&#24433;&#21098;&#26525;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22522;&#20110;&#36719;&#21028;&#26029;&#30340;&#29256;&#26412;&#65292;&#31216;&#20026;soft-subRPA&#65292;&#23427;&#19981;&#20165;&#25913;&#36827;&#20102;subRPA&#30340;&#24615;&#33021;&#65292;&#36824;&#20351;&#24471;&#35793;&#30721;&#31639;&#27861;&#21487;&#24494;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reed-Muller (RM) codes achieve the capacity of general binary-input memoryless symmetric channels and are conjectured to have a comparable performance to that of random codes in terms of scaling laws. However, such results are established assuming maximum-likelihood decoders for general code parameters. Also, RM codes only admit limited sets of rates. Efficient decoders such as successive cancellation list (SCL) decoder and recently-introduced recursive projection-aggregation (RPA) decoders are available for RM codes at finite lengths. In this paper, we focus on subcodes of RM codes with flexible rates. We first extend the RPA decoding algorithm to RM subcodes. To lower the complexity of our decoding algorithm, referred to as subRPA, we investigate different approaches to prune the projections. Next, we derive the soft-decision based version of our algorithm, called soft-subRPA, that not only improves upon the performance of subRPA but also enables a differentiable decoding algorithm. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#35843;&#25972;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35843;&#25972;&#25513;&#30721;&#24212;&#29992;&#20110;PPO&#21644;IMPALA&#20195;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.11110</link><description>&lt;p&gt;
&#20351;&#29992;&#35843;&#25972;&#25513;&#30721;&#30340;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lifelong Reinforcement Learning with Modulating Masks. (arXiv:2212.11110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#35843;&#25972;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35843;&#25972;&#25513;&#30721;&#24212;&#29992;&#20110;PPO&#21644;IMPALA&#20195;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#26088;&#22312;&#21019;&#24314;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20013;&#25345;&#32493;&#21644;&#36880;&#27493;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#31867;&#20284;&#29983;&#29289;&#23398;&#20064;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#26041;&#38754;&#30340;&#23581;&#35797;&#36935;&#21040;&#20102;&#38382;&#39064;&#65292;&#21253;&#25324;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#20219;&#21153;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#20197;&#21450;&#26080;&#27861;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23398;&#20064;&#28041;&#21450;&#36755;&#20837;&#20998;&#24067;&#21464;&#21270;&#30340;&#22810;&#20010;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20294;&#26159;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#24517;&#39035;&#22788;&#29702;&#29366;&#24577;&#21644;&#36716;&#25442;&#20998;&#24067;&#20197;&#21450;&#22870;&#21169;&#20989;&#25968;&#30340;&#21464;&#21270;&#12290;&#26368;&#36817;&#38024;&#23545;&#20998;&#31867;&#38382;&#39064;&#24320;&#21457;&#30340;&#20351;&#29992;&#22266;&#23450;&#39592;&#24178;&#32593;&#32476;&#30340;&#35843;&#25972;&#25513;&#30721;&#23545;&#20110;&#22788;&#29702;&#22914;&#27492;&#22823;&#33539;&#22260;&#30340;&#20219;&#21153;&#21464;&#21270;&#29305;&#21035;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35843;&#25972;&#25513;&#30721;&#24212;&#29992;&#20110;&#28145;&#23618;&#27425;&#30340;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#20307;&#21253;&#25324;PPO&#21644;IMPALA&#20195;&#29702;&#12290;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#19982;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20808;&#21069;&#20219;&#21153;&#30340;&#32447;&#24615;&#32452;&#21512;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning aims to create AI systems that continuously and incrementally learn during a lifetime, similar to biological learning. Attempts so far have met problems, including catastrophic forgetting, interference among tasks, and the inability to exploit previous knowledge. While considerable research has focused on learning multiple supervised classification tasks that involve changes in the input distribution, lifelong reinforcement learning (LRL) must deal with variations in the state and transition distributions, and in the reward functions. Modulating masks with a fixed backbone network, recently developed for classification, are particularly suitable to deal with such a large spectrum of task variations. In this paper, we adapted modulating masks to work with deep LRL, specifically PPO and IMPALA agents. The comparison with LRL baselines in both discrete and continuous RL tasks shows superior performance. We further investigated the use of a linear combination of previousl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#26469;&#20272;&#35745;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#65292;&#20854;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.17505</link><description>&lt;p&gt;
&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Space-fluid Adaptive Sampling by Self-Organisation. (arXiv:2210.17505v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#26469;&#20272;&#35745;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#65292;&#20854;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#35843;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#31649;&#29702;&#65288;&#20272;&#35745;&#12289;&#39044;&#27979;&#25110;&#25511;&#21046;&#65289;&#38543;&#31354;&#38388;&#21464;&#21270;&#30340;&#20449;&#21495;&#65292;&#20363;&#22914;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31454;&#20105;&#21644;&#29983;&#38271;/&#32553;&#23567;&#30340;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#26041;&#27861;&#65292;&#21327;&#21516;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#20272;&#35745;&#31354;&#38388;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#22330;&#30340;&#21327;&#35843;&#26694;&#26550;&#20013;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#33258;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20272;&#35745;&#22797;&#26434;&#20989;&#25968;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#21644;&#36319;&#36394;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#26102;&#31354;&#29616;&#35937;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recurrent task in coordinated systems is managing (estimating, predicting, or controlling) signals that vary in space, such as distributed sensed data or computation outcomes. Especially in large-scale settings, the problem can be addressed through decentralised and situated computing systems: nodes can locally sense, process, and act upon signals, and coordinate with neighbours to implement collective strategies. Accordingly, in this work we devise distributed coordination strategies for the estimation of a spatial phenomenon through collaborative adaptive sampling. Our design is based on the idea of dynamically partitioning space into regions that compete and grow/shrink to provide accurate aggregate sampling. Such regions hence define a sort of virtualised space that is "fluid", since its structure adapts in response to pressure forces exerted by the underlying phenomenon. We provide an adaptive sampling algorithm in the field-based coordination framework, and prove it is self-sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;AI&#38598;&#25104;&#21040;3D&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20214;3DALL-E&#65292;&#35774;&#35745;&#24072;&#21487;&#20197;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#28789;&#24863;&#26500;&#24314;3D&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35774;&#35745;&#24072;&#23545;&#20110;3DALL-E&#22312;&#24037;&#20316;&#27969;&#31243;&#20013;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#21442;&#32771;&#22270;&#20687;&#12289;&#38450;&#27490;&#35774;&#35745;&#22266;&#21270;&#24182;&#28608;&#21457;&#35774;&#35745;&#30340;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2210.11603</link><description>&lt;p&gt;
3DALL-E: &#23558;&#25991;&#26412;&#21040;&#22270;&#20687;AI&#38598;&#25104;&#21040;3D&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#20013;
&lt;/p&gt;
&lt;p&gt;
3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows. (arXiv:2210.11603v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;AI&#38598;&#25104;&#21040;3D&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20214;3DALL-E&#65292;&#35774;&#35745;&#24072;&#21487;&#20197;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#28789;&#24863;&#26500;&#24314;3D&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35774;&#35745;&#24072;&#23545;&#20110;3DALL-E&#22312;&#24037;&#20316;&#27969;&#31243;&#20013;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#21442;&#32771;&#22270;&#20687;&#12289;&#38450;&#27490;&#35774;&#35745;&#22266;&#21270;&#24182;&#28608;&#21457;&#35774;&#35745;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;AI&#33021;&#22815;&#29983;&#25104;&#26032;&#39062;&#30340;&#22270;&#20687;&#20197;&#20379;&#28789;&#24863;&#65292;&#20294;&#20854;&#22312;3D&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#24072;&#22914;&#20309;&#21033;&#29992;AI&#25552;&#20379;&#30340;&#28789;&#24863;&#26500;&#24314;3D&#27169;&#22411;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;DALL-E&#12289;GPT-3&#21644;CLIP&#38598;&#25104;&#21040;CAD&#36719;&#20214;&#20013;&#65292;&#21019;&#24314;&#20102;3DALL-E&#25554;&#20214;&#65292;&#29992;&#20110;&#29983;&#25104;3D&#35774;&#35745;&#30340;2D&#22270;&#20687;&#28789;&#24863;&#12290;3DALL-E&#20801;&#35768;&#29992;&#25143;&#22522;&#20110;&#20182;&#20204;&#27491;&#22312;&#24314;&#27169;&#30340;&#20869;&#23481;&#26500;&#24314;&#25991;&#26412;&#21644;&#22270;&#20687;&#25552;&#31034;&#12290;&#22312;&#19968;&#39033;&#28041;&#21450;13&#21517;&#35774;&#35745;&#24072;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35774;&#35745;&#24072;&#20204;&#35748;&#20026;3DALL-E&#22312;&#20182;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;AI&#29983;&#25104;&#21442;&#32771;&#22270;&#20687;&#65292;&#38450;&#27490;&#35774;&#35745;&#22266;&#21270;&#65292;&#24182;&#28608;&#21457;&#35774;&#35745;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#35814;&#36848;&#20102;&#22312;3D&#24314;&#27169;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#25552;&#31034;&#27169;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#21442;&#19982;&#32773;&#25152;&#35266;&#23519;&#21040;&#30340;&#25552;&#31034;&#22797;&#26434;&#24615;&#30340;&#24230;&#37327;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;3DALL-E&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#30456;&#32467;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#31034;&#25991;&#29486;&#30446;&#24405;&#20316;&#20026;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#35774;&#35745;&#21382;&#21490;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image AI are capable of generating novel images for inspiration, but their applications for 3D design workflows and how designers can build 3D models using AI-provided inspiration have not yet been explored. To investigate this, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a plugin that generates 2D image inspiration for 3D design. 3DALL-E allows users to construct text and image prompts based on what they are modeling. In a study with 13 designers, we found that designers saw great potential in 3DALL-E within their workflows and could use text-to-image AI to produce reference images, prevent design fixation, and inspire design considerations. We elaborate on prompting patterns observed across 3D modeling tasks and provide measures of prompt complexity observed across participants. From our findings, we discuss how 3DALL-E can merge with existing generative design workflows and propose prompt bibliographies as a form of human-AI design history.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#22330;&#22806;&#20132;&#26131;&#24066;&#22330;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#21644;&#20849;&#20139;&#31574;&#30053;&#23398;&#20064;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#21040;&#28085;&#30422;&#21033;&#28070;&#25439;&#22833;&#12289;&#26368;&#20248;&#25191;&#34892;&#21644;&#24066;&#22330;&#20221;&#39069;&#31561;&#22810;&#30446;&#26631;&#30340;&#26032;&#20852;&#34892;&#20026;&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26657;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.07184</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#22330;&#22806;&#20132;&#26131;&#24066;&#22330;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Towards Multi-Agent Reinforcement Learning driven Over-The-Counter Market Simulations. (arXiv:2210.07184v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#22330;&#22806;&#20132;&#26131;&#24066;&#22330;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#21644;&#20849;&#20139;&#31574;&#30053;&#23398;&#20064;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#21040;&#28085;&#30422;&#21033;&#28070;&#25439;&#22833;&#12289;&#26368;&#20248;&#25191;&#34892;&#21644;&#24066;&#22330;&#20221;&#39069;&#31561;&#22810;&#30446;&#26631;&#30340;&#26032;&#20852;&#34892;&#20026;&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26657;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22330;&#22806;&#20132;&#26131;&#24066;&#22330;&#20013;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#21644;&#27969;&#21160;&#24615;&#25509;&#21463;&#32773;&#20195;&#29702;&#20043;&#38388;&#30340;&#21338;&#24328;&#65292;&#20856;&#22411;&#31034;&#20363;&#26159;&#22806;&#27719;&#24066;&#22330;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#21442;&#25968;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#23478;&#26063;&#65292;&#24182;&#32467;&#21512;&#20849;&#20139;&#31574;&#30053;&#23398;&#20064;&#65292;&#26500;&#24314;&#20986;&#36825;&#20010;&#38382;&#39064;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#30456;&#20114;&#23545;&#25112;&#65292;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#21040;&#30456;&#23545;&#20110;&#21033;&#28070;&#25439;&#22833;&#12289;&#26368;&#20248;&#25191;&#34892;&#21644;&#24066;&#22330;&#20221;&#39069;&#31561;&#24191;&#27867;&#30446;&#26631;&#30340;&#26032;&#20852;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#33258;&#28982;&#22320;&#23398;&#20064;&#21040;&#24179;&#34913;&#23545;&#20914;&#21644;&#20559;&#26012;&#30340;&#31574;&#30053;&#65292;&#20854;&#20013;&#20559;&#26012;&#26159;&#25351;&#26681;&#25454;&#20854;&#24211;&#23384;&#37327;&#23558;&#20080;&#20837;&#20215;&#26684;&#21644;&#21334;&#20986;&#20215;&#26684;&#35774;&#32622;&#20026;&#38750;&#23545;&#31216;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26657;&#20934;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#22312;&#23545;&#21338;&#24328;&#22343;&#34913;&#26045;&#21152;&#32422;&#26463;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#25105;&#20204;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#22312;&#36716;&#25442;&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a game between liquidity provider and liquidity taker agents interacting in an over-the-counter market, for which the typical example is foreign exchange. We show how a suitable design of parameterized families of reward functions coupled with shared policy learning constitutes an efficient solution to this problem. By playing against each other, our deep-reinforcement-learning-driven agents learn emergent behaviors relative to a wide spectrum of objectives encompassing profit-and-loss, optimal execution and market share. In particular, we find that liquidity providers naturally learn to balance hedging and skewing, where skewing refers to setting their buy and sell prices asymmetrically as a function of their inventory. We further introduce a novel RL-based calibration algorithm which we found performed well at imposing constraints on the game equilibrium. On the theoretical side, we are able to show convergence rates for our multi-agent policy gradient algorithm under a tran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#21152;&#26435;&#26377;&#21521;&#22270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21487;&#20197;&#22810;&#27425;&#35745;&#31639;&#65288;&#20272;&#35745;&#65289;&#36793;&#32536;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#35299;&#20915;&#20102;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.11489</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#20010;&#36793;&#32536;&#25104;&#26412;&#20272;&#35745;&#30340;&#22270;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
A Generalization of the Shortest Path Problem to Graphs with Multiple Edge-Cost Estimates. (arXiv:2208.11489v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#21152;&#26435;&#26377;&#21521;&#22270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21487;&#20197;&#22810;&#27425;&#35745;&#31639;&#65288;&#20272;&#35745;&#65289;&#36793;&#32536;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#35299;&#20915;&#20102;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a generalized framework for weighted directed graphs, where edge weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense, solving the uncertainty of the shortest path problem.
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#26159;AI&#29702;&#35770;&#21644;&#24212;&#29992;&#30340;&#22522;&#30707;&#12290;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#24573;&#30053;&#36793;&#32536;&#26435;&#37325;&#35745;&#31639;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#21152;&#26435;&#26377;&#21521;&#22270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21487;&#20197;&#22810;&#27425;&#35745;&#31639;&#65288;&#20272;&#35745;&#65289;&#36793;&#32536;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65292;&#20248;&#21270;&#36335;&#24452;&#25104;&#26412;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#20219;&#20309;&#26102;&#20505;&#35299;&#20915;&#26041;&#26696;&#31639;&#27861;&#65292;&#23454;&#35777;&#35777;&#26126;&#20102;&#20854;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The shortest path problem in graphs is a cornerstone of AI theory and applications. Existing algorithms generally ignore edge weight computation time. In this paper we present a generalized framework for weighted directed graphs, where edge weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense. This raises a generalized shortest path problem that optimize different aspects of path cost and its uncertainty. We present a complete anytime solution algorithm for the generalized problem, and empirically demonstrate its efficacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#27169;&#24335;&#33021;&#22815;&#35299;&#20915;&#20197;&#33258;&#21033;&#20026;&#23548;&#21521;&#30340;&#32456;&#36523;&#23398;&#20064;&#20010;&#20307;&#20013;&#30340;&#20998;&#24037;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.05568</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#65292;&#20998;&#24037;&#30340;&#24418;&#25104;
&lt;/p&gt;
&lt;p&gt;
The emergence of division of labor through decentralized social sanctioning. (arXiv:2208.05568v4 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#27169;&#24335;&#33021;&#22815;&#35299;&#20915;&#20197;&#33258;&#21033;&#20026;&#23548;&#21521;&#30340;&#32456;&#36523;&#23398;&#20064;&#20010;&#20307;&#20013;&#30340;&#20998;&#24037;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29983;&#24577;&#25104;&#21151;&#20381;&#36182;&#20110;&#25105;&#20204;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21363;&#28789;&#27963;&#33258;&#32452;&#32455;&#25104;&#21512;&#20316;&#31038;&#20250;&#32676;&#20307;&#65292;&#20854;&#20013;&#26368;&#25104;&#21151;&#30340;&#32676;&#20307;&#37319;&#29992;&#20102;&#22823;&#37327;&#30340;&#19987;&#19994;&#21270;&#21644;&#20998;&#24037;&#12290;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#21160;&#29289;&#19981;&#21516;&#65292;&#20154;&#31867;&#36890;&#36807;&#19968;&#29983;&#30340;&#35797;&#38169;&#20013;&#23398;&#20064;&#33258;&#24049;&#35201;&#25198;&#28436;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#20851;&#38190;&#35282;&#33394;&#27604;&#20854;&#20182;&#35282;&#33394;&#26356;&#20855;&#21560;&#24341;&#21147;&#65292;&#24182;&#19988;&#20010;&#20307;&#26159;&#33258;&#21033;&#30340;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#31038;&#20250;&#22256;&#22659;&#65306;&#27599;&#20010;&#20010;&#20307;&#37117;&#24076;&#26395;&#20854;&#20182;&#20154;&#25198;&#28436;&#20851;&#38190;&#20294;&#26080;&#25253;&#37228;&#30340;&#35282;&#33394;&#65292;&#36825;&#26679;&#20182;&#20204;&#21487;&#20197;&#33258;&#30001;&#36873;&#25321;&#19968;&#20010;&#25253;&#37228;&#26356;&#39640;&#30340;&#35282;&#33394;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#27599;&#20010;&#20154;&#37117;&#36825;&#26679;&#34892;&#20107;&#65292;&#19988;&#19968;&#20010;&#20851;&#38190;&#35282;&#33394;&#32570;&#20047;&#22635;&#34917;&#65292;&#23601;&#20250;&#21457;&#29983;&#28798;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#26368;&#20339;&#35282;&#33394;&#20998;&#37197;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65306;&#22914;&#20309;&#22312;&#19968;&#32676;&#20197;&#33258;&#21033;&#20026;&#23548;&#21521;&#30340;&#32456;&#36523;&#23398;&#20064;&#20010;&#20307;&#20013;&#24418;&#25104;&#20998;&#24037;&#21602;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#27169;&#22411;&#65288;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#20998;&#25955;&#31038;&#20250;&#21046;&#35009;&#30340;&#20986;&#29616;&#27169;&#24335;&#65289;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human ecological success relies on our characteristic ability to flexibly self-organize into cooperative social groups, the most successful of which employ substantial specialization and division of labor. Unlike most other animals, humans learn by trial and error during their lives what role to take on. However, when some critical roles are more attractive than others, and individuals are self-interested, then there is a social dilemma: each individual would prefer others take on the critical-but-unremunerative roles so they may remain free to take one that pays better. But disaster occurs if all act thusly and a critical role goes unfilled. In such situations learning an optimum role distribution may not be possible. Consequently, a fundamental question is: how can division of labor emerge in groups of self-interested lifetime-learning individuals? Here we show that by introducing a model of social norms, which we regard as emerging patterns of decentralized social sanctioning, it be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2207.11749</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26410;&#30693;&#25968;&#37327;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#28304;&#20998;&#31163;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders. (arXiv:2207.11749v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#26410;&#30693;&#25968;&#37327;&#20449;&#21495;&#30340;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#31995;&#32479;&#30340;&#24615;&#33021;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#36755;&#20986;&#19982;&#30446;&#26631;&#23545;&#40784;&#24341;&#36215;&#30340;&#25490;&#21015;&#38382;&#39064;&#23548;&#33268;&#30340;&#32500;&#24230;&#28798;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20004;&#27493;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;&#26377;&#38745;&#38899;&#36890;&#36947;&#30340;&#24773;&#20917;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#28151;&#21512;&#30340;&#36752;&#23556;&#33337;&#22122;&#22768;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36798;&#21040;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few existing studies focus on the source separation problem with unknown numbers of signals, and how to evaluate the performances of the systems is not yet clear. We propose a solution with a fixed number of output channels to address these two problems, enabling it to avoid the dimensional disaster caused by the permutation problem induced by the alignment of outputs to targets. Specifically, we propose a two-step algorithm based on autoencoders and a new performance evaluation method for situations with mute channels. Experiments conducted on simulated mixtures of radiated ship noise show that the proposed solution can achieve similar separation performance to that attained with a known number of signals. The proposed algorithm achieved competitive performance as two algorithms developed for known numbers of signals, which is highly explainable and extensible and get the state of the art under this framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#35843;&#35797;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.09787</link><description>&lt;p&gt;
&#21487;&#20105;&#35758;&#31070;&#32463;&#32593;&#32476;&#30340;&#22240;&#26524;&#21457;&#29616;&#19982;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery and Knowledge Injection for Contestable Neural Networks. (arXiv:2205.09787v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#35843;&#35797;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#23398;&#20064;&#21040;&#20102;&#30456;&#20851;&#30340;&#22240;&#26524;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#65292;&#32780;&#23427;&#20204;&#30340;&#40657;&#31665;&#29305;&#24615;&#20351;&#24471;&#27169;&#22411;&#26500;&#24314;&#32773;&#38590;&#20197;&#29702;&#35299;&#21644;&#35843;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#26426;&#22120;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#23454;&#29616;&#21452;&#21521;&#20114;&#21160;&#12290;&#25152;&#23398;&#27169;&#22411;&#20445;&#35777;&#31526;&#21512;&#22240;&#26524;&#22270;&#24182;&#36981;&#24490;&#19987;&#23478;&#30693;&#35782;&#65292;&#20854;&#20013;&#37096;&#20998;&#30693;&#35782;&#20063;&#21487;&#20197;&#20107;&#20808;&#32473;&#23450;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#34892;&#20026;&#36827;&#34892;&#21487;&#35270;&#21270;&#24182;&#23454;&#29616;&#30693;&#35782;&#27880;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#24182;&#25903;&#25745;&#39044;&#27979;&#30340;&#20174;&#19994;&#32773;&#36827;&#34892;&#35843;&#35797;&#12290;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#39640;&#36798;2.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have proven to be effective at solving machine learning tasks but it is unclear whether they learn any relevant causal relationships, while their black-box nature makes it difficult for modellers to understand and debug them. We propose a novel method overcoming these issues by allowing a two-way interaction whereby neural-network-empowered machines can expose the underpinning learnt causal graphs and humans can contest the machines by modifying the causal graphs before re-injecting them into the machines. The learnt models are guaranteed to conform to the graphs and adhere to expert knowledge, some of which can also be given up-front. By building a window into the model behaviour and enabling knowledge injection, our method allows practitioners to debug networks based on the causal structure discovered from the data and underpinning the predictions. Experiments with real and synthetic tabular data show that our method improves predictive performance up to 2.4x while pr
&lt;/p&gt;</description></item><item><title>&#36229;&#32500;&#35745;&#31639;&#65288;HDC/VSA&#65289;&#26159;&#19968;&#31181;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#39640;&#32500;&#20998;&#24067;&#24335;&#34920;&#31034;&#21644;&#20195;&#25968;&#25805;&#20316;&#30340;&#29305;&#24615;&#65292;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#31526;&#21495;&#34920;&#31034;&#21644;&#21521;&#37327;&#20998;&#24067;&#24335;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;&#26412;&#32508;&#36848;&#30340;&#31532;&#20108;&#37096;&#20998;&#20171;&#32461;&#20102;HDC/VSA&#30340;&#24212;&#29992;&#12289;&#35748;&#30693;&#27169;&#22411;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2112.15424</link><description>&lt;p&gt;
&#36229;&#32500;&#35745;&#31639;&#65288;&#20063;&#34987;&#31216;&#20026;&#21521;&#37327;&#31526;&#21495;&#21270;&#26550;&#26500;&#65289;&#32508;&#36848;&#65292;&#31532;&#20108;&#37096;&#20998;&#65306;&#24212;&#29992;&#12289;&#35748;&#30693;&#27169;&#22411;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges. (arXiv:2112.15424v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.15424
&lt;/p&gt;
&lt;p&gt;
&#36229;&#32500;&#35745;&#31639;&#65288;HDC/VSA&#65289;&#26159;&#19968;&#31181;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#39640;&#32500;&#20998;&#24067;&#24335;&#34920;&#31034;&#21644;&#20195;&#25968;&#25805;&#20316;&#30340;&#29305;&#24615;&#65292;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#31526;&#21495;&#34920;&#31034;&#21644;&#21521;&#37327;&#20998;&#24067;&#24335;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;&#26412;&#32508;&#36848;&#30340;&#31532;&#20108;&#37096;&#20998;&#20171;&#32461;&#20102;HDC/VSA&#30340;&#24212;&#29992;&#12289;&#35748;&#30693;&#27169;&#22411;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#32508;&#36848;&#30340;&#31532;&#20108;&#37096;&#20998;&#65292;&#19987;&#38376;&#20171;&#32461;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#26368;&#24120;&#34987;&#31216;&#20026;&#36229;&#32500;&#35745;&#31639;&#21644;&#21521;&#37327;&#31526;&#21495;&#21270;&#26550;&#26500;&#65288;HDC/VSA&#65289;&#12290;&#36825;&#20004;&#20010;&#21517;&#31216;&#37117;&#25351;&#30340;&#26159;&#19968;&#31995;&#21015;&#20351;&#29992;&#39640;&#32500;&#20998;&#24067;&#24335;&#34920;&#31034;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#20381;&#36182;&#20110;&#20854;&#20851;&#38190;&#25805;&#20316;&#30340;&#20195;&#25968;&#24615;&#36136;&#65292;&#20197;&#34701;&#21512;&#32467;&#26500;&#21270;&#31526;&#21495;&#34920;&#31034;&#21644;&#21521;&#37327;&#20998;&#24067;&#24335;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;&#20840;&#24687;&#21270;&#31616;&#34920;&#31034;&#26159;&#19968;&#20010;&#26377;&#24433;&#21709;&#21147;&#30340;HDC/VSA&#27169;&#22411;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#38750;&#24120;&#30693;&#21517;&#65292;&#24182;&#24120;&#29992;&#20110;&#25351;&#20195;&#25972;&#20010;&#23478;&#26063;&#12290;&#28982;&#32780;&#65292;&#20986;&#20110;&#19968;&#33268;&#24615;&#32771;&#34385;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#20351;&#29992;HDC/VSA&#26469;&#31216;&#21628;&#35813;&#39046;&#22495;&#12290;&#26412;&#32508;&#36848;&#30340;&#31532;&#19968;&#37096;&#20998;&#28085;&#30422;&#20102;&#35813;&#39046;&#22495;&#30340;&#22522;&#30784;&#26041;&#38754;&#65292;&#20363;&#22914;&#23548;&#33268;HDC/VSA&#21457;&#23637;&#30340;&#21382;&#21490;&#32972;&#26223;&#12289;&#20219;&#20309;HDC/VSA&#27169;&#22411;&#30340;&#20851;&#38190;&#35201;&#32032;&#12289;&#24050;&#30693;&#30340;HDC/VSA&#27169;&#22411;&#65292;&#20197;&#21450;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#21512;HDC/VSA&#30340;&#39640;&#32500;&#21521;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is Part II of the two-part comprehensive survey devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Holographic Reduced Representations is an influential HDC/VSA model that is well-known in the machine learning domain and often used to refer to the whole family. However, for the sake of consistency, we use HDC/VSA to refer to the field. Part I of this survey covered foundational aspects of the field, such as the historical context leading to the development of HDC/VSA, key elements of any HDC/VSA model, known HDC/VSA models, and the transformation of input data of various types into high-dimensional vectors suitable for H
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20171;&#32461;&#20102;&#36229;&#32500;&#35745;&#31639;&#21644;&#30690;&#37327;&#31526;&#21495;&#21270;&#26550;&#26500;&#65288;HDC/VSA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#39640;&#32500;&#20998;&#24067;&#24335;&#34920;&#31034;&#21644;&#20195;&#25968;&#23646;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#32467;&#26500;&#21270;&#31526;&#21495;&#34920;&#31034;&#21644;&#21521;&#37327;&#20998;&#24067;&#24335;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;&#35813;&#39046;&#22495;&#28041;&#21450;&#22810;&#20010;&#23398;&#31185;&#65292;&#24182;&#20171;&#32461;&#20102;&#22810;&#20010;&#30456;&#20851;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2111.06077</link><description>&lt;p&gt;
&#12298;&#36229;&#32500;&#35745;&#31639;&#32508;&#36848;&#21450;&#30690;&#37327;&#31526;&#21495;&#21270;&#26550;&#26500;&#12299;&#31532;&#19968;&#37096;&#20998;&#65306;&#27169;&#22411;&#21644;&#25968;&#25454;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations. (arXiv:2111.06077v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20171;&#32461;&#20102;&#36229;&#32500;&#35745;&#31639;&#21644;&#30690;&#37327;&#31526;&#21495;&#21270;&#26550;&#26500;&#65288;HDC/VSA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#39640;&#32500;&#20998;&#24067;&#24335;&#34920;&#31034;&#21644;&#20195;&#25968;&#23646;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#32467;&#26500;&#21270;&#31526;&#21495;&#34920;&#31034;&#21644;&#21521;&#37327;&#20998;&#24067;&#24335;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;&#35813;&#39046;&#22495;&#28041;&#21450;&#22810;&#20010;&#23398;&#31185;&#65292;&#24182;&#20171;&#32461;&#20102;&#22810;&#20010;&#30456;&#20851;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20849;&#20998;&#20004;&#37096;&#20998;&#65292;&#33268;&#21147;&#20110;&#20171;&#32461;&#19968;&#20010;&#34987;&#31216;&#20026;&#36229;&#32500;&#35745;&#31639;&#21644;&#30690;&#37327;&#31526;&#21495;&#21270;&#26550;&#26500;&#65288;HDC/VSA&#65289;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;HDC/VSA&#26159;&#19968;&#31867;&#20351;&#29992;&#39640;&#32500;&#20998;&#24067;&#24335;&#34920;&#31034;&#21644;&#20381;&#36182;&#20854;&#20851;&#38190;&#25805;&#20316;&#30340;&#20195;&#25968;&#23646;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20197;&#34701;&#21512;&#32467;&#26500;&#21270;&#31526;&#21495;&#34920;&#31034;&#21644;&#21521;&#37327;&#20998;&#24067;&#24335;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;HDC/VSA&#23478;&#26063;&#20013;&#30340;&#26174;&#33879;&#27169;&#22411;&#21253;&#25324;&#24352;&#37327;&#31215;&#34920;&#31034;&#12289;&#20840;&#24687;&#20943;&#23569;&#34920;&#31034;&#12289;&#20056;&#21152;&#32622;&#25442;&#12289;&#20108;&#20803;&#25955;&#23556;&#30721;&#21644;&#31232;&#30095;&#20108;&#20803;&#20998;&#24067;&#24335;&#34920;&#31034;&#65292;&#36824;&#26377;&#20854;&#20182;&#27169;&#22411;&#12290;HDC/VSA&#26159;&#19968;&#20010;&#39640;&#24230;&#36328;&#23398;&#31185;&#30340;&#39046;&#22495;&#65292;&#28041;&#21450;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#30005;&#23376;&#24037;&#31243;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#25968;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#12290;&#36825;&#19968;&#20107;&#23454;&#20351;&#24471;&#23545;&#35813;&#39046;&#22495;&#36827;&#34892;&#20840;&#38754;&#27010;&#36848;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary field with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the field. However, due to a surge of new researchers joining the field in recent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21160;&#24577;&#38598;&#20307;&#26234;&#33021;&#23398;&#20064;&#65288;DCIL&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#28860;&#30340;&#26799;&#24230;&#26356;&#26032;&#21098;&#26525;&#26435;&#37325;&#65292;&#36890;&#36807;&#24418;&#25104;&#21452;&#21521;&#36716;&#21457;&#36335;&#24452;&#26469;&#23547;&#25214;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#21098;&#26525;&#21644;&#26410;&#21098;&#26525;&#26435;&#37325;&#30340;&#38598;&#20307;&#26234;&#33021;&#20043;&#38388;&#30340;&#23398;&#20064;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2109.04660</link><description>&lt;p&gt;
&#21160;&#24577;&#38598;&#20307;&#26234;&#33021;&#23398;&#20064;&#65306;&#36890;&#36807;&#31934;&#28860;&#30340;&#26799;&#24230;&#25214;&#21040;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#20197;&#21098;&#26525;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights. (arXiv:2109.04660v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21160;&#24577;&#38598;&#20307;&#26234;&#33021;&#23398;&#20064;&#65288;DCIL&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#28860;&#30340;&#26799;&#24230;&#26356;&#26032;&#21098;&#26525;&#26435;&#37325;&#65292;&#36890;&#36807;&#24418;&#25104;&#21452;&#21521;&#36716;&#21457;&#36335;&#24452;&#26469;&#23547;&#25214;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#21098;&#26525;&#21644;&#26410;&#21098;&#26525;&#26435;&#37325;&#30340;&#38598;&#20307;&#26234;&#33021;&#20043;&#38388;&#30340;&#23398;&#20064;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#22686;&#38271;&#65292;DNN&#21442;&#25968;&#30340;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#12290;&#36825;&#20351;&#24471;DNN&#27169;&#22411;&#38590;&#20197;&#37096;&#32626;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#21160;&#24577;&#21098;&#26525;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#30452;&#36890;&#20272;&#35745;&#65288;STE&#65289;&#26469;&#36817;&#20284;&#21098;&#26525;&#26435;&#37325;&#30340;&#26799;&#24230;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23547;&#25214;&#19981;&#21516;&#30340;&#31232;&#30095;&#27169;&#24335;&#12290;STE&#21487;&#20197;&#24110;&#21161;&#21098;&#26525;&#26435;&#37325;&#22312;&#23547;&#25214;&#21160;&#24577;&#31232;&#30095;&#27169;&#24335;&#30340;&#36807;&#31243;&#20013;&#22797;&#27963;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#31895;&#31961;&#30340;&#26799;&#24230;&#20250;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;STE&#36817;&#20284;&#30340;&#26799;&#24230;&#20449;&#21495;&#19981;&#21487;&#38752;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31934;&#28860;&#30340;&#26799;&#24230;&#26469;&#26356;&#26032;&#21098;&#26525;&#26435;&#37325;&#65292;&#36890;&#36807;&#20174;&#20004;&#32452;&#65288;&#21098;&#26525;&#21644;&#26410;&#21098;&#26525;&#65289;&#26435;&#37325;&#24418;&#25104;&#21452;&#21521;&#36716;&#21457;&#36335;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#38598;&#20307;&#26234;&#33021;&#23398;&#20064;&#65288;DCIL&#65289;&#65292;&#21033;&#29992;&#20004;&#32452;&#26435;&#37325;&#30340;&#38598;&#20307;&#26234;&#33021;&#20043;&#38388;&#30340;&#23398;&#20064;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growth of deep neural networks (DNN), the number of DNN parameters has drastically increased. This makes DNN models hard to be deployed on resource-limited embedded systems. To alleviate this problem, dynamic pruning methods have emerged, which try to find diverse sparsity patterns during training by utilizing Straight-Through-Estimator (STE) to approximate gradients of pruned weights. STE can help the pruned weights revive in the process of finding dynamic sparsity patterns. However, using these coarse gradients causes training instability and performance degradation owing to the unreliable gradient signal of the STE approximation. In this work, to tackle this issue, we introduce refined gradients to update the pruned weights by forming dual forwarding paths from two sets (pruned and unpruned) of weights. We propose a novel Dynamic Collective Intelligence Learning (DCIL) which makes use of the learning synergy between the collective intelligence of both weight sets. We verify
&lt;/p&gt;</description></item><item><title>&#36229;&#21322;&#24452;&#30340;&#25195;&#38647;&#28216;&#25103;&#23646;&#20110;P&#31867;&#38382;&#39064;&#65292;&#36825;&#19968;&#21457;&#29616;&#19981;&#20165;&#36866;&#29992;&#20110;&#25195;&#38647;&#28216;&#25103;&#26412;&#36523;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#36229;&#21322;&#24452;&#24179;&#38754;&#19978;&#23884;&#20837;&#22270;&#30340;&#35868;&#39064;&#12290;</title><link>http://arxiv.org/abs/2002.09534</link><description>&lt;p&gt;
&#36229;&#21322;&#24452;&#30340;&#25195;&#38647;&#28216;&#25103;&#22312;P&#31867;&#38382;&#39064;&#20013;&#12290;(arXiv:2002.09534v2 [cs.CC] &#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Minesweeper is in P. (arXiv:2002.09534v2 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.09534
&lt;/p&gt;
&lt;p&gt;
&#36229;&#21322;&#24452;&#30340;&#25195;&#38647;&#28216;&#25103;&#23646;&#20110;P&#31867;&#38382;&#39064;&#65292;&#36825;&#19968;&#21457;&#29616;&#19981;&#20165;&#36866;&#29992;&#20110;&#25195;&#38647;&#28216;&#25103;&#26412;&#36523;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#36229;&#21322;&#24452;&#24179;&#38754;&#19978;&#23884;&#20837;&#22270;&#30340;&#35868;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#25195;&#38647;&#28216;&#25103;&#26159;NP&#23436;&#20840;&#30340;&#65292;&#20294;&#20854;&#36229;&#21322;&#24452;&#21464;&#20307;&#23646;&#20110;P&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#19981;&#20381;&#36182;&#20110;&#25195;&#38647;&#28216;&#25103;&#30340;&#35268;&#21017;&#65292;&#32780;&#26159;&#36866;&#29992;&#20110;&#22522;&#20110;&#28385;&#36275;&#36229;&#21322;&#24452;&#24179;&#38754;&#19978;&#23884;&#20837;&#22270;&#30340;&#23616;&#37096;&#32422;&#26463;&#30340;&#20219;&#20309;&#35868;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that, while Minesweeper is NP-complete, its hyperbolic variant is in P. Our proof does not rely on the rules of Minesweeper, but is valid for any puzzle based on satisfying local constraints on a graph embedded in the hyperbolic plane.
&lt;/p&gt;</description></item></channel></rss>